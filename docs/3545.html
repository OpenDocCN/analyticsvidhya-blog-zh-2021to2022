<html>
<head>
<title>Explaining neural networks 101</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释神经网络101</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/explaining-neural-networks-101-a36356113cbd?source=collection_archive---------9-----------------------#2021-07-08">https://medium.com/analytics-vidhya/explaining-neural-networks-101-a36356113cbd?source=collection_archive---------9-----------------------#2021-07-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/8f5aadfd095287cc57a68ea4769a1d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7XryJtBBZPn25PkKNJWGWQ.jpeg"/></div></div></figure><p id="0447" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">神经网络反映了人脑的行为。它们允许程序识别模式并解决机器学习中的常见问题。这是执行回归分析分类的另一种选择。如果你没有看到我的关于<a class="ae jn" rel="noopener" href="/geekculture/logistics-regression-in-5-minutes-1-3-64a890ed79e1">物流回归</a>的系列文章，那么先看看那些，因为这个系列将使用相同的一组数据。在<a class="ae jn" href="http://www.rapidtrade.com" rel="noopener ugc nofollow" target="_blank"> Rapidtrade </a>，我们使用神经网络对数据进行分类，并运行回归场景。</p><p id="e2cf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以为了<strong class="ir hi">让</strong>我们将在这个系列中使用的数据可视化，请看下面。我们将使用它来训练网络根据j列对我们的客户进行分类。我们还将使用突出显示的3个特征对我们的客户进行分类。特性选择很重要，看看这篇<a class="ae jn" rel="noopener" href="/geekculture/classifying-customers-with-logistics-regression-one-vs-all-f34ed2e5f042">文章</a>就知道我为什么选择这3个特性了。</p><blockquote class="jo jp jq"><p id="1d84" class="ip iq jr ir b is it iu iv iw ix iy iz js jb jc jd jt jf jg jh ju jj jk jl jm ha bi translated">请记住，我们将把所有的字母字符串值转换成数字。毕竟，我们不能将字符串插入方程；-)</p></blockquote><figure class="jw jx jy jz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es jv"><img src="../Images/e728c373a2114047d4abe569ca945ab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZRZKJPQ8eyUn7NW8LNU-qQ.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">图1</figcaption></figure></div><div class="ab cl ke kf go kg" role="separator"><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj"/></div><div class="ha hb hc hd he"><p id="5798" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">神经网络总是由层组成，如图2所示。这一切看起来很复杂，但让我们解开它，使它更容易理解。</p><figure class="jw jx jy jz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kl"><img src="../Images/6e53a4798900e76ded19ca148988920b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*coQrImdArcAsRdB7VHdFXg.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">图2</figcaption></figure><p id="9965" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一个神经网络有6个重要的概念，我将在这里简单解释一下，但在这一系列文章中会详细介绍。</p><p id="6f9e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">- <strong class="ir hi">权重</strong> — <em class="jr">这些就像我们在其他算法中使用的</em> <strong class="ir hi"> <em class="jr"> theta的</em></strong><em class="jr"><br/>-<strong class="ir hi">层</strong> — <em class="jr">我们的网络将有3层</em> <br/> - <strong class="ir hi">前向传播</strong> — <em class="jr">使用特征/权重来获得Z和一个</em> <br/> - <strong class="ir hi">反向传播</strong></em></p><p id="5181" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这个系列中，我们将建立一个三层的神经网络。我们的将有以下3层。</p><h1 id="f872" class="km kn hh bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">输入层</h1><p id="674f" class="pw-post-body-paragraph ip iq hh ir b is lk iu iv iw ll iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">我们将这个结果称为<strong class="ir hi"> A1 </strong>。该图层的<strong class="ir hi">大小</strong> (# units) <strong class="ir hi"> </strong>取决于我们数据集中要素的<strong class="ir hi">数量。构建我们的输入层并不困难，你只需简单地用X </strong>复制<strong class="ir hi">，但是添加一个被称为<strong class="ir hi">偏向层</strong>，默认为“1”。<br/>第1栏:<strong class="ir hi">偏向</strong>图层默认为“1”<br/>第2栏:<strong class="ir hi">“曾经结婚”</strong>我们的第一个特征，并已被重新标记为1/2 <br/>第3栏:<strong class="ir hi">“毕业”</strong>我们的第二个特征，并被重新标记为1/2 <br/>第4栏:<strong class="ir hi">“家庭规模”</strong>我们的第四个特征</strong></p><figure class="jw jx jy jz fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/e1032d57a3af8d454f9bc36a74f21b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*zgYUhSI_qifZqtupfTxNVA.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">图3:可视化A1 —输入层</figcaption></figure><h1 id="fc53" class="km kn hh bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">隐蔽层</h1><p id="6fc6" class="pw-post-body-paragraph ip iq hh ir b is lk iu iv iw ll iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">我们只有<strong class="ir hi"> 1个隐藏层</strong>，但是你可以为每个特征设置一个隐藏层。如果你有比我下面提到的逻辑更多的隐藏层，你将为每个隐藏层重复计算。大小(单位数)由您决定，我们选择了#features * 2。</p><p id="a197" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该层是在向前和向后传播期间计算的。运行<strong class="ir hi">和</strong>这两个步骤后，我们<strong class="ir hi">计算每个单元的Z2、A2和S2 </strong>。运行每个步骤后的输出如下。</p><p id="dd1d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">正向传播</strong></p><p id="0deb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这一步，我们计算Z2和A2。你可以看到下面的结果。<br/> - <strong class="ir hi"> Z2 </strong>包含我们对隐藏层中的<strong class="ir hi">假设</strong>计算的结果。<br/> -而<strong class="ir hi"> A2 </strong>也包括偏置层(col 1)并具有应用于Z2中每个单元格的sigmoid函数。</p><p id="5af4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此Z2有6列，A2有7列。</p><blockquote class="jo jp jq"><p id="479e" class="ip iq jr ir b is it iu iv iw ix iy iz js jb jc jd jt jf jg jh ju jj jk jl jm ha bi translated">现在还不要担心方程，这将在下一篇文章中介绍。</p></blockquote><figure class="jw jx jy jz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lq"><img src="../Images/c7e089123ae62afeb1c7e2b975ea8ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_z5-6uRErVsLRVoz63BmMw.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">图4:可视化Z2和A2</figcaption></figure><p id="a8fd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">反向传播</strong></p><p id="cee4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，在前向传播穿过所有层之后，我们然后执行<strong class="ir hi">反向传播</strong>步骤来计算<strong class="ir hi"> S2 </strong>。S2被称为该单位假设计算的<strong class="ir hi">δ</strong>。这用于计算该θ的<strong class="ir hi">梯度</strong>，随后，结合该单元的<strong class="ir hi">成本</strong>，帮助梯度下降计算出最佳θ/重量。</p><blockquote class="jo jp jq"><p id="de18" class="ip iq jr ir b is it iu iv iw ix iy iz js jb jc jd jt jf jg jh ju jj jk jl jm ha bi translated">再一次，方程将在后面出现，现在，理解back prop帮助我们决定每个单元中每个假设的成本/梯度。</p></blockquote><figure class="jw jx jy jz fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/ad1b929cde7e307d5a6d4a2646b30c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*faJO2RFfomX031_K6oQyXg.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">图5:可视化S2的梯度</figcaption></figure><h1 id="68a3" class="km kn hh bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">输出层</h1><p id="6aa0" class="pw-post-body-paragraph ip iq hh ir b is lk iu iv iw ll iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">我们的输出层给出了我们假设的结果。即。如果应用这些thetas，我们对这些客户分类的最佳猜测是什么。<strong class="ir hi">大小</strong>(#单位)来自Y的数字标签，或者在图1中的j列。从图1中可以看出，有7个标签，因此输出层的大小为7。</p><p id="9558" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">与隐藏层一样，这是在<strong class="ir hi">向前和向后</strong>传播的2个步骤中计算的。运行这两个步骤后，结果如下:</p><p id="df02" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">正向传播</strong></p><p id="99f4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">同样，在这一步中，我们将为输出层计算<strong class="ir hi"> Z3 </strong>和<strong class="ir hi"> A3 </strong>，就像我们为隐藏层所做的一样。参考上面的图1，可以看到不需要偏置柱，下面可以看到Z3和A3的结果。</p><figure class="jw jx jy jz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ls"><img src="../Images/f0c51e4e2e0624f94ca6688a1447594c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LLogQprqak3foK-C1UnGIw.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">图6:可视化Z3和A3</figcaption></figure><p id="9050" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">反向传播</strong></p><p id="406f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在(参考图1)我们有了Z3和A3，让我们来计算<strong class="ir hi"> S3 </strong>。由于S3只是一个基本的成本计算，从Y中减去A3，所以我们将在接下来的文章中探索这些等式，但是我们仍然可以看到下面的结果</p><figure class="jw jx jy jz fd ii er es paragraph-image"><div class="er es lt"><img src="../Images/26989b2d9d2578f76ef35f96fff91697.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*-igledduGGP4lmynC6oRpA.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">图9:想象S3</figcaption></figure><h1 id="9424" class="km kn hh bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">把所有的放在一起</h1><p id="65c5" class="pw-post-body-paragraph ip iq hh ir b is lk iu iv iw ll iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">因此，上面是一个有点尴尬，因为它可视化的输出在每一层。我们在神经网络中的主要焦点是计算神经网络成本的函数。该函数的编码将采取以下步骤。</p><ol class=""><li id="c244" class="lu lv hh ir b is it iw ix ja lw je lx ji ly jm lz ma mb mc bi translated"><strong class="ir hi">初始化</strong>一组权重/θ</li><li id="69ac" class="lu lv hh ir b is md iw me ja mf je mg ji mh jm lz ma mb mc bi translated">执行<strong class="ir hi">成本优化</strong>，执行步骤<strong class="ir hi"> (3)至(6) </strong>，直到找到用于预测的最佳权重/θ</li><li id="0446" class="lu lv hh ir b is md iw me ja mf je mg ji mh jm lz ma mb mc bi translated">按以下顺序进行<strong class="ir hi">正向传播</strong>计算:<br/>Z1&gt;A1&gt;Z2&gt;A2&gt;Z3&gt;A3</li><li id="7ab5" class="lu lv hh ir b is md iw me ja mf je mg ji mh jm lz ma mb mc bi translated">执行<strong class="ir hi">反向传播</strong>这样按顺序计算:<br/> S3 &gt; S2</li><li id="7746" class="lu lv hh ir b is md iw me ja mf je mg ji mh jm lz ma mb mc bi translated">计算正向/反向传播的<strong class="ir hi">成本</strong></li><li id="5af9" class="lu lv hh ir b is md iw me ja mf je mg ji mh jm lz ma mb mc bi translated">计算<strong class="ir hi">增量</strong>，然后计算<strong class="ir hi">梯度</strong>。(由梯度下降或成本优化使用)</li></ol><p id="baab" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好了，这是一大堆信息，继续第二部分<a class="ae jn" href="https://shaun-enslin.medium.com/forward-propagation-deep-dive-102-bbeabe4d2fb2" rel="noopener">，详细介绍正向传播</a>。</p><h1 id="cb18" class="km kn hh bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">来源</h1><p id="8217" class="pw-post-body-paragraph ip iq hh ir b is lk iu iv iw ll iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">很棒的文章:<a class="ae jn" href="https://towardsdatascience.com/under-the-hood-of-neural-network-forward-propagation-the-dreaded-matrix-multiplication-a5360b33426" rel="noopener" target="_blank">https://towards data science . com/under-the-hood-of-neural-network-forward-propagation-the-terrorible-matrix-multiplication-a 5360 b 33426</a></p><p id="1913" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你在找课程:<a class="ae jn" href="https://www.coursera.org/learn/machine-learning/" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/learn/machine-learning/</a></p></div></div>    
</body>
</html>