<html>
<head>
<title>L1, L2, and L0.5 Regularization Techniques.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">L1、L2和L0.5正则化技术。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/l1-l2-and-l0-5-regularization-techniques-a2e55dceb503?source=collection_archive---------2-----------------------#2021-05-10">https://medium.com/analytics-vidhya/l1-l2-and-l0-5-regularization-techniques-a2e55dceb503?source=collection_archive---------2-----------------------#2021-05-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/aa98033a5063df22861907dd9f2254bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*fXukzfZ5XLX1bEJsDYZj8g.jpeg"/></div></figure><p id="174b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在本文中，我的目的是给L1，L2和L0.5正则化技术一点介绍，这些技术也被称为套索，岭和弹性网回归技术。本文末尾提供了引用的链接。在开始之前，我们需要回答两个问题。</p><ol class=""><li id="1047" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jp jq jr js bi translated">什么是正规化？</li><li id="9e70" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">为什么需要它？</li></ol><p id="07dd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从第一个问题开始，简单地描述它，正则化是将系数缩小到零的行为，在这种情况下，系数是在模型训练期间学习的权重。根据这个定义，可以说在训练期间获得的权重或系数值越高，我们的机器学习模型就变得越复杂，并且这种复杂性引入了一个问题。这个问题是什么，由第二个问题回答。</p><p id="d89a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为什么需要它？简而言之，正则化用于解决训练机器学习模型时出现的过拟合问题。一个这样的例子是，获得高训练精度，但是当在“相似”但不同的数据集上使用时，您观察到难以置信的低精度。我所说的“相似”是指提供与用于训练的先前数据集密切相关的信息的数据集。现在，为了定义过拟合，这是当机器学习模型试图捕获数据集中的所有信息，但反过来也捕获数据集中的噪声时发生的情况。在这个意义上，噪声是不提供任何形式的信息，但由于随机“出现”而在数据集中发现的数据点。</p></div><div class="ab cl jy jz gp ka" role="separator"><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd"/></div><div class="hb hc hd he hf"><p id="fbd2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们知道了正则化用于防止模型在特定数据集上过度拟合，以使它们在其他数据集上表现良好，我们可以开始了。</p></div><div class="ab cl jy jz gp ka" role="separator"><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd"/></div><div class="hb hc hd he hf"><p id="a03c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> L1正则化(Lasso Regression) </strong>:</p><p id="e05d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">L1正则化技术是一种通过将系数(权重)收缩到零来减少过拟合的技术，该动作间接执行模型的特征选择。另一方面，特征选择是在数据集中选择特定特征的行为，人们认为这些特征将为机器学习模型提供有价值的信息，以便表现良好。</p><p id="31d6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">虽然特征选择的想法很好，因为在大型数据集中，一些特征对模型的学习没有贡献，因此应该被移除，但是L1正则化进行这种特征选择，以便只留下将最小化预测误差的变量。这样做的一个小缺点是，我认为这种特征选择模式引入了偏见，因为在消除一个变量之前没有考虑很多因素，从人类的角度来看，这可能不是有效的行为。</p><p id="ac2e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">控制L1正则化技术的方程是:</p><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es kf"><img src="../Images/72f314303bb8ae4259989f7a43a376a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/1*nuNRx1Wo7bpXA_5BLBjE9w.gif"/></div></figure><p id="9ce2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，当引入L1正则化方程以及残差平方和的误差函数时，我们有:</p><figure class="kg kh ki kj fd ij er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kk"><img src="../Images/f6fc2ebf6e91fc0668129e89eff7a8b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/1*-qWDHyGbXNnYaN_DIdDoIQ.gif"/></div></div></figure><p id="386a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在上述第一个等式中，<strong class="io hj">ω(</strong><strong class="io hj">ω)可以记为正则项</strong>，<strong class="io hj"> Wi是权重</strong>，而<strong class="io hj">λ(λ)是正则化参数</strong>，简单来说，正则化参数控制我们在模型上执行多少正则化。正则化项的值越高，收缩到零的系数越多，出现的特征选择越多。</p><p id="7bcc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> L2正则化(岭回归)</strong>:</p><p id="3eb8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在L2正则化中，我们将系数(权重)向零收缩，而不消除它们。这种类型的正则化迫使模型依赖于它可用的所有特征，同时仍然防止过度拟合。</p><p id="f76c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">支配L2正则化技术的方程是:</p><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/3a0661b7c0e717ec878b5ec3b7b55522.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/1*CpXfpjWTA-FsSVOt9YKa7Q.gif"/></div></figure><p id="4ed0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当我们包括残差平方和的误差函数时，我们有:</p><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/7d615f594f2805f4996dc56d08c3e70f.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/1*JA2Htdlpqj8Qm85YILAzog.gif"/></div></figure><p id="2917" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">与Lasso回归(L1)不同，在岭回归(L2)中，正则化参数的值越高，系数越向零收缩，而不是向零收缩，因此没有特征选择。</p><p id="19f1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> L0.5正则化(弹性网回归)</strong>:</p><p id="7722" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">正则化技术是L1和L2正则化技术的结合。这种技术是为了在一定程度上克服套索回归技术(L1)的小缺点而创造的。这里，弹性网络仍然执行特征选择，但是同时它使用岭回归技术(L2)找到系数(权重)的最优值。</p><p id="bfe5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">L0.5正则化技术由以下等式控制:</p><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/dd7157ba6c257ebc2b036f456bcd1e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/1*R7vNQgDRKaiGRVObxXIggg.gif"/></div></figure><p id="532c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">而当我们加上残差平方和的误差函数时，我们有:</p><figure class="kg kh ki kj fd ij er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ks"><img src="../Images/e516b04e3f844a52606cabd3b7c7b1d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/1*8TmJ1634Sbx1860ncJTVaQ.gif"/></div></div></figure><p id="e029" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">如上图r定义为混合比</strong>，即当we r=0时弹性网回归等价于岭回归，当r = 1时弹性网回归等价于套索回归。</p><p id="ce9e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们已经结束了这个简短的介绍，也很好地注意到，还有其他形式的正则化技术，特别是在深度学习中，其中一些技术包括退出、提前停止和数据扩充。</p><p id="6c9b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">参考资料:</p><div class="kt ku ez fb kv kw"><a href="https://becominghuman.ai/machine-learning-bias-vs-variance-641f924e6c57" rel="noopener  ugc nofollow" target="_blank"><div class="kx ab dw"><div class="ky ab kz cl cj la"><h2 class="bd hj fi z dy lb ea eb lc ed ef hh bi translated">机器学习:偏差与方差</h2><div class="ld l"><h3 class="bd b fi z dy lb ea eb lc ed ef dx translated">什么是偏见？</h3></div><div class="le l"><p class="bd b fp z dy lb ea eb lc ed ef dx translated">becominghuman.ai</p></div></div><div class="lf l"><div class="lg l lh li lj lf lk ik kw"/></div></div></a></div><div class="kt ku ez fb kv kw"><a href="https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a" rel="noopener follow" target="_blank"><div class="kx ab dw"><div class="ky ab kz cl cj la"><h2 class="bd hj fi z dy lb ea eb lc ed ef hh bi translated">机器学习中的正则化</h2><div class="ld l"><h3 class="bd b fi z dy lb ea eb lc ed ef dx translated">训练机器学习模型的一个主要方面是避免过度拟合。该模型将有一个低…</h3></div><div class="le l"><p class="bd b fp z dy lb ea eb lc ed ef dx translated">towardsdatascience.com</p></div></div><div class="lf l"><div class="ll l lh li lj lf lk ik kw"/></div></div></a></div><div class="kt ku ez fb kv kw"><a href="https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036" rel="noopener follow" target="_blank"><div class="kx ab dw"><div class="ky ab kz cl cj la"><h2 class="bd hj fi z dy lb ea eb lc ed ef hh bi translated">深度学习中的正规化——L1、L2和辍学</h2><div class="ld l"><h3 class="bd b fi z dy lb ea eb lc ed ef dx translated">深度学习中最重要的正则化技术的理论和实践指南</h3></div><div class="le l"><p class="bd b fp z dy lb ea eb lc ed ef dx translated">towardsdatascience.com</p></div></div><div class="lf l"><div class="lm l lh li lj lf lk ik kw"/></div></div></a></div><div class="kt ku ez fb kv kw"><a rel="noopener follow" target="_blank" href="/@ODSC/classic-regularization-techniques-in-neural-networks-68bccee03764"><div class="kx ab dw"><div class="ky ab kz cl cj la"><h2 class="bd hj fi z dy lb ea eb lc ed ef hh bi translated">神经网络中的经典正则化技术</h2><div class="ld l"><h3 class="bd b fi z dy lb ea eb lc ed ef dx translated">众所周知，神经网络很难优化。没有办法计算重量的全局最优值…</h3></div><div class="le l"><p class="bd b fp z dy lb ea eb lc ed ef dx translated">medium.com</p></div></div><div class="lf l"><div class="ln l lh li lj lf lk ik kw"/></div></div></a></div><div class="kt ku ez fb kv kw"><a href="https://analyticsindiamag.com/types-of-regularization-techniques-to-avoid-overfitting-in-learning-models/" rel="noopener  ugc nofollow" target="_blank"><div class="kx ab dw"><div class="ky ab kz cl cj la"><h2 class="bd hj fi z dy lb ea eb lc ed ef hh bi translated">避免过度拟合的正则化技术类型</h2><div class="ld l"><h3 class="bd b fi z dy lb ea eb lc ed ef dx translated">正则化是一组技术，可以帮助避免神经网络中的过拟合，从而改善神经网络的性能</h3></div><div class="le l"><p class="bd b fp z dy lb ea eb lc ed ef dx translated">analyticsindiamag.com</p></div></div><div class="lf l"><div class="lo l lh li lj lf lk ik kw"/></div></div></a></div><p id="66ce" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你想学习如何写出你自己的方程，这篇文章可以帮助你:</p><div class="kt ku ez fb kv kw"><a href="https://satyaganesh987.medium.com/how-to-write-mathematics-on-medium-4a46226c7b1e" rel="noopener follow" target="_blank"><div class="kx ab dw"><div class="ky ab kz cl cj la"><h2 class="bd hj fi z dy lb ea eb lc ed ef hh bi translated">如何在介质上写数学？</h2><div class="ld l"><h3 class="bd b fi z dy lb ea eb lc ed ef dx translated">找到在介质上写数学方程的最佳方法</h3></div><div class="le l"><p class="bd b fp z dy lb ea eb lc ed ef dx translated">satyaganesh987.medium.com</p></div></div><div class="lf l"><div class="lp l lh li lj lf lk ik kw"/></div></div></a></div></div></div>    
</body>
</html>