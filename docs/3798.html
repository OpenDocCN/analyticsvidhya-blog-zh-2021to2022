<html>
<head>
<title>Backpropagation for Dummies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">假人的反向传播</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/backpropagation-for-dummies-e069410fa585?source=collection_archive---------1-----------------------#2021-07-27">https://medium.com/analytics-vidhya/backpropagation-for-dummies-e069410fa585?source=collection_archive---------1-----------------------#2021-07-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/5bcc6d1b4e4ddc1f258c89c2935e2d4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*VF9xl3cZr2_qyoLfDJajZw.gif"/></div><figcaption class="il im et er es in io bd b be z dx translated">通过<a class="ae ip" href="https://machinelearningknowledge.ai/wp-content/uploads/2019/10/Backpropagation.gif" rel="noopener ugc nofollow" target="_blank">machine learning knowledge . ai</a>进行反向传播图解</figcaption></figure><h1 id="c34b" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">目录:</h1><ol class=""><li id="f01a" class="jo jp hh jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">简介和动机</li><li id="e403" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">前提</li><li id="f578" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">算法</li><li id="b878" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">单个单元的计算</li><li id="f135" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">通用单位的计算</li><li id="30c1" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">总结和解释</li><li id="8a61" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">结论</li><li id="756d" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">代码片段</li><li id="e5ca" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">参考</li><li id="4808" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">其他有用的来源</li></ol></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><h1 id="5170" class="iq ir hh bd is it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn bi translated">1.简介和动机</h1><p id="ff5e" class="pw-post-body-paragraph kx ky hh jq b jr js kz la jt ju lb lc jv ld le lf jx lg lh li jz lj lk ll kb ha bi translated">反向传播算法在70年代就已经存在，但是直到大卫·鲁梅尔哈特、罗纳德·威廉姆斯和(猜猜是谁)的一篇<a class="ae ip" href="https://www.nature.com/articles/323533a0" rel="noopener ugc nofollow" target="_blank">著名论文</a>发表后，它的重要性才被充分认识到。)杰弗里·辛顿于1986年出版。</p><p id="d69c" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">今天，反向传播算法是ML中的一个里程碑:它是神经模型中学习的主力。</p><p id="40a5" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">我强烈认为理解这种算法背后的数学是有用和有趣的，原因有很多:首先，停止将神经网络学习仅仅视为“黑盒”是有用的，不幸的是，这是一种非常常见的方法，因为存在为我们执行计算的快速而简单的库。许多人止步于他们的应用程序，很少有人阅读文档，几乎没有人对背后的微积分感兴趣(除非他们在大学有考试要通过！).另一个原因是“<a class="ae ip" href="http://static.latexstudio.net/article/2018/0912/neuralnetworksanddeeplearning.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lr">虽然表达有些复杂，但也有一种美，每个元素都有一种自然，直观的解释</em> </a>”。大多数数学步骤都有特定的解释和意义，这对更好地理解神经模型非常有趣和有用:它们实际上让我们详细了解了改变权重和偏差如何改变网络的整体行为。</p><p id="e2cd" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">我希望我提供了足够的动机，但是如果你需要其他的，我建议你阅读A. Karpathy的这篇文章。</p></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><p id="14f2" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">在这篇文章中，我将回顾反向传播算法背后的数学过程，并以最简单的方式一步一步地展示所有的推导和计算。</p><blockquote class="ls lt lu"><p id="ffe1" class="kx ky lr jq b jr lm kz la jt ln lb lc lv lo le lf lw lp lh li lx lq lk ll kb ha bi translated">我希望这能帮助你克服对“反向投影数学地狱”的恐惧，让你相信它根本不存在。</p></blockquote><h1 id="3f4f" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">2.前提</h1><h1 id="299e" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">2.1符号和上下文</h1><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es ly"><img src="../Images/9e6f4b9b30ac1aa182af6525c4bfdaea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b-lLJVhzCbL0Sykhe3qHog.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">多层感知器结构和我们的符号。由<a class="mh mi ge" href="https://medium.com/u/c5118da752b?source=post_page-----e069410fa585--------------------------------" rel="noopener" target="_blank"> D格利亚</a>手工绘制的草图。</figcaption></figure><p id="9d5f" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">我们考虑一个非常<strong class="jq hi">的基本架构</strong>，所谓的<a class="ae ip" href="https://en.wikipedia.org/wiki/Multilayer_perceptron" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hi">多层感知器</strong> </a>，即只有一个隐藏层的前馈全连接神经网络。</p><ul class=""><li id="5aa5" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb mm kd ke kf bi translated"><strong class="jq hi"> <em class="lr"> i </em> </strong> =指一个通用的<strong class="jq hi">输入</strong>单元(这样输入单元就是<strong class="jq hi"> <em class="lr"> i_1，…，i_i </em> </strong>)。</li><li id="ecf0" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb mm kd ke kf bi translated"><strong class="jq hi"> <em class="lr"> j </em> </strong> =指一个通用的<strong class="jq hi">隐藏的</strong>单元(这样隐藏的单元就是<strong class="jq hi"> <em class="lr"> h_1，…，h_j </em> </strong>)。</li><li id="8252" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb mm kd ke kf bi translated"><strong class="jq hi"> <em class="lr"> k </em> </strong> =指一个通用的<strong class="jq hi">输出</strong>单元(这样隐藏的单元就是<strong class="jq hi"> <em class="lr"> o_1，…，o_k </em> </strong>)。</li><li id="97ce" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb mm kd ke kf bi translated"><strong class="jq hi"> <em class="lr"> w_ji </em> </strong> =将输入单元<strong class="jq hi"> <em class="lr"> i </em> </strong>关联到隐藏单元<strong class="jq hi"> <em class="lr"> j </em> </strong>的连接的权重。</li><li id="83eb" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb mm kd ke kf bi translated"><strong class="jq hi"> <em class="lr"> w_kj </em> </strong> =将隐藏单元<strong class="jq hi"> <em class="lr"> j </em> </strong>关联到输出单元<strong class="jq hi"> <em class="lr"> k </em> </strong>的连接的权重。</li><li id="2e30" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb mm kd ke kf bi translated"><strong class="jq hi"> <em class="lr"> d </em> </strong> =与每个输出单元相关的目标值。</li><li id="84a5" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb mm kd ke kf bi translated">我们在一个<a class="ae ip" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hi">监督学习</strong> </a>的设定中。</li><li id="34e2" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb mm kd ke kf bi translated"><strong class="jq hi">训练集</strong>被定义为<strong class="jq hi"> <em class="lr"> {(x_1，d_1)，…，(x_l，d_l)} </em> </strong>，因此它包含从1到<em class="lr"> l </em>的所有对(输入，目标)。</li></ul><h1 id="e56a" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">2.2成本函数</h1><p id="3fd6" class="pw-post-body-paragraph kx ky hh jq b jr js kz la jt ju lb lc jv ld le lf jx lg lh li jz lj lk ll kb ha bi translated">从<a class="ae ip" href="https://www.nature.com/articles/323533a0" rel="noopener ugc nofollow" target="_blank"> 1986年的论文</a>中出现的是<em class="lr">反向传播旨在通过调整网络的权重和偏差</em>来最小化  <em class="lr">所谓的</em> <strong class="jq hi"> <em class="lr">代价函数</em> </strong> <em class="lr">(基本上，我们可以通过改变<strong class="jq hi"><em class="lr"/>和</strong>来控制这个函数相对于这些参数，调整水平由成本函数的</em> <strong class="jq hi"> <em class="lr">梯度</em> </strong> <em class="lr">确定。</em></p><p id="c697" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">因此，反向传播的<strong class="jq hi">目标是计算成本函数</strong>相对于网络中任意权重<strong class="jq hi"><em class="lr"/></strong>或偏差<strong class="jq hi"> <em class="lr"> b </em> </strong>的<strong class="jq hi">偏导数。</strong></p></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><p id="43bc" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">为了进行反向传播，我们需要对成本函数的形式做出两个主要假设。然而，在陈述这些假设之前，记住一个示例成本函数以及成本函数本身的定义是有用的。</p><blockquote class="ls lt lu"><p id="07e3" class="kx ky lr jq b jr lm kz la jt ln lb lc lv lo le lf lw lp lh li lx lq lk ll kb ha bi translated">尽管我们想当然地认为，在本文中，读者对ML有一些基本的了解，但我们提供了一个非常快速的成本函数定义(也称为“度量”)。这是一个函数，它(I)测量给定数据的模型性能，(ii)量化预测值和期望值之间的误差(iii)并以单个实数的形式呈现。</p></blockquote><p id="4bac" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">根据问题的不同，可以用许多不同的方式来形成成本函数。在我们的上下文中，成本函数的目的是<strong class="jq hi">最小化，</strong>即返回值通常被称为<strong class="jq hi">成本、</strong>或<strong class="jq hi">误差、</strong>，目标是找到模型参数的值，函数本身返回尽可能小的数字。因此，当训练我们的网络时，目标将是使这个成本函数的值尽可能低。</p><p id="fc1b" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">这里我们将使用一个最常用的指标，即<strong class="jq hi">均方误差</strong>，即预测和预期结果之间的平均平方差(换句话说，是MAE的一种变体，不是取差的绝对值，而是取差的平方)。</p></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><p id="65a7" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">我们为什么选择MSE？你应该知道用MAE做微分学是有问题的。相反，MSE具有良好的数学特性，这使得其导数的计算更容易，并且这在使用依赖于梯度下降算法的模型时是相关的。</p><blockquote class="ls lt lu"><p id="860f" class="kx ky lr jq b jr lm kz la jt ln lb lc lv lo le lf lw lp lh li lx lq lk ll kb ha bi translated">MSE可以用Python在<a class="ae ip" href="https://www.notion.so/e677359e2e5c2d0cbbda55718afb5f69" rel="noopener ugc nofollow" target="_blank">中这样写</a>。</p></blockquote></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><p id="bfdd" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">那么为了应用反向传播，我们需要对我们的成本函数做什么样的假设呢？</p><p id="8b84" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated"><em class="lr">我们需要的第一个假设</em>是<strong class="jq hi">成本函数可以写成单个训练例子的平均成本函数</strong>，我们称之为“模式”<strong class="jq hi"> p </strong>。有了这个假设，我们就假设模式<strong class="jq hi"> <em class="lr"> p </em> </strong>已经固定，有时会把<strong class="jq hi"> <em class="lr"> p </em> </strong>下标去掉，把成本<strong class="jq hi"> <em class="lr"> E_p </em> </strong>写成<strong class="jq hi"> <em class="lr"> E </em> </strong>。</p><p id="fe9c" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated"><em class="lr">我们对成本做出的第二个假设</em>是<strong class="jq hi">它可以写成神经网络</strong>输出的函数。根据MSE的定义，它完全满足这个假设。</p></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><p id="2ff2" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">鉴于这些前提，我们将使用的误差函数定义如下:</p><ul class=""><li id="53ab" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb mm kd ke kf bi translated">每个单一模式的误差<strong class="jq hi">T5 pT7定义为:</strong></li></ul><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es mn"><img src="../Images/1b4a63f2c1bfedd44894a1d2ff0ab7c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*V5sDuEZssuZ9HHOMR-f5_Q.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">每个训练示例的MSE</figcaption></figure><p id="68e4" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">即对于模式<strong class="jq hi"> <em class="lr"> p </em> </strong>计算实际目标<strong class="jq hi"><em class="lr"/></strong>与产生的输出<strong class="jq hi"> <em class="lr"> o </em> </strong>、w.r.t所有的<strong class="jq hi"> <em class="lr"> k </em> </strong>输出单元(如前所述，为简化符号<strong class="jq hi"> <em class="lr"> p </em> </strong>最终将被省略)。</p><ul class=""><li id="1a40" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb mm kd ke kf bi translated">所有模式的(总)误差定义为每个模式的误差之和:</li></ul><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es mo"><img src="../Images/c657ec614c216e968f811a2cf9aa5b7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*bj0SKdRPJwDTN6XyHds2IA.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">总误差</figcaption></figure><p id="06b8" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">基于该误差函数值，模型“知道”调整其参数多少，以便更接近预期输出<strong class="jq hi"> <em class="lr"> d </em> </strong>。这正是使用反向传播算法发生的。</p><p id="3fb0" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated"><em class="lr">如果想对现有的其他常用于神经网络的代价函数有一个大概的了解，我建议阅读</em> <a class="ae ip" href="https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications" rel="noopener ugc nofollow" target="_blank"> <em class="lr">这篇文章</em> </a> <em class="lr">。此外，对于使用比MSE更复杂的成本函数进行的反向传播导数计算，如交叉熵似然，我建议阅读</em> <a class="ae ip" rel="noopener" href="/@pdquant/all-the-backpropagation-derivatives-d5275f727f60"> <em class="lr">这篇文章</em> </a> <em class="lr">以及</em> <a class="ae ip" href="http://neuralnetworksanddeeplearning.com/chap3.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr">这篇文章</em> </a> <em class="lr">。</em></p><h1 id="d897" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">2.3需要微积分基础</h1><p id="2b65" class="pw-post-body-paragraph kx ky hh jq b jr js kz la jt ju lb lc jv ld le lf jx lg lh li jz lj lk ll kb ha bi translated">这些是我们需要知道的微分规则，以便解决所有下面的计算。</p><p id="d1dc" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">如你所见，他们只有三个！它们将有力地帮助我们一步一步地处理梯度计算。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es mp"><img src="../Images/0f1bbd5f67c76a61ce722b450bdcda78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*m9bFzhkZ8mSHwtB64vSTRg.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">微分学基本公式</figcaption></figure><h1 id="4b78" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">3.算法</h1><h1 id="fc7f" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">3.1梯度下降</h1><p id="bf14" class="pw-post-body-paragraph kx ky hh jq b jr js kz la jt ju lb lc jv ld le lf jx lg lh li jz lj lk ll kb ha bi translated">我们记得，我们的目标是找到使总误差<strong class="jq hi"> <em class="lr"> E_tot </em> </strong>最小的<strong class="jq hi"> <em class="lr"> w </em> </strong>参数值。</p><blockquote class="ls lt lu"><p id="e6b0" class="kx ky lr jq b jr lm kz la jt ln lb lc lv lo le lf lw lp lh li lx lq lk ll kb ha bi translated">剧透！为此，我们使用<strong class="jq hi">最小均方(LMS)方法</strong>，即通过连续迭代最小化均方误差(MSE)。</p></blockquote><p id="3fb4" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">我们如何在训练过程中最小化成本函数，以找到一组最适合我们目标的权重？</p><p id="f7fb" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">为了做到这一点，我们来看看一类算法，称为<strong class="jq hi">迭代优化算法</strong>，它们逐步朝着最优解前进。这些算法中最基本的是<strong class="jq hi">梯度下降，</strong>，它跟随导数基本上沿着斜坡“滚动”，直到找到(全局)最小值。</p></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><p id="50cb" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">一个问题可能会出现:<strong class="jq hi">为什么</strong>计算<strong class="jq hi">梯度</strong>？要回答这个问题，我们首先需要重温一些微积分术语:</p><ul class=""><li id="741d" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb mm kd ke kf bi translated">函数在点<em class="lr"> x </em>的梯度是函数在<em class="lr"> x </em>的<a class="ae ip" href="https://en.wikipedia.org/wiki/Partial_derivative" rel="noopener ugc nofollow" target="_blank">偏导数</a>的向量。</li><li id="0b7f" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb mm kd ke kf bi translated"><a class="ae ip" href="https://en.wikipedia.org/wiki/Derivative" rel="noopener ugc nofollow" target="_blank"> <em class="lr">函数的导数衡量函数值(输出值)相对于其自变量x(输入值)变化的敏感度</em> </a> <em class="lr">。换句话说，导数告诉我们函数的走向。</em></li><li id="f4f4" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb mm kd ke kf bi translated">梯度显示了参数<em class="lr"> x </em>需要改变多少(正向或负向)以最小化函数。</li></ul></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><p id="b802" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">因此，我们的目标是计算误差的梯度，然后取其相反值，以使<strong class="jq hi">下降方向在<em class="lr">E _ tot</em>T25】的表面上(最小化<strong class="jq hi">梯度下降法</strong>)。如果我们重复这个过程足够多(迭代)，我们很快就会发现自己几乎在曲线的底部，更接近我们网络的最佳权重配置。</strong></p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es mq"><img src="../Images/2254014dffe17359e8aa67b7ef362a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x4Ia04-xY_mYEnz_-3Hgcw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><em class="mr">例</em><strong class="bd is"><em class="mr">E _ tot</em></strong><em class="mr">景观在空间中的两个权重(w1和w2)；局部梯度显示在z点。沿着(梯度)方向，我们可以到达Zmin点。</em></figcaption></figure><p id="33bc" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated"><em class="lr">注意:对于依赖梯度下降来优化模型参数的算法，每个函数都必须是可微分的！(而且，一般来说，也应该或多或少是凸的——</em><a class="ae ip" href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener ugc nofollow" target="_blank"><em class="lr">)现实中，任何网络或代价函数几乎不可能是真正凸的</em></a><em class="lr">——)。</em></p><h1 id="e14e" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">3.2升级规则</h1><p id="154e" class="pw-post-body-paragraph kx ky hh jq b jr js kz la jt ju lb lc jv ld le lf jx lg lh li jz lj lk ll kb ha bi translated">更正式的说法是，梯度下降看起来像这样:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es ms"><img src="../Images/522ee67661c214884be166b6c8950c10.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*6zeIZoLGkUIaAHSmBIOfVg.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">权重更新的增量规则</figcaption></figure><p id="de88" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">这就是梯度下降更新规则(又名<a class="ae ip" href="https://en.wikipedia.org/wiki/Delta_rule" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hi"> delta规则</strong> </a>)。它告诉我们如何更新我们网络的权重，以使我们更接近我们正在寻找的最小值。</p><p id="7778" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">我不会在这里详述学习速率、动量、正则化和其他技术以及神经网络的<strong class="jq hi">超参数</strong>。相反，我会在最后参考部分放一些有趣的链接到其他文章，以防你想了解更多！(或者也许我会写另一个关于参数调整的中等故事…谁知道呢？).同样是为了考虑关于反向传播算法的最优性和复杂性，以及选择迭代优化算法的其他选项(Adagrad，Adam，…)。</p><p id="e507" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">基本原则始终保持不变——逐渐更新权重，使其更接近最小值。但是，无论您使用哪种优化算法，我们仍然需要能够计算成本函数相对于每个权重的梯度。</p><p id="b30c" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">在我们的上下文中，权重的变化被定义为每个模式相对于其权重的误差在所有模式上的总和。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/e9c91bec54d569eeb2585a5db93580e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*aQWEWqBpaUN2DVQvvG6v-w.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">梯度计算</figcaption></figure><h1 id="f510" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">3.3伪代码</h1><p id="b6a1" class="pw-post-body-paragraph kx ky hh jq b jr js kz la jt ju lb lc jv ld le lf jx lg lh li jz lj lk ll kb ha bi mu translated"><span class="l mv mw mx bm my mz na nb nc di"> T </span> <strong class="jq hi">何问题:</strong>计算代价函数的梯度，建立所谓的<strong class="jq hi">广义delta规则。</strong></p><p id="31c7" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi mu translated"><span class="l mv mw mx bm my mz na nb nc di"> T </span> <strong class="jq hi">何思路:</strong>计算损失函数(<strong class="jq hi"> <em class="lr"> E_tot </em> </strong> ) →计算梯度→更新网络中所有权重<strong class="jq hi"> <em class="lr"> w </em> </strong>。然后重复直到收敛或直到其它停止标准。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nd"><img src="../Images/ebf5584ad73fa042a9ff4984719ac2d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*GZIowbJIms29zmmPGozYIg.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">由格利亚制作的反向传播伪代码。</figcaption></figure><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es ne"><img src="../Images/1cb84f84245f6e14355a31d708b87cb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NVcL16P5LZQB6k3tqRzkSQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">比萨大学计算机科学系A. Micheli的反向传播伪代码</figcaption></figure><p id="8047" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated"><em class="lr">步骤(1) </em>表示适当的梯度计算。我们将集中讨论这一点。</p><p id="8c3a" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated"><em class="lr">步骤(2) </em>是<strong class="jq hi"> <em class="lr"> w </em> </strong>的升级规则——参见上面的“delta规则”。</p><h1 id="02ec" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">4.单个单元的计算</h1><p id="cdf9" class="pw-post-body-paragraph kx ky hh jq b jr js kz la jt ju lb lc jv ld le lf jx lg lh li jz lj lk ll kb ha bi translated">作为初步练习，我建议我们从一个简单的计算开始，只是为了熟悉我们将在第5节中需要的数学步骤和技巧。此外，通过这个练习，我们获得了一个以后需要的结果。如果你对微分学很有信心，你可以跳过这一节，然后回到这里得到你需要的步骤，已经为你计算好了。</p><p id="2744" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">正如我所说的，作为初步的练习，我建议从考虑单个神经元<strong class="jq hi"> <em class="lr"> i </em> </strong> <em class="lr">开始。</em></p><blockquote class="ls lt lu"><p id="a958" class="kx ky lr jq b jr lm kz la jt ln lb lc lv lo le lf lw lp lh li lx lq lk ll kb ha bi translated">我们要做的是:<em class="hh">计算误差的导数w.r.t. </em> <strong class="jq hi"> <em class="hh">单个单元</em></strong><em class="hh"/><strong class="jq hi">I</strong><em class="hh">，即计算</em></p></blockquote><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/da157a3e6cc88d3ffc71da099637f6d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:248/format:webp/1*J5OEpWbBmDJpN-1tNnzobg.png"/></div></figure><p id="a1ff" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">我们首先编写如上定义的误差函数，概述输出<strong class="jq hi"><em class="lr"/></strong>作为<strong class="jq hi"> <em class="lr">网</em> </strong>的函数的作用:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es ng"><img src="../Images/08509713eaa49f00236b104ff9f0f581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q175VNDNmvLDBZ5iusZCeg.png"/></div></div></figure><p id="7e88" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">这里用粗体突出显示了输入向量和相应权重之间的标量积，即<strong class="jq hi"> <em class="lr">网。</em>T11】</strong></p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nh"><img src="../Images/c9b77fc46fb1972a8cd98579e7b333c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*5D2fxkipZZdZ0TU_W3iFaw.png"/></div></figure><p id="ebde" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">为了简单起见，正如前面已经提到的，我们只考虑误差<strong class="jq hi"> <em class="lr"> E_p </em> </strong>，所以w.r.t .一个单一的模式<strong class="jq hi"> <em class="lr"> p </em> </strong>，然后我们在最后总结所有的东西。</p><p id="4564" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">现在我们应用链式法则，因此我们分解w . r . t .<strong class="jq hi"><em class="lr">网</em> </strong> <em class="lr">。</em></p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es ni"><img src="../Images/dc7eb6d0890ecfcc8ea176df6d434828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*YocJHIGp2jNL5jCzGaHayw.png"/></div></figure><p id="408e" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">其中函数<strong class="jq hi"> <em class="lr"> f </em> </strong>对应误差，函数<em class="lr"> g </em>对应网，两者都是变量<strong class="jq hi"> <em class="lr"> x </em> </strong> <em class="lr">，</em>的函数，对应与我们正在考虑的单元<em class="lr"> i </em>相关联的权重<em class="lr"> w </em>。</p><p id="3f31" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">因此我们得到:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nj"><img src="../Images/f0b95f6cecbcfd543aaf2afe2fa8f887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*K3MNJ5kVi0UBgoZYXqZSJg.png"/></div></figure><p id="fc64" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">我们将分别求解(4)中的两个分量。</p><ul class=""><li id="73ad" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb mm kd ke kf bi translated">首先，<strong class="jq hi">我们关注第二项</strong>，我们用所有分量的和来扩展点积:</li></ul><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nk"><img src="../Images/b5c1cfd5f34611db1c5c5b0d1adb0d75.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*35nExC002I56WTWpzWtw4g.png"/></div></figure><p id="ad47" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">这是因为除了第I个<em class="lr"/><strong class="jq hi"><em class="lr"/></strong>组件之外，每个组件的术语都等于零。为了更好地理解，我们给出了所有可能术语的总和:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nl"><img src="../Images/80b934b23903e69eb9aa9762a17cd01f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*VjKtWvcS_geXDQA1W3VbkA.png"/></div></figure><ul class=""><li id="50d1" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb mm kd ke kf bi translated">现在<strong class="jq hi">我们关注(4)中的第一项</strong>:我们再次使用(3)中的链式法则，但是现在我们认为函数<em class="lr"> g </em>输出<strong class="jq hi"> <em class="lr"> o </em> </strong>，为此我们使用符号<strong class="jq hi"> <em class="lr"> out </em> </strong>。</li></ul><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nm"><img src="../Images/a8d27b7905faef64e810d6821829e83b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*tygiDrn42Csv5paSQjrclg.png"/></div></figure><p id="759b" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">其中<strong class="jq hi"> <em class="lr"> f </em> </strong>为<strong class="jq hi">激活功能。</strong></p><p id="ebb1" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">因此我们得到:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nn"><img src="../Images/255604bbddfa5e4281d591f62cac58cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*m9-Y1jizVUEsxu8uO0hi6g.png"/></div></figure><p id="cd99" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">通过明确定义<strong class="jq hi"><em class="lr"/></strong>和<strong class="jq hi"> <em class="lr">网</em> </strong>分别为(2)和(5)，我们得到:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es no"><img src="../Images/5ba050f86a214d13f79c11c1093ed1f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nB3X4tX2CSQ4yzGOm39ulA.png"/></div></div></figure><p id="353a" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">现在我们再次分别考虑这两个术语。</p><ul class=""><li id="c568" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb mm kd ke kf bi translated">对于(6)中的<strong class="jq hi">第一项</strong>，通过回忆(1)，我们得到:</li></ul><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es np"><img src="../Images/c12d2278cb0a990c38c1dc37e570c924.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WteNOHyLZsuEJmJMJxhDOA.png"/></div></div></figure><p id="e9e5" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">这是因为，对于公式</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nq"><img src="../Images/ad02a427e9d4eb0fe8b7ae925e874319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*EIqA5Z0Q1ApM1q24NkiGeQ.png"/></div></figure><p id="a0f5" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">我们有</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nr"><img src="../Images/b80735ef9e738455be5ffb6a071d1861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*-nk23ez10qvtpL8e9gnzyw.png"/></div></figure><p id="f7b8" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">其中</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es ns"><img src="../Images/f659c71fa1649c197a5ceec514f8b51b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*WFveTrh1-AnD-TpNt6K2BQ.png"/></div></figure><ul class=""><li id="41d6" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb mm kd ke kf bi translated">对于(6)中的第二项,我们有:</li></ul><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nt"><img src="../Images/3b8498c56220fa1af441e85d5f2b4602.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*-rw6U-Nf9ov4Qa7i2S6S0w.png"/></div></figure><p id="5fd3" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">将所有内容放在一起，我们得到(6)的结果:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es nu"><img src="../Images/4f606337a2032d6ef526c92418c82866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*IlP_hknQbI60ekFgmo1sDA.png"/></div></div></figure><p id="447f" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">并且我们可以写出(4)的<strong class="jq hi">最终结果</strong>:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nv"><img src="../Images/4fc0130f191622575def6b474712bfd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*ss1y-5Ha1HQOd0HJrXVnvg.png"/></div></figure><p id="4a7e" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">在哪里</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nw"><img src="../Images/f7b44eba2dd48493c7f55a6cd5ec6e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*kwInzPbkzTu4y9YAX2qgrA.png"/></div></figure><p id="7f93" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">所以我们可以这样写结果</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nx"><img src="../Images/0f5effbb6249daeed585655e5b99de0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*aQrMWZL5CcmhIF4DFFbriw.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">新德尔塔规则</figcaption></figure><blockquote class="ls lt lu"><p id="4965" class="kx ky lr jq b jr lm kz la jt ln lb lc lv lo le lf lw lp lh li lx lq lk ll kb ha bi translated">这就是<strong class="jq hi">新德尔塔法则</strong>。</p></blockquote><h1 id="3ee9" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">5.通用单位的计算。</h1><blockquote class="ny"><p id="e9bb" class="nz oa hh bd ob oc od oe of og oh kb dx translated">你知道链式法则吗？那你就知道神经网络反向传播算法了！</p></blockquote><p id="c62f" class="pw-post-body-paragraph kx ky hh jq b jr oi kz la jt oj lb lc jv ok le lf jx ol lh li jz om lk ll kb ha bi translated">现在，与前面的简单情况不同，我们有不止一个单元，而是多个单元，所以我们需要引入索引<strong class="jq hi"> <em class="lr"> t </em> </strong>来表示多个单元中的一个单元。</p><p id="e439" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">现在改变的是<strong class="jq hi">我们不知道单元<em class="lr"> t </em>在网络架构</strong>中的位置(这就是为什么它是<em class="lr">泛型</em>)，所以我们必须考虑和区分两种情况<strong class="jq hi">是隐藏单元还是输出单元</strong>。</p><p id="3d00" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi mu translated"><span class="l mv mw mx bm my mz na nb nc di"> W </span> <strong class="jq hi">我们要做什么</strong> : <em class="lr">假设一个特定的模式</em> <strong class="jq hi"> p </strong>，<em class="lr">计算误差的导数w.r.t. </em> <strong class="jq hi"> <em class="lr">一个类属单位</em> t </strong> <em class="lr">，即计算</em></p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es on"><img src="../Images/7c35e1753b8fb56783534fa49c66b45d.png" data-original-src="https://miro.medium.com/v2/resize:fit:112/format:webp/1*6VdL1pjyskD78hsAgDYa1w.png"/></div></figure><p id="2e6c" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">回想3.2节中的“梯度计算公式”，我们可以改写为</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es oo"><img src="../Images/184cd1aa50e025d05196fc685bef138b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*SIx2tcUHCnepmvAAA5Ewig.png"/></div></figure><p id="a1f1" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">其中<em class="lr"> </em> <strong class="jq hi"> <em class="lr"> t </em> </strong>是网络中的一个通用单元<strong class="jq hi"> <em class="lr"> i </em> </strong> <em class="lr"> </em>从一个通用单元<em class="lr"/><strong class="jq hi"><em class="lr">out _ I</em></strong>到单元<em class="lr"/><em class="lr">【t</em><em class="lr">。</em></p><p id="bb8d" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">这是因为，如前所述，</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es op"><img src="../Images/e3274a7740b525b99369a8315c6e2977.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*CYoQCkG3FZn9oZG74NE0LQ.png"/></div></figure><p id="a69f" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">注意:记住，一如既往，为了我们的计算目的，我们考虑一个<em class="lr">单个</em>模式<strong class="jq hi"> <em class="lr"> p </em> </strong>。</p><p id="897d" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">我们首先应用链式法则(3)w . r . t .<strong class="jq hi"><em class="lr">网</em> </strong>在公式(10)<em class="lr"/>中，我们得到:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es oq"><img src="../Images/c31f2451bc5b823d3d7fd818f6fe8a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*BqJlAmXg_G95ozCKiK0q-w.png"/></div></figure><p id="bd68" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">与第4节一样，我们继续分别考虑这两个术语。</p><ul class=""><li id="5a40" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb mm kd ke kf bi translated">对于(11)中的第二项,我们有</li></ul><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es or"><img src="../Images/62f39208a14c39f55928c1d8d4d17c1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WIMQf2Hw4PCnBDLMU6-xag.png"/></div></div></figure><p id="c22b" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">这是因为，回想一下<em class="lr">网的定义</em> (2)和<em class="lr">出</em> (5)</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es os"><img src="../Images/e8ce9c1dc59125df7b18452c5289bb73.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*WIgG_lh9DPTMLr3KpRAhJg.png"/></div></figure><p id="e822" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">其中<em class="lr"> a </em>用作内部总和的通用索引。</p><p id="e54b" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">既然我们不知道我们在网络中的什么位置，最好用<strong class="jq hi"> <em class="lr"> out </em> </strong>来表示它是另一个单元的输出。</p><p id="8244" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">(12)的所有分量都为零，除了<strong class="jq hi"><em class="lr">j</em></strong><em class="lr">=</em><strong class="jq hi"><em class="lr">I</em></strong><em class="lr">的情况。</em>所以我们有</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es ot"><img src="../Images/ccbb569aeffe018adc5df96651ad2afd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*4rAWs-Cp7jY41mWPZrO7UQ.png"/></div></figure><p id="66ca" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">(这表示从一个通用单元<strong class="jq hi"> <em class="lr"> out_i </em> </strong>到单元<strong class="jq hi"> <em class="lr"> t </em> </strong>的输入<strong class="jq hi"> <em class="lr"> i </em> </strong>)。</p><ul class=""><li id="fb6e" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb mm kd ke kf bi translated">现在我们考虑(11)中的第一项,我们引入一个新变量来表示它。然后我们应用链式法则w . r . t .<strong class="jq hi"><em class="lr">out</em></strong>来计算这个量</li></ul><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es ou"><img src="../Images/6a60e7ef93f73b9d5705d8672253c14b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0N39BDHnrT7bZX-ZB6YupQ.png"/></div></div></figure><p id="dc0b" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">同时，我们用我们得到的结果重写(10 ),以便总结我们到目前为止所做的工作:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es ov"><img src="../Images/b3fb34a5ac6047f7dc96bd7e645d873f.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*dB2AaFBmdMiq1yQUt0UvKw.png"/></div></div></figure></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><p id="aa77" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">现在，为了解决(13)中的剩余项，即</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es ow"><img src="../Images/84f7a92d46afa8dd5e9532019b1dfd80.png" data-original-src="https://miro.medium.com/v2/resize:fit:322/format:webp/1*PfS2IZjBoDQUzmIHNWNjjA.png"/></div></figure><p id="7192" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">我们必须根据<strong class="jq hi">在网络中的位置来区分案例。</strong></p><ol class=""><li id="26af" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb kc kd ke kf bi translated">单元<strong class="jq hi"> <em class="lr"> t </em> </strong>是<strong class="jq hi">输出单元</strong>吗？</li><li id="084b" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">单元<strong class="jq hi"> <em class="lr"> t </em> </strong>是<strong class="jq hi">隐藏单元</strong>吗？</li></ol><p id="cec9" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">这是因为两者对误差的贡献<em class="lr"> E </em>不同。事实上，由于我们正在计算误差w.r.t .的偏导数，单位<strong class="jq hi"> <em class="lr"> t </em> </strong> <em class="lr">，</em> <strong class="jq hi"> <em class="lr"> </em> </strong>这种影响在情况1和情况2中呈现出完全不同的形状，因此我们<em class="lr">必须</em>考虑并区分这两种情况。</p><blockquote class="ls lt lu"><p id="189c" class="kx ky lr jq b jr lm kz la jt ln lb lc lv lo le lf lw lp lh li lx lq lk ll kb ha bi translated"><em class="hh">提示:使用下面的网络图来定位每种情况下的单元t！</em></p></blockquote><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es ox"><img src="../Images/e441e85ca290c06e0deca4e2dfb069d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*53NyapC0sNIU_yCPwWFx_Q.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">我们的神经网络的结构(MLP的简单放大)由D·格利亚<a class="mh mi ge" href="https://medium.com/u/c5118da752b?source=post_page-----e069410fa585--------------------------------" rel="noopener" target="_blank">制作</a>。在接下来的计算过程中关注它，想象我们在两种情况下所处的位置，会非常有帮助。</figcaption></figure><p id="cf35" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">让我们一次深入一个案例。</p><h2 id="a14d" class="oy ir hh bd is oz pa pb iw pc pd pe ja jv pf pg je jx ph pi ji jz pj pk jm pl bi translated">案例1</h2><p id="48f3" class="pw-post-body-paragraph kx ky hh jq b jr js kz la jt ju lb lc jv ld le lf jx lg lh li jz lj lk ll kb ha bi translated">通用单元<strong class="jq hi"> <em class="lr"> t </em> </strong> <em class="lr"> </em>是一个<strong class="jq hi">输出</strong>单元<strong class="jq hi"> <em class="lr"> k </em> </strong> <em class="lr">。</em></p><p id="f506" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">在这种情况下:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es pm"><img src="../Images/fb91782da51666c0aaf2f6184e81f5ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*5s5NT-Ofb7SOB2UjhbIy9Q.png"/></div></figure><p id="9e9a" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">注意，在下文中，我们区分索引<strong class="jq hi"> <em class="lr"> k </em> </strong>和<strong class="jq hi"><em class="lr">k’</em></strong><em class="lr">。</em>特别是，<em class="lr"/><strong class="jq hi"><em class="lr">k’</em></strong>是我们仅用于内部求和的值，而<strong class="jq hi"> <em class="lr"> k </em> </strong>是一个固定值，表示我们正在推导的<strong class="jq hi"><em class="lr"/></strong>w . r . t。</p><p id="200b" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">因此，通过应用公式(8)和(14)，我们得到</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es pn"><img src="../Images/b54a62508e430b14e696f61530c2019e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*KmMqTUBCHi6E0TL6DfS2vg.png"/></div></figure><p id="8cd1" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">这是因为，在做求和显式时，除了<strong class="jq hi"><em class="lr">k’</em></strong><em class="lr">=</em><strong class="jq hi"><em class="lr">k</em></strong>中的一项外，所有项都为零。于是<strong class="jq hi"> ∑ </strong>符号消失了。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es po"><img src="../Images/18ec228f836e9ee02892e2e8d1726d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*CF_bSl2Nn8p3fcVYNQmjLg.png"/></div></figure><p id="073d" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">最后的结果是:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es pp"><img src="../Images/ffc9a99aef52aa98bc329b30dce27f52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k7ig2kEPp330aGYzJKvgWw.png"/></div></div></figure><p id="0593" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">通过将(13)和(15)放在一起，我们得到输出单位<strong class="jq hi"> <em class="lr"> k </em> </strong>的导数:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es pq"><img src="../Images/5b8f14041cf00d02f3496355286a0438.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ei7zdD5ZU_Cx5k0f3jxH0w.png"/></div></div></figure><p id="d366" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">这恰恰是一个<strong class="jq hi">误差信号</strong>，因为它包含了产生的输出和期望目标之间的<strong class="jq hi">差异(记住我们是在监督学习设置中)<strong class="jq hi">。</strong></strong></p><p id="b3b4" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">(<em class="lr">现在你可以体会到初步练习的效用了！</em>)</p></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><h2 id="bbe8" class="oy ir hh bd is oz pa pb iw pc pd pe ja jv pf pg je jx ph pi ji jz pj pk jm pl bi translated">案例2</h2><p id="df6d" class="pw-post-body-paragraph kx ky hh jq b jr js kz la jt ju lb lc jv ld le lf jx lg lh li jz lj lk ll kb ha bi translated">通用单元<strong class="jq hi"> <em class="lr"> t </em> </strong>是一个<strong class="jq hi">隐藏</strong>单元<strong class="jq hi"> <em class="lr"> j </em> </strong>。</p><blockquote class="ls lt lu"><p id="fec6" class="kx ky lr jq b jr lm kz la jt ln lb lc lv lo le lf lw lp lh li lx lq lk ll kb ha bi translated"><em class="hh">提示:</em>作为进一步的帮助，利用上面的图片，尝试找出我们在计算的每一步在网络结构中的位置。</p></blockquote><p id="276a" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">这是一个更有趣的情况，因为这正是定义MLP学习算法的问题出现的地方:模型不容易将相应的信用分配给隐藏单元(<em class="lr">信用分配问题——CAP</em>)，因为<strong class="jq hi">误差信号不可直接测量</strong>。我们不知道隐藏单元的误差或期望的答案(更新它们的权重绝对需要的信息！).因此，正是从这种背景下，需要反向传播算法出现。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es pr"><img src="../Images/7b791a6edfc7ba9aa252d4c7681e0f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*cr6c6Ody0TK-4gR0Dt8ajQ.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">信用分配问题(CAP):如何调整隐藏层中单元的权重？</figcaption></figure><p id="9d8c" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">于是我们有了<strong class="jq hi"><em class="lr">t = j</em></strong><em class="lr"/>现在我们重点解决这个术语:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es ps"><img src="../Images/8fff698286f6a5d2c37ef9c8abb84630.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*KYqPjKvHANN5M8dF3ZuUsQ.png"/></div></figure><p id="ad32" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">我们再次应用链式法则，但是<strong class="jq hi">利用我们所知道的关于我们网络</strong>的结构的知识:我们知道隐藏单元与所有的输出单元相连接(即对于每个<strong class="jq hi"> <em class="lr"> k </em> </strong>存在与<strong class="jq hi"> <em class="lr"> j </em> </strong>的连接)。所以我们将<strong class="jq hi">对输出层</strong>中所有单元<em class="lr"> k </em>的贡献进行求和，因为我们不知道它们中的哪一个会感兴趣，而<strong class="jq hi">隐藏单元<em class="lr"> j </em>的影响是在整个输出层上。</strong></p><blockquote class="ny"><p id="272c" class="nz oa hh bd ob oc od oe of og oh kb dx translated">考虑隐藏单元对所有输出单元的所有这些贡献(并将它们相加)是反向传播算法的关键点。</p></blockquote><figure class="pu pv pw px py ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es pt"><img src="../Images/8e2b8939240001192d103024f0f86aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3zdgNh7AfLk_odVOZY1WZw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><em class="mr">这种形式表达了误差E w r t的变化。所有输出单元k。每个out_k(和net_k)取决于out_j，因此我们在k上引入一个和。</em></figcaption></figure><p id="d7c2" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">这样我们已经分解考虑了<strong class="jq hi"> <em class="lr">网</em> </strong>对输出层<strong class="jq hi"> <em class="lr"> k </em> </strong>的影响，以及所有输出单元<strong class="jq hi"> <em class="lr"> j </em> </strong>对<strong class="jq hi"><em class="lr"/></strong><strong class="jq hi"><em class="lr">k</em></strong><em class="lr">的影响。</em></p><p id="5d0f" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">特别要注意的是,( 17)中的第一项之前已经求解过了！解决方法正好是<strong class="jq hi">情况1 </strong>中的(16)。</p><p id="7100" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">所以我们只专注于求解(17)的第二项。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es pz"><img src="../Images/61e1938d493090e44b120dfd3e08b3dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8j1wDynGd6cI4YNqVTnrg.png"/></div></div></figure><p id="0590" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">和前面的例子一样，所有的偏导数都为零，除了当<strong class="jq hi"><em class="lr">‘</em></strong><em class="lr">=</em><strong class="jq hi"><em class="lr">j</em></strong><em class="lr">。</em>所以我们把(16)和(18)放在一起，(17)的最终解是</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es qa"><img src="../Images/b19736f731547bc1ed2cf06412bf2b55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*01PuW6ksmb2U4EJPyQBajA.png"/></div></div></figure><p id="870e" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">因此，对于<strong class="jq hi">情况2 </strong>，我们可以为隐藏单元写入<strong class="jq hi">错误信号。</strong></p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es pt"><img src="../Images/c7985e884e0a0911bd7866e29999c0b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1HhlUwyPtXrFHmgwVWwEIQ.png"/></div></div></figure><p id="5d0d" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">注意，如果你愿意，你也可以直接导出(19)，如下图所示。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es qb"><img src="../Images/bf64e75b3e3c9112e03b3a621f262d80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bfpnbGgBiWCQcADRNH_bw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><a class="mh mi ge" href="https://medium.com/u/e42f64a3e15?source=post_page-----e069410fa585--------------------------------" rel="noopener" target="_blank"> Federico Errica </a>对案例2推导的替代版本(直接)</figcaption></figure><p id="e6db" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">现在我们有了输出(16)和隐藏(20)单元的偏导数，我们终于可以<strong class="jq hi">升级权重</strong>(如果需要，回到3.2节)。</p><p id="553c" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated"><em class="lr">网络中的任何</em>权重都有一个一般化的delta规则(10)，其中<strong class="jq hi"><em class="lr">∏t</em></strong>项采用(16)或(20)的形式，这取决于所考虑的是哪种单元(分别为输出或隐藏——情况1或2)。</p><p id="85ba" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">(20)中值得注意的是权重<strong class="jq hi"> <em class="lr"> w_kj </em> </strong>的存在。这意味着<strong class="jq hi">我们正在将<em class="lr">∏k</em>从输出层反向传播到隐藏层</strong>，正好通过连接它们的权重。所以我们是<strong class="jq hi">通过整个网络反向传播错误信号</strong>(因此得名<em class="lr"> </em> <strong class="jq hi">)，以一种与输出和隐藏单元之间的连接权重成比例的方式。</strong></p><p id="0368" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">注意:当然我们总是要考虑到网络的形状。在这种情况下，我们假设在单元<strong class="jq hi"><em class="lr"/></strong>和<strong class="jq hi"> <em class="lr"> k </em> </strong>之间存在连接(我们认为参考模型a <em class="lr">完全连接的前馈神经网络)</em>。</p><p id="9fdc" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">一般来说，<strong class="jq hi">根据您网络架构上的连接，您可以通过修改之前偏导数计算中的相应数学步骤来改变增量</strong>的传播。</p><blockquote class="ls lt lu"><p id="7adf" class="kx ky lr jq b jr lm kz la jt ln lb lc lv lo le lf lw lp lh li lx lq lk ll kb ha bi translated">数学遵循网络的架构。</p></blockquote><h1 id="6525" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">6.总结和解释</h1><p id="5fae" class="pw-post-body-paragraph kx ky hh jq b jr js kz la jt ju lb lc jv ld le lf jx lg lh li jz lj lk ll kb ha bi translated">对于一个通用单元<strong class="jq hi"> <em class="lr"> t </em> </strong>，用来自<strong class="jq hi"> <em class="lr"> out_i </em> </strong>的输入，我们推导出:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es qc"><img src="../Images/413c90d256f13e9e6ae72c77859585fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*C8JyA_nNoNFaXpa4aWsdNQ.png"/></div></figure><ul class=""><li id="fa17" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb mm kd ke kf bi translated">如果<strong class="jq hi"> <em class="lr"> t = k </em> </strong>(即<strong class="jq hi"> <em class="lr"> t </em> </strong>为输出单位)</li></ul><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es qd"><img src="../Images/8f1cf96b73c748dc7ba734b046a600f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*cvXZO-l3E2fAtTGRuwoKaQ.png"/></div></figure><ul class=""><li id="8379" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb mm kd ke kf bi translated">如果<strong class="jq hi"> <em class="lr"> t = j </em> </strong>(即<strong class="jq hi"> <em class="lr"> t </em> </strong>是一个隐藏单元)</li></ul><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es nn"><img src="../Images/f6c97011e9968e9388422dfbf0e19786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*JuXBw2Y_tvQFgvwbLEqf6g.png"/></div></figure><blockquote class="ls lt lu"><p id="f081" class="kx ky lr jq b jr lm kz la jt ln lb lc lv lo le lf lw lp lh li lx lq lk ll kb ha bi translated"><em class="hh">实施提示:</em>对于每种模式<strong class="jq hi"> p </strong>该计算可直接用于在线表单或对批量表单求和。</p></blockquote><p id="a587" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">为了更好地理解和可视化来自输出层的误差信号的<strong class="jq hi">反向传播(这是算法的关键部分)，由连接<strong class="jq hi"> <em class="lr"> w_kj </em> </strong>表示，我们关注(22)中的和，并在下图中给出其图形表示。</strong></p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es qe"><img src="../Images/d93d99f3e2a167f619219e3c768e4588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I5-L-8qOxUx8jSxXUzjk1A.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">从输出层反推增量值，以获得隐藏层的误差信号(通过<a class="mh mi ge" href="https://medium.com/u/c5118da752b?source=post_page-----e069410fa585--------------------------------" rel="noopener" target="_blank"> D格利亚</a>)。基于δk和相关的权重w_kj，它表示单位j对误差的影响。</figcaption></figure><p id="2246" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">图像显示了delta在每个输出单元<strong class="jq hi"> <em class="lr"> k </em> </strong>的贡献，求和，用各自的连接权重<strong class="jq hi"> <em class="lr"> w_kj </em> </strong>加权，传递给隐藏单元<strong class="jq hi"> <em class="lr"> j </em> </strong>。当然这个过程代表每个隐藏单元<strong class="jq hi"><em class="lr"/>j</strong>。在某种意义上，我们可以说关联权重<strong class="jq hi"> <em class="lr"> w_kj </em> </strong>代表单元<strong class="jq hi"> <em class="lr"> k </em> </strong>对其误差负责的程度。在这种情况下，总误差的一般<strong class="jq hi">责任将根据权重<em class="lr">w _ kj</em>T57】通过网络进行分配(这里我们只计算了局部值:但是权重的变化对网络具有全局影响)。</strong></p><blockquote class="ls lt lu"><p id="c23e" class="kx ky lr jq b jr lm kz la jt ln lb lc lv lo le lf lw lp lh li lx lq lk ll kb ha bi translated">反向传播的美妙之处在于，通过这种“delta思想”,它能够在整个网络结构上分解误差的影响，以及每个权重对误差的影响。</p></blockquote></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><p id="b9a2" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi mu translated">这种逆向传播的过程可以<strong class="jq hi">推广并应用于具有许多隐藏层的更深层次的模型</strong>。在这种情况下，增量不仅来自输出层，还来自当前层之上的<em class="lr">任何通用层(通用上层K)。</em></p><p id="66e3" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">因此，对于每个图案<strong class="jq hi"> <em class="lr"> p </em> </strong>:</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es qf"><img src="../Images/62074ba2366b42779f108b77c65c8e41.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*t7et5Jmgvv2Y2VNbx6niiw.png"/></div></figure><p id="c68a" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">其中<strong class="jq hi"> <em class="lr"> δt </em> </strong>为单位<strong class="jq hi"> <em class="lr"> t </em> </strong> <em class="lr">，</em><strong class="jq hi">o _ I</strong>为单位<strong class="jq hi"> <em class="lr"> i </em> </strong>到单位<em class="lr"> </em> <strong class="jq hi"> <em class="lr"> t </em> </strong>通过连接<strong class="jq hi"> <em class="lr"> w_ti </em> </strong>的输入。</p><p id="be5f" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">因此，反向传播算法<strong class="jq hi">不仅适用于MLP，而且通常适用于任何神经模型</strong>(对模型本身的结构进行适当的修改和调整)。它是训练递归神经网络、深度学习模型等的基础</p></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es ie"><img src="../Images/5bcc6d1b4e4ddc1f258c89c2935e2d4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*VF9xl3cZr2_qyoLfDJajZw.gif"/></div><figcaption class="il im et er es in io bd b be z dx translated"><a class="ae ip" href="https://machinelearningknowledge.ai/wp-content/uploads/2019/10/Backpropagation.gif" rel="noopener ugc nofollow" target="_blank">machine learning knowledge . ai</a>反向传播插图</figcaption></figure><p id="ee01" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated">回到本文开头的图像，我们可以再次观察这个过程，但现在已经完全理解它了。动画中的后箭头显示了误差如何反向传播到前层，直到最后，同时网络中的神经元开始调整其权重。</p><h1 id="3eef" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">7.结论</h1><p id="8c49" class="pw-post-body-paragraph kx ky hh jq b jr js kz la jt ju lb lc jv ld le lf jx lg lh li jz lj lk ll kb ha bi translated">算法运行的“秘密”是，一旦我们选择了激活和误差函数，我们计算的每个导数都可以被简化，这样整个结果将代表一个数值。在这一点上，任何抽象已经被移除，并且网络中的任何权重的误差导数w.r.t .可以在<strong class="jq hi">梯度下降</strong> <strong class="jq hi">方法</strong>(如前所述)中使用，以迭代地改进权重。</p><p id="ff40" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi mu translated"><span class="l mv mw mx bm my mz na nb nc di"> T </span> <a class="ae ip" href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener ugc nofollow" target="_blank"> <em class="lr">他这是反向传播！简单地计算导数，这些导数被提供给凸优化算法。在现实中，反向传播只是链规则的一个相当乏味的应用(但同样，对于一个一般化的实现，计算机会处理这个)。</em>T41】</a></p><blockquote class="ls lt lu"><p id="cb58" class="kx ky lr jq b jr lm kz la jt ln lb lc lv lo le lf lw lp lh li lx lq lk ll kb ha bi translated"><em class="hh">学习提示:</em>阅读3个层次的推导:(i) <strong class="jq hi"> <em class="hh"> </em> </strong>奇异数学步骤(ii)解释一个量相对于其它量的(局部)变化(由于偏导数)(iii)所提供的分解的含义的一般框架，即找到分解手头网络上的δ误差的每个层次的δ值。</p></blockquote><h1 id="d07a" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">8.代码片段</h1><p id="18e9" class="pw-post-body-paragraph kx ky hh jq b jr js kz la jt ju lb lc jv ld le lf jx lg lh li jz lj lk ll kb ha bi translated">如果您需要一些现成的代码作为您实现的起点或作为参考来完全理解本文的范围，请查看<a class="ae ip" href="https://github.com/dilettagoglia/impl-NN-from-scratch" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hi"> <em class="lr">这个GitHub库</em> </strong> </a>。</p><p id="7e11" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated"><em class="lr">我强烈建议尝试您自己的不同实现，作为进一步掌握反向传播功能的手段，探索不同的行为，并从该算法中获得乐趣！</em></p></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><h1 id="0209" class="iq ir hh bd is it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn bi translated">9.参考</h1><ol class=""><li id="efa9" class="jo jp hh jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">Rumelhart，D. E .、Hinton，G. E .和Williams，R. J .,“并行分布式处理”麻省理工学院出版社，剑桥，MA (1986) <a class="ae ip" href="https://web.stanford.edu/class/psych209a/ReadingsByDate/02_06/PDPVolIChapter8.pdf" rel="noopener ugc nofollow" target="_blank">第8章</a>。</li><li id="a653" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">汤姆·米契尔，《机器学习》。麦格劳·希尔(1996)第4章。</li><li id="0b99" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">Haykin，s，“神经网络”，第二版。普伦蒂斯霍尔(1998年)。</li><li id="af95" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">“基于反向传播的学习算法的推导”，<a class="ae ip" href="http://pages.di.unipi.it/micheli/" rel="noopener ugc nofollow" target="_blank"> A. Micheli </a>，比萨大学计算机科学系，(乳胶版，作者<a class="ae ip" href="http://pages.di.unipi.it/errica/" rel="noopener ugc nofollow" target="_blank"> F. Errica </a>)</li><li id="ce93" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">《神经网络和深度学习》，第二章，网址:<a class="ae ip" href="http://neuralnetworksanddeeplearning.com/" rel="noopener ugc nofollow" target="_blank">neuralnetworksanddeeplearning.com</a></li><li id="11a4" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">Kamil Krzyk，“初学者的<a class="ae ip" href="https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-part-2-cost-function-49545303d29f" rel="noopener" target="_blank">编码深度学习—线性回归(第二部分):成本函数</a>”，载于<a class="ae ip" rel="noopener" href="/">medium.com</a></li><li id="79a9" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">Simeon Kostadinov，<a class="ae ip" href="https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd" rel="noopener" target="_blank">理解反向传播算法</a>，2019，在<a class="ae ip" href="http://towardsdatascience.com" rel="noopener" target="_blank">towardsdatascience.com</a></li><li id="cc2e" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated"><a class="ae ip" href="https://machinelearningknowledge.ai/animated-explanation-of-feed-forward-neural-network-architecture/" rel="noopener ugc nofollow" target="_blank"/><a class="ae ip" href="https://machinelearningknowledge.ai/" rel="noopener ugc nofollow" target="_blank">machine learning knowledge . ai</a>中的<a class="ae ip" href="https://machinelearningknowledge.ai/animated-explanation-of-feed-forward-neural-network-architecture/" rel="noopener ugc nofollow" target="_blank">前馈神经网络架构</a>动画讲解</li><li id="4ea9" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">Kapur，r .“<a class="ae ip" href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener ugc nofollow" target="_blank">神经网络&amp;反向传播算法，在<a class="ae ip" rel="noopener" href="/">medium.com</a>发表于2016年</a></li></ol><h1 id="5d64" class="iq ir hh bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">10.其他有用的来源</h1><p id="f242" class="pw-post-body-paragraph kx ky hh jq b jr js kz la jt ju lb lc jv ld le lf jx lg lh li jz lj lk ll kb ha bi translated">我希望这篇文章能够揭示反向传播背后的数学原理。为了进一步提高你的知识，我在这里推荐一些其他有用的资源来阅读。<em class="lr">尽情享受吧！</em></p><ul class=""><li id="3116" class="jo jp hh jq b jr lm jt ln jv mj jx mk jz ml kb mm kd ke kf bi translated">我的<a class="ae ip" href="https://youtube.com/playlist?list=PLVQpN6IRSo6mXO6LzzlIRLdb2VSf9mgZL" rel="noopener ugc nofollow" target="_blank"> Youtube播放列表</a>关于反向传播</li><li id="4408" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb mm kd ke kf bi translated"><a class="ae ip" href="https://machinelearningknowledge.ai/cost-functions-in-machine-learning/#MSE_Vs_MAE_8211_Which_one_to_choose" rel="noopener ugc nofollow" target="_blank">《机器学习</a>中成本函数的傻瓜指南》，载于<a class="ae ip" href="https://machinelearningknowledge.ai/" rel="noopener ugc nofollow" target="_blank">machinelengknowledge . ai</a></li><li id="51cb" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb mm kd ke kf bi translated">Prakash Jay，"<a class="ae ip" rel="noopener" href="/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c">反向传播非常简单。谁让它变得复杂了？</a>、<a class="ae ip" rel="noopener" href="/">medium.com</a></li><li id="8d12" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb mm kd ke kf bi translated">图沙尔·古普塔，“<a class="ae ip" href="https://towardsdatascience.com/back-propagation-414ec0043d7#.tje3h7wi0" rel="noopener" target="_blank">深度学习:反向传播</a>”，<a class="ae ip" href="https://towardsdatascience.com/" rel="noopener" target="_blank">towardsdatascience.com</a></li><li id="1b88" class="jo jp hh jq b jr kg jt kh jv ki jx kj jz kk kb mm kd ke kf bi translated">Bernard Widrow和Eugene Walach，“<a class="ae ip" href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470231616.app7" rel="noopener ugc nofollow" target="_blank">自适应神经网络三十年:感知器、Madaline和反向传播</a>”，载于《自适应逆控制:信号处理方法》，2008年再版</li></ul></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><p id="a089" class="pw-post-body-paragraph kx ky hh jq b jr lm kz la jt ln lb lc jv lo le lf jx lp lh li jz lq lk ll kb ha bi translated"><em class="lr">如果你喜欢这篇文章，请推荐分享。谢谢你。</em></p></div></div>    
</body>
</html>