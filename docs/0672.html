<html>
<head>
<title>Movie Recommender using Collaborative Filtering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用协同过滤的电影推荐器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/movie-recommender-using-collaborative-filtering-7cf54d54d663?source=collection_archive---------12-----------------------#2021-01-26">https://medium.com/analytics-vidhya/movie-recommender-using-collaborative-filtering-7cf54d54d663?source=collection_archive---------12-----------------------#2021-01-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/74aadf4d61d5aa9f54a2b01f2b3976b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6cH5wogUlyXLH8A-"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">维多利亚诺·伊斯基耶多在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="4e68" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如今，我们在网上所到之处似乎都有类似“你的最佳选择”或“基于你以前的…”；这些更经常出现的网飞电影，或者持续出现在你的歌曲电台中的Spotify歌曲，都是推荐系统的结果。这些系统有时可能会令人讨厌，有点打扰，但是，值得称赞的是，它们通过给我们一个现实的候选名单来节省我们筛选大量内容的时间。</p><p id="af4a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">与拥有更多数据的网飞相比，我创建的推荐系统是有限的；比如用户年龄，这对偏好有很大的影响。我使用的是缩减的<a class="ae it" href="https://grouplens.org/datasets/movielens/" rel="noopener ugc nofollow" target="_blank">电影镜头</a>数据集，它有10万个收视率，而完整数据集只有2500万个收视率。对于预处理和训练，我使用Fastai库，因为它有很多有用的内置功能，我也参加了他们的课程。这里是我的<a class="ae it" href="https://github.com/jacKlinc/movie-recommender" rel="noopener ugc nofollow" target="_blank"> GitHub </a>回购。用于托管<a class="ae it" href="https://share.streamlit.io/jacklinc/movie-recommender/main/src/src_recommender.py" rel="noopener ugc nofollow" target="_blank">电影仪表盘</a>的Streamlit。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="a15a" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">协同过滤</h1><p id="deeb" class="pw-post-body-paragraph iu iv hh iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">最常用的方法被称为协同过滤，它的名字对于机器学习来说出奇的好。它依赖于具有相似兴趣的其他用户的推荐，来过滤结果以最适合您的需要。提到的兴趣被称为协同过滤中的潜在因素，是决定电影推荐的因素。</p><p id="2e92" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">基本步骤</strong>:</p><ol class=""><li id="aa56" class="lc ld hh iw b ix iy jb jc jf le jj lf jn lg jr lh li lj lk bi translated"><em class="ll">随机初始化参数— </em>这些是帮助预测某人是否喜欢某部电影的潜在因素。</li><li id="183c" class="lc ld hh iw b ix lm jb ln jf lo jj lp jn lq jr lh li lj lk bi translated"><em class="ll">计算预测— </em>这是通过获取电影和用户矩阵的矩阵点积来完成的。一个例子是，如果用户对国际电影的喜爱程度很高，而一部电影是外国的；这两者的产品将会很大，并且，对于该用户来说，该电影将会排名更高。</li><li id="ef58" class="lc ld hh iw b ix lm jb ln jf lo jj lp jn lq jr lh li lj lk bi translated"><em class="ll">计算损失— </em>找出预测和用户喜欢之间的差异。使用均方误差(MSE)。</li><li id="d07d" class="lc ld hh iw b ix lm jb ln jf lo jj lp jn lq jr lh li lj lk bi translated"><em class="ll">用SGD优化— </em>现在，通过比较步骤3的误差和预测值的误差，用梯度下降法调整前面提到的参数(潜在因素)。更新参数以改进预测，并且重复该过程，直到精度令人满意。</li></ol><p id="41a3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">奥赖利的下图非常形象地展示了用户和电影评分的交集是如何影响预测的。</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lr"><img src="../Images/74d2595281df0ac3922bdd203e153dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ib6kJOTwYh7yVti6.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">照片由<a class="ae it" href="https://www.oreilly.com/content/deep-matrix-factorization-using-apache-mxnet/" rel="noopener ugc nofollow" target="_blank">奥赖利</a></figcaption></figure><h2 id="28e2" class="lw ka hh bd kb lx ly lz kf ma mb mc kj jf md me kn jj mf mg kr jn mh mi kv mj bi translated">计算预测</h2><p id="4054" class="pw-post-body-paragraph iu iv hh iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">下面的Excel表格显示了步骤1中使用矩阵分解相乘的参数的交叉表。为了简单起见，下面的潜在因素的数量是五。</p><pre class="ls lt lu lv fd mk ml mm mn aw mo bi"><span id="5788" class="lw ka hh ml b fi mp mq l mr ms">user_factors = torch.randn(n_users, 5)</span><span id="e7a1" class="lw ka hh ml b fi mt mq l mr ms">movie_factors = torch.randn(n_movies, 5)</span></pre><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mu"><img src="../Images/b25664f4a0b3c10efa91cd40e53a75b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SZunHKjiybIuDnln"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">由<a class="ae it" href="https://colab.research.google.com/github/fastai/fastbook/blob/master/08_collab.ipynb" rel="noopener ugc nofollow" target="_blank"> FastAI </a>拍摄</figcaption></figure><p id="7788" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">一热编码</strong></p><p id="8831" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一个热编码是机器学习中分类变量所需的预处理步骤。字符串变量(分类的)和模型合不来，所以把它们转换成模型能理解的向量是至关重要的。计算特定用户的预测如下所示:</p><pre class="ls lt lu lv fd mk ml mm mn aw mo bi"><span id="1b73" class="lw ka hh ml b fi mp mq l mr ms">one_hot_user = one_hot(12, n_users).float()</span><span id="d392" class="lw ka hh ml b fi mt mq l mr ms">user_factors.t() @ one_hot_user</span></pre><p id="f7dc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">第一行在索引12处为用户创建一个独热编码向量。这个向量和用户矩阵的乘积进行预测。这种方法可以用于预测，但是效率非常低，因为它需要为每个预测创建和存储一个向量。</p><p id="80fb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">矩阵分解</strong></p><p id="9fcd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">更好的解决方案是使用矩阵分解或嵌入，这使用py torch(FastAI下的API)内置的一个层，支持直接索引到矩阵中。下面的“create_params”函数返回一个满是归一化随机参数的秩为2的张量。</p><figure class="ls lt lu lv fd ii"><div class="bz dy l di"><div class="mv mw l"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">矩阵分解实现</figcaption></figure><p id="f7e6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">“DotProductBias”类的“y_range”参数首先出现在第15行，用于限制第40行的sigmoid函数。电影分级范围是0-5，但由于sigmoid渐近线的最大值，5.5更好。在第15行看到的偏差参数说明了受到高偏差(挑剔/容易高兴)影响的用户和电影；像《年少轻狂》这样的电影对大多数用户来说是得分很高的例子。</p><h2 id="2fd6" class="lw ka hh bd kb lx ly lz kf ma mb mc kj jf md me kn jj mf mg kr jn mh mi kv mj bi translated">计算损失</h2><p id="0892" class="pw-post-body-paragraph iu iv hh iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">传递给下面的学习者对象的第一个变量是包含训练集和验证集的DataLoaders对象。接下来是架构本身，上面的“dp_bias_model”是DotProductBias类。幸运的是，FastAI提供了损失函数，因此，不需要编写任何额外的代码。“MSELossFlat()”与PyTorch的“MSELoss()”相同，但它使输入变平。</p><pre class="ls lt lu lv fd mk ml mm mn aw mo bi"><span id="28c5" class="lw ka hh ml b fi mp mq l mr ms">learn = Learner(dls, dp_bias_model, loss_func=MSELossFlat())</span></pre><p id="94b4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">均方误差(MSE)查找预测值和实际值之间的误差，对其进行平方，然后查找其平均值，这也称为平方L2归一化。这只是毕达哥拉斯定理，但不是三角形的两条边，而是五十个电影因素。</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es mx"><img src="../Images/e2a328327c0678a002ff4ae86e90cfd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/1*mWq0sad41621ADxBUtrRxA.gif"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">MSE损失方程</figcaption></figure><h2 id="4464" class="lw ka hh bd kb lx ly lz kf ma mb mc kj jf md me kn jj mf mg kr jn mh mi kv mj bi translated">使用SGD优化</h2><p id="9926" class="pw-post-body-paragraph iu iv hh iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">同样，不需要实现随机梯度下降，因为FastAI默认具有Adam优化器。Adam依赖于过去梯度的指数衰减，但是细节超出了本文的范围——查看<a class="ae it" href="https://machinelearningjourney.com/index.php/2021/01/09/adam-optimizer/" rel="noopener ugc nofollow" target="_blank"> this </a>了解更多信息。</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es my"><img src="../Images/50aef85bcb092a536890b6cdfbbad9f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/1*je440AazOKkaCPoVKZerJA.gif"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">SGD方程</figcaption></figure><p id="e886" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">该模型在下面以0.005的学习率被训练五个时期。</p><pre class="ls lt lu lv fd mk ml mm mn aw mo bi"><span id="e660" class="lw ka hh ml b fi mp mq l mr ms">learn.fit_one_cycle(5, 5e-3)</span></pre><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="ab fe cl mz"><img src="../Images/94630b5f2420efcd1bddf080a325ecd3.png" data-original-src="https://miro.medium.com/v2/format:webp/1*l5RNevvR5T5kPJedyvLGlQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">模型损失</figcaption></figure><p id="244b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于左侧的每个时期，训练误差减少，但是验证误差增加，这种行为表明对数据的过度拟合。</p><h2 id="caba" class="lw ka hh bd kb lx ly lz kf ma mb mc kj jf md me kn jj mf mg kr jn mh mi kv mj bi translated">规范化</h2><p id="f552" class="pw-post-body-paragraph iu iv hh iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">也称为权重衰减，L2正则化旨在减轻数据集过度拟合的影响。该过程将权重的平方和加到损失函数上；这确保了参数尽可能小。这一项随后被添加到标准损失函数中。</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es na"><img src="../Images/780400cdc5ec2c9e8ea3d9915c0c32e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/1*DzMs0ja9yYLk1geAo73JqQ.gif"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">损失函数正则化项</figcaption></figure><p id="6afc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上面的lambda表达式用于控制模型与数据的拟合程度，项越大；抛物线越窄，数据拟合越不紧密。FastAI在下面提供了一个例子，其中a是lambda。</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es nb"><img src="../Images/1079bcd7b79563a2befffe3f2f3a8a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*aAbYFqe08-sdjhTjdWrOnA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">FastAI规范化示例</figcaption></figure><p id="e3d8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用正则化就像给学习者对象添加权重衰减参数一样简单。</p><pre class="ls lt lu lv fd mk ml mm mn aw mo bi"><span id="3ccb" class="lw ka hh ml b fi mp mq l mr ms">learn.fit_one_cycle(5, 5e-3, wd=0.1)</span></pre><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nc"><img src="../Images/445a3144431bbf1467a3bc8407b85f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*iVvvgzrAmDZhQQ0DiW_8XQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">正则化损失模型</figcaption></figure><p id="50a8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">不需要太多的努力就能有很大的进步！防止模型与数据拟合得太好，可以让它更好地概括新数据。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="86ae" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">推荐</h1><p id="c470" class="pw-post-body-paragraph iu iv hh iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">那么模型训练好了，我们怎么给一个用户预测一部电影呢？比方说，我们喜欢塔伦蒂诺电影，所以我们尝试真正的浪漫。</p><pre class="ls lt lu lv fd mk ml mm mn aw mo bi"><span id="9b15" class="lw ka hh ml b fi mp mq l mr ms">fav_movie = 'True Romance (1993)'<br/>movie_factors = learn.model.movie_factors</span></pre><p id="848a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，从DataLoaders对象中选择了与电影同名的行。</p><pre class="ls lt lu lv fd mk ml mm mn aw mo bi"><span id="cd06" class="lw ka hh ml b fi mp mq l mr ms">idxs = dls.classes['title'].o2i[fav_movie]</span></pre><p id="4248" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">找到与真正浪漫距离最小的电影，就会找到最喜欢的:</p><pre class="ls lt lu lv fd mk ml mm mn aw mo bi"><span id="a45c" class="lw ka hh ml b fi mp mq l mr ms">dist = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idxs][None])</span></pre><p id="a345" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，我们需要对距离进行排序，并选择前3名:</p><pre class="ls lt lu lv fd mk ml mm mn aw mo bi"><span id="f42b" class="lw ka hh ml b fi mp mq l mr ms">idx = distances.argsort(descending=True)[1:4]</span></pre><p id="6b03" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">索引到数据加载器以获取电影标题:</p><pre class="ls lt lu lv fd mk ml mm mn aw mo bi"><span id="c586" class="lw ka hh ml b fi mp mq l mr ms">[dls.classes['title'][i] for i in idx]</span></pre><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es nd"><img src="../Images/4899907027d302aeaa82db81624b8ded.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*XsAhZC7vm1uHDz9lon7zkA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">真爱至上3</figcaption></figure><h2 id="2f71" class="lw ka hh bd kb lx ly lz kf ma mb mc kj jf md me kn jj mf mg kr jn mh mi kv mj bi translated">结论</h2><p id="71b4" class="pw-post-body-paragraph iu iv hh iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">有许多方面的推荐系统被排除在本文之外，主要是因为我对这个主题还不熟悉，并且计算资源有限。这种模式的缺点之一是电影数量有限，只有1600部左右；我需要推荐的许多电影都不在名单上。使用完整的MovieLens数据集可以访问更多的电影和更多的用户评级，这将提高准确性，但需要相当长的时间来训练。这里没有涉及的另一个方面是深度学习，它不是表格数据的显而易见的选择，但对所有机器学习问题越来越有用。</p><p id="7439" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我会推荐我以前的<a class="ae it" rel="noopener" href="/analytics-vidhya/mask-detector-w-fastai-and-streamlit-sharing-62448b4cb7b6">文章</a>，以获取任何关于我如何在Streamlit上主持它的信息。我也会去看看我一直密切关注的FastAI课程。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h2 id="9ef8" class="lw ka hh bd kb lx ly lz kf ma mb mc kj jf md me kn jj mf mg kr jn mh mi kv mj bi translated">参考</h2><ul class=""><li id="6ca4" class="lc ld hh iw b ix kx jb ky jf ne jj nf jn ng jr nh li lj lk bi translated"><a class="ae it" href="https://course.fast.ai/" rel="noopener ugc nofollow" target="_blank"> FastAI第八课</a></li><li id="8797" class="lc ld hh iw b ix lm jb ln jf lo jj lp jn lq jr nh li lj lk bi translated"><a class="ae it" href="https://grouplens.org/datasets/movielens/" rel="noopener ugc nofollow" target="_blank">电影镜头</a>数据集</li><li id="7548" class="lc ld hh iw b ix lm jb ln jf lo jj lp jn lq jr nh li lj lk bi translated"><a class="ae it" href="https://www.oreilly.com/content/deep-matrix-factorization-using-apache-mxnet/" rel="noopener ugc nofollow" target="_blank">奥赖利</a>照片</li><li id="e0f6" class="lc ld hh iw b ix lm jb ln jf lo jj lp jn lq jr nh li lj lk bi translated"><a class="ae it" href="https://machinelearningjourney.com/index.php/2021/01/09/adam-optimizer/" rel="noopener ugc nofollow" target="_blank">亚当乐观者</a></li></ul></div></div>    
</body>
</html>