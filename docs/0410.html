<html>
<head>
<title>Sentiment Analysis of Movie Reviews pt.1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">电影评论的情感分析第一部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sentiment-analysis-of-movie-reviews-pt-1-1a52daa90cdc?source=collection_archive---------18-----------------------#2021-01-15">https://medium.com/analytics-vidhya/sentiment-analysis-of-movie-reviews-pt-1-1a52daa90cdc?source=collection_archive---------18-----------------------#2021-01-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="58cc" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">第1部分—基础知识</h2></div><p id="6a75" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">更多代码链接到我的Github:<a class="ae js" href="https://github.com/charliezcr/Sentiment-Analysis-of-Movie-Reviews/blob/main/sa_p1.ipynb" rel="noopener ugc nofollow" target="_blank">https://Github . com/charliezcr/情操分析-电影评论/blob/main/sa_p1.ipynb </a></p><p id="d01d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当你有大量的影评时，如何知道它们是补充还是批评？由于数据集的量很大，不能一一标注，需要使用自然语言处理工具对文本的情感进行分类。特别是在Python中，像nltk和scikit-learn这样强大的包可以帮助我们进行文本分类。在这个项目中，我从imdb 上的<a class="ae js" href="https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences" rel="noopener ugc nofollow" target="_blank">评论数据集对电影评论进行了情感分析，该数据集来自UCI机器学习知识库的情感标签句子数据集。</a></p><h2 id="e0e3" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">文本预处理</h2><p id="5945" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">在这个数据集中，有1000条电影评论，包括500条正面(称赞)和500条负面(批评)。例如，<em class="kt">‘一部非常、非常、非常缓慢、漫无目的的电影，讲的是一个苦恼、漂泊的年轻人。’</em>被标记为差评。</p><p id="61a8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然而，在原始文本中，我们可以看到许多单词并不包含重要的语义。数字和标点符号在我们的原始文本中出现很多，但并不表达积极或消极的情绪。因此，我们需要剥离数字和标点符号。</p><p id="75d2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了进一步清理数据，我们需要对单词进行词干处理，以便具有不同词形变化的单词可以被视为相同的标记，因为它们传达相同的语义。例如,“distress”和“distressed”都将作为“distress”词干。</p><p id="8234" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">经过文本预处理后，我们有2个数据列表。“标签”是我们分类的目标列表。“预处理”是分类的目标特征。对于“预处理”的句子，我们将原始文本翻译成完全不可读的文本。例如，<em class="kt">‘一部非常、非常、非常缓慢、漫无目的的电影，讲的是一个苦恼、漂泊的年轻人。’</em>被预处理为<em class="kt">“一部关于一个遇险漂流青年的漫无目的的慢动作电影”</em></p><p id="f4c7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你可能会想:我们将原始文本翻译成这些不可读的文本，因为我们希望每个标记都传达重要的语义。那么，为什么不去掉停用词，因为它们不传达重要的语义，但非常频繁，如“a”和“about”？在下一个特征提取部分，我们将使用TF-IDF来处理这些停用词</p><h2 id="1367" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">特征抽出</h2><p id="ad5b" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">在文本预处理之后，我们将从清理后的数据中提取特征。我们将使用TF-IDF矢量器作为我们的单词嵌入来对文本进行矢量化和规范化。</p><p id="dbd2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">TF-IDF代表术语频率-逆文档频率。它评估标记对语料库中的文档的重要性。TF-IDF将数据作为我们的模型，因为它将词频或简单的字数标准化了。它还减少了停用字词的干扰。</p><p id="72a2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这种情况下，我们将采用TF-IDF的变体。正规TF-IDF的公式是<a class="ae js" href="http://www.tfidf.com/" rel="noopener ugc nofollow" target="_blank">这里是</a>。与原来的TF-IDF不同，我们改为使用sublinear_tf，用WF = 1 + log(TF)代替TF。这个变体解决了<a class="ae js" href="https://nlp.stanford.edu/IR-book/html/htmledition/sublinear-tf-scaling-1.html" rel="noopener ugc nofollow" target="_blank">的问题:“一个术语在一个文档中出现二十次，其意义并不等于一次出现的二十倍。”</a>例如，在我们的第一个评论<em class="kt">中，“一部非常、非常、非常缓慢、漫无目的的电影，讲述了一个痛苦、漂泊的年轻人。”</em>‘非常’出现了三次。因此，对于我们的数据集，我们需要应用次线性TF标度。这极大地提高了我们模型预测的准确性。</p><p id="7b8f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在特征提取之后，我们以压缩的稀疏行格式存储Tf-IDF加权的文档术语矩阵。每一个目标都是这句话的感悟。1表示正，0表示负。但是为了使数据适合我们的模型，我们需要将数据分成特征和目标。Scikit-learn的train_test_split对数据进行随机洗牌，拆分成训练集和测试集。在这种特定情况下，我将使用整个测试集的1/5，其余的4/5作为训练集。下面是整个预处理过程的代码:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="167e" class="jt ju hh kz b fi ld le l lf lg"><strong class="kz hi">from</strong> <strong class="kz hi">nltk.stem</strong> <strong class="kz hi">import</strong> PorterStemmer    <em class="kt"># stem the words</em><br/><strong class="kz hi">from</strong> <strong class="kz hi">nltk.tokenize</strong> <strong class="kz hi">import</strong> word_tokenize <em class="kt"># tokenize the sentences into tokens</em><br/><strong class="kz hi">from</strong> <strong class="kz hi">string</strong> <strong class="kz hi">import</strong> punctuation<br/><strong class="kz hi">from</strong> <strong class="kz hi">sklearn.feature_extraction.text</strong> <strong class="kz hi">import</strong> TfidfVectorizer <em class="kt"># vectorize the texts</em><br/><strong class="kz hi">from</strong> <strong class="kz hi">sklearn.model_selection</strong> <strong class="kz hi">import</strong> train_test_split <em class="kt"># split the testing and training sets</em></span><span id="f3aa" class="jt ju hh kz b fi lh le l lf lg"><strong class="kz hi">def</strong> preprocess(path):<br/>    <em class="kt">'''generate cleaned dataset</em><br/><em class="kt">    </em><br/><em class="kt">    Args:</em><br/><em class="kt">        path(string): the path of the file of testing data</em></span><span id="0cf1" class="jt ju hh kz b fi lh le l lf lg"><em class="kt">    Returns:</em><br/><em class="kt">        X_train (list): the list of features of training data</em><br/><em class="kt">        X_test (list): the list of features of test data</em><br/><em class="kt">        y_train (list): the list of targets of training data ('1' or '0')</em><br/><em class="kt">        y_test (list): the list of targets of training data ('1' or '0')</em><br/><em class="kt">    '''</em><br/>    <br/>    <em class="kt"># text preprocessing: iterate through the original file and </em><br/>    <strong class="kz hi">with</strong> open(path, encoding='utf-8') <strong class="kz hi">as</strong> file:<br/>        <em class="kt"># record all words and its label</em><br/>        labels = []<br/>        preprocessed = []<br/>        <strong class="kz hi">for</strong> line <strong class="kz hi">in</strong> file:<br/>            <em class="kt"># get sentence and label</em><br/>            sentence, label = line.strip('<strong class="kz hi">\n</strong>').split('<strong class="kz hi">\t</strong>')<br/>            labels.append(int(label))<br/>            <br/>            <em class="kt"># remove punctuation and numbers</em><br/>            <strong class="kz hi">for</strong> ch <strong class="kz hi">in</strong> punctuation+'0123456789':<br/>                sentence = sentence.replace(ch,' ')<br/>            <em class="kt"># tokenize the words and stem them</em><br/>            words = []<br/>            <strong class="kz hi">for</strong> w <strong class="kz hi">in</strong> word_tokenize(sentence):<br/>                words.append(PorterStemmer().stem(w))<br/>            preprocessed.append(' '.join(words))<br/>    <br/>    <em class="kt"># vectorize the texts</em><br/>    vectorizer = TfidfVectorizer(stop_words='english', sublinear_tf=<strong class="kz hi">True</strong>)<br/>    X = vectorizer.fit_transform(preprocessed)<br/>    <em class="kt"># split the testing and training sets</em><br/>    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)<br/>    <strong class="kz hi">return</strong> X_train, X_test, y_train, y_test</span><span id="e0ae" class="jt ju hh kz b fi lh le l lf lg">X_train, X_test, y_train, y_test = preprocess('imdb_labelled.txt')</span></pre><h2 id="28e5" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">建模</h2><p id="0fce" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">我们可以用训练集训练模型，让模型对测试集进行分类，并通过检查模型的精度分数和时间消耗来评价模型的性能。以下是分类代码:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="81b0" class="jt ju hh kz b fi ld le l lf lg"><strong class="kz hi">from</strong> <strong class="kz hi">sklearn.metrics</strong> <strong class="kz hi">import</strong> accuracy_score<br/><strong class="kz hi">from</strong> <strong class="kz hi">sklearn.metrics</strong> <strong class="kz hi">import</strong> plot_confusion_matrix<br/><strong class="kz hi">from</strong> <strong class="kz hi">matplotlib</strong> <strong class="kz hi">import</strong> pyplot <strong class="kz hi">as</strong> plt<br/><strong class="kz hi">from</strong> <strong class="kz hi">time</strong> <strong class="kz hi">import</strong> time</span><span id="ddb1" class="jt ju hh kz b fi lh le l lf lg"><strong class="kz hi">def</strong> classify(clf, todense=<strong class="kz hi">False</strong>):<br/>    <em class="kt">'''to classify the data using machine learning models</em><br/><em class="kt">    </em><br/><em class="kt">    Args:</em><br/><em class="kt">        clf: the model chosen to analyze the data</em><br/><em class="kt">        todense(bool): whether to make the sparse matrix dense</em><br/><em class="kt">        </em><br/><em class="kt">    '''</em><br/>    <strong class="kz hi">global</strong> X_train, X_test, y_train, y_test<br/>    t = time()<br/>    <strong class="kz hi">if</strong> todense:<br/>        clf.fit(X_train.todense(), y_train)<br/>        y_pred = clf.predict(X_test.todense())<br/>    <strong class="kz hi">else</strong>:<br/>        clf.fit(X_train, y_train)<br/>        y_pred = clf.predict(X_test)<br/>    print(f'Time cost of <strong class="kz hi">{</strong>str(clf)<strong class="kz hi">}</strong>: <strong class="kz hi">{</strong>round(time()-t,2)<strong class="kz hi">}</strong>s<strong class="kz hi">\n</strong>The accuracy of <strong class="kz hi">{</strong>str(clf)<strong class="kz hi">}</strong>: <strong class="kz hi">{</strong>accuracy_score(y_test,y_pred)<strong class="kz hi">}\n</strong>')</span></pre><p id="f702" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因为目标是分类的和二分法的，特征不具有假定的分布，所以我们可以用于文本分类的模型是逻辑回归、随机梯度下降分类器(SGDClassifier)、支持向量分类器(SVC)和神经网络(MLPClassifier)。因为我们的特征数据是稀疏的，所以SVC和SGD是有用的。在3种朴素贝叶斯分类器(伯努利、多项式和高斯)中，我们需要选择多项式，因为特征是由TF-IDF归一化的。这些特征既不符合高斯分布也不符合伯努利分布。</p><p id="e070" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我将在下面分析每个选定型号的性能。在这一部分，我不会调整每个模型的参数，但以后会这样做。</p><p id="17b8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们也可以在技术上使用线性判别分析。然而，像我们的特征数据那样计算稀疏矩阵在计算上是昂贵的。这种模型的准确性也较低。所以这次不考虑LDA。下面是LDA的表现:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="d5b0" class="jt ju hh kz b fi ld le l lf lg"><strong class="kz hi">from</strong> <strong class="kz hi">sklearn.discriminant_analysis</strong> <strong class="kz hi">import</strong> LinearDiscriminantAnalysis<br/>classify(LinearDiscriminantAnalysis(),todense=<strong class="kz hi">True</strong>)</span></pre><p id="0eb7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">线性判别分析的时间成本():0.79秒<br/>线性判别分析的准确度():0.71 </p><p id="662d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">以下是入选车型的表现:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="d7a5" class="jt ju hh kz b fi ld le l lf lg"><strong class="kz hi">from</strong> <strong class="kz hi">sklearn.linear_model</strong> <strong class="kz hi">import</strong> LogisticRegression<br/><strong class="kz hi">from</strong> <strong class="kz hi">sklearn.naive_bayes</strong> <strong class="kz hi">import</strong> MultinomialNB<br/><strong class="kz hi">from</strong> <strong class="kz hi">sklearn.svm</strong> <strong class="kz hi">import</strong> SVC<br/><strong class="kz hi">from</strong> <strong class="kz hi">sklearn.linear_model</strong> <strong class="kz hi">import</strong> SGDClassifier<br/><strong class="kz hi">from</strong> <strong class="kz hi">sklearn.neural_network</strong> <strong class="kz hi">import</strong> MLPClassifier<br/><strong class="kz hi">for</strong> model <strong class="kz hi">in</strong> [LogisticRegression(), MultinomialNB(), SVC(), SGDClassifier(), MLPClassifier()]:<br/>    classify(model)</span></pre><p id="9305" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="kt">物流回收的时间成本():0.03秒<br/>物流回收的准确度():0.825 </em></p><p id="46ce" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="kt">多项式的时间成本inb():0.0s<br/>多项式的精度inb():0.825</em></p><p id="f723" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="kt">SVC()的时间成本:0.09秒<br/>SVC()的精度:0.835 </em></p><p id="3fd6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="kt">SGD classifier()的时间成本:0.0s<br/>SGD classifier()的精度:0.82 </em></p><p id="b747" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="kt">MLP分类器的时间成本():3.47秒<br/>MLP分类器的准确度():0.81 </em></p><h2 id="207c" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">集成学习</h2><p id="23e7" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">虽然我们希望提高模型预测的准确性，但我们也希望避免过度拟合，以便我们可以使用模型预测其他数据集。构建集成方法是解决这个问题的方法。对于每个评论，我们将让每个选定的模型为其自己的预测投票，并采用所有投票的模式来生成集合预测。选择的模型有逻辑回归、多项式回归、SVC和SGD。因为神经网络需要复杂的调整，并且非常耗时，所以我不会将MLPClassifier包含在这个集成学习中。从下面的准确度分数和混淆矩阵中，我们可以看到，尽管时间成本增加了，但是集合模型的性能是令人满意的。</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="225d" class="jt ju hh kz b fi ld le l lf lg"><strong class="kz hi">from</strong> <strong class="kz hi">statistics</strong> <strong class="kz hi">import</strong> mode<br/><strong class="kz hi">def</strong> ensemble(models):<br/>    <em class="kt">'''to ensemble the models and classify the data based on each model's vote</em><br/><em class="kt">    </em><br/><em class="kt">    Args:</em><br/><em class="kt">        models: the list of models chosen to analyze the data</em><br/><em class="kt">        </em><br/><em class="kt">    '''</em><br/>    <strong class="kz hi">global</strong> X_train, X_test, y_train, y_test<br/>    t = time()<br/>    <em class="kt"># iterate through all the models and collect all their predictions</em><br/>    y_preds = []<br/>    <strong class="kz hi">for</strong> clf <strong class="kz hi">in</strong> models:<br/>        clf.fit(X_train, y_train)<br/>        y_preds.append(clf.predict(X_test))<br/>    <br/>    <em class="kt"># Count their votes and get the mode of each prediction as the decision</em><br/>    y_pred = []<br/>    <strong class="kz hi">for</strong> i <strong class="kz hi">in</strong> range(len(y_preds[0])):<br/>        y_pred.append(mode([y[i] <strong class="kz hi">for</strong> y <strong class="kz hi">in</strong> y_preds]))<br/>    print(f'Time cost: <strong class="kz hi">{</strong>round(time()-t,2)<strong class="kz hi">}</strong>s<strong class="kz hi">\n</strong>Accuracy: <strong class="kz hi">{</strong>accuracy_score(y_test,y_pred)<strong class="kz hi">}\n</strong>')<br/>    plot_confusion_matrix(clf, X_test, y_test, values_format = 'd',display_labels=['positive','negative'])</span><span id="3e48" class="jt ju hh kz b fi lh le l lf lg">ensemble([LogisticRegression(),MultinomialNB(),SVC(),SGDClassifier()])</span></pre><p id="3d42" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="kt">时间成本:0.12秒<br/>精度:0.83 </em></p><figure class="ku kv kw kx fd lj er es paragraph-image"><div class="er es li"><img src="../Images/2be60bbf36805638773d2b7658782ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*gCvRvjnY9oPffX14WVMyqA.png"/></div></figure></div></div>    
</body>
</html>