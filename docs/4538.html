<html>
<head>
<title>Feed Forward Neural Networks | Intuition on Forward Propagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">前馈神经网络|对正向传播的直觉</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feed-forward-neural-networks-intuition-on-forward-propagation-f77468fad625?source=collection_archive---------4-----------------------#2021-11-10">https://medium.com/analytics-vidhya/feed-forward-neural-networks-intuition-on-forward-propagation-f77468fad625?source=collection_archive---------4-----------------------#2021-11-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="85b3" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">为什么是神经网络？神经网络是如何做到这些的？正向传播是如何工作的？</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/faa22333f5ef6068477df61a56dd6ff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Om3zPGAi7BLKGsVDrB0IfA.png"/></div></div></figure><p id="25d7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">神经网络可以被认为是一种可以在输入和输出之间进行映射的功能。理论上，无论这个函数有多复杂，神经网络都应该能够逼近这个函数。然而，大多数监督学习(如果不是全部的话)都是关于学习一个映射给定的<strong class="jl hj"> X </strong>和<strong class="jl hj"> Y </strong>、<strong class="jl hj">T5】的函数，然后使用该函数为一个<strong class="jl hj">新X找到合适的<strong class="jl hj">Y</strong>。</strong>如果是这样，传统的机器学习算法和神经网络之间有什么区别？答案是所谓的<em class="kf">感应偏置。这个术语可能看起来很新。但是，它只不过是我们在将机器学习模型拟合到X和Y之间的关系之前所做的假设。</em></strong></p><p id="b0b7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">例如，如果我们认为X和Y之间的关系是线性的，我们可以使用线性回归。线性回归的归纳偏差是X和Y的关系是线性的。因此，它用一条线或一个超平面来拟合数据。<br/>但是，当X和Y之间存在非线性和复杂的关系时，线性回归算法可能无法很好地预测Y。在这种情况下，我们可能需要曲线或多维曲线来近似这种关系。神经网络的主要优势是它的归纳偏差非常弱，因此，无论这种关系或功能有多复杂，网络都能够以某种方式逼近它。</p><p id="21a1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">而且，基于函数的复杂性，我们可能不得不手动设置每层中神经元的数量以及网络的层数。这通常是通过反复试验和经验来完成的。因此，这些参数被称为超参数。</p><blockquote class="kg kh ki"><p id="7af5" class="jj jk kf jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">神经网络不过是复杂的曲线拟合机器。— <a class="km kn ge" href="https://medium.com/u/6648478c1752?source=post_page-----f77468fad625--------------------------------" rel="noopener" target="_blank">乔希·斯塔默</a></p></blockquote><h1 id="ae3e" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">神经网络的结构和工作</h1><p id="eb89" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">在我们了解神经网络工作的原因之前，先展示一下神经网络的功能是合适的。在理解神经网络的架构之前，我们需要先看看神经元是做什么的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ll"><img src="../Images/47aa4b9dfdd3104cef2bd996a677c75e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T4ARzySpEQvEnr_9pc78pg.jpeg"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">图1.1:人工神经元</figcaption></figure><p id="89b5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">人工神经元的每个输入都有一个与之相关的权重。输入首先与它们各自的权重相乘，并向结果添加偏差。我们可以称之为加权和。然后，加权和通过一个激活函数，这基本上是一个非线性函数。</p><p id="1ea5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">因此，人工神经元可以被认为是一个简单的或多元的线性回归模型，在末端有一个激活函数。说到这里，让我们继续讨论神经网络架构</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lq"><img src="../Images/0440a4b7a39e402a44e571cde6510a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TXEzexxE_XJPcpJqyuYKhw.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">图1.2:神经网络</figcaption></figure><p id="72ae" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">神经网络通常具有多层，每层具有多个神经元，其中一层中的所有神经元都连接到下一层中的所有神经元，依此类推。</p><p id="df5c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在图1.2中，我们有4层。第一层是输入层，看起来它包含6个神经元，但实际上，它只是作为神经网络<em class="kf">输入的数据(有6个神经元，因为输入数据可能有6列)</em>。最后一层是输出层。最终层和第一层的神经元数量由数据集和问题类型<em class="kf">(输出类的数量等)</em>预先确定。隐藏层中神经元的数量和隐藏层的数量将通过反复试验来选择。</p><p id="ec4a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">来自层<strong class="jl hj"> <em class="kf"> i </em> </strong>的神经元将来自层<strong class="jl hj"> <em class="kf"> i-1 </em> </strong>的所有神经元的输出作为输入，并计算加权和，向其添加偏置，然后最终通过激活函数发送，正如我们在上面看到的人工神经元的情况。第一个隐藏层的第一个神经元将连接到前一层的所有输入(<em class="kf">输入层</em>)。同样，第一个隐藏层的第二个神经元也将连接到前一层的所有输入，第一个隐藏层的所有神经元也是如此。对于第二个隐藏层中的神经元，前一个隐藏层的输出被认为是输入，同样，这些神经元中的每一个都连接到所有前一个神经元。</p><p id="55c9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">具有m个神经元的层，在具有n个神经元的层之前，将具有n*m + m个<em class="kf">(包括偏差)</em>连接或链接，其中每个链接携带权重。这些权重是随机初始化的，但是在训练时，它们达到它们的最优值，以便减少我们选择的损失函数。我们将在接下来的博客中看到关于学习这些重量的细节。</p><h1 id="7b54" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">正向传播示例</h1><p id="f9b1" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">让我们考虑一下图1.2中的神经网络，然后为了更好的理解，展示前向传播是如何与这个网络一起工作的。我们可以看到，在输入层有6个神经元，这意味着有6个输入。<br/> <em class="kf">注意:为了计算的目的，我没有包括偏差。但是，如果要包含偏差，那么将会有一个额外的输入I0，其值将始终为1，并且在权重矩阵w01、w02…w04的开头将会有一个额外的行</em></p><p id="2b89" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">设输入为I = [ I1，I2，I3，I4，I5，I6 ]。我们可以看到第一个隐藏层有4个神经元。因此，在输入层和第一个隐藏层之间将有<strong class="jl hj"> 6*4 </strong>个连接(无偏置)。这些连接在下面的权重矩阵中用绿色表示，其值w_ij表示来自输入层的第<em class="kf"> i </em>个神经元和来自第一隐藏层的第<em class="kf"> j </em>个神经元之间的连接的权重。如果我们将<em class="kf">(矩阵乘法)</em><em class="kf">1 * 6</em>输入矩阵与<em class="kf"> 6*4 </em>权重矩阵相乘，我们将得到第一个隐藏层的输出，即<em class="kf"> 1*4 </em>。这是有意义的，因为在第一个隐藏层中实际上有4个神经元。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lr"><img src="../Images/a8ab5e79356f85daa374f62358a31967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*QlifnaEZTP7UkhLlnzo3dg.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">图2.1</figcaption></figure><p id="3322" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这4个输出在图2.1中用红色表示。一旦获得这些值，我们通过激活函数发送它们，以便引入非线性，然后，这些值将是第一个隐藏层的精确输出。<br/>现在，我们用不同的权重矩阵对第二个隐藏层继续同样的步骤。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ls"><img src="../Images/89b563dce890a9fcfe83849e28bff06e.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*HBSPacAN_kroCKLJ84lD5w.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">图2.2</figcaption></figure><p id="146c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">i1、i2等。不过是前一层的输出。为了便于理解，我使用了相同的变量I。类似于我们之前看到的，输入的1*4矩阵要乘以4*3的权重矩阵<em class="kf">(因为第二个隐层有3个神经元)</em>，输出一个1*3的矩阵。该矩阵中各个元素的激活将成为下一层的输入。</p><blockquote class="kg kh ki"><p id="c458" class="jj jk kf jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">猜猜最后一层的权重矩阵的形状</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lt"><img src="../Images/5f8ef75c15e5a06a1debc6433ad246a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*i2YK6yRSXMAtkkY5PqdMnQ.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">图2.3</figcaption></figure><p id="6232" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">由于最后一层只有1个神经元，前一层有3个输出，权重矩阵的大小将为<strong class="jl hj"> 3*1，</strong>，这标志着简单前馈神经网络中正向传播的结束。</p><h1 id="b325" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">为什么这种方法有效？</h1><p id="6f3f" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">我们已经看到网络中每个神经元的行为与线性回归没有太大区别。此外，神经元在末端增加了一个激活函数，每个神经元有一个不同的权重向量。但是，为什么会这样呢？</p><p id="2b66" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在我们已经看到了计算是如何工作的。但是，我写这篇博客的主要目的是阐明为什么这种方法有效。理论上，神经网络应该能够逼近任何连续函数，无论它是多么复杂和非线性。我将尽力说服你和我自己，使用正确的参数<em class="kf">(权重和偏差)</em>，网络应该能够用我们上面看到的方法学习任何东西。</p><h2 id="50ac" class="lu kp hi bd kq lv lw lx ku ly lz ma ky js mb mc la jw md me lc ka mf mg le mh bi translated">非线性是关键</h2><p id="6e58" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">在我们进一步探讨之前，我们需要了解非线性的力量。当我们添加两个或更多的线性对象，如直线、平面或超平面时，结果也是线性对象:分别是直线、平面或超平面。不，不管我们以什么比例添加这些线性对象，我们仍然会得到一个线性对象。</p><p id="3793" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">但是，对于非线性对象之间的加法，情况并非如此。当我们添加两条不同的曲线时，我们可能会得到一条更复杂的曲线。这显示在下面的要点。如果我们能够以不同的比例添加这些非线性曲线的不同部分，我们应该能够以某种方式影响最终曲线的形状。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="5426" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">除了添加非线性对象或像“超平面”这样的“超曲线”，我们还通过这些激活函数在每一层引入非线性。这基本上意味着，我们正在对一个已经非线性的对象应用一个非线性函数。通过调整这些偏差和权重，我们能够改变合成曲线或函数的形状。</p><p id="8096" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这就是为什么更复杂的问题需要更多的隐藏层和更多的隐藏神经元，而不太复杂的问题或关系可以用更少的层和神经元来近似。每个神经元都充当一个问题解决者。它们都解决了自己的小问题，结合起来，它们解决了一个更大的问题，通常是降低成本函数。这里用的确切词是<strong class="jl hj">分而治之。</strong></p><h1 id="24fc" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">如果神经网络不使用激活函数会怎样？</h1><p id="fef4" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">如果神经网络不使用激活函数，它只是一个大的线性单元，可以很容易地被一个单一的线性回归模型所取代。</p><p id="d16d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">y = m * x+c<br/>z = k * y+t =&gt;k *(m * x+c)+t =&gt;k * m * x+k * c+t =&gt;(k * m)* x+(k * c+t)</p><p id="314d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这里，Z也线性依赖于x，因为k*m可以用另一个变量代替，k*c+t可以用另一个变量代替，因此，如果没有激活函数，无论有多少层和多少个神经元，所有这些都是多余的。</p><h1 id="4321" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">结论</h1><p id="4cb8" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">我们看到了神经网络如何计算它们的输出，以及这种方法为什么有效。简而言之，神经网络能够学习复杂关系的主要原因是因为在每一层，我们都会引入非线性，并添加不同比例的输出曲线，以获得所需的结果，并且该结果还会经过激活函数，重复相同的过程以进一步定制结果。网络中的所有权重和偏差都很重要，它们可以通过某些方式进行调整以逼近关系。尽管分配给每个神经元的权重最初是随机的，但它们将通过一种叫做<strong class="jl hj">反向传播</strong>的特殊算法来学习，我们将在下一篇博客中看到。</p></div></div>    
</body>
</html>