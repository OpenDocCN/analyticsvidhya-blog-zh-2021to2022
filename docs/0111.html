<html>
<head>
<title>Understanding Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-gradient-descent-106ad1142ff4?source=collection_archive---------32-----------------------#2021-01-04">https://medium.com/analytics-vidhya/understanding-gradient-descent-106ad1142ff4?source=collection_archive---------32-----------------------#2021-01-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/4f040764294f3715bc2870fa12cd1315.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qLD9GxG0hl3-VyKw.png"/></div></div></figure><blockquote class="ip iq ir"><p id="fc96" class="is it iu iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">如果你在早上8点左右被丢在喜马拉雅山的某个地方，我们如何在日落前到达安全的地方？—梯度下降是你思维过程背后的实际解决方案。是的，我们需要根据最初的假设找到前进的方向，这样我们就能按时或提前到达安全的地方。</p></blockquote></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><p id="ad60" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd jy jf jg jh jz jj jk jl ka jn jo jp jq ha bi translated">在我以前的文章中，我已经解释了关于<a class="ae kb" rel="noopener" href="/analytics-vidhya/parameterized-learning-91f701f9cabb">参数化学习</a>和基本的<a class="ae kb" rel="noopener" href="/analytics-vidhya/loss-functions-multiclass-svm-loss-and-cross-entropy-loss-9190c68f13e0">损失函数</a>技术。如果你没有读过，请读一读，以便更好地理解梯度下降。</p><p id="2f20" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd jy jf jg jh jz jj jk jl ka jn jo jp jq ha bi translated">在基于深度学习的问题中，优化算法通过根据训练过程中发生的损失来更新随机初始化的参数，在实现更高精度方面起着重要作用。我们如何将这与喜马拉雅山的例子联系起来？</p><p id="c3b2" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd jy jf jg jh jz jj jk jl ka jn jo jp jq ha bi translated">在徒步旅行到安全的地方时，人类在向下迈出一步之前可以在多个方向上思考，而当你训练一台机器时，你必须在选择向下移动的方向之前尝试所有的可能性，并根据最初的假设找到损失，并在第二次尝试中更新，以便训练机器到达安全屋。</p><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es kc"><img src="../Images/94ee14242f8567f3ad6a42624e2f9823.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*niZ8Iw5pSa1Fmrjs-Eq6_g.png"/></div></figure><p id="a751" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd jy jf jg jh jz jj jk jl ka jn jo jp jq ha bi translated">在上图中，我们可以看到两个极小点，看起来很像，实际上不是。因此，正确的参数是达到实际/真正的最小点(全局最小值)所必需的</p></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><h1 id="37cb" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak">评估梯度的步骤</strong></h1><ol class=""><li id="4357" class="lf lg hh iv b iw lh ja li jy lj jz lk ka ll jq lm ln lo lp bi translated"><strong class="iv hi">损耗</strong>:用于计算当前参数W和输入数据的损耗的函数。</li><li id="3c20" class="lf lg hh iv b iw lq ja lr jy ls jz lt ka lu jq lm ln lo lp bi translated"><strong class="iv hi">数据</strong>:我们的训练数据，其中每个训练样本由一个图像表示</li><li id="e1ca" class="lf lg hh iv b iw lq ja lr jy ls jz lt ka lu jq lm ln lo lp bi translated"><strong class="iv hi"> W: </strong>我们正在优化的实际权重矩阵。我们的目标是应用梯度下降法找到一个产生最小损失的W。</li></ol><blockquote class="ip iq ir"><p id="b51f" class="is it iu iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">梯度下降算法将有助于使用以下公式计算所有维度上的梯度(方向)<strong class="iv hi"> W </strong></p></blockquote><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es lv"><img src="../Images/5d4a7e37869b244744c36c9afa32d27d.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*VTlQGTwTIH-wEGjfXYlLbA.png"/></div></figure></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><h1 id="41f4" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak">实际实施</strong></h1><pre class="kd ke kf kg fd lw lx ly lz aw ma bi"><span id="af2a" class="mb ki hh lx b fi mc md l me mf"># import the necessary packages<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import classification_report<br/>from sklearn.datasets import make_blobs<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import argparse</span><span id="106d" class="mb ki hh lx b fi mg md l me mf">def sigmoid_activation(x):<br/>  # compute the sigmoid activation value for a given input<br/>  return 1.0 / (1 + np.exp(-x))</span><span id="a2cd" class="mb ki hh lx b fi mg md l me mf">def sigmoid_deriv(x):<br/>  # compute the derivative of the sigmoid function ASSUMING<br/>  # that the input `x` has already been passed through the sigmoid<br/>  # activation function<br/>  return x * (1 — x)</span><span id="68cf" class="mb ki hh lx b fi mg md l me mf">def predict(X, W):<br/>  # take the dot product between our features and weight matrix<br/>  preds = sigmoid_activation(X.dot(W))</span><span id="81ed" class="mb ki hh lx b fi mg md l me mf">  # apply a step function to threshold the outputs to binary<br/>  # class labels<br/>  preds[preds &lt;= 0.5] = 0<br/>  preds[preds &gt; 0] = 1</span><span id="02c6" class="mb ki hh lx b fi mg md l me mf">  # return the predictions<br/>  return preds</span><span id="ccf1" class="mb ki hh lx b fi mg md l me mf"># construct the argument parse and parse the arguments<br/>ap = argparse.ArgumentParser()<br/>ap.add_argument(“-e”, “ — epochs”, type=float, default=100,<br/> help=”# of epochs”)<br/>ap.add_argument(“-a”, “ — alpha”, type=float, default=0.01,<br/> help=”learning rate”)<br/>args = vars(ap.parse_args())</span><span id="194d" class="mb ki hh lx b fi mg md l me mf"># generate a 2-class classification problem with 1,000 data points,<br/># where each data point is a 2D feature vector<br/>(X, y) = make_blobs(n_samples=1000, n_features=2, centers=2,<br/> cluster_std=1.5, random_state=1)<br/>y = y.reshape((y.shape[0], 1))</span><span id="db81" class="mb ki hh lx b fi mg md l me mf"># insert a column of 1’s as the last entry in the feature<br/># matrix — this little trick allows us to treat the bias<br/># as a trainable parameter within the weight matrix<br/>X = np.c_[X, np.ones((X.shape[0]))]</span><span id="1d97" class="mb ki hh lx b fi mg md l me mf"># partition the data into training and testing splits using 50% of<br/># the data for training and the remaining 50% for testing<br/>(trainX, testX, trainY, testY) = train_test_split(X, y,<br/> test_size=0.5, random_state=42)</span><span id="b53b" class="mb ki hh lx b fi mg md l me mf"># initialize our weight matrix and list of losses<br/>print(“[INFO] training…”)<br/>W = np.random.randn(X.shape[1], 1)<br/>losses = []</span><span id="948c" class="mb ki hh lx b fi mg md l me mf"># loop over the desired number of epochs<br/>for epoch in np.arange(0, args[“epochs”]):<br/>  # take the dot product between our features `X` and the weight<br/>  # matrix `W`, then pass this value through our sigmoid activation<br/>  # function, thereby giving us our predictions on the dataset<br/>  preds = sigmoid_activation(trainX.dot(W))</span><span id="898c" class="mb ki hh lx b fi mg md l me mf">  # now that we have our predictions, we need to determine the<br/>  # `error`, which is the difference between our predictions and<br/>  # the true values<br/>  error = preds — trainY<br/>  loss = np.sum(error ** 2)<br/>  losses.append(loss)</span><span id="af82" class="mb ki hh lx b fi mg md l me mf">  # the gradient descent update is the dot product between our<br/>  # (1) features and (2) the error of the sigmoid derivative of<br/>  # our predictions<br/>  d = error * sigmoid_deriv(preds)<br/>  gradient = trainX.T.dot(d)</span><span id="a2f8" class="mb ki hh lx b fi mg md l me mf">  # in the update stage, all we need to do is “nudge” the weight<br/>  # matrix in the negative direction of the gradient (hence the<br/>  # term “gradient descent” by taking a small step towards a set<br/>  # of “more optimal” parameters<br/>  W += -args[“alpha”] * gradient</span><span id="53d5" class="mb ki hh lx b fi mg md l me mf">  # check to see if an update should be displayed<br/>  if epoch == 0 or (epoch + 1) % 5 == 0:<br/>    print(“[INFO] epoch={}, loss={:.7f}”.format(int(epoch + 1),<br/>    loss))</span><span id="7447" class="mb ki hh lx b fi mg md l me mf"># evaluate our model<br/>print(“[INFO] evaluating…”)<br/>preds = predict(testX, W)<br/>print(classification_report(testY, preds))</span><span id="2025" class="mb ki hh lx b fi mg md l me mf"># plot the (testing) classification data<br/>plt.style.use(“ggplot”)<br/>plt.figure()<br/>plt.title(“Data”)<br/>plt.scatter(testX[:, 0], testX[:, 1], marker=”o”, c=testY[:, 0], s=30)</span><span id="0c63" class="mb ki hh lx b fi mg md l me mf"># construct a figure that plots the loss over time<br/>plt.style.use(“ggplot”)<br/>plt.figure()<br/>plt.plot(np.arange(0, args[“epochs”]), losses)<br/>plt.title(“Training Loss”)<br/>plt.xlabel(“Epoch #”)<br/>plt.ylabel(“Loss”)<br/>plt.show()</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mh"><img src="../Images/ebf752c7c247896ba8e01b9b980926a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dqxI8R0tnphjTTLOm1vixQ.png"/></div></div></figure></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><p id="d33d" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd jy jf jg jh jz jj jk jl ka jn jo jp jq ha bi translated"><strong class="iv hi">参考文献:</strong></p><p id="4893" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd jy jf jg jh jz jj jk jl ka jn jo jp jq ha bi translated">Adrian Rosebrock用Python实现计算机视觉的深度学习(入门包)</p></div></div>    
</body>
</html>