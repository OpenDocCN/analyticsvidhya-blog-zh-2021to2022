<html>
<head>
<title>Introduction to BERT and its application in Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">BERT简介及其在情感分析中的应用</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-bert-and-its-application-in-sentiment-analysis-9c593e955560?source=collection_archive---------2-----------------------#2021-11-30">https://medium.com/analytics-vidhya/introduction-to-bert-and-its-application-in-sentiment-analysis-9c593e955560?source=collection_archive---------2-----------------------#2021-11-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b343316c223c64f92bac3b84ce2a7ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MIbJXlkGr5-Xvc7T"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">布雷特·乔丹在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="bb34" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BERT是超级令人兴奋的算法，不仅对我，对整个NLP(自然语言处理)社区都是如此。</p><p id="16cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">超级厉害。超级有意思。而且真的很高兴和大家分享，怎么用，怎么用。</p><p id="d18b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在去BERT之前，我们先从更一般的角度来看一下NLP。NLP(自然语言处理)是人工智能处理人类语言的一部分。实际上，它几乎无处不在。例如，Web或您的搜索引擎使用NLP来优化结果。在最近的一份报告中，语音助手如Siri或Alexa使用NLP技术来理解我们所说的话。NLP也用于电子邮件箱的垃圾邮件检测。NLP用于广泛使用的翻译器中。聊天机器人是使用NLP构建的。</p><p id="e183" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以这个领域做了很多研究，BERT是由<strong class="ix hj"> Google </strong>发布的NLP高级算法。</p><h2 id="3286" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">以下是关于伯特的几点。</h2><ol class=""><li id="ee9e" class="ko kp hi ix b iy kq jc kr jg ks jk kt jo ku js kv kw kx ky bi translated">谷歌2018年底发布的NLP算法</li><li id="3cc5" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">过去5年来，NLP带来了最大的改变</li><li id="5dbe" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">更好地理解上下文中的单词和句子</li><li id="7b1a" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">例如，已经在谷歌搜索引擎中实现</li></ol><p id="3375" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BERT是一个应该理解语言并提供我们称之为语言建模的工具。谷歌已经开始在搜索引擎中使用它。下图显示了BERT实施前后的示例。</p><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/bfbfa74fe16e679af9b234d938875755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rex5qm2kn7YBx38ues6dTA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来自<a class="ae iu" href="https://cloud.google.com/ai-platform/training/docs/algorithms/bert-start" rel="noopener ugc nofollow" target="_blank">https://cloud . Google . com/ai-platform/training/docs/algorithms/Bert-start</a></figcaption></figure><p id="a542" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lj">参考上图，</em>使用BERT模型，我们可以更好地理解“为某人”是该查询的重要部分，而之前我们忽略了它的含义，以及关于配药的一般结果。</p><h2 id="4a03" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak">伯特是什么？</strong></h2><p id="aaad" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">BERT代表来自变压器的双向编码器表示。我将把这张完整的表格分成三部分。</p><p id="53d8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">编码器表示:</strong> BERT是一个<em class="lj">语言建模系统</em>，经过预先训练，拥有庞大的语料库和巨大的处理能力。我们只需要使用这个预先训练好的模型，并根据我们的需要进行微调。</p><p id="0622" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lj">什么是语言建模？</em>这意味着BERT为单词和序列提供了最好、最有效和最灵活的表示。例如，我们给BERT一个句子或两个句子，它将生成一个单独的向量或向量列表，可以根据我们的需要使用。</p><p id="0365" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">来自Transformer: </strong> Transformer是BERT架构的基本构建模块。因此，谷歌开发了transformer，它旨在处理许多序列，以将任务排序，从而构建一个翻译器或聊天机器人。然后，他们使用相同的变压器的一小部分，并使用它更聪明的方式来创建伯特。</p><p id="1ee7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">双向:</strong>在NLP项目的大部分时间里，我们想要预测句子中的下一个单词。为了预测下一个单词，即句子的右边部分，我们需要访问句子的左边部分。甚至有时我们只能接触到句子的右边部分，我们需要预测句子的左边部分。一个特殊的情况是，当某个模型分别用句子的左右两部分训练，然后将两部分连接起来。所以它变成伪双向的。BERT在处理单词时使用左右语境，实现了完全双向的模型。这意味着它可以访问整个上下文或句子来预测单词。这使伯特更有力量。</p><p id="b91d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">了解了上述概念后，让我们跳到BERT可以应用的地方。</p><h2 id="31c1" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak">伯特的应用:</strong></h2><ol class=""><li id="6d84" class="ko kp hi ix b iy kq jc kr jg ks jk kt jo ku js kv kw kx ky bi translated">使用记号赋予器处理数据</li><li id="7f5b" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">使用BERT作为嵌入层</li><li id="bc91" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">微调伯特，你的模型的核心</li></ol><p id="d2ef" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇博客中，我们将学习<strong class="ix hj"> BERT的数据处理标记器(情感分析器)。</strong></p></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h1 id="0e98" class="lu ju hi bd jv lv lw lx jz ly lz ma kd mb mc md kg me mf mg kj mh mi mj km mk bi translated">情感分析器:</h1><p id="c2da" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">在这个项目中，我们将尝试使用带有BERT的tokenizer来改进我们的个人模型(<em class="lj">在这个例子中，CNN用于分类</em>)。</p><p id="de62" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，我们将首先建立一个分类模型来评估一条推文在感觉上是积极的还是消极的..</p><h2 id="1b13" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak">训练和测试数据:</strong></h2><p id="3f03" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">你可以从下面的链接下载数据。<a class="ae iu" href="http://help.sentiment140.com/for-students" rel="noopener ugc nofollow" target="_blank">http://help.sentiment140.com/for-students</a></p><h2 id="c05e" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak">完整的项目编码:</strong></h2><p id="e4d4" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">我使用Google Colab作为这个项目的编辑器，因为它是一个托管的Jupyter笔记本服务，不需要设置就可以使用，同时提供对计算资源(包括GPU)的免费访问。我已经把完整的代码上传到了我的Github库。</p><p id="d5ea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://github.com/Tariqueakhtar/Machine-Learning/tree/master/Sentiment%20Analysis%20BERT" rel="noopener ugc nofollow" target="_blank">https://github . com/Tariqueakhtar/Machine-Learning/tree/master/perspective % 20 analysis % 20 Bert</a></p><h2 id="66f2" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak">安装并导入与BERT相关的包:</strong></h2><p id="1b78" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">除了像Pandas，numpy，BeautifulSoup这样的基本库之外，我们还需要安装两个与BERT相关的包，即bert-for-tf2和sentencepiece，如下所示。</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="8c48" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们还必须使用像tensorflow、bert和tensorflo_hub( <em class="lj">这样的库，它是一个经过训练的机器学习模型的仓库，可以随时进行微调，可以部署在任何地方。只需几行代码就可以重用经过训练的模型，如BERT和更快的R-CNN。</em>)</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><h2 id="6038" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">在colab上加载数据:</h2><p id="58b5" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">一旦我们使用Pandas加载了训练数据，让我们来看看数据样本。</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/157998d5d0f11f6b653dd2fcb29cb6bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oPU2okGdsrnT8JaRAa6xnA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="1add" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是数据集中的列。</p><ol class=""><li id="ebf4" class="ko kp hi ix b iy iz jc jd jg mo jk mp jo mq js kv kw kx ky bi translated">情感(二元目标变量)</li><li id="750d" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">身份证明（identification）</li><li id="c28a" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">日期</li><li id="4bb4" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">询问</li><li id="ceb7" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">用户</li><li id="af01" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">用户的文本或推文</li></ol><p id="7963" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个项目我们只需要两栏，即<strong class="ix hj">文本和情绪</strong>，因为我们只需要通过分析用户的推文来预测积极或消极的感觉。</p><h2 id="ba44" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">数据清理:</h2><p id="4c25" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">在colab上导入训练数据后，我们需要删除不必要的列，如id、date、query和user。下面是相同的代码。</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="aae2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据帧如下所示。</p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/81da396c72ff61d6ebccad653ec1a917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*jyv7GSzfJJJIL2e3nmYoPA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="b5d4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们清理文本，因为它有特殊字符。下面是清理的代码。</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="785e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是清洗后的结果。</p><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/def97ef6a45ab9cd9037dc7db83b33fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vFLs-i4IuaLgjNZ9hL8enA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><h2 id="4fe3" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">使用BERT进行标记化:</h2><p id="a475" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">在项目的这一部分，我们将使用BERT工具进行标记化，如下所示。</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="6165" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">那么我们在上面的代码中做了什么。让我们从每一行代码开始一步步理解。</p><ol class=""><li id="66ba" class="ko kp hi ix b iy iz jc jd jg mo jk mp jo mq js kv kw kx ky bi translated">从bert创建fullTokenizer。</li><li id="70b2" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">通过调用tensorflow_hub创建一个bert层，并将预构建模型的路径作为url在kers layer中传递。在这种情况下，我们将直接使用这个模型，不会微调它，这就是为什么我们传递参数<em class="lj">可训练=假。</em>这一行代码只是给分词器提供信息。</li><li id="373c" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">现在我们使用来自tokenizer的vocab文件。</li><li id="c246" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">为tokenizer做小写。</li><li id="6437" class="ko kp hi ix b iy kz jc la jg lb jk lc jo ld js kv kw kx ky bi translated">有了以上所有的信息，我们就能够创建记号赋予器了</li></ol><h2 id="6b84" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">将推文/文本传递给Tokenizer:</h2><p id="9c2b" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">现在使用cleaned_data中的所有tweet/text，将它们传递给tokenizer，并将每个令牌转换为id。下面是相同的代码。</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="9d76" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上述代码的示例输出如下图所示。</p><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/6b52b7bf5098fadac7a3671e79c7d6f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*_Vlhc5xgbA4vYS71sj8zKg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><h2 id="f753" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak">数据集创建:</strong></h2><p id="2310" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">我们将创建填充批次(因此我们为每个批次独立填充句子)，这样我们尽可能添加最少的填充标记。为此，我们按长度对句子进行排序，应用填充批处理，然后进行洗牌。</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者的源代码</figcaption></figure><h2 id="8b74" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><em class="mu">训练和测试数据集:</em></h2><p id="37a2" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">为了创建训练和测试数据集，我们将采用BATCH_SIZE = 32，并在整个数据集上使用该批处理大小。然后，我们将混洗数据，并获得第十批测试数据，其余的是训练数据。</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><h2 id="e7dd" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">模型构建和培训:</h2><p id="ceb9" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">因此，我们已经完成了所有的数据处理阶段，现在我们准备开始建立我们的模型。这将是我在这篇文章开头谈到的CNN。这里的想法是有三个不同的卷积滤波器，大小分别为2、3和4，我们只需取最大值，连接所有东西并使用它，然后尝试完成分类。</p><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mv"><img src="../Images/83a9dbffd7039589ca56b65920201367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K6Ka0BxKE6AmocB27Sda1w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="1133" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在建立了模型的结构之后，我们需要精确地传递参数来开始训练。低于本例中传递的参数。</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><h2 id="9ca6" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">将CNN模型拟合到训练数据；</h2><p id="f173" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">我们运行了5个时期来拟合模型，并且我们得到了如下的训练准确度分数。</p><p id="b830" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">历元1/5—损失:0.4289 —精度:0.8025</p><p id="d421" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">历元2/5 —损失:0.4289 —精度:0.8025</p><p id="a04e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">历元3/5-损失:0.3412-精度:0.8517</p><p id="31fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">历元4/5 —损失:0.3010 —精度:0.8715</p><p id="e4f2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">历元5/5-损失:0.2638-精度:0.8885</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/69beba50c81f33aff7fa77a0e8269365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-IOBWz9Fg8dYTOwtI1gOcQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><h2 id="ccc3" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">在测试数据集上评估模型:</h2><p id="2c9c" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">一旦CNN模型被训练，我们需要评估它在测试数据上的表现，下面是损失和准确性。</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="3ecc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">损耗:0.4114 —精度:0.8322</p><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/1dcc8631b5c7b620a8c244b9527fc02e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wf0XQWK-xS3mdyfHTGZSXA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="85f3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们用几个句子来评价这个模型。为此，我编写了一个单独的函数。下面是功能。</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者的源代码</figcaption></figure><p id="9314" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们只需要将句子传递到这个函数中，就可以得到它的正面或负面情绪。</p><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es my"><img src="../Images/c8265303b2b56d34a379449e2b46e18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*qdkBm_dldBQcX5jbFyRFJA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><figure class="lf lg lh li fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/0803feeea21eb26b1ae81a3b43c331f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*w4RtgtMOkLlj2y82vBIZSQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><h2 id="5fc9" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">结论:</h2><p id="2e21" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg lk ji jj jk ll jm jn jo lm jq jr js hb bi translated">看起来训练和测试精度的差异大约是5%,这意味着我们仍然需要对模型进行微调，这是我留给你的任务。让我知道你在调整这个模型方面的想法。</p><p id="3685" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢阅读..</p><h2 id="39a2" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">参考:</h2><div class="na nb ez fb nc nd"><a href="https://www.tensorflow.org/hub" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab dw"><div class="nf ab ng cl cj nh"><h2 class="bd hj fi z dy ni ea eb nj ed ef hh bi translated">张量流集线器</h2><div class="nk l"><h3 class="bd b fi z dy ni ea eb nj ed ef dx translated">TensorFlow Hub是一个经过训练的机器学习模型库，可随时进行微调，并可部署在任何地方。重复使用…</h3></div><div class="nl l"><p class="bd b fp z dy ni ea eb nj ed ef dx translated">www.tensorflow.org</p></div></div><div class="nm l"><div class="nn l no np nq nm nr io nd"/></div></div></a></div><div class="na nb ez fb nc nd"><a href="https://cloud.google.com/ai-platform/training/docs/algorithms/bert-start" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab dw"><div class="nf ab ng cl cj nh"><h2 class="bd hj fi z dy ni ea eb nj ed ef hh bi translated">内置BERT算法入门</h2><div class="nk l"><h3 class="bd b fi z dy ni ea eb nj ed ef dx translated">本教程向您展示了如何在人工智能上训练来自变压器(BERT)模型的双向编码器表示</h3></div><div class="nl l"><p class="bd b fp z dy ni ea eb nj ed ef dx translated">cloud.google.com</p></div></div><div class="nm l"><div class="ns l no np nq nm nr io nd"/></div></div></a></div><div class="na nb ez fb nc nd"><a href="http://help.sentiment140.com/for-students" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab dw"><div class="nf ab ng cl cj nh"><h2 class="bd hj fi z dy ni ea eb nj ed ef hh bi translated">对于学者——sensition 140——一个Twitter情绪分析工具</h2><div class="nk l"><h3 class="bd b fi z dy ni ea eb nj ed ef dx translated">Sentiment140不是开源的，但是有一些开源代码的资源有类似的实现:数据…</h3></div><div class="nl l"><p class="bd b fp z dy ni ea eb nj ed ef dx translated">help.sentiment140.com</p></div></div><div class="nm l"><div class="nt l no np nq nm nr io nd"/></div></div></a></div></div></div>    
</body>
</html>