<html>
<head>
<title>Building a Movie Recommendation Using FAISS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用FAISS构建电影推荐</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-a-movie-recommendation-using-faiss-60d65b104dda?source=collection_archive---------9-----------------------#2021-01-27">https://medium.com/analytics-vidhya/building-a-movie-recommendation-using-faiss-60d65b104dda?source=collection_archive---------9-----------------------#2021-01-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/adbdf7a61567815d31e1f8c4dd6beadd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jnIsSvMADouwHAx1xE5HpA.jpeg"/></div></div></figure><p id="14e0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">推荐系统是亚马逊、Flipkart、网飞、脸书等公司的关键组成部分。推荐系统的目标是推荐项目，以根据用户先前与系统的交互来预测用户喜欢某个项目的可能性。两种最常见的推荐系统是<strong class="ir hi">基于内容的</strong>和<strong class="ir hi">协同过滤</strong>。</p><p id="9e42" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">基于内容的推荐系统</strong>关注物品的属性，推荐与之相似的物品。</p><p id="8a88" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">协同过滤</strong>关注用户对物品的态度来推荐物品。它利用人群的知识来预测物品。</p><h1 id="8f64" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">问题陈述</h1><p id="d480" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">IMDB movies是一个巨大的数据宝库，包含有关电影的信息，如导演、电影名称、上映时间、电影摘要、电影评级等。我们将使用该数据集来识别相似的电影，给定电影概要或描述，该数据集可以在<a class="ae kq" href="https://www.kaggle.com/stefanoleone992/imdb-extensive-dataset" rel="noopener ugc nofollow" target="_blank">这里</a>找到。这篇文章的代码可以在这里找到Kaggle笔记本<a class="ae kq" href="https://www.kaggle.com/aiswaryaramachandran/building-movie-recommendation-system-using-faiss/edit/run/52369178" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="26c4" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">FAISS简介</h1><p id="4fb8" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated"><a class="ae kq" href="https://github.com/facebookresearch/faiss" rel="noopener ugc nofollow" target="_blank"> FAISS </a>是一个来自脸书的开源库，用于高效的相似性搜索和密集向量聚类。这允许我们搜索任何大小的向量，甚至是那些不适合RAM的向量。FAISS有GPU支持。</p><p id="864b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">安装FAISS </strong></p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="cebd" class="la jo hh kw b fi lb lc l ld le">pip install faiss-cpu ## For CPU Version </span><span id="53f8" class="la jo hh kw b fi lf lc l ld le">pip install faiss-gpu ## For the GPU Version</span></pre><p id="c78e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">FAISS包含多种类型的索引，允许进行相似性搜索，并假设数据表示为具有唯一相关整数id的密集向量，允许使用L2(欧几里德距离)或内积进行距离计算。<strong class="ir hi">通常，对于大多数相似性问题，我们使用余弦相似性作为常用的度量。为了在FAISS中实现这一点，我们必须归一化向量，因为在这种情况下，余弦相似性与内积相同</strong></p><p id="3a05" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">大多数方法，如基于二进制向量和紧凑量化码的方法，仅使用向量的压缩表示，而不需要保留原始向量。这通常以不太精确的搜索为代价，但是这些方法可以扩展到单个服务器的主存储器中的数十亿个向量。</p><h1 id="2576" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">体系结构</h1><p id="f9ef" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">构建搜索系统包括两个步骤:<strong class="ir hi">存储数据和检索相关结果</strong></p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lg"><img src="../Images/fa6a3d32f02659dd800b465dae22a3f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Tb_d0y569ZgvtRk2.png"/></div></div></figure><p id="45d3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在存储数据时，我们需要提取电影描述的嵌入内容，并将其索引到FAISS中。每个描述都与一个唯一的标识符相关联，这样我们就可以在搜索阶段将结果映射回电影数据。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lg"><img src="../Images/c4bbe11841e262485d68773087660e13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dB-IDivfDODLqKAt.png"/></div></div></figure><h1 id="b181" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">加载和理解数据</h1><p id="87cc" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">IMDB数据集有多个文件——电影数据集、评级数据集、包含演员详细信息的姓名数据集和包含角色重要性信息的片头主体数据集。出于我们的目的，我们将使用IMDb电影数据集(movies.csv ),它包含85，855部电影的详细信息，以及与电影描述、平均评级、电影名称、类型等相关的信息。</p><p id="423a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因为我们关注于使用电影描述来构建搜索，所以我们需要删除描述为空的行</p><p id="2fcd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">此外，我们需要每一行的唯一标识符——这可以是该行的索引。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="6c3c" class="la jo hh kw b fi lb lc l ld le">import numpy as np <br/>import pandas as pd imdb_movies=pd.read_csv("/kaggle/input/imdb-extensive-dataset/IMDb movies.csv") </span><span id="2909" class="la jo hh kw b fi lf lc l ld le">## Dropping of rows where description is NULL <br/>imdb_movies=imdb_movies[pd.notnull(imdb_movies['description'])] </span><span id="7381" class="la jo hh kw b fi lf lc l ld le"><br/>imdb_movies=imdb_movies.reset_index(drop=True) </span><span id="ca94" class="la jo hh kw b fi lf lc l ld le">## Create an unique identifier for each row </span><span id="96b7" class="la jo hh kw b fi lf lc l ld le">imdb_movies['id']=imdb_movies.index</span></pre><h1 id="283a" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">创建嵌入</h1><p id="4564" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">伯特、罗伯塔、XLM等基于Transformer的模型使得迁移学习应用于NLP(ImageNet Moment)成为可能。这些模型允许我们根据您的问题对模型进行微调，从而提供优于传统方法的性能。为了给每个电影描述创建嵌入(密集向量),我们可以得到每个单词的嵌入，然后进行平均。</p><p id="b75a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">相反，我们将使用python中的句子转换器库，其中不同的基于转换器的预训练模型专门针对有意义的句子嵌入进行调整，以便具有相似含义的句子在向量空间中接近。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="4d20" class="la jo hh kw b fi lb lc l ld le">pip install sentence_transformers ## Installs the library</span><span id="d441" class="la jo hh kw b fi lf lc l ld le">##Extracting list of description sentences=imdb_movies['description'].tolist()</span><span id="9b2c" class="la jo hh kw b fi lf lc l ld le">from sentence_transformers import SentenceTransformer, util </span><span id="c1a6" class="la jo hh kw b fi lf lc l ld le">##Loading the pretrained distil Roberta model </span><span id="d995" class="la jo hh kw b fi lf lc l ld le">model = SentenceTransformer('paraphrase-distilroberta-base-v1') <br/>## This model gives us 768 Dimension Vector </span><span id="c8d7" class="la jo hh kw b fi lf lc l ld le">embeddings=model.encode(sentences) ## Extract the sentenceembeddings</span></pre><h1 id="d2cd" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">索引数据</h1><p id="b625" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">FAISS在索引上有许多不同的类型——平面索引允许我们直接索引向量而不进行任何压缩。使用量化技术的索引允许我们压缩向量——为了压缩和实现有效的搜索，使用k-means将向量分成簇。这里我们将使用带有产品量化的倒排索引来存储我们的嵌入。这种类型的索引允许我们基于最近邻搜索的概念来执行近似搜索。</p><p id="6150" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">创建IndexIVFPQ类型的FAISS索引包括两个步骤</p><ol class=""><li id="0ace" class="lh li hh ir b is it iw ix ja lj je lk ji ll jm lm ln lo lp bi translated"><strong class="ir hi">创建一个k形心的训练索引</strong>。这一步允许我们创建向量空间的集群。对于这一步，整个数据需要在内存中</li><li id="5f46" class="lh li hh ir b is lq iw lr ja ls je lt ji lu jm lm ln lo lp bi translated"><strong class="ir hi">添加或更新索引</strong> —在这里加载训练好的索引(它存储关于不同聚类中心的信息)并添加新数据。</li></ol><p id="bcf7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">创建K形质心</strong></p><p id="b178" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了提供压缩，在该步骤中，N维向量(N=768)在聚类之前被分成m个块。因此，我们最终对每个块进行K个聚类。此外，在创建质心之前，必须对嵌入进行规范化。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="71df" class="la jo hh kw b fi lb lc l ld le">dim=768 ## Embedding Dimension </span><span id="6e1c" class="la jo hh kw b fi lf lc l ld le">ncentroids=50 ## This is a hyperparameter, and indicates number of clusters to be split into</span><span id="cb45" class="la jo hh kw b fi lf lc l ld le">m=16 ## This is also a hyper parameter indicating number chunks the embeddings must be divided into </span><span id="e881" class="la jo hh kw b fi lf lc l ld le">quantiser = faiss.IndexFlatL2(dim) </span><span id="35f5" class="la jo hh kw b fi lf lc l ld le">index = faiss.IndexIVFPQ (quantiser, dim,ncentroids, m , 8) <br/>## Before we train the embeddings, we will normalise the Embeddings </span><span id="3afc" class="la jo hh kw b fi lf lc l ld le">faiss.normalize_L2(embeddings) </span><span id="a1af" class="la jo hh kw b fi lf lc l ld le">index.train(embeddings) ## This step, will do the clustering and create the clusters for each chunk </span><span id="1fac" class="la jo hh kw b fi lf lc l ld le">print(index.is_trained) ## Will print true if the index has been trained (The cluster centroids have been identified) </span><span id="f6d8" class="la jo hh kw b fi lf lc l ld le">faiss.write_index(index, "trained.index") ## The trained.index contains the details about the cluster centroids.</span></pre><p id="2ba0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">添加数据</strong></p><p id="51bb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">FAISS index允许我们使用<strong class="ir hi"> add_with_ids </strong>函数索引嵌入和唯一标识符</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="6a14" class="la jo hh kw b fi lb lc l ld le">ids=imdb_movies['id'].tolist() </span><span id="3acc" class="la jo hh kw b fi lf lc l ld le">ids=np.array(ids)</span><span id="5369" class="la jo hh kw b fi lf lc l ld le">index.add_with_ids(embeddings,ids) </span><span id="7ab4" class="la jo hh kw b fi lf lc l ld le">print("Total Number of Embeddings in the index" ,index.ntotal)</span><span id="24e3" class="la jo hh kw b fi lf lc l ld le">faiss.write_index(index,"movies_desc.index")</span></pre><h1 id="86aa" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">检索相关结果</h1><p id="d940" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">为了找到相似的项目，余弦相似性是最常用的度量。当我们归一化一个向量时，内积和余弦相似度是一样的。我们可以根据L2距离计算内积。关系如下:</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es lv"><img src="../Images/c8b13857e425850163333dbc4696d639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*FX5AE0Hk9DtfNV1E0weqMw.png"/></div></figure><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="f562" class="la jo hh kw b fi lb lc l ld le">## This function converts the L2_score into inner product def </span><span id="63b3" class="la jo hh kw b fi lf lc l ld le">calculateInnerProduct(L2_score): <br/>   return (2-math.pow(L2_score,2))/2</span></pre><p id="9306" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了搜索索引，我们需要首先使用句子转换器将查询嵌入到向量中。为了减少搜索所需的时间，我们可以通过指定“<strong class="ir hi"> nprobe </strong>参数来识别与查询向量最近的“k”个聚类。nprobe的值小于创建索引时指定的簇数。</p></div><div class="ab cl lw lx go ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ha hb hc hd he"><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="04a6" class="la jo hh kw b fi lb lc l ld le">def searchFAISSIndex(data,id_col_name,query,index,nprobe,model,topk=20): ## Convert the query into embeddings query_embedding=model.encode([query])[0] </span><span id="8069" class="la jo hh kw b fi lf lc l ld le">dim=query_embedding.shape[0] </span><span id="ebab" class="la jo hh kw b fi lf lc l ld le">query_embedding=query_embedding.reshape(1,dim) ## Normalise the Query Embedding </span><span id="b8a3" class="la jo hh kw b fi lf lc l ld le">faiss.normalize_L2(query_embedding) </span><span id="26b1" class="la jo hh kw b fi lf lc l ld le">index.nprobe=nprobe D,I=index.search(query_embedding,topk) </span><span id="ad09" class="la jo hh kw b fi lf lc l ld le">ids=[i for i in I][0] L2_score=[d for d in D][0] </span><span id="a4c3" class="la jo hh kw b fi lf lc l ld le">inner_product=[calculateInnerProduct(l2) for l2 in L2_score] </span><span id="4b33" class="la jo hh kw b fi lf lc l ld le">search_result=pd.DataFrame() </span><span id="270b" class="la jo hh kw b fi lf lc l ld le">search_result[id_col_name]=ids search_result['cosine_sim']=inner_product search_result['L2_score']=L2_score dat=data[data[id_col_name].isin(ids)] dat=pd.merge(dat,search_result,on=id_col_name) dat=dat.sort_values('cosine_sim',ascending=False) <br/>return dat</span><span id="3854" class="la jo hh kw b fi lf lc l ld le">query="A seventeen-year-old aristocrat falls in love with a kind but poor artist" search_result=searchFAISSIndex(imdb_movies,"id",query,index,nprobe=10,model=model,topk=20) search_result=search_result[['id','description','title','cosine_sim','L2_score']]</span></pre><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lg"><img src="../Images/fc41551165a5ac264e2849d1b13d4e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*BvNZxLhEZMEudAR2.png"/></div></div><figcaption class="md me et er es mf mg bd b be z dx translated">查询结果:"<code class="du mh mi mj kw b"><strong class="bd jp">A seventeen-year-old aristocrat falls in love with a kind but poor artist</strong></code>"</figcaption></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lg"><img src="../Images/c620191bf0589e0055676189bec68b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EABDvMMP5ncaFzTB.png"/></div></div><figcaption class="md me et er es mf mg bd b be z dx translated">查询结果:"<strong class="bd jp">前足球运动员训练足球队"</strong></figcaption></figure><p id="d77e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">正如我们所看到的，即使没有彻底的搜索，我们也可以使用FAISS检索到非常好的结果。此外，对于83000部电影，我们可以在几毫秒内基于语义关系检索搜索结果。这允许我们从基于关键字的搜索转向使用嵌入的能力来构建高效的搜索，即使我们没有很多RAM。</p><p id="445c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用FAISS相对于其他库(比如Annoy)的优势在于FAISS允许我们更容易地执行增量更新。FAISS的强大功能可以与弹性搜索相结合，构建强大的搜索系统。</p><p id="bc22" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">本文仅使用电影描述来构建推荐系统。可以包含其他功能来构建更强大的推荐系统。此外，人们可以在FAISS中试验其他类型的索引，以构建更有效和可伸缩的搜索。</p></div></div>    
</body>
</html>