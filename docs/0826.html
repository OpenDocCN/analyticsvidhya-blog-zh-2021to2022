<html>
<head>
<title>Convolutions and Sequence Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积和序列预测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/convolutions-and-sequence-prediction-a7700bd22125?source=collection_archive---------27-----------------------#2021-02-01">https://medium.com/analytics-vidhya/convolutions-and-sequence-prediction-a7700bd22125?source=collection_archive---------27-----------------------#2021-02-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex ie if ig ih er es paragraph-image"><div role="button" tabindex="0" class="ii ij di ik bf il"><div class="er es et"><img src="../Images/ba2bcbe2e6df618840988e145da35e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AvHRZ8-mAuk35QSKgxNA6g.jpeg"/></div></div></figure><p id="bf51" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">我们都知道RNNs擅长序列建模。递归的思想非常直观，当应用于各种任务时，如时间序列分析和NLP，也证明了这一点。我们最初没有考虑序列建模问题中的卷积，因为我们痴迷于CNN如何未能捕获时间关系，而是特征之间的空间关系，正是这种质量使得CNN多年来在图像处理方面备受青睐。但是在序列建模和卷积之间有一个很好的关系，我们将在这里用一个示例代码一行一行地解释它。我们还将看到我们如何用一个技巧来解决时间关系的问题，并研究注意力，这形成了理解变形金刚的一个非常好的基础(似乎已经发展成为每个深度学习问题的通用答案)。</p><p id="d856" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">首先，让我们理解一个基于RNN的序列到序列架构，并从那里偏离到卷积。</p><figure class="jn jo jp jq fd ih er es paragraph-image"><div class="er es jm"><img src="../Images/beadb6690891cb40e1c380794476a0a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*CWDpXtddQgUobqPO64TBdQ.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">基于RNNs的序列对序列模型</figcaption></figure><p id="58a9" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">正如我们在上面看到的，该架构有两个部分，一个是位于潜在向量“Z”左侧的编码器，另一个是位于右侧的解码器。橙色方块是嵌入层，绿色和蓝色方块是潜在向量两侧的RNN细胞(LSTM/GRU)。观察值被一个接一个地送入构成潜在向量的网络。在解码器端，潜在向量作为输入之一被馈送到RNN单元。</p><p id="04e7" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">在这种架构中，我们可以用卷积层代替RNN单元吗？是的，但这并不简单，因为CNN需要看到整个样本，而rnn看到的是一个接一个的“重现”。但是如果我们使用CNN，我们如何补偿时间信息的损失呢？</p><p id="018d" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">让我们用工作代码找出所有的答案。</p><figure class="jn jo jp jq fd ih er es paragraph-image"><div role="button" tabindex="0" class="ii ij di ik bf il"><div class="er es jv"><img src="../Images/270a23d7a3b54f71b7e2eb666e276404.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d8uQDIF9boTWgQI5wMznMw.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">基于细胞神经网络的序列对序列模型</figcaption></figure><p id="5630" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">上述架构可能有一些可怕的组成部分，但它们都有一个目的。与之前的架构不同，这里的网络同时接收所有的单词。让我们先来看看编码器。</p><figure class="jn jo jp jq fd ih er es paragraph-image"><div role="button" tabindex="0" class="ii ij di ik bf il"><div class="er es jw"><img src="../Images/54e53e0e0a2417988d49ed54fecbf9bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KdbEiPCrofMK1mcnsiGozA.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">编码器</figcaption></figure><p id="7137" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">在底部，我们看到“令牌和位置嵌入”层，它采用令牌嵌入，因为这个网络没有递归，所以位置与单词嵌入一起添加(元素相加)。这样，网络就知道单词在句子或序列中的位置(这就是我们补偿时间信息损失的方式)。卷积块前后有线性层来转换维度。在顶部，我们看到来自元素态和的虚线连接，这类似于剩余网络(ResNet)中的“高速公路连接”。在这里，剩余连接有助于保留组合输出的位置信息，我们稍后将在关注层中使用该信息。</p><p id="5e65" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">现在让我们看看卷积模块中发生了什么。</p><figure class="jn jo jp jq fd ih er es paragraph-image"><div role="button" tabindex="0" class="ii ij di ik bf il"><div class="er es jx"><img src="../Images/ecb5427bb924bf10bbd0bae2e1b55d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v2EOelQBCmNth3qMfxyF6g.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">编码器—卷积模块</figcaption></figure><p id="54a6" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">卷积由一维卷积层执行，根据问题的复杂性，我们需要多少卷积层就堆叠多少卷积层。门控线性单元(GLU)被用作卷积层之间的激活函数。GLUs的工作类似于LSTM网络中的闸门，控制着有多少信息应该通过连续的层。该网络还保持类似于剩余网络的高速公路/跳跃连接。</p><p id="0ae5" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">现在让我们编码编码器部分。</p><figure class="jn jo jp jq fd ih"><div class="bz dy l di"><div class="jy jz l"/></div></figure><p id="645c" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">我们已经成功地建立了编码器部分。现在让我们来看看解码器部分。</p><figure class="jn jo jp jq fd ih er es paragraph-image"><div role="button" tabindex="0" class="ii ij di ik bf il"><div class="er es ka"><img src="../Images/67dda18a55ac2f1e14b84d16bdb18eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OHFR0R0dKSScWfu0yIhdgg.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">解码器</figcaption></figure><p id="4ff1" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">不同之处当然是我们将发送目标，但除此之外，您会注意到没有剩余连接，因为我们处于信息流的末端，编码器输出直接作为卷积层的输入。但是与我们一次发送一个单词来预测目标的RNNs不同，我们同时发送所有的目标。如果我们没有以正确的方式填充目标，模型可能会以不希望的顺序预测目标，因此我们按照下面卷积块中提到的方式进行填充。</p><figure class="jn jo jp jq fd ih er es paragraph-image"><div role="button" tabindex="0" class="ii ij di ik bf il"><div class="er es kb"><img src="../Images/da41d29b17b2d74ca0250bafac06d8d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hKxDAs9DaAwseQUuiytRYw.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">解码器卷积模块</figcaption></figure><p id="4601" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">正如我们在上面看到的，填充只在左边完成，大小为kernel_size-1。这将迫使模型在每个卷积过程中只看到目标的左侧。如我们在编码器部分看到的，如果填充在任何一侧，第一个卷积将看到“<sos>”和“2”，这在如下所示的推理中不会发生</sos></p><figure class="jn jo jp jq fd ih er es paragraph-image"><div class="er es kc"><img src="../Images/ce94bcc871f4e9e3e76f4bf85433c295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*HarpLRJo_9l8AIFe6Truag.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">填充错误</figcaption></figure><p id="224e" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">有了这个理解，现在让我们编码解码器部分。</p><figure class="jn jo jp jq fd ih"><div class="bz dy l di"><div class="jy jz l"/></div></figure><p id="03e8" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">编码器输出用于注意功能。注意力涉及的步骤如下图所示。</p><figure class="jn jo jp jq fd ih er es paragraph-image"><div role="button" tabindex="0" class="ii ij di ik bf il"><div class="er es kd"><img src="../Images/1dd526286832aa5e69baa1367ca3f18a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6akYrV7CnSMr9W-EvIxyNw.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">注意力</figcaption></figure><p id="f303" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">当用这种架构训练时，我们得到了极好的结果。</p><figure class="jn jo jp jq fd ih"><div class="bz dy l di"><div class="jy jz l"/></div></figure><p id="a0ce" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">6.1的测试分数令人印象深刻。找到下面一些翻译的注意力地图。</p><figure class="jn jo jp jq fd ih er es paragraph-image"><div role="button" tabindex="0" class="ii ij di ik bf il"><div class="er es ke"><img src="../Images/f35aa88e94669fc5cb73c18d37ce78f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UaYqf642iQ9p1ekuj2Iq_g.png"/></div></div></figure><p id="ee7d" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">完整代码见Colab笔记本<a class="ae kf" href="https://colab.research.google.com/drive/1-cfH-PzUTX9m5sAFQyFGT3i_Ji_AEnzr?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://Colab . research . Google . com/drive/1-cfH-pzutx 9m 5 safqyfgt 3 I _ Ji _ AEnzr？usp =分享</a></p></div></div>    
</body>
</html>