# 关于最大似然估计的家庭聚餐

> 原文：<https://medium.com/analytics-vidhya/a-family-dinner-over-maximum-likelihood-estimation-39d0236e985f?source=collection_archive---------10----------------------->

最大似然估计，或更通俗地称为 MLE，是一个重要的概念，但不幸的是，在很大程度上被忽视。本文旨在发展一种关于最大似然估计及其在机器学习中的应用的直觉。

**什么是 MLE？**简单来说，MLE 是一种参数估计方法。*最大似然估计是一种从观测数据中估计模型参数的方法，以最大化获得观测数据的概率。*就这样。请注意，MLE 并不试图找到模型，它只是试图找到模型的参数。这是因为，在应用 MLE 之前，期望你知道是什么过程产生了这个数据&你可以为这个过程开发一个数学模型，然后 MLE 被用来估计这个模型的参数。然而，在这一点上，让我们忘记一切，享受在教授家关于 MLE 的晚餐谈话。

奶奶:你能给我解释一下什么是 MLE 吗？我一直听到很多关于它的事情。

教授:最大似然估计是一种利用手头的可用数据对总体进行推断的简单方法。我举个例子:假设一个矿业大亨有兴趣购买一个金矿，但是为了确定他应该为这个矿支付的价格，他想知道这个矿的岩石中金矿石的百分比是多少。他派了三个人去挑选他们认为是金矿的东西。他们三个人每人收集了 5 块矿石。经实际检查(无论是岩石还是金矿石)，原来第一人的 5 镐全部是矿石，第二人的 2 镐是岩石，第三人的 5 镐全部是岩石(可怜的家伙！).然后，他们需要根据他们的观察报告地雷的质量。你认为每个人会报告什么？使用 MLE，第一个人会说这个矿里没有石头，第二个人会说矿里有 40%的石头，而第三个人会说这是个骗局，矿里只有石头。所以，你看，每个人都对人口做了假设，在某种程度上，它最好地解释了他可用的数据。

**母亲:**好的！在前面的例子中，你简单地计算了样本量，因为你不能看到整个人口，你称之为人口参数。但是我认为，关于人口的推论是通过假设检验得出的？

**教授:**如果你不为总体假设任何底层参数模型，只对一个固定的测度感兴趣，那么仅凭假设检验就足以对总体做出推断。但是，如果你认为你的总体遵循一个参数模型，并且你有兴趣知道这个模型的参数，那么你将需要 MLE &假设检验。这两种技术一起工作来对总体进行推断。最大似然法用于估计模型参数，假设检验用于确认这些估计对总体是否成立。

我们举个例子:假设你要在一个炉子里安装冷却系统，这样你就可以一直保持炉温在一定的水平。为此，你需要知道炉内温度如何随时间变化。所以，你放了一个装置，在两天内每小时记录一次温度。现在，根据行业研究，您已经知道炉温遵循高斯分布。但是要定义高斯分布，你需要两个参数。平均值μ和标准差σ。这些参数的不同值将导致不同的曲线。一种方法是，假设不同的μ和σ值，绘制不同的曲线，比较获得数据点的概率。这些概率最大的曲线是模型的最佳估计。一旦有了μ和σ的估计值，就可以使用假设检验来确认这些估计值是否可以推广。

**妻子:**假设不同的曲线，比较每条曲线的概率，这听起来不实际。必须有更科学的方法。

**教授:**是的，有。在实际操作中，会创建一个表示手头数据的数学函数，并最大化该函数来估计总体参数的值。在我们的示例中，对于高斯分布，我们将为每个数据点&编写高斯概率分布函数，然后将它们相乘(假设每个数据点都是独立的)以获得最终函数，然后求解该函数的最大值以获得 MLE 估计。

妻子:你在谈论最大化概率，然而 MLE 致力于最大化可能性。这两者之间有什么联系？

教授:实际上，甚至在数学上，可能性和概率是一样的，这就是为什么我在前面的例子中使用了概率这个词，而不是可能性。然而，对于统计学家来说，这两者之间确实存在细微的差别。看看下面的等式:

p(数据；μ，σ)表示“用模型参数μ和σ观察数据的概率”。反之，L(μ，σ；数据)是指“参数μ和σ在我们拥有的数据中取特定值的可能性。”当我们知道模型参数并使用它们来计算个体概率时，这称为概率函数；当我们已经有数据点并使用这些数据点来计算模型参数时，这称为似然函数。实际上，你可以把可能性想象成概率，如果这样更简单的话。从数学上讲，它们是一样的，只是使用了不同的符号。

妻子:好吧，但是我在很多博客上看到，梯度下降也是一种流行的参数估计技术。这两者之间有什么联系？

**教授:**梯度下降是一种优化算法。类似于 MLE &假设检验如何协同工作以对总体做出推断，MLE &优化一起工作于参数估计。他们都有不同的工作要做。MLE 是定义问题的方法，优化是解决问题的引擎。最大似然法的作用是定义需要最大化的目标函数。一旦定义了目标函数，MLE 工作就结束了。然后使用优化算法来求解该目标函数，以估计参数值。一旦这些参数值被估计出来，它们将进一步接受假设检验，以决定这些值是否可以推广到总体。

**女儿:**我能把所有的线索联系起来。基本上，MLE 用于开发一个数学函数来估计模型参数，优化技术用于求解该函数，假设检验用于确认这些估计对总体是否成立。但这都是理论，你能在一个实际的机器学习问题的背景下解释这些吗？

**教授:**我们以逻辑回归算法为例。在逻辑回归中，假设 sigmoid 函数表示对于给定的特征值获得目标类的概率。因此，sigmoid 函数用于计算训练集中每个观察值的个体概率。训练集中的所有观察值都被假定为彼此独立，因此这些概率被相乘在一起。然后将该函数设置为最大化。到目前为止，这个过程都是基于 MLE 方法。

然后是优化部分。然后取该函数的自然对数，以使偏导数计算更容易。这个函数叫做对数似然函数。对数似然的最大化问题然后乘以-1，以将其转换成最小化问题。这给了我们负的对数似然，这是逻辑回归的成本函数。然后使用梯度下降算法求解该函数。

一旦负对数似然函数被求解，我们就获得了模型中每个特征的系数的估计。这些是最大似然估计。使用 Wald 假设检验来检验这些估计值是否显著，而使用卡方来评估模型的总体拟合度。

漫长的讨论到此结束。如果有兴趣可以参考我的假设检验文章 [**这里**](https://www.linkedin.com/pulse/intuition-behind-hypothesis-testing-arvind-shukla/) 。

【https://www.linkedin.com】最初发表于[](https://www.linkedin.com/pulse/demystifying-statistics-maximum-likelihood-estimation-arvind-shukla/)**。**