<html>
<head>
<title>Encoder-Decoder Seq2Seq Models, Clearly Explained!!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">编解码Seq2Seq型号，讲解清楚！！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b?source=collection_archive---------1-----------------------#2021-03-12">https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b?source=collection_archive---------1-----------------------#2021-03-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="b699" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">详细了解编码器-解码器序列到序列模型的分步指南！</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/01185923456b745d7fe74bc6fdc44a2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BzVwaKGaKqAgVCW4MH08sQ.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图片来自<a class="ae jn" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5233295" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae jn" href="https://pixabay.com/users/coxinhafotos-3726685/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5233295" rel="noopener ugc nofollow" target="_blank">coxinafotos</a></figcaption></figure><h1 id="52f6" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">目录</h1><ol class=""><li id="cf24" class="kg kh hi ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">介绍</li><li id="7eef" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">动机</li><li id="6b4d" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">先决条件</li><li id="e72b" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">序列建模问题</li><li id="f0ed" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">编码器-解码器模型的结构</li><li id="69d1" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">培训和测试阶段:进入张量</li><li id="a8be" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">“苏茨基弗模式”</li><li id="ae2b" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">图像字幕直觉</li><li id="359d" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">进一步阅读</li><li id="5e5f" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">参考</li></ol><h1 id="c4ac" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">介绍</h1><blockquote class="ld le lf"><p id="cd81" class="lg lh li ki b kj lj ij lk kl ll im lm ln lo lp lq lr ls lt lu lv lw lx ly kt hb bi translated">传统的深度神经网络(DNNs)是强大的机器学习模型，在语音识别和视觉对象识别等困难问题上取得了优异的性能。但是它们只能用于大的标记数据，其中输入和目标可以用固定维度的向量进行合理的编码。它们不能用于序列到序列的映射(例如机器翻译)</p><p id="2b4c" class="lg lh li ki b kj lj ij lk kl ll im lm ln lo lp lq lr ls lt lu lv lw lx ly kt hb bi translated"><a class="ae jn" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">用神经网络进行序列对序列学习</a> [1]</p></blockquote><p id="86d3" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">这是提出一种可以解决一般序列间问题的架构的动机，因此编码器-解码器模型诞生了。</p><p id="8aa6" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">在本文中，我的目标是详细解释编码器-解码器序列到序列模型，并帮助您建立其工作背后的直觉。为此，我采取了一步一步的方法，试图直观地解释这些概念。我将主要参考Ilya Sutskever等人的开创性研究论文<a class="ae jn" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">Sequence to Sequence Learning with Neural Networks</a><a class="ae jn" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">，该论文实现了用于神经机器翻译的编码器-解码器模型。最后，我还将在安德烈·卡帕西、李菲菲等人的论文<em class="li"> </em> </a><a class="ae jn" href="https://arxiv.org/abs/1412.2306" rel="noopener ugc nofollow" target="_blank">用于生成图像描述的深度视觉语义对齐中给出用于图像字幕的编码器-解码器的改进版本背后的直觉</a></p><h1 id="5b4c" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">动机</h1><p id="d336" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">编码器-解码器架构相对较新，已在2016年底被用作谷歌翻译服务的核心技术。它构成了高级序列到序列模型的基础，如注意力模型、GTP模型、变形金刚和BERT。因此，在进入高级机制之前，理解它是非常重要的。</p><h1 id="9329" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">先决条件</h1><p id="bf97" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">这篇文章假设你熟悉神经网络，尤其是RNNs/lstm的工作原理。如果你不确定，那么我强烈建议你去看看克里斯多佛·奥拉的博客<a class="ae jn" href="https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9" rel="noopener" target="_blank">图文并茂的循环神经网络指南</a><em class="li"/><em class="li"/>迈克尔·皮<em class="li">和</em> <a class="ae jn" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">了解LSTM网络</a>。</p><h1 id="9173" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">序列建模问题</h1><p id="d7fc" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">序列建模问题是指输入和/或输出是数据序列(单词、字母等)的问题。)</p><p id="bf9a" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">考虑一个预测电影评论是正面还是负面的非常简单的问题。这里我们的输入是一个单词序列，输出是一个介于0和1之间的数字。如果我们使用传统的DNNs，那么我们通常必须使用BOW、Word2Vec等技术将输入文本编码成固定长度的向量。但是请注意，这里单词的顺序没有保留，因此当我们将输入向量输入到模型中时，它不知道单词的顺序，因此它丢失了关于输入的一条非常重要的信息。</p><p id="f79e" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">因此，为了解决这个问题，RNNs应运而生。本质上，对于具有可变数量特征的任何输入X = (x₀，x₁，x₂，… xₜ)，在每个时间步，RNN单元将一个项目/令牌<strong class="ki hj"> xₜ </strong>作为输入，并产生一个输出<strong class="ki hj"> hₜ </strong>，同时将一些信息传递到下一个时间步。可以根据手头的问题使用这些输出。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mc"><img src="../Images/d4b4fa61084a110c666fb16bfed307d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HLoBU9zzXITTJWK8aBvzFQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">克里斯托弗·奥拉的《了解LSTM网络》。</figcaption></figure><p id="983b" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">电影评论预测问题是一个非常基本的序列问题的例子，称为多对一预测。对于不同类型的序列问题，使用这种RNN体系结构的修改版本。</p><p id="8c9e" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">序列问题可以大致分为以下几类:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es md"><img src="../Images/1fb14cd18b1df8b0b9d08173871b0c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kje9TpsSvMGeBF6vIv6k5Q.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">递归神经网络的不合理有效性</a>作者Andrej Karpathy</figcaption></figure><blockquote class="ld le lf"><p id="ca87" class="lg lh li ki b kj lj ij lk kl ll im lm ln lo lp lq lr ls lt lu lv lw lx ly kt hb bi translated">每个矩形是一个向量，箭头代表函数(如矩阵乘法)。输入向量是红色的，输出向量是蓝色的，绿色向量代表RNN状态(稍后会有更多介绍)。从左至右:<strong class="ki hj"> (1) </strong>没有RNN的香草处理模式，从固定大小的输入到固定大小的输出(例如图像分类)。<strong class="ki hj"> (2) </strong>顺序输出(如图像字幕拍摄图像，输出一句话)。<strong class="ki hj"> (3) </strong>序列输入(例如，情感分析，其中给定句子被分类为表达积极或消极情感)。<strong class="ki hj"> (4) </strong>顺序输入和顺序输出(比如机器翻译:一个RNN读一句英文，然后输出一句法文)。<strong class="ki hj"> (5) </strong>同步序列输入和输出(例如，视频分类，其中我们希望标记视频的每一帧)。请注意，在每种情况下，长度序列都没有预先指定的约束，因为递归转换(绿色)是固定的，可以应用任意多次。</p><p id="ed2a" class="lg lh li ki b kj lj ij lk kl ll im lm ln lo lp lq lr ls lt lu lv lw lx ly kt hb bi translated">— Andrej Karpathy，<a class="ae jn" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">递归神经网络的不合理有效性</a> [5]</p></blockquote><h2 id="1173" class="me jp hi bd jq mf mg mh ju mi mj mk jy kn ml mm ka kp mn mo kc kr mp mq ke mr bi translated">序列间问题</h2><p id="207c" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">序列对序列(Seq2Seq)问题是一类特殊的序列建模问题，其中输入和输出都是序列。编码器-解码器模型最初是为了解决这样的Seq2Seq问题而构建的。在这篇文章中，我将使用神经机器翻译(NMT)的多对多类型问题作为运行示例。稍后，我们还将看到它们如何用于图像字幕(一对多示例)。</p><h1 id="5ef0" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">编码器-解码器模型的结构</h1><p id="a4cb" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">我的主要目标是帮助您理解Ilya Sutskever等人的论文中使用的架构。这是第一篇介绍机器翻译的编码器-解码器模型和更一般的序列到序列模型的论文之一。该模型被应用于英语到法语的翻译。</p><p id="cb7d" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><strong class="ki hj"> <em class="li">注:为了得到最终的模型，我将试图简化事物并逐一介绍概念，希望人们在没有太多深入的主题知识的情况下更容易理解，并在需要时填补空白。</em>T19】</strong></p><h2 id="542c" class="me jp hi bd jq mf mg mh ju mi mj mk jy kn ml mm ka kp mn mo kc kr mp mq ke mr bi translated">神经机器翻译问题</h2><p id="d309" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">为了帮助你更好地理解，在这篇文章中，我将把神经机器翻译作为一个运行的例子。在神经机器翻译中，输入是一系列单词，一个接一个地处理。同样，输出是一系列单词。</p><p id="53dc" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">任务:预测每个用作输入的英语句子的法语翻译。</p><p id="47ff" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">为了简单起见，让我们考虑在我们的数据语料库中只有一个句子。所以让我们的数据是:</p><ul class=""><li id="c6b3" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt mv kv kw kx bi translated"><strong class="ki hj">输入:英语句子:“很高兴见到你”</strong></li><li id="7fb6" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt mv kv kw kx bi translated"><strong class="ki hj">输出:法文翻译:《ravi de vous ren conter》</strong></li></ul><h2 id="e1f3" class="me jp hi bd jq mf mg mh ju mi mj mk jy kn ml mm ka kp mn mo kc kr mp mq ke mr bi translated"><strong class="ak"> <em class="mw">术语使用</em> </strong></h2><p id="0f3b" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">为避免任何混淆:</p><ul class=""><li id="cceb" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt mv kv kw kx bi translated">我将输入句子<strong class="ki hj">“很高兴见到你”</strong>称为<strong class="ki hj">X/输入序列</strong></li><li id="ed3d" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt mv kv kw kx bi translated">我会把输出句子<strong class="ki hj"> "ravi de vous rencontrer" </strong>称为<strong class="ki hj">Y _ true/target-sequence</strong>→这是<strong class="ki hj"> </strong>我们希望我们的模型预测的(地面真实)。</li><li id="0636" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt mv kv kw kx bi translated">我将把模型的预测输出句子称为<strong class="ki hj">Y _ pred/预测序列</strong></li><li id="141c" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt mv kv kw kx bi translated">英语和法语句子中的单个单词被称为<strong class="ki hj">标记</strong></li></ul><p id="639d" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">因此，给定输入序列“很高兴见到你”，我们希望我们的模型预测目标序列/Y_true，即“ravi de vous rencontrer”</p><h2 id="fc9b" class="me jp hi bd jq mf mg mh ju mi mj mk jy kn ml mm ka kp mn mo kc kr mp mq ke mr bi translated">高级概述</h2><p id="8165" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">在非常高的水平上，编码器-解码器模型可以被认为是两个块，编码器和解码器通过我们将称为“上下文向量”的向量连接。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mx"><img src="../Images/7b9e962fe520af390f69588c2947e79e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GvN7ekDa8rFPAZIc_xUmIw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><ul class=""><li id="efe7" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt mv kv kw kx bi translated"><strong class="ki hj">编码器</strong>:编码器处理输入序列中的每个令牌。它试图将关于输入序列的所有信息塞进一个固定长度的向量，即“上下文向量”。在遍历完所有的令牌后，编码器将这个向量传递给解码器。</li><li id="ce98" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt mv kv kw kx bi translated"><strong class="ki hj">上下文向量</strong>:向量是以这样一种方式构建的，它被期望封装输入序列的全部含义，并帮助解码器做出准确的预测。我们稍后会看到，这是我们的编码器模块的最终内部状态。</li><li id="b7e7" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt mv kv kw kx bi translated"><strong class="ki hj">解码器</strong>:解码器读取上下文向量，并尝试逐个令牌地预测目标序列。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es my"><img src="../Images/3caf1c13cf803fc19aa34e843a8180a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6yI-Ecx36JYixgomc-inPg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><h2 id="fca6" class="me jp hi bd jq mf mg mh ju mi mj mk jy kn ml mm ka kp mn mo kc kr mp mq ke mr bi translated">引擎盖下是什么？</h2><p id="11d4" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">这两个块的内部结构如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mz"><img src="../Images/47101496a5a7bb08547e85a0429e18cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0aHodc667UfSyZj-UY8OQw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="ddf3" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">就建筑而言，这很简单。这个模型可以被认为是两个LSTM细胞，它们之间有某种联系。这里最重要的是我们如何处理输入和输出。我将逐一解释每一部分。</p><h2 id="f6ed" class="me jp hi bd jq mf mg mh ju mi mj mk jy kn ml mm ka kp mn mo kc kr mp mq ke mr bi translated">编码器模块</h2><p id="d71e" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">编码器部分是一个LSTM单元。随着时间的推移，它被输入到输入序列中，并试图封装其所有信息，并将其存储在其最终的内部状态<strong class="ki hj"> hₜ </strong>(隐藏状态)<strong class="ki hj"> </strong>和<strong class="ki hj"> cₜ </strong>(单元状态)。内部状态然后被传递到解码器部分，它将使用该部分来尝试产生目标序列。这就是我们之前提到的“上下文向量”。</p><p id="b3eb" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">编码器部分的每个时间步长的输出都被丢弃</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es na"><img src="../Images/f800c9eae5112323c06be38b6fd55543.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W02VVwUd0aDiitTnfopx0g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="2ec0" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><em class="li">注意:上图是我们在时间轴上展开LSTM/GRU细胞时的样子。即，单个LSTM/GRU单元在每个时间戳获取单个字/令牌。</em></p><p id="3508" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><em class="li">在论文中，他们使用了LSTMs而不是传统的rnn，因为它们更适合长期依赖关系。我也见过有人使用GRUs。</em></p><h2 id="d854" class="me jp hi bd jq mf mg mh ju mi mj mk jy kn ml mm ka kp mn mo kc kr mp mq ke mr bi translated">解码器模块</h2><p id="bd6b" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">因此，在读取整个输入序列后，编码器将内部状态传递给解码器，这就是输出序列预测的开始。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nb"><img src="../Images/ba2ef96901d82b10c412e4602edc75b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yYL1eM6UWbBZa9N975ShpA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="a504" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">解码器模块也是一个LSTM单元。这里要注意的主要是解码器的初始状态<strong class="ki hj"> (h₀，c₀) </strong>被设置为编码器的最终状态<strong class="ki hj"> (hₜ，cₜ) </strong>。这些作为“上下文”向量，帮助解码器产生所需的目标序列。</p><p id="fa42" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">现在解码器的工作方式是，它在任何时间步<strong class="ki hj"> t </strong>的输出是<strong class="ki hj"> </strong>应该是目标序列中的<strong class="ki hj"> tᵗʰ </strong>字/y _ true(“ravi de vous ren controller”)。为了解释这一点，让我们看看在每个时间步发生了什么。</p><p id="da4f" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><strong class="ki hj">在时间步长1 </strong></p><p id="69b8" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">在第一时间步馈送到解码器的输入是特殊符号<strong class="ki hj">&lt;开始&gt;</strong>。这用于表示输出序列的开始。现在，解码器使用该输入和内部状态<strong class="ki hj"> (hₜ，cₜ) </strong>来产生第一时间步中的<strong class="ki hj"> </strong>输出，该输出应该是目标序列中的第一个字/标记，即<strong class="ki hj">【ravi】</strong>。</p><p id="74ed" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><strong class="ki hj">在时间步2 </strong></p><p id="e5ce" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">在时间步骤2，来自第一时间步骤<strong class="ki hj">“ravi”</strong>的输出作为输入馈入第二时间步骤。第二时间步中的输出应该是目标序列中的第二个字，即<strong class="ki hj">【de】</strong></p><p id="8126" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">类似地，每个时间步长的输出被作为输入馈送到下一个时间步长。这一直持续到我们得到“<end>”符号，这也是一个用于标记输出序列结束的特殊符号。解码器的最终内部状态被丢弃。</end></p><p id="a56e" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><em class="li">注意，这些特殊符号不一定必须是“</em> &lt;开始&gt;”和“&lt;结束&gt;”<em class="li">。这些可以是任何字符串，只要这些字符串不在我们的数据语料库中，因此模型不会将它们与任何其他单词混淆。在论文中，他们使用了符号</em>&lt;【EOS】&gt;<em class="li">，并且使用了稍微不同的方式</em>。<em class="li">稍后我会详细介绍这一点。</em></p><p id="5610" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><strong class="ki hj">注:</strong>上面提到的过程就是<strong class="ki hj">理想</strong>解码器在测试阶段的工作方式。但是在训练阶段，需要稍微不同的实现，以使它训练得更快。我将在下一节解释这一点。</p><h1 id="cb75" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">培训和测试阶段:进入张量</h1><h2 id="198c" class="me jp hi bd jq mf mg mh ju mi mj mk jy kn ml mm ka kp mn mo kc kr mp mq ke mr bi translated">向量化我们的数据</h2><p id="bb9f" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">在进入细节之前，我们首先需要对数据进行矢量化。</p><p id="d0ea" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">我们拥有的原始数据是</p><ul class=""><li id="4f3b" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt mv kv kw kx bi translated"><strong class="ki hj"> X = </strong> <strong class="ki hj">“很高兴见到你”→Y _ true = " ravi de vous ren conter "</strong></li></ul><p id="5812" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">现在我们将特殊符号“<start>”和“<end>”放在目标序列的开始和结束处</end></start></p><ul class=""><li id="00b5" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt mv kv kw kx bi translated"><strong class="ki hj"> X = </strong> <strong class="ki hj">“很高兴见到你”→Y _ true = "&lt;START&gt;ravi de vous ren contrer&lt;END&gt;"</strong></li></ul><p id="ad1d" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">接下来，使用一次热编码(ohe)对输入和输出数据进行矢量化。让输入和输出表示为</p><ul class=""><li id="258a" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt mv kv kw kx bi translated"><strong class="ki hj"> X = (x1，x2，x3，x4) → Y_true = (y0_true，y1_true，y2_true，y3_true，y4_true，y5_true) </strong></li></ul><p id="07f9" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">其中和易分别代表输入序列和输出序列的ohe向量。它们可以表现为:</p><p id="5f43" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><strong class="ki hj">为输入X </strong></p><p id="afe7" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">‘不错’→x1:[1 0 0 0]</p><p id="45bf" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">to' → x2 : [0 1 0 0 ]</p><p id="f04e" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">'满足'→x3 : [0 0 1 0]</p><p id="029b" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">你'→ x4 : [0 0 0 1]</p><p id="47ee" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><strong class="ki hj">为输出Y_true </strong></p><p id="0870" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><start>'→y0 _ true:[1 0 0 0 0]</start></p><p id="717e" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">拉维'→ y1_true : [0 1 0 0 0 0]</p><p id="aad4" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">de' → y2_true : [0 0 1 0 0 0]</p><p id="bd67" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">vous' → y3_true : [0 0 0 1 0 0]</p><p id="bc74" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">rencontrer' → y4_true : [0 0 0 0 1 0]</p><p id="43f5" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><end>'→y5 _ true:[0 0 0 0 1]</end></p><p id="d867" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">注意:为了便于解释，我使用了这种表示法。术语“真实序列”和“目标序列”都被用来指代我们希望我们的模型学习的同一个句子“ravi de vous rencontrer”。</p><h2 id="1e8b" class="me jp hi bd jq mf mg mh ju mi mj mk jy kn ml mm ka kp mn mo kc kr mp mq ke mr bi translated">编码器的培训和测试</h2><p id="8836" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">编码器的工作在训练和测试阶段是相同的。它一个接一个地接受输入序列的每个标记/单词，并将最终状态发送给解码器。其参数随时间使用反向传播来更新。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nc"><img src="../Images/00cd802cc067fe7e084e6ef75d190f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5nD2k6cRx1GJwB_qTtypEw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><h2 id="4706" class="me jp hi bd jq mf mg mh ju mi mj mk jy kn ml mm ka kp mn mo kc kr mp mq ke mr bi translated">培训阶段的解码器:教师强迫</h2><p id="1dc1" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">与编码器部分不同，解码器的工作在训练和测试阶段是不同的。因此，我们将两者分开来看。</p><p id="77d6" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">为了训练我们的解码器模型，我们使用一种称为“<strong class="ki hj">教师强制”</strong>的技术，其中我们将来自前一时间步的<strong class="ki hj">真实</strong>输出/令牌(以及<strong class="ki hj">而不是预测的</strong>输出/令牌)作为输入馈送到当前时间步。</p><p id="1113" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">为了解释，我们来看看训练的第1次迭代。这里，我们将输入序列输入到编码器，编码器对其进行处理，并将最终的内部状态传递给解码器。现在对于解码器部分，参考下图。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nd"><img src="../Images/f258b01f637b0eef88c15c822fb0ec50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3gCQBomC_AolBeF1klw_ig.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="cb2f" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">在继续之前，注意在解码器中，在任何时间步<strong class="ki hj"> t </strong>，输出<strong class="ki hj"> yt_pred </strong>是通过使用Softmax激活函数生成的输出数据集中整个词汇的概率分布。具有最大概率的单词被选为预测单词。</p><p id="1e3d" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">例如，参考上图，y1 _ pred =[0.02 0.12 0.36 0.1 0.3 0.1]告诉我们，我们的模型认为输出序列中第一个令牌为'<start>'的概率是0.02，' ravi '是0.12，' de '是0.36等等。我们将预测的单词作为概率最高的单词。因此，这里预测的单词/单词是概率为0.36的<strong class="ki hj">‘de’</strong></start></p><p id="02c5" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">继续前进...</p><p id="f5c2" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><strong class="ki hj">在时间步长1 </strong></p><p id="43d9" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">单词'<start>'的向量[1 0 0 0 0 0]作为输入向量。现在，我希望我的模型预测输出为y1_true=[0 1 0 0 0 0],但由于我的模型刚刚开始训练，它将随机输出一些内容。假设时间步长1处的预测值为y1 _ pred =[0.02 0.12 0.36 0.1 0.3 0.1]意味着它预测第一个令牌为<strong class="ki hj">‘de’</strong>。现在，我们是否应该使用这个y1_pred作为时间步长2的输入？。我们可以做到这一点，但在实践中，这导致了收敛缓慢、模型不稳定和技能低下等问题，如果你认为这是很合理的。</start></p><p id="2fad" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">于是，<strong class="ki hj">老师强制</strong>就被引入来纠正这一点。其中我们将前一时间步的真实输出/令牌(而不是预测输出)作为输入提供给当前时间步。这意味着时间步长2的输入将是y1_true=[0 1 0 0 0 0]而不是y1_pred。</p><p id="0c45" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">现在，时间步长2的输出将是某个随机向量y2_pred。但是在时间步长3，我们将使用input作为y2_true=[0 0 1 0 0 0]而不是y2_pred。同样，在每个时间步，我们将使用前一个时间步的真实输出。</p><p id="b67d" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">最后，根据每个时间步的预测输出计算损失，误差随时间反向传播以更新模型参数。所使用的损失函数是目标序列/ <strong class="ki hj"> Y_true </strong>和预测序列/ <strong class="ki hj"> Y_pred </strong>之间的分类交叉熵损失函数，使得</p><ul class=""><li id="b345" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt mv kv kw kx bi translated">Y_true = [y0_true，y1_true，y2_true，y3_true，y4_true，y5_true]</li><li id="a043" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt mv kv kw kx bi translated">Y_pred = [' <start>'，y1_pred，y2_pred，y3_pred，y4_pred，y5_pred]</start></li></ul><p id="e515" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">解码器的最终状态被丢弃</p><h2 id="57ab" class="me jp hi bd jq mf mg mh ju mi mj mk jy kn ml mm ka kp mn mo kc kr mp mq ke mr bi translated">测试阶段的解码器</h2><p id="1522" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">在实际应用中，我们没有Y_true，只有x。因此，我们不能使用我们在训练阶段所做的，因为我们没有target-sequence/Y_true。因此，当我们测试我们的模型时，来自前一时间步的预测输出(而不是与训练阶段不同的真实输出)被作为输入提供给当前时间步。休息和训练阶段一样。</p><p id="b981" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">假设我们已经训练了我们的模型，现在我们用我们训练过的单个句子来测试它。<strong class="ki hj">现在，如果我们对模型进行了很好的训练，并且只对一个句子进行了训练，那么它应该表现得几乎完美，但为了解释起见，假设我们的模型没有经过很好的训练或部分训练，现在我们对其进行测试。下面的图表描绘了这个场景</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ne"><img src="../Images/a6102fccb631e866b4fe3b773f874955.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QaJKYwjg06vzooJVGKOIEw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="249a" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><strong class="ki hj">在时间步长1 </strong></p><p id="c1e5" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">y1_pred = [0 0.92 0.08 0 0 0]表明该模型预测输出序列中的第一个单词/词以0.92的概率为“ravi”，因此现在在下一个时间步，该预测的单词/词将仅用作输入。</p><p id="6caa" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><strong class="ki hj">在时间步长2 </strong></p><p id="0783" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">第一时间步的预测单词/单词<strong class="ki hj">“ravi”</strong>在此用作输入。这里，模型以0.98的概率预测输出序列中的下一个单词/标记为<strong class="ki hj">‘de’</strong>，然后在时间步骤3将其用作输入</p><p id="8d0f" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">并且在每个时间步重复类似的过程，直到到达“<end>”标记</end></p><p id="ef71" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">更好的可视化效果是:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nf"><img src="../Images/4e3a020c5818a5ecb74d33e3ac707e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*PdckBUuynt8wCVJ6f197Rw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="807f" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">因此，根据我们训练的模型，测试时的预测序列是“ravi de rencontrer rencontrer”。因此，尽管模型在第三次预测时是不正确的，我们仍然将其作为输入输入到下一个时间步。模型的正确性取决于可用数据的数量和训练的好坏。该模型可能预测错误的输出，但是，相同的输出仅被馈送到测试阶段的下一个时间步。</p><h2 id="5c8b" class="me jp hi bd jq mf mg mh ju mi mj mk jy kn ml mm ka kp mn mo kc kr mp mq ke mr bi translated">嵌入层</h2><p id="8202" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">我之前没有告诉你的一个小细节是，解码器和编码器中的输入序列都通过嵌入层，以降低输入单词向量的维度，因为在实践中，一次性编码的向量可能非常大，而嵌入的向量是单词的更好表示。对于编码器部分，这可以在下面示出，其中嵌入层将字向量的维度从四个减少到三个。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ng"><img src="../Images/28de7bf449fa74093986e99313cd4bdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OWrsfe6vtTUU--cuYWSrtA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="090d" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><em class="li">该嵌入层可以像Word2Vec嵌入一样进行预训练，也可以使用模型本身进行训练。</em></p><h2 id="6c3b" class="me jp hi bd jq mf mg mh ju mi mj mk jy kn ml mm ka kp mn mo kc kr mp mq ke mr bi translated">测试时的最终可视化</h2><p id="4694" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">试图在一张图中捕捉一切</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nh"><img src="../Images/010922b9f9fdd402e01415209e62d2e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B5pqh5hTgTaNiuUQhtxBQA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="4ae4" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><em class="li">您可能会发现不同地方的实现略有不同，但主要思想是相同的。</em></p><h1 id="79da" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">“苏茨基弗模式”</h1><p id="f4b1" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">Ilya Sutskever等人的论文<a class="ae jn" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">Sequence to Sequence Learning with Neural Networks<em class="li"/>是介绍机器翻译的编码器-解码器模型以及一般Seq2Seq建模的先驱论文之一。</a></p><p id="685e" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">这里，我将提到论文中的要点，以及它们的实现与我们所学的玩具示例有何不同。</p><ul class=""><li id="ab6c" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt mv kv kw kx bi translated">该模型应用于英语到法语的翻译</li><li id="b2f4" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt mv kv kw kx bi translated">每个句子都以一个特殊的句尾符号“<eos>结束</eos></li></ul><blockquote class="ld le lf"><p id="f848" class="lg lh li ki b kj lj ij lk kl ll im lm ln lo lp lq lr ls lt lu lv lw lx ly kt hb bi translated">请注意，我们要求每个句子以一个特殊的句尾符号“<eos>”结束，这使模型能够定义所有可能长度的序列的分布[1]</eos></p></blockquote><p id="c471" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">在文中，为了解释，他们把输入序列看作“A B C ”,把目标序列看作“W X Y Z”</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ni"><img src="../Images/22e596c67b85d04f02508970e1e811b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8oJefm50Ic3Tv7yxAYM9Sw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://www.slideshare.net/quangntta/sequence-to-sequence-learning-with-neural-networks" rel="noopener ugc nofollow" target="_blank">https://www . slide share . net/Quang TTA/sequence-to-sequence-learning-with-neural-networks</a></figcaption></figure><ul class=""><li id="25be" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt mv kv kw kx bi translated"><strong class="ki hj">数据集详情</strong></li></ul><blockquote class="ld le lf"><p id="a008" class="lg lh li ki b kj lj ij lk kl ll im lm ln lo lp lq lr ls lt lu lv lw lx ly kt hb bi translated">“我们在1200万个句子的子集上训练我们的模型，这些句子包括3.48亿个法语单词和3.04亿个英语单词，这是从[29]中“精选”出来的干净子集。我们选择这个翻译任务和这个特定的训练集子集，是因为一个标记化的训练和测试集以及来自基线SMT的1000个最佳列表的公共可用性[29]。由于典型的神经语言模型依赖于每个单词的向量表示，我们对两种语言都使用了固定的词汇。我们对源语言使用了160，000个最常用的单词，对目标语言使用了80，000个最常用的单词。每一个不在词汇表中的单词都被替换成一个特殊的“UNK”符号[1]</p></blockquote><ul class=""><li id="cb49" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt mv kv kw kx bi translated"><strong class="ki hj">输入序列被颠倒。</strong></li></ul><blockquote class="ld le lf"><p id="9b35" class="lg lh li ki b kj lj ij lk kl ll im lm ln lo lp lq lr ls lt lu lv lw lx ly kt hb bi translated">“虽然LSTM能够解决长期依赖的问题，但我们发现，当源句颠倒时(目标句不颠倒)，LSTM学得更好”[1]</p></blockquote><ul class=""><li id="32ba" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt mv kv kw kx bi translated">使用1000维的单词嵌入层来表示输入单词。Softmax用于输出层。</li><li id="c341" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt mv kv kw kx bi translated">输入和输出模型有4层，每层1000个单元。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nj"><img src="../Images/8a02cddfb210282ce8756eb12e13d9f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eh20nFQQxwip4bkkkc_98g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://www.slideshare.net/quangntta/sequence-to-sequence-learning-with-neural-networks" rel="noopener ugc nofollow" target="_blank">https://www . slide share . net/Quang TTA/sequence-to-sequence-learning-with-neural-networks</a></figcaption></figure><ul class=""><li id="827e" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt mv kv kw kx bi translated">达到了34.81的BLEU分数。</li></ul><blockquote class="ld le lf"><p id="7d8e" class="lg lh li ki b kj lj ij lk kl ll im lm ln lo lp lq lr ls lt lu lv lw lx ly kt hb bi translated">“……我们获得了34.81的BLEU分数[……]这是迄今为止通过大型神经网络直接翻译获得的最好结果。作为比较，该数据集上SMT基线的BLEU得分为33.30”[1]</p></blockquote><p id="3663" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">这里有一篇谈在同一篇论文上，供你参考。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nk nl l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://www.youtube.com/watch?v=-uyXE7dY5H0" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=-uyXE7dY5H0</a></figcaption></figure><h1 id="1bf7" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">图像字幕直觉</h1><p id="0f80" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">由安德烈·卡帕西、李菲菲等人 <strong class="ki hj"> </strong>撰写的论文<a class="ae jn" href="https://arxiv.org/abs/1412.2306" rel="noopener ugc nofollow" target="_blank">用于生成图像描述的深度视觉语义对齐使用了不同版本的图像字幕编码器-解码器模型。图像字幕是生成图像文本描述的过程。</a></p><p id="43a6" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">在他们提出的模型中，图像被输入你选择的CNN网络。然后，最后一个密集层被送入LSTM网络。这一层在某种意义上充当了上下文向量，因为它捕捉了图像的本质。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nm"><img src="../Images/8c48527334345515de5136d656686349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*krQS1HdwYlu9IwrX_V6Jhw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://arxiv.org/pdf/1412.2306.pdf" rel="noopener ugc nofollow" target="_blank">用于生成图像描述的深度视觉语义对齐</a></figcaption></figure><h1 id="8752" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">进一步阅读</h1><p id="f6b3" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">如果你找到了来这里的路，那么恭喜你！！我希望你已经发现这是有用的。因为这是我第一次尝试写博客，我希望读者能原谅和忽略我可能犯的任何小错误。如果你想更深入地了解这个话题，我会提供更多的资源。</p><ol class=""><li id="c848" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt ku kv kw kx bi translated">我建议你自己看一遍我们讨论过的Ilya Sutskever等人的  <strong class="ki hj"> </strong>关于神经网络的<a class="ae jn" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank"> <strong class="ki hj">序列到序列学习的论文。</strong></a></li><li id="6e96" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">没有什么比亲自实践更能帮助你理解一个概念了。我发现了这个惊人的博客<a class="ae jn" href="https://towardsdatascience.com/neural-machine-translation-using-seq2seq-with-keras-c23540453c74" rel="noopener" target="_blank"> <strong class="ki hj">神经机器翻译与Keras </strong> </a> <strong class="ki hj"> </strong>，其中一个编码器-解码器模型被从头开始训练，用于英语到法语的翻译。</li><li id="a6a6" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">如前所述，针对Seq2Seq建模，针对编码器-解码器模型开发了许多高级技术。对于它们，我强烈建议您按顺序参考以下内容</li></ol><ul class=""><li id="79d6" class="kg kh hi ki b kj lj kl ll kn ms kp mt kr mu kt mv kv kw kx bi translated">注意机制→ <em class="li">杰伊·阿拉玛，</em> <a class="ae jn" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank"> <strong class="ki hj">可视化一个神经机器翻译模型</strong> </a> <strong class="ki hj"> </strong>博文，2018。</li><li id="ee58" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt mv kv kw kx bi translated">变形金刚→ <em class="li">杰伊·阿拉玛，</em> <a class="ae jn" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> <strong class="ki hj">图文并茂的变形金刚</strong> </a>博文<em class="li">，</em> 2018。</li><li id="67b0" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt mv kv kw kx bi translated">伯特→杰·阿拉玛，<a class="ae jn" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank"> <strong class="ki hj">图文并茂伯特</strong> </a> <strong class="ki hj"> </strong>博文，2018。</li></ul><h1 id="5b6d" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">参考</h1><p id="252d" class="pw-post-body-paragraph lg lh hi ki b kj kk ij lk kl km im lm kn lz lp lq kp ma lt lu kr mb lx ly kt hb bi translated">[1]<a class="ae jn" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">Ilya Sutskever等人</a>利用神经网络进行序列间学习<em class="li"> </em></p><p id="ef5e" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">[2] <a class="ae jn" href="https://arxiv.org/abs/1412.2306" rel="noopener ugc nofollow" target="_blank">安德烈·卡帕西、李菲菲等人</a>用于生成图像描述的深度视觉语义比对</p><p id="4f33" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">[3]<a class="ae jn" href="https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9" rel="noopener" target="_blank">https://towards data science . com/illustrated-guide-to-recurrent-neural-networks-79 E5 EB 8049 c 9</a></p><p id="3e6c" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><a class="ae jn" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p id="f1c7" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated"><a class="ae jn" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p><p id="7fe3" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">[6]<a class="ae jn" href="https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/</a></p><p id="f1d4" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">[7]<a class="ae jn" href="https://www.slideshare.net/quangntta/sequence-to-sequence-learning-with-neural-networks" rel="noopener ugc nofollow" target="_blank">https://www . slide share . net/Quang TTA/sequence-to-sequence-learning-with-neural-networks</a></p></div><div class="ab cl nn no gp np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="hb hc hd he hf"><p id="50ab" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">帖子中的图表是使用<a class="ae jn" href="https://creately.com/" rel="noopener ugc nofollow" target="_blank"> creately </a>制作的</p><p id="ae17" class="pw-post-body-paragraph lg lh hi ki b kj lj ij lk kl ll im lm kn lo lp lq kp ls lt lu kr lw lx ly kt hb bi translated">PS:如果你认为他们可以改进这个博客，请随时提供意见/批评，我一定会努力做出必要的修改。</p></div></div>    
</body>
</html>