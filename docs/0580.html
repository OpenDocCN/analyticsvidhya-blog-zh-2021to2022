<html>
<head>
<title>Deep Learning Specialization Course Notes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习专业化课程笔记</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-learning-specialization-course-notes-138ecd5ad4ef?source=collection_archive---------19-----------------------#2021-01-22">https://medium.com/analytics-vidhya/deep-learning-specialization-course-notes-138ecd5ad4ef?source=collection_archive---------19-----------------------#2021-01-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="391d" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">课程2:改进神经网络:超参数调整、正则化和优化(第一周笔记继续..)</h2></div><p id="b110" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">到目前为止，我们已经学会了将测试集分为训练集、开发集和测试集。我们还学习了正规化。在本文中，我们将了解加速神经网络训练的方法。不要再拖延了，让我们开始吧。</p><p id="632e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你还没有看过我关于第一周笔记的文章，请快速参考<a class="ae js" href="https://link.medium.com/7dLKWMxMfdb" rel="noopener">这个</a>链接。</p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h2 id="918f" class="ka kb hh bd kc kd ke kf kg kh ki kj kk jf kl km kn jj ko kp kq jn kr ks kt ku bi translated">加速神经网络训练的方法</h2><h2 id="1a6b" class="ka kb hh bd kc kd ke kf kg kh ki kj kk jf kl km kn jj ko kp kq jn kr ks kt ku bi translated"><strong class="ak"> 1。标准化输入</strong></h2><p id="0dca" class="pw-post-body-paragraph iw ix hh iy b iz kv ii jb jc kw il je jf kx jh ji jj ky jl jm jn kz jp jq jr ha bi translated">在训练神经网络时，加速训练的技术之一是标准化输入。例如，如果我们的训练集中有两个特征，那么我们可以计算这两个特征的均值和方差，并使用以下公式使用均值和方差对这两个特征进行归一化。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es la"><img src="../Images/2f806253f9481ee2b2cf4e94c5f40f52.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*BVhOlnrab0Csj43C6aQRYg.png"/></div></figure><p id="a305" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们应该对测试数据使用相同的均值和方差，因为我们需要两个数据都经过相同的转换。</p><p id="c904" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">正常化起作用的原因</strong></p><p id="4f68" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果我们使用非标准化的特征，代价函数可能看起来很长。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es li"><img src="../Images/a9aa5774327bb52485595088bd4dcca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*Ys3pK9cAZhubW92PQp0kww.png"/></div></figure><p id="8473" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">而如果我们使用标准化的输入，成本函数可能看起来非常对称。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lj"><img src="../Images/610a54190dfbd810b426e06b9fa7d311.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*bCCniXDMN19Ru0d-EfLLpw.png"/></div></div></figure><p id="340f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果特征在非常不同的尺度上，假设特征X1的范围是从1到1，000，而特征X2的范围是从0到1，那么结果是参数w1和w2的比值或值的范围将最终呈现非常不同的值。此外，对于未标准化的特征，我们将不得不保持最小的学习速率，因为在它达到最小值之前将需要几个步骤，而如果我们具有球形轮廓，我们可以保持较大的学习步骤，从而导致较大的步骤达到最小值。</p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h2 id="5122" class="ka kb hh bd kc kd ke kf kg kh ki kj kk jf kl km kn jj ko kp kq jn kr ks kt ku bi translated"><strong class="ak"> 2。消失/爆炸渐变</strong></h2><p id="0e22" class="pw-post-body-paragraph iw ix hh iy b iz kv ii jb jc kw il je jf kx jh ji jj ky jl jm jn kz jp jq jr ha bi translated">训练深度神经网络的问题之一是数据消失或爆炸梯度。在训练时，导数/斜率可能取一个巨大的值或一个非常小的值，这会使训练变得困难。为了理解爆炸梯度问题，让我们从线性函数开始，</p><p id="9f09" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">g(z) = z。</p><p id="8695" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">y= w[l]*w[l-1]*w[l-2]…w[1]*X但是w[1]*x = z[1]。当我们使用线性激活函数时，a[1]将等于g(z)，g(z)又等于z[1]。以及a[2] = w[2]*a[1] = w[2]*w[1]*X。</p><p id="7425" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，如果w的值很大，那么y的值将爆炸，如果w的值减小，那么y的值将指数减小。因此，可以说激活函数将作为层l的函数指数地增加或减少</p><p id="e0f9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个问题的部分解决方案是在初始化权重时仔细选择。</p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h2 id="f55b" class="ka kb hh bd kc kd ke kf kg kh ki kj kk jf kl km kn jj ko kp kq jn kr ks kt ku bi translated">3.深度网络的权重初始化</h2><p id="f9a3" class="pw-post-body-paragraph iw ix hh iy b iz kv ii jb jc kw il je jf kx jh ji jj ky jl jm jn kz jp jq jr ha bi translated">在n个特征的情况下，我们希望w尽可能小，因为z是w[i]x[i]的和。</p><p id="ee87" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一个合理的做法是将w的方差设置为1/n，其中n是进入神经元的特征数量。因此，对于l层，w的值可以初始化如下。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lo"><img src="../Images/cb113c9a2d92dbe172516520bc97a3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c25lpVA2ITbMmmLfRIgZ6A.png"/></div></div></figure><p id="f8e7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果我们使用Relu函数，那么2是上式中的首选值。如果w的值是通过方差控制的，那么z也将在相似的尺度上取值。它不能完全解决问题，但是它有助于控制w的值，使得梯度不会爆炸或消失。</p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h2 id="98af" class="ka kb hh bd kc kd ke kf kg kh ki kj kk jf kl km kn jj ko kp kq jn kr ks kt ku bi translated">4.梯度的数值近似</h2><p id="b650" class="pw-post-body-paragraph iw ix hh iy b iz kv ii jb jc kw il je jf kx jh ji jj ky jl jm jn kz jp jq jr ha bi translated">在实现反向传播时，有一种称为梯度检查的测试有助于确保反向传播的正确实现，但在建立梯度检查之前，让我们先讨论一下如何近似计算梯度。</p><p id="acb6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">考虑一个函数f，其中f(theta) = theta，现在如果我们绘制theta的函数并将其值微移至两侧，这将为我们提供一个更好的梯度近似值，而不是仅微移一侧的值，如下图所示。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es lp"><img src="../Images/ed0d3db414ac4e2647012227ed98557d.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*kfkzIBNG8W-Q7WQ07hHY5g.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">图片来源:吴恩达</figcaption></figure><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es lu"><img src="../Images/0dc1ad68c1ad62f31170d9f609b7e39a.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*WdCiezhdJAAtPqeQDyss9w.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">图片来源:吴恩达</figcaption></figure><h2 id="cae9" class="ka kb hh bd kc kd ke kf kg kh ki kj kk jf kl km kn jj ko kp kq jn kr ks kt ku bi translated">梯度检查</h2><p id="3ae3" class="pw-post-body-paragraph iw ix hh iy b iz kv ii jb jc kw il je jf kx jh ji jj ky jl jm jn kz jp jq jr ha bi translated">要执行梯度检查，请遵循下面提到的步骤。</p><ol class=""><li id="ae28" class="lv lw hh iy b iz ja jc jd jf lx jj ly jn lz jr ma mb mc md bi translated">取所有的参数w和b，把它们重塑成一个巨大的向量θ。因此，成本函数将只是向量θ的函数。</li><li id="69ea" class="lv lw hh iy b iz me jc mf jf mg jj mh jn mi jr ma mb mc md bi translated">取所有导数参数dw1，db1…dw[l]，db[l]并将它们整形为一个巨型向量dtheta。</li><li id="9b3d" class="lv lw hh iy b iz me jc mf jf mg jj mh jn mi jr ma mb mc md bi translated">评估dtheta是否是成本函数J的斜率</li><li id="d1ec" class="lv lw hh iy b iz me jc mf jf mg jj mh jn mi jr ma mb mc md bi translated">要评估斜率，请遵循下图所示的步骤。</li></ol><figure class="lb lc ld le fd lf er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mj"><img src="../Images/0b3d1fb56c2aab4108d4ad304b377648.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XMGCJiCZG1luXn2MqUQV-g.png"/></div></div></figure><p id="1e60" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，用I的每一个值计算上面的等式，并通过计算两个向量之间的欧几里德距离来比较它们。如果欧几里德距离的值大约小于或等于10^-7，那么我们可以假设反向传播被正确地实现。如果该值在10^-5附近，那么我们可以说我们将不得不重新考虑我们的反向传播步骤。</p><h2 id="32d4" class="ka kb hh bd kc kd ke kf kg kh ki kj kk jf kl km kn jj ko kp kq jn kr ks kt ku bi translated"><strong class="ak">坡度检查实施说明</strong></h2><ul class=""><li id="bf01" class="lv lw hh iy b iz kv jc kw jf mk jj ml jn mm jr mn mb mc md bi translated">仅在调试期间使用梯度检查。</li><li id="0391" class="lv lw hh iy b iz me jc mf jf mg jj mh jn mi jr mn mb mc md bi translated">如果算法没有通过等级检查，那么尝试识别组件值中的错误。</li><li id="19d6" class="lv lw hh iy b iz me jc mf jf mg jj mh jn mi jr mn mb mc md bi translated">请记住在计算成本函数时包括正则化</li><li id="3921" class="lv lw hh iy b iz me jc mf jf mg jj mh jn mi jr mn mb mc md bi translated">它不适用于辍学正规化。</li></ul></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h2 id="3ea4" class="ka kb hh bd kc kd ke kf kg kh ki kj kk jf kl km kn jj ko kp kq jn kr ks kt ku bi translated"><strong class="ak">其他来源</strong></h2><p id="90b8" class="pw-post-body-paragraph iw ix hh iy b iz kv ii jb jc kw il je jf kx jh ji jj ky jl jm jn kz jp jq jr ha bi translated"><strong class="iy hi"> Kurtis Pykes: </strong> <a class="ae js" href="https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11" rel="noopener" target="_blank">深度神经网络中的消失/爆炸梯度问题</a></p><p id="f007" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">坚持阅读，快乐学习</strong>:)</p></div></div>    
</body>
</html>