<html>
<head>
<title>Deploying Apache Beam pipelines on Google DataFlow.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Google数据流上部署Apache Beam管道。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deploying-apache-beam-pipelines-on-google-dataflow-70e9e90624d9?source=collection_archive---------4-----------------------#2021-03-16">https://medium.com/analytics-vidhya/deploying-apache-beam-pipelines-on-google-dataflow-70e9e90624d9?source=collection_archive---------4-----------------------#2021-03-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="bba6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">利用集群的力量！</p><p id="efe3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我的上一篇文章中，我们研究了如何开始使用Apache Beam库来构建数据处理管道。现在让我们来看看如何让这些管道在能够并行处理我们的数据并减少处理时间的计算机集群上运行！</p><p id="2d42" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将在谷歌的数据流服务上部署管道。因此，部署过程有两个终点:我们的本地机器，我们目前可能有我们的数据和代码，以及Google云服务，我们希望通过云计算资源处理或分析这些数据。</p><p id="deec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在您的本地存储库中，您的根文件夹应该有一个包含您的主处理应用程序的. py文件，我们称之为“beam_pipeline.py”。这个必须用“if __name__ = __main__/run()”格式写。</p><p id="1abc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">看看下面的代码:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/92ef6aecb83ccde4439e0441538e6c50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ArlWyj_IN_XJmsuTY1DFxg.png"/></div></div></figure><p id="1d51" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在python中经常会遇到if __name__ == "__main__ "这一行，尤其是在部署的上下文中。我第一次尝试部署一个Dash应用程序，它是建立在Flask框架之上的，Flask框架也使用它。但是它有什么用呢？</p><p id="0426" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据一个高票的slackoverflow答案:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jo"><img src="../Images/68ca6f12e6f577a4522d1eff1cad8545.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*bsDGbGv2zX9Sue_71m4xtA.png"/></div></figure><p id="6c96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果直接从命令行执行文件，python解释器会设置python模块的__name__变量(即。您执行的py文件)到“_main_”。这可以向代码的其余部分发出信号，表明py文件正在直接运行，这有多种用途。</p><p id="af05" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，如果你。py文件不是你执行的那个，而是包含了一些额外的类或函数或逻辑，然后被导入到mains cript中，interpeter会把“__name__”变量设置为该模块的名称。</p><p id="6bef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过这种方式，您可以将您的代码划分成段，并且可以灵活地让脚本在执行时和导入时表现不同。许多框架似乎用这个作为钩子来知道程序从哪里“开始”或者什么代码应该首先运行。例如，一个dash应用程序需要在一个主函数中编写所有代码，然后使用回调函数来处理交互。通过包含“if _ _ name _ _ = = main ”, dash将执行一个函数，当脚本由nginx这样的服务器执行时，该函数将显示数据和控件。或者，在我们的例子中，Apache Beam框架。</p><p id="3ffb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上面的脚本中，我们定义了一个“运行”函数。然后，在最后我们包含“if__name__…”块并调用函数。这是Beam将执行的第一个函数，因此这是我们定义主要管道的地方。</p><p id="57d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">回想一下上一篇文章，我们的pipline将数据读入PCollections，然后对该PCollection应用自定义转换。然后，它将输出写入另一个拼花文件。当我们在本地执行脚本时，我们传递磁盘上文件的本地文件路径。</p><p id="a7d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">部署到云时，我们将这些输入和输出参数指向Google云存储桶。这就是我们开始谷歌云端工作的地方。这很简单。Google存储桶是一个简单的数据存储容器。您只需创建一个桶，将文件上载到该桶中，复制文件的URI并将其传递到您的函数中。</p><p id="3f8b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有两种方法可以告诉Apache beam在哪里寻找输入和输出路径。可以在脚本中对其进行硬编码，也可以在命令行中执行脚本时将路径传递到脚本中。如何实现这一点取决于您，但在运行时将参数传递到脚本中是执行管道过程的关键部分。</p><p id="4349" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本地计算机上，在终端环境中，您需要安装Google Cloud SDK。这将允许您的本地机器连接到Google云服务并与之交互，最终将作业提交给数据流服务。您还可以使用它与存储桶和其他Google云服务进行交互。<a class="ae jp" href="https://cloud.google.com/sdk/docs/install" rel="noopener ugc nofollow" target="_blank">这里</a>是标准指南。</p><p id="116c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您还需要pip安装Apache beam本身。此库在运行时导入到主脚本中。这就是你的脚本如何与Apache beam交互并告诉它做什么。然后可以使用各种Apache beam对象来控制管道的部署和执行。</p><p id="68bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在提交脚本以供执行时，Apache Beam使用“runners”与执行环境交互。例如，这可以是您的本地机器，也可以是云机器或云集群。有许多预构建的runners可以处理不同环境的执行。“DirectRunner”是在本地计算机上运行管道的梁流管理器。有针对不同环境的跑步者，包括Dataflow和Apache Spark。</p><p id="cc16" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">“DataflowRunner”用于将管道提交给Dataflow计算实例。此实例可以是单个虚拟机，也可以是群集。现在，您需要设置数据流服务，您的脚本将提交给该服务以供执行。这里有一个很好的端到端<a class="ae jp" href="https://cloud.google.com/dataflow/docs/quickstarts/quickstart-python" rel="noopener ugc nofollow" target="_blank">指南</a>。</p><p id="d84d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意一些重要的事情。您需要在活动python环境中运行“pip安装‘Apache-beam[GCP]’”来安装runner。然后，在代码本身的主体中，您将在Pipeline选项中指定这个runner，并且您的脚本将被定向到Dataflow环境，假设您设置正确的话。</p><p id="2eef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们来谈谈管道选项。请注意代码中的以下行:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jq"><img src="../Images/f309b903e0b0fe14dc8e85996f77537e.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*ItbJltheyE2wRACy3ZtSvw.png"/></div></figure><p id="5c6c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在您的脚本中，您使用beam实例化一个管道对象。这是发送到执行环境的对象。当它被发送时，您可以包含一些选项来告诉系统如何处理管道。其中一些选项是标准的，许多是必需的并且有默认值，如果您愿意，您也可以添加自己的选项。其中一个选项是使用哪个流道。通过将其设置为“数据流”，您将让脚本知道这将被发送到数据流服务。</p><p id="0e5d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意，对于选项，我们传入“pipeline_options”。这是梁库中的一个对象，它被实例化并用于以健壮和方便的方式指定选项。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jr"><img src="../Images/14a58230a74260a0460a873d9f4f86ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*w3uiXrI4BR1cHtITn-rIqA.png"/></div></figure><p id="4aeb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将管道选项存储在pipeline options对象中是向beam发送执行管道所需信息的好方法。注意，我们在实例化PipelineOptions时传递了“p_options”。这是一个字典，其中包含我们作为键/值对讨论的所有选项设置。我们使用**将这些解包到PipelineOptions的实例化调用中。</p><p id="0107" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这本字典是这样的:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es js"><img src="../Images/751fc55dd341bb096dd5e6f322de7d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y9v1yAYKxXbuSdooYglxQw.png"/></div></div></figure><p id="b423" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些是我们用来在数据流服务上配置管道执行的主要选项。您可以看到,“runner”键已将流道指定为“DataflowRunner”。</p><p id="4dc8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">“项目”参数指向您在Google Cloud控制台中创建的项目。项目用于在GCS中组织资源，您的数据流作业需要与项目相关联。为您正在处理的任何东西创建一个，或者选择一个现有的并将名称传递给该参数。您可以从GCS区域的<a class="ae jp" href="https://cloud.google.com/compute/docs/regions-zones" rel="noopener ugc nofollow" target="_blank">列表中选择一个合适的区域，例如‘us-west 4’。</a></p><p id="55ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们必须指定一个中转位置。这是一个存储区域，Apache beam可以在其中存储执行期间生成的中间文件。在存储桶中创建另一个名为“staging”的文件夹，并将其URI复制到“staging_location”的值中。类似地,‘temp _ location’应该指向另一个文件夹，在你的桶中称它为‘temp’。</p><p id="77e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另一个非常重要的设置是“setup_file”。这将把Apache指向所需的安装文件，以确保环境拥有执行脚本所需的正确包。根据Apache Beam文档，您并不总是需要安装文件，如果您只使用从pip安装的普通python包，您可以简单地指定一个需求文件(可以使用pip freeze &gt; requirements.txt方法生成)。</p><p id="f500" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，安装文件是可取的。这将是一个. py文件，我们将在其中使用setuptools库编写一个安装脚本。安装工具允许我们创建一个可安装的库包。下面是setup.py文件的样子:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jt"><img src="../Images/3a9910fdf8e019e48eefb546544758e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*5fVXTrNAOTDip1gYY3Denw.png"/></div></figure><p id="0a2b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在RQUIRED_PACKAGES变量中，我们列出了脚本运行所需的所有pip可安装包。Setuptools还可以用来在我们的根文件夹中打包我们希望由我们的主模块使用的附加模块，但是现在我们只包括我们的脚本在执行过程中需要的python库。然后，该列表被传递到setup方法的“install_requires”关键字参数中。以这种方式创建安装文件可以很容易地重新创建python环境。只需在命令行中调用“python setup.py install ”,我们也可以在一个干净的虚拟环境中一步安装所有必需的模块。</p><p id="0c57" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当需要启动集群中的工作节点时，Apache Beam和Google Dataflow也会使用setup.py文件，因为setup文件将用于将所有需要的库引入每个工作节点。最后一个管道选项“save_main_session”用于将脚本状态的副本发送给每个工作线程。例如，如果有主模块引入或生成的数据，它可能对工作节点不可用。我们将“save_main_session”设置为True，以便与他们共享主会话及其所有对象的pickled版本。</p><p id="4d0b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们要做的就是到我们的终端，键入“python beam_pipeline.py”。管道应该自动提交给数据流(假设您在机器上设置了Google Services SDK，如上所述，并且经过了正确的身份验证)。现在，您可以前往您的Google Cloud控制台，然后转到数据流页面，查看您提交到队列的作业。单击作业以监控来自主机和工作机的日志。它应该开始处理你的工作。</p><p id="0b9c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">希望这有所帮助！</p></div></div>    
</body>
</html>