<html>
<head>
<title>A Primer on Knowledge Distillation in NLP — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的知识蒸馏入门——第一部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-primer-on-knowledge-distillation-in-nlp-part-1-754ac00dadc6?source=collection_archive---------0-----------------------#2021-12-22">https://medium.com/analytics-vidhya/a-primer-on-knowledge-distillation-in-nlp-part-1-754ac00dadc6?source=collection_archive---------0-----------------------#2021-12-22</a></blockquote><div><div class="ds gz ha hb hc hd"/><div class="he hf hg hh hi"><div class=""/><p id="bc69" class="pw-post-body-paragraph ii ij hl ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf he bi translated">近年来，深度神经网络在各种自然语言处理(NLP)任务上取得了令人印象深刻的性能，然而，高计算和存储要求对其在现实世界应用中的部署提出了巨大的挑战，特别是在资源有限的设备上，如手机。为此，已经开发了各种模型压缩和加速技术。一种这样的技术是知识提炼，它可以有效地从一个大的<strong class="ik hm">老师</strong>模型中学习一个小的<strong class="ik hm">学生</strong>模型。在这一系列文章中，我们将首先研究…</p></div></div>    
</body>
</html>