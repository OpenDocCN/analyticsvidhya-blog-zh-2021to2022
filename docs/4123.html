<html>
<head>
<title>Facial Expression Recognition &amp; Comparative Study on Densenet161 and Resnet152</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人脸表情识别&amp; dense net 161和Resnet152的比较研究</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/facial-expression-recognition-comparative-study-on-densenet161-and-resnet152-e88ee02b6734?source=collection_archive---------3-----------------------#2021-08-29">https://medium.com/analytics-vidhya/facial-expression-recognition-comparative-study-on-densenet161-and-resnet152-e88ee02b6734?source=collection_archive---------3-----------------------#2021-08-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/2510bdcb5726021c1c567e25179f9bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0mScfREzbuPywmJl6juwFw.png"/></div></div></figure><div class=""/><p id="df06" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jn">利用</em> <a class="ae jo" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank"> <em class="jn">深度学习</em> </a> <em class="jn">，</em><a class="ae jo" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"><em class="jn">py torch</em></a><em class="jn">，</em> <a class="ae jo" href="https://en.wikipedia.org/wiki/Transfer_learning" rel="noopener ugc nofollow" target="_blank"> <em class="jn">迁移学习</em> </a></p><p id="2f75" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi jp translated">面部表情识别可以作为人们可能希望包含在计算机视觉系统中的分类工作之一。我们项目的工作将是通过一个将被用作机器眼睛的相机进行观察，并根据人当前的表情/情绪对人的面部(如果有的话)进行分类。</p><p id="2ddd" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">人脸识别是一种利用人脸识别或验证个人身份的方法。这是最重要的<a class="ae jo" href="https://viso.ai/applications/computer-vision-applications/" rel="noopener ugc nofollow" target="_blank">计算机视觉应用</a>之一，具有巨大的商业利益。最近，人脸识别技术随着基于深度学习的方法而大大进步。</p><p id="bc44" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在不受约束的记录条件下捕获的静态图像和视频序列中的人脸识别是计算机视觉中最广泛研究的课题之一，因为它在监控、执法、生物计量、市场营销等方面有广泛的应用。</p><h1 id="3e6a" class="jy jz hs bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">深度人脸识别的历史:</h1><ul class=""><li id="fdb7" class="kw kx hs ir b is ky iw kz ja la je lb ji lc jm ld le lf lg bi translated">在20世纪90年代早期，随着历史特征脸方法的引入，人脸识别开始流行。在20世纪90年代和21世纪初，<a class="ae jo" href="https://doi.org/10.1109/IC3I.2014.7019610" rel="noopener ugc nofollow" target="_blank">整体方法</a>主导了人脸识别社区。整体方法通过某些分布假设导出低维表示，例如线性子空间、流形和稀疏表示。整体方法的问题在于它们无法解决偏离其先前假设的不受控制的面部变化。这导致了21世纪初基于局部特征的人脸识别的发展。</li><li id="3a9f" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm ld le lf lg bi translated">在2000年代早期和2010年代，引入了基于<a class="ae jo" href="http://azadproject.ir/wp-content/uploads/2014/07/2012-Monogenic-Binary-Coding-An-Efficient-Local-Feature-Extraction-Approach-to-Face-Recognition.pdf" rel="noopener ugc nofollow" target="_blank">局部特征</a>的人脸识别和基于<a class="ae jo" href="http://mmlab.ie.cuhk.edu.hk/archive/2010/cvpr10_face.pdf" rel="noopener ugc nofollow" target="_blank">学习的局部描述符</a>。使用<a class="ae jo" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.1199&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank"> Gabor滤波器</a>和<a class="ae jo" href="https://www.researchgate.net/deref/http%3A%2F%2Fdx.doi.org%2F10.1007%2F978-3-540-30548-4_21" rel="noopener ugc nofollow" target="_blank">局部二进制模式(LBP) </a>以及它们的多级和高维扩展的人脸识别，通过局部滤波的一些不变特性实现了鲁棒的性能。不幸的是，手工制作的特征缺乏独特性和紧凑性。在2010年代早期，<a class="ae jo" href="http://mmlab.ie.cuhk.edu.hk/archive/2010/cvpr10_face.pdf" rel="noopener ugc nofollow" target="_blank">基于学习的局部描述符</a>被引入人脸识别，其中局部滤波器被学习以获得更好的区别性，编码码本被学习以获得更好的紧凑性。</li><li id="facf" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm ld le lf lg bi translated">2014年，脸书的<a class="ae jo" href="https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/" rel="noopener ugc nofollow" target="_blank"> DeepFace </a>和<a class="ae jo" href="https://www.ee.cuhk.edu.hk/~xgwang/DeepID.pdf" rel="noopener ugc nofollow" target="_blank"> DeepID </a>在著名的<a class="ae jo" href="http://vis-www.cs.umass.edu/lfw/" rel="noopener ugc nofollow" target="_blank">标记的野生(LFW) </a>人脸基准上取得了最先进的精度，首次超越了人类在无约束场景下的表现。从那以后，研究重点转移到了基于深度学习的方法。深度学习方法使用多层处理单元的级联来进行特征提取和转换。因此，已经开发了更大规模的人脸数据库和高级人脸处理技术来促进深度人脸识别。因此，随着表示管道变得更深，LFW(标记为野外人脸)性能从大约60%稳步提高到97%以上。</li></ul><h1 id="9734" class="jy jz hs bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">人脸识别和深度学习:</h1><p id="c1f3" class="pw-post-body-paragraph ip iq hs ir b is ky iu iv iw kz iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">深度学习，特别是深度<a class="ae jo" href="https://viso.ai/deep-learning/artificial-neural-network/" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a> (CNN)，在人脸识别方面受到了越来越多的关注，已经提出了几种<a class="ae jo" href="https://viso.ai/deep-learning/what-is-deep-learning/" rel="noopener ugc nofollow" target="_blank">深度学习</a>方法。</p><p id="e984" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">自2014年以来，深度学习技术重塑了人脸识别的研究格局，这是由<a class="ae jo" href="https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/" rel="noopener ugc nofollow" target="_blank"> DeepFace </a>和<a class="ae jo" href="https://www.ee.cuhk.edu.hk/~xgwang/DeepID.pdf" rel="noopener ugc nofollow" target="_blank"> DeepID </a>方法的突破发起的。从那时起，深度人脸识别技术(利用分层架构来学习有区别的人脸表示)极大地提高了最先进的性能，并培养了许多成功的现实世界应用。深度学习应用多个处理层来学习具有多级特征提取的数据表示。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lp"><img src="../Images/c2dc6948999b289e69f86e499b9b6e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ebRrH_ZSUnLuxzgS.jpg"/></div></div></figure><h1 id="d5de" class="jy jz hs bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">数据集:</h1><p id="ef55" class="pw-post-body-paragraph ip iq hs ir b is ky iu iv iw kz iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">我在<a class="ae jo" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上找到了这个数据集，它代表了一个图像分类问题；因为数据由六个类别组成，每个类别包含3000-7000张图片，六个类别为“快乐”、“惊讶”、“悲伤”、“恐惧”、“愤怒”、“中性”。</p><blockquote class="lu lv lw"><p id="c46a" class="ip iq jn ir b is it iu iv iw ix iy iz lx jb jc jd ly jf jg jh lz jj jk jl jm ha bi translated"><strong class="ir ht">数据集链接:</strong></p><p id="0ed1" class="ip iq jn ir b is it iu iv iw ix iy iz lx jb jc jd ly jf jg jh lz jj jk jl jm ha bi translated"><a class="ae jo" href="https://www.kaggle.com/apollo2506/facial-recognition-dataset" rel="noopener ugc nofollow" target="_blank"><strong class="ir ht">https://www . ka ggle . com/Apollo 2506/面部识别数据集</strong> </a></p></blockquote><h1 id="ecc4" class="jy jz hs bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">描述:</h1><p id="da71" class="pw-post-body-paragraph ip iq hs ir b is ky iu iv iw kz iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">愤怒类有3995张图片。悲伤类有4830张图片。惊喜类有3171张图片。恐惧类有4097张图片。快乐类有7215张图片。中性类别有4965张图片。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ma"><img src="../Images/bbf8d85ad29f7ba6c60cba6eb8366bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fTrirra0c7ATTkrmcRaMxg.png"/></div></div></figure><h1 id="dce9" class="jy jz hs bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">程序:</h1><ol class=""><li id="86e2" class="kw kx hs ir b is ky iw kz ja la je lb ji lc jm mb le lf lg bi translated">下载数据集</li><li id="4147" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm mb le lf lg bi translated">导入库</li><li id="fded" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm mb le lf lg bi translated">GPU实用程序</li><li id="6996" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm mb le lf lg bi translated">创建自定义PyTorch数据集</li><li id="e7f3" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm mb le lf lg bi translated">创建训练集和验证集</li><li id="692c" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm mb le lf lg bi translated">PyTorch数据加载器</li><li id="013f" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm mb le lf lg bi translated">一些例子</li><li id="d483" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm mb le lf lg bi translated">修改预训练模型(DenseNet161，ResNet152)</li><li id="46c8" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm mb le lf lg bi translated">迁移学习</li><li id="b965" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm mb le lf lg bi translated">训练循环</li><li id="2ee7" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm mb le lf lg bi translated">微调预训练模型</li><li id="1450" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm mb le lf lg bi translated">结果</li><li id="09bf" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm mb le lf lg bi translated">根据测试数据集测试模型以检查准确性</li><li id="f1fd" class="kw kx hs ir b is lh iw li ja lj je lk ji ll jm mb le lf lg bi translated">保存训练好的模型</li></ol><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/543d264b979eae39645df16f7b3a5040.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EEl8P_3PwR-ofbXeBe_IBg.png"/></div></div></figure><h1 id="205d" class="jy jz hs bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">修改预训练模型(DenseNet161):</h1><p id="d33c" class="pw-post-body-paragraph ip iq hs ir b is ky iu iv iw kz iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">最近的工作表明，如果卷积网络在靠近输入的层和靠近输出的层之间包含较短的连接，则卷积网络可以训练得更深入、更准确和有效。在本文中，我们接受这种观察，并介绍了密集卷积网络(DenseNet)，它以前馈方式将每一层与每一层连接起来。传统的L层卷积网络有L个连接——每层与其后续层之间有一个连接——而我们的网络有L(L+1)/2个直接连接。对于每一层，所有先前层的特征地图被用作输入，并且它们自己的特征地图被用作所有后续层的输入。DenseNets有几个引人注目的优点:它们缓解了消失梯度问题，加强了特征传播，鼓励特征重用，并大大减少了参数的数量。我们在四个极具竞争力的目标识别基准任务(CIFAR-10、CIFAR-100、SVHN和ImageNet)上评估了我们提出的架构。DenseNets在大多数方面都取得了显著的进步，同时需要更少的内存和计算来实现高性能。</p><p id="3da2" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">作者:黄高，刘庄，基里安·q·温伯格，劳伦斯·范德马滕</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mc"><img src="../Images/f69794883a84aba9909295349464731f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*x37oN_kC5z0sD-rI.jpg"/></div></div></figure><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es md"><img src="../Images/b7a6ae6b546f0e2ad37600828e4e9ac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8u_aFzHgNyW3a1ENM0BoTg.jpeg"/></div></div></figure><h1 id="b280" class="jy jz hs bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">修改预训练模型(ResNet152):</h1><p id="1d09" class="pw-post-body-paragraph ip iq hs ir b is ky iu iv iw kz iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">更深层次的神经网络更难训练。我们提出了一个剩余学习框架来简化比以前使用的网络更深入的网络训练。我们明确地将这些层重新表述为学习关于层输入的剩余函数，而不是学习未被引用的函数。我们提供了全面的经验证据，表明这些残差网络更容易优化，并可以从显著增加的深度获得准确性。在ImageNet数据集上，我们评估了深度高达152层的残差网络——比VGG网络深8倍，但复杂度仍然较低。</p><p id="7d1c" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这些残差网络的集合在ImageNet测试集上实现了3.57%的误差。该结果在ILSVRC 2015分类任务中获得第一名。我们还对具有100层和1000层的CIFAR-10进行了分析。</p><p id="d492" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">表征的深度对于许多视觉识别任务是至关重要的。仅仅由于我们非常深入的表示，我们在COCO对象检测数据集上获得了28%的相对改进。深度残差网络是我们提交给ILSVRC和COCO 2015竞赛的基础，在该竞赛中，我们还在ImageNet检测、ImageNet定位、COCO检测和COCO分割任务中获得了第一名。</p><p id="1903" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">作者:，何，，，任，</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es me"><img src="../Images/4d99f6480ad415c28a7b6f32f1346ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5Dx4cUwjE-tlVqYo.png"/></div></div></figure><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mf"><img src="../Images/da5c1d777416e6816eeeab3dd50a9f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*p-Iws916-UReCrrQ.png"/></div></div></figure><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es mg"><img src="../Images/10f9215ad30b760c3191bc8760166687.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/0*XSpJD10nN-rJ49GR.png"/></div></figure><h1 id="169d" class="jy jz hs bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">Densenet161和Resnet152的比较研究；</h1><p id="dae6" class="pw-post-body-paragraph ip iq hs ir b is ky iu iv iw kz iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">ResNet还是DenseNet？如今，大多数基于深度学习的方法都是通过开创性的主干网络实现的，其中最著名的两个可以说是ResNet和DenseNet。尽管它们的竞争性能和压倒性的受欢迎程度，但它们都存在固有的缺点。对于ResNet来说，稳定训练的身份快捷方式也限制了它的表示能力，而DenseNet通过多层特征串联具有更高的能力。然而，密集的串联导致了需要高GPU内存和更多训练时间的新问题。部分由于这个原因，在ResNet和DenseNet之间做出选择并不容易。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es mh"><img src="../Images/c846a149495a77ff35cd306c29431da3.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*t1hx6E2cAeLuE8firzTwKw.jpeg"/></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">剩余单元框图</figcaption></figure><p id="f477" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">它们的实现是深度剩余网络(ResNet ),具有跳过一定层数的快捷连接。在执行身份映射之后，这些连接的输出和堆叠层的输出被相加。根据他们的结论，剩余网络易于优化，并从深度增加中受益匪浅。他们的152层单一模型(当时ImageNet上最深的模型)在ILSVRC 2015分类竞赛中获得第一名，其复杂性明显低于VGG网络。此外，ResNet152以0.94的前5名准确度优于以前的系综。为了避免过度拟合，在小数据集上训练更深层次的网络时，建议进行更激烈的正则化</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es mm"><img src="../Images/2dd51feafa99d8617bc9d96a15d4ec32.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*ZTavPELtD_oMcDosdmWw4g.jpeg"/></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">瓶颈设计</figcaption></figure><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mn"><img src="../Images/307cd8902108e25dd7affb3fac72f847.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*pFnFhkJr-LZ9MVFj8jXTIw.jpeg"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">DenseNet-B</figcaption></figure><p id="a54c" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">与ResNets相比，使用复合函数将输入连接起来，从而得到更简单、更高效的解决方案。DenseNets被划分为多个密集块，由执行批量标准化、卷积和池化的过渡层连接，更加紧凑并促进特征重用；瓶颈层和压缩层用于减少特征图的数量，提高计算效率。此外，由于每层的过滤器数量相对较少，DenseNets易于训练和扩展到数百层，同时不会引起优化问题。他们的研究结果表明，DenseNets实现了与ResNets相似的性能，而所需的参数却少得多。DenseNet201在ImageNet数据集上实现了0.93的前5名精度，并被认为通过超参数调整获得了进一步的增益；过度拟合问题通过连接的正则化效果来解决。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es mo"><img src="../Images/6307a0112487257b6fdf092485fc8263.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*5pBcwtKpzGl_5T1FRjfYEg.jpeg"/></div></figure><p id="eb1e" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在ILSVRC中竞争意味着将图像分类到ImageNet数据库的1，000个类别中的一个。从搜索引擎收集的大约150，000张图像用于验证和测试，每种算法产生一个按置信度递减排序的标签列表。PyTorch是深度学习社区中一个流行的科学计算机库，具有简单的调试和对硬件加速器的支持。它提供了解决图像分类的通用图像变换和模型架构。表1提供了所选预训练模型的前1名和前5名错误率。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es mp"><img src="../Images/e55484a75fb975eb38e0dc53420479ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*j3sFcDvKgn40yfSwsfUPLQ.jpeg"/></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">ImageNet上的训练曲线。</figcaption></figure><p id="956c" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">提出的密集加权归一化捷径也有利于加快收敛速度。另外需要注意的是，DS2Net的训练误差要小得多。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es mq"><img src="../Images/9b8b7184ab04289cc3ca901488d67893.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*Q0Wv2kA00Mab8emiMuvFjQ.jpeg"/></div></figure><p id="4069" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们提供了稠密求和的统一视角，以便于理解ResNet和DenseNet之间的核心区别。我们证明，核心差异在于卷积参数是否为前面的特征图所共享。我们提出了一种密集加权规范化快捷方式作为替代的密集连接方法，它优于现有的两种密集连接技术:ResNet中的相同快捷方式和DenseNet中的密集级联。我们发现来自聚合输出的密集求和提供了优于来自卷积块输出的性能。简而言之，密集快捷方式解决了ResNet中表示能力下降的问题，同时避免了DenseNet中需要更多GPU资源的缺点。</p><h1 id="d9b9" class="jy jz hs bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">迁移学习:</h1><p id="e04e" class="pw-post-body-paragraph ip iq hs ir b is ky iu iv iw kz iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">迁移学习的基本前提很简单:采用一个在大型数据集上训练的模型，并将其知识转移到一个较小的数据集。对于使用CNN的对象识别，我们冻结网络的早期卷积层，并且仅训练做出预测的最后几层。这个想法是，卷积层提取适用于图像的一般、低级特征，如边缘、图案、梯度，后面的层识别图像中的特定特征，如眼睛或车轮。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es mr"><img src="../Images/2ee5d7359e1fa9a5fdb08d6bd00cec20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*I4hdRPcDNFcOZTIf.jpeg"/></div></figure><h1 id="5267" class="jy jz hs bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">结果:</h1><p id="b708" class="pw-post-body-paragraph ip iq hs ir b is ky iu iv iw kz iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">以下是的结果</p><h2 id="e050" class="ms jz hs bd ka mt mu mv ke mw mx my ki ja mz na km je nb nc kq ji nd ne ku nf bi translated">1.Resnet152:</h2><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es ng"><img src="../Images/95900d8f494ad72eef17694229de9020.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*ATgOZDttH6gHLi7ZA7O_aw.png"/></div></figure><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es nh"><img src="../Images/fb71d489551d5332897d007a912bb21b.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*1nOGjVJu-ibRN5TKs7fyWw.png"/></div></figure><p id="baba" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">从历元对损失图中，我们可以看到，首先，训练数据和测试数据同时减少，直到历元15。之后，训练图线开始增加一段时间，而验证图线开始平行。</p><p id="5edb" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最初，培训和验证损失似乎都随着时间的推移而减少。但是，如果你训练模型的时间足够长，你会注意到训练损失持续减少，而验证损失停止减少，甚至在某个点之后开始增加！</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ni"><img src="../Images/329b9691b1e6f7c7c6834c7c263c1c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*p3F28L8rAi98XIdg.png"/></div></div></figure><p id="8b9e" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这种现象被称为过度拟合，这是许多机器学习模型在真实世界数据上给出相当糟糕的结果的首要原因。这是因为模型为了尽量减少损失，开始学习训练数据特有的模式，有时甚至会记住特定的训练示例。因此，该模型不能很好地推广到以前未见过的数据。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es nj"><img src="../Images/90268ca3999ef00e91321e390dcc5a7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*Kss6j1MMs5hxjAZzQUQvzQ.png"/></div></figure><h2 id="ea19" class="ms jz hs bd ka mt mu mv ke mw mx my ki ja mz na km je nb nc kq ji nd ne ku nf bi translated">2.Densenet161:</h2><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es nh"><img src="../Images/56a2c28af5e3c0e4fa7224c32e721cab.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*-NAE3T2SsBZaqG_tQaMMgg.png"/></div></figure><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es nk"><img src="../Images/97bd4f1df9737d7d6c74ac94a567950a.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*K6Xaoi-TOFHhBmOhpyal9Q.png"/></div></figure><p id="50ef" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">从历元对损失图中，我们可以看到，首先，训练数据和测试数据都同时减少，直到历元5。之后，训练图线开始增加一段时间，而验证图线平行进行。</p><h1 id="8150" class="jy jz hs bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">结论:</h1><blockquote class="lu lv lw"><p id="0d81" class="ip iq jn ir b is it iu iv iw ix iy iz lx jb jc jd ly jf jg jh lz jj jk jl jm ha bi translated">1.面部表情识别的训练比我想象的要困难得多，一些表情非常相似，当试图识别某些表情时，似乎会产生更多的错误。</p><p id="70ff" class="ip iq jn ir b is it iu iv iw ix iy iz lx jb jc jd ly jf jg jh lz jj jk jl jm ha bi translated">2.也许更大尺寸的图片调整和随机裁剪可以给出更好的结果，但为了达到最高质量，GPU是非常必要的，否则它会给出一个“内存不足”的错误。</p><p id="8f26" class="ip iq jn ir b is it iu iv iw ix iy iz lx jb jc jd ly jf jg jh lz jj jk jl jm ha bi translated">3.我也尝试了不同的学习速度和其他方法。也试过DenseNet，不过ResNet更好。</p></blockquote><h1 id="1205" class="jy jz hs bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">项目链接:</h1><blockquote class="lu lv lw"><p id="19cc" class="ip iq jn ir b is it iu iv iw ix iy iz lx jb jc jd ly jf jg jh lz jj jk jl jm ha bi translated"><a class="ae jo" href="https://github.com/soham2707/Facial-Expression-Recognition-Using-Deep-Learning.git" rel="noopener ugc nofollow" target="_blank">https://github . com/soham 2707/face-Expression-Recognition-Using-Deep-learning . git</a></p></blockquote><h1 id="0380" class="jy jz hs bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">未来工作:</h1><p id="1e72" class="pw-post-body-paragraph ip iq hs ir b is ky iu iv iw kz iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">由于结果不是很令人满意，我将尝试使用迁移学习方法进行另一种图像分类。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es nl"><img src="../Images/51e73a2359079aeb4deb023588acfdc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OuiamACBJGuFKXpS"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">伊恩·施耐德在<a class="ae jo" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure></div></div>    
</body>
</html>