<html>
<head>
<title>Why gradient descent doesn’t converge with unscaled features?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么梯度下降不收敛于未缩放的特征？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/why-gradient-descent-doesnt-converge-with-unscaled-features-8b7ed0c8cab6?source=collection_archive---------9-----------------------#2021-03-01">https://medium.com/analytics-vidhya/why-gradient-descent-doesnt-converge-with-unscaled-features-8b7ed0c8cab6?source=collection_archive---------9-----------------------#2021-03-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c576" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有没有对这个众所周知的公理感到好奇:“<em class="jd">总是缩放你的特征”？</em>好吧，请继续阅读，获得快速的图形和直观的解释！</p><h1 id="f331" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">动机</h1><p id="b906" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">我相信我们所有人都见过机器学习中的这条流行的公理:<em class="jd">在训练之前，永远要缩放你的特征！</em> <br/>虽然我们大多数人都知道它的实际重要性，但却没有多少人意识到潜在的数学原因。</p><p id="7060" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇超级短的博客中，我解释了在我们最喜欢的梯度下降算法中，当输入的特征具有非常不同的量级时会发生什么。</p><h1 id="3e58" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">通过示例了解</h1><p id="06dd" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">假设我们试图使用两个数字预测变量/特征来预测一个人的预期寿命(以年为单位):x1和x2，其中x1是这个人的年龄，x2是他/她的工资。Cleary，x1 &lt;&lt; x2.</p><p id="ae79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">This is a regression problem where we aim to learn the weights theta1 and theta2 for x1 and x2 respectively by minimizing the cost function — Mean Squared Error (MSE).</p><p id="46d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">If we plot theta1, theta2, and cost:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/84270b49c33a59b46f5ed483be5ea337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fn7LI7Hgp9GFL92rY20zYA.jpeg"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated"><strong class="bd jg">左:</strong>具有缩放特征的成本函数<strong class="bd jg">右:</strong>具有未缩放特征的成本函数(在较小幅度特征的方向上拉长)</figcaption></figure><p id="fe54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">左图:</em> <strong class="ih hj">带特征缩放</strong> <br/>代价函数为正圆(在2D)或半球(在3D)。梯度下降能够在更短的时间内容易地达到最小值(中心)。</p><p id="bde4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">右图:</em> <strong class="ih hj">无特征缩放</strong> <br/>代价函数变成一个椭圆，向较小幅度特征方向<strong class="ih hj">拉伸/延伸。</strong></p><p id="f3d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ️Extended方向➡坡度越小➡到达最小值所需的步数越多</strong></p><p id="3bfa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于x1(年龄)在数量级上小于x2(薪水)，所以θ1需要更大的变化来影响成本函数。为了使θ1的值发生甚至很小的变化，梯度下降需要在水平方向上行进更长的距离，即在这种情况下x1的方向。</p><blockquote class="kx"><p id="2f11" class="ky kz hi bd la lb lc ld le lf lg jc dx translated">理想情况下，我们希望:<br/> -在小梯度方向上快速移动，而<br/> -在大梯度方向上缓慢移动</p></blockquote><p id="3119" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">⭐️ <em class="jd">因此，梯度下降需要特征缩放以易于收敛。</em></p><p id="3696" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意<strong class="ih hj">不是所有的ML算法都需要特征缩放。</strong>例如，在训练决策树或随机森林模型之前，不强制缩放特征。这是因为他们的成本函数(基尼指数，熵等。)不是基于距离的(即不需要梯度下降)，因此它不受特征比例的影响。你可以在这里阅读更多关于这个<a class="ae lm" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">的内容。</a></p><h1 id="cfda" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">摘要</h1><p id="60af" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">没有特征缩放，梯度下降将需要更多的<em class="jd">步骤</em>来达到最小值。换句话说，梯度下降将花费大量时间来收敛，从而增加模型训练时间。<br/>为了避免这种情况，<strong class="ih hj">使用基于距离的成本函数(如MSE、KMeans、SVM等)时，建议使用特征缩放。).</strong></p><p id="4dee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您已经阅读了这么多，并且发现这篇文章很有用，那么请点击👏。这对我保持动力大有帮助，谢谢！</p></div></div>    
</body>
</html>