<html>
<head>
<title>Multi-Model Endpoints with Hugging Face Transformers and Amazon SageMaker</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多模型终端，带拥抱脸变形器和亚马逊SageMaker</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multi-model-endpoints-with-hugging-face-transformers-and-amazon-sagemaker-c0e5a3693fac?source=collection_archive---------2-----------------------#2022-07-02">https://medium.com/analytics-vidhya/multi-model-endpoints-with-hugging-face-transformers-and-amazon-sagemaker-c0e5a3693fac?source=collection_archive---------2-----------------------#2022-07-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="2e90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">#拥抱脸# AWS #伯特#GPT-2 #SageMaker #Mlops</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/7045d2c765064c20056e52dcafb40286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*05oYAlvm7t86tCAbvLmuzw.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图片来自<a class="ae js" href="https://www.mydomaine.com/dragon-tree-care-4797058" rel="noopener ugc nofollow" target="_blank">my domain</a></figcaption></figure><p id="dc72" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以通过<a class="ae js" href="https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html" rel="noopener ugc nofollow" target="_blank"> Amazon SageMaker多模型端点</a>托管多达数千种模型。</p><p id="9df9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在某些情况下，可变延迟是可以容忍的，并且成本优化更重要，我们也可以决定使用Mme进行A/B/n测试，而不是这里讨论的更典型的基于<a class="ae js" href="https://aws.amazon.com/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/" rel="noopener ugc nofollow" target="_blank">生产变量的策略</a>。</p><p id="fa16" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每当应用程序需要一致的低推理延迟时，实时端点仍然是最佳选择。</p><p id="bc32" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将使用拥抱脸推理DLCs和Amazon SageMaker来部署多个transformer模型作为多模型端点。Amazon SageMaker多型号端点可用于提高端点利用率和优化成本。</p><p id="3cb0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章演示了如何在一个端点后的一个容器中托管2个预训练的变压器模型。</p><ol class=""><li id="fbbd" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">使用BERT模型从文本中提取嵌入内容。</li><li id="079a" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">使用GPT-2模型为给定文本生成合成文本。</li></ol><p id="8baa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">注意</strong>:在撰写本文时，多模型端点仅支持<code class="du kh ki kj kk b">CPU</code>个实例。</p><p id="72a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">开发环境和权限</strong></p><p id="1d11" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意:您可以在SageMaker Studio、您的本地机器或SageMaker笔记本实例中运行这个演示</p><p id="e8c3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果您打算在本地环境中使用SageMaker(而不是SageMaker Studio或Notebook实例)。您需要访问具有SageMaker所需权限的IAM角色。你可以在这里找到更多关于它的信息。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kl"><img src="../Images/a66ddf3545df7f83f528f384993127eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*t-Dp1iuKSorhXaOE.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">开发环境和权限(作者截图)</figcaption></figure><p id="ede6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">检索模型工件</strong></p><p id="6a54" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><code class="du kh ki kj kk b"><strong class="ig hi">BERT model</strong></code></p><p id="0938" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，我们将下载预训练<a class="ae js" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>模型的模型工件。BERT是一种流行的自然语言处理(NLP)模型，它从文本中提取含义和上下文。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es km"><img src="../Images/647f8ebd21a0d28935bd11496932d355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*BYEv5bKP616sAVmc.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">检索BERT模型工件(作者截图)</figcaption></figure><p id="379e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><code class="du kh ki kj kk b"><strong class="ig hi">GPT-2 model</strong></code></p><p id="f3ea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其次，我们将下载预训练的<a class="ae js" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>模型的模型工件。GPT-2是一个流行的文本生成模型，由OpenAI开发。给定一个文本提示，它可以生成合成文本。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kn"><img src="../Images/4b1f0a4e703d8eb3ecc94f91592ad8d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Etod7Gowo6BK2gsZ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">找回GPT-2模型文物(作者截图)</figcaption></figure><p id="e181" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">编写推理脚本</strong></p><p id="500d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><code class="du kh ki kj kk b"><strong class="ig hi">BERT model</strong></code></p><p id="06af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于我们要将一个模型带到SageMaker，所以我们必须创建一个推理脚本。脚本将在我们的拥抱脸容器中运行。我们的脚本应该包括一个用于模型加载的函数，以及可选的生成预测和输入/输出处理的函数。拥抱脸容器提供了生成预测和输入/输出处理的默认实现。通过在脚本中包含这些函数，您可以覆盖默认函数。你可以在这里找到更多的<a class="ae js" href="https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#serve-a-pytorch-model" rel="noopener ugc nofollow" target="_blank">细节</a>。</p><p id="76eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">注意</strong>:</p><p id="50a0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1.<strong class="ig hi">单一模型部署</strong>:为了在容器启动时安装额外的库，我们可以添加一个requirements.txt文本文件，指定要使用pip安装的库。在存档中，拥抱脸容器期望所有的推理代码和requirements.txt文件都在<code class="du kh ki kj kk b"><strong class="ig hi">code/</strong></code>目录中。</p><p id="e91d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.<strong class="ig hi">多模型部署</strong>:要在容器上安装额外的库，需要使用推理脚本中的pip来安装requirements.txt文本文件中的库。在档案中，拥抱脸容器期望所有的推理代码都在<code class="du kh ki kj kk b"><strong class="ig hi">code/</strong></code>目录中。</p><p id="7a3e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下一个单元格中，我们将看到BERT模型的推理脚本，它帮助我们从文本中提取嵌入内容。</p><p id="bb02" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你会注意到它使用了来自拥抱脸的<a class="ae js" href="https://huggingface.co/docs/transformers/index" rel="noopener ugc nofollow" target="_blank">变形库，并在推理脚本中使用pip命令安装，同样，如果需要，我们需要安装额外的库。</a></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ko"><img src="../Images/a699240630ebb4d5cf83a5a304cadfa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*r4H6oFwE9J4N70ka.png"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kp"><img src="../Images/6d3946785b69591e8233c6f34f5fd176.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fBqUQEozApeUOxfg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">BERT模型的推理脚本(作者截图)</figcaption></figure><p id="4ca9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><code class="du kh ki kj kk b"><strong class="ig hi">GPT-2 model</strong></code></p><p id="b410" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下一个单元格中，我们将看到GPT-2模型的推理脚本，它帮助我们为给定文本生成合成文本。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kq"><img src="../Images/9d38f826ab1d656038b2e2bf8af5eb8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*28tojhaAW11JL70O.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">GPT-2模型的推理脚本(作者截图)</figcaption></figure><p id="25b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">包装型号</strong></p><p id="374e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于托管，SageMaker要求部署包以兼容的格式构建。它希望所有文件都打包在一个名为“model.tar.gz”(我们称之为bertmodel.tar.gz和gptmodel.tar.gz)的tar文件中，并使用gzip压缩。在存档中，拥抱脸容器期望所有的推理代码文件都在代码/目录中。有关所需目录结构的详细解释，请参见此处的指南。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kr"><img src="../Images/2e0464a331a9dc99d56c6a695f1d4ffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Wl-v3b0WQZCv_AMb.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">包装模型(作者截图)</figcaption></figure><p id="0570" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">上传多张拥抱脸模型给S3 </strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ks"><img src="../Images/512a77547eb6d5b91d012bf8bb55ff4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yM7wQOjFVVOQ3Ie_.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">上传模特到S3(作者截图)</figcaption></figure><p id="68ff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">创建多模型端点</strong></p><p id="a775" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将BERT模型上传到S3后，我们就可以部署端点了。要使用boto3创建/部署实时端点，您需要创建一个“SageMaker模型”、“SageMaker端点配置”和“SageMaker端点”。“SageMaker模型”包含我们的多模型配置，包括我们的S3路径，其中我们上传/部署多个拥抱脸模型。“SageMaker端点配置”包含端点的配置。“SageMaker端点”是实际的端点。</p><p id="d242" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">验证docker文件中的<code class="du kh ki kj kk b">multi-models</code>标签，以表明任何预先构建的容器能够同时装载和服务多个型号。</p><p id="8af7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">标签<code class="du kh ki kj kk b">com.amazonaws.sagemaker.capabilities.multi-models=true</code></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kt"><img src="../Images/2cafee9f7e96422e1e69a7e7dad5e43d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*j7QxsaoiQuDWf2LQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">定义DLC(作者截图)</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ku"><img src="../Images/be61930e7e3889e555a278beb753b60e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dtEdwu0Q7oF70IpK.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">创建SM端点配置和端点(作者截图)</figcaption></figure><p id="0da8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">获得预测</strong></p><p id="b440" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><code class="du kh ki kj kk b"><strong class="ig hi">BERT model</strong></code></p><p id="ee79" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们的API端点已经部署好了，我们可以向它发送文本来从我们的BERT模型中获取预测。您可以使用SageMaker SDK或SageMaker运行时API来调用端点。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kv"><img src="../Images/5b3d0447e1be3f0ce3acb0c14e699f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PV33Aw3UoS07iEb1.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">从BERT模型获得预测(作者截图)</figcaption></figure><p id="84e3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><code class="du kh ki kj kk b"><strong class="ig hi">GPT-2 model</strong></code></p><p id="8ad0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">既然我们的RESTful API端点已经部署好了，我们可以向它发送文本，以从我们的GPT-2模型中获取预测。您可以使用SageMaker Python SDK或SageMaker运行时API来调用端点。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kw"><img src="../Images/aa8679c53db003766fb91a9027eb9528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Uv2RUXM4hrIfdnac.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">从GPT-2模型获得预测(作者截图)</figcaption></figure><p id="9374" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">向端点动态部署模型和更新模型</strong></p><p id="de65" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了动态地部署模型和更新模型，您将遵循与上面相同的方法，并将其添加为一个新模型。例如，如果您已经重新训练了<code class="du kh ki kj kk b">bertmodel.tar.gz</code>模型，并且想要开始调用它，那么您将会在下面的S3前缀后面上传更新的模型工件，并使用一个新的名称，比如<code class="du kh ki kj kk b">bertmodel_v2.tar.gz</code>，然后将<code class="du kh ki kj kk b">TargetModel</code>字段更改为调用<code class="du kh ki kj kk b">bertmodel_v2.tar.gz</code>而不是<code class="du kh ki kj kk b">bertmodel.tar.gz</code></p><pre class="jd je jf jg fd kx kk ky kz aw la bi"><span id="2091" class="lb lc hh kk b fi ld le l lf lg">multimodels_path = ‘s3://sagemaker-us-east-2-(account-id)/huggingface-multimodel-deploy/models/’</span></pre><p id="7726" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您应该避免在Amazon S3中覆盖模型工件，因为旧版本的模型可能仍然被加载到端点的运行容器或端点上实例的存储卷上:这将导致调用仍然使用旧版本的模型。</p><p id="fef3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">或者，您可以停止端点并重新部署一组新的模型。</p><p id="5408" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">删除多模型端点</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ks"><img src="../Images/310921125c1e93d6dc5598470eeeb692.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OsS6Wj-xu_dqiNK_.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">清理(作者截图)</figcaption></figure><p id="3e4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">结论</strong></p><p id="ad8c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们成功地将两个拥抱脸变形器部署到Amazon SageMaker，使用多模型端点进行推理。多模型端点是优化模型计算利用率和成本的绝佳选择。尤其是当您由于用例差异而拥有独立的推理工作负载时。</p></div><div class="ab cl lh li go lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ha hb hc hd he"><p id="d9b3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章的完整源代码可以在<a class="ae js" href="https://github.com/Vinayaks117/AWS-SageMaker-Examples/blob/main/03_MultiModelEndpointWithHuggingFace/huggingface-sagemaker-multi-model-endpoint.ipynb" rel="noopener ugc nofollow" target="_blank"> github repo </a>找到</p><p id="eb97" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢阅读！！如果你有任何问题，随时联系我。</p></div></div>    
</body>
</html>