# AO3 ç²‰ä¸æŒ–æ˜ç¬¬ä¸‰éƒ¨åˆ†:åŸºäºå†³ç­–æ ‘å’Œèšç±»çš„è‹±æ±‰æ–‡æœ¬åˆ†æ

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/mining-fanfics-on-ao3-part-3-english-chinese-text-analysis-with-decision-tree-clustering-6dcaa0e8a7a6?source=collection_archive---------17----------------------->

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹è¿„ä»Šä¸ºæ­¢ä» AO3 æ”¶é›†çš„æ•°æ®:

```
Title                     object
Author                    object
ID                         int64
Date_updated      datetime64[ns]
Rating                    object
Pairing                   object
Warning                   object
Complete                  object
Language                  object
Word_count                 int64
Num_chapters               int64
Num_comments               int64
Num_kudos                  int64
Num_bookmarks              int64
Num_hits                   int64
Tags                      object
Summary                   object
Date_published    datetime64[ns]
Content                   object
Comments                  object
```

(æ•°æ®æ”¶é›†è¿‡ç¨‹è§[ç¬¬ 1 éƒ¨åˆ†](https://moinmoin150.medium.com/mining-fanfics-on-ao3-part-1-data-collection-eac8b5d7a7fa))

åœ¨ç¬¬ 2 éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹æ”¾åœ¨äº†æ›´ç»“æ„åŒ–çš„æ•°æ®ä¸Šï¼Œå¿½ç•¥äº†æ ‡ç­¾ã€æ‘˜è¦ã€å†…å®¹(å®é™…ä¸Šæ˜¯è™šæ„çš„æ–‡æœ¬)å’Œæ³¨é‡Šã€‚è¿™äº›å°†æ˜¯è¿™ç¯‡æ–‡ç« çš„é‡ç‚¹ã€‚

é™¤äº†ç®€å•çš„è¯é¢‘ç»Ÿè®¡ï¼Œæˆ‘é¦–å…ˆæ¢ç´¢äº†å°†å†³ç­–æ ‘(DT)åº”ç”¨äºæ ‡ç­¾çš„å¯èƒ½æ€§ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å°†å°è¯´åˆ†ä¸ºé«˜æµè¡Œå’Œä½æµè¡Œä¸¤ç±»ã€‚ç„¶åï¼Œæˆ‘å°è¯•äº†å¸¦æ‘˜è¦çš„ k-means èšç±»ã€‚

# å†³ç­–å›¾è¡¨

è¿™æœ¬ç”± [Gustavo Hideo](https://medium.com/u/d7e4530589?source=post_page-----6dcaa0e8a7a6--------------------------------) æ’°å†™çš„ [DT å®æ–½æŒ‡å—](https://towardsdatascience.com/decision-tree-build-prune-and-visualize-it-using-python-12ceee9af752)ç»™äº†æˆ‘å¾ˆå¤§çš„å¸®åŠ©ã€‚ç†è§£ sk learn DT åˆ†ç±»å™¨ä¸­çš„å„ç§å¯ç”¨å‚æ•°å¾ˆæœ‰å¸®åŠ©ã€‚æœ‰ä¸€æ¬¡ï¼Œæˆ‘ä¸å¾—ä¸ç”¨ Python ä»å¤´å¼€å§‹æ„å»º DT æ¨¡å‹ï¼Œä½†å®ƒçš„çµæ´»æ€§å’Œç»†å¾®å·®åˆ«è¿œè¿œä¸åŠ sklearn çš„â€œç½è£…â€æ¨¡å‹æ‰€èƒ½æä¾›çš„æ‰€æœ‰ä¿®å‰ªé€‰é¡¹ã€‚ç®€ç›´æ˜¯å¤©èµè‰¯æœºã€‚

è¿™æ˜¯æˆ‘ä»¬åœ¨è¿™ä¸€éƒ¨åˆ†éœ€è¦çš„ä¸œè¥¿:

```
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.tree import export_graphviz
import pydotplus
import matplotlib.pyplot as plt
import matplotlib.image as pltimg
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np
```

## æ•°æ®æ“ä½œ

ä¸ºäº†åº”ç”¨ DTï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ‰‹å¤´çš„æ–‡æœ¬æ•°æ®åˆ›å»ºè™šæ‹Ÿå˜é‡ã€‚æ­¤å¤–ï¼Œæ ‡ç­¾ä»¥é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²æ ¼å¼å­˜å‚¨ï¼Œä¾‹å¦‚

> G å¯¹æš´åŠ›ã€ä¸»è¦äººç‰©æ­»äº¡ã€å¦ä¸€ä¸ªå®‡å®™ã€POV ç¬¬ä¸€äººç§°ã€äº¤å‰çš„å›¾å½¢æè¿°â€¦â€¦

å› æ­¤ï¼Œåœ¨åº”ç”¨ pandas çš„ get_dummies()å‡½æ•°ä¹‹å‰ï¼Œå®ƒä»¬éœ€è¦ä¸€äº›é¢å¤–çš„å¤„ç†æ¥å°†å®ƒä»¬è½¬æ¢æˆåˆ—è¡¨:

```
df.Tags = df.Tags.apply(lambda x: x.split(','))# first option
t = pd.get_dummies(pd.DataFrame(df.Tags.values.tolist()), prefix_sep='', prefix='')
```

ç¬¬ä¸€ä¸ªé€‰é¡¹æ²¡æœ‰åƒé¢„æœŸçš„é‚£æ ·å·¥ä½œï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒç”¨é‡å¤å€¼æ‰©å¤§äº†åˆ—ã€‚å¯¹äºæœ‰ç»„ç»‡çš„ã€æœ‰åºçš„æˆ–å•å€¼çš„åˆ—ï¼Œå®ƒåº”è¯¥å·¥ä½œå¾—å¾ˆå¥½ï¼Œä½†æ˜¯å¯¹äºä¸åŒé•¿åº¦çš„æ— åºåˆ—è¡¨ï¼Œè€ƒè™‘æ¯ä¸€åˆ—(å‚ç›´å¯¹é½)ï¼Œç„¶åä¾æ¬¡â€œè™šæ‹ŸåŒ–â€å®ƒä»¬ä¼šå¯¼è‡´æ„æƒ³ä¸åˆ°çš„ç»“æœã€‚

```
# second option
t2 = df.Tags.astype(str).str.strip('[]').str.get_dummies(', ')
```

è¿™ç¬¬äºŒä¸ªé€‰é¡¹ä½¿ç”¨ [pdã€‚Series.str.get_dummies()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.get_dummies.html) é‡‡ç”¨äº†ä¸€ç§æ›´å¤æ‚çš„æ–¹æ³•ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹å¯ä»¥æŒ‰é¢„æœŸå·¥ä½œã€‚

ä½ å¯èƒ½æƒ³æ•°ä¸€ä¸‹æœ€å—æ¬¢è¿çš„æ ‡ç­¾ã€‚è™½ç„¶æœ‰è®¸å¤šå…¶ä»–æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½†ç”±äºæˆ‘ä»¬æ‰‹å¤´å·²ç»æœ‰äº†è¿™ä¸ªè™šæ‹Ÿè¡¨ï¼Œæˆ‘ä»¬å¯ä»¥åƒè¿™æ ·æ„å»ºä¸€ä¸ªå­—å…¸:

```
dic = {}
for val, i in zip(t2.sum(axis=0), t2.sum(axis=0).index):
    dic[i] = val
dic
```

æˆ‘ä»¬çš„æ ‘çš„ç‰¹å¾åˆ—è¡¨å¯ä»¥è¢«é¢„å…ˆä¿®å‰ªã€‚æˆ‘è®¤ä¸ºä¸€å¼€å§‹å»æ‰æå…¶ç½•è§çš„æ ‡ç­¾æ˜¯æœ‰æ„ä¹‰çš„ï¼Œä½†è¿™æ˜¯å¦æœ‰å¿…è¦ï¼Œæ˜¯å¦æ˜¯æœ€ä½³å®è·µè¿˜æœ‰å¾…è®¨è®ºã€‚

```
# I examined the number of rare tags and found the cutoff point I'm most happy with
len([key for key, val in dic.items() if val < 30])t2 = t2.drop([key for key, val in dic.items() if val < 30], axis=1)# You may also want to exclude other non-informative tags, such as ['Creator Chose Not To Use Archive Warnings','No Archive Warnings Apply', 'Don't copy to another site'], which turned out to skew the result a lot
```

æˆ‘çš„æ•°æ®æ²¡æœ‰ä»»ä½•é¢„å®šä¹‰çš„åˆ†ç±»æ ‡ç­¾ã€‚å‡ºäºæˆ‘è‡ªå·±çš„æ¢ç´¢ç›®çš„ï¼Œæˆ‘ä»»æ„åˆ›å»ºäº†ä¸€ä¸ªç”±é«˜äºä¸­å€¼çš„å·¥è—¤æ•°å®šä¹‰çš„â€œæˆåŠŸâ€å˜é‡:

```
mark = np.median(df.Num_kudos)
df['Success'] = df.Num_kudos.apply(lambda x: int(x>=mark))
```

ç„¶åï¼Œæˆ‘å°†è¿™ä¸ªå˜é‡ä»¥åŠå…¶ä»–æ„Ÿå…´è¶£çš„å˜é‡æ·»åŠ åˆ°æˆ‘çš„è™šæ‹Ÿè¡¨ä¸­:

```
for_tree = pd.concat([df[['Success','Word_count']],t2], axis=1)
```

## è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹

æˆ‘ä»¬éœ€è¦å®šä¹‰é¢„æµ‹å€¼(x)å’Œé¢„æµ‹å€¼(y ),ç„¶åå°†å®ƒä»¬åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†:

```
X = for_tree.iloc[:,1:]
y = for_tree[['Success']]
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1) #default 75% train
```

æˆ‘ä»¬å¯ä»¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹â€”

```
dt = DecisionTreeClassifier(criterion="entropy", max_depth=5, min_impurity_decrease=0.003)
dt.fit(X_train, y_train)
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å“ªäº›ç‰¹å¾åœ¨åˆ†ç±»ä¸­è¢«è®¤ä¸ºæ˜¯æœ€é‡è¦çš„ï¼Œå³åœ¨åˆ†å‰²åæœ€æœ‰æ•ˆåœ°ä½¿ç»„æ›´çº¯:

```
for i in dt.feature_importances_.argsort()[:-20:-1]: # Top 20
    print(X.columns[i])
```

æˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ£€æŸ¥æ·±åº¦

```
dt.get_depth() # when max_depth is set low (to improve interpretability in my case), it usually reaches the maximum, but in some cases it might not.
```

ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹åœ¨åˆ†ç±»æ–¹é¢çš„è¡¨ç°ï¼Œæ··æ·†çŸ©é˜µå°±æ´¾ä¸Šäº†ç”¨åœº:

```
y_pred = dt.predict(X_test)
a = confusion_matrix(y_test, y_pred)
np.diag(a).sum()/a.sum() # percent of accurately classified fictions
```

## è§£é‡Šç»“æœ

ç”±äº DT åœ¨è§†è§‰è¡¨ç°ä¸Šæœ‰ä¼˜åŠ¿ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›åœ¨æ ‘å½¢å›¾ä¸­çœ‹åˆ°æˆ‘ä»¬çš„æœ€ç»ˆç»“æœï¼Œå¹¶è¯•å›¾è§£é‡Šå®ƒã€‚

```
data = tree.export_graphviz(dt, out_file=None, feature_names=list(X.columns))
graph = pydotplus.graph_from_dot_data(data)
graph.write_png('mydecisiontree.png')# I encounter "ValueError: Program dot not found in path" when running this. Using "brew install" instead of "pip" somehow solved the issue for me.
```

![](img/7ff8699856e92e5c24baa0ba3d00b31a.png)

ç»“æœæ ‘å›¾çš„ä¸€éƒ¨åˆ†

æŒ‰ç…§è¿™ä¸ªå›¾ï¼Œæˆ‘ä»¬åªéœ€è¦è®°ä½ï¼Œå¯¹äºæ ‡ç­¾ä¼ªå˜é‡ï¼Œä¸åŒ…å«å®ƒ(0)å¯¼è‡´å·¦æ ‘ï¼ŒåŒ…å«å®ƒ(1)å¯¼è‡´å³æ ‘ã€‚åœ¨â€œvalue = [aï¼Œb]â€ä¸­ï¼Œa è¡¨ç¤ºâ€œå‡â€(0)çš„æ•°é‡ï¼Œè€Œ b ç»™å‡ºè¿˜æœ‰å¤šå°‘â€œçœŸâ€(1)ã€‚(ç»éªŒæ³•åˆ™:å·¦è¾¹æ€»æ˜¯æ›´å°)

ä»ä¸Šé¢çš„å›¾è¡¨ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå­—æ•°ä¼¼ä¹æ¯”ä»»ä½•æ ‡ç­¾éƒ½é‡è¦ï¼Œå¹¶ä¸”è¯¥æ ‘å°†å°è¯´åˆ†ä¸º 14k å­—ä»¥ä¸‹ã€14k åˆ° 41k ä¹‹é—´å’Œ 41k ä»¥ä¸Šã€‚å¯¹äºä¸åŒé•¿åº¦çš„å°è¯´ï¼ŒåŒ…å«æŸäº›æ ‡ç­¾ä¼šå¯¼è‡´å®Œå…¨ä¸åŒçš„ç»“æœã€‚ä¾‹å¦‚ï¼Œå¯¹äºè¾ƒçŸ­çš„å°è¯´ï¼ŒåŒ…æ‹¬â€œä½³èƒ½åŒæ€§æ‹å…³ç³»â€ä¼¼ä¹æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚ä½†æ˜¯è¿™å†³ä¸æ˜¯è§„å®šæ€§çš„ï¼Œå› ä¸ºæˆ‘ä»¬ä¸èƒ½åœ¨è¿™é‡Œå»ºç«‹å› æœå…³ç³»ã€‚ä½†æˆ‘ä»¬ç¡®å®å¯ä»¥ä»æ•°æ®ä¸­è§‚å¯Ÿåˆ°ä¸€èˆ¬è¯»è€…çš„åå¥½ã€‚

ä½ å¯èƒ½æƒ³çŸ¥é“æˆ‘æ˜¯å¦‚ä½•ä¸º DT åˆ†ç±»å™¨é€‰æ‹©å‚æ•°å€¼çš„ã€‚ä¸ºä»€ä¹ˆæ˜¯ç†µè€Œä¸æ˜¯åŸºå°¼ï¼Ÿäº‹å®ä¸Šï¼Œè¿™äº›é€‰æ‹©å¯èƒ½ä¼šä¸¥é‡å½±å“æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå¾ªç¯æ¥æ”¶é›†ä¸åŒå€¼çš„å½±å“ã€‚ä¾‹å¦‚ï¼Œmin _ infinity _ decrease ä½œä¸ºæ˜¯å¦ä¼šå‘ç”Ÿåˆ†è£‚çš„é˜ˆå€¼æ˜¯éå¸¸æœ‰å½±å“çš„ã€‚æˆ‘ä»¬å¯ä»¥æ£€æŸ¥å¹¶æƒ³è±¡å®ƒçš„æ•ˆæœï¼Œä»¥é€‰æ‹©æœ€ä½³æ–¹æ¡ˆ:

```
min_decrease = []

for i in range(1,10):
    dtree = DecisionTreeClassifier(criterion='entropy', max_depth=20, min_impurity_decrease=i/1000)
    dtree.fit(X_train, y_train)
    pred = dtree.predict(X_test)
    acc_entropy.append(accuracy_score(y_test, pred))

    min_decrease.append(i)d = pd.DataFrame({'acc_entropy':pd.Series(acc_entropy), 'min_decrease':pd.Series([i/1000 for i in min_decrease])})
plt.plot('min_decrease','acc_entropy', data=d, label='entropy')
plt.xlabel('min_decrease')
plt.ylabel('accuracy')
plt.legend()
```

![](img/c2decda8f5ce51b7c88933726f90ee53.png)

æˆ‘ä»¬å¯ä»¥æ„å»ºå¤šçº¿å›¾è¡¨æ¥æ¯”è¾ƒåŸºå°¼ç³»æ•°å’Œç†µä¸ä¸åŒæ·±åº¦æˆ–åˆ†å‰²é˜ˆå€¼çš„å…³ç³»ã€‚å‰é¢æåˆ°çš„æ–‡ç« æ›´è¯¦ç»†åœ°è¯´æ˜äº†è¿™ä¸ªè¿‡ç¨‹ã€‚

# k å‡å€¼èšç±»

æˆ‘è¿˜å¯¹å°è¯´ä½œè€…åœ¨è¿™ä¸ªçˆ±å¥½è€…åœˆå­é‡Œå†™çš„ä¸»é¢˜æ„Ÿå…´è¶£ï¼Œè€Œå¯¹å°è¯´æ–‡æœ¬è¿›è¡Œèšç±»å°†æ˜¯ä¸€é¡¹è¦æ±‚è¿‡é«˜çš„å·¥ä½œï¼Œå¯èƒ½ä¼šçƒ§åæˆ‘å¯æ€œçš„ç¬”è®°æœ¬ç”µè„‘â€”â€”*å¯¹æˆ‘æ¥è¯´ï¼Œå¯¹åƒå°è¯´è¿™æ ·çš„é•¿æ–‡æœ¬è¿›è¡Œèšç±»æ˜¯å¦æ˜¯ä¸€ä»¶æ ‡å‡†/æ˜æ™º/å¯è¡Œçš„äº‹æƒ…ä»ç„¶ä¸æ¸…æ¥šã€‚*

ä¸ºäº†ä¿æŒè¿™ä¸ªé¡¹ç›®çš„èƒƒå£ï¼Œæˆ‘å†³å®šå…ˆæŠŠæ‘˜è¦èšé›†èµ·æ¥ï¼Œå¸Œæœ›å®ƒä»¬èƒ½ä¸ºçœŸå®çš„å°è¯´æä¾›ä¸€ä¸ªå¾ˆå¥½çš„é¢„è§ˆã€‚

ä»¥ä¸‹æ˜¯æ‰€éœ€çš„é™„åŠ åº“:

```
import nltk
import re
from sklearn import feature_extraction
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from nltk.stem.snowball import SnowballStemmer
# from gensim import corpora, models, similarities <-- use if LDA
```

## æ•°æ®æ“ä½œ

è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¿¡æ¯æ£€ç´¢çš„ç†è®ºæˆ‘å°±ä¸æ·±ç©¶äº†ã€‚ä¸€ä¸ªç®€åŒ–çš„è¿‡ç¨‹åŒ…æ‹¬æ ‡è®°åŒ–ã€è¯å¹²åŒ–ã€åˆ é™¤åœç”¨è¯å’ŒçŸ¢é‡åŒ–ã€‚

ä»¥ä¸‹æ˜¯è‹±è¯­çš„æµç¨‹:

æˆ‘ä»¬æ„å»ºäº†è‡ªå®šä¹‰çš„åœç”¨è¯è¡¨ã€æ ‡è®°å™¨å’Œè¯å¹²åˆ†æå™¨ï¼Œä»¥è¾“å…¥åˆ° TfidfVectorizer ä¸­ï¼Œç„¶åä½¿ç”¨å®ƒä¾æ¬¡å¯¹æ¯ä¸ªæ‘˜è¦è¿›è¡Œç¼–ç ã€‚ç»“æœå°†æ˜¯å½¢çŠ¶çš„ç¨€ç–çŸ©é˜µâ€”(æ–‡æ¡£æ•°ã€ç‰¹å¾æ•°)ã€‚

ä½ å¯ä»¥æ·±å…¥ç ”ç©¶ TfidfVectorizer çš„[æ–‡æ¡£](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)æ¥å¯»æ‰¾æ›´å¤šçš„æ æ†ã€‚ä¾‹å¦‚ï¼ŒMax_df=0.8 è¡¨ç¤ºæˆ‘ä¸ä¼šæŸ¥çœ‹åœ¨è¶…è¿‡ 80%çš„æ–‡æ¡£ä¸­å‡ºç°çš„æœ¯è¯­(è¿™å¯èƒ½ä¸ä¼šæä¾›å¤ªå¤šæœ‰ç”¨çš„ä¿¡æ¯)ã€‚min_df=5 è¡¨ç¤ºè¯¥æœ¯è¯­éœ€è¦å‡ºç°åœ¨ 5 ä¸ªä»¥ä¸Šçš„æ–‡æ¡£ä¸­æ‰èƒ½è¢«è®¤ä¸ºæ˜¯æœ‰ä»·å€¼çš„ã€‚

ä»¥ä¸‹æ˜¯è·å–è¿™äº›åŠŸèƒ½çš„æ–¹æ³•:

```
terms = tfidf_vectorizer.get_feature_names()
```

æˆ‘ä»¬ä½¿ç”¨ TF-IDF(æœ¯è¯­é¢‘ç‡ä¹˜ä»¥é€†æ–‡æ¡£é¢‘ç‡)è€Œä¸æ˜¯ç®€å•çš„è®¡æ•°ï¼Œå› ä¸ºå®ƒè€ƒè™‘äº†æœ¯è¯­åœ¨ç‰¹å®šæ–‡æ¡£ä¸­çš„é‡è¦æ€§åŠå…¶ç›¸å¯¹äºæ•´ä¸ªè¯­æ–™åº“çš„é‡è¦æ€§ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œå¦‚æœä¸€ä¸ªåœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­ç½•è§çš„æœ¯è¯­ç¡®å®å‡ºç°åœ¨æŸä¸ªæ–‡æ¡£ä¸­ï¼Œå®ƒä¸€å®šå¯¹è¯¥æ–‡æ¡£æœ‰ç‰¹æ®Šçš„æ„ä¹‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é™ä½äº†å‡ºç°åœ¨å¤§å¤šæ•°æ–‡æ¡£ä¸­çš„æ ‡è®°çš„æƒé‡ï¼ŒåŒæ—¶çªå‡ºäº†æ›´æœ‰æ„ä¹‰çš„æ ‡è®°ã€‚

ç”±äºæˆ‘ä»¬çš„ tfidf_matrix ä¸­çš„å€¼è¡¨ç¤ºå®ƒä»¬çš„ç›¸å¯¹é‡è¦æ€§ï¼Œæˆ‘ä»¬ç”šè‡³å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ‰¾åˆ°æŸä¸ªæ–‡æ¡£çš„å…³é”®å­—â€”â€”

```
for i in tfidf_matrix[your document's index].toarray()[0].argsort()[::-1][:30]:
    print(terms[i])
```

æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ¥æŸ¥æ‰¾æœ€æ¥è¿‘çš„æ–‡æ¡£ï¼Œä½†æˆ‘ä¸ä¼šåœ¨è¿™é‡Œè¯¦ç»†ä»‹ç»:

```
cosine_similarity(tfidf_matrix)[your document's index].argsort()[:-5:-1] # top 4 neighbors since the first would always be itself
```

æ­¤å¤–ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªå…³äº[ä½¿ç”¨åœç”¨è¯](https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words)çš„æœ‰ç”¨è¯´æ˜ï¼Œå› ä¸ºæˆ‘ä¸€è·¯ä¸Šé‡åˆ°äº†å¾ˆå¤šè¿™æ ·çš„è­¦å‘Š:

> Y æ‚¨è¿˜åº”è¯¥ç¡®ä¿åœç”¨è¯è¡¨å·²ç»åº”ç”¨äº†ä¸çŸ¢é‡å™¨ä¸­ä½¿ç”¨çš„ç›¸åŒçš„é¢„å¤„ç†å’Œæ ‡è®°åŒ–ã€‚å•è¯*we have*è¢« CountVectorizer çš„é»˜è®¤æ ‡è®°å™¨æ‹†åˆ†ä¸º *we* å’Œ *ve* ï¼Œå› æ­¤å¦‚æœ*we have*åœ¨`stop_words`ä¸­ï¼Œè€Œ *ve* ä¸åœ¨ï¼Œé‚£ä¹ˆåœ¨è½¬æ¢åçš„æ–‡æœ¬ä¸­ï¼Œå°†ä»*we have*ä¸­ä¿ç•™ *ve* ã€‚æˆ‘ä»¬çš„çŸ¢é‡å™¨å°†è¯•å›¾è¯†åˆ«å’Œè­¦å‘ŠæŸäº›ç±»å‹çš„ä¸ä¸€è‡´ã€‚

Processing Chinese summaries is a totally different story. While English has clearly space-delimited words as tokens, Chinese token definition can be a little complicated. More than one characters can stick together to constitute one token, such as â€˜å­¦æ ¡â€™ for â€˜schoolâ€™ â€” either of these characters alone wouldnâ€™t carry the intended meaning.

å®ƒè¿˜éœ€è¦ä¸€ç»„ä¸åŒçš„åœç”¨è¯ã€å¥å°¾å’Œæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åŒ¹é…è§„åˆ™ã€‚å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬æœ‰å¾ˆå¤šå¯ç”¨çš„å¥½èµ„æº:

> åœå­—:[https://github.com/stopwords-iso/stopwords-zh](https://github.com/stopwords-iso/stopwords-zh)
> 
> unicode ä¸­çš„æ±‰å­—:ã€https://github.com/tsroten/zhon/blob/develop/zhon/hanzi.py 
> 
> æ ‡è®°åŒ–:ã€https://github.com/fxsjy/jiebaã€‘T4
> 
> æ·±åº¦å­¦ä¹ :[https://github.com/PaddlePaddle/Paddle](https://github.com/PaddlePaddle/Paddle)

è¦åŠ è½½çš„å…¶ä»–å·¥å…·åŒ…å¯èƒ½éœ€è¦å…ˆå®‰è£…:

```
import jieba
from __future__ import unicode_literals
import paddle
```

ä¸­å›½äººçš„æµç¨‹æ˜¯è¿™æ ·çš„:

å¯¹äºè¿™ä¸¤ç§è¯­è¨€ï¼Œä¸‹é¢çš„è¿‡ç¨‹æ˜¯ç›¸åŒçš„ã€‚

## æ„å»ºæ¨¡å‹

ä¸ºäº†æ£€æŸ¥æœ€ä½³çš„é›†ç¾¤æ•°é‡ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å»ºç«‹ä¸€ä¸ªè‚˜å½¢å›¾

```
km = [KMeans(n_clusters=i) for i in range(1,11)]
score = [k.fit(tfidf_matrix).inertia_ for k in km]
plt.plot(range(1,11),score)
```

![](img/752044f1751d6703824d848e2b1f3b39.png)

è™½ç„¶çœ‹èµ·æ¥åªæœ‰ 3 ä¸ªé›†ç¾¤æ˜¯æœ€ä½³çš„ï¼Œä½†ç»è¿‡å‡ æ¬¡å°è¯•åï¼Œæˆ‘å‘ç° 5 ä¸ªé›†ç¾¤çš„è§£å†³æ–¹æ¡ˆæ˜¯æœ€åˆç†çš„ã€‚

```
km = KMeans(n_clusters=5)
km.fit(tfidf_matrix)# append cluster labels back to the dataframe
clusters = km.labels_.tolist()
eng_sum['Clusters'] = clusters# compare each cluster's centrality measures
grouped = eng_sum.groupby('Clusters') # or chi_sum
grouped.mean()
```

![](img/611f78d104919a4d356e4e0514817d8d.png)

```
# to know how many members each cluster has
grouped.size()
```

## è§£é‡Šç»“æœ

ä¸ºäº†ç†è§£è¿™äº›èšç±»ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ç…§ TF-IDF å€¼ä»¥åŠå‡ ä¸ªæœ‰ä»£è¡¨æ€§çš„å‡è®¾æ¥æŸ¥çœ‹æ¯ä¸ªèšç±»çš„å…³é”®å­—:

```
importance_by_cluster = km.cluster_centers_.argsort()[:, ::-1] for i in range(5):
    print("Cluster {} words:".format(i))

    for ind in importance_by_cluster[i, :20]:
        print(terms[ind]+', ', end=' ') # top 20 words
        # terms are the list of features

    subset = eng_sum[eng_sum.Clusters==i].copy() # or chi_sum
    subset = subset.sort_values(by='Num_hits', ascending=False)
    print(subset.iloc[:10,[0,2]]) # print out most popular fictions belong to the cluster as representatives
```

ä¾‹å¦‚ï¼Œåœ¨æˆ‘æ¢ç´¢çš„ç²‰ä¸åœˆä¸­ï¼Œæˆ‘å‘ç°äº†ä¸€ä¸ªå…³æ³¨å“ˆåˆ©Â·æ³¢ç‰¹å’Œä¼åœ°é­”ä¹‹é—´äº’åŠ¨çš„é›†ç¾¤ï¼Œå…³é”®è¯æ˜¯â€œå“ˆåˆ©ã€ä¸–ç•Œã€æ³¢ç‰¹ã€å·«å¸ˆã€æ±¤å§†ã€é»‘æš—ã€æ—¶é—´ã€ç”Ÿå‘½ã€æ­»äº¡ã€éœæ ¼æ²ƒèŒ¨ã€ä¼åœ°é­”ã€é­”æ³•ã€riddlã€æ´»ç€ã€åŠ›é‡ï¼Œâ€å¦ä¸€ä¸ªé›†ç¾¤æ˜¾ç„¶ä» tumblr çš„å†™ä½œæç¤ºä¸­è·å¾—äº†çµæ„Ÿâ€”â€”â€œæç¤ºã€æç¤ºå»ã€å»ã€ç¬¦å·ã€Tumblrã€drabblã€exchangã€challengï¼Œâ€è¿˜æœ‰å¦ä¸€ä¸ªæƒ…æ„Ÿä¸°å¯Œçš„ä½œè€…ç¾¤è§¦æ‘¸ç€å„é‡Œæ–¯ä¹‹é•œâ€”â€”â€œé•œå­ã€é•œå­å„é‡Œæ–¯ã€å„é‡Œæ–¯ã€çœ‹è§ã€è¢œå­ã€çœ‹é•œå­

While Chinese and English topics are vastly different, interestingly, they both had one cluster dedicated to the mirror of erised. The Chinese version has somewhat similar keywords â€” â€œå„é‡Œæ–¯ (erised), é­”é•œ (mirror), è‡ªæˆ‘, ç«™, æ¨, é‚“æ ¡, æ¬ºéª—, åŒå¼ƒ, å¹´å°‘è½»ç‹‚, å°‘ä¸æ›´äº‹, å¾·å§†æ–¯ç‰¹æœ—, çœ‹åˆ° (see), æ’’è° (lie), é¢å‰ (front).â€ (overlapping ones translated)

æˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬ä»æŸ¥çœ‹è¿™äº›å…³é”®å­—ä¸­è·å¾—çš„å¯¹èšç±»çš„ç†è§£ä¸æ¥è‡ªå…¶ä»–æ•°å€¼å˜é‡çš„ä¸­å¿ƒæ€§åº¦é‡ç»“åˆèµ·æ¥ï¼Œä»¥å¾—å‡ºæ›´æœ‰è§åœ°çš„ç»“è®ºï¼Œä¾‹å¦‚ä»€ä¹ˆä¸»é¢˜ä¼šå¼•å‘æœ€å¤šçš„ååº”ï¼Œä»¥åŠå“ªäº›ä¸»é¢˜ä¼šæ˜¯æ›´å¥½çš„ç‚¹å‡»è¯±é¥µã€‚

è™½ç„¶æˆ‘å¦å¤–å°è¯•äº†ä¸»é¢˜å»ºæ¨¡ï¼Œä½†å®ƒåŸºæœ¬ä¸Šè¯å®äº†æ¥è‡ªé›†ç¾¤çš„è§è§£ï¼Œæ‰€ä»¥æˆ‘ä¸ä¼šåœ¨è¿™é‡Œä»‹ç»å®ƒã€‚

å½“ç„¶è¿˜æœ‰æ›´å¤šçš„å¯èƒ½æ€§å¯ä»¥ç©ã€‚æˆ‘ä¼šç»§ç»­æ¢ç´¢æ–°çš„é€‰æ‹©ğŸ˜Š