<html>
<head>
<title>Naive-Bayes for mixed typed data in scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">scikit-learn中混合类型数据的朴素贝叶斯</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/naive-bayes-for-mixed-typed-data-in-scikit-learn-fb6843e241f0?source=collection_archive---------4-----------------------#2021-01-15">https://medium.com/analytics-vidhya/naive-bayes-for-mixed-typed-data-in-scikit-learn-fb6843e241f0?source=collection_archive---------4-----------------------#2021-01-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="499b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们看看scikit-learn中的朴素贝叶斯(NB)实现，我们将能够看到各种各样的NB。举几个例子…</p><ul class=""><li id="9001" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">高斯朴素贝叶斯</li><li id="21c7" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">多项式朴素贝叶斯</li><li id="6d44" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">分类朴素贝叶斯</li></ul><p id="6788" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有的实现都是专为适应特定类型的数据或分布而设计的。高斯NB假设您的数据是独立的和正态分布的，类似于多项式和分类，其中分布是我们选择实现的原因。</p><p id="c5ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但在实时ML问题中，我们的数据集可能包含混合分布，如<strong class="ih hj">连续、分类、多项式和伯努利</strong>。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es jr"><img src="../Images/53a06ca6fc2288e6e3231192e621ace3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7al7Jm0Z_K1dUexL"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">由<a class="ae kh" href="https://unsplash.com/@usmanyousaf?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乌斯曼·优素福</a>在<a class="ae kh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="9e9a" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">在这种情况下，我们如何使用NB？在scikit-learn中有它的实现吗？</h2><p id="f591" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">Ans:没有！</p><h2 id="e23e" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">但是我们自己能做到吗？</h2><p id="c02e" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">Ans:很明显！是的。</p><h2 id="ff6b" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">我应该从零开始实施吗？</h2><p id="226f" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">Ans:没有！我们将利用现有的scikit-learn实现。</p><h2 id="ae99" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">好了，让我们快速回顾一下NB的基本原理。</h2><p id="3189" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">数据集中的每个特征都是相互独立的，因此<strong class="ih hj"> <em class="li">利用独立性的特性，我们将每个特征的后验概率</em> </strong> <em class="li"> </em>与标签的先验概率相乘，得到标签的预测概率。</p><p id="d6ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="li">我们以多项式NB为例。此处</em></p><ul class=""><li id="a705" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj"> P(W) </strong> —单词的先验概率(W= w，w …)</li><li id="c026" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj"> P(L) </strong> —标签的先验概率(L= l，l …)</li><li id="76d2" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj"> P(w^k/l^n) </strong> —给定标签为l^n.，找到单词w^k的后验概率(其中k = 1…单词总数，n=1…标签总数)</li></ul><p id="72de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有一个的单词序列</p><blockquote class="lj"><p id="b2e7" class="lk ll hi bd lm ln lo lp lq lr ls jc dx translated">" w1 w2 w3 w4 "</p></blockquote><p id="9891" class="pw-post-body-paragraph if ig hi ih b ii lt ik il im lu io ip iq lv is it iu lw iw ix iy lx ja jb jc hb bi translated"><em class="li">通过</em>计算其属于标签L1的概率</p><blockquote class="lj"><p id="19e3" class="lk ll hi bd lm ln lo lp lq lr ls jc dx translated">π(给定标签L的序列中每个单词的后半部分)*标签L的前半部分<br/>注:π —的乘积</p></blockquote><h2 id="db6e" class="ki kj hi bd kk kl ly kn ko kp lz kr ks iq ma ku kv iu mb kx ky iy mc la lb lc bi translated">P(L1)* P(w1/L1)* P(w2/L1)* P(w3/L1)* P(w4/L1)</h2><p id="0ded" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">我们增加了概率，因为我们假设一个单词的出现与另一个单词无关。因此，这些是同时发生的独立事件。</p><p id="8bda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，在我们的混合数据挑战中，如果我们的数据同时包含连续值和分类值，我们遵循以下步骤。</p><blockquote class="md me mf"><p id="946d" class="if ig li ih b ii ij ik il im in io ip mg ir is it mh iv iw ix mi iz ja jb jc hb bi translated">1.假设数据是相互独立的。</p><p id="8af7" class="if ig li ih b ii ij ik il im in io ip mg ir is it mh iv iw ix mi iz ja jb jc hb bi translated">2.将连续数据拟合到高斯NB。并得到该连续数据发生的概率/对数概率。</p><p id="0f86" class="if ig li ih b ii ij ik il im in io ip mg ir is it mh iv iw ix mi iz ja jb jc hb bi translated">3.将分类数据拟合到分类NB。并得到该分类数据出现的概率/对数概率。</p><p id="2273" class="if ig li ih b ii ij ik il im in io ip mg ir is it mh iv iw ix mi iz ja jb jc hb bi translated">4.让我们得到步骤(2)和(3)的对数概率。让它们成为<strong class="ih hj"> Log_Proba_Cont </strong>和<strong class="ih hj"> Log_Proba_Cat </strong>。</p><p id="30ec" class="if ig li ih b ii ij ik il im in io ip mg ir is it mh iv iw ix mi iz ja jb jc hb bi translated">5.<strong class="ih hj"> Log_Proba_Cont和Log_Proba_Cat在计算中使用标签的先验概率。因此，我们将减去一次，同时增加两个概率</strong>。</p><p id="fe5a" class="if ig li ih b ii ij ik il im in io ip mg ir is it mh iv iw ix mi iz ja jb jc hb bi translated">6.即整个数据集的最终对数概率可以是<br/><strong class="ih hj"><em class="hi">Log _ Proba _ Cont+Log _ Proba _ Cat—P(L)</em></strong></p><p id="d3ff" class="if ig li ih b ii ij ik il im in io ip mg ir is it mh iv iw ix mi iz ja jb jc hb bi translated">7.<strong class="ih hj">这相当于proba(cont)* proba(cat)/prior(Label)</strong>。第(6)行中probas的Log将乘法转换为加法。</p></blockquote><p id="ae98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们试着在<a class="ae kh" href="https://www.kaggle.com/guruprasad91/titanic-naive-bayes" rel="noopener ugc nofollow" target="_blank"> kaggle </a>的Titanic数据集中实现这个。</p><p id="2bbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:在scikit-learn repo中有一个针对上述流程的持续公关。</p><p id="7523" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考:<br/><a class="ae kh" href="https://stackoverflow.com/a/14255284" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/a/14255284</a></p></div></div>    
</body>
</html>