<html>
<head>
<title>Implementing neural networks in matlab 105</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在matlab 105中实现神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementing-neural-networks-in-matlab-105-6b71c5872b3c?source=collection_archive---------8-----------------------#2021-07-09">https://medium.com/analytics-vidhya/implementing-neural-networks-in-matlab-105-6b71c5872b3c?source=collection_archive---------8-----------------------#2021-07-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/8f5aadfd095287cc57a68ea4769a1d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7XryJtBBZPn25PkKNJWGWQ.jpeg"/></div></div></figure><p id="7a8a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们实现一个神经网络，根据客户的关键特征对他们进行分类。一旦你理解了方程式，在matlab中运行神经网络就很容易理解了。</p><p id="25a9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是我的神经网络系列的第5部分。欢迎从<a class="ae jn" href="https://shaun-enslin.medium.com/explaining-neural-networks-101-a36356113cbd" rel="noopener"> part 1 </a>开始。以下是之前解释成本函数的文章。</p><ol class=""><li id="f639" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><a class="ae jn" href="https://shaun-enslin.medium.com/forward-propagation-deep-dive-102-bbeabe4d2fb2" rel="noopener">正向传播</a></li><li id="8d65" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><a class="ae jn" href="https://shaun-enslin.medium.com/backward-propagation-deep-dive-103-60390714d2b0" rel="noopener">反向传播</a></li><li id="f0b1" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><a class="ae jn" href="https://shaun-enslin.medium.com/cost-and-gradient-calculation-in-neural-networks-deep-dive-104-2e16f26ce3f3" rel="noopener">计算成本&amp;坡度</a></li><li id="6737" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><a class="ae jn" href="https://shaun-enslin.medium.com/cost-and-gradient-calculation-in-neural-networks-deep-dive-104-2e16f26ce3f3" rel="noopener">计算坡度</a></li></ol><p id="e410" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里用到的源代码可以从<a class="ae jn" href="https://github.com/shaunenslin/machinelearning/tree/master/matlab/neuralnetworks/classification" rel="noopener ugc nofollow" target="_blank"> github </a>下载。剧透一下，我们只能得到66%的准确率，但是，这是我们现有的数据；-)</p><p id="6555" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">提醒一下，这是我们从<a class="ae jn" href="https://www.kaggle.com/abisheksudarshan/customer-segmentation" rel="noopener ugc nofollow" target="_blank"> kaggle </a>下载的数据集，你可以看这篇<a class="ae jn" rel="noopener" href="/geekculture/classifying-customers-with-logistics-regression-one-vs-all-f34ed2e5f042">文章</a>来了解我们为什么选择下面的3个特性。</p><figure class="kd ke kf kg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kc"><img src="../Images/3994f8c8b04e86167ba9889d65932ab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RiNwdmLzezVzbzT5jv8MhQ.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图1</figcaption></figure></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><h1 id="c9bc" class="ks kt hh bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">主程序</h1><p id="c071" class="pw-post-body-paragraph ip iq hh ir b is lq iu iv iw lr iy iz ja ls jc jd je lt jg jh ji lu jk jl jm ha bi translated">我们将有一些实用功能，但这些将被单独涵盖。让我们首先讨论主程序，它将有以下步骤:</p><ol class=""><li id="e327" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">加载数据</li><li id="12ea" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">将我们的字符串字段标记为数值</li><li id="632c" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">设置我们的神经网络大小</li><li id="3ae6" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">初始化θ/重量</li><li id="7000" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">成本优化以找到最佳theta</li><li id="a028" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">使用我们的最佳theta进行预测，以计算我们的训练集准确性</li></ol><p id="6733" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们一个一个地讨论这些</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="d499" class="ma kt hh lw b fi mb mc l md me">clear;</span><span id="27bf" class="ma kt hh lw b fi mf mc l md me">% open csv file<br/>tbl = readtable(‘test.csv’);</span><span id="7a51" class="ma kt hh lw b fi mf mc l md me">% replace strings fields with labels and create dataset matrix<br/>ds(:,1) = grp2idx(tbl{:,2});<br/>ds(:,2) = grp2idx(tbl{:,3});<br/>ds(:,3) = tbl{:,4};<br/>ds(:,4) = grp2idx(tbl{:,5});<br/>ds(:,5) = grp2idx(tbl{:,6});<br/>ds(:,6) = tbl{:,7};<br/>ds(:,7) = grp2idx(tbl{:,8});<br/>ds(:,8) = tbl{:,9};<br/>[ds(:,9),labels] = grp2idx(tbl{:,10});</span><span id="2fb9" class="ma kt hh lw b fi mf mc l md me">% remove rows with NaN in any field values<br/>ds = rmmissing(ds);<br/>[m,n] = size(ds);</span><span id="3291" class="ma kt hh lw b fi mf mc l md me">% Create X and y matrix<br/>X = ds(:,[2 4 8]);<br/>y = ds(:,n);</span></pre><p id="bdfd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">运行此步骤后，您应该会在X和Y矩阵中看到以下结果。</p><figure class="kd ke kf kg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mg"><img src="../Images/9128263629f84f0147379ea2a18752f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sE-OodRukH_bMYnpohh8jw.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图2</figcaption></figure><p id="0bf3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后，我们可以设置我们的神经网络的大小，首先，下面是我们要放在一起的神经网络。</p><figure class="kd ke kf kg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mh"><img src="../Images/7727f7418c157a2339569bf13201bb9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sp8_p6NJrYVWSQjF0SN7wg.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图3</figcaption></figure><p id="d132" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在下面的初始化中，确保实现了上述网络。</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="1b17" class="ma kt hh lw b fi mb mc l md me">input_layer_size = size(X,2);           % Dimension of features<br/>hidden_layer_size = input_layer_size*2; % # of units in hidden layer <br/>output_layer_size = size(labels,1);     % number of labels</span></pre><p id="e747" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们可以将θ初始化为随机的小值<em class="mi">(记住这些也叫做权重)</em>，你可以在图4中看到结果。</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="6caf" class="ma kt hh lw b fi mb mc l md me">% initialise 2 thetas for hidden layer and output layer<br/>initial_Theta1 = initializeWeights(input_layer_size, hidden_layer_size);<br/>initial_Theta2 = initializeWeights(hidden_layer_size, output_layer_size);</span><span id="1127" class="ma kt hh lw b fi mf mc l md me">% Unroll parametersinitial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mj"><img src="../Images/8d66b0c5e52f8810661ce365eaebed4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ViEK38Ohs_kdACxMTwtBnA.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图4</figcaption></figure><p id="1e75" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们可以运行我们的fminunc成本优化来找到最佳的theta。<strong class="ir hi"> <em class="mi">我们稍后会创建myCostFunction。</em> </strong></p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="141f" class="ma kt hh lw b fi mb mc l md me">% Run cost function to find lowest cost thetas<br/>options = optimset(‘MaxIter’, 50);<br/>lambda = 1;</span><span id="76b6" class="ma kt hh lw b fi mf mc l md me">costFunction = @(p) myCostFunction(p, input_layer_size, hidden_layer_size, output_layer_size, X, y, lambda);<br/>[nn_params, ~] = fminunc(costFunction, initial_nn_params, options);</span></pre><p id="1c47" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们有了最好的θ值，让我们把它重新整形成2组θ值，这样我们就可以检查准确性了。</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="6fd3" class="ma kt hh lw b fi mb mc l md me">% Obtain Theta1 and Theta2 back from nn_params<br/>Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), hidden_layer_size, (input_layer_size + 1));</span><span id="661d" class="ma kt hh lw b fi mf mc l md me">Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), output_layer_size, (hidden_layer_size + 1));</span></pre><p id="4211" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们有了θ，我们可以保留它们用于以后的预测，但是让我们使用它们，现在检查我们的训练集准确性。</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="5bd0" class="ma kt hh lw b fi mb mc l md me">pred = predict(Theta1, Theta2, X);<br/>fprintf(‘\nTraining Set Accuracy: %f\n’, mean(double(pred == y)) * 100);</span></pre><p id="e5e9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">运行您的解决方案后，您现在应该看到以下内容:</p><figure class="kd ke kf kg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mk"><img src="../Images/b45c1009922911ca91c07d167a183979.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lzCx63nwj_OI8PuRhW7Viw.png"/></div></div></figure><h1 id="9ff0" class="ks kt hh bd ku kv ml kx ky kz mm lb lc ld mn lf lg lh mo lj lk ll mp ln lo lp bi translated">霉菌功能</h1><p id="a17b" class="pw-post-body-paragraph ip iq hh ir b is lq iu iv iw lr iy iz ja ls jc jd je lt jg jh ji lu jk jl jm ha bi translated">我将在这里粘贴代码以供参考，但请阅读本系列的第2、3和4部分，以获得关于向前传播<strong class="ir hi">、向后传播</strong>以及计算成本和梯度的<strong class="ir hi">的详细解释。代码也可以在<a class="ae jn" href="https://github.com/shaunenslin/machinelearning/tree/master/matlab/neuralnetworks/classification" rel="noopener ugc nofollow" target="_blank"> github </a>中找到。</strong></p><figure class="kd ke kf kg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mq"><img src="../Images/a28bccaed70dc40840e6d83ec3802855.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N4VpUqbuQaJXSlLtBtAWoA.png"/></div></div></figure><h1 id="b33e" class="ks kt hh bd ku kv ml kx ky kz mm lb lc ld mn lf lg lh mo lj lk ll mp ln lo lp bi translated">其他实用功能</h1><p id="c810" class="pw-post-body-paragraph ip iq hh ir b is lq iu iv iw lr iy iz ja ls jc jd je lt jg jh ji lu jk jl jm ha bi translated">我不会详细讨论其他的实用函数，因为在本系列的前几篇文章中已经介绍了很多。同样，它们的代码在<a class="ae jn" href="https://github.com/shaunenslin/machinelearning/tree/master/matlab/neuralnetworks/classification" rel="noopener ugc nofollow" target="_blank"> github </a>中，列表如下:</p><ol class=""><li id="3f16" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">乙状结肠梯度</li><li id="b369" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">乙状结肠的</li><li id="fedb" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">初始化权重</li></ol><h1 id="1526" class="ks kt hh bd ku kv ml kx ky kz mm lb lc ld mn lf lg lh mo lj lk ll mp ln lo lp bi translated"><strong class="ak">结论</strong></h1><p id="3209" class="pw-post-body-paragraph ip iq hh ir b is lq iu iv iw lr iy iz ja ls jc jd je lt jg jh ji lu jk jl jm ha bi translated">我希望本系列已经为您提供了实现神经网络的良好概述。我已经给了你结果的截屏，让你看到每一步，希望有所帮助。</p><p id="0ea0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后一句话，这个<a class="ae jn" href="https://www.coursera.org/learn/machine-learning/home/welcome" rel="noopener ugc nofollow" target="_blank">课程</a>非常棒。如果你想成为一名专家，那么这是你的课程。</p></div></div>    
</body>
</html>