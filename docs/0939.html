<html>
<head>
<title>Google’s TRILLION Parameters Transformer Model: Switch Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谷歌的万亿参数变压器模型:开关变压器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/googles-trillion-parameters-transformer-model-switch-transformer-6b27d6b60920?source=collection_archive---------9-----------------------#2021-02-07">https://medium.com/analytics-vidhya/googles-trillion-parameters-transformer-model-switch-transformer-6b27d6b60920?source=collection_archive---------9-----------------------#2021-02-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4b6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在过去的几年里，Transformer模型在解决自然语言处理(NLP)任务方面获得了很大的流行。变压器是大型预训练神经网络，在海量数据上进行训练，以掌握语言中的模式。这些模型可以被微调，以解决许多自然语言处理任务，如语义分析，问答，机器翻译等。</p><p id="1f08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">OpenAI的GPT-3当然创造了许多炒作，并成为过去几年中最著名的深度学习变压器模型之一。众所周知，它拥有1750亿个参数，在某种程度上，它只不过是拥有更多参数的GPT-2。</p><p id="54b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">谷歌的Switch Transformer目前受到了很多关注，因为它的1.6万亿参数模型大小和在多个NLP基准中的排名超过了T5模型。</p><p id="156d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">开关变压器的1.6万亿参数令人印象深刻，但最令人印象深刻的方面是它的简单和有效的计算，这不像GPT-3(计算昂贵)。在计算资源相同的情况下，Switch Transformer实现了与T5 -Base和T5-Large模型相同的复杂度，速度提高了7倍。</p><h2 id="5a2f" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">简介:</strong></h2><p id="3b28" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">在深度学习中，通常模型对所有输入重复使用参数。而专家混合(MoE)为每个输入的例子选择不同的参数。尽管MoE在机器翻译方面取得了显著的成功，但它的采用受到复杂性、通信成本和培训不稳定性的阻碍。在这里，专家只不过是前馈网络(FFN)。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/a47db4de2783c9ab5e38fa7c2c7e071e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WVIr7gYbNRnn1ohiIb1yYQ.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">图来自<a class="ae kt" href="https://arxiv.org/pdf/1701.06538.pdf" rel="noopener ugc nofollow" target="_blank">稀疏门控MoE层论文</a>:嵌入在递归语言模型中的MoE层。这里，稀疏选通函数选择两个专家来执行计算。</figcaption></figure><p id="8827" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在开关变压器中，开关意味着将输入路由到隐含遵循<strong class="ih hj">的特定参数子集</strong>，并非所有知识始终有用。开关变压器架构简化并改进了专家混合(MoE ),以获得训练稳定性和计算优势。Switch Transformer有许多基于参数计数的实例，名称有Switch-Base、Switch-Large、Switch-XXL、Switch-C(1.6万亿个参数)。</p><p id="71ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">注:</em> </strong>开关变压器不仅仅指开关-C(1.6万亿参数初始化)。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kv"><img src="../Images/558337edeca3c967a18aba15f41ffd94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HmPE2rpfRdvWhq5HTddcgg.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">来自<a class="ae kt" href="https://arxiv.org/pdf/2101.03961.pdf" rel="noopener ugc nofollow" target="_blank">开关变压器论文</a>的图:开关变压器编码器模块的图示。</figcaption></figure><p id="a497" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上图所示，我们将变压器中密集的前馈网络(FFN)层替换为稀疏的开关FFN层。该交换FFN层对输入序列中的令牌独立操作。x1和x2的令牌嵌入(由底层产生)被路由到四个FFN专家之一，在那里路由器独立地路由每个令牌。</p><p id="75e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">路由器如何知道该接通哪个专家？</em> </strong></p><p id="f1c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让嵌入、自我关注和Add+Normalize层后的标记z为<em class="ku"> x </em>(标记嵌入)。路由器需要确定通过令牌嵌入(x)的最佳专家。如何选择专家的步骤:</p><ol class=""><li id="d9c9" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated">让<em class="ku"> Wᵣ </em>作为路由器变量(一个可学习的参数)乘以嵌入的<em class="ku"> x </em>产生logit<em class="ku">h(x)=wᵣ* x</em></li><li id="88b0" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><em class="ku"> h(x) </em>通过该层可用N名专家的softmax分布进行归一化。专家<em class="ku"> i </em>门限值如下:<em class="ku"> pᵢ = softmax(h(x)ᵢ) </em>该<em class="ku"> pᵢ </em>表示嵌入通过专家<em class="ku"> i </em>的概率。</li><li id="ea73" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">嵌入<em class="ku"> x </em>以最高概率通过专家<em class="ku"> i </em>。最后，输出(即，更新的令牌嵌入)是由专家产生的激活，通过其概率得分进行加权:<em class="ku"> y= pᵢ * Eᵢ (x) </em></li></ol><p id="cfff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里嵌入只通过概率最高的专家，而在正常的MoE层我们通过<em class="ku"> k(k &gt; 1) </em>专家。这种<em class="ku"> k=1 </em>路由策略被称为<strong class="ih hj"> <em class="ku">交换层。</em> </strong>像教育部这样的前期研究在LSTMs环境下指出，网络选择多个专家(至少两个)并汇总结果。开关变压器只选择了一名专家，因此它否定了至少有两名专家对路由参数进行可靠培训的先验直觉。</p><p id="0586" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">开关层的好处</em> </strong>:</p><ol class=""><li id="1fbe" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated">路由器的计算量减少了，因为我们只需要将一个令牌路由给一个专家。</li><li id="afd7" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">每个专家的批量至少可以减半，因为每个令牌只发送给一个专家。</li><li id="0bce" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">简化了路由实现，降低了通信成本(因为计算只需要一名专家)。</li></ol><p id="8bbb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Switch Transformer是一个稀疏激活的专家模型，其中稀疏性来自于为每个传入的示例激活神经网络权重的子集。模型的稀疏性会引入训练的不稳定性(由于参数的初始化，不同的训练运行会导致不同的性能)。</p><p id="6462" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">训练不稳定原因:</em> </strong></p><p id="ecc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于每一层的硬切换(路由)决策和bfloat16(大脑浮点)等低精度格式，可能会导致不稳定。低精度格式会加剧我们路由器的softmax计算中的问题。</p><p id="1420" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">训练不稳定的解决方法:</em> </strong></p><p id="cfc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过在模型的局部部分(路由器)内选择性地铸造<em class="ku"> float32 </em>精度来实现稳定性，而不会导致float32张量的昂贵通信成本。路由器输入被转换为float32精度，消除了softmax计算中的恶化问题。Precision仅在路由器函数体中使用，因此没有昂贵的通信成本，但我们仍然受益于增加的稳定性。</p><p id="0382" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">减小初始化规模导致更好的模型质量和更稳定的开关变压器训练。</p><p id="3e03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习模型的一个更自然的问题是过度拟合。缓解这一问题的简单方法是通过增加专家内部的流失称为<strong class="ih hj"> <em class="ku">专家流失</em> </strong>。因此，非专家层的较小辍学率(0.1)和专家层的较大辍学率(0.4)可以提高性能。</p><h2 id="5625" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak"> <em class="lk">对缩放属性的观察:</em> </strong></h2><p id="47e0" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">开关变压器架构在许多方面进行了扩展，如专家、层数。</p><p id="86fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">专家的数量是缩放我们的模型的最有效的维度，因为增加专家保持计算成本固定，因为模型只选择一个专家令牌。选择专家是一个轻量级的计算，所以专家的增加不会影响模型的计算。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ll"><img src="../Images/f4f15b5e391a2e1c604bdb0432f4da73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o5RT9_aHw3Escbm3eAzFCA.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">来自<a class="ae kt" href="https://arxiv.org/pdf/2101.03961.pdf" rel="noopener ugc nofollow" target="_blank">开关变压器论文</a>的图4:开关变压器的缩放特性</figcaption></figure><p id="57d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">从图4左图:</em> </strong>从左上到右下，我们将专家数量从1增加到2、4、8、16、32、64、128、256。我们可以观察到，随着专家数量的增加，在同等计算预算的情况下，性能会不断提高。</p><p id="0966" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">从图4右图:</em> </strong>开关变压器更早实现了与T5-model相同的困惑。随着专家数量的增加，开关变压器可以更早地获得结果。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es lm"><img src="../Images/cd4a2109489957cbc556dd65b7e7d175.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zonuWSLnydEWzJklKP6LtQ.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">图5来自<a class="ae kt" href="https://arxiv.org/pdf/2101.03961.pdf" rel="noopener ugc nofollow" target="_blank">开关变压器论文</a>:开关变压器的速度优势</figcaption></figure><p id="b5ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">上图观察:</em> </strong>在计算量和训练时间相同的情况下，开关变压器明显优于密集变压器(T5-model)，64专家开关基模型达到相同质量的速度是T5-Base模型的7倍。</p><h2 id="7144" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak"> <em class="lk">下游结果:</em> </strong></h2><p id="017b" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">在本节中，作者使用高度调谐223米参数T5-基本模型，739米参数T5-大模型作为基线，并与7.4米参数开关-基本模型，26.3米参数开关-大模型进行比较。作者对基本模型使用124B浮点运算(FLOPS ),对大型模型使用425B浮点运算(FLOPS)。在大多数NLP任务中，如问题回答、分类、总结，开关变压器模型比翻牌匹配的T5-Base和T5-Large模型表现得更好。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ln"><img src="../Images/69811067e8c12096c9bd1f85abea8a44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-40C663VGcKYXw3zKAeBpw.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">跨不同NLP数据集的T5基线和交换机模型的微调结果。</figcaption></figure><p id="ab21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">结论:</em> </strong></p><p id="86b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在大型模型不断增加的今天，看到这些大型模型使用有限的计算能力是一种激励。这项研究也激发了稀疏模型作为一个有效的架构，并缓解了稀疏模型的问题，如模型复杂性，训练困难，通信成本。开关变压器的研究使我们在自然语言任务中考虑这些灵活的模型。</p><blockquote class="lo lp lq"><p id="3eca" class="if ig ku ih b ii ij ik il im in io ip lr ir is it ls iv iw ix lt iz ja jb jc hb bi translated"><strong class="ih hj">参考文献:</strong></p><p id="6152" class="if ig ku ih b ii ij ik il im in io ip lr ir is it ls iv iw ix lt iz ja jb jc hb bi translated">Fedus等人:<a class="ae kt" href="https://arxiv.org/pdf/2101.03961.pdf" rel="noopener ugc nofollow" target="_blank">开关变压器:利用简单有效的稀疏性扩展到万亿参数模型</a> (2021)</p><p id="ceb8" class="if ig ku ih b ii ij ik il im in io ip lr ir is it ls iv iw ix lt iz ja jb jc hb bi translated">Raffel等人:<a class="ae kt" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank">用统一的文本到文本转换器探索迁移学习的极限</a> (2019)</p><p id="5690" class="if ig ku ih b ii ij ik il im in io ip lr ir is it ls iv iw ix lt iz ja jb jc hb bi translated">Shazeer等人:<a class="ae kt" href="https://arxiv.org/pdf/1701.06538.pdf" rel="noopener ugc nofollow" target="_blank">异常庞大的神经网络:稀疏门控的专家混合层(</a> 2017)</p></blockquote></div></div>    
</body>
</html>