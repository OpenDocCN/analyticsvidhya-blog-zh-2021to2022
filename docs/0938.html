<html>
<head>
<title>How does ReLU activation work?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ReLU激活是如何工作的？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-relu-works-f317a947bdc6?source=collection_archive---------8-----------------------#2021-02-07">https://medium.com/analytics-vidhya/how-relu-works-f317a947bdc6?source=collection_archive---------8-----------------------#2021-02-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="018b" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">对于我们大多数人来说，ReLU是如何工作的是一件神秘的事情。因此，很难理解神经网络是如何运作的，也很难理解我们从训练中获得了什么样的学习功能。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/8ece0c82eff793c7760d86e915376eca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1fER6JTCHJOD7sWMQ8qqvA.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">乔尔·菲利普在<a class="ae jn" href="https://unsplash.com/s/photos/dimensions?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="e767" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">自从2012年由伊利亚·克里日夫斯基和杰弗里·辛顿在<a class="ae jn" href="https://webcache.googleusercontent.com/search?q=cache:7Ne-wJBSKH8J:https://arxiv.org/pdf/1803.01164+&amp;cd=5&amp;hl=en&amp;ct=clnk&amp;gl=lk" rel="noopener ugc nofollow" target="_blank">发表的AlexNet论文</a>以来，神经网络的真正潜力开始自己揭开。它的主要部分是ReLU激活函数，后面是卷积层。所以，我们来讨论一下ReLU激活函数的秘密，以及它对深度学习领域的改变。</p><blockquote class="kl km kn"><p id="8dfd" class="jo jp kk jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated"><strong class="jq hj">关于ReLU的一般定义</strong></p></blockquote><p id="5001" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="kk">“在人工神经网络的上下文中，整流器是一个激活函数，定义为其自变量的正部分:f(x)=max(0，x)。其中x是神经元的输入。这也称为斜坡函数，类似于电气工程中的半波整流。”— </em> <a class="ae jn" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank">维基百科</a></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kr"><img src="../Images/0ef5276e3474eeabc9bb3aeaa60f9d7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*t2qFY1xGn2xpo1Ezw7ivWQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://images.app.goo.gl/BpGPGc3SRbNsKXNR7" rel="noopener ugc nofollow" target="_blank">参考</a>:ReLU函数图。</figcaption></figure><p id="9092" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">简单来说，ReLU是一个非线性函数。因此，当我们将ReLU的<em class="kk">域</em>设置为它所遵循的线性函数的<em class="kk">范围</em>时，我们已经对我们原来的<em class="kk">学习函数进行了非线性激活。</em></p><p id="f569" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">从信号处理的角度来看，这是输入信号的整流，或者从拓扑(数学)的角度来看，这是空间的折叠。</p><p id="d5c4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">想象u(x) = x，合成f(u(x)) = max(0，u(x))。它对我们的输入空间有什么影响？这实际上是“折叠”平滑输入。</p><p id="a2da" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了理解折叠，想象一个2D输入平面。现在沿着一个轴折叠它，返回两个在“折叠”处相交的单独光滑的平面。在ReLU的情况下，其中一个是平的，另一个是有角度的。通过重复这一过程来创建许多“折叠”，我们可以使用ReLU函数从平滑的2D输入平面生成一个“n-折叠超平面”。(我们可以将这个想法扩展为一般输入，但那是另一篇文章的内容)</p><h2 id="f0b5" class="ks kt hi bd ku kv kw kx ky kz la lb lc jx ld le lf kb lg lh li kf lj lk ll lm bi translated">在讨论ReLU如何工作之前，我们必须先讨论ReLU应该做什么。</h2><p id="a957" class="pw-post-body-paragraph jo jp hi jq b jr ln ij jt ju lo im jw jx lp jz ka kb lq kd ke kf lr kh ki kj hb bi translated">传统的机器学习算法严重遭受被称为<em class="kk">维数灾难</em>的现象。简而言之，当维度超过某个值后，模型的性能会随着输入的维度数量而急剧下降。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ls"><img src="../Images/2109e1a3417b542a6e131bdbdfd2e75d.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*4l6PN-tLGUF34BWNw3AY_A.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" rel="noopener" href="/@ODSC/confronting-the-curse-of-dimensionality-5bcf2998b30d">具有维度的性能曲线</a></figcaption></figure><p id="bab4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这种现象的一个主要原因是模型的学习函数相对于高维数据点<strong class="jq hj">偏差更小</strong>。另一方面，我们不能建立一个非常高功率的学习功能。因为这种模型的计算复杂性将是巨大的，并且对大数据集的训练需要很长时间。</p></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><blockquote class="kl km kn"><p id="72f8" class="jo jp kk jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated"><strong class="jq hj">学习功能</strong></p></blockquote><p id="7478" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">每个数学模型都必须基于一套明确定义的规则。类似地，在深度学习中，我们也有一个学习函数和与之相关的优化规则。深度学习非常擅长建立正确平衡<em class="kk">偏差&amp;方差</em>值的学习函数。同样，这也是深度学习在机器学习涉及的每个领域都出名的主要原因。</p><p id="26bf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">根据定义，单个神经元的学习功能是线性变换功能。利用线性变换的性质，我们可以很容易地得出这样的结论:“单个神经元可以完成一系列神经元的变换”。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ma"><img src="../Images/11bb11793a524515e96a430b23709f34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/1*4WgsnqoVGHfCbco5JGcSKg.gif"/></div></figure><p id="d725" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此我们用非线性函数从线性函数给出非线性函数。那么神经元的最终学习函数是线性变换函数和非线性函数的复合函数。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mb"><img src="../Images/6bd162fd51f432fd2a96c91a45a7081c.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/1*t3yEPdeQBC8LKXRckznZ8Q.gif"/></div></figure><p id="29b6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">由于神经元的最终输出不是线性的，这些函数的组合不能由一个简单的函数来代替。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mc"><img src="../Images/121b44e0a5e8377cfe0f8443eeb83c30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/1*soP0TKdj7Ld1Hgx030qdcw.gif"/></div></figure><p id="ba2b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，我们确信神经网络的结果不会是简单的线性学习函数。但是，<strong class="jq hj">主要的问题仍然存在:<em class="kk">这个非线性函数如何鲁棒而精确地建立这样一个复杂的学习函数。</em>T15】</strong></p></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><p id="2153" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了说明ReLU是如何工作的，让我们使用一个简单的3个神经元的神经网络。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es md"><img src="../Images/1ba878812757f9f0568d7ea2c2cb9c83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jiE1faTzeBRoyJhnWKdQig.png"/></div></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es me"><img src="../Images/a8c8c94f6b6d8a64ec25446e6a07b33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/1*Yq2uC7cN9pSU9IANPOMTQQ.gif"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mf"><img src="../Images/f7398b3e12cd70866e4a0a2e0ef9a1e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OQsEjlyp03gHDyEUwXH3WQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">单神经元的中间和最终结果图</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mf"><img src="../Images/2847b336755a5e3603ac0e1af2ff8bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QBfqieZmqf4l004FIPQX0A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">2-神经元的中间和最终输出函数。</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mf"><img src="../Images/d60a656d64fde3ed2795229e88dd8f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e-9i6wrRpChVARWUAbzOYg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">三神经元的最终输出</figcaption></figure><p id="46c6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如图所示，神经网络的最终复合函数相当复杂。此外，这个插图让你了解为什么我们在神经网络的每一层使用大量的神经元。两个Relu形成了一条“两次折叠线”。</p><p id="5715" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">沿着这条思路思考，ReLU激活的组合导致下一层的多重折叠功能输入。换句话说，我们可以通过增加单层中可再激活神经元的数量来生成一个复杂得多的函数。</p><p id="a435" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">然而，我们最初的问题的一部分仍然存在；【ReLU如何在高维空间中创建精确的学习函数<em class="kk">一般是凸函数</em>？既然有一些详细的概念要谈，让我们把我们最重要的问题留给另一篇文章。在那里我们将揭示为什么ReLU在深度学习领域的表现优于其他激活函数。最后你就会明白，为什么神经网络的学习功能不会考虑凸函数来做全局优化问题，反而最终会解决一个局部优化问题。</p><p id="4219" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">同时，试着自己想出我们的<strong class="jq hj">最高题</strong>。在本文的下一部分再见。</p><p id="efd7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">谢谢大家！</p><p id="fff1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">更新:</p><p id="9b96" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><a class="ae jn" rel="noopener" href="/analytics-vidhya/how-does-relu-activation-work-part-2-8bb4feeb3b42">本文第二部分的链接</a>。</p></div></div>    
</body>
</html>