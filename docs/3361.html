<html>
<head>
<title>FOTS: Fast Oriented Text Spotting with a Unified Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">FOTS:用统一的网络快速定位文本</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fots-fast-oriented-text-spotting-with-a-unified-network-76e665afc449?source=collection_archive---------4-----------------------#2021-06-27">https://medium.com/analytics-vidhya/fots-fast-oriented-text-spotting-with-a-unified-network-76e665afc449?source=collection_archive---------4-----------------------#2021-06-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="c268" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在日常生活中，我们会遇到许多自然图像/场景包含一些文本的地方，我们会在脑海中处理这些文本以获得其意图。因此，自然语言处理在分析文本中起着至关重要的作用，并且通常当这些文本被视为自然图像时，文本识别成为ML/DL中的一个重要领域。例如，在现实生活中，我们在海报、交通标志、商店、横幅等中看到文本。因此，这些文本定位(检测文本位置并提取文本)成为一项重要的技术，可用于实现许多其他ML/DL实际应用，如文档分析、无人驾驶汽车等。</p><h1 id="ef89" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">业务问题陈述</h1><p id="9e17" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">在现代世界中，自动化在许多现实世界的领域中是首选的。为了支持它，在某些情况下，图像处理/计算机视觉(CV)和自然语言处理(NLP)通过深度学习技术对此提供了重要支持。当涉及自动驾驶汽车、扫描文档、图像扫描/信息检索、汽车/机器人导航等自动化时，处理图像并从中提取信息非常有用。在现实生活的图像中，我们看到图像中的文本可以在大小、字体、方向等方面变化，这成为从图像中提取它们的一个挑战。这里，提取意味着检测图像中的文本并识别文本，以便我们可以将它们作为自然语言使用，而不是图像的一部分。</p><h1 id="ac2d" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated"><strong class="ak">机器学习问题陈述</strong></h1><p id="f9ff" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">根据业务需求和研究论文，文本识别可以由两个子问题组成。首先，我们需要在边界框的帮助下检测图像中的文本。之后，我们只需要考虑识别的边界框部分，其中边界框内的文本将被提取为普通的自然文本。在研究论文中，除了检测和识别之外，还考虑了文本方向，因为可能存在具有不同方向的文本。这被称为ROIRotate(感兴趣区域旋转)。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kf"><img src="../Images/d82df84b4a97f85c98b227925d65b1d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QLWl9suTWJLGUwzhZKLSKg.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">文本定位</figcaption></figure><p id="9fd9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于文本检测，我们可以使用VGG-16，ResNet-50等与Deconv层。同样，对于识别，我们可以使用CNN与身高最大池和RNN模型(LSTM/GRU)。这里我们用的是GRU。</p><h1 id="458d" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated"><strong class="ak">数据集</strong></h1><p id="30e9" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">在这个实现中，我们使用两种类型的数据集。一个是Synth数据集，我们有一些自然图像，上面手动粘贴了一些文本，这样可以有效地训练模型来检测图像中的文本并识别它。第二个是ICDAR2015数据集，它包含许多从现实世界中拍摄的具有不同大小、字体和方向的文本的纯自然图像。它有助于模型很好地处理包含文本的真实世界图像。</p><p id="6834" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">合成数据集:<a class="ae kv" href="https://www.robots.ox.ac.uk/~vgg/data/scenetext/" rel="noopener ugc nofollow" target="_blank">https://www.robots.ox.ac.uk/~vgg/data/scenetext/</a></p><p id="bd58" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">ICDAR数据集:【https://rrc.cvc.uab.es/?ch=4】T2&amp;com =任务</p><p id="2e3f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们需要在下载数据集之前注册。</p><h1 id="eddb" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">探索性数据分析</h1><h2 id="b541" class="kw jd hh bd je kx ky kz ji la lb lc jm ip ld le jq it lf lg ju ix lh li jy lj bi translated">基于Synth数据的EDA</h2><p id="d863" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">我们总共有200个文件夹，每个文件夹大约有4293张图片。我们总共有80万张图片，这是一个庞大的数据集。为了使其在计算上可行，我们从每个文件夹中提取了样本图像，每个文件夹都包含一种特定类型的图像，这些图像对文本和图像对象进行了细微的修改，如下所示。</p><p id="3b8c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">图像上的EDA:</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lk"><img src="../Images/a4a32bcab47956575a9f04bb51f4a903.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4AKI8-X8B_svM3nNjm9eLg.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">不同类型的图像(豹纹和巧克力色)</figcaption></figure><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ll"><img src="../Images/7fc79e76c04b4accfb8f6978a67c7e4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pe4YdHG_2_1pmB-QAVBrNg.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">蚂蚁和蝙蝠侠的照片</figcaption></figure><p id="21bc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们绘制了图像的高度和宽度的PDF图进行分析。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lm"><img src="../Images/2d99bf4e562d440b5aa4139969316286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zUYqZTam7gr-bM0NsAPx8A.png"/></div></div></figure><p id="3378" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从图中，我们可以看到大多数图像的高度大约在580到630之间。高度在390和420之间的图像很少。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ln"><img src="../Images/01eab596dd0eb3840f0658d996c29107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MLL3Xf5rq7ps0Xe7OU9qdg.png"/></div></div></figure><p id="1430" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从图中，我们可以看到大多数图像的高度在300到500之间。有些图像的高度介于550和630之间。所以我们可以将图像的大小调整为(512，512，3)。(3-用于RGB通道)</p><p id="c344" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">边界框上的EDA:</p><p id="e711" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在Synth数据集中，我们有一个包含图像路径、边界框坐标和相应文本的gt文件。我们可以提取这些信息并准备一个数据帧供以后使用。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lo"><img src="../Images/cae61cfa93a259007cd9ad2e1b063ce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-jzxlHC4S82dw7jIuMbnmA.png"/></div></div></figure><p id="6368" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们已经观察到边界框坐标类似于(2，4，15)，其中第一个值代表其X或Y坐标。那么第二个值代表边界框的点的索引(一个边界框有4个点)。最终值是指该图像中边界框的数量。我们可以提取这些值，并在从指定路径获得的图像上绘制边界框，如下所示。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lp"><img src="../Images/5e08754a37d1619dcd60b6e89f5ac07b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t15TG1q7-zKL0bQroC7k0w.png"/></div></div></figure><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lq"><img src="../Images/1f2fa70503ae2fb37b5d485917204e64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7YsxzF1nM-CELUq5YhgBbw.png"/></div></div></figure><p id="a19f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如你所看到的，所有的文本都被矩形框包围着。对于给定的图像，我们需要预测文本检测模型中每个文本/单词的边界框的坐标。基于图像中出现的单词数量，我们可以有多个边界框。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lr"><img src="../Images/c87d264587c80d53aa418d0b5ba8b728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*glAZubVh6towIu9zZbZCSA.png"/></div></div></figure><p id="9e96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">大多数图像不到20个框，平均有5到6个框。</p><p id="7bde" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">类似地，我们可以分析边界框的高度和宽度，并检查无效/异常框。但实际上，随着图像大小的变化，边界框的坐标值可以与其他值相差很大。但是如果你绘制图像和边界框，它看起来会很正常。我们收集了一些图像，它们的边界框带有异常坐标值，但是通过绘图，看起来很正常。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lk"><img src="../Images/e1271632c04c2a101a8e1e99f5c617bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2QT3_AEoHMP2iuoxbLFWsA.png"/></div></div></figure><h2 id="83aa" class="kw jd hh bd je kx ky kz ji la lb lc jm ip ld le jq it lf lg ju ix lh li jy lj bi translated">基于ICDAR2015数据的EDA</h2><p id="07f2" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">这些图像是真实世界的图像，我们总共有1000张图像，如下所示。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ls"><img src="../Images/416c206093cbbe38f1d8cdeeaea57bf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZDJVyjhvYLzbgZOGu7ZGrg.png"/></div></div></figure><p id="000b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这个数据集中，所有图像的高度和宽度都不变，分别为1280和720。</p><p id="f1ac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们有一个单独的文本文件专门用于每个图像，它包含了关于边界框坐标和文本的信息。但是这里的坐标值设置与Synth数据集完全不同。文本文件包含多行，每一行引用一个单词及其相应的边界框坐标。因此每行至少有9个值(8个坐标值，最后一个是实际文本)。在建模之前，我们需要将Synth和ICDAR边界框值转换成一个通用的结构。因此，我们提取了这些信息，并为ed a创建了一个类似于Synth数据框的数据框。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lt"><img src="../Images/31be509356c1eae07e7f8cfb563fbe71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TgNc2ym8LmGMG0TzJMREkA.png"/></div></div></figure><p id="4f5a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们需要忽略文本中的“###”值，因为这些值对最终用户没有用。下面是一些带有边框的ICDAR图像示例。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lu"><img src="../Images/d87b719e695431449f682d2191b2c06d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oa1TsR_-Tx4q3QsKu-yLXA.png"/></div></div></figure><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lv"><img src="../Images/82cb250e9a35110e0ff6de462b16b7f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l-pkvCNUSzH_IubMMNGnMA.png"/></div></div></figure><p id="f712" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于这些是真实世界的图像，每个图像可以包含许多文本和边界框。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lw"><img src="../Images/ab71b750dcf1bccb6f5a48f2d86bd106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C0_O12cLdha_tjIR_ujd9A.png"/></div></div></figure><p id="6220" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">大多数图像的边界框在10到50之间。但是有许多微小的文本和许多文本区域。所以我们看到很少的图像有大量的边界框。</p><h1 id="35b3" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">数据生成/地面实况生成</h1><p id="e5b8" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">到目前为止，我们只有原始图像和边界框坐标的值。但是在模型中，我们不能直接使用它们。因此，我们需要一个生成器函数，它可以将图像、文本和边界框值作为输入，并生成组织良好的矩阵/张量值，因为模型只能处理数值。</p><h2 id="029b" class="kw jd hh bd je kx ky kz ji la lb lc jm ip ld le jq it lf lg ju ix lh li jy lj bi translated">用于文本检测的数据生成</h2><p id="7c0a" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">根据FOTS的论文，对于给定的图像路径，我们需要检索图像、它的边界框坐标值和文本。然后，我们需要产生以下渠道。</p><ol class=""><li id="d21c" class="lx ly hh ig b ih ii il im ip lz it ma ix mb jb mc md me mf bi translated"><strong class="ig hi">评分图:</strong>根据像素是否包含文本，评分图对于图像中的每个像素只包含0或1。基本上，它设置它是否是一个文本区域。</li><li id="cb92" class="lx ly hh ig b ih mg il mh ip mi it mj ix mk jb mc md me mf bi translated"><strong class="ig hi"> Geo map: </strong>它有5个通道，其中4个通道是指一个像素到边界框的距离(左、右、上、下)，最后一个通道是指边界框的方向角。这些值仅适用于那些属于文本区域的像素。</li><li id="3125" class="lx ly hh ig b ih mg il mh ip mi it mj ix mk jb mc md me mf bi translated"><strong class="ig hi">训练掩码:</strong>该值代表一个“无关”区域。它基本上是一个没有文本或者文本远离那些部分的区域。它将在训练检测模型时使用。</li></ol><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ml"><img src="../Images/5a64150b288eb489eb1bb941c5656fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ImyydSJ9k5OpvoZOmg0ZqQ.png"/></div></div></figure><p id="3a6d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是检测数据生成器的代码片段。</p><figure class="kg kh ki kj fd kk"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="a0e9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们有其他的实用函数来支持生成器，比如shrink_poly，check_and_validate_poly，rectangle _ from _ parallelogram等等。(Github Repo中的详细信息—查看链接)</p><h2 id="7709" class="kw jd hh bd je kx ky kz ji la lb lc jm ip ld le jq it lf lg ju ix lh li jy lj bi translated">用于文本识别的数据生成</h2><p id="fd80" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">在文本识别的情况下，它需要精确文本的图像(不得包含周围的对象)。在ICDAR数据集中，我们可以下载带有相应文本的文本word图像。但是对于Synth数据集，我们需要手动生成单词图像和实际单词。代码片段如下。</p><figure class="kg kh ki kj fd kk"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="bddc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们有了一组图像和相应的文本。我们可以用它创建一个数据框架，并混合Synth和ICDAR数据，如下所示。</p><div class="kg kh ki kj fd ab cb"><figure class="mo kk mp mq mr ms mt paragraph-image"><img src="../Images/56cfbd8e223896ad9be77dffbe5c4608.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*rAGH0wBfYQgSrIEb7lmXeA.png"/></figure><figure class="mo kk mu mq mr ms mt paragraph-image"><img src="../Images/03bd10b38c2ea409e0a4e7ca998a2253.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*THlkt40h9-Lydgr7YIjFTQ.png"/></figure></div><p id="ffac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将创建一个识别生成器，它将使用此数据框并生成图像数据和矢量化文本，以便在识别模型训练期间使用它们。在我们的例子中，我们需要每个单词图像的大小为(64，128，3)，文本向量的长度为25。</p><figure class="kg kh ki kj fd kk"><div class="bz dy l di"><div class="mm mn l"/></div></figure><h1 id="7136" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">FOTS建筑</h1><p id="9640" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">整个架构由以下三部分组成:</p><ol class=""><li id="a819" class="lx ly hh ig b ih ii il im ip lz it ma ix mb jb mc md me mf bi translated">共享卷积</li><li id="4369" class="lx ly hh ig b ih mg il mh ip mi it mj ix mk jb mc md me mf bi translated">文本检测分支</li><li id="2ce6" class="lx ly hh ig b ih mg il mh ip mi it mj ix mk jb mc md me mf bi translated">认可处。</li></ol><p id="1df9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个额外的分支是ROIRotate，在文本检测之后使用。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es mv"><img src="../Images/a242c793c1e6af68e0156e06a3ec6dc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JOrZpacr1k7Yb3Duhw3OLA.png"/></div></div></figure><ol class=""><li id="ade5" class="lx ly hh ig b ih ii il im ip lz it ma ix mb jb mc md me mf bi translated"><strong class="ig hi">共享卷积:</strong>这个分支使用ResNet模型，在每一层之后，图像大小减少一半。然后，去卷积层会逐渐恢复这些大小。</li></ol><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es mw"><img src="../Images/b2f1a657e446ab1a5b3542f754a150f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wche08mDrvw1t-EdEQy7rg.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">共享conv图层</figcaption></figure><p id="2179" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上述架构中，橙色层是使用imageNet数据预先训练的ResNet层。我们从Res2、Res3和Res4获得输出，并在各自的解卷积层中重用它们。在要素地图大小为原始输入的1/32的Res5图层之后，将使用一系列去卷积图层。在将输入提供给下一层之前，之前的输出与Deconv输出之间存在残余连接。最后我们得到输出特征图，它的大小是原始输入的1/4。现在，这个特征图将用于文本检测分支。</p><p id="4e2c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.<strong class="ig hi">文本检测分支:</strong>文本<strong class="ig hi">T5】检测分支从共享卷积中取输入。它的实现类似于EAST paper(高效准确的场景文本检测器)。在这个分支中，我们在输入中增加了两个反卷积层，这样它的大小就和图像输入的大小一样了(512，512，64)。现在一个全卷积网络后，会给出6个信道(1个分数图，5个geo-map)。在我们获得给定图像的这些特征图/通道之后，NMS(非最大抑制)和IOU分数将用于获得边界框。</strong></p><p id="12b3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">NMS链接:<a class="ae kv" rel="noopener" href="/analytics-vidhya/non-max-suppression-nms-6623e6572536">https://medium . com/analytics-vid hya/non-max-suppression-NMS-6623 e 6572536</a></p><p id="af16" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">IOU评分:<a class="ae kv" rel="noopener" href="/@nagsan16/object-detection-iou-intersection-over-union-73070cb11f6e">https://medium . com/@ nag San 16/object-detection-IOU-intersection-over-union-73070 CB 11 f 6 e</a></p><p id="d111" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">roiorotate:</strong>它基本上将定向文本框转换为轴平行的框，这将有助于文本识别，因为我们不必再次处理定向。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es mx"><img src="../Images/f3c3e5f1973ce3aea4f58ba4e6287e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3GAIE8bQh8UOvCfsUXageQ.png"/></div></div></figure><p id="fb30" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.文本识别分支:这个分支负责从检测和旋转后得到的文本图像中获取文本标签。这里，输入图像仅包含文本，没有任何具有形状(64，128，3)的周围对象。我们有矢量化的文本输出，可以转换回文本。该识别分支由一系列conv和海最大池层组成，最终将使用LSTM/GRU层，然后是softmax输出层，如下所示。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es my"><img src="../Images/cde61a4b9cc87cbdad170235c50fe274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*a0U0vIlfwzmptSY_svSIcw.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">文本识别体系结构</figcaption></figure><h1 id="52a8" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">损耗</h1><p id="8322" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">我们在FOTS有两种模式(文本检测、文本识别)。所以他们会用各自的损失函数如下。</p><h2 id="4124" class="kw jd hh bd je kx ky kz ji la lb lc jm ip ld le jq it lf lg ju ix lh li jy lj bi translated">文本检测分支丢失</h2><p id="3ae3" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">文本检测模型使用骰子损失和IOU分数作为其损失函数。</p><p id="4e8d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">骰子损失:</strong>该损失通过与地面真值进行比较，确定像素是否是边界框边界的一部分(1或0)。参见<a class="ae kv" rel="noopener" href="/ai-salon/understanding-dice-loss-for-crisp-boundary-detection-bb30c2e5f62b">https://medium . com/ai-salon/understanding-dice-loss-for-crisp-boundary-detection-bb 30 C2 e 5 f 62 b</a>。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es mz"><img src="../Images/122470670170de0da185441e9c70c4d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*GKXSzjmwdACLlkk1_OvuMw.png"/></div></figure><p id="ba92" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">IOU损失:这种损失有助于为文本区域获得合适的边界框。它指并集上的交集。在这种情况下，它还考虑了方向损失。参见<a class="ae kv" rel="noopener" href="/nodeflux/distance-iou-loss-an-improvement-of-iou-based-loss-for-object-detection-bounding-box-regression-4cbdd23d8660">https://medium . com/node flux/distance-iou-loss-an-improvement-of-iou-based-loss-for-object-detection-bounding-box-regression-4c BDD 23d 8660</a></p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es na"><img src="../Images/68fa230539cd78beb5a42cd44a3974c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*9aciwxeIYLE1C0UHtgH0pg.png"/></div></figure><p id="bd25" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在总的检测损耗如下，</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es nb"><img src="../Images/910a9c35835911b2cafaf822843dd341.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*LLE55dO44poWTxGQO_c6Eg.png"/></div></figure><p id="da9c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，L_cls指芯片损耗，L_reg指IOU损耗。</p><p id="ca83" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是检测损失函数的代码片段。</p><figure class="kg kh ki kj fd kk"><div class="bz dy l di"><div class="mm mn l"/></div></figure><h2 id="c9f8" class="kw jd hh bd je kx ky kz ji la lb lc jm ip ld le jq it lf lg ju ix lh li jy lj bi translated">文本识别部门的损失</h2><p id="0100" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">在识别分支中使用的损失函数是CTC损失，其有助于将文本图像转换成实际的文本标签。参见<a class="ae kv" href="https://keras.io/examples/vision/captcha_ocr/" rel="noopener ugc nofollow" target="_blank">https://keras.io/examples/vision/captcha_ocr/</a>，<a class="ae kv" href="https://stackoverflow.com/questions/64321779/how-to-use-tf-ctc-loss-with-variable-length-features-and-labels" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/64321779/how-to-use-TF-CTC-loss-with-variable-length-features-and-labels</a></p><figure class="kg kh ki kj fd kk"><div class="bz dy l di"><div class="mm mn l"/></div></figure><h1 id="37bd" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">培训模型</h1><p id="aad9" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">我们有两个模型，即文本检测和文本识别，我们必须分别训练。我们已经有发电机来处理数据。最后，通过推理，我们可以使用ROIRotate一次性使用这两个模型。</p><p id="535d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">采样后我们有10k Synth图像和1000个ICDAR图像，我们可以用它们来训练检测模型。然后，利用从Synth生成的单词图像和下载的ICDAR中给定的单词图像，我们可以训练识别模型。</p><h2 id="3ccc" class="kw jd hh bd je kx ky kz ji la lb lc jm ip ld le jq it lf lg ju ix lh li jy lj bi translated">文本检测模型训练</h2><p id="0ace" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">Deconv操作需要一个自定义Deconv层，它将在ResNet层之后使用，最后，我们将使用完全连接的卷积层，它最终将为(512，512，3)的输入提供6个通道(512，512，6)的输出。它使用骰子损失和IOU损失作为其损失函数。</p><p id="3c65" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是检测模型的代码片段。</p><figure class="kg kh ki kj fd kk"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="0abd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，用Synth图像对模型进行50个时期的训练，学习率为0.0005，批次大小为3。回调函数可以用来获得更好的性能。损失时期图如下。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es nc"><img src="../Images/ad56c9e2ad1c13c09355e5ac6a84cfc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6mVQoZsFmg-oixDLfbmNbw.png"/></div></div></figure><p id="09f5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后用ICDAR数据训练同一个模型(可以用来自Synth和ICDAR数据的混合图像一次性训练)。它使用Synth数据中使用的相同参数进行训练。损失时期图如下。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es nd"><img src="../Images/c82b25896273f9abdde6ef3be93fa3e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRNGqazxFFmbZXtwuk_FRQ.png"/></div></div></figure><h2 id="a031" class="kw jd hh bd je kx ky kz ji la lb lc jm ip ld le jq it lf lg ju ix lh li jy lj bi translated">文本识别模型训练</h2><p id="e889" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">文本识别使用单词图像和文本向量进行训练。它使用ctc_loss作为它的损失函数。给定(68，128，3)和文本向量的输入，它为每个vocab索引产生256个向量的输出(这里是its (256，99))。</p><p id="3ff8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是识别模型的代码片段。</p><figure class="kg kh ki kj fd kk"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="8948" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在训练识别模型时，诸如ReduceLROnPlateau、EarlyStopping和ModelCheckpoint之类的回调函数被用于更好的控制和性能。该模型以0.001的学习率被训练了50个时期。在epoch 18，训练停止了，因为我们已经使用了上面的回调。损失时期图如下。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ne"><img src="../Images/fa28726c66e51c9e197c47d8e7ad38e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yq6VIpLvL2zs93I1rcSFTg.png"/></div></div></figure><p id="d546" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在两段训练结束后，模型权重被保存以备后用(推断)。</p><h1 id="c7bd" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">推理管道</h1><p id="3b98" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">训练完两个模型后，这是使用模型生成边界框和预测文本的最后一步。基本上给定一个输入图像，我们将在文本检测模型的帮助下预测得分图和地理图。通过NMS和IOU，我们可以得到包围盒，并通过旋转把它们转换成轴平行盒。现在，对于每个框，我们将运行识别模型来获取文本标签。</p><p id="40ff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们有两条管道用于文本检测和文本识别以及文本检测。检测管道返回带有边界框的输入图像，而识别管道返回带有边界框的图像和其中的文本标签。</p><p id="34fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">检测和识别管道的代码片段如下。</p><figure class="kg kh ki kj fd kk"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="6f63" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于给定的图像，我们使用这些管道来给出可以在推理中显示的输出。</p><p id="89de" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用检测管道，通过仅使用Synth数据训练的模型进行文本检测的输出:</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es nf"><img src="../Images/32bc3257d57840d4a31e351941effafb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*GIoSh2AN7m82DzWzbUm5Iw.png"/></div></figure><p id="87a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用检测管道通过Synth和ICDAR数据训练的模型进行文本检测的输出:</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es ng"><img src="../Images/426b03a2da7f43017745e70e4c7a92e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*ac6rb_GGOz2LzkMs873oCg.png"/></div></figure><p id="dc99" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如我们所见，探测器模型总体运行良好。</p><p id="0975" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在在最后的推理中，我们创建模型的对象并加载权重。然后，对于给定的输入，我们使用这些模型和管道类来给出最终输出。</p><p id="3d0a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在识别管道的帮助下，使用Synth和ICDAR数据训练的文本检测和文本识别模型的结果:</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es nh"><img src="../Images/9283f6df46c40cf46876ca46e7cd6c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*drpOLIwK2U5gHkeJBMZoDg.png"/></div></figure><p id="9070" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">文本识别模型的表现不如预期，因为我们用非常少的数据训练了该模型。</p><h1 id="a86f" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">部署</h1><p id="7ad6" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">对于部署，我们需要一个webapp，以便用户可以进行交互，并为他/她选择的任何给定输入获得带有边框和文本信息的输出图像。这里，我们构建了一个streamlit应用程序，如下所示。</p><figure class="kg kh ki kj fd kk"><div class="bz dy l di"><div class="ni mn l"/></div></figure><h1 id="a9bc" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">未来的工作</h1><ol class=""><li id="4a10" class="lx ly hh ig b ih ka il kb ip nj it nk ix nl jb mc md me mf bi translated">正如我们已经观察到的，我们在检测中没有使用足够的数据，特别是在识别模型中没有使用足够的数据来提供更好的性能。所以有了高计算能力，我们可以尝试更多的图像和不同类型的图像。</li><li id="3a94" class="lx ly hh ig b ih mg il mh ip mi it mj ix mk jb mc md me mf bi translated">我们可以利用模型架构和各种超参数来改进模型。</li></ol><h1 id="30e2" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">参考</h1><ol class=""><li id="3c62" class="lx ly hh ig b ih ka il kb ip nj it nk ix nl jb mc md me mf bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1801.01671.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1801.01671.pdf</a></li><li id="655a" class="lx ly hh ig b ih mg il mh ip mi it mj ix mk jb mc md me mf bi translated">https://www.appliedaicourse.com/<a class="ae kv" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank"/></li><li id="b454" class="lx ly hh ig b ih mg il mh ip mi it mj ix mk jb mc md me mf bi translated">【https://github.com/Masao-Taketani/FOTS_OCR T4】</li><li id="311e" class="lx ly hh ig b ih mg il mh ip mi it mj ix mk jb mc md me mf bi translated"><a class="ae kv" href="https://github.com/jiangxiluning/FOTS.PyTorch/tree/master/FOTS" rel="noopener ugc nofollow" target="_blank">https://github.com/jiangxiluning/FOTS.PyTorch/tree/master/FOTS </a></li></ol><h1 id="34fc" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">链接</h1><p id="e13a" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">GitHub库:<a class="ae kv" href="https://github.com/Amlan-Gopal/fots" rel="noopener ugc nofollow" target="_blank">https://github.com/Amlan-Gopal/fots</a></p><p id="15fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">LinkedIn Id:<a class="ae kv" href="https://www.linkedin.com/in/amlan-gopal-dhalasamanta-451570128" rel="noopener ugc nofollow" target="_blank">https://www . LinkedIn . com/in/am LAN-gopal-dhalasamanta-451570128</a></p></div></div>    
</body>
</html>