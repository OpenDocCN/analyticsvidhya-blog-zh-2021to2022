<html>
<head>
<title>Deep-dive into Variational Autoencoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入研究各种自动编码器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-dive-into-variational-autoencoders-d66c4a3df236?source=collection_archive---------5-----------------------#2021-06-06">https://medium.com/analytics-vidhya/deep-dive-into-variational-autoencoders-d66c4a3df236?source=collection_archive---------5-----------------------#2021-06-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="cf9e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="c60b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在之前关于自动编码器的文章中(<a class="ae ka" rel="noopener" href="/analytics-vidhya/under-and-over-autoencoders-3d695f428c1a">第一部分</a> &amp; <a class="ae ka" rel="noopener" href="/analytics-vidhya/implementing-under-over-autoencoders-using-pytorch-4ddaf458947e">第二部分</a>，我们探讨了欠自动编码器和过自动编码器的直觉、理论和实现。自动编码有两个部分:编码器和解码器。编码器将输入移动到潜在空间，而解码器试图从潜在空间表示中取回输入表示。</p><h2 id="63d0" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">经典自动编码器的问题是</h2><p id="53d1" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">然而，这些情况下的编码器本质上是确定性的，即，将输入中的值映射到潜在空间中的点，该点随后被解码器映射回输入空间。然而，一个好的自动编码器不应该学习输入数据的点表示，而是更多地学习潜在空间特征的分布，原因有两个:</p><ul class=""><li id="ae2a" class="kp kq hh je b jf kr jj ks jn kt jr ku jv kv jz kw kx ky kz bi translated">数据的特征没有点的表示。他们有分布。</li><li id="7548" class="kp kq hh je b jf la jj lb jn lc jr ld jv le jz kw kx ky kz bi translated">输入数据的流形结构应该是平滑的，而不是不连续的。</li></ul><p id="ab06" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">varrational auto encoders(VAE)试图通过使用潜在表示的概率模型来解决这些问题，该模型更好地理解潜在的因果关系，有助于更有效的概括。</p><h1 id="7617" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结构</h1><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/092adcc7ecee677a1bd94064438a0de6.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*-Rq74Gjw9F4vx2c7rD1niw.png"/></div></figure><p id="76e9" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">考虑<em class="lq"> z </em>，潜在空间或隐藏表征，输入<em class="lq"> x </em>，设<em class="lq"> z </em>有一个概率分布<em class="lq"> p(z) </em>。</p><p id="c616" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">为了一般化，我们希望有<em class="lq"> p(x)。</em>我们只能访问<em class="lq"> x </em>，我们想要捕获<em class="lq"> p(z|x)。</em></p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es lr"><img src="../Images/dbee89d968addebe3a616e2db632d2aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*7sVpsjYmQuaCalzkpiKxig.png"/></div></figure><p id="533f" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">所以，为了捕捉<em class="lq"> p(x) </em>，我们需要<em class="lq"> p(z)。</em>然而，由于<em class="lq"> z </em>不可接近，我们无法知道z的分布以及随后的p(z)，使得这个问题变得棘手。</p><p id="4535" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">但是，有另一种方法可以解决这个问题。潜在变量z可以被强制遵循已知的分布。VAE就是这么做的。在VAE，我们对潜在变量z实施高斯分布。高斯分布可以通过均值和方差来表征，而均值和方差是通过输入值来估计的。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es ls"><img src="../Images/2b9372a14de97195444a9d8615d67843.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AVIuncW8MA_bWTAOTx3Wgg.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">VAE结构。来源:作者</figcaption></figure><p id="c0b5" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">VAE有三个组成部分:</p><ul class=""><li id="33da" class="kp kq hh je b jf kr jj ks jn kt jr ku jv kv jz kw kx ky kz bi translated">编码器:将输入的<em class="lq"> n </em>维向量编码成<em class="lq"> 2h </em>维向量。<em class="lq"> h </em>是潜在空间的尺寸。此处，<em class="lq"> 2h </em>尺寸表示<em class="lq"> h </em>平均值和<em class="lq"> h </em>方差串联。</li><li id="e195" class="kp kq hh je b jf la jj lb jn lc jr ld jv le jz kw kx ky kz bi translated">采样器:取<em class="lq"> 2h </em>维长向量，基于h均值和h方差创建高斯分布，从中采样用于输出的<em class="lq"> z </em>值。</li><li id="16f0" class="kp kq hh je b jf la jj lb jn lc jr ld jv le jz kw kx ky kz bi translated">解码器:它采用高斯分布并输出重构向量，将其与输入向量进行比较以计算损失。</li></ul><h2 id="082f" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">如何加强潜在变量分布的结构</h2><p id="c30f" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">让我们考虑一下经典的自动编码器损失函数，即重建损失在这里的作用。z的分布可以认为是潜在空间中的气泡。当气泡完全不重叠时，重建损失被最小化，因为重叠将带来模糊点。因此，重建损失将z分布的气泡彼此推得尽可能远。如果没有关于这些气泡可以存在多远的规定，重建损失会将它们推得太远，使得分布与气泡所在的空间相比可以忽略不计，从而违背了首先获得分布的整个目的。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mb"><img src="../Images/98dba272f3123040df3c4cbbf826709d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QF84UzVP6Uf2XZuYF2UMXQ.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">由于重建损失，z分布气泡漂走。来源:作者</figcaption></figure><p id="d0fd" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">因此，我们需要一个损失项来约束分布气泡，并在单个气泡上实施高斯分布。这种作用是通过高斯分布相对于具有零均值和单位方差的正态分布的KL散度来实现的。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es mc"><img src="../Images/90d99498a5c82aff092c4869e8d06f43.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*xHif6Q0a81AGy7EgtSwY3w.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">VAE的损失。来源:作者</figcaption></figure><p id="8a17" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">高斯相对于标准正态分布的KL散度(或相对熵)为:</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es md"><img src="../Images/9455add35d8cd64de207dd169b22908b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*50knaAlYx5UZ7-kIqrwD3A.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">均值为E(z)方差为V(z)的高斯分布的KL散度。来源:作者</figcaption></figure><p id="2a9d" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">KL背离的最后一项对潜在变量的均值施加了L2惩罚，将它们拉向原点，阻止它们偏离。其余项，即<code class="du me mf mg mh b">V(z)-log(V(z))-1</code>在V(z)为1时具有最小值，这在我们寻找相对于标准正态分布的相对熵时是显而易见的。因此，复合损失项在减少重建误差和与标准正态分布的差异之间取得了平衡。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mi"><img src="../Images/121b8c0f3dc0c3a5da74b110e240abd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*eMlpu1EVYoJv_cjNtXI1vQ.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated"><code class="du me mf mg mh b">V(z) vs V(z)-log(V(z))-1. source: author</code></figcaption></figure><h2 id="acce" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">重新参数化的技巧</h2><p id="2bc8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们对输入解码器的潜在变量(z)进行高斯分布采样。然而，这在反向传播和随后的优化中产生了问题，因为当我们进行梯度下降来训练VAE模型时，我们不知道如何通过采样模块进行反向传播。</p><p id="99df" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">相反，我们使用重新参数化技巧对<em class="lq"> z </em>进行采样。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es mj"><img src="../Images/b168c3c2ff3baddb5737a549ccb4060e.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*ZQQYsAemGKJ7aF71mUbDcQ.png"/></div></figure><p id="1a8f" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">现在，反向传播是可能的，因为相对于<em class="lq"> z </em>的梯度只需要通过和与积函数。</p><h1 id="4d8d" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">使用PyTorch实现</h1><p id="9fec" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们将使用MNIST数据集进行解释。</p><p id="e95d" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">数据加载和转换步骤与传统编码器相似。我们将在这里关注VAE的建筑。</p><p id="2f76" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">我们用三个单元来定义一个简单的VAE，即编码器、解码器和采样器。采样器实现了上面讨论的重新参数化技巧。</p><h2 id="c0d2" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">基础架构</h2><figure class="lj lk ll lm fd ln"><div class="bz dy l di"><div class="mk ml l"/></div></figure><p id="bbe2" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">输入尺寸(784)对应于28×28的MNIST图像尺寸。输入数据从784维移动到400( <em class="lq"> dxd </em> ) dim，经过非线性层(ReLU)然后移动到<em class="lq"> 2d </em>超空间，这里d = 20。潜在空间是<em class="lq"> d </em> (20)维的。我们需要<em class="lq"> 2d </em>尺寸，因为它是<em class="lq"> d </em>平均值和<em class="lq"> d </em>方差的串联。</p><p id="f028" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">解码器模块也非常类似于经典的自动编码器。它获取一个d维潜在空间向量，通过一组线性和非线性层，输出大小为784的最终向量，与输入相同。</p><p id="912e" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">神奇的事情发生在采样器层。</p><figure class="lj lk ll lm fd ln"><div class="bz dy l di"><div class="mk ml l"/></div></figure><p id="3b88" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">采样器模块获取2d向量，并基于重新参数化技巧返回z。这里需要注意的一点是，我们没有将方差输入到采样器层，而是输入方差的对数。这是因为方差必须为正，而方差对数也可以为负。它确保方差总是正的，并且我们能够使用全范围的值作为输入。这也使过程更加稳定。</p><p id="fc01" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">接下来，我们将损失定义为重建损失和KL散度之和。该培训类似于传统的自动编码器，已在之前的<a class="ae ka" rel="noopener" href="/analytics-vidhya/implementing-under-over-autoencoders-using-pytorch-4ddaf458947e">文章</a>中介绍过。</p><p id="2d94" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">在训练之前，这是我们得到的重建图像，上面一行是原始图像，下面是重建图像:</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mm"><img src="../Images/c99b3d32637e8907a63a5be50b083ad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5stT1UOGa1H7tjIWShSZug.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">训练前的重建。来源:作者</figcaption></figure><p id="cc8c" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">仅仅20个时期之后，以下是重建的结果:</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mn"><img src="../Images/0ae96fe1868c101f5b61baae2de1363d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hFDdfr1fDsFRSQ-6TsScSA.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">实际vs训练后VAE重建。来源:作者</figcaption></figure><p id="0af2" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">为了查看潜在空间向量，我们生成几个随机样本，并使用解码器来生成样本:</p><p id="5ff1" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated"><code class="du me mf mg mh b">z_sample = torch.randn((8, d)).to(device)<br/>sample_out = model.decoder(z_sample)</code></p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mn"><img src="../Images/48d6c1ed6ee39db30b2dd3ed26d670c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jpqp1x7Z3hlHP_1DINSMgQ.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">潜在空间解码数据</figcaption></figure><p id="ec05" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">我们可以看到，潜在空间已经学会了数字的表示，尽管还不完全。随着训练次数的增加，隐含层可以更好地捕捉输入特征。</p><h2 id="79e6" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">两个输入之间的插值</h2><p id="bd91" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">通过取潜在空间表示的加权平均值，我们可以使用潜在空间表示在两个输入(这里是图像)之间进行插值。考虑数字0和6。我们可以看看从0到6的渐进过程。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mo"><img src="../Images/f40a6895ade654157e5c53b1401c70c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0mmN0c3Rh2lCgSzZm-OPag.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">将0转到6。来源:作者</figcaption></figure><p id="341f" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated">变换的每一步将插值图像的潜在变量定义为:</p><p id="f467" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz ha bi translated"><code class="du me mf mg mh b">z_interpolation = ((i/N)*z_6 + (1-i/N)*z_0) where i ranges from 0 to 8<br/>output_interpolation = model.decoder(z_interpolation)</code></p><h1 id="9e3e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><p id="5894" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">正如我们已经看到的，变分自动编码器代表的潜在变量不是点，而是概率云或分布。它不仅有助于使潜在的空间表现更普遍，而且它使流形更连接和光滑。VAE的基本概念模拟数据生成过程，可进一步用于GANs。</p></div></div>    
</body>
</html>