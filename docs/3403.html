<html>
<head>
<title>Scania Trucks Air Pressure System Failure Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">斯堪尼亚卡车气压系统故障预测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/scania-trucks-air-pressure-system-failure-prediction-ad6c43539d38?source=collection_archive---------2-----------------------#2021-06-29">https://medium.com/analytics-vidhya/scania-trucks-air-pressure-system-failure-prediction-ad6c43539d38?source=collection_archive---------2-----------------------#2021-06-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e96e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">使用机器学习解决预防性维护的初学者指南</em></p><blockquote class="jd je jf"><p id="6064" class="ie if jc ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated"><strong class="ig hi">描述:</strong>本博客总结了在重型斯堪尼亚卡车上使用机器学习预测气压系统故障的案例研究的方法和结果。</p></blockquote><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jj"><img src="../Images/9e50954d55d9d1e66e67cae85db43f19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*626vKVsFk-WXxpxLV_3BVQ.jpeg"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">图片来自斯堪尼亚网站。<a class="ae jz" href="https://www.scania.com/group/en/home/products-and-services/trucks.html" rel="noopener ugc nofollow" target="_blank">https://www . Scania . com/group/en/home/products-and-services/trucks . html</a></figcaption></figure><h1 id="3f8f" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">介绍</h1><p id="3318" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated"><strong class="ig hi">重型车辆</strong>作为道路运输的主要媒介，在工业领域发挥着巨大的作用。它们是最灵活、最经济的运输方式，每天都在各行各业运行。</p><p id="5fa7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">良好的维护是避免任何意外故障的关键，从而节省金钱和时间。在这种情况下，对所有车辆部件进行定期维护至关重要。</p><p id="b384" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中一个重要部件是<strong class="ig hi">气压系统(APS) </strong>。APS产生压缩空气，用于diﬀerent任务，如制动、换档等。，使其成为一个非常重要的维护主题。</p><h1 id="1dc6" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">内容</h1><ol class=""><li id="a1df" class="ld le hh ig b ih ky il kz ip lf it lg ix lh jb li lj lk ll bi translated">商业问题</li><li id="35b3" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">为什么是机器学习？</li><li id="aad9" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">业务限制</li><li id="6b58" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">数据集概述</li><li id="4ffb" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">绩效指标</li><li id="55a3" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">现有方法</li><li id="8773" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">探索性数据分析</li><li id="4003" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">缺失值插补</li><li id="29bb" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">首次切割方法</li><li id="02c5" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">使用自动编码器的特征工程</li><li id="136b" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">使用SMOTE进行过采样</li><li id="2cba" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">尝试机器学习模型</li><li id="2081" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">使用简化共享进行部署</li><li id="df04" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">结论</li><li id="6f96" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">潜在的改进</li><li id="2fab" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">链接到Github和LinkedIn</li><li id="563d" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">参考</li></ol><h1 id="f782" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">商业问题</h1><p id="e29b" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated"><strong class="ig hi">气压系统故障</strong>在重型车辆中很常见，这种故障的维修和保养费用很高。因此，当前的任务是开发一种解决方案，帮助预测卡车中的这种故障，从而防止或降低维护成本。</p><p id="1702" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该问题的数据集由斯堪尼亚为IDA 2016<a class="ae jz" href="https://ida2016.blogs.dsv.su.se/?page_id=1387" rel="noopener ugc nofollow" target="_blank">【1】</a>工业挑战赛提供。</p><h1 id="3767" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">为什么是机器学习？</h1><p id="231c" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">在试图解决这种预防性维护问题之前，人们需要问，“为什么要使用机器学习来解决这个问题？机器学习如何比现有的方法更好地解决这个问题。”要回答这个问题，让我们首先了解什么是预防性维护。</p><blockquote class="jd je jf"><p id="1b60" class="ie if jc ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated"><strong class="ig hi">什么是预防性维护？</strong>在预防性维护场景中，随着时间的推移收集数据以监控设备的状态。目标是找到有助于预测并最终防止失败的模式。</p></blockquote><p id="b119" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在工业中，这种预防性维护通常由某些设施管理者在SCADA系统<em class="jc">(SCADA:</em><em class="jc">A</em><strong class="ig hi"><em class="jc"/></strong><em class="jc">计算机系统用于收集和分析实时数据)的帮助下完成。</em>然而，机器学习方法的优势在于它消除了大部分猜测，并帮助设施经理专注于其他重要任务。此外，与SCADA系统不同，使用机器学习可以完全自动化这种预防性维护技术，并且还可以获得反映数据中所有特征的详细预测。</p><h1 id="79a2" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">业务限制</h1><p id="d927" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">这个问题的主要约束是最小化做出错误预测的总成本，解释如下。</p><p id="0886" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">总成本=成本1 *误报+成本2 *漏报</strong></p><p id="6ce4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">成本1:由技工进行的不必要的检查(10美元)</em></p><p id="eda7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">成本2:丢失一辆有故障的卡车，这可能会导致未来的故障(500美元)</em></p><p id="cf29" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">目标是最小化总成本，人们可以注意到成本2远远超过成本1。这意味着我们的ML模型应该能够尽可能多地减少假阴性。</p><p id="2f9c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另一个限制是模型应该能够<strong class="ig hi">在几秒钟内预测APS的故障。</strong></p><h1 id="b4b7" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">数据集概述</h1><p id="35ce" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">该数据集包括从日常使用的重型斯堪尼亚卡车上收集的数据。数据集的正类由APS的特定组件的组件故障组成。负类包括与APS无关的部件出现故障的卡车。</p><p id="5580" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">数据集链接:</strong></p><ol class=""><li id="5bbd" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb li lj lk ll bi translated"><a class="ae jz" href="https://ida2016.blogs.dsv.su.se/?page_id=1387" rel="noopener ugc nofollow" target="_blank">https://ida2016.blogs.dsv.su.se/?page_id=1387【1】</a></li><li id="9c0f" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><a class="ae jz" href="https://www.kaggle.com/uciml/aps-failure-at-scania-trucks-data-set" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/UC IML/APS-failure-at-Scania-trucks-data-set[2]</a></li></ol><p id="fd35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据集中有两组文件:</p><ol class=""><li id="d83c" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb li lj lk ll bi translated">包含60，000个样本的训练集，其中59，000个属于负类，1，000个属于正类。</li><li id="494e" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">总共包含16，000个示例的测试集，其中15，625个属于负类，375个属于正类。</li></ol><p id="dc61" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这两个训练集和测试集都是从相同的源采样的，具有关于类的分层。因此，它们应该包含大致相同的故障和非故障组件分布。</p><p id="11bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">出于专有原因，数据的属性名称已被匿名化。它由单个数字计数器和直方图组成，直方图由不同条件的仓组成。</p><p id="edfb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据集总共有171个要素，包括类标签，缺失值用“na”表示。在这171个特征中，70个特征是7个直方图的直方图特征，每个直方图具有10个bin值。</p><h1 id="ca42" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">绩效指标</h1><p id="4045" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">我们将使用<strong class="ig hi">宏F1分数</strong>作为性能指标。宏F1分数<a class="ae jz" href="https://peltarion.com/knowledge-center/documentation/evaluation-view/classification-loss-metrics/macro-f1-score" rel="noopener ugc nofollow" target="_blank"> [3] </a>分别考虑每个班级的F1分数，并计算其平均值。这种方法适用于像我们这样不平衡的数据集。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es lu"><img src="../Images/1245c976f3e5c84720a31c1a62e0beac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*cp6kUR59EMfbNAKLUxsljg.png"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">宏观F1分数</figcaption></figure><p id="5d7b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于这个问题的误分类成本很高，所以我们的模型应该有很高的精度和召回率。F1分数是一个很好的指标，它结合了精确度和召回率。</p></div><div class="ab cl lv lw go lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ha hb hc hd he"><h1 id="9198" class="ka kb hh bd kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt mg kv kw kx bi translated">现有最佳方法</h1><blockquote class="jd je jf"><p id="d32d" class="ie if jc ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated"><a class="ae jz" href="https://link.springer.com/chapter/10.1007/978-3-319-46349-0_33" rel="noopener ugc nofollow" target="_blank">卡蜜拉·费雷拉·科斯塔和马里奥·纳西门托。<strong class="ig hi"><em class="hh">“IDA 2016工业挑战:使用机器学习预测故障。”</em> </strong>智能数据分析国际研讨会。施普林格，2016 [4]。</a></p></blockquote><ol class=""><li id="74b1" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb li lj lk ll bi translated">以上是这个问题的赢家解决方案。作者尝试了不同的算法，即<em class="jc">逻辑回归、K-NN、SVM、决策树和随机森林</em>。</li><li id="088c" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">作为预处理步骤，作者通过实现软估算算法来处理缺失数据。软估算<a class="ae jz" href="https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf" rel="noopener ugc nofollow" target="_blank">【5】</a>是一种用于大规模矩阵补全的eﬃcient算法。它用猜测代替缺失值，然后使用软阈值奇异值分解(SVD)在完整矩阵上解决优化问题，即核范数正则化最小二乘问题。</li><li id="31d8" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">类别不平衡是通过给负面和正面实例不同的类别权重来处理的。每个类别的权重被设置为与相应类别的病例分数成反比。此外，增加了对数据点进行分类的概率阈值。这意味着一个实例只有在非常确定的情况下才会被归类为阴性。</li><li id="354b" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">结果<a class="ae jz" href="https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks" rel="noopener ugc nofollow" target="_blank">【6】</a>显示随机森林模型表现最佳，给出了9920美元的<strong class="ig hi">分数，有542个假阳性和9个假阴性。</strong></li></ol><blockquote class="jd je jf"><p id="8096" class="ie if jc ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">克里斯托弗·冈德克、丹尼尔·哈夫纳和奥利弗·r·桑普森。<strong class="ig hi"> <em class="hh">“使用随机森林和特征工程预测斯堪尼亚卡车气压系统的故障。”</em> </strong>智能数据分析国际研讨会。施普林格，2016 [7]。</p></blockquote><ol class=""><li id="d324" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb li lj lk ll bi translated">这是这个问题的第二个最好的解决方案。作者在直方图特征上使用随机森林模型和特征工程。</li><li id="3d88" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">中位数插补用于处理缺失值，即缺失值用每个特征的中位数替换。</li><li id="b142" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">作为特征工程步骤，为每个直方图计算16个新的diﬀerent特征。这些新要素是使用两个diﬀerent距离函数得出的与数据中现有要素的距离。</li><li id="9836" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">给定和计算的特征组合在一起产生282个维度，不包括类列。因此，使用特征选择技术，并且使用随机森林仅取顶部特征进行训练。</li><li id="47fa" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">通过从默认值0.5增加概率阈值来处理类别不平衡问题。每当预测概率低于或等于阈值时，预测类别被设置为“pos”</li><li id="27b4" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">在超参数调整之后，当使用210个顶部特征且概率阈值为0.95时，找到最佳可能预测。结果<a class="ae jz" href="https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks" rel="noopener ugc nofollow" target="_blank">【6】</a>显示该模型给出了10，900美元的<strong class="ig hi">分数，其中有490个假阳性和12个假阴性。</strong></li></ol></div><div class="ab cl lv lw go lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ha hb hc hd he"><h1 id="ae91" class="ka kb hh bd kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt mg kv kw kx bi translated">探索性数据分析</h1><p id="73c2" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">对数据集进行全面的EDA将使我们更好地理解数据集的功能以及它们之间的关系。它也有助于得出结论和获得特征工程的想法。</p><p id="ab6a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据集中总共有170个要素(不包括类别标注),出于专有原因，所有要素都被匿名化。首先，我们将做一些基本的数据清理，如将类标签从“pos”和“neg”替换为0和1，用“NaN”替换丢失的值。</p><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="mh mi l"/></div></figure><h2 id="2f91" class="mj kb hh bd kc mk ml mm kg mn mo mp kk ip mq mr ko it ms mt ks ix mu mv kw mw bi translated">检查数据不平衡</h2><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es mx"><img src="../Images/ee39acdf3396d654cd45ef38d7b50b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*Z7cw_zbeuJb2h9tT2g0LFA.png"/></div></figure><p id="119b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们有一个高度不平衡的数据集，其中59，000个数据点属于负类，1，000个数据点属于正类。</p><h2 id="c946" class="mj kb hh bd kc mk ml mm kg mn mo mp kk ip mq mr ko it ms mt ks ix mu mv kw mw bi translated">移除零方差要素</h2><p id="caef" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">由于总共有170个特征，我们必须检查任何具有零方差的特征(所有数据点具有相同的值)并删除它们。</p><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="2351" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们观察到特征“cd_000”具有零方差，并将其移除。</p><h2 id="7903" class="mj kb hh bd kc mk ml mm kg mn mo mp kk ip mq mr ko it ms mt ks ix mu mv kw mw bi translated">删除重复的数据点</h2><p id="5659" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">此外，我们还必须删除数据中的任何重复项。在总共60，000个数据点中，只有一个数据点被发现是重复的并被删除。现在，数据点的总数是59，999。</p><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="mh mi l"/></div></figure><h2 id="50c9" class="mj kb hh bd kc mk ml mm kg mn mo mp kk ip mq mr ko it ms mt ks ix mu mv kw mw bi translated">分离直方图和数字特征</h2><p id="f46c" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">在数据集描述中，已经给出了在170个特征中，一些是由具有不同<br/>条件的仓组成的直方图特征。让我们将这些特征分离为直方图特征，并将剩余的特征分离为数字特征。</p><p id="6d00" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据集中的每个要素都使用术语“Identifier_Bin”来命名。(例如:- 'ah_000 '，' ag_001 ')。只有直方图要素的条柱值大于0。对于数字特征，此bin值始终为“000”。因此，使用bin值，我们可以识别直方图特征。</p><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="545c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总共有70个直方图特征，具有7个不同的标识符。换句话说，有<strong class="ig hi"> 7个直方图被分成10个仓，每个仓有</strong>。这7个标识符是[ag，ay，az，ba，cn，cs，ee]。对于每个标识符，我们有从0到9的bin值。标识符“ag”的示例，特征为“ag_000”、“ag_001”、“ag_002”、“ag_003”、“ag_004”、“ag_005”、“ag_006”、“ag_007”、“ag_008”、“ag_009”。</p><p id="9a9a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">既然我们已经分离了直方图和数字特征，我们将从中提取10个最重要的特征，并对它们执行EDA。我们将使用<strong class="ig hi"/><strong class="ig hi"/><a class="ae jz" href="https://machinelearningmastery.com/rfe-feature-selection-in-python/" rel="noopener ugc nofollow" target="_blank"/>递归特征消除技术来获取重要特征。RFE是一种包装类型的向后特征选择算法。这意味着在该方法的核心中给出并使用不同的机器学习算法，该算法由帮助选择特征的RFE包装。</p><h2 id="7227" class="mj kb hh bd kc mk ml mm kg mn mo mp kk ip mq mr ko it ms mt ks ix mu mv kw mw bi translated">数值和直方图特征分析</h2><p id="cb65" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">我们从递归特征消除中获得的前10个直方图特征是:<strong class="ig hi"> ['ag_001 '，' ag_002 '，' ag_003 '，' ay_005 '，' ba_004 '，' cn_004 '，' cs_002 '，' ee_005 '，' ba _ 000 ']</strong></p><p id="bce5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">并且前10位的数值特征是:<strong class="ig hi"> ['ah_000 '，' ai_000 '，' al_000 '，' am_0 '，' aq_000 '，' bj_000 '，' bk_000 '，' cj_000 '，' df_000 '，' dn_000']。</strong></p><p id="60ac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们将对这些特征执行<strong class="ig hi">单变量分析</strong>，方法是将数据与r.t类标签可视化。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es my"><img src="../Images/3d67482c45c47d070f5807e07c4d68d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fLCf4P03ew3agdVLy2uwZg.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">直方图特征上的小提琴图</figcaption></figure><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es mz"><img src="../Images/ca936e1911ee53e568fe021ed049ab91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RswOqS2PDFHjoHRcpVgZJw.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">数字特征上的小提琴图</figcaption></figure><ol class=""><li id="2b86" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb li lj lk ll bi translated">从我们在violin图中看到的pdf来看，与正数据点的值相比，大多数负数据点的值较小。随着特征值的增加，数据点属于正类的概率也增加。</li><li id="374f" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">让我们考虑直方图特征[ 'ba_004 '，' cn_004 '，' cs_002 '，' ee_005 '，' ba_000' ]和数值特征[ 'ah_000 '，' aq_000 '，' bj_000 '，' dn_000' ]。对于负数据点，四分位间距(IQR)非常小，几乎看不见。与正数据点相比，它需要非常小的值。对于正数据点，IQR相当宽，其第25百分位远高于负数据点的IQR。这意味着我们可以使用这些特征很好地对数据点进行分类。</li><li id="a87a" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">直方图特征[ 'ag_001 '，' ag_002 '，' ag_003 '，' ay_005 '，' cn_000' ]和数字特征[ 'ai_000 '，' al_000 '，' am_0 '，' cj_000 '，' df_000' ]对大多数负数据点取接近0的值。因此，如果这些特性的值接近于0，则APS中没有故障的可能性很高。但是随着这些特征值的增加，APS中出现故障的可能性变高。</li><li id="f7bb" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">对于数字特征‘bk _ 000’，正负数据点之间有明显的重叠。大多数负数据点的值小于0.5倍10⁶.此外，正数据点的第75百分位具有接近0.5倍10⁶的值，这意味着大约25%的正数据点具有大于0.5倍10⁶.的值也就是说，如果特征值大于0.5 x 10⁶，那么APS很有可能出现故障。</li><li id="9038" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">从violin图中，我们看到直方图特征<strong class="ig hi"> [ 'ba_004 '，' cn_004 '，' cs_002 '，' ee_005 '，' ba_000' ] </strong>和数值特征<strong class="ig hi"> [ 'ah_000 '，' aq_000 '，' bj_000 '，' dn_000' ] </strong>对于数据点的分类相当好。让我们看看这些特征相对于目标变量的<strong class="ig hi">百分位值</strong></li></ol><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es na"><img src="../Images/9a422c3d73be115299b4622809388792.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*-xPLmK-SDst-IhmZGy3KBg.png"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">顶部直方图特征的百分位值</figcaption></figure><p id="b242" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可以看出，对于每个特征，负数据点的第90个百分点小于正数据点的第25个百分点。因此，使用这些直方图特征，75%的数据点可以被正确分类，误差因子为10%。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es nb"><img src="../Images/fa42bbf0fb27815fdde218a077e6e73e.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*aN36jcUYYPS4dCQPk9BjMw.png"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">顶级数字特征的百分位值</figcaption></figure><p id="d6ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种情况下，可以注意到对于每个特征，负数据点的第90个百分点小于正数据点的第10个百分点。因此，once可以得出结论，大约90%的数据点可以使用这些数字特征以10%的误差因子进行正确分类。</p><p id="d030" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们研究这些特征的<strong class="ig hi">CDF</strong>，以获得每个特征在数据集分类中的大致范围。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es nc"><img src="../Images/18d01c95b7c335f6b0517cd863fcfe2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mc6IZc2fqicBUnA6IPfhJA.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">直方图特征上的CDF图</figcaption></figure><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es nd"><img src="../Images/5bcc04330fa02bd8ca2b0b611f1c251e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*APYYdJJKduCcizuvZ9MqeA.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">数字特征上的CDF图</figcaption></figure><ol class=""><li id="09bb" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb li lj lk ll bi translated">对于直方图特征[ 'cn_004 '，' ba_000' ]和[ 'ba_004 '，' cs_002 '，' ee_005' ]，超过95%的负数据点的特征值分别小于0.1倍10⁸和0.1倍10⁷，而超过40%的正数据点的特征值大于0.1倍10⁸和0.1倍10⁷.这意味着大约60%的数据点可以使用这些直方图特征进行正确分类，误差为5%。</li><li id="d303" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">在数值特征['dn_000']和[ 'ah_000 '，' aq_000 '，' bj_000' ]的情况下，超过95%的负数据点分别取小于0.1倍10⁶和0.1倍10⁷的特征值，而超过90%的正数据点取大于0.1倍10⁶和0.1倍10⁷.的特征值因此，人们可以得出结论，使用这些数字特征，几乎95%的数据点可以被正确分类，误差为5%。</li></ol><p id="a2d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们对前5个直方图和数字特征执行<strong class="ig hi">双变量分析</strong>。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es ne"><img src="../Images/fc2feb8b10bb9edb2e754be328325dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kjew1F88MegwiFztNeWDGg.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">对前5个直方图特征进行配对绘图。</figcaption></figure><ol class=""><li id="b527" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb li lj lk ll bi translated">如果特征‘ag _ 001’的值大于0.1倍10⁶，并且特征‘ag _ 002’的值大于0.1倍10⁷，那么这些数据点将属于正类。即AP中出现故障的可能性很高。这些特征与其他特征高度不相关['ba_000 '，' ee_005 '，' cs_002' ]。</li><li id="4ee0" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">从特征‘ee _ 005’与‘ba _ 000’以及‘ee _ 005’与‘cs _ 002’之间的关系图来看，如果特征‘ee _ 005’的值大于1x10⁷，那么很可能是APS出现故障。</li><li id="2adc" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">所有其他图都有明显的重叠，无法得出明确的结论。</li></ol><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es ne"><img src="../Images/cddbb00533ffaf589be952161519f12c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jnDZM7gsRzsnZAbW7bdVLA.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">前5个数字特征的配对图</figcaption></figure><ol class=""><li id="9c53" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb li lj lk ll bi translated">如果特征‘am _ 0’和‘al _ 000’的值大约大于0.2 x 10⁷，那么APS很可能出现故障。此外，这两个特性与其他特性高度不相关。</li><li id="3a62" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">所有剩余的图都有明显的重叠，不能从中得出明确的结论。</li></ol><h2 id="748b" class="mj kb hh bd kc mk ml mm kg mn mo mp kk ip mq mr ko it ms mt ks ix mu mv kw mw bi translated">总结探索性数据分析得出的结论</h2><ol class=""><li id="393e" class="ld le hh ig b ih ky il kz ip lf it lg ix lh jb li lj lk ll bi translated">数据高度不平衡，几乎98%的数据点属于负类。</li><li id="e705" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">与正数据点相比，负数据点的大部分特征值较小，这意味着当特征值较高时，APS出现故障的可能性较高。</li><li id="9c82" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">使用顶部直方图特征['ba_004 '，' cn_004 '，' cs_002 '，' ee_005 '，' ba_000']，大约60%的数据点可以被正确分类。</li><li id="94f0" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">使用顶部数字特征[ 'ah_000 '，' aq_000 '，' bj_000 '，' dn_000' ]，大约90%的数据点可以被正确分类。</li><li id="16a8" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">发现与直方图特征相比，数字特征在分类数据点方面更好。</li><li id="13d2" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">要素之间存在大量多重共线性。</li><li id="d401" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">存在一些具有高特征值的负数据点，使得难以在正数据点之间进行分类。</li></ol><h1 id="e994" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">缺失值插补</h1><p id="e20e" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">首先，让我们找出数据集中每个要素缺失值的百分比。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es nf"><img src="../Images/4cba4c80e8d5c9c15ade547910079060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XAtweY8PusiwpKXmTTsAHw.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">要素缺失值的百分比。</figcaption></figure><p id="dc22" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上面的情节我们看到，</p><ul class=""><li id="feb9" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb ng lj lk ll bi translated">8个特征的缺失值超过60%，</li><li id="f7e8" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb ng lj lk ll bi translated">16个特性的缺失值在20%-60%之间，并且</li><li id="a865" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb ng lj lk ll bi translated">其余特征的缺失值小于20%。</li></ul><p id="e6b9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们遵循的估算策略是:</p><ol class=""><li id="e7a9" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb li lj lk ll bi translated">我们将消除缺失值大于60%的要素。</li><li id="d420" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">我们将对缺失值小于20%的特征进行中值插补</li><li id="017d" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">对于20%-60%缺失值之间的特征，我们将执行基于模型的插补，称为MICE插补<a class="ae jz" href="https://stats.stackexchange.com/questions/421545/multiple-imputation-by-chained-equations-mice-explained" rel="noopener ugc nofollow" target="_blank">【9】</a>。</li></ol><p id="d24b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">MICE使用分而治之的方法，即一次只关注一个变量，来估算数据集变量中的缺失值。一旦将焦点放在一个变量上，MICE就会使用数据集中的所有其他变量(或这些变量中明智选择的子集)来预测该变量中缺少的值。预测基于回归模型，模型的形式取决于焦点变量的性质(例如，年龄和收入将需要线性回归模型来预测其缺失值，但性别将需要逻辑回归模型)。</p><blockquote class="jd je jf"><p id="0ea4" class="ie if jc ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated"><strong class="ig hi">在进行插补之前，我们将把数据集分成训练和CV。我们不必拆分测试数据集，因为它已经可用。</strong></p></blockquote><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="95ac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于MICE是基于模型的插补，我们将在进行插补之前<strong class="ig hi">标准化</strong>数据集。</p><p id="2a4b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于中位数和MICE插补，<strong class="ig hi">插补模型仅适用于训练数据</strong>，然后使用该模型我们将转换CV和测试数据。</p><h1 id="9cff" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">首次切割方法</h1><p id="d30a" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">既然我们已经为建模准备好了预处理和估算数据集，我将简要介绍我解决这个问题的方法。预处理后的数据有161个特征，不包括类别标签。根据这些数据，执行以下步骤。</p><ol class=""><li id="5f28" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb li lj lk ll bi translated"><strong class="ig hi">使用自动编码器的特征工程:</strong>自动编码器是一种神经网络，可用于学习原始数据的压缩表示。使用自动编码器，数据被压缩成50个维度，并与现有数据相结合，然后用于建模。</li><li id="f43b" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">使用SMOTE的上采样:</strong>SMOTE<a class="ae jz" href="https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/" rel="noopener ugc nofollow" target="_blank">【10】</a>是一种过采样技术，从少数类生成合成<strong class="ig hi">样本</strong>。使用这种思想解决了数据不平衡的问题。</li><li id="a50c" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">在执行上述两个特征工程技术之后，创建随机模型(哑模型),其将所有数据点分类为属于多数类(负)。使用此模型，可以获得成本指标的最差值。使用这个分数，检查其他传统模型的性能。</li><li id="e88e" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">现在，数据集在传统模型上进行训练，如逻辑回归、线性SVM、决策树、随机森林、XGBoost和AdaBoost。此外，数据集是在一个定制的集合模型上训练的。</li><li id="e13a" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">使用GridSearchCV和RandomizedSearchCV对上述所有模型执行超参数调整。然后，根据我们从超参数调整中获得的最佳参数值重新训练模型。之后，对所有这些模型的训练、CV和测试数据计算宏观F1分数。</li><li id="a56b" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">在我们的问题中，假阴性(FN)的成本高于假阳性(FP)的成本。这就要求召回比精确更重要。因此，概率阈值必须从默认值0.5改变。</li><li id="22fe" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">问题中提到的代价度量是<br/> Cost = (10 x FP) + (500 x FN)。正确的概率阈值是给出最小成本的阈值。</li><li id="4a3b" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">Sklearn的精确召回曲线用于获取阈值。精确召回曲线计算不同概率阈值的精确召回对。</li><li id="8e99" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">使用这些概率阈值，计算成本，并取给出最小成本的概率值。<strong class="ig hi">交叉验证数据集用于</strong>寻找该概率阈值。</li><li id="a26c" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">使用该概率阈值，预测训练、CV和测试数据的类别标签，并且计算所有模型的训练、CV和测试成本。给出最小测试成本的模型被认为是最佳模型。</li></ol><h1 id="d07b" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">使用自动编码器的特征工程</h1><p id="93c7" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">自动编码器<a class="ae jz" href="https://machinelearningmastery.com/autoencoder-for-classification/" rel="noopener ugc nofollow" target="_blank">【11】</a>是一种神经网络，可以用来学习原始数据的压缩表示。</p><p id="9f21" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">自动编码器由编码器和解码器子模型组成。编码器压缩输入，解码器试图从编码器提供的压缩版本中重建输入。训练后，保存编码器模型，丢弃解码器。</p><p id="cb69" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，编码器可以用作数据准备技术，以对原始数据执行特征提取，该原始数据可以用于训练不同的机器学习模型。</p><h2 id="7f7e" class="mj kb hh bd kc mk ml mm kg mn mo mp kk ip mq mr ko it ms mt ks ix mu mv kw mw bi translated">定义编码器解码器模型</h2><ol class=""><li id="3a9d" class="ld le hh ig b ih ky il kz ip lf it lg ix lh jb li lj lk ll bi translated">我们将编码器定义为具有两个隐藏层，第一个具有两倍的输入数量(这里是161 x 2 = 322)，第二个具有相同的输入数量(这里是161)，后面是大小为50的瓶颈层。也就是说，我们将数据从161维压缩到50维。</li><li id="85d4" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">为了确保模型学习良好，我们将使用批量规范化和泄漏ReLU激活。</li><li id="e416" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">解码器将以类似的结构来定义，尽管是相反的。它有两个隐藏层，第一个具有数据集中的输入数(此处为361)，第二个具有两倍的输入数(此处为361 x 2 = 322)。输出图层的节点数与输入数据中的列数相同，并使用线性激活函数输出数值。</li><li id="0127" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">使用Adam optimizer拟合该模型，并尝试最小化均方误差。</li></ol><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es nh"><img src="../Images/3db245ede4104c16f159f638883b0d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*xlc89r9RVGXP16PSRgpHww.png"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">MSE与Epoch</figcaption></figure><p id="d1cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在训练编码器解码器模型50个时期之后，可以看到均方误差(MSE)非常低。使用模型的编码器部分，我们将训练、CV和测试数据编码为50维。我们将使用这50维特征作为现有数据的附加特征，从而总共得到161+50 = 211个特征。</p><h1 id="4776" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">使用SMOTE进行过采样</h1><p id="b13b" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">由于数据极不平衡，模型可能无法有效地学习决策边界。SMOTE<a class="ae jz" href="https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/" rel="noopener ugc nofollow" target="_blank">【10】</a>能够平衡数据的分类分布，从而更好地概括分类器。为此，可以采用以下步骤:</p><ol class=""><li id="3b0c" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb li lj lk ll bi translated">按多数类的50%对少数类进行过采样。</li><li id="01e8" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">然后，对多数类进行欠采样，将多数类中的样本数量减少到比少数类多60%左右。</li></ol><p id="3be1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">关于SMOTE<a class="ae jz" href="https://arxiv.org/pdf/1106.1813.pdf" rel="noopener ugc nofollow" target="_blank">【12】</a>和各种实验的原始论文表明，过采样和欠采样的组合比单独使用少数类的过采样更有效。</p><p id="fd83" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">采样后，我们得到34，415个负数据点和20，649个正数据点。每个数据点的特征总数为211。</p><h1 id="0414" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">尝试机器学习模型</h1><p id="8a89" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">既然我们已经有了预处理和特征工程数据集，我们就可以开始建模了。我们将在传统的机器学习模型上训练数据集，并使用宏F1分数作为度量来执行超参数调整。然后我们将在最佳超参数上重新训练模型。我们将尝试的不同模型是<strong class="ig hi">逻辑回归、线性SVM、决策树、随机森林、XGBoost、AdaBoost和定制集成。</strong></p><p id="287a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为与假阳性相比，假阴性的误分类成本较高，所以我们应该改变概率阈值的默认值0.5。为此，我们将使用<strong class="ig hi"> sklearn的精确召回曲线。</strong>精确召回曲线计算<strong class="ig hi">交叉验证数据集上不同概率阈值的精确召回对。</strong>正确的阈值将是给出最小成本的阈值。使用这个概率阈值，我们将评估训练、cv和测试数据的成本。</p><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="mh mi l"/></div></figure><h2 id="00f3" class="mj kb hh bd kc mk ml mm kg mn mo mp kk ip mq mr ko it ms mt ks ix mu mv kw mw bi translated"><strong class="ak">定制合奏</strong></h2><p id="7c03" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">要实现定制集合，让我们执行以下步骤:</p><ol class=""><li id="173e" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb li lj lk ll bi translated">将训练数据集拆分为D1和D2(50-50)。</li><li id="ac15" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">从这个d1开始，进行替换取样，以创建D1、d2、d3…dk (k个样本)。</li><li id="ff44" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">创建“k”个模型，并用这k个样本中的每一个来训练每个模型。</li><li id="251e" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">将D2集传递给这k个模型中的每一个，就可以从这些模型中的每一个得到对D2的k个预测。</li><li id="fc82" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">使用这k个预测，创建一个新的数据集。对于D2，我们已经知道它相应的目标值。因此，用这k个预测训练一个元模型。</li><li id="7f58" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">对于模型评估，将交叉验证数据集传递给每个基础模型，其中一个得到“k”个预测。用这k个预测创建一个新的数据集，并将其传递给元模型进行最终预测。现在，使用最终预测以及交叉验证数据集的目标，可以计算模型的性能得分。</li><li id="2326" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">在给定的问题中，使用的基本模型是决策树，随机森林用作元模型。必须对k个样本、样本大小和元模型进行超参数调整。</li><li id="aee4" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">然后，我们将在最佳超参数上重新训练该模型，并以我们对传统模型所做的相同方式计算概率阈值。使用这个概率阈值，我们将计算训练、cv和测试成本。</li></ol><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="ebf2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在对上述所有模型进行实验后，发现随机森林模型给出了最好的结果，给出了8820美元的最低测试<strong class="ig hi">分数，有482个误报和8个漏报。</strong></p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es ni"><img src="../Images/54798b0f3c7c876a4bda8afd66313086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*DJlqysB-zic8lY4BjHeWiQ.png"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">模型测试分数</figcaption></figure><h1 id="d843" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">使用简化共享进行部署</h1><p id="1795" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">为了给读者更好的用户体验，我使用Streamlit<a class="ae jz" href="https://streamlit.io" rel="noopener ugc nofollow" target="_blank">【13】</a>部署了该模型，这是一个用于机器学习和数据科学项目的开源应用框架。</p><p id="5451" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">部署的模型可以在这里访问:<a class="ae jz" href="https://share.streamlit.io/sharathsolomon/scaniaapsstudy/main/final.py" rel="noopener ugc nofollow" target="_blank">https://share . streamlit . io/sharath Solomon/scaniaapsstudy/main/final . py</a></p><p id="beb5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">创建的web应用程序有两个选项。</p><ol class=""><li id="d16e" class="ld le hh ig b ih ii il im ip lr it ls ix lt jb li lj lk ll bi translated">样本预测:从测试数据中随机抽取10个样本，并显示相应的预测。这里不需要数据上传。</li><li id="4b4e" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">对上传数据的预测:您可以自己上传单个/多个数据点并获得预测。</li></ol><p id="1b3f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里有一个视频展示了部署的模型是如何进行预测的。</p><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="nj mi l"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">部署ML模型进行预测。</figcaption></figure><h1 id="e1b1" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">结论</h1><p id="5649" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">解决这个问题的主要挑战是数据集中的极度不平衡和丢失值的巨大百分比。此外，由于专有原因，数据集中的要素被匿名化。即每个特征的定义是未知的。</p><p id="d9f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用<strong class="ig hi">中值和MICE插补</strong>处理缺失数据问题。使用<strong class="ig hi"> SMOTE </strong>解决数据不平衡问题。这两项技术，以及使用自动编码器的<strong class="ig hi">特征工程</strong>和使用精确召回曲线找到正确的<strong class="ig hi">概率阈值，被证明是解决这个问题的最重要的步骤。即使特征的定义未知，也可以进行故障预测。</strong></p><p id="0627" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从获得的结果来看，我们看到随机森林模型表现最好，给出的测试成本为8，820美元，概率阈值为0.00555。这一分数甚至优于本次挑战的获胜者解决方案，后者的<a class="ae jz" href="https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks" rel="noopener ugc nofollow" target="_blank">测试分数为9，920美元[6]。</a></p><h1 id="f23d" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">潜在的改进</strong></h1><ol class=""><li id="8679" class="ld le hh ig b ih ky il kz ip lf it lg ix lh jb li lj lk ll bi translated">人们可以尝试各种其他基于模型的插补技术，如软插补、数据加权插补等。</li><li id="3e6d" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">此外，也可以使用深度学习方法。一个深层次的MLP关系网应该运作得很好。</li><li id="d48f" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">当执行超参数调谐时，还可以包括更多的参数。</li></ol><h1 id="f439" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">链接到GitHub和LinkedIn</h1><p id="4001" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">您可以在我的<a class="ae jz" href="https://github.com/sharathsolomon/scaniaAPSstudy" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> Github资源库</strong> </a> <strong class="ig hi">中找到这个案例研究的完整代码。</strong>请随时通过<a class="ae jz" href="https://www.linkedin.com/in/sharath-solomon-0a5436112/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> LinkedIn </strong> </a>与我联系，或者通过电子邮件<strong class="ig hi">sharath.solomon@outlook.com</strong>与我联系</p><h1 id="f77a" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">参考</h1><ol class=""><li id="3c99" class="ld le hh ig b ih ky il kz ip lf it lg ix lh jb li lj lk ll bi translated"><strong class="ig hi"> IDA 2016工业挑战赛:</strong><a class="ae jz" href="https://ida2016.blogs.dsv.su.se/?page_id=1387" rel="noopener ugc nofollow" target="_blank">https://ida2016.blogs.dsv.su.se/?page_id=1387</a></li><li id="82d3" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">ka ggle:</strong><a class="ae jz" href="https://www.kaggle.com/uciml/aps-failure-at-scania-trucks-data-set" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/UC IML/APS-failure-at-Scania-trucks-data-set</a></li><li id="c09a" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">宏F1评分:</strong><a class="ae jz" href="https://peltarion.com/knowledge-center/documentation/evaluation-view/classification-loss-metrics/macro-f1-score" rel="noopener ugc nofollow" target="_blank">https://pelta rion . com/knowledge-center/documentation/evaluation-view/class ification-loss-metrics/Macro-F1-Score</a></li><li id="6238" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">现有方法1:</strong><a class="ae jz" href="https://link.springer.com/chapter/10.1007/978-3-319-46349-0_33" rel="noopener ugc nofollow" target="_blank">https://link . springer . com/chapter/10.1007/978-3-319-46349-0 _ 33</a></li><li id="94b0" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">软估算:</strong><a class="ae jz" href="https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf</a></li><li id="6cde" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi"> UCI机器学习资源库:</strong><a class="ae jz" href="https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/datasets/APS+Failure+at+Scania+Trucks</a></li><li id="783f" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">现有方法二:</strong><a class="ae jz" href="https://link.springer.com/chapter/10.1007/978-3-319-46349-0_36" rel="noopener ugc nofollow" target="_blank">https://link . springer . com/chapter/10.1007/978-3-319-46349-0 _ 36</a></li><li id="b424" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">递归特征消除:</strong><a class="ae jz" href="https://machinelearningmastery.com/rfe-feature-selection-in-python/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/rfe-Feature-selection-in-python/</a></li><li id="6b69" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi"> MICE插补</strong>:<a class="ae jz" href="https://stats.stackexchange.com/questions/421545/multiple-imputation-by-chained-equations-mice-explained" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/421545/multiple-attachment-by-chained-equations-MICE-explained</a></li><li id="b669" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">SMOTE:</strong><a class="ae jz" href="https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/" rel="noopener ugc nofollow" target="_blank">https://machinelingmastery . com/SMOTE-over sampling-for-unbalanced-class ification/</a></li><li id="37bd" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">自动编码器:</strong><a class="ae jz" href="https://machinelearningmastery.com/autoencoder-for-classification/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/auto encoder-for-class ification/</a></li><li id="9bd1" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">击杀原文:【https://arxiv.org/pdf/1106.1813.pdf】T22</strong></li><li id="877e" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">Streamlit:</strong><a class="ae jz" href="https://streamlit.io" rel="noopener ugc nofollow" target="_blank">https://Streamlit . io</a></li><li id="64c3" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">应用人工智能课程:</strong><a class="ae jz" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></li></ol></div></div>    
</body>
</html>