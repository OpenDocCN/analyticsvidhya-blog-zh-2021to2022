<html>
<head>
<title>Glass Classification using Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于神经网络的玻璃分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/glass-classification-using-neural-networks-165a97242de6?source=collection_archive---------7-----------------------#2021-06-14">https://medium.com/analytics-vidhya/glass-classification-using-neural-networks-165a97242de6?source=collection_archive---------7-----------------------#2021-06-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="9ef2" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">将不同类型的玻璃分类</p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/2c9e0b53add7ee942e0238d054d63500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*ehxtQv2M5nzGFzQmyyzsqA.jpeg"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">来源:<a class="ae js" href="https://images.unsplash.com/photo-1481026469463-66327c86e544?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=524&amp;q=80" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><p id="393c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">问题陈述<br/> </strong>这个问题与根据某些特征对玻璃进行分类有关。这个特殊的问题可以通过使用机器学习算法来解决，如SVC、随机森林或任何其他分类算法，但我使用的是神经网络。然而，选择简单的算法会更好，因为我们没有太多的数据。</p><p id="b385" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">数据来源<br/> </strong>数据可以通过<strong class="ik hi"> Kaggle </strong>从<a class="ae js" href="https://www.kaggle.com/uciml/glass" rel="noopener ugc nofollow" target="_blank">这里</a>你可以很容易地下载。</p><p id="2443" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">关于数据<br/> Ba)中，除了Id列之外的所有列在确定玻璃的类型中起着重要作用，这也是我们的目标变量。在提供的关于数据集的描述中有7种类型的玻璃，但是在玻璃的数据集中，我们没有关于类型4玻璃的数据。每种类型的玻璃都有其自己的名称，但是在数据中，目标变量从1到7编号。 因此，基于可用的特征，我们必须预测目标变量(玻璃的类型)。</strong></p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="270d" class="kb kc hh jx b fi kd ke l kf kg"><strong class="jx hi">import</strong> <strong class="jx hi">pandas</strong> <strong class="jx hi">as</strong> <strong class="jx hi">pd</strong><br/><strong class="jx hi">import</strong> <strong class="jx hi">seaborn</strong> <strong class="jx hi">as</strong> <strong class="jx hi">sns</strong><br/><strong class="jx hi">import</strong> <strong class="jx hi">numpy</strong> <strong class="jx hi">as</strong> <strong class="jx hi">np</strong><br/><strong class="jx hi">import</strong> <strong class="jx hi">matplotlib.pyplot</strong> <strong class="jx hi">as</strong> <strong class="jx hi">plt</strong><br/><strong class="jx hi">from</strong> <strong class="jx hi">imblearn.over_sampling</strong> <strong class="jx hi">import</strong> SMOTE<br/><strong class="jx hi">from</strong> <strong class="jx hi">sklearn.model_selection</strong> <strong class="jx hi">import</strong> train_test_split<br/><strong class="jx hi">from</strong> <strong class="jx hi">sklearn.preprocessing</strong> <strong class="jx hi">import</strong> MinMaxScaler, StandardScaler<br/><strong class="jx hi">from</strong> <strong class="jx hi">sklearn.metrics</strong> <strong class="jx hi">import</strong> classification_report<br/><strong class="jx hi">from</strong> <strong class="jx hi">sklearn.model_selection</strong> <strong class="jx hi">import</strong> train_test_split<br/><strong class="jx hi">from</strong> <strong class="jx hi">sklearn.metrics</strong> <strong class="jx hi">import</strong> confusion_matrix<br/><strong class="jx hi">import</strong> <strong class="jx hi">tensorflow</strong> <strong class="jx hi">as</strong> <strong class="jx hi">tf<br/></strong>from <strong class="jx hi">tensorflow</strong> <strong class="jx hi">as</strong> keras<strong class="jx hi"><br/>from</strong> <strong class="jx hi">tensorflow.keras.optimizers</strong> <strong class="jx hi">import</strong> Adam<br/><strong class="jx hi">from</strong> <strong class="jx hi">tensorflow.keras.utils</strong> <strong class="jx hi">import</strong> to_categorical, normalize<br/><strong class="jx hi">from</strong> <strong class="jx hi">keras.callbacks</strong> <strong class="jx hi">import</strong> Callback, EarlyStopping<br/><strong class="jx hi">import</strong> <strong class="jx hi">warnings</strong><br/>warnings.filterwarnings('ignore')</span></pre><p id="bb24" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">我可用的数据集不包含任何标题。如果您下载的数据有标题，则忽略"<em class="ij"> header=None" </em></p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="9412" class="kb kc hh jx b fi kd ke l kf kg">df=pd.read_csv('/content/glass_data.csv',header=<strong class="jx hi">None</strong>)<br/>df.columns=['Id','refractive_index','Sodium','Magnesium',\<br/>'Aluminium','Silicon','Potassium','Calcium','Barium','Iron','Glass_type']<br/>df.head()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kh"><img src="../Images/6349dc730645549e8ae684cbe0892389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BGqDnGZvO7hOngvyIBrVjA.png"/></div></div></figure><p id="07a7" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">Id列没有用，所以删除它是一个好的选择。这里需要注意的一件事是，当我们删除Id列时，我们发现该数据也包含一些重复值。</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="a09e" class="kb kc hh jx b fi kd ke l kf kg">df.drop('Id',axis=1,inplace=<strong class="jx hi">True</strong>)</span></pre><p id="33d5" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">数据汇总</strong></p><p id="3b10" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">数据汇总是对数据帧的有用操作之一，它为我们提供了<em class="ij">计数、平均值、标准偏差</em>以及关于数据特征的5个数字汇总。<br/>这5个数摘要包含:</p><ol class=""><li id="0b3d" class="km kn hh ik b il im ip iq jt ko ju kp jv kq jf kr ks kt ku bi translated">福建话</li><li id="1ed9" class="km kn hh ik b il kv ip kw jt kx ju ky jv kz jf kr ks kt ku bi translated">雌三醇环戊醚</li><li id="3256" class="km kn hh ik b il kv ip kw jt kx ju ky jv kz jf kr ks kt ku bi translated">中位数</li><li id="0cf3" class="km kn hh ik b il kv ip kw jt kx ju ky jv kz jf kr ks kt ku bi translated">Q3</li><li id="5ffa" class="km kn hh ik b il kv ip kw jt kx ju ky jv kz jf kr ks kt ku bi translated">最大</li></ol><p id="3ebf" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">因此<em class="ij">描述</em>函数返回5个数字摘要以及其他统计方法，如标准差、计数和平均值</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="6e74" class="kb kc hh jx b fi kd ke l kf kg">df.describe()</span></pre><p id="27d5" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">空检查</strong></p><p id="3205" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">数据集中没有空值。</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="96f0" class="kb kc hh jx b fi kd ke l kf kg">df.isna().sum()</span></pre><p id="f223" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">重复检查</strong></p><p id="e0b8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">从数据帧中删除Id为的列，我们留下了一个重复的值，因此删除该列是避免数据冗余的更好选择。</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="5dd5" class="kb kc hh jx b fi kd ke l kf kg">df[df.duplicated()]</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es la"><img src="../Images/565b3887e9612b7bc7f9878724ed685b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U4BlhEjVfXfgcSOHu2VyLQ.png"/></div></div></figure><p id="4520" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">删除重复的</strong></p><p id="14ff" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">如何处理重复记录</strong> <br/>有多种方法可以处理重复记录，但我们采用了保留数据集中最后一行并删除最先出现的行的方法。</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="182e" class="kb kc hh jx b fi kd ke l kf kg">df.drop_duplicates(keep='last',inplace=<strong class="jx hi">True</strong></span></pre><p id="d175" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">简明摘要</strong></p><p id="4a54" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">info</strong>Data frame的函数提供了特性的简明摘要，包括有多少个非空值，每个特性的数据类型和内存使用情况。</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="04b6" class="kb kc hh jx b fi kd ke l kf kg">df.info()</span></pre><p id="4b50" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">配对图</strong></p><p id="e407" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi"> Pairplot </strong>显示特征之间的成对关系。每个特征都是沿着轴的网格绘制的，所以每个特征都是沿着行和列绘制的</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="43cc" class="kb kc hh jx b fi kd ke l kf kg">corr_mat=df.corr()<br/>plt.figure(figsize=(16,10))<br/>sns.heatmap(corr_mat,annot=<strong class="jx hi">True</strong>,fmt='.2f',alpha = 0.7,   cmap= 'coolwarm')<br/>plt.show())</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lb"><img src="../Images/8bfe9c5813b525eaffd2d7c0242614d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CpVT_sLsz8v8O_cLZbO2PQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">相互关系</figcaption></figure><p id="d912" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">班级分布</strong></p><p id="3493" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">玻璃类型数据集的分布，显示每种玻璃在数据集中的分布，即特定玻璃在数据集中出现的次数。这种分布告诉我们数据是不平衡的</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="6eed" class="kb kc hh jx b fi kd ke l kf kg">plt.figure(figsize=(10,10))<br/>sns.countplot(x='Glass_type', data=df, order=df['Glass_type'].value_counts().index);</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lc"><img src="../Images/206e77f025a96294b33bece3a8674d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*5CsGkcPTg28h7qeLnnWlrQ.png"/></div></figure><p id="ea93" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">分离特征和目标变量</strong></p><p id="d55e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">我们已经分离了特征和目标变量，所有自变量都存储在X变量中，而因变量存储在y变量中。<br/>通过使用Keras中的归一化函数对独立变量进行归一化。Keras的Util API。也可以通过使用标准缩放器、最小最大缩放器或健壮缩放器的Scikit-Learn API来执行标准化。有许多方法可以处理这种情况。</p><p id="e444" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">为什么要正常化？</strong> <br/>通常，执行标准化是为了在相同的尺度上降低所有的特征。<br/>通过将所有功能降低到相同的规模，好处是模型将每个功能视为相同。</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="53da" class="kb kc hh jx b fi kd ke l kf kg">X=df.drop('Glass_type',axis=1)<br/>X=normalize(X)<br/>y=df['Glass_type']</span></pre><p id="6488" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">阶级平衡</strong></p><p id="750e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">如上所述，从类的分布可以看出，类是不平衡的，因此如果我们开发不平衡数据集的模型，模型将偏向包含大多数样本的类，因此处理不平衡类将有助于开发公平模型</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="fd2b" class="kb kc hh jx b fi kd ke l kf kg"><strong class="jx hi">from</strong> <strong class="jx hi">imblearn.over_sampling</strong> <strong class="jx hi">import</strong> RandomOverSampler<br/><br/>ros = RandomOverSampler(random_state=42)<br/>x_ros, y_ros = ros.fit_resample(X, y)</span></pre><p id="1369" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">分割数据</strong> <br/>现在数据是平衡的，所以我们将平衡的数据集分割成训练测试和验证数据。通过使用Scikit-Learn API训练测试拆分两次，我们拆分了数据，75%的数据作为训练数据，并且我们进一步将25%的测试数据拆分为测试和验证数据。<br/>通过打印数据的形状，我们可以看到75%的数据位于训练集中，剩余的25%的数据被进一步分成测试和验证数据</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="d2bf" class="kb kc hh jx b fi kd ke l kf kg">X_train, X_test, y_train, y_test = train_test_split(x_ros,y_ros,test_size=0.25,random_state=42)<br/>X_test, X_val, y_test, y_val = train_test_split(X_test,y_test,stratify=y_test,test_size = 0.5,random_state=42)</span><span id="4808" class="kb kc hh jx b fi ld ke l kf kg">y_train=to_categorical(y_train)<br/>y_test=to_categorical(y_test)<br/>y_val=to_categorical(y_val)</span><span id="b8e5" class="kb kc hh jx b fi ld ke l kf kg">print('X_train :',X_train.shape)<br/>print('y_train :',y_train.shape)<br/>print('X_test :',X_test.shape)<br/>print('y_test :',y_test.shape)<br/>print('X_val :',X_val.shape)<br/>print('y_val :',y_val.shape)</span><span id="bd31" class="kb kc hh jx b fi ld ke l kf kg"># Output:<br/># X_train : (342, 9)<br/># y_train : (342, 8)<br/># X_test : (57, 9)<br/># y_test : (57, 8)<br/># X_val : (57, 9)<br/># y_val : (57, 8)</span></pre><p id="4832" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">模型建立</strong></p><p id="9d6c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">选择一个既不过度拟合也不欠拟合的模型就是应用试凑法，选择一个能提供更好结果的模型。</p><p id="28fc" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">人们可以通过使用Keras的库来调整参数并找出最佳层数，从而找到一个好的模型。但是我采用了一种简单方法，因为我们的问题并不复杂。</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="5c42" class="kb kc hh jx b fi kd ke l kf kg">model = tf.keras.models.Sequential([<br/>tf.keras.layers.Dense(40,input_shape=(9,),activation='relu'),<br/><br/>tf.keras.layers.Dense(20,activation='relu'),<br/><br/>tf.keras.layers.Dense(8, activation='softmax')<br/>])<br/><br/>model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])</span><span id="ab4c" class="kb kc hh jx b fi ld ke l kf kg">model.summary()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es le"><img src="../Images/aacd315a60ad0d8164fae44a2f32d97e.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*KbZthWHbngHAl9Ay2nsVJQ.png"/></div></figure><p id="76c1" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">止损</strong></p><p id="20e6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">在根据数据训练模型之前，我们定义了早期停止方法，当历元较大时，反复训练模型可能非常耗时。因此，添加早期停止是测量验证损失的更好选择，只要模型验证损失在使用耐心参数设置的20个时期内停止改善，模型将停止训练。模型在训练数据上拟合，同时在验证集上验证。</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="44de" class="kb kc hh jx b fi kd ke l kf kg">early_stop = EarlyStopping(<br/>                monitor='val_loss', <em class="ij">#Quantity to be monitored.</em><br/>                mode='auto', <em class="ij">#direction is automatically inferred from the name of the monitored quantity</em><br/>                verbose=1, <em class="ij">#verbosity mode.</em><br/>                patience=20 <em class="ij">#Number of epochs with no improvement after which training will be stopped</em><br/>              )</span></pre><p id="a173" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">模特培训</strong></p><p id="1b6b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">现在模型架构已经最终确定，我们也已经设置了早期停止。现在是时候根据训练数据训练模型，并使用验证数据对其进行验证了。如果验证损失持续减少，则模型将训练1000个时期。如果验证损失停止减少，则模型将在进一步检查后停止迭代，直到20个时期</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="6920" class="kb kc hh jx b fi kd ke l kf kg">history = model.fit(X_train, y_train,<br/>                    epochs=1000,<br/>                    validation_data=(X_val, y_val),<br/>                    callbacks=[early_stop],<br/>                    verbose=2,<br/>                   )</span></pre><p id="e382" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">精度图</strong></p><p id="040d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">该模型已成功训练并通过数据验证，现在我们可以绘制模型在训练集上的准确性和验证数据集上的准确性。下图显示了两种精度的直观表示，并在右下角显示了图例</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="3828" class="kb kc hh jx b fi kd ke l kf kg">plt.figure(figsize=(18,8))<br/>plt.plot(history.history['accuracy'])<br/>plt.plot(history.history['val_accuracy'])<br/>plt.title('Model Accuracy')<br/>plt.ylabel('Accuracy')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training-Accuracy', 'validation-Accuracy'], loc='lower right')<br/>plt.show()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lf"><img src="../Images/7f03cd7cf403e402908ee5e2d03c403a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cye92NmgtiAAzr-LSbfRJg.png"/></div></div></figure><p id="411b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">损失剧情</strong></p><p id="1dbb" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">在下图中可以看到模型损耗表示以及图例</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="bcd7" class="kb kc hh jx b fi kd ke l kf kg">plt.figure(figsize=(18,8))<br/>plt.plot(history.history['loss'])<br/>plt.plot(history.history['val_loss'])<br/>plt.title('Model Loss')<br/>plt.ylabel('Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training-Loss', 'validation-Loss'], loc='upper right')<br/>plt.show()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lg"><img src="../Images/1d974aa37b667db9c6252cd986a39996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*liNvBJtLSLfFNNfaRm20rQ.png"/></div></div></figure><p id="9c38" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">模型评估</strong></p><p id="af7e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">现在，我们已经完成了所有与模型训练相关的事情，现在是时候在模型没有看到的测试数据上测试我们的模型，以检查模型如何在看不见的数据集上执行。我们可以看到，该模型也达到了超过测试数据63%的准确率。准确性分数不是衡量模型性能的好选项，该模型是在不平衡数据集上训练的，但是我们已经平衡了数据集，因此我们可以认为这是一个可靠的指标。</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="d436" class="kb kc hh jx b fi kd ke l kf kg">model.evaluate(X_test, y_test)</span></pre><p id="e8f8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">分类报告</strong></p><p id="54da" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi lh translated"><span class="l li lj lk bm ll lm ln lo lp di"> C </span>分类报告给出了模型预测的正确和不正确的类别。对角线中的值代表预测正确的类别，而对角线以外的值则是不正确的预测</p><pre class="jh ji jj jk fd jw jx jy jz aw ka bi"><span id="c2d9" class="kb kc hh jx b fi kd ke l kf kg">y_pred = model.predict(X_test) <em class="ij"># model predicions of test data</em> y_pred_max = np.argmax(y_pred,axis=1) <em class="ij"># choosing the max probability predicted by model</em> y_test_max = np.argmax(y_test,axis=1) <em class="ij"># selecting max from y_test for comparison</em> confusion_matrix(y_test_max, y_pred_max)</span><span id="47d5" class="kb kc hh jx b fi ld ke l kf kg">#Output<br/>array([[1, 2, 8, 0, 0, 0],<br/>      [0, 6, 6, 0, 0, 0],<br/>      [0, 1, 5, 0, 0, 0],<br/>      [0, 0, 0, 7, 0, 0],<br/>      [0, 0, 0, 0, 8, 2], <br/>      [0, 0, 1, 1, 0, 9]])</span></pre><p id="6808" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated"><strong class="ik hi">结论</strong></p><p id="8521" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jt iu iv iw ju iy iz ja jv jc jd je jf ha bi translated">对于神经网络，我们实现了大约65%的准确度，这是通过应用试错法来找到既不过拟合也不欠拟合的神经网络的最佳架构而实现的。由于神经网络被认为最适合大量数据，因此可以通过添加更多数据来提高精确度。对于较小的数据集，传统的机器学习算法效果更好。我们拥有的数据越多，就越有助于模型学习最佳参数。</p></div></div>    
</body>
</html>