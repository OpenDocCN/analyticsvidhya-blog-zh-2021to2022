# 使用 TPU 为预训练 BERT 选择正确的参数

> 原文：<https://medium.com/analytics-vidhya/choosing-the-right-parameters-for-pre-training-bert-using-tpu-4584a598ca50?source=collection_archive---------3----------------------->

![](img/51d772691f194a25e156d5b77364a5ba.png)

来源:[谷歌搜索](https://346034.smushcdn.com/236617/wp-content/uploads/2017/03/Are-You-Giving-Your-Customers-Too-Many-Options-e1490278059717.jpg?lossy=0&strip=1&webp=1)

预先训练一个 BERT 模型并不容易，很多文章都给出了关于 BERT 是什么以及它能做的惊人事情的高层次概述，或者深入讨论了一个非常小的实现细节。这使得像我不久前一样有抱负的数据科学家经常看着笔记本，心想“它看起来很棒，很有效，但为什么作者选择了这个批量大小的数字或这个序列长度，而不是另一个呢？

在本文中，我想给出一些直觉，告诉你如何做出一些决定，找到正确的配置来预训练 BERT 模型。

带有代码和输出的完整文章可以作为笔记本在 [Github](https://github.com/google-research/bert/blob/master/README.md#pre-training-with-bert) 上找到。

让我们开始查看每个输入参数。

## **1。做小写**

您可以通过将 *do_lower_case* 设置为 true 或 false 来预训练“无案例”或“有案例”版本的 BERT 模型。

> 不区分大小写意味着文本在标记化之前已经小写，例如 john smith 变成了 John Smith。无外壳模型还去除了任何重音标记。Cased 表示保留真实的大小写和重音标记。

您可以根据任务类型和输入数据的语言来决定其值。

***任务类型***

对于像这样的任务，

1.  命名实体识别
2.  词性标注
3.  情感检测

病例信息是一个重要的信号。想象一下“我们”这个词。它可以是在句子中代表“我们”或国家“美国”的代词。

对于其他任务，我们应该只使用“无外壳”模型。它没有不必要的重复，例如，dhoni 和 Dhoni 都将出现在“cased”vocab 中，但不在其中，因此，能够用相同大小的 vocab 更好地表示语言。

***输入语言***

从梵文和其他非拉丁文字中去除重音符号会改变单词的意思，从而改变句子的意思。

例如:कुरान (kuran)变成了करान (kran，ु被删除)

因此，建议对非拉丁语言使用“装箱”模型。

## **2。最大序列长度**

*max_seq_length* 指定输入的最大令牌数。输入令牌根据其值被截断或填充。它被设置为 2 的幂，例如 64，128，512。

您可以根据最终任务来决定它的值。你想预测文章、句子或短语吗？

人们应该为文章保留较大的价值，为短语保留较小的价值。

## 3.训练批量

训练批次大小是模型更新前处理的样本数量。优选较大的批量，以获得对整个数据集的梯度的足够稳定的估计。批量大小始终设置为 2 的幂，例如 512、1024、2048。

最终输入形状看起来像(batch_size，max_seq_length，embedding_size)。对于基于 BERT 的语言模型，嵌入大小通常为 768，并且序列长度基于如上所述的最终任务来决定。

我们的目的是充分利用我们的资源。因此，您应该根据可用 ram 将训练批次大小设置为最大值。

## 4.每个序列的最大预测

max_predictions_per_seq 是每个序列的屏蔽 LM 预测的最大数量。这是按如下方式计算的。

```
max_predictions_per_seq= (max_seq_length* masked_lm_prob)For instance
max_seq_length= 512 and max_seq_length = 0.15
max_predictions_per_seq = 77
```

其中 masked_lm_prob 是每个序列中被[MASK]标记替换的单词的百分比。

## **5。训练步骤数**

训练步骤是我们通过该批来训练模型的次数。其计算如下

```
steps = (epoch * examples)/batch size
For instance
epoch = 100, examples = 1000 and batch_size = 1000
steps = 100
```

其中 epoch 是您希望传递完整数据集的次数。您应该保持它的值大于 1。

在现实世界中，我们通常保持一个很高的值，一旦亏损见底就手动止损。

## **6。学习率**

> *学习率，一个决定步长大小的正标量。*

我们不应该使用太大或太小的学习率。当学习率太大时，梯度下降会无意中增加而不是减少训练误差。当学习率太小时，训练不仅较慢，而且可能永久地陷入高训练误差。

## 7.预热步骤的数量

如果您的数据集高度分化，您可能会遭受某种“早期过度拟合”。如果你的混洗数据碰巧包括一组相关的、强特征的观察结果，你的模型的初始训练可能会严重偏向这些特征，或者更糟，偏向与主题完全不相关的附带特征。

热身是减少早期训练例子首因效应的一种方式。如果没有它，你可能需要运行一些额外的时期来获得期望的收敛，因为模型消除了那些早期的迷信。

在预热期间，学习率线性增加。如果目标学习速率为 p，预热周期为 n，那么第一批迭代使用 1*p/n 作为其学习速率；第二个使用 2*p/n，依此类推:迭代 I 使用 i*p/n，直到我们在迭代 n 达到名义速率。

热身时间通常设定为总训练时间的 1%。

## 结论

在本文中，我讨论了用于预训练 BERT 模型的输入参数，并解释了有助于决定其配置的因素。

*干杯！*