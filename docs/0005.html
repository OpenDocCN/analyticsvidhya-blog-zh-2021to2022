<html>
<head>
<title>NLP Transformer Testing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP变压器测试</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nlp-transformer-unit-test-95459fefbea9?source=collection_archive---------4-----------------------#2021-01-01">https://medium.com/analytics-vidhya/nlp-transformer-unit-test-95459fefbea9?source=collection_archive---------4-----------------------#2021-01-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="69c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在机器学习中，很难可视化或测试小东西。当它是自然语言处理时，领域是自然语言，这个任务变得更加困难。<br/>我试图创建一个非常非常小的样本数据集，以最简单的方式测试transformer。我从教程中获取了基础代码，并根据我的调试意图更改了路径和变量名。</p><p id="e79a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我从<a class="ae jc" href="#https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb" rel="noopener ugc nofollow">这里</a>得到基础代码。但是我变化太大了，所以你可能跟不上我的台词。代码在github，但使用nbviewer.com查看颜色。nbviewer中的<br/>代码<a class="ae jc" href="https://nbviewer.jupyter.org/github/mcelikkaya/medium_articles/blob/main/transformer_debugging.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>github中的<br/>代码<a class="ae jc" href="https://github.com/mcelikkaya/medium_articles/blob/main/transformer_debugging.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="4757" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我将重点测试transformer的学习阶段。如果你不知道主题，请阅读关于变压器的好教程。这只是为了更好地理解变压器架构的可视化。在接下来的两篇文章中，你会看到变压器逻辑中的<strong class="ig hi">自我关注</strong>和<strong class="ig hi"> Q，K，V </strong>。</p><p id="b2cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://celikkam.medium.com/how-self-attention-works-in-transformer-6c76a12396b0" rel="noopener">第二部分:自我关注</a> <br/> <a class="ae jc" href="https://celikkam.medium.com/understanding-q-k-v-in-transformer-self-attention-9a5eddaa5960" rel="noopener">第三部分:Q、K、V </a></p><p id="774a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，你必须知道编码器-解码器架构和编码器-解码器与<strong class="ig hi">注意</strong>架构，以了解变压器。我在帖子里一般用向量这个词代替张量。</p><p id="05eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该数据集包含英语到德语的句子。这是一个平衡的集合，每个单词在他的组(主语、宾语、动词)中出现的频率相同。有一种“想”的模式，导致翻译长短不一。此外,“能”和“想要”模式在翻译中导致不同的顺序，因此翻译也必须了解位置。还有简单的句型“我吃苹果”，动词+宾语产生不同的顺序。即使你不知道德语单词是如此简单。你可以查下表，看看哪个单词对应哪个英语单词。此外，句子非常简单，不正确(丢失的文章…)事实上，你可以改变第二栏，输入你的语言并尝试。(还必须将笔记本中的德语单词换成您的语言中的单词。我将为日本人做一个样品)</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="jp jq l"/></div></figure><p id="aee6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">transformer不同于其他网络架构的地方在于<strong class="ig hi">自我关注</strong>。在神经网络中，我们有一个输入域和输出域，我们为映射训练网络。</p><p id="10de" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在NLP任务中，我们有句子作为输入，我们给出这些句子作为输入，我们通常有一个嵌入层来从这些句子中创建向量。在pytorch中，torch.nn.Embedding是一个可训练的查找表。可训练意味着它将通过训练更好地模拟单词之间的关系。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es jr"><img src="../Images/78df9f5d4a42cf3f195322b5b10cd2f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*e7pPdXq9dFKijPfDwV5M1w.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">建模单词</figcaption></figure><p id="edad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们想要捕捉一个单词的每一个方面，我们必须创建一个如上的表，并有数千个列来完美地创建每个单词的向量。在嵌入层中，我们通过训练网络用低维(64，128，512…)实现这一点。这里嵌入层通过检查丢失在每个时期更新它的矢量。</p><p id="fef3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我想我把所有的英文单词都分了列。就拿“金星”这个词来说吧，历史上有没有人“我在金星吃苹果”，正常情况下肯定没有，所以在“金星”一栏下面肯定有0或者某个非常非常低的值来平滑。所以训练一个语言模型实际上是试图理解这些关系，以此作为起点。</p><p id="2ae2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在transformer上，我们仍然有嵌入层，但是我们也使用了另一种方法来创建单词的向量。(Q，K，V我们用嵌入作为开始向量，尝试学习更好的)。如果我们根据上下文生成向量，那么我们就获得了对解空间的类似人类的理解。人类就是这样做的，对吗？当我们在一个句子中看到“苹果”时，我们可以期待那个句子有一个动词<strong class="ig hi">“吃”</strong>。因此，当你用大量的句子训练一个网络时，它会模拟单词之间的关系。</p><p id="b79b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">transformer架构具有自我关注功能，可以从句子本身创建向量。所以在自我关注时，输入和输出领域是相同的。(在<strong class="ig hi">的</strong>层，这是完全真实的，<strong class="ig hi">的</strong>层同样真实，但是在<strong class="ig hi">的</strong>层就有点不同了。检查MultiHeadAttentionLayer代码。</p><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="jp jq l"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">代码的相关部分</figcaption></figure><p id="eb1d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在自我关注层，我们尝试学习3个权重(线性层)。(Q，K，V) <br/>我们通过输入数据来训练这些网络，并试图根据每个单词的句子来获得它的良好表示。假设我们有两个句子。<br/><strong class="ig hi">-我们吃苹果<br/>-我们吃面包</strong> <br/>我们会为【我们，吃，苹果，面包】生成向量，为句子生成向量。但是<strong class="ig hi">【吃面包】</strong>一定是和<strong class="ig hi">【吃苹果】</strong>不同的动作。因此，在这些步骤中，网络生成的矢量肯定略有不同。基本上，我们试图根据上下文来学习好的向量表示。</p><p id="2866" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">矢【我们，吃，苹果】</strong> =矢【我们】，矢【吃】，矢【苹果】<br/> <strong class="ig hi">矢【我们，吃，面包】</strong> =矢【我们】，矢【吃】，矢【面包】</p><p id="d027" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">想想<strong class="ig hi"/>的“吃”字，即使是同一个字，在不同的句子里也一定有不同的、可分的向量。</p><p id="b637" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另一种思考方式是，在我的集合中,“can”与许多单词一起出现，但“apple”只与“eat”一起出现，因此“eat”和“apple”必须比“can”更多地添加到彼此的表示中。这实际上是人类的思维方式，我们通过检查句子中的其他单词来捕捉句子中一个单词的意思。如果我要你填下面这句话<strong class="ig hi">“我要___苹果”</strong>，你不会说“我要踢苹果”或者“我要读苹果”，因为你从来没见过这样的样本。这就是我们在这里用向量数学所做的。</p><p id="2ca5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">变形金刚如何学习？</strong></p><p id="3e6f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了展示学习过程，我创建了一个实用程序类来记录每个时期的向量值。</p><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="jp jq l"/></div></figure><p id="c37c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你检查MultiHeadAttentionLayer类，有4个重要的向量。q，K，V，X1。<br/> X1是MultiHeadAttentionLayer的最终向量。(请检查代码，解释一切会很长。)</p><p id="62b3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以我记录了重要向量Q，K，V，X1…训练的所有中间步骤。训练有30个纪元。所以纪元是1… 29，30。下图显示了这些向量随时间的变化。</p><p id="f1ec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些向量实际上是64维的。为了形象化，我把它们缩小到2或3维。下图显示了它们是如何改变位置以获得最佳空间位置的。正如你所看到的，通过训练，网络稍微改变向量以更好地学习。正如我所说，训练是学习重量，并学习如何将输入转换为最佳向量，以减少损失。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jy"><img src="../Images/8978cf0ad63c582d74c103c71a1014fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z5Iclwx4dlKEbwsNha5Jdw.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated"><strong class="bd kd">向量如何随着训练而变化。</strong></figcaption></figure><p id="096e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个句子都有不同的颜色。您将在图表中看到如下项目。</p><p id="914e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">we@X1@30 <br/>的意思是:<br/> <strong class="ig hi">字</strong> : we <br/> <strong class="ig hi">向量</strong> :X1 <br/> <strong class="ig hi">在历元</strong> : 30 <br/>我得到第1、15、30步(只是试多了也看)</p><p id="066f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">检查<strong class="ig hi"> we@X1@1 </strong>、<strong class="ig hi"> we@X1@15 </strong>、<strong class="ig hi"> we@X1@30 </strong>如何改变位置。下图为<strong class="ig hi">“吃”</strong>。</p><p id="3136" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">检查Q、K、V和X1群集。你能看到星团彼此靠近吗？在新标签页打开图片，你需要看到完整的图片。你也可以玩代码，看看更多的步骤或所有步骤。(搜索章节<strong class="ig hi">#随时间变化的矢量</strong>)</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es ke"><img src="../Images/52b9b856471ad640ec814f6cb4dc9e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*--uufcl0GMkT8_1cNLX0gA.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated"><strong class="bd kd">训练过程如何利用q、k、v生成不同的x1 </strong></figcaption></figure><p id="444f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上面可以看出，网络对Q，K，V做了很小的改变，使得X1对于不同的上下文是不同的。虽然第一历元值是随机的，但是对于Q、K、V，最终结果不会改变太多，因为我们用学习率进行小的更新，但是X1可能会有大的变化，因为它是用这些值的组合计算的。(Softmax( (K*V)/Scale) ) * V)</p><p id="16d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我用一张图展示给你看。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es kf"><img src="../Images/cdee0e10a365934dca45d9c5241f848e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*ftJ88sd0jU5zgi0Cpj2hiQ.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated"><strong class="bd kd">网络如何遍历每个单词的不同位置(向量)。</strong></figcaption></figure><p id="2ebc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们来玩一个不同的向量。<strong class="ig hi"> src_final </strong>。在编码器层<strong class="ig hi">中，src_final </strong>是最后一个矢量。(检查我的代码命名)你可以认为它是层的最后输出。所以它意味着你如何总结你的输入句子。<strong class="ig hi">“我能吃苹果”</strong>是给人类的。计算机用向量来表达这一点。所以向量的每一个维度都代表了一些特征。</p><pre class="jk jl jm jn fd kg kh ki kj aw kk bi"><span id="a9c5" class="kl km hh kh b fi kn ko l kp kq">test_sentences = [<br/>"i can eat apple",<br/>"i can eat bread",<br/>"i can <strong class="kh hi">eat book</strong>",<br/>"i can <strong class="kh hi">eat newspaper</strong>",<br/>"i can <strong class="kh hi">eat apple book</strong>"]</span></pre><p id="121d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我创造了5个测试句子。如你所见，第一句和第二句是有效句。第三句<strong class="ig hi">“吃书”</strong>和第四句<strong class="ig hi">“吃报纸”</strong>是无意义的句子。在第五句话中,<strong class="ig hi">“苹果”</strong>和<strong class="ig hi">“书”</strong>都存在。以上5句翻译如下:(3，4，5翻译不正确)</p><pre class="jk jl jm jn fd kg kh ki kj aw kk bi"><span id="5a80" class="kl km hh kh b fi kn ko l kp kq"><strong class="kh hi">translation </strong>['ich', 'konnen', 'apfel', 'essen', '&lt;eos&gt;']<br/><strong class="kh hi">translation </strong>['ich', 'konnen', 'brot', 'essen', '&lt;eos&gt;']<br/><strong class="kh hi">translation </strong>['ich', 'konnen', 'apfel', 'essen', '&lt;eos&gt;']<br/><strong class="kh hi">translation </strong>['ich', 'konnen', 'brot', 'essen', '&lt;eos&gt;']<br/><strong class="kh hi">translation </strong>['ich', 'konnen', 'apfel', 'essen', '&lt;eos&gt;']</span></pre><p id="ba9c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">他们给出了以上翻译。如你所见,“吃”在坏句子中占主导地位，他们用“吃”造出句子。(网络也能产生阅读书籍，但动词更占优势。)</p><p id="baf6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们看看网络是如何为这些生成矢量的。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es kr"><img src="../Images/72e55b685e4d3348cafa37ce120d9cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*XvkrR5yg9ljUA9Fepok4vA.png"/></div></figure><p id="036f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如你所看到的，有正确单词(1和2)和5(包括相同的单词)的句子有相似的向量。观察到<strong class="ig hi">“can”</strong>向量位于相似的位置。因此，即使我们为同一个单词<strong class="ig hi">“吃”</strong>生成向量，我们也位于空间的不同部分。“can”向量也可以形成一个更紧凑的簇。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="a138" class="ks km hh bd kd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">网络学习与训练</h1><p id="6a7d" class="pw-post-body-paragraph ie if hh ig b ih lo ij ik il lp in io ip lq ir is it lr iv iw ix ls iz ja jb ha bi translated">现在让我们试着将注意力的训练过程形象化。我会通过步骤转储训练步骤注意力分数。在训练步骤1中，你会发现注意力是随机的。随着时间的推移，它变得更好，最后一步是把注意力放在适当的地方。在下面的代码中，你可以看到我是如何收集各个时期(0，10，20，30)的注意力向量的，如果你想在不同的时期看到更多或不同的注意力向量，可以看看第5行。</p><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="jp jq l"/></div></figure><p id="ca21" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下图中，你可以看到翻译步骤中注意力向量的变化。<br/> <strong class="ig hi">栏目</strong>【SOS，we，can，eat，apple，eos，pad】为源词。<br/> <strong class="ig hi">索引</strong>【1】SOS-wir，2)wir- &gt; konnen，3)konnen- &gt; apfel，4)apfel- &gt; essen，5)essen- &gt; eos】显示每一步的翻译单词。<br/>你一定认为它是最后一个生成的字——&gt;在这一步生成的字。</p><p id="5372" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> "1)sos- &gt; wir" </strong>:表示翻译步骤1，从“sos”到“wir”，意味着翻译中的第一个单词预测，在初始步骤中，我们只有“sos”，我们预测“wir”。</p><p id="2ca4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在纪元1，我们可以看到注意力集中在<strong class="ig hi">【SOS】</strong>。如果你检查纪元30(最后一个纪元)，注意力在<strong class="ig hi">“我们”</strong>。检查其他纪元。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es lt"><img src="../Images/002182d3d12ba0d7abcde5001ba57382.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*Uzy-1_4_R_-HQAr6y-05AQ.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated"><strong class="bd kd">各时期注意力的变化</strong></figcaption></figure><p id="1dbc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在<strong class="ig hi">【埃森】</strong><strong class="ig hi">【SOS】</strong>之后的最后一个纪元似乎有了更多的关注。但是不要忘记，注意力并不是我们用来产生最终结果的唯一媒介。所以让我把培训后的翻译过程甩了。让我们打印翻译信息。</p><p id="4e24" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我有一个方法叫做translate_info，它展示了翻译的逻辑和注意事项。<strong class="ig hi"> Logits </strong>是Softmax之前分类模型的原始(非归一化)分数。然后softmax生成这些分数的概率总和为1。</p><p id="dc8f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">翻译是从<strong class="ig hi"> </strong>输入<strong class="ig hi">【SOS】</strong>生成下一个单词的过程，直到生成输出<strong class="ig hi">【EOS】</strong>(或者到我们尝试的最大长度)。所以当你看到下图时，它显示了每个翻译步骤的逻辑和注意事项。所以如果输入句子是<strong class="ig hi">“我能吃苹果”</strong>，翻译过程有5个步骤，<strong class="ig hi">“ich konnen apfel essen&lt;EOS&gt;</strong>。</p><p id="9687" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如下图所示，在最后一步，虽然注意力更多地集中在<strong class="ig hi">【SOS】</strong>上，但Logit显示了最高点的<strong class="ig hi">【EOS】</strong>。表格显示了每个步骤的逻辑，热图显示了(翻译流程的)每个步骤的注意力。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lu"><img src="../Images/bbd8dcd9bf7866d0c5c1bc2e8416e2cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G34ntbH0D0_5GSO5joCJUA.png"/></div></div></figure></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="9611" class="ks km hh bd kd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">自我关注的重要性</h1><p id="f649" class="pw-post-body-paragraph ie if hh ig b ih lo ij ik il lp in io ip lq ir is it lr iv iw ix ls iz ja jb ha bi translated">如前所述，transformer是对编码器-解码器架构的改进。Transformer的要点是在每个状态中以最佳方式使用编码器状态。正如你在上面的热图中看到的，它改变了翻译过程中每一步的注意力。现在我将试着想象一下注意力机制的重要性。我将使用默认的注意机制(第25行下面的代码)以正常的方式翻译所有的句子。<br/>然后，我将对所有的编码器状态(第27行以下的代码)进行同等关注的翻译(我们不在乎我们通过训练学到的向量，认为所有的单词在源句子中具有同等的重要性)(你可以在本教程的第二部分看到这个逻辑的细节)</p><p id="aec9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我使用的矢量是DecoderLayer末尾的<strong class="ig hi">“tr G4”</strong>矢量。也认为我得到了翻译过程的所有中间步骤。对于翻译[ich，mochten，apfel，essen，eos]创建5个向量。我还检查生成句子的长度。(第14行)。</p><p id="2e0d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是当我没有以适当的方式运用注意力时，句子的顺序就会改变。如果你检查下面的方法，我发送过滤字。这意味着，在翻译步骤1，我们希望网络生成<strong class="ig hi">【ich】</strong>或<strong class="ig hi">【wir】</strong>。对于无效的注意力应用句子，我只过滤产生<strong class="ig hi">【ich】</strong>或<strong class="ig hi">【wir】</strong>的句子。在这里，我试图说明，即使网络生成同一个单词，它的编码也是不同的。我这样做不是“比较苹果<strong class="ig hi">和橘子</strong>的。我所比较的向量属于同一类型，象征着同一事物。(我只是在比较[ich，wir]有效向量(有自我注意)和[ich，wir]无效向量(同等注意)。</p><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="jp jq l"/></div></figure><p id="7c41" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">同样在所有的句子中，我用粉红色标记无效的句子，无效句子的结尾有如下后缀:</p><p id="93d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">吃</strong>::！！!"<br/> <strong class="ig hi">喝</strong>:"]]"<br/><strong class="ig hi">读作</strong> : " &lt; &lt; &lt;"</p><p id="2605" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第一张图片下面是翻译步骤1。在翻译步骤1中，我们通常会生成什么，要么是“ich”，要么是“wir”。所以你可以看到左边部分是<strong class="ig hi">“ich”(我)</strong>，右边部分是<strong class="ig hi">“wir”(我们)</strong>。网络很好地分离了这些媒介。(我使用源句子作为标签，这给出了更多的想法)</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lv"><img src="../Images/1ef80111449a2e83ebcff418f54e6720.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZNDiDRYxet9IaTILPlo2nw.png"/></div></div></figure><p id="5b46" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下图中，当我们包括残疾人时，你可以看到他们位于中间。不够分离。此外，即使注意力没有适当地应用，网络仍然在第一步生成“ich”或“wir ”,但是正如你所看到的，它们的表示与其他的非常不同。</p><p id="0a86" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面我们来口头陈述一下图片<strong class="ig hi"/>:</p><blockquote class="lw lx ly"><p id="d5e8" class="ie if lz ig b ih ii ij ik il im in io ma iq ir is mb iu iv iw mc iy iz ja jb ha bi translated"><em class="hh">没有</em> <strong class="ig hi">注意</strong> <br/>即使我们创建了<br/>相同的向量[ <strong class="ig hi"> ich，wir </strong> ] <br/>在同一个步骤(翻译的第一步)<br/>那些向量[ <strong class="ig hi"> ich，wir </strong> ]不同于用<strong class="ig hi">注意</strong>创建的相同向量[ <strong class="ig hi"> ich，wir </strong> ] <br/>。</p></blockquote><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es md"><img src="../Images/1da334d4303afa6c28be7599a10419ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NSsyA6mZGquFJIeDDmel1A.png"/></div></div></figure><p id="cd60" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在翻译步骤2中，我们有["konnen，" mochten"]，所以下图显示了网络如何学会分离它们。右边可以看到“能”，左边是“要”。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es me"><img src="../Images/e757326bec274ad977bdb4c9bba4b1c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lrDuqGXIiAW7-wNCNHJgHw.png"/></div></div></figure><p id="b2e7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面我们看到，当我们没有应用自我关注时，只有很少的句子生成[konnen，mochten]，正如你看到的，它们远离正常的集群。</p><blockquote class="lw lx ly"><p id="6e97" class="ie if lz ig b ih ii ij ik il im in io ma iq ir is mb iu iv iw mc iy iz ja jb ha bi translated">没有<strong class="ig hi">关注</strong> <br/>即使我们创建了<br/>相同的向量[ <strong class="ig hi"> konnen，mochten </strong> ] <br/>在同一个步骤(2步翻译)<br/>向量[ <strong class="ig hi"> konnen，mochten </strong> ]不同于用关注创建的相同向量【konnen，mochten】<br/>。</p></blockquote><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mf"><img src="../Images/a492b8c6261df11fe552c17209cccf65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Chowk1fEebvzJOwO_cQn5g.png"/></div></div></figure><p id="5172" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在翻译步骤3中，我们有["buch "，" zeitung "，" apfel "，" brot "，" wasser "，" beer"]，所以下图显示了网络如何将它们分开。你可以看到物体集群。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mg"><img src="../Images/43204f9d4aeb9a512f121b4484a19609.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vwyqXIv5jnYuffO_lVPUVw.png"/></div></div></figure><p id="1bd4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下图中，当添加了残疾人，你可以看到粉红色的句子是如何分散的。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mh"><img src="../Images/71ec9a6cf6cde044c01e1742ffed8208.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hs6jru4Y8NJIW90vkh80GQ.png"/></div></div></figure><p id="8c4c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在4个步骤中，我们生成动词。如下图所示，它们在下图中是分开的。甚至你可以看到动词被动词+宾语分开。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mi"><img src="../Images/d46498a07da3b362aeebf59e32d8987f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BwDo3m8Jw-qCwwAmRIkL3Q.png"/></div></div></figure><p id="a3f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们添加残疾人时，你可以看到粉红色分散在图像上。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mj"><img src="../Images/8a58e30fca12e6125b02da1c0f0d026b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YTstEzsEhbGMqSy104BmAQ.png"/></div></div></figure><p id="4ef4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如你所看到的，当我们不应用transformer <strong class="ig hi">自我关注</strong>(在所有翻译步骤中将注意力平均分配给所有单词)时，表示相同单词的向量被破坏了。因为缺乏良好的关注，他们没有足够的分离。他们没有抓住上下文。</p><p id="8a2c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我试图展示Transformer architecture正在学习什么，以及我们如何更好地了解它。阅读关于<strong class="ig hi">注意力</strong>和<strong class="ig hi"> Q、K、V </strong>的帖子，以获得更多更详细的想法。</p></div></div>    
</body>
</html>