<html>
<head>
<title>Variational Inference in context of Variational Autoencoders(VAEs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变分自动编码器环境中的变分推理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/variational-inference-in-context-of-variational-autoencoders-vaes-e96bfe859980?source=collection_archive---------6-----------------------#2021-06-24">https://medium.com/analytics-vidhya/variational-inference-in-context-of-variational-autoencoders-vaes-e96bfe859980?source=collection_archive---------6-----------------------#2021-06-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/e2713d2830a20081667059c0fe3637b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mcBYRZs3K9b-B_m-nw15dw.jpeg"/></div></div></figure><p id="0394" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这篇博客中，我们将介绍变分推理，之后将介绍变分自动编码器(VAEs)。这个博客包含了VAEs的所有细节，即从基本定义到与自动编码器的比较，VAE使用的损失函数的数学推导，以及使用Pytorch实现VAE。稍后，我们将把VAEs与其他生成模型进行比较，如生成对抗网络(GANs)。</p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><h1 id="3807" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">介绍</h1><p id="6a9d" class="pw-post-body-paragraph ip iq hh ir b is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji kw jk jl jm ha bi translated">在许多有趣的潜变量模型中，计算后验概率是不容易的。在这种情况下，变分推理提供了一种计算近似后验分布的方法。在这篇博客中，我们将在变分自动编码器(VAE)上下文中研究变分推理，这是一种生成性深度学习模型。在我们继续之前，下面是潜在变量的定义。</p><blockquote class="kx ky kz"><p id="6af1" class="ip iq la ir b is it iu iv iw ix iy iz lb jb jc jd lc jf jg jh ld jj jk jl jm ha bi translated"><strong class="ir hi">潜在变量:</strong>潜在变量是不能直接观察到的变量，而是从其他变量中推断出来的，或者我们可以说潜在变量允许我们描述可见变量上非常复杂的分布。<br/>让我们考虑一个粗略的例子:看看下图。让我们假设我把这张图片发给你，把它作为壁纸放在屏幕上。正如我们已经知道的，图像中的像素依赖于邻居(因为纹理颜色和所有)。现在你可能会问为什么天空是蓝色的而不是黑色的？(因为我决定给你看白天而不是晚上的时间)。你也可以问为什么天空不多云？(因为我决定给你看一个晴天)等等。<br/>所以这些决定(白天、晴天等)你并不清楚(它们对你是隐藏的)。因此，我们只观察图像，但我们所观察到的取决于这些潜在的(隐藏的)决定。</p></blockquote><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es le"><img src="../Images/624e668d1d91662d8ec769e48e4754fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*E5h2c5asBkhjIhOFZhKgzQ.jpeg"/></div></figure><h1 id="a184" class="ju jv hh bd jw jx lj jz ka kb lk kd ke kf ll kh ki kj lm kl km kn ln kp kq kr bi translated">变分推理</h1><p id="02a0" class="pw-post-body-paragraph ip iq hh ir b is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji kw jk jl jm ha bi translated">在研究朴素贝叶斯、线性回归等简单模型时，我们假设数据来自基础分布，更可能来自正态分布。此外，在朴素贝叶斯中，假设所有样本都是独立同分布的，但事实并非如此。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/41eaed0f58d31555fa7e18def33964da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*q6_nNBGdLTMTn7a118KzGA.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">获得后验分布的隐变量模型和Bayes法则。</figcaption></figure><p id="dcd2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上面的图像中，我们有一个潜在变量模型和贝叶斯规则来获得<strong class="ir hi">潜在变量z. </strong>的后验分布，但是，很难计算分母，即证据，因为它在高维空间中转化为难以处理的积分。因此，由于难以处理的证据，我们无法计算后验概率。</p><p id="05af" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">变分推断建议我们使用另一个参数为φ的<strong class="ir hi">分布q作为后验概率的近似值。最著名的变分推理算法是<a class="ae lt" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#:~:text=In%20statistics%2C%20an%20expectation%E2%80%93maximization,depends%20on%20unobserved%20latent%20variables." rel="noopener ugc nofollow" target="_blank"><strong class="ir hi"/></a><strong class="ir hi">算法</strong>和<strong class="ir hi">变分自动编码器。</strong>现在，在转向变型自动编码器之前，让我们对自动编码器做一个简单的概述。</strong></p><h2 id="2c6c" class="lu jv hh bd jw lv lw lx ka ly lz ma ke ja mb mc ki je md me km ji mf mg kq mh bi translated">自动编码器</h2><p id="1774" class="pw-post-body-paragraph ip iq hh ir b is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji kw jk jl jm ha bi translated">自动编码器被训练来重建输入数据。自动编码器包含一个编码器，它接受输入<strong class="ir hi"> X </strong>并将其映射到一个隐藏的表示。然后，解码器采用这个隐藏的表示，并尝试将输入重构为x。自动编码器的基本架构如下:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mi"><img src="../Images/0db1ad0518cbb3a5c1510aa547f5798f.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*CrM2T2HQEQQWyJDHVDKSJA.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">自动编码器的体系结构</figcaption></figure><h1 id="180b" class="ju jv hh bd jw jx lj jz ka kb lk kd ke kf ll kh ki kj lm kl km kn ln kp kq kr bi translated">变分自动编码器</h1><p id="0061" class="pw-post-body-paragraph ip iq hh ir b is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji kw jk jl jm ha bi translated">可变自动编码器属于生成模型，它提供了一种从模型分布中进行采样的原则方法。可变自动编码器与自动编码器具有相同的结构。但是在自动编码器中，我们只能重建输入，因为<strong class="ir hi"><em class="la"/></strong>(高维潜在空间)是一个非常高维的向量，并且这个空间中只有少数向量实际上对应于输入的有意义的潜在表示。因此，在VAEs中，我们只输入那些极有可能的值<strong class="ir hi"> <em class="la"> h </em> </strong>。换句话说，我们感兴趣的是从<strong class="ir hi"><em class="la">【P(h | X)</em></strong>中取样，以便我们只挑选那些概率较高的<strong class="ir hi"> <em class="la"> h的</em> </strong>。因此，变分自动编码器可以同时进行重建和生成。</p><ol class=""><li id="d8e5" class="mj mk hh ir b is it iw ix ja ml je mm ji mn jm mo mp mq mr bi translated">在VAEs中，使用神经网络<strong class="ir hi">(编码器)</strong>将训练数据映射到潜在空间。然后将潜在空间后验分布(<strong class="ir hi"> P(z|X)) </strong>和先验分布<strong class="ir hi"> (P(z)) </strong>建模为高斯。网络的输出是两个参数- <strong class="ir hi">均值()</strong>和<strong class="ir hi">方差(σ)</strong>，即后验分布的参数。</li><li id="af45" class="mj mk hh ir b is ms iw mt ja mu je mv ji mw jm mo mp mq mr bi translated">假设来自潜在空间分布的随机样本生成输入数据。</li><li id="8603" class="mj mk hh ir b is ms iw mt ja mu je mv ji mw jm mo mp mq mr bi translated">然后使用另一个神经网络<strong class="ir hi">(解码器)</strong>将潜在空间向量映射到输入数据。</li></ol><p id="0b9b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，变分自动编码器是一种神经网络，它以无监督的方式从输入中学习隐藏模式。它基本上包含<strong class="ir hi">两个部分</strong>:第一个是<strong class="ir hi">编码器</strong>类似于卷积神经网络或任何前馈神经网络。编码器旨在从图像中学习有效的模式，并将它们传递到瓶颈架构中。自动编码器的另一部分是一个<strong class="ir hi">解码器</strong>，它使用瓶颈层中的潜在空间来重新生成类似于输入的数据。</p><h2 id="90a7" class="lu jv hh bd jw lv lw lx ka ly lz ma ke ja mb mc ki je md me km ji mf mg kq mh bi translated">VAEs架构</h2><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mx"><img src="../Images/489781d25add2502e6f62b8ed7828361.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*aPphKB2bZc_Wem38PdkPPA.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">VAE建筑</figcaption></figure><p id="05c9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">编码器:</strong>编码器将训练数据作为输入，并产生潜在表示<strong class="ir hi">z。</strong>潜在表示是随机的，即它们是概率分布(高斯)的参数。</p><p id="6e4d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">采样器:</strong>给定编码器的均值和对数方差，我们使用通过<strong class="ir hi">重新参数化技巧</strong>创建的确定性函数从Qθ(z|X)采样。</p><blockquote class="kx ky kz"><p id="d339" class="ip iq la ir b is it iu iv iw ix iy iz lb jb jc jd lc jf jg jh ld jj jk jl jm ha bi translated"><strong class="ir hi">重新参数化技巧:</strong>我们知道，在神经网络中，我们通过<a class="ae lt" href="https://towardsdatascience.com/how-does-back-propagation-in-artificial-neural-networks-work-c7cad873ea7" rel="noopener" target="_blank">反向传播</a>来学习参数。因此，VAE也使用反向传播算法来学习参数。但是在VAEs中，我们需要的是里面的<em class="hh">采样器</em>节点本质上应该是随机的，并且我们应该能够计算采样节点相对于<code class="du my mz na nb b">mean</code>和<code class="du my mz na nb b">log-variance</code>向量的梯度。因此，采样器<code class="du my mz na nb b">epsilon</code> <em class="hh"> </em>中的小家伙实际上<strong class="ir hi">重新参数化了</strong>我们的价值观。这允许<code class="du my mz na nb b">mean</code>和<code class="du my mz na nb b">log-variance</code>向量仍然作为网络的可学习参数，同时仍然通过<code class="du my mz na nb b">epsilon</code>保持整个系统的随机性。</p></blockquote><p id="2389" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> KL-Divergence: </strong>我们计算变分近似Qθ(z|X)的KL-Divergence和潜变量z的真实分布，这也用在总损失函数中，起正则化作用。</p><blockquote class="kx ky kz"><p id="4f35" class="ip iq la ir b is it iu iv iw ix iy iz lb jb jc jd lc jf jg jh ld jj jk jl jm ha bi translated"><strong class="ir hi"> KL-Divergence: </strong>用于衡量两个分布内包含的信息的差异。关于KL-Divergence的推导和更多的了解，你可以查看youtube上的这个视频。</p></blockquote><p id="d2d6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">解码器:</strong> z从编码器的输出中取样，作为解码器的输入。解码器误差用于总损失函数中，代表重建损失。</p><p id="e81d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">总损失函数:</strong>用于训练自动编码器的总损失函数L(x)是KL散度和解码器误差的和。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nc"><img src="../Images/c39ea6200e5118226c38169ee6f6aaa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*fGS07qhLlNstrURz4G3MRw.png"/></div></div></figure><p id="42a5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">损失由两个表达式组成。损失函数中的第一个表达式捕获了潜在变量的概率分布，并作为正则项。并且，第二个表达式抓住了所有自动编码器中使用的重构误差的概念。</p><p id="1217" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在进入数学领域，这是损失函数的推导:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nd"><img src="../Images/542c36af184302c200ea8e65bb478d1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*93QvovIaSmGf8kZIt7EIKg.jpeg"/></div></div></figure><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ne"><img src="../Images/41919cae79ad5ac829bb164a2ec81c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w2CwSV-cPYz_nD9pO75PIw.jpeg"/></div></div></figure><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nf"><img src="../Images/72f28c27ecf240ab50a7465fe17d2115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ki-w-pzF2o9XMmdihbUyCA.jpeg"/></div></div></figure></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><h1 id="019f" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">履行</h1><p id="92b0" class="pw-post-body-paragraph ip iq hh ir b is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji kw jk jl jm ha bi translated">我在PyTorch中实现了MNIST数据集的变分自动编码器，并从VAE执行了重建和生成。</p><p id="8d03" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是VAE的代码:</p><figure class="lf lg lh li fd ii"><div class="bz dy l di"><div class="ng nh l"/></div></figure><p id="d427" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下载数据集并从数据集中绘制一些样本图像。</p><figure class="lf lg lh li fd ii"><div class="bz dy l di"><div class="ng nh l"/></div></figure><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es ni"><img src="../Images/930b923812554fdd100d7b371ed4c84a.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*rAKLnZ_CThsyiE7ibdHOSA.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">带有原始标签的数据集样本图像</figcaption></figure><p id="3c98" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为编码器和解码器创建类，并将它们合并到一个名为VAE的类中。</p><figure class="lf lg lh li fd ii"><div class="bz dy l di"><div class="ng nh l"/></div></figure><p id="b399" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">训练VAE并绘制损失函数和时期之间的图形。</p><figure class="lf lg lh li fd ii"><div class="bz dy l di"><div class="ng nh l"/></div></figure><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es nj"><img src="../Images/1fcf3321320d85055af84103854a9c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*taiKvT5egYJm9P5anTvz7A.png"/></div></figure><p id="70c9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">绘制了从编码器获得的潜在空间。</p><figure class="lf lg lh li fd ii"><div class="bz dy l di"><div class="ng nh l"/></div></figure><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nk"><img src="../Images/1493bd47330e2a56db9fe7fbb6b446d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*jyqxljREcxjDLTCFyRR2nw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">潜在空间</figcaption></figure><p id="85f5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下面的代码显示了潜在空间中数字之间的插值。这里我在数字7和2之间插值。这可以被认为是VAE的重建。</p><figure class="lf lg lh li fd ii"><div class="bz dy l di"><div class="ng nh l"/></div></figure><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es nl"><img src="../Images/249de1de270ca039dfff1660a6aa9a1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*Eq3Esz09kqPqxuiCLjF6ug.gif"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">来自潜在空间的数字之间的插值。</figcaption></figure><p id="235c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后，从VAE生成图像。</p><figure class="lf lg lh li fd ii"><div class="bz dy l di"><div class="ng nh l"/></div></figure><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es nm"><img src="../Images/ad097c65339fa93def0bde6808491fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*Ldnt4cx3XeGaAdpasRypeg.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">VAE生成的数字</figcaption></figure><p id="25de" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">除此之外，我还为CIFAR10数据集实施了VAE。下面是GitHub回购的链接:<a class="ae lt" href="https://github.com/bamboriyaankit/ML-Course-Project/blob/main/ML_Project_VAE_CIFAR.ipynb" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/bamboriyaankit/ML-Course-Project/blob/main/ML _ Project _ VAE _ cifar . ipynb</a></p><p id="6d14" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上面代码的链接:<a class="ae lt" href="https://github.com/bamboriyaankit/ML-Course-Project/blob/main/ML_Project_VAE_MNIST%20(1).ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/bamboriyaankit/ML-Course-Project/blob/main/ML _ Project _ VAE _ MNIST % 20(1)。ipynb </a></p><h2 id="4236" class="lu jv hh bd jw lv lw lx ka ly lz ma ke ja mb mc ki je md me km ji mf mg kq mh bi translated">VAEs与GANs的简单比较</h2><p id="3613" class="pw-post-body-paragraph ip iq hh ir b is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji kw jk jl jm ha bi translated">vae是概率图形模型，它们的明确目标是潜在建模，而GAN是显式建立的，以优化生成任务。虽然VAE和甘斯都使用无监督学习来学习底层数据分布，但甘斯生成的图像似乎比VAEs更好。</p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="816c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">以下是我用过的一些重要参考资料:</p><ol class=""><li id="4b4e" class="mj mk hh ir b is it iw ix ja ml je mm ji mn jm mo mp mq mr bi translated"><a class="ae lt" href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Variational_Bayesian_methods</a></li><li id="8fa2" class="mj mk hh ir b is ms iw mt ja mu je mv ji mw jm mo mp mq mr bi translated"><a class="ae lt" href="https://ermongroup.github.io/cs228-notes/inference/variational/" rel="noopener ugc nofollow" target="_blank">https://ermongroup . github . io/cs 228-notes/inference/variable/</a></li><li id="0984" class="mj mk hh ir b is ms iw mt ja mu je mv ji mw jm mo mp mq mr bi translated"><a class="ae lt" href="https://www.youtube.com/watch?v=oJu649cDGyo&amp;list=PLEAYkSg4uSQ1r-2XrJ_GBzzS6I-f8yfRU&amp;index=148" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=oJu649cDGyo&amp;list = pleayksg 4 usq1r-2 xrj _ gbzzs 6 I-F8 yfru&amp;index = 148</a></li></ol><p id="2920" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">关于任何解释或更正，请在评论区告诉我。</p></div></div>    
</body>
</html>