<html>
<head>
<title>An Introduction to Selenium and Beautiful Soup</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">硒和美丽的汤介绍</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-introduction-to-selenium-and-beautiful-soup-11e19e7697dc?source=collection_archive---------5-----------------------#2021-03-14">https://medium.com/analytics-vidhya/an-introduction-to-selenium-and-beautiful-soup-11e19e7697dc?source=collection_archive---------5-----------------------#2021-03-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d447754c42ecd57185e475466b1b99b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WB0_1L5ignQKGWEm"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">尼古拉斯·皮卡德在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="d48a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在学习python时，你可能听说过的许多事情之一就是web抓取。当我在做一个个人项目时，我必须从多个国家收集天气数据，并通过网络搜集来收集这些数据。两个最有用的工具，你可以利用当处理网页抓取是硒和美丽的汤。</p><p id="aacc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Beautiful Soup是一个python库，可以让用户轻松地从网页中抓取数据。这些工具使浏览html或XML文件以及通过树状结构搜索信息变得更加容易。您可以搜索特定的标签、属性或id。这些工具还允许用户浏览诸如子女和兄弟姐妹之类的关系。</p><p id="a465" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Selenium是一套允许用户自动化web驱动程序的工具。为了真正理解这一点的重要性，让我们举个例子。在我的项目中，当我需要天气数据时，我需要几个月的数据。然而，这些信息不仅仅出现在一个网页上。我不得不移动到多个页面来收集这些数据。您最不想做的事情就是手动移动到每个站点，获取您正在寻找的特定数据，然后继续移动到下一个站点。selenium的工具允许用户自动访问每个站点并收集必要的信息。这使得收集数据更快、更顺畅。更不用说在收集数据时你不必呆在电脑上。你可以设置一个程序，让它为你收集数据。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><p id="2338" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">美汤</strong></p><p id="789f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当然，使用这个库的第一步是安装包。这个包截至2020年10月3日的当前版本是美人汤4.9.3。一个简单的方法来安装美丽的汤是</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="eab2" class="kj kk hi kf b fi kl km l kn ko">pip install beautifulsoup4</span></pre><p id="5e94" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果您使用的是jupyter notebook，在您设置了一个新的笔记本后，您希望使用以下代码导入该包:</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="d197" class="kj kk hi kf b fi kl km l kn ko">from bs4 import BeautifulSoup</span></pre><p id="f20e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">既然库已经设置好了，我们希望能够访问一个网页。如果你刚刚开始，想尝试抓取一个页面，我建议你使用请求模块。请求模块允许用户收集网页。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="5465" class="kj kk hi kf b fi kl km l kn ko">import requests</span></pre><p id="6a8b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来就是找网站刮了。有一些网站是专门设计来让人们学习如何刮胡子的。一个例子是从书上刮来的。这只是一个演示网站，所以一些信息，如价格和评级被分配了随机值，但你不会有被踢出网站的危险。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="7f94" class="kj kk hi kf b fi kl km l kn ko">html_page = requests.get(‘http://books.toscrape.com/’)</span></pre><p id="e716" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦我们有一个网站要求，我们想通过美丽的汤页面。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="da67" class="kj kk hi kf b fi kl km l kn ko">soup = BeautifulSoup(html_page.content, 'html.parser')</span></pre><p id="6bd7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当通过Beautiful Soup构造函数时，页面被转换成Unicode。Unicode是“使用不同语言和文字的国际编码标准”通过使用下面的代码，我们可以快速浏览信息并了解其结构。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="299d" class="kj kk hi kf b fi kl km l kn ko">soup.prettify</span></pre><p id="7cfc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这将显示页面上的所有信息，这是很多。当抓取时，我们只想要该信息的特定部分。因此，我们可以使用Inspect Element特性，而不是尝试通读整个页面。当你在一个特定的页面上，如果你右击该页面，有一个检查按钮。例如，在Book to Scrape页面上，如果你右击第一本书的价格，你会看到一个Inspect按钮。</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kp"><img src="../Images/3ba2d7ae2ac175d330d0e4240f48b245.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F_W8kptmI4TM75dqdTGUWA.png"/></div></div></figure><p id="de88" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">按下此按钮，将打开一个侧窗口，直接导航到文档上的位置并高亮显示。</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kq"><img src="../Images/042337a2ddc9290d02e81cecc56d6640.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sRH9EsOe9OXoBNudHlzVfg.png"/></div></div></figure><p id="9074" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果突出显示“检查元素”页面的不同部分，它将突出显示页面的相应部分。</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/c5f28d06c0599edab56814bdb6e02f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O836s1UdZR6gzTu2GsXBmA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">一本书</figcaption></figure><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/a2302022cae205ed6bcf2a50172733af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CgQJX6L4hizMHTWYwGfnqw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">书籍的<div/></figcaption></figure><p id="a0c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，如果我们想专门寻找书籍的价格，我们会想要一个变量，我们只通过选择查看。第一步是使用find方法。当我们在网站上时，我们可以使用inspect元素找到包含所有书籍的<div>。在那之上，有一个独特的<div>有一个警戒警告。</div></div></p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="c5cc" class="kj kk hi kf b fi kl km l kn ko">select = soup.find('div', class_="alert alert-warning")</span></pre><p id="af40" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从那里，我们可以向下导航到装有书籍的容器。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="8f45" class="kj kk hi kf b fi kl km l kn ko">books = select.next_sibling.next_sibling</span></pre><p id="273d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">既然我们有了所有的书，我们可以仔细看看价格。如果你仔细观察元素，你会发现价格在一个段落中，这个段落有一个名为<code class="du kt ku kv kf b">price_color</code>的类。有一个findAll方法允许我们在books容器中查找每个案例。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="e7c4" class="kj kk hi kf b fi kl km l kn ko">prices = books.findAll('p', class_='price_color')</span></pre><p id="450e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这给了我们一份价目表。如果我们想在页面上找到更多的信息，比如价格的最小值、最大值或平均值，我们需要首先将信息转换成文本，然后将文本从string转换成float。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="5478" class="kj kk hi kf b fi kl km l kn ko">prices_text = [price.text for price in prices]</span></pre><p id="8348" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们有了一个字符串格式的价格列表，如果去掉英镑符号，我们可以转换成浮点数。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="2cbe" class="kj kk hi kf b fi kl km l kn ko">prices_fl = [float(price[1:]) for price in prices_text]</span></pre><p id="e09a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们在页面上有一个价格列表，我们可以通过找到这样的信息来了解更多，比如列表中哪本书最贵。然而，如上所述，有时您需要来自多个网站的信息，这就是Selenium可以进入的地方。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><p id="90d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">硒</strong></p><p id="49a4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">和以前一样，第一步是安装工具套件。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="de8f" class="kj kk hi kf b fi kl km l kn ko">pip install selenium<br/>pip install webdriver-manager<br/>pip install selectorlib</span></pre><p id="a70d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我们将一些工具导入笔记本。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="1b4b" class="kj kk hi kf b fi kl km l kn ko">from selenium import webdriver<br/>from webdriver_manager.chrome import ChromeDriverManager<br/>from selenium.webdriver.support.ui import WebDriverWait<br/>from selenium.webdriver.support import expected_conditions as EC<br/>from selenium.webdriver.common.by import By<br/>from selenium.common.exceptions import NoSuchElementException<br/>from selenium.common.exceptions import TimeoutExceptionfrom <br/>from selectorlib import Extractor<br/>import time</span></pre><p id="1708" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在这看起来似乎很多，但是如果你正在检查多个网站，并且你的互联网移动缓慢，你不希望因为第一个网站没有加载而跳过一个网站。</p><p id="50cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">回到我收集天气数据的例子，我运行了一个函数，它从多个页面获取一个表格。然而，当我在功能完成后返回时，我发现我的数据集有一半是重复的，因为网站还没有加载。</p><p id="6edf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">关于如何访问多个网站的基本示例，让我们回到Books to Scrape网站。这个网站有多页书要迭代。在这种情况下，第一步是决定要使用哪个浏览器。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="ed44" class="kj kk hi kf b fi kl km l kn ko">driver = webdriver.Chrome(ChromeDriverManager().install())</span></pre><p id="7e9d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这将在电脑控制的Chrome浏览器中打开一个新窗口。接下来，我们命令电脑去网站。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="fe75" class="kj kk hi kf b fi kl km l kn ko">driver.get('http://books.toscrape.com/’)</span></pre><p id="3266" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用Selenium，我们可以让计算机点击某些按钮，如果你知道你想点击什么元素的话。在驱动程序中，你可以使用find_element方法，你可以选择你想要的查找元素的方式，然后选择这个元素是什么。在这种情况下，将查找XPath。要查找XPath，可以使用想要单击的按钮上的inspect元素，然后右键单击突出显示的元素，并选择复制XPath。</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/4d57c69da36aa4120de48e2d7e12fc23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WsAxIFglMUE4luTLrYOYeQ.png"/></div></div></figure><p id="5872" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，下面的代码将单击按钮，将浏览器带到下一页。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="6525" class="kj kk hi kf b fi kl km l kn ko">click_next = driver.find_element(By.XPATH, /html/body/div/div/div/div/section/div[2]/div/ul/li[2]/a)</span></pre><p id="8657" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，可以将页面放入漂亮的汤里，并在漂亮的汤里经历与上面看到的相同的过程。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="0ce2" class="kj kk hi kf b fi kl km l kn ko">soup = BeautifulSoup(driver.page_source, 'html.parser')</span></pre><p id="5fe6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">确认你的页面已经加载</strong></p><p id="30c8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如前所述，有时页面在整个程序中不会以相同的速度加载。要确认页面已经加载，一种方法是等待元素显示。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="5daf" class="kj kk hi kf b fi kl km l kn ko">timeout = 5<br/>    try:<br/>        element_present = EC.presence_of_element_located((By.ID, 'main'))<br/>        WebDriverWait(driver, timeout).until(element_present)<br/>    except TimeoutException:<br/>        print("Timed out waiting for page to load")<br/>    finally:<br/>        print("Page loaded")</span></pre><p id="2461" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">看这段代码，<code class="du kt ku kv kf b">presence_of_element_located()</code>方法会告诉我们网站是否已经被加载。<code class="du kt ku kv kf b">WebDriverWait</code>将使用<code class="du kt ku kv kf b">until</code>方法强制程序等待，直到元素被定位，这就是确认页面已经被加载。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><p id="4f1c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当一起使用时，Selenium和Beautiful Soup是强大的工具，允许用户高效快速地从web上收集数据。我希望这篇介绍能帮助你对使用这些伟大的工具进行网络抓取感到更舒服一些。</p><p id="28a5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有关更多信息，您可以查看以下工具的文档:</p><p id="f4ba" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">靓汤:<a class="ae iu" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank">https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a></p><p id="5589" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">硒:<a class="ae iu" href="https://www.selenium.dev/documentation/en/" rel="noopener ugc nofollow" target="_blank">https://www.selenium.dev/documentation/en/</a></p></div></div>    
</body>
</html>