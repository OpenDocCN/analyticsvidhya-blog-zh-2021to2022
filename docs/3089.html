<html>
<head>
<title>Few Shot Learning Using SBERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用SBERT的少量镜头学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/few-shot-learning-using-sbert-95f8b08248bf?source=collection_archive---------1-----------------------#2021-06-06">https://medium.com/analytics-vidhya/few-shot-learning-using-sbert-95f8b08248bf?source=collection_archive---------1-----------------------#2021-06-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1fce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">简介</strong></p><p id="0118" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">“如果数据集真的很小，下面的两种技术(Bert和SBERT)可能不会给出很好的结果，这纯粹取决于文档的上下文。如果文档已经彼此相似，则很难识别它们之间的差异。想象一个任务，我们需要建立一个每类只有一两个样本的分类，每个样本都超级难找。”</em></p><p id="4a7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将借助数量非常少的文档来实现文档分类。</p><p id="1190" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基本上，根据标记数据的可用性，文档分类主要分为3类:</p><p id="7b73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.<strong class="ih hj">简单的分类，</strong>丰富的数据，我们有大量的数据用于训练和测试我们的模型</p><p id="eeac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">少量分类</strong>，每个类别的数据量非常少，例如每个类别10-40个数据点</p><p id="ca2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3 <strong class="ih hj">一次/单次拍摄分类，</strong>当每个类别只有一个数据点时</p><p id="dd70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将讨论第二种情况，在这种情况下，每个类别可用的标记数据点非常少</p><p id="3639" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">少量拍摄学习:</strong></p><p id="2572" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，上一篇关于伯特的文章，我们也将使用句子的嵌入来得到预测。</p><div class="je jf ez fb jg jh"><a href="https://deepaksaini121993.medium.com/bert-embedding-for-classification-7c51aead26d9" rel="noopener follow" target="_blank"><div class="ji ab dw"><div class="jj ab jk cl cj jl"><h2 class="bd hj fi z dy jm ea eb jn ed ef hh bi translated">用于分类的BERT嵌入</h2><div class="jo l"><h3 class="bd b fi z dy jm ea eb jn ed ef dx translated">机器学习的最新进展和越来越多的可用数据已经对……领域产生了巨大的影响</h3></div><div class="jp l"><p class="bd b fp z dy jm ea eb jn ed ef dx translated">deepaksaini121993.medium.com</p></div></div><div class="jq l"><div class="jr l js jt ju jq jv jw jh"/></div></div></a></div><p id="b36e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是在之前的实现和这个实现之间有一个细微的区别，那就是嵌入创建的方式。在Bert中，我们创建了标记嵌入，但是在SBERT中，我们在句子嵌入的帮助下创建了文档嵌入。</p><p id="3dee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> SBERT </strong></p><p id="f951" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Sentence-Transformers是一个Python库，用于最先进的句子、文本和图像嵌入。然后，这些嵌入可以用于分类或聚类。例如，借助于嵌入之间的余弦相似性，我们可以找到具有相似含义的句子。这对于语义文本相似性、语义搜索或释义挖掘非常有用。</p><p id="d386" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jx" href="https://www.sbert.net/" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">https://www.sbert.net/</strong></a></p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="b09a" class="kh ki hi kd b fi kj kk l kl km">pip install -U sentence-transformers</span></pre><p id="678c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">少投学习的实现</strong></p><p id="f892" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在假设我们有3个不同的类A、B和c，每个类只有10个标记数据点。现在，我们需要借助这些最少的数据创建一个健壮的分类器。</p><p id="ae1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将创建已标记数据集的嵌入，并在推理时，使用每个类别的已保存嵌入来测量新文档的距离，用最接近的类别对其进行分类。</p><p id="6897" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在SBERT的帮助下为文档创建嵌入时，我们需要将文档分成多个句子。因为SBERT在句子层面维护上下文，并据此进行比较。</p><p id="9386" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了将文档分成多个句子，我们使用了SynTOK库。这个库使用多个正则表达式和规则将文档分解成一系列句子。</p><figure class="jy jz ka kb fd ko er es paragraph-image"><div class="er es kn"><img src="../Images/61487065435bf19db06947b3b658715c.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*Bi-39aWOdFElYQOI7DSnWg.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">创建文档嵌入的流程</figcaption></figure><div class="je jf ez fb jg jh"><a href="https://github.com/fnl/syntok" rel="noopener  ugc nofollow" target="_blank"><div class="ji ab dw"><div class="jj ab jk cl cj jl"><h2 class="bd hj fi z dy jm ea eb jn ed ef hh bi translated">民族解放力量/syntok</h2><div class="jo l"><h3 class="bd b fi z dy jm ea eb jn ed ef dx translated">syntok包提供了两个模块，syntok.segmenter和syntok.tokenizer</h3></div><div class="jp l"><p class="bd b fp z dy jm ea eb jn ed ef dx translated">github.com</p></div></div><div class="jq l"><div class="ku l js jt ju jq jv jw jh"/></div></div></a></div><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="88ec" class="kh ki hi kd b fi kj kk l kl km">import syntok.segmenter as segmenter<br/><br/>document = open('README.txt').read()<br/><br/># choose the segmentation function you need/prefer<br/><br/>for paragraph in segmenter.process(document):<br/>    for sentence in paragraph:<br/>        for token in sentence:<br/>            # roughly reproduce the input,<br/>            # except for hyphenated word-breaks<br/>            # and replacing "n't" contractions with "not",<br/>            # separating tokens by single spaces<br/>            print(token.value, end=' ')<br/>        print()  # print one sentence per line<br/>    print()  # separate paragraphs with newlines<br/><br/>for paragraph in segmenter.analyze(document):<br/>    for sentence in paragraph:<br/>        for token in sentence:<br/>            # exactly reproduce the input<br/>            # and do not remove "imperfections"<br/>            print(token.spacing, token.value, sep='', end='')<br/>    print("\n")  # reinsert paragraph separators</span><span id="c708" class="kh ki hi kd b fi kv kk l kl km">###### Two diffrent types of segmentator are available, we could use any one of them as per our convenience. #####</span></pre><p id="abf3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了文档的句子列表，我们将在SBERT的帮助下提取嵌入内容。</p><p id="e1c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们使用预训练模型'<strong class="ih hj"> stsb-bert-base </strong>'进行句子相似度任务</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="12ed" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">from</strong> <strong class="kd hj">sentence_transformers</strong> <strong class="kd hj">import</strong> SentenceTransformer ,  util<br/>import torch</span><span id="a9ca" class="kh ki hi kd b fi kv kk l kl km">model = SentenceTransformer('stsb-bert-base')</span><span id="a813" class="kh ki hi kd b fi kv kk l kl km"><em class="jd">#Our sentences we like to encode</em><br/>sentences = ['This framework generates embeddings for each input sentence',<br/>    'Sentences are passed as a list of string.',<br/>    'The quick brown fox jumps over the lazy dog.']<br/><br/><em class="jd">#Sentences are encoded by calling model.encode()</em><br/>embedding_list = model.encode(sentences)</span></pre><p id="cf05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的代码片段中，我们有句子级别的嵌入，因为我们将文档分成了3个不同的句子，所以要获得文档嵌入，我们需要取文档中所有句子嵌入的平均值。</p><p id="0dd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">" Document _ embed = torch . mean(torch . stack(embedding _ list)，dim=0)" </strong></p><p id="d08b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是我们如何计算文档嵌入。</p><p id="e00f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们可以对每个类别的每个文档重复相同的过程。因此，我们为每个类提供了10种不同的文档嵌入。所以为了得到单个的类嵌入，我们取了所有文档嵌入的平均值。</p><p id="b833" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">推理</strong></p><p id="f535" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于新文档的分类，我们可以提取它的嵌入，就像我们提取标记数据一样。</p><p id="059d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，计算每一类嵌入与新文档嵌入的余弦距离。并将其分配给最近的一个。</p><p id="74ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">" distance = util . py torch _ cos _ sim(doc _ 1，doc_2)" </strong></p><p id="5d88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这项技术的帮助下，我们可以生成更多的标签数据，并且具有相当的准确性。</p><p id="c99b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读！！</p><p id="73a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">贡献者:</p><p id="4083" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1 Nikhil(<a class="ae jx" href="https://nkhandelwal204.medium.com/" rel="noopener">https://nkhandelwal204.medium.com/</a>)</p><p id="b009" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2迪帕克·塞尼</p><p id="ff60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献</strong>:</p><div class="je jf ez fb jg jh"><a href="https://www.sbert.net/" rel="noopener  ugc nofollow" target="_blank"><div class="ji ab dw"><div class="jj ab jk cl cj jl"><h2 class="bd hj fi z dy jm ea eb jn ed ef hh bi translated">句子变压器文件-句子-变压器文件</h2><div class="jo l"><h3 class="bd b fi z dy jm ea eb jn ed ef dx translated">你可以使用pip安装它:我们推荐Python 3.6或更高版本，至少PyTorch 1.6.0。请参见安装…</h3></div><div class="jp l"><p class="bd b fp z dy jm ea eb jn ed ef dx translated">www.sbert.net</p></div></div></div></a></div><p id="ef7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jx" href="https://github.com/fnl/segtok" rel="noopener ugc nofollow" target="_blank">https://github.com/fnl/segtok</a></p></div></div>    
</body>
</html>