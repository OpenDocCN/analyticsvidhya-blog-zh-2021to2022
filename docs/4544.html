<html>
<head>
<title>How long dependencies can LSTM &amp; T-CNN really remember?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM &amp; T-CNN真的能记住多久的依赖关系？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-long-dependencies-can-lstm-t-cnn-really-remember-df3720dd4e4a?source=collection_archive---------10-----------------------#2021-11-10">https://medium.com/analytics-vidhya/how-long-dependencies-can-lstm-t-cnn-really-remember-df3720dd4e4a?source=collection_archive---------10-----------------------#2021-11-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="4464" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">免责声明:本文假设读者对LSTM和CNN神经网络的模型直觉和架构有初步的了解。</p><p id="9931" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">LSTMs是广泛用于顺序建模任务的技术，例如语言建模和时间序列预测。这样的任务通常有长期模式和短期模式，因此学习这两种模式对于准确的预测和估计是很重要的。基于变压器的技术正在兴起，这种技术有助于对长期依赖关系进行建模，远远优于LSTMs。然而，由于数据密集型培训要求和部署复杂性，变压器无法用于所有应用。在这篇文章中，我将比较LSTM和T-CNN在长期信息学习方面的差异。</p><p id="a1f3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们开始…</p><h2 id="0ce9" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">技术TLDR</h2><p id="f90c" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated"><strong class="ig hi"> LSTM </strong>是一种长短期记忆神经网络，广泛用于学习序列数据(NLP，时间序列预测等..).由于递归神经网络(<strong class="ig hi"/>)受到消失梯度问题的困扰，阻碍了网络学习长时间尺度依赖性，而LSTM通过引入遗忘门、输入门和输出门来减少这一问题。有了这些门，它就有了代表长期记忆的细胞状态，而隐藏状态则代表短期记忆。遗憾的是，LSTM仍然不是保留长期信息的完美解决方案，因为遗忘之门倾向于从之前的步骤中移除这样的模式(信息衰减)——如果模式对于50步来说不重要，我为什么要保留它？</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/a5032578046ed88e7c286ac0d624af26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T2B-0QNCxxD3bcO25OgGsQ.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">香草LSTM。作者图片</figcaption></figure><p id="5ab3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">幂律遗忘门控LSTM (pLSTM) 是最近由英特尔和约翰·霍普斯金的研究人员开发的。尽管LSTM有所改进，但遗忘机制表现出信息的指数衰减，限制了他们捕捉长时间信息的能力。可以阅读<a class="ae kt" href="https://arxiv.org/pdf/2105.05944.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="jc">论文</em> </strong> </a>进行详细讲解。总之，由于信息衰减遵循LSTM的指数模式，<strong class="ig hi"> pLSTM </strong>向遗忘门添加衰减因子<em class="jc"> p，这允许LSTM控制信息衰减速率，帮助其更好地学习长期依赖性。</em></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ku"><img src="../Images/a0ed15c8018c369d564013f59f8d1e07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*37NEmRML6_pIrOnqA_jQrA.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">pLSTM。图像<a class="ae kt" href="https://arxiv.org/pdf/2105.05944.pdf" rel="noopener ugc nofollow" target="_blank"> src </a></figcaption></figure><p id="a29c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">时间CNN (T-CNN) </strong>是简单的1D卷积网络，可以应用于时间序列数据而不是图像数据。已知这些层具有时间属性，以学习数据中的全局和局部模式。卷积层还有助于改善模型延迟，因为预测可以并行化，不需要按顺序进行。由于CNN可能是因果性的，这意味着每个预测只能依赖于它以前的预测，因此没有来自未来的泄漏。使用深度神经网络和扩张卷积的组合，TCN构成非常长的有效历史大小。T-CNN有几种变体，如基于注意力的CNN，将LSTM与CNN结合起来，融合其他类型的架构，然而，在这篇文章中，我将坚持使用普通的T-CNN，以使它对我的读者简单，TCNN的变体本身可以是一篇单独的博客文章。可以阅读<a class="ae kt" href="https://arxiv.org/pdf/1608.08242.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="jc">论文</em> </strong> </a>进行详细解释。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kv"><img src="../Images/493dbefe4fd6c08613f6e60a69e50ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ntyGJT_emRgBtsBqNv_Hww.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">因果卷积与标准卷积构成了T-CNN的基础。作者图片</figcaption></figure><p id="8b0d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">合成数据</strong></p><p id="8dc7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们从一个简单的加法函数开始:</p><p id="9b1c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y = f(xⁿ)+ f(x)</p><p id="7f0c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">人们可以设计他们自己的功能，但是现在，让我们用这个。我们的假设是，序列越长，LSTM应该越难记住X⁰值，例如，具有2的序列是第1和第2个元素的相加，而100的序列是第1(0)和第100(或-1)个元素的相加。序列越大，LSTM需要携带信息的步骤就越多。</p><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="kw kx l"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">按作者分类的数据生成器</figcaption></figure><p id="ca18" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> LSTM模型建筑</strong></p><p id="2016" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我创建了一个香草LSTM架构，并用超参数和堆叠LSTM层进行实验，以验证我们的假设</p><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="kw kx l"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">香草LSTM建筑和作者培训</figcaption></figure><p id="e9bc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> CNN模型架构</strong></p><p id="1cc3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我创建了一个普通的T-CNN架构，并用超参数如内核大小、过滤器和卷积数来验证我们的假设</p><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="kw kx l"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">作者的香草T-CNN架构和培训</figcaption></figure><p id="232b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">模型性能结果</strong></p><p id="32ff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">MSE是评估非偏斜数据的一个很好的方法，但是不太容易解释(除了数学家可以！).为了简化，我们可以看一下当X中的一个改变时，Y的百分比变化。由于合成函数是相加的，将X中的一个改变N个百分点，应该会有y的N/2个百分点的变化，其次我们可以看看<strong class="ig hi"> <em class="jc">致盲</em> </strong>。当我们将任一个X设为0时，我们可以测量Y是否等于非零X的MSE。理论上，MSE应该是0.0</p><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="kw kx l"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">按作者的度量</figcaption></figure><p id="f026" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">结果和结论</strong></p><p id="c797" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我分别对X[0]和X[-1]应用了50%的变化，以了解Y中的变化，并计算第0个变化列和第-1个变化列。高达68个序列长度LSTM能够记住从时间序列开始的先前信息，但最终在第69个序列长度时忘记，而CNN仍然能够做出准确的预测，这仅仅是因为我增加了内核大小，以便它有机会查看序列的初始和结束值。盲法显示了相同的结果。当序列长度69的第0个值设置为0时，MSE跳到0.251%。总的来说，LSTM似乎对-1的变化比对0的变化更敏感。[ <em class="jc">这些结果会随着不同的数据大小和函数而变化，但信息是明确的“LSTMs并不真的长！”] </em></p><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="kw kx l"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">按作者列出的结果</figcaption></figure><p id="c594" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些结果与pLSTM作者在他们的信息衰减部分描述的路径相同</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ky"><img src="../Images/aaec178176d5f053bbeba4961f6ce6cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*_xLWYkCG6U5wgTNGI7GflA.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">pLSTM的信息衰减<a class="ae kt" href="https://arxiv.org/pdf/2105.05944.pdf" rel="noopener ugc nofollow" target="_blank">作者</a></figcaption></figure><p id="552c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">在本文中，我没有与pLSTM进行比较，因为目前还没有可用的开源实现，不过，我将在下一篇博客中跟进我自己的pLSTM实现。</em></p><p id="145d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">** <a class="ae kt" href="https://github.com/dwipam/medium-3" rel="noopener ugc nofollow" target="_blank"> <em class="jc">在这里</em> </a> <em class="jc">，你可以找到</em><em class="jc">链接</em> <em class="jc">的完整代码。</em></p></div></div>    
</body>
</html>