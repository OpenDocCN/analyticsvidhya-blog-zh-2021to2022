<html>
<head>
<title>How to Parse Parley Data from Parler</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何解析来自Parler的Parley数据</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-parse-parley-data-from-parler-2b6a39cfe86?source=collection_archive---------9-----------------------#2021-03-19">https://medium.com/analytics-vidhya/how-to-parse-parley-data-from-parler-2b6a39cfe86?source=collection_archive---------9-----------------------#2021-03-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="b908" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如何从“Parleys”中解析出信息来分析来自社交媒体网站的数据。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/f51ad6241e3d8cf786e9e8653d361f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P-r9mMWZKdNYFfDkOpTzXA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图片来自https://parler.com</figcaption></figure><p id="4f86" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我将演示如何解析来自社交媒体网站Parler的parleys数据以进行分析。在这个练习中，我将使用Python和1月6日美国国会大厦事件后最近从Parler废弃的数据(可在<a class="ae jc" href="https://ddosecrets.com/wiki/Parler" rel="noopener ugc nofollow" target="_blank">这里</a>获得)。</p><h2 id="d8c3" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">什么是帕勒？</h2><p id="6ccd" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">对于那些不知道的人来说，<a class="ae jc" href="https://company.parler.com/" rel="noopener ugc nofollow" target="_blank"> Parler </a>是一个社交媒体平台，它“建立在尊重隐私和个人数据、言论自由、自由市场以及道德、透明的公司政策的基础之上”众所周知，它也有大量包含极右内容、反犹太主义和阴谋理论(如QAnon)的帖子，因此对于那些有兴趣研究这些现象的人来说，它是一个有价值的研究资料来源</p><h2 id="7a00" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">谈判</h2><p id="8571" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">Parler用户通常通过使用“<a class="ae jc" href="https://pamelahazelton.medium.com/parler-101-how-to-use-the-free-speech-social-media-platform-dd55e88cd064" rel="noopener"> parley </a>”来相互通信这些谈判就像任何其他社交媒体网站上的帖子一样，可以收到向上投票、评论或回应(类似于Twitter上的转发或引用)。parley的内容可以包括许多东西，例如文本、图像、多媒体数据和外部网站的链接。parley的所有这些元素都呈现为一个HTML页面。下图给出了本文所用数据集中的一个谈判的例子</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kt"><img src="../Images/7fa0590c87ff493090c593222c8966fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bu7wzaaEaJSobqD85Oa-Rg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">例子谈判。自从撰写了这篇文章，Parler已经重新上线，所以一些图片和链接被渲染。</figcaption></figure><h2 id="2935" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">从谈判中解析出数据</h2><p id="b142" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">由于parleys是HTML，我们将不得不遍历parleys中的各种标签来提取重要的信息。为了检查HTML，我使用了Chrome的开发工具。我们将从导入一些将在整个解析过程中使用的代码开始。</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="923b" class="jt ju hh kv b fi kz la l lb lc">import re, pandas as pd<br/>from bs4 import BeautifulSoup<br/>from zipfile import ZipFile</span></pre><p id="6ad5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">接下来，我们将通过每一个谈判和工艺代码来获得重要的元素。首先，我们注意到几乎所有的parley内容都包含在类的一个分部中:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="739c" class="jt ju hh kv b fi kz la l lb lc">&lt;div class=‘card card — post-container w — 100’&gt;</span></pre><p id="c6c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该部门之外的所有内容都只是标准的Parler页眉和页脚。为了开始解析所有的parleys，我们首先打开包含parleys的目录(在本例中，是一个压缩文件)，在目录中创建一个文件列表，然后开始将每个文件读入<a class="ae jc" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a>来解析HTML。我们将把parleys中的各种数据元素存储到一个Python dict对象中。</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="7679" class="jt ju hh kv b fi kz la l lb lc">with ZipFile(zipped_files, 'r') as f:<br/>    file_list = f.namelist()<br/>    for i in range(len(file_list)):<br/>        post = BeautifulSoup(f.read(file_list[i]),'html.parser')  <br/>        parsed_post = {}<br/>        parsed_post['id'] = file_list[i]</span></pre><p id="c49b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们将从解析出用户帐户信息开始。对于任何给定的谈判，我们都可以获得关于原始谈判的用户信息和关于回应谈判的用户的信息(如果是回应的话)。作者信息由两个元素组成，一个是“作者姓名”，它是用户可以给自己取的自由文本名称，另一个是“作者用户名”，它是用户的唯一用户名(即他们的“@”名称或句柄)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ld"><img src="../Images/3c14f77b040db95dff1e513d6f2f78fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MbNGCkLr2UzIJz33ZdFeKw.png"/></div></div></figure><p id="9cf2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面的代码块首先检查是否有作者姓名——因为有时您可能会在谈判中遇到麻烦，导致数据丢失——然后解析出该信息。</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="e89f" class="jt ju hh kv b fi kz la l lb lc">try:<br/><br/>    parsed_post['author_user_name'] = post.find_all("span", {'class': "author--username"})[0].string<br/>    parsed_post['author_name'] = post.find_all("span", {'class': "author--name"})[0].string<br/>except:<br/>    continue</span></pre><p id="1ea1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">接下来，我们想要得到所有的回应信息，如果这是一个回应的谈判。特别是，我们希望获得回显作者信息，回显的时间，以及回显作者是否给出了任何附加的回显文本(比如Twitter上的引用)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es le"><img src="../Images/c0858dda619caec3793590cdc1978cc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vTv85ppeflUyOxu1zdpuag.png"/></div></div></figure><p id="85c9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面的代码从Parley中解析出所有的echo信息。您会注意到echo作者的用户名实际上并不在echo块中，但是可以在HTML title部分找到。</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="8080" class="jt ju hh kv b fi kz la l lb lc">echoed = post.find_all("div", {'class': "eb--col eb--statement"})<br/>if len(echoed) &gt;0:<br/>    parsed_post['echoed_by_author'] = list(echoed[0].children)[1].string[10:]<br/>    parsed_post['echoed_by_author_user_name'] = post.title.get_text().split()[0]<br/>    echoed_time = post.find_all("div", {'class': "eb--col eb--timestamp"})<br/>    parsed_post['when_echoed'] = list(echoed_time[0].children)[1].string<br/>    echo_comment = post.find("span", {'class': "reblock post show-under-echo"})<br/>    if echo_comment is not None:<br/>         parsed_post['echo_comment'] = echo_comment.find("div", {"class": 'card--body'}).get_text().strip()<br/>    else:<br/>         parsed_post['echoed_by_author'] =None<br/>         parsed_post['echoed_by_author_user_name'] = None<br/>         parsed_post['when_echoed'] =None</span></pre><p id="af87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">已经从parley获得了用户和可能的社交网络(即，谁回应谁)数据，我们现在转向parley的内容数据。特别是，我们希望解析出任何文本数据、parley的印象计数(parley被放在多少新闻提要中)，以及任何社交媒体工件，如标签或提及。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lf"><img src="../Images/cbe92e7873bb3d8898c525cf89564a09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UiL3BLm877XB_2JrkPWTHw.png"/></div></div></figure><p id="e32d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面的代码解析出内容，还解析出文本中出现的任何提及或标签。</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="9541" class="jt ju hh kv b fi kz la l lb lc">parsed_post['impressions_count'] = int(post.find_all("span", {'class': "impressions--count"})[0].string)<br/>main_body = post.find("div", {'class': "card--body"})<br/>parsed_post['text'] = main_body.p.get_text()<br/>parsed_post['hashtags'] = re.findall("#(\w+)", parsed_post['text'])<br/>parsed_post['mentions'] = re.findall("@([a-zA-Z0-9]{1,15})", parsed_post['text'])</span></pre><p id="297b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，内容的另一个重要部分是媒体内容，如图像或视频，以及在Parley中共享的任何外部网站。每种类型的外部内容都有不同的HTML标签与之相关联。因此，我们将需要检查每一种不同类型的内容标签，并解析出内容(如果存在的话)。在我们的例子中，我们有一个共享的外部网站。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lg"><img src="../Images/64bb8cc4895c03715e73cce37da0bb5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zBTjoOy6sf2L5Noqy9Cejg.png"/></div></div></figure><p id="c745" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面的代码将检查parley中是否有任何共享的图像、媒体或外部网站，然后解析出它们。</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="0980" class="jt ju hh kv b fi kz la l lb lc">external_urls =[]<br/>externals = post.find_all("span", {'class': "mc-article--link"})<br/>externals = externals + post.find_all("span", {'class': "mc-iframe-embed--link"})<br/>externals = externals + post.find_all("span", {'class': "mc-website--link"})<br/>if len(externals) &gt; 0:<br/>   for url in externals:<br/>       external_urls.append(list(url.a.children)[-1].strip())</span><span id="d913" class="jt ju hh kv b fi lh la l lb lc">parsed_post['external_urls'] = external_urls<br/>                <br/>image_urls =[]<br/>images = post.find_all("div", {'class': "mc-image--wrapper"})<br/>if len(images) &gt; 0:<br/>   for image in images:<br/>       image_urls.append(image.img['src'])<br/>       <br/>parsed_post['internal_image_urls'] = image_urls<br/>            <br/>media_urls =[]<br/>video_hashes =[]<br/>medias = post.find_all("span", {'class': "mc-video--link"})<br/>if len(medias) &gt; 0:<br/>   for media in medias:<br/>       media_urls.append(list(media.a.children)[-1].strip())<br/>       video_hashes.append(media_urls[-1].split('/')[-1].replace('.mp4', ''))</span><span id="c199" class="jt ju hh kv b fi lh la l lb lc">parsed_post['internal_media_urls'] = media_urls<br/>parsed_post['internal_video_hash'] = video_hashes</span></pre><p id="683d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，我还解析出了媒体链接的哈希值。这些相同的散列可以与这些parleys附带的媒体数据集一起使用，以便将任何抓取的媒体数据链接回其parleys。</p><p id="bde3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们还想获得关于Parley的元数据，比如它的投票数和评论数。请注意，虽然这个数据集确实有Parley上的评论计数，但它实际上没有评论本身。这只是这个数据集的一个局限，也许是将来要探索的东西。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es li"><img src="../Images/9eb9fe2bfcfd042ff5de17a268665054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HmifHGJwR9-faovwtCbcvw.png"/></div></div></figure><p id="af4f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下代码可用于解析页脚信息，以获得有关parley的辅助信息。请注意，有时parleys会遗漏这个信息，所以我将它包装在一个try-except块中。</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="6892" class="jt ju hh kv b fi kz la l lb lc">footer =post.find("div", {'class': "post--actions--row pa--main-row p--flex pf--ac pf--jsb"})<br/>try:<br/>    footer_counts = footer.find_all("span", {'class': "pa--item--count"})<br/>    parsed_post['comments_count'] = int(footer_counts[0].string)<br/>    parsed_post['echoes_count'] = int(footer_counts[1].string)<br/>    parsed_post['upvotes_count'] = int(footer_counts[2].string)<br/>except:<br/>    parsed_post['comments_count'] = None<br/>    parsed_post['echoes_count'] = None<br/>    parsed_post['upvotes_count'] = None</span></pre><p id="cd9c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在已经建立了解析parley的所有必要成分，我们可以把所有东西放在一起了。具体来说，我选择将所有的HTML解析打包到一个生成器函数中，然后可以将该函数提供给类似Pandas DataFrame的东西，以便获得所有Parler帖子的一个良好的、结构化的、可分析的版本。所以，把所有的东西放在一起，我们有如下:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="a64a" class="jt ju hh kv b fi kz la l lb lc">def parse_parler_archive(zipped_files):<br/>    with ZipFile(zipped_files, 'r') as f:<br/>        file_list = f.namelist()<br/>        for i in range(len(file_list)):<br/>            post = BeautifulSoup(f.read(file_list[i]), 'html.parser')<br/>            <br/>            parsed_post = {}<br/>            parsed_post['id'] = file_list[i]<br/>            try:<br/>                '''<br/>                Check for a bad scrape<br/>                '''<br/>                parsed_post['author_user_name'] = post.find_all("span", {'class': "author--username"})[0].string<br/>                parsed_post['author_name'] = post.find_all("span", {'class': "author--name"})[0].string<br/>            except:<br/>                continue<br/>            <br/>            parsed_post['timestamp'] = post.find_all("span", {'class': "post--timestamp"})[0].string<br/>            <br/>            echoed = post.find_all("div", {'class': "eb--col eb--statement"})<br/>            if len(echoed) &gt;0:<br/>                parsed_post['echoed_by_author'] = list(echoed[0].children)[1].string[10:]<br/>                parsed_post['echoed_by_author_user_name'] = post.title.get_text().split()[0]<br/>                echoed_time = post.find_all("div", {'class': "eb--col eb--timestamp"})<br/>                parsed_post['when_echoed'] = list(echoed_time[0].children)[1].string<br/>                echo_comment = post.find("span", {'class': "reblock post show-under-echo"})<br/>                if echo_comment is not None:<br/>                    parsed_post['echo_comment'] = echo_comment.find("div", {"class": 'card--body'}).get_text().strip()<br/>            else:<br/>                parsed_post['echoed_by_author'] =None<br/>                parsed_post['echoed_by_author_user_name'] = None<br/>                parsed_post['when_echoed'] =None<br/>                <br/>            parsed_post['impressions_count'] = int(post.find_all("span", {'class': "impressions--count"})[0].string)<br/>            main_body = post.find("div", {'class': "card--body"})<br/>            parsed_post['text'] = main_body.p.get_text()<br/>            parsed_post['hashtags'] = re.findall("#(\w+)", parsed_post['text'])<br/>            parsed_post['mentions'] = re.findall("@([a-zA-Z0-9]{1,15})", parsed_post['text'])<br/>            <br/>            external_urls =[]<br/>            externals = post.find_all("span", {'class': "mc-article--link"})<br/>            externals = externals + post.find_all("span", {'class': "mc-iframe-embed--link"})<br/>            externals = externals + post.find_all("span", {'class': "mc-website--link"})<br/>            if len(externals) &gt; 0:<br/>                for url in externals:<br/>                    external_urls.append(list(url.a.children)[-1].strip())<br/>            parsed_post['external_urls'] = external_urls<br/>                <br/>            image_urls =[]<br/>            images = post.find_all("div", {'class': "mc-image--wrapper"})<br/>            if len(images) &gt; 0:<br/>                for image in images:<br/>                    image_urls.append(image.img['src'])<br/>            parsed_post['internal_image_urls'] = image_urls<br/>            <br/>            media_urls =[]<br/>            video_hashes =[]<br/>            medias = post.find_all("span", {'class': "mc-video--link"})<br/>            if len(medias) &gt; 0:<br/>                for media in medias:<br/>                    media_urls.append(list(media.a.children)[-1].strip())<br/>                    video_hashes.append(media_urls[-1].split('/')[-1].replace('.mp4', ''))<br/>            parsed_post['internal_media_urls'] = media_urls<br/>            parsed_post['internal_video_hash'] = video_hashes<br/>                <br/>            footer =post.find("div", {'class': "post--actions--row pa--main-row p--flex pf--ac pf--jsb"})<br/>            try:<br/>                footer_counts = footer.find_all("span", {'class': "pa--item--count"})<br/>                parsed_post['comments_count'] = int(footer_counts[0].string)<br/>                parsed_post['echoes_count'] = int(footer_counts[1].string)<br/>                parsed_post['upvotes_count'] = int(footer_counts[2].string)<br/>            except:<br/>                parsed_post['comments_count'] = None<br/>                parsed_post['echoes_count'] = None<br/>                parsed_post['upvotes_count'] = None<br/>        <br/>            yield parsed_post</span><span id="bdae" class="jt ju hh kv b fi lh la l lb lc">zipped_files = '''path to your data'''<br/>  <br/>parler_data = pd.DataFrame(parse_parler_archive(zipped_files))</span></pre><p id="27f3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这段代码也可以在我的GitHub页面上找到(<a class="ae jc" href="https://github.com/ijcruic/Parse-Parler-Data" rel="noopener ugc nofollow" target="_blank">https://github.com/ijcruic/Parse-Parler-Data</a>)。我希望你喜欢这篇文章，并喜欢在你的研究中使用Parler数据！-骗子</p></div></div>    
</body>
</html>