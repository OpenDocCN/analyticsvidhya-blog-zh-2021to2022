<html>
<head>
<title>A Quick Summary of Linear Regression.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归的快速总结。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-quick-summary-of-linear-regression-42d1dab85e3e?source=collection_archive---------9-----------------------#2021-08-22">https://medium.com/analytics-vidhya/a-quick-summary-of-linear-regression-42d1dab85e3e?source=collection_archive---------9-----------------------#2021-08-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="405c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">监督学习算法综述</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/f616fa5182d77eac3398c9b0f64d6126.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*h3eG2R8JC4jAZLd0fJ44UQ.png"/></div></figure><p id="d88e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">线性回归</strong></p><ol class=""><li id="b17f" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb jp jq jr js bi translated">线性回归:最常用的回归方法之一。</li><li id="3b0b" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">如果X1，X2，X3…Xn是独立变量，Y是目标变量。给定X，我们需要预测Y</li><li id="26b9" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">线性回归方法是一种统计模型，它拟合Y与X的线性关系</li><li id="018b" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">这种关系在X的系数中是线性的。</li><li id="e0e6" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">在二维空间中，方程是一条直线。</li><li id="fe5c" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">在三维空间中，这个方程是一个平面，在高维空间中，它被称为超平面。</li></ol><p id="9547" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果一个预测变量(x) →简单线性回归</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jy"><img src="../Images/e68c3319f029a600fdd652b7dc7abf33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jt-pyQQ7bgL2lyganse0nQ.png"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">线性回归</figcaption></figure><p id="9910" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果多个预测变量(x1，x2，x3，…..xn) →多元线性回归</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kh"><img src="../Images/2ac2cded2ec33b73f5be627239b04865.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V7uqWXtRkvnQyJtxoLe42Q.png"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">多元线性回归</figcaption></figure><p id="6cd2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">为什么是线性回归？</strong></p><ul class=""><li id="75c4" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb ki jq jr js bi translated">基于回归的模型可以用作简单的基线模型，可以相对容易地构建。</li><li id="9efe" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb ki jq jr js bi translated">与倾向于黑箱的其他更强大的模型相比，该模型的解释通常非常简单</li></ul><h1 id="17fc" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">最佳拟合线有多好？</h1><p id="9f57" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">可能有许多可能的线(由上面的等式定义)穿过给定的数据，</p><p id="0d2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是最佳拟合线是具有最小误差平方和的线。</p><ul class=""><li id="206c" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb ki jq jr js bi translated">每个数据点都有一个实际的y值，根据这个等式，我们可以预测目标ŷ.</li><li id="7ab7" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb ki jq jr js bi translated">实际值和预测值之间的差异就是误差。</li><li id="0682" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb ki jq jr js bi translated">减少所有数据点的SSE(误差平方和)的线称为最佳拟合线。</li><li id="1d43" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb ki jq jr js bi translated">这个过程称为普通最小二乘法(OLS法)。</li></ul><p id="e07a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">成本函数</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lm"><img src="../Images/1fb9456dd211fb0e69bcd5c3ed258ae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*tGhNGRoAkSEpWgAdNvNmLg.jpeg"/></div></figure><p id="be59" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">拟合优度的基本度量:</strong></p><ul class=""><li id="81c3" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb ki jq jr js bi translated">相关系数(r)</li><li id="7b79" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb ki jq jr js bi translated">决定系数(R或R)</li></ul><h1 id="c123" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">线性回归的假设</h1><p id="2f69" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">线条</p><p id="5828" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">线性度:</strong></p><ul class=""><li id="d71d" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb ki jq jr js bi translated">每个预测值xi的响应均值E(Yi)是xi的线性函数</li></ul><p id="caf4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">误差的独立性:</strong></p><ul class=""><li id="d27a" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb ki jq jr js bi translated">误差εi是独立的</li></ul><p id="3fa1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">误差的正态性:</strong></p><ul class=""><li id="5e3f" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb ki jq jr js bi translated">每个预测值xi的误差εi呈正态分布。</li></ul><p id="36be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">同方差(恒定方差):</strong></p><ul class=""><li id="f51a" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb ki jq jr js bi translated">在预测值xi的每个值处的误差εi具有相等的方差(表示为σ),即误差项的方差对于所有的x值都是恒定的，并且不依赖于xi。</li></ul><p id="c154" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">描述上述假设:</strong></p><p id="8959" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">误差Ei是独立的正态随机变量，均值为零，方差恒定，σ</p></div></div>    
</body>
</html>