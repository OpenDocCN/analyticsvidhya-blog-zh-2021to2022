<html>
<head>
<title>Learning to walk using Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用强化学习学习走路</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/learning-to-walk-using-reinforcement-learning-4e237aaf64a0?source=collection_archive---------0-----------------------#2021-03-22">https://medium.com/analytics-vidhya/learning-to-walk-using-reinforcement-learning-4e237aaf64a0?source=collection_archive---------0-----------------------#2021-03-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8bbb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">教机器人走路</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/ebfca28d82794ceda140dfbe4cc5d901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vMxiDBFKTztHcFQ-.png"/></div></div></figure><p id="e0a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">作者Antonio Lisi <br/>致奶奶，我会想你的</em></p><h1 id="c3ce" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">介绍</h1><p id="0c2b" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">大家好，在这篇文章中，我们将使用一种最新的最先进的算法，称为SAC，来教机器人走路。</p><p id="7021" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像往常一样，我们使用Tensorflow 2从头开始实现一切。我们将使用帖子中关于<a class="ae kt" rel="noopener" href="/analytics-vidhya/reinforcement-learning-in-continuous-action-spaces-ddpg-bbd64aa5434"> DDPG </a>的大量代码。所以如果你没有读过，我建议你先读一读。</p><h1 id="d6c5" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">环境</h1><p id="ba3b" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">在<a class="ae kt" rel="noopener" href="/analytics-vidhya/reinforcement-learning-in-continuous-action-spaces-ddpg-bbd64aa5434"> DDPG邮报</a>中，我们解决了OpenAI提供的两个环境。但是它们太简单了，我想尝试更具挑战性的连续动作空间环境。</p><p id="c63f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最受欢迎的套装之一是MuJoCo。MuJoCo代表多关节动力学与接触，引用自该网站“是一个物理引擎，旨在促进机器人，生物力学，图形和动画以及其他需要快速准确模拟的领域的研究和开发。它提供了速度、准确性和建模能力的独特组合，但它不仅仅是一个更好的模拟器”。不幸的是，MuJoCo不是免费的，它需要一个许可证。</p><p id="1be3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在谷歌搜索了一下之后，我发现了一个叫做PyBullet的开源替代品，它与MuJoco非常相似。您可以通过运行pip install pybullet来安装它，您可以在这里找到可用环境的列表<a class="ae kt" href="https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.wz5to0x8kqmr" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="0cb6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从pybullet上所有可用的环境中，我决定解决“人形”，目标是让人形学会走路。人形和其他环境是移植的机器人学校环境到pybullet。机器人学校的环境比体操馆的环境更艰苦。特别是，从文件“人形受益于更现实的能源成本(=扭矩×角速度)减去奖励。”</p><p id="251a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看随机代理在这种情况下如何表现:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ku kv l"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">他只是继续下降…</figcaption></figure><p id="056a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好吧，我们还有很多需要改进的地方。让我们开始讨论我们将要用来教这个机器人如何行走的算法。</p><h1 id="e830" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">软演员评论家(SAC)</h1><p id="e844" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">在<em class="jd">托马斯·塔诺贾等人</em>的论文<a class="ae kt" href="http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="jd">软行动者-批评家:非策略最大熵深度强化学习</em> </a>中介绍，被认为是解决连续行动空间环境的最佳算法之一。</p><p id="f423" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SAC将非政策更新与稳定的随机行动者-批评家公式相结合。它在随机政策优化和DDPG式方法之间架起了一座桥梁。</p><p id="2af7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SAC同时试图使用我们在PPO中看到的同样的直觉来最大化政策的预期收益和熵(你可以在这里找到文章<a class="ae kt" rel="noopener" href="/analytics-vidhya/beating-pong-using-reinforcement-learning-part-2-a2c-and-ppo-b83391dd3657"/>)。主要区别在于，在SAC中，我们试图最大化熵，而在PPO中，我们使用熵作为正则项。但目标是一样的，我们要用低熵鼓励探索，惩罚政策。</p><p id="364f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从原始论文中，我们可以看到SAC将最大化什么:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es la"><img src="../Images/283c6b9b4b9c4d33be3c30cfd65a4e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/0*xviTvzECaAxzCkVo.png"/></div></figure><p id="517a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一部分是预期收益，第二部分是保单的熵。α参数是温度，它决定了熵项相对于预期收益的相对重要性。如论文所述，通过将温度乘以α1，可以将其归入奖励中。</p><p id="453c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，从论文中可以看出，最大熵强化学习可以利用Q函数和值函数的<br/>:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lb"><img src="../Images/4b7c243247fb5bacd002ccafc1f44631.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*9REEz8E5T5Rf0bQb.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lc"><img src="../Images/7bdb5dca6141dbadd4a9ecb5e175a098.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/0*ayWffFrJr4FpJWnf.png"/></div></figure><p id="0c74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SAC利用三种类型的网络来近似:</p><ul class=""><li id="8e6f" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">状态值函数V</li><li id="3d67" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">软Q函数</li><li id="7b57" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">策略函数π</li></ul><p id="2218" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如您在上面的公式中看到的，V和Q函数通过策略相关联。所以原则上，我们可以从一个导出另一个，我们不需要两个独立的近似。但是作者说，在实践中，拥有独立的函数逼近器有助于收敛。</p><p id="adb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SAC算法还使用削波双Q技巧，该技巧在Fujimoto等人的<a class="ae kt" href="https://arxiv.org/pdf/1802.09477v3.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="jd">寻址演员-评论家方法</em> </a>中的函数近似误差中介绍。铝..因此将有两个网络来预测Q值，我们将取两个预测值中的最小值。这有助于在训练期间处理Q值高估。</p><p id="d200" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们总共有5个网络:</p><ul class=""><li id="5613" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">定义策略的参与者</li><li id="5093" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">两个批评家Q值</li><li id="326b" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">批评家价值</li><li id="998f" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">目标批评值</li></ul><p id="78a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们在《DDPG邮报》中看到的那样，使用了目标评论家值。它是原网络的延时拷贝，缓慢更新权值，提高学习稳定性。</p><p id="193b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过使用目标值网络进行贝尔曼近似，使用MSE目标来训练Q网络:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lb"><img src="../Images/033e2c55fac29d9773e31e4e6ada7727.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*jJu_1CmZ804r5fZX.png"/></div></figure><p id="aa8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用MSE目标训练V-网络，目标如下:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lc"><img src="../Images/372e2a3cb4d1d92d8d452c6e5d45d25e.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/0*CBnNH6F0OFkbGXjn.png"/></div></figure><p id="d7b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参与者被训练成最大化预期未来回报加上预期未来熵:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lr"><img src="../Images/1e84d0d43e84f65925fa223db1cf9dd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/0*XuYrp7y-AndiUX4s.png"/></div></figure><p id="ae8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将在这里使用重新参数化技巧，使随机性成为网络的输入。这在代码中会更清楚。</p><h1 id="b15d" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">重放缓冲器</h1><p id="9081" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">像往常一样，我们从重放缓冲区开始:</p><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="0bd8" class="lx jr hi lt b fi ly lz l ma mb">class ReplayBuffer():<br/>    def __init__(self, env, buffer_capacity=BUFFER_CAPACITY, batch_size=BATCH_SIZE, min_size_buffer=MIN_SIZE_BUFFER):<br/>        self.buffer_capacity = buffer_capacity<br/>        self.batch_size = batch_size<br/>        self.min_size_buffer = min_size_buffer<br/>        self.buffer_counter = 0<br/>        self.n_games = 0<br/>        <br/>        self.states = np.zeros((self.buffer_capacity, env.observation_space.shape[0]))<br/>        self.actions = np.zeros((self.buffer_capacity, env.action_space.shape[0]))<br/>        self.rewards = np.zeros((self.buffer_capacity))<br/>        self.next_states = np.zeros((self.buffer_capacity, env.observation_space.shape[0]))<br/>        self.dones = np.zeros((self.buffer_capacity), dtype=bool)<br/>        <br/>        <br/>    def __len__(self):<br/>        return self.buffer_counter<br/>    def add_record(self, state, action, reward, next_state, done):<br/>        # Set index to zero if counter = buffer_capacity and start again (1 % 100 = 1 and 101 % 100 = 1) so we substitute the older entries<br/>        index = self.buffer_counter % self.buffer_capacity<br/>        self.states[index] = state<br/>        self.actions[index] = action<br/>        self.rewards[index] = reward<br/>        self.next_states[index] = next_state<br/>        self.dones[index] = done<br/>        <br/>        # Update the counter when record something<br/>        self.buffer_counter += 1<br/>    <br/>    def check_buffer_size(self):<br/>        return self.buffer_counter &gt;= self.batch_size and self.buffer_counter &gt;= self.min_size_buffer<br/>    <br/>    def update_n_games(self):<br/>        self.n_games += 1<br/>    <br/>    def get_minibatch(self):<br/>        # If the counter is less than the capacity we don't want to take zeros records, <br/>        # if the cunter is higher we don't access the record using the counter <br/>        # because older records are deleted to make space for new one<br/>        buffer_range = min(self.buffer_counter, self.buffer_capacity)<br/>        <br/>        batch_index = np.random.choice(buffer_range, self.batch_size, replace=False)<br/>        # Convert to tensors<br/>        state = self.states[batch_index]<br/>        action = self.actions[batch_index]<br/>        reward = self.rewards[batch_index]<br/>        next_state = self.next_states[batch_index]<br/>        done = self.dones[batch_index]<br/>        <br/>        return state, action, reward, next_state, done<br/>    <br/>    def save(self, folder_name):<br/>        """<br/>        Save the replay buffer<br/>        """<br/>        if not os.path.isdir(folder_name):<br/>            os.mkdir(folder_name)<br/>        np.save(folder_name + '/states.npy', self.states)<br/>        np.save(folder_name + '/actions.npy', self.actions)<br/>        np.save(folder_name + '/rewards.npy', self.rewards)<br/>        np.save(folder_name + '/next_states.npy', self.next_states)<br/>        np.save(folder_name + '/dones.npy', self.dones)<br/>        <br/>        dict_info = {"buffer_counter": self.buffer_counter, "n_games": self.n_games}<br/>        <br/>        with open(folder_name + '/dict_info.json', 'w') as f:<br/>            json.dump(dict_info, f)<br/>    def load(self, folder_name):<br/>        """<br/>        Load the replay buffer<br/>        """<br/>        self.states = np.load(folder_name + '/states.npy')<br/>        self.actions = np.load(folder_name + '/actions.npy')<br/>        self.rewards = np.load(folder_name + '/rewards.npy')<br/>        self.next_states = np.load(folder_name + '/next_states.npy')<br/>        self.dones = np.load(folder_name + '/dones.npy')<br/>        <br/>        with open(folder_name + '/dict_info.json', 'r') as f:<br/>            dict_info = json.load(f)<br/>        self.buffer_counter = dict_info["buffer_counter"]<br/>        self.n_games = dict_info["n_games"]</span></pre><p id="33f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这与DDPG文章中的代码基本相同。我们存储所有的状态、动作、奖励、next_states以及从调用函数add_record的环境的交互中导出的终端标志。我们使用get_minibatch方法获得一个随机的minibatch，并且我们可以使用save和load方法保存和加载整个重放缓冲区。</p><h1 id="0c35" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">网络</h1><p id="d624" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">如前所述，我们需要定义三种近似状态值函数V、软Q函数和策略函数π的网络。所以我们需要两种类型的评论家和一个演员。</p><h1 id="ba6b" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">评论家Q值</h1><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="14d4" class="lx jr hi lt b fi ly lz l ma mb">class Critic(tf.keras.Model):<br/>    def __init__(self, name, hidden_0=CRITIC_HIDDEN_0, hidden_1=CRITIC_HIDDEN_1):<br/>        super(Critic, self).__init__()<br/>        self.hidden_0 = hidden_0<br/>        self.hidden_1 = hidden_1<br/>        self.net_name = name<br/>        self.dense_0 = Dense(self.hidden_0, activation='relu')<br/>        self.dense_1 = Dense(self.hidden_1, activation='relu')<br/>        self.q_value = Dense(1, activation=None)<br/>    def call(self, state, action):<br/>        state_action_value = self.dense_0(tf.concat([state, action], axis=1))<br/>        state_action_value = self.dense_1(state_action_value)<br/>        q_value = self.q_value(state_action_value)<br/>        return q_value</span></pre><p id="c1c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里没有什么新东西，它基本上与DDPG文章中使用的一样。</p><h1 id="db82" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">批评价值</h1><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="9d02" class="lx jr hi lt b fi ly lz l ma mb">class CriticValue(tf.keras.Model):<br/>    def __init__(self, name, hidden_0=CRITIC_HIDDEN_0, hidden_1=CRITIC_HIDDEN_1):<br/>        super(CriticValue, self).__init__()<br/>        self.hidden_0 = hidden_0<br/>        self.hidden_1 = hidden_1<br/>        self.net_name = name<br/>        <br/>        self.dense_0 = Dense(self.hidden_0, activation='relu')<br/>        self.dense_1 = Dense(self.hidden_1, activation='relu')<br/>        self.value = Dense(1, activation=None)<br/>    def call(self, state):<br/>        value = self.dense_0(state)<br/>        value = self.dense_1(value)<br/>        value = self.value(value)<br/>        return value</span></pre><p id="84c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，我们不需要动作作为输入来近似状态值，而只需要状态(状态值函数将状态映射到它的预期回报)。</p><h1 id="c2ca" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">行动者</h1><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="ede2" class="lx jr hi lt b fi ly lz l ma mb">class Actor(tf.keras.Model):<br/>    def __init__(self, name, upper_bound, actions_dim, hidden_0=CRITIC_HIDDEN_0, hidden_1=CRITIC_HIDDEN_1, epsilon=EPSILON, log_std_min=LOG_STD_MIN, log_std_max=LOG_STD_MAX):<br/>        super(Actor, self).__init__()<br/>        self.hidden_0 = hidden_0<br/>        self.hidden_1 = hidden_1<br/>        self.actions_dim = actions_dim<br/>        self.net_name = name<br/>        self.upper_bound = upper_bound<br/>        self.epsilon = epsilon<br/>        self.log_std_min = log_std_min<br/>        self.log_std_max = log_std_max<br/>        self.dense_0 = Dense(self.hidden_0, activation='relu')<br/>        self.dense_1 = Dense(self.hidden_1, activation='relu')<br/>        self.mean = Dense(self.actions_dim, activation=None)<br/>        self.log_std = Dense(self.actions_dim, activation=None)<br/>    def call(self, state):<br/>        policy = self.dense_0(state)<br/>        policy = self.dense_1(policy)<br/>        mean = self.mean(policy)<br/>        log_std = self.log_std(policy)<br/>        log_std = tf.clip_by_value(log_std, self.log_std_min, self.log_std_max)<br/>        return mean, log_std<br/>    def get_action_log_probs(self, state, reparameterization_trick=True):<br/>        mean, log_std = self.call(state)<br/>        std = tf.exp(log_std)<br/>        normal_distr = tfp.distributions.Normal(mean, std)<br/>        # Reparameterization trick<br/>        z = tf.random.normal(shape=mean.shape, mean=0., stddev=1.)<br/>        if reparameterization_trick:<br/>            actions = mean + std * z<br/>        else:<br/>            actions = normal_distr.sample()<br/>        action = tf.math.tanh(actions) * self.upper_bound<br/>        log_probs = normal_distr.log_prob(actions) - tf.math.log(1 - tf.math.pow(action,2) + self.epsilon)<br/>        log_probs = tf.math.reduce_sum(log_probs, axis=1, keepdims=True)<br/>        return action, log_probs</span></pre><p id="a4bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Actor网络返回两个输出，平均值和对数标准偏差。我们使用对数标准偏差，因为指数总是给出正数。对数标准偏差也被限制在一个区间内，以避免在训练开始时产生问题的极值。</p><p id="67a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意，在get_action_log_probs方法中，我们使用一个标志来激活重新参数化技巧。如果为真，我们从标准正态分布中抽取一些噪声，乘以标准偏差(参与者返回的对数的指数值)，然后将结果加到平均值上。如果为假，我们只使用平均值和标准差等于网络返回值的正态分布。</p><p id="6984" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">金玛等人在<a class="ae kt" href="https://arxiv.org/pdf/1312.6114v10.pdf" rel="noopener ugc nofollow" target="_blank">自动编码变分贝叶斯</a>中引入了重新参数化技巧的概念，我们需要它通过随机节点进行反向传播。如果你想更深入地了解这个概念，我在这里<a class="ae kt" href="https://gregorygundersen.com/blog/2018/04/29/reparameterization/" rel="noopener ugc nofollow" target="_blank">找到了一个很好的解释</a>，它为我澄清了这个概念。但是我们为什么在这里使用它呢？因为它具有更低的方差，并且有助于算法更快的收敛。</p><h1 id="a0ae" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">代理人</h1><p id="7e81" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">我们现在可以看一下<em class="jd">【大脑】</em>的代理人:</p><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="a34e" class="lx jr hi lt b fi ly lz l ma mb">class Agent:<br/>    def __init__(self, env, path_save=PATH_SAVE, path_load=PATH_LOAD, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR, gamma=GAMMA, tau=TAU, reward_scale=REWARD_SCALE):<br/>        self.gamma = gamma<br/>        self.tau = tau<br/>        self.replay_buffer = ReplayBuffer(env)<br/>        self.actions_dim = env.action_space.shape[0]<br/>        self.upper_bound = env.action_space.high[0]<br/>        self.lower_bound = env.action_space.low[0]<br/>        self.actor_lr = actor_lr<br/>        self.critic_lr = critic_lr<br/>        self.path_save = path_save<br/>        self.path_load = path_load</span><span id="a917" class="lx jr hi lt b fi mc lz l ma mb">        self.actor = Actor(actions_dim=self.actions_dim, name='actor', upper_bound=env.action_space.high)<br/>        self.critic_0 = Critic(name='critic_0')<br/>        self.critic_1 = Critic(name='critic_1')<br/>        self.critic_value = CriticValue(name='value')<br/>        self.critic_target_value = CriticValue(name='target_value')</span><span id="8114" class="lx jr hi lt b fi mc lz l ma mb">        self.actor.compile(optimizer=opt.Adam(learning_rate=self.actor_lr))<br/>        self.critic_0.compile(optimizer=opt.Adam(learning_rate=self.critic_lr))<br/>        self.critic_1.compile(optimizer=opt.Adam(learning_rate=self.critic_lr))<br/>        self.critic_value.compile(optimizer=opt.Adam(learning_rate=self.critic_lr))<br/>        self.critic_target_value.compile(optimizer=opt.Adam(learning_rate=self.critic_lr))</span><span id="9846" class="lx jr hi lt b fi mc lz l ma mb">        self.reward_scale = reward_scale</span><span id="f323" class="lx jr hi lt b fi mc lz l ma mb">        self.critic_target_value.set_weights(self.critic_value.weights)</span></pre><p id="b2c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在__init__方法中，我们定义了代理将使用的配置文件中的所有参数。我们定义了重播缓冲器和我们谈到的5个网络。与往常一样，我们将目标网络权重设置为等于相应的训练网络。</p><p id="7360" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们有一系列的实用方法，我们也在DDPG代码中使用:</p><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="6772" class="lx jr hi lt b fi ly lz l ma mb">def add_to_replay_buffer(self, state, action, reward, new_state, done):<br/>        self.replay_buffer.add_record(state, action, reward, new_state, done)<br/>        <br/>    def save(self):<br/>        date_now = time.strftime("%Y%m%d%H%M")<br/>        if not os.path.isdir(f"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}"):<br/>            os.makedirs(f"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}")<br/>        self.actor.save_weights(f"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}/{self.actor.net_name}.h5")<br/>        self.critic_0.save_weights(f"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}/{self.critic_0.net_name}.h5")<br/>        self.critic_1.save_weights(f"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}/{self.critic_1.net_name}.h5")<br/>        self.critic_value.save_weights(f"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}/{self.critic_value.net_name}.h5")<br/>        self.critic_target_value.save_weights(f"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}/{self.critic_target_value.net_name}.h5")<br/>        <br/>        self.replay_buffer.save(f"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}")</span><span id="7d13" class="lx jr hi lt b fi mc lz l ma mb">    def load(self):<br/>        self.actor.load_weights(f"{self.path_load}/{self.actor.net_name}.h5")<br/>        self.critic_0.load_weights(f"{self.path_load}/{self.critic_0.net_name}.h5")<br/>        self.critic_1.load_weights(f"{self.path_load}/{self.critic_1.net_name}.h5")<br/>        self.critic_value.load_weights(f"{self.path_load}/{self.critic_value.net_name}.h5")<br/>        self.critic_target_value.load_weights(f"{self.path_load}/{self.critic_target_value.net_name}.h5")<br/>        <br/>        self.replay_buffer.load(f"{self.path_load}")</span><span id="1c12" class="lx jr hi lt b fi mc lz l ma mb">    def get_action(self, observation):<br/>        state = tf.convert_to_tensor([observation])<br/>        actions, _ = self.actor.get_action_log_probs(state, reparameterization_trick=False)</span><span id="c377" class="lx jr hi lt b fi mc lz l ma mb">        return actions[0]</span></pre><p id="5187" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里没有什么新东西，让我们看看学习方法，事情变得有趣了:</p><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="48ed" class="lx jr hi lt b fi ly lz l ma mb">def learn(self):<br/>        if self.replay_buffer.check_buffer_size() == False:<br/>            return</span><span id="b62d" class="lx jr hi lt b fi mc lz l ma mb">        state, action, reward, new_state, done = self.replay_buffer.get_minibatch()</span><span id="4e90" class="lx jr hi lt b fi mc lz l ma mb">        states = tf.convert_to_tensor(state, dtype=tf.float32)<br/>        new_states = tf.convert_to_tensor(new_state, dtype=tf.float32)<br/>        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)<br/>        actions = tf.convert_to_tensor(action, dtype=tf.float32)</span><span id="108a" class="lx jr hi lt b fi mc lz l ma mb">        with tf.GradientTape() as tape:<br/>            value = tf.squeeze(self.critic_value(states), 1)<br/>            target_value = tf.squeeze(self.critic_target_value(new_states), 1)</span><span id="fd99" class="lx jr hi lt b fi mc lz l ma mb">            policy_actions, log_probs = self.actor.get_action_log_probs(states, reparameterization_trick=False)<br/>            log_probs = tf.squeeze(log_probs,1)<br/>            q_value_0 = self.critic_0(states, policy_actions)<br/>            q_value_1 = self.critic_1(states, policy_actions)<br/>            q_value = tf.squeeze(tf.math.minimum(q_value_0, q_value_1), 1)</span><span id="458a" class="lx jr hi lt b fi mc lz l ma mb">            value_target = q_value - log_probs<br/>            value_critic_loss = 0.5 * tf.keras.losses.MSE(value, value_target)</span><span id="267a" class="lx jr hi lt b fi mc lz l ma mb">        value_critic_gradient = tape.gradient(value_critic_loss, self.critic_value.trainable_variables)<br/>        self.critic_value.optimizer.apply_gradients(zip(value_critic_gradient, self.critic_value.trainable_variables))<br/></span><span id="d1e5" class="lx jr hi lt b fi mc lz l ma mb">        with tf.GradientTape() as tape:<br/>            new_policy_actions, log_probs = self.actor.get_action_log_probs(states, reparameterization_trick=True)<br/>            log_probs = tf.squeeze(log_probs, 1)<br/>            new_q_value_0 = self.critic_0(states, new_policy_actions)<br/>            new_q_value_1 = self.critic_1(states, new_policy_actions)<br/>            new_q_value = tf.squeeze(tf.math.minimum(new_q_value_0, new_q_value_1), 1)<br/>        <br/>            actor_loss = log_probs - new_q_value<br/>            actor_loss = tf.math.reduce_mean(actor_loss)</span><span id="9737" class="lx jr hi lt b fi mc lz l ma mb">        actor_gradient = tape.gradient(actor_loss, self.actor.trainable_variables)<br/>        self.actor.optimizer.apply_gradients(zip(actor_gradient, self.actor.trainable_variables))<br/>        </span><span id="3082" class="lx jr hi lt b fi mc lz l ma mb">        with tf.GradientTape(persistent=True) as tape:<br/>            q_pred = self.reward_scale * reward + self.gamma * target_value * (1-done)<br/>            old_q_value_0 = tf.squeeze(self.critic_0(state, action), 1)<br/>            old_q_value_1 = tf.squeeze(self.critic_1(state, action), 1)<br/>            critic_0_loss = 0.5 * tf.keras.losses.MSE(old_q_value_0, q_pred)<br/>            critic_1_loss = 0.5 * tf.keras.losses.MSE(old_q_value_1, q_pred)<br/>    <br/>        critic_0_network_gradient = tape.gradient(critic_0_loss, self.critic_0.trainable_variables)<br/>        critic_1_network_gradient = tape.gradient(critic_1_loss, self.critic_1.trainable_variables)</span><span id="0d70" class="lx jr hi lt b fi mc lz l ma mb">        self.critic_0.optimizer.apply_gradients(zip(critic_0_network_gradient, self.critic_0.trainable_variables))<br/>        self.critic_1.optimizer.apply_gradients(zip(critic_1_network_gradient, self.critic_1.trainable_variables))</span><span id="35e6" class="lx jr hi lt b fi mc lz l ma mb">        self.update_target_networks(tau=self.tau)<br/>        <br/>        self.replay_buffer.update_n_games()</span></pre><p id="9ed1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有四个网络需要训练:</p><ul class=""><li id="8e77" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">批评价值</li><li id="4560" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">行动者</li><li id="ca61" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">评论家Q值为0</li><li id="27ad" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">评论家Q值1</li></ul><h1 id="2e46" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">批评价值</h1><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="baf7" class="lx jr hi lt b fi ly lz l ma mb">with tf.GradientTape() as tape:<br/>            value = tf.squeeze(self.critic_value(states), 1)<br/>            target_value = tf.squeeze(self.critic_target_value(new_states), 1)</span><span id="023b" class="lx jr hi lt b fi mc lz l ma mb">            policy_actions, log_probs = self.actor.get_action_log_probs(states, reparameterization_trick=False)<br/>            log_probs = tf.squeeze(log_probs,1)<br/>            q_value_0 = self.critic_0(states, policy_actions)<br/>            q_value_1 = self.critic_1(states, policy_actions)<br/>            q_value = tf.squeeze(tf.math.minimum(q_value_0, q_value_1), 1)</span><span id="640e" class="lx jr hi lt b fi mc lz l ma mb">            value_target = q_value - log_probs<br/>            value_critic_loss = tf.keras.losses.MSE(value, value_target)<br/>        value_critic_gradient = tape.gradient(value_critic_loss, self.critic_value.trainable_variables)<br/>        self.critic_value.optimizer.apply_gradients(zip(value_critic_gradient, self.critic_value.trainable_variables))</span></pre><p id="af67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们把价值网络和它对应的目标网络的预测。然后，我们使用参与者的策略，通过我们讨论过的限幅双Q技巧，从Q值网络中获得Q值。我们将预测值/目标值定义为Q值和对数概率之间的差值。最后，损失是估计值和价值网络预测值之间的均方误差。</p><h1 id="4c5f" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">行动者</h1><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="418b" class="lx jr hi lt b fi ly lz l ma mb">with tf.GradientTape() as tape:<br/>            new_policy_actions, log_probs = self.actor.get_action_log_probs(states, reparameterization_trick=True)<br/>            log_probs = tf.squeeze(log_probs, 1)<br/>            new_q_value_0 = self.critic_0(states, new_policy_actions)<br/>            new_q_value_1 = self.critic_1(states, new_policy_actions)<br/>            new_q_value = tf.squeeze(tf.math.minimum(new_q_value_0, new_q_value_1), 1)<br/>        <br/>            actor_loss = log_probs - new_q_value<br/>            actor_loss = tf.math.reduce_mean(actor_loss)</span><span id="2547" class="lx jr hi lt b fi mc lz l ma mb">        actor_gradient = tape.gradient(actor_loss, self.actor.trainable_variables)<br/>        self.actor.optimizer.apply_gradients(zip(actor_gradient, self.actor.trainable_variables))</span></pre><p id="6b03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">和以前一样，我们从行动者那里得到行动和对数概率，从两个批评家那里得到Q值，我们将损失定义为对数分布和Q值之间的差值。请注意，我们希望最大化Q值——log(π),因此我们将损耗定义为目标函数乘以-1。</p><h1 id="7d28" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">评论家Q值</h1><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="5b87" class="lx jr hi lt b fi ly lz l ma mb">with tf.GradientTape(persistent=True) as tape:<br/>            q_pred = self.reward_scale * reward + self.gamma * target_value * (1-done)<br/>            old_q_value_0 = tf.squeeze(self.critic_0(state, action), 1)<br/>            old_q_value_1 = tf.squeeze(self.critic_1(state, action), 1)<br/>            critic_0_loss = tf.keras.losses.MSE(old_q_value_0, q_pred)<br/>            critic_1_loss = tf.keras.losses.MSE(old_q_value_1, q_pred)<br/>    <br/>        critic_0_network_gradient = tape.gradient(critic_0_loss, self.critic_0.trainable_variables)<br/>        critic_1_network_gradient = tape.gradient(critic_1_loss, self.critic_1.trainable_variables)</span><span id="cb46" class="lx jr hi lt b fi mc lz l ma mb">        self.critic_0.optimizer.apply_gradients(zip(critic_0_network_gradient, self.critic_0.trainable_variables))<br/>        self.critic_1.optimizer.apply_gradients(zip(critic_1_network_gradient, self.critic_1.trainable_variables))</span></pre><p id="2387" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Critic Q值网络，我们需要通过相同的计算来计算多个梯度，因此我们使用一个持久的梯度带。我们将预测Q值定义为单集的奖励加上目标评论家价值网络预测的价值的现值。我们可以在这里看到奖励尺度，如前所述，是熵温度的倒数。两个网络的损失是该值和它们的预测值之间的均方误差。</p><h1 id="a760" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">训练循环和结果</h1><p id="b44a" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">我们现在可以看到训练循环和结果。</p><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="0a53" class="lx jr hi lt b fi ly lz l ma mb">config = dict(<br/>  learning_rate_actor = ACTOR_LR,<br/>  learning_rate_critic = ACTOR_LR,<br/>  batch_size = BATCH_SIZE,<br/>  architecture = "SAC",<br/>  infra = "Colab",<br/>  env = ENV_NAME<br/>)</span><span id="2497" class="lx jr hi lt b fi mc lz l ma mb">wandb.init(<br/>  project=f"tensorflow2_sac_{ENV_NAME.lower()}",<br/>  tags=["SAC", "FCL", "RL"],<br/>  config=config,<br/>)</span><span id="ffca" class="lx jr hi lt b fi mc lz l ma mb">env = gym.make(ENV_NAME)<br/>agent = Agent(env)</span><span id="6f8c" class="lx jr hi lt b fi mc lz l ma mb">scores = []<br/>evaluation = True</span><span id="508c" class="lx jr hi lt b fi mc lz l ma mb">if PATH_LOAD is not None:<br/>    print("loading weights")<br/>    observation = env.reset()<br/>    action, log_probs = agent.actor.get_action_log_probs(observation[None, :], False)<br/>    agent.actor(observation[None, :])<br/>    agent.critic_0(observation[None, :], action)<br/>    agent.critic_1(observation[None, :], action)<br/>    agent.critic_value(observation[None, :])<br/>    agent.critic_target_value(observation[None, :])<br/>    agent.load()<br/>    print(agent.replay_buffer.buffer_counter)<br/>    print(agent.replay_buffer.n_games)</span></pre><p id="738b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们配置<a class="ae kt" href="https://wandb.ai/" rel="noopener ugc nofollow" target="_blank"> wandb </a>进行日志记录，并加载任何预先训练的代理。</p><p id="a59f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们有一个训练循环:</p><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="471c" class="lx jr hi lt b fi ly lz l ma mb">for _ in tqdm(range(MAX_GAMES)):<br/>    start_time = time.time()<br/>    states = env.reset()<br/>    done = False<br/>    score = 0<br/>    while not done:<br/>        action = agent.get_action(states)<br/>        new_states, reward, done, info = env.step(action)<br/>        score += reward<br/>        agent.add_to_replay_buffer(states, action, reward, new_states, done)<br/>        agent.learn()<br/>        states = new_states<br/>    <br/>    scores.append(score)<br/>    agent.replay_buffer.update_n_games()</span><span id="1b04" class="lx jr hi lt b fi mc lz l ma mb">    wandb.log({'Game number': agent.replay_buffer.n_games, '# Episodes': agent.replay_buffer.buffer_counter, <br/>               "Average reward": round(np.mean(scores[-10:]), 2), \<br/>                      "Time taken": round(time.time() - start_time, 2)})<br/>    <br/>    if (_ + 1) % SAVE_FREQUENCY == 0:<br/>        print("saving...")<br/>        agent.save()<br/>        print("saved")</span></pre><p id="7236" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练循环总是相同的，我们与环境交互，将状态、动作、奖励和终端标志保存在重放缓冲区中，我们使用之前看到的learn()方法训练代理。当这一集结束时，我们评估记录最后十场比赛的平均值的结果，并且我们每SAVE_FREQUENCY(在配置文件中定义为200)次保存网络和重放缓冲区。</p><p id="ce98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看结果:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es md"><img src="../Images/98c4168964fc683f23edd71cf01df196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sPa6c2qQ6CVFfvEF.png"/></div></div></figure><p id="0ca2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所见，平均奖励在1k互动后开始增加。这花了一段时间，但我们可以看到代理如何开始学习走路:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ku kv l"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">我们从不给风格打分…</figcaption></figure><p id="9710" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好了，这家伙可以站起来走几步了，但是正如你所看到的，还有很多东西要学。但此时只是训练时间。</p><p id="5189" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在我的<a class="ae kt" href="https://antonai.blog/learning-to-walk-using-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">博客</a>上找到原文，在我的<a class="ae kt" href="https://github.com/antonai91/reinforcement_learning/tree/master/sac" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到所有代码。有任何问题，可以通过<a class="ae kt" href="https://www.linkedin.com/in/lisiantonio/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我。</p><p id="f903" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你喜欢这篇文章，分享给你的朋友和同事吧！我会在下一篇文章中看到你。与此同时，要小心，保持安全，记住<em class="jd">不要成为另一块墙砖</em>。</p><p id="911a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Anton.ai</p></div></div>    
</body>
</html>