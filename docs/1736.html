<html>
<head>
<title>Principal Component Analysis (PCA) :- A saviour in the sea of features in predective modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析:预测建模中特征海洋的救星</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/principal-component-analysis-pca-a-saviour-in-the-sea-of-features-in-predective-modeling-fc40c3d9d95a?source=collection_archive---------11-----------------------#2021-03-15">https://medium.com/analytics-vidhya/principal-component-analysis-pca-a-saviour-in-the-sea-of-features-in-predective-modeling-fc40c3d9d95a?source=collection_archive---------11-----------------------#2021-03-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/78da8ce3f3c460a8e355d98eff4e46b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JDh43eLN8kWM9otoTblRiQ.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="6f31" class="pw-subtitle-paragraph ip hr hs bd b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg dx translated">诅咒-</h2></div><p id="f80a" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">在数据科学领域，在构建预测模型时，特征的作用非常重要。为了获得良好的结果和准确性，通常认为特征的数量越多，模型的准确性越高。功能也称为尺寸，今后我们将交替使用它们。所以说重点，这并不总是对的。拥有成百上千个维度会产生巨大的问题，最终会产生比原来更多的问题。</p><p id="cf88" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">两个最臭名昭著的大规模因素是:<br/> - &gt;计算时间的增加<br/> - &gt;很难(或不可能)可视化特征之间的关系。</p><p id="1f6c" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">让我们更深入地研究一下这些问题。我们将从第一个问题开始——与计算资源相关的问题。大多数机器学习算法依赖于距离的计算来建立模型，并且随着维度数量的增加，从维度创建模型变得越来越计算密集。举个例子，如果我们只需要计算一维空间中两点之间的距离，比如2轴平面中一条线上的两点，我们只需要用一个点的坐标减去另一个点的坐标，然后取大小:<br/>距离= 𝑥1−𝑥2 <br/>如果我们需要计算二维空间中两点之间的距离呢？</p><p id="9682" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">同样的公式转化为:距离=</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div class="er es kd"><img src="../Images/d3b728a00428f147d187f33e5d305193.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*FDU0Z5XRodArWD-E7JLgCA.png"/></div></figure><p id="de50" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">如果我们需要计算三维空间中两点之间的距离呢？</p><p id="a5a1" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">同样的公式转化为:距离=</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div class="er es ki"><img src="../Images/5c2d144fe32e4c67f00a8112c696de40.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*4sV9UIfODuc_Ia3U8vBw2A.png"/></div></figure><p id="f918" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">对于N维，公式变为:距离=</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div class="er es kj"><img src="../Images/dce055453cbf5fd2d6b3d272d2be0be8.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*_PaeHWLnWf1R5sRuEjuvcQ.png"/></div></figure><p id="fb5c" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">这是计算两点间距离的努力。想象一下所有相关数据点所需的计算量。这些计算量需要非常高的CPU、内存和其他计算资源，这使得操作非常昂贵。还有一点要考虑的是，随着维数的增加，点与点之间的距离越来越远。这意味着当我们测试模型时，任何新的点都将远离我们的训练点。这导致模型不太可靠，并且使我们的模型过度适应训练数据。</p><p id="3fb0" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">第二个问题与用户或构建模型的人有关。由于人类非常适合在三维空间中观察和思考，我们可以很容易地在三维空间中想象一个形状、图形或地图。超越三维，任何人都很难理解。通常，数据科学家会得到具有数千个要素的数据集，为了对数据集有一个基本的概念，我们需要首先将它们可视化。为了简单起见，我们创建二维图形。假设数据集中有1000个要素。这导致总共有(1000*999)/2= 499500种可能的组合来创建二维图形。人类不可能分析所有这些图表来理解变量之间的关系。因此，解决方案是从这个n维数据集创建二维或三维图形。</p><p id="a97c" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">这些情况通常被称为“维数灾难”。现在，为了处理这些维度，我们必须在不影响数据的情况下将它们减少到更少的可用维度，这被称为“降维”,这种技术被称为<em class="kk">降维技术。</em> <br/>为了使这种技巧奏效我们必须决定下面几点</p><ul class=""><li id="f213" class="kl km hs jj b jk jl jn jo jq kn ju ko jy kp kc kq kr ks kt bi translated">所有的特征真的有助于决策吗？</li><li id="99bb" class="kl km hs jj b jk ku jn kv jq kw ju kx jy ky kc kq kr ks kt bi translated">有没有办法用较少的特征得出相同的结论？</li><li id="ec90" class="kl km hs jj b jk ku jn kv jq kw ju kx jy ky kc kq kr ks kt bi translated">有没有一种方法可以将特征组合起来创建一个新的特征并删除旧的特征？</li><li id="1ff1" class="kl km hs jj b jk ku jn kv jq kw ju kx jy ky kc kq kr ks kt bi translated">有没有一种方法可以重塑特征，让它们在视觉上更容易理解？</li></ul><h2 id="ee0b" class="kz la hs bd lb lc ld le lf lg lh li lj jq lk ll lm ju ln lo lp jy lq lr ls lt bi translated">降维技术到底是什么？</h2><p id="56f0" class="pw-post-body-paragraph jh ji hs jj b jk lu it jm jn lv iw jp jq lw js jt ju lx jw jx jy ly ka kb kc ha bi translated">降维是一种特征选择技术，使用它我们可以减少用于建立模型的特征数量，而不会丢失与原始数据集相比的大量信息。换句话说，降维技术将更高维的数据投影到更低维的子空间。在将数据输入机器学习算法之前，应使用维数缩减。</p><h2 id="2b52" class="kz la hs bd lb lc ld le lf lg lh li lj jq lk ll lm ju ln lo lp jy lq lr ls lt bi translated">PCA来救援了！</h2><p id="9e46" class="pw-post-body-paragraph jh ji hs jj b jk lu it jm jn lv iw jp jq lw js jt ju lx jw jx jy ly ka kb kc ha bi translated">主成分分析(<strong class="jj ht"> PCA </strong>)是一种无监督的机器学习算法，用于使用降维技术进行特征选择。顾名思义，它从数据中找出主要成分。PCA将数据从高维空间转换并拟合到新的较低维的子空间，这产生了全新的点坐标系，其中第一轴对应于解释数据中最大差异的第一主分量。<strong class="jj ht">主成分</strong>是解释数据中最大方差的衍生特征。第一个主成分解释了最大的方差，第二个解释了稍小的方差，以此类推。使用PCA发现的每个新维度都是旧特征的线性组合。</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lz"><img src="../Images/9575bcb6444398ce7ad6ca62f78423bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OcII8OCE7bFrpD9z9JDyBw.png"/></div></div></figure><p id="c9fc" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">在上图中，我们考虑3个正交(<em class="kk"> C3在第三维</em>)轴来显示数据的分布。在右边的图表中，前两个轴<strong class="jj ht"> C1 </strong>和<strong class="jj ht"> C2 </strong>成功地解释了数据中的最大变化，而轴<strong class="jj ht"> C3 </strong>只包含较少数量的点。因此，在考虑主要成分时，C1和C2将是我们的选择。</p><h2 id="f4c4" class="kz la hs bd lb lc ld le lf lg lh li lj jq lk ll lm ju ln lo lp jy lq lr ls lt bi translated">计算PCA的步骤</h2><p id="4182" class="pw-post-body-paragraph jh ji hs jj b jk lu it jm jn lv iw jp jq lw js jt ju lx jw jx jy ly ka kb kc ha bi translated">PCA使用一种称为奇异值分解(SVD)的方法，该方法分解数据集矩阵，使其成为三个单独矩阵相乘的产物。</p><p id="5c88" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">x(原始数据)= 1</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div class="er es ma"><img src="../Images/a8adbe0cb01806576118948af564ae51.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*H-XctdiU32e9RprjgEI_cg.png"/></div></figure><p id="5fb5" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">其中V是包含主分量的矩阵。PCA假设所有单个列的平均值为零，标准偏差为1。因此，在应用PCA之前，应该对数据进行适当的预处理。<br/>让我们将它绘制在XY平面上，并计算所有点的平均大小。蓝色的是实际点数，黄色的是平均点数。</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mb"><img src="../Images/3e78f33add3c0dcb67e70d99d25d1fa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ALT3pLG8rzbnwfcRn7WvVA.png"/></div></div></figure><p id="15a1" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">让我们移动这些点，使平均点在原点上。这叫做平行翻译。尽管这些点的坐标发生了变化，但它们之间相应的距离保持不变。</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mc"><img src="../Images/3e319f9a95c3b9abe3edff440b55ffbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I5FQePP68zq90PpTqB_Stw.png"/></div></div></figure><p id="5e7d" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">让我们为新数据点创建最佳拟合线。为此，我们首先从一条随机线(蓝色线)开始，然后尝试找到最佳拟合线(绿色线),使与各个数据点的距离最小，从而使与原点的距离最大。这条最佳拟合线称为主成分1或PC1。然后画一条称为PC2的线，它垂直于PC1。之后，轴PC1和PC2旋转，使得PC1成为水平轴，如下图所示。</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es md"><img src="../Images/a8f7c7e52aab46c3dedbc7f525dc82c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OYicXXEUVCiVrNIA01ldCw.png"/></div></div></figure><p id="8e90" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">然后，根据采样点，使用PC1和PC2投影新点。这样我们就得到派生的特征。</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div class="er es me"><img src="../Images/19e9b7397ad2233bfff425f86a1ba66f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*MstR8UjGwpwbf-3WRn_Rfg.png"/></div></figure><p id="b3f6" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">我们知道主成分是找到创建模型所需的特征数量的关键，但是如何确定所需的最佳主成分数量以及如何选择PC2呢？</p><p id="c0f4" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">这些问题可以用<em class="kk">解释方差比</em>或EVR来回答。它代表了每个主成分能够解释的差异量。主成分解释的方差越多，它就越有可能代表最大维度数，或者该成分解释最大数据方差。例如，如果位于PC1上的所有点到原点的距离平方为50，而位于PC2上的所有点到原点的距离平方为5，则</p><p id="9834" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">PC1的EVR =</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div class="er es mf"><img src="../Images/fee5cdc59462736904f13996870727af.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*CwTgM_7SQNQrfkicBZisgg.png"/></div></figure><p id="7637" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">PC2的EVR =</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div class="er es mg"><img src="../Images/06a5ed69cd2dd7e6fee950dbc9307a2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*vMIO01cbQWSXNhKewObtfQ.png"/></div></figure><p id="67ed" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">因此，PC1解释了91%的数据差异。然而，PC2仅解释了9%的差异。因此，我们只能使用PC1作为我们模型的输入，因为它解释了方差的大部分。</p><p id="2cf8" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">在建立模型时，这些过程都是由Scree图来完成并找到主成分的。</p><h2 id="2a85" class="kz la hs bd lb lc ld le lf lg lh li lj jq lk ll lm ju ln lo lp jy lq lr ls lt bi translated">碎石图</h2><p id="bbb6" class="pw-post-body-paragraph jh ji hs jj b jk lu it jm jn lv iw jp jq lw js jt ju lx jw jx jy ly ka kb kc ha bi translated">Scree图是传达对应的主成分解释了多少方差的图形。</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mh"><img src="../Images/d420a75f1ef2d421eb68c7578ff01422.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EUcY6YwTvp-SRltBDtZ8RQ.png"/></div></div></figure><p id="c690" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">这个图也可以用肘法来使用，即找出图变得分散或连续的点，这个点可以认为是主分量(维数)。如上图所示，大约75个主成分解释了大约90 %的方差。因此，根据这种情况，75可能是一个不错的选择。我们必须从图表中考虑我们的数据集有400个特征。</p><h2 id="3df1" class="kz la hs bd lb lc ld le lf lg lh li lj jq lk ll lm ju ln lo lp jy lq lr ls lt bi translated">σ的特征分解</h2><p id="c231" class="pw-post-body-paragraph jh ji hs jj b jk lu it jm jn lv iw jp jq lw js jt ju lx jw jx jy ly ka kb kc ha bi translated">有两种方法可以找到PCA。一种是通过特征分解，另一种是通过协方差。PCA可以通过数据协方差(或相关性)矩阵的特征值分解或数据矩阵的奇异值分解来完成。<br/>σ是实对称矩阵；因此，它有<br/> 1)实特征值<br/> 2)正交特征向量<br/>所以我们必须计算这2个值来找到PCA。</p><p id="fa6b" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">线性变换<strong class="jj ht"> T </strong>的<strong class="jj ht">特征向量v </strong>是非零向量，当对其应用<strong class="jj ht"> T </strong>时，不会改变方向。将<strong class="jj ht"> T </strong>应用于特征向量，仅将特征向量缩放标量值λ，称为<strong class="jj ht">特征值</strong>。这种情况可以写成等式:</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div class="er es mi"><img src="../Images/54f9c7e59aee3d575df1f379702f0569.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*l25n0TQlq45Ffy_EpM_Gdw.png"/></div></figure><h2 id="c11d" class="kz la hs bd lb lc ld le lf lg lh li lj jq lk ll lm ju ln lo lp jy lq lr ls lt bi translated">特征分解方法</h2><ol class=""><li id="7c28" class="kl km hs jj b jk lu jn lv jq mj ju mk jy ml kc mm kr ks kt bi translated">归一化𝐴A列，使每个特征的平均值为零</li><li id="49b9" class="kl km hs jj b jk ku jn kv jq kw ju kx jy ky kc mm kr ks kt bi translated">计算样本协方差矩阵σ=𝐴𝑇𝐴/(𝑚−1)σ=ata/(m−1)</li><li id="cda7" class="kl km hs jj b jk ku jn kv jq kw ju kx jy ky kc mm kr ks kt bi translated">使用<code class="du mn mo mp mq b">np.linalg.eig(Sigma)</code>执行σσ的特征分解</li><li id="a119" class="kl km hs jj b jk ku jn kv jq kw ju kx jy ky kc mm kr ks kt bi translated">通过根据最大e值排序𝑘k向量并计算𝐴𝑋𝑘AXk来压缩</li><li id="63a7" class="kl km hs jj b jk ku jn kv jq kw ju kx jy ky kc mm kr ks kt bi translated">通过计算𝐴𝑋𝑘从压缩版本重建</li></ol><p id="0694" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">以上所有步骤都可以用下面这张gif来概括。</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mr"><img src="../Images/ea81306d3cd9a904a92c387ab7e7e0b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*UpFltkN-kT9aGqfLhOR9xg.gif"/></div></div><figcaption class="ms mt et er es mu mv bd b be z dx translated"><a class="ae mw" href="https://i.stack.imgur.com/Q7HIP.gif" rel="noopener ugc nofollow" target="_blank">https://i.stack.imgur.com/Q7HIP.gif</a></figcaption></figure><p id="3dce" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">特征分解(或PCA)找到线，其中:<br/> 1。沿着黑线的值的分布是<strong class="jj ht">最大<br/> </strong> 2 <strong class="jj ht">。</strong>投影误差(红线总和)为<strong class="jj ht">最小</strong></p><h2 id="dc18" class="kz la hs bd lb lc ld le lf lg lh li lj jq lk ll lm ju ln lo lp jy lq lr ls lt bi translated">协方差</h2><p id="6f38" class="pw-post-body-paragraph jh ji hs jj b jk lu it jm jn lv iw jp jq lw js jt ju lx jw jx jy ly ka kb kc ha bi translated"><em class="kk">方差</em>是衡量一个变量如何变化或变动的尺度，而<em class="kk"> co </em>表示一起。因此，<em class="kk">协方差</em>是两个变量如何一起变化的度量。</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div class="er es mx"><img src="../Images/2f70116072a1f0e60fe961190c2f83bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*p4_u0HM66mjA-bO99V9I6w.png"/></div></figure><p id="705c" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">如果协方差很高，这意味着变量高度相关，一个变量的变化也会导致另一个变量的变化。通常，我们避免在建立机器学习模型时使用高度相关的变量。为了找到协方差，我们可以使用下面的公式。这里数据集名称是A，它有两个特征/变量A0-7a 1。</p><figure class="ke kf kg kh fd hj er es paragraph-image"><div class="er es my"><img src="../Images/f7630e669b15959f1963275ffdcc2c36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*IkXxwBkf7zU9bJNe_1WosQ.png"/></div></figure><p id="ba06" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">其中𝑎0是列𝑎0的平均值，𝑎1是列𝑎1.的平均值</p><p id="02b9" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">在对PCA有了进一步的了解之后，我们来看看它的利弊。</p><h2 id="c03a" class="kz la hs bd lb lc ld le lf lg lh li lj jq lk ll lm ju ln lo lp jy lq lr ls lt bi translated">PCA的优点</h2><ul class=""><li id="9c7d" class="kl km hs jj b jk lu jn lv jq mj ju mk jy ml kc kq kr ks kt bi translated">相关特征被移除。</li><li id="0304" class="kl km hs jj b jk ku jn kv jq kw ju kx jy ky kc kq kr ks kt bi translated">减少了模型训练时间。</li><li id="e66f" class="kl km hs jj b jk ku jn kv jq kw ju kx jy ky kc kq kr ks kt bi translated">减少了过度拟合。</li><li id="ee08" class="kl km hs jj b jk ku jn kv jq kw ju kx jy ky kc kq kr ks kt bi translated">有助于更好的可视化</li><li id="75cc" class="kl km hs jj b jk ku jn kv jq kw ju kx jy ky kc kq kr ks kt bi translated">处理噪音的能力</li></ul><h2 id="11df" class="kz la hs bd lb lc ld le lf lg lh li lj jq lk ll lm ju ln lo lp jy lq lr ls lt bi translated">PCA的缺点</h2><ul class=""><li id="3ef6" class="kl km hs jj b jk lu jn lv jq mj ju mk jy ml kc kq kr ks kt bi translated">产生的主成分比原始数据更难解释</li><li id="3c6a" class="kl km hs jj b jk ku jn kv jq kw ju kx jy ky kc kq kr ks kt bi translated">如果解释的差异阈值没有被适当地考虑，可能导致信息丢失。</li></ul><h2 id="4fa8" class="kz la hs bd lb lc ld le lf lg lh li lj jq lk ll lm ju ln lo lp jy lq lr ls lt bi translated">结论</h2><p id="1ddc" class="pw-post-body-paragraph jh ji hs jj b jk lu it jm jn lv iw jp jq lw js jt ju lx jw jx jy ly ka kb kc ha bi translated">从上面的讨论中，我们可以得出结论，PCA是一种非常强大的技术，用于降低数据的维度，将数据从较高的维度投影到较低的维度，有助于数据可视化，有助于数据压缩，并且最重要的是，通过减少计算中涉及的变量数量，极大地提高了模型训练速度。</p></div></div>    
</body>
</html>