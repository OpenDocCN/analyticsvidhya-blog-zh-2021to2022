# 文本处理工具我希望我早点知道

> 原文：<https://medium.com/analytics-vidhya/text-processing-tools-i-wish-i-knew-earlier-a6960e16a9c9?source=collection_archive---------6----------------------->

![](img/adca74af89e8eb74303d9d0ea0623251.png)

# 介绍

在我从事数据科学的过程中，从学校工作到实习再到全职工作，自然语言处理(NLP)一直是我感兴趣的领域。阅读像《变形金刚》这样最新最伟大的 NLP 技术，一直都是耐人寻味，引人入胜的。然而，有一件事仍然是 NLP 的基石，那就是文本处理。

实际上，我已经写了一篇中型文章，概述了一些常见的过程，如词汇化和词干化，以及停用词和标点符号的删除。如果你想了解更多，这里有一个我正在谈论的文章的链接。尽管有无数的其他文章已经讨论过了。这就引出了我写这篇文章的原因。我想分享两个文本处理函数，我觉得它们在我的数据科学之旅中很有用，而且我没有找到太多关于它们的文献。

它们是**标签处理**和**乱码检测**。

# 标签处理

当我在一家专注于影响者的 MarTech 初创公司工作时，我必须处理大量来自社交媒体的数据。这意味着与新闻文章或研究论文相比，文本数据将会更加混乱，结构化程度更低。鉴于大多数社交媒体文本都是个性化的，它们也与亚马逊评论等文本略有不同。有了社交媒体数据，会有更多的表情符号、提及、俚语和标签。

标签无疑是文本中最有问题的部分之一。移除表情符号和提及，或者替换俚语会很容易。但是，即使移除了 hashtag 符号，它仍然会留下字符组合，中间没有空格。因为大多数文本处理通过分隔空格来标记句子中的单词

```
"This is a sentence" -> ["This", "is", "a" "sentence"]
```

简单地删除 hashtag 符号会导致字符组合本身被认为是一个标记:

```
"foodislife" -> ["foodislife"]

# instead of

"foodislife" -> ["food", "is", "life"]
```

因此，hashtag 处理函数能够将这个组合拆分成单个的单词。

以下是完整的代码:

# 它是如何工作的

基于语料库，我们生成单词计数的字典。所以我们从字面上统计每个词在语料库中的出现次数。由此，我们通过字母的组合逐个字符地迭代，并且在每次迭代中，我们使用字典计算它是一个单词的概率。

基于字母组合的窗口成为单词的可能性以及窗口的起始索引成为单词的良好起点的概率来计算概率。

通过这种方法，在字母组合中的每一个可能的拆分处，我们将得到该拆分形成一个单词的概率。然后，该算法将跟踪概率最高的索引，并在这些点上拆分字母组合以形成一个句子

```
'ilovefood'# iteration 1:
'i'
## at 'i', since it's such a common word, it would have a high probability
## hence, we consider it a word# iteration 2:
'l'
## 'l' is not common by itself and hence would have a low probability of it being a word# iteration 3:
'lo'
## 'lo' is also not common.# and so on.. until we split at 
'i', 'love', 'food'
```

注意，`word_dict.json`文件来自上面的`generate_word_dictionary`函数。我只是将字典导出到一个. json 文件中。为了方便起见，这里有一个将字典导出到. json 文件的函数:

```
import jsondef export_dict_to_json(dictionary: dict, file_name: str):
		with open(file_name, 'w') as file:
				json.dump(dictionary, file)
```

# 胡言乱语检测

这并不像 hashtag 函数那样有用，但是我真的觉得这很有趣，而且在需要的时候非常有用。我发现这个函数在执行数据探索之前特别有用。这是因为数据集中的一些句子可能有非常长的乱码。这最终扭曲了句子和数据集的度量，并使真正理解数据集变得困难。

在我解释算法如何工作之前，让我展示完整的代码:

乱码检测功能的工作方式类似于 hashtag 处理功能。从某种意义上说，他们都利用了概率。然而，乱码检测函数得到的不是单词的概率，而是一个字母跟随另一个字母的概率。比如‘a’在‘b’之后的概率。或者字母“z”在“k”后面的概率。

# 它是如何工作的

从语料库中，`generate_gibberish_model`将读取每个句子，并计算两个字母之间的出现次数。

```
'this is not gibberish''t' -> 'h'
'h' -> 'i'
'i' -> 's'
's' -> ' '
' ' -> 'i'
...
'b' -> 'e'
'e' -> 'r'
'r' -> 'i'
'i' -> 's'
's' -> 'h'
```

这将生成两个字母之间的出现次数。这个函数的字典输出将在函数中使用，以计算一个字符串是否是乱码。其中较高的概率表示字符串是可能的，而较低的概率表示字符的组合没有意义并且是乱码。

方法是遍历一个字符串，检索两个字母中每一个出现的概率，然后将所有的概率相加得到一个概率值。如代码所示，使用对数概率而不是直接概率，以避免长文本中可能出现的数字下溢问题。

这个方法将返回一个概率值，不管这个令牌是不是乱码。因此，需要声明一个阈值来确定在什么概率值下我们可以确信该令牌是乱码。在上面的代码中，我使用了`0.0188`。任何值都可以使用，但是让我解释一下我是如何得到这个值的。

我有一些好的文本和乱码文本的例子。然后，我对所有好的和乱码的文本使用了乱码检测功能，发现了一个介于被视为非乱码的最小阈值和被视为乱码的最大阈值之间的值。而对我来说，这个值就是`0.0188`。

# 结论

这些函数的逻辑非常简单，但是在处理文本时会产生很好的性能。我希望这些功能能够像帮助我一样帮助别人，并帮助我成为一名更好的数据科学家。如果你们中的任何人遇到了任何有趣的文本处理方法/功能/页面，请在下面的评论中留下它们！感谢阅读！