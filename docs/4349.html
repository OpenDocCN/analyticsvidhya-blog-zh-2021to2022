<html>
<head>
<title>An Overview of Topic Modeling with NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理的主题建模综述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-overview-of-topic-modeling-with-nlp-17d3bf3e3624?source=collection_archive---------1-----------------------#2021-09-27">https://medium.com/analytics-vidhya/an-overview-of-topic-modeling-with-nlp-17d3bf3e3624?source=collection_archive---------1-----------------------#2021-09-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="7c53" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">学习使用奇异值分解和NMF在文本语料库中查找主题</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/f6632906c784dacae83423865a92a61c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YYLZF-1fHcZ657xq"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://unsplash.com/@edurnetx?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Edurne Tx </a>在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="c5c1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">自然语言处理(NLP)是计算机理解人类语言的能力。NLP是一个广阔的领域，包含各种任务，包括<em class="kk">词性标注、命名实体识别、问答、语音识别、文本到语音、语言建模、翻译、语音到文本和主题建模</em>。主题建模是对文本集合中内容的课程级分析。话题是话语的主题或主题，其中话题被表示为词的分布，而文档被假设为话题的混合。不同的主题建模方法是可用的，包括概率潜在语义分析(PLSA)、潜在狄利克雷分配(LDA)、奇异值分解(SVD)和非矩阵分解(NMF)。</p><p id="3cf2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了实际执行任务，我们将使用来自sklearn库的<a class="ae jn" href="https://www.kaggle.com/crawford/20-newsgroups" rel="noopener ugc nofollow" target="_blank">20个新闻组</a>数据集。我们将使用奇异值分解和NMF进行我们的实验，因为LDA和PLSA还有其他很好的资源。要获得完整的代码，请访问GitHub <a class="ae jn" href="https://github.com/Adeelzafar/NLP-Course/blob/main/NLP_Lab_1_Topic_Modeling.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><h1 id="7874" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">让我们深入研究代码</h1><pre class="iy iz ja jb fd ld le lf lg aw lh bi"><span id="abc6" class="li km hi le b fi lj lk l ll lm">import numpy as np<br/>from sklearn.datasets import fetch_20newsgroups<br/>from sklearn import decomposition<br/>from scipy import linalg<br/>import matplotlib.pyplot as plt<br/>from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer</span></pre><p id="af77" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们将使用类别的样本子集来测试我们的模型。该数据集包括20个主题的18，000篇新闻组帖子</p><pre class="iy iz ja jb fd ld le lf lg aw lh bi"><span id="f98f" class="li km hi le b fi lj lk l ll lm">categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']<br/>remove = ('headers', 'footers', 'quotes')<br/>newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)<br/>newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)</span><span id="7748" class="li km hi le b fi ln lk l ll lm">newsgroups_train.filenames.shape, newsgroups_train.target.shape</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lo"><img src="../Images/d54fe8153e1da96dde749e4290aebc29.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*hcc6axIBcvYZrM5c9ASK_Q.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">文件名和目标的形状</figcaption></figure><p id="a698" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们看一条新闻</p><pre class="iy iz ja jb fd ld le lf lg aw lh bi"><span id="2a4d" class="li km hi le b fi lj lk l ll lm">print("\n".join(newsgroups_train.data[:3]))</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/55e759f4dd1ffc97cf5749593d70bc90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TDoQd9LtKqVxeNMR_1uFKw.png"/></div></div></figure><p id="55e7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">那么它看起来像什么，让我们看看</strong></p><pre class="iy iz ja jb fd ld le lf lg aw lh bi"><span id="f695" class="li km hi le b fi lj lk l ll lm">np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]]</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lq"><img src="../Images/026c6c742cc021348836d6372f8ca342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iCSqOjWNGOpCKXP71lr6Uw.png"/></div></div></figure><h1 id="01da" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">数据处理</h1><p id="9d3d" class="pw-post-body-paragraph jo jp hi jq b jr lr ij jt ju ls im jw jx lt jz ka kb lu kd ke kf lv kh ki kj hb bi translated">对于数据处理，我们将使用sklearn的<em class="kk">计数矢量器</em>方法。它将为我们提取所有字数。2034是总发帖量，26576是字数。</p><pre class="iy iz ja jb fd ld le lf lg aw lh bi"><span id="0797" class="li km hi le b fi lj lk l ll lm">vectorizer = CountVectorizer(stop_words='english')<br/>vectors = vectorizer.fit_transform(newsgroups_train.data).todense()<br/>vectors.shape</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lw"><img src="../Images/b40d3d404c0442db76897a16d1a722cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*VC3zTd_wYvAh6ifSZPuf7A.png"/></div></figure><p id="be4c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们来看看词汇或词组</p><pre class="iy iz ja jb fd ld le lf lg aw lh bi"><span id="8543" class="li km hi le b fi lj lk l ll lm">vocab = np.array(vectorizer.get_feature_names())<br/>vocab[7000:7020]</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/f38fecf37d625a8bf18af3bfcd31ae01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_T0p2lTiOJE_bXtNP5P1fw.png"/></div></div></figure><h1 id="706e" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">奇异值分解</h1><p id="7843" class="pw-post-body-paragraph jo jp hi jq b jr lr ij jt ju ls im jw jx lt jz ka kb lu kd ke kf lv kh ki kj hb bi translated">SVD算法将一个矩阵分解成一个具有<strong class="jq hj">正交列</strong>的矩阵和一个具有<strong class="jq hj">正交行</strong>的矩阵(以及一个对角矩阵，其中包含每个因子的<strong class="jq hj">相对重要性</strong>)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/2d2d189a989697880c9580da3d0893ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/0*IYfnoEqlzKKnV1QL.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">参考:<a class="ae jn" href="https://research.fb.com/fast-randomized-svd/" rel="noopener ugc nofollow" target="_blank">https://research.fb.com/fast-randomized-svd/</a></figcaption></figure><p id="3171" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">SVD广泛用于线性代数，尤其是数据科学，包括语义分析、协同过滤/推荐和数据压缩。潜在语义分析(LSA)使用奇异值分解。你有时会听到主题建模被称为LSA。我们将使用sklearn的SVD实现。</p><pre class="iy iz ja jb fd ld le lf lg aw lh bi"><span id="d87c" class="li km hi le b fi lj lk l ll lm">%time U, s, Vh = linalg.svd(vectors, full_matrices=False)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lz"><img src="../Images/2fad33de02d2c7fa3d7dd88afea70fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*cg7eMJkYIByloAE9mJYMuw.png"/></div></figure><p id="55b9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们看看结果矩阵的形状，并对主题进行建模。函数show_topics遍历所有的vocab，选择最大的值作为每个主题的顶部单词。</p><pre class="iy iz ja jb fd ld le lf lg aw lh bi"><span id="c219" class="li km hi le b fi lj lk l ll lm">print(U.shape, s.shape, Vh.shape)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ma"><img src="../Images/854df6f5e4986095ea89773076f58a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*ot5_KOGGbOS_rLzqqSKK6w.png"/></div></figure><pre class="iy iz ja jb fd ld le lf lg aw lh bi"><span id="bc77" class="li km hi le b fi lj lk l ll lm">num_top_words=8<br/><br/>def show_topics(a):<br/>    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-                     num_top_words-1:-1]]<br/>    topic_words = ([top_words(t) for t in a])<br/>    return [' '.join(t) for t in topic_words]<br/>#Show topics <br/>show_topics(Vh[:10])</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/f2ec497afb6e12ac934fa6cbb78e26e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A30RSeUGruC6zgIYJbROjA.png"/></div></div></figure><p id="1e4c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">第一个不映射到我们的任何主题，第二个映射到图形，下一个映射到宗教，等等。这些聚类显示了<strong class="jq hj">一种无监督算法</strong>的有用性——也就是说，我们实际上从未告诉算法我们的文档是如何分组的。现在让我们进入下一个主题建模技术:<em class="kk"> NMF。</em></p><h1 id="8133" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">非负矩阵分解(NMF)</h1><p id="ffae" class="pw-post-body-paragraph jo jp hi jq b jr lr ij jt ju ls im jw jx lt jz ka kb lu kd ke kf lv kh ki kj hb bi translated">NMF是一个非负数据集的因子分解:分解成非负矩阵。通常积极的因素会<strong class="jq hj">更容易解读</strong>(这也是NMF受欢迎背后的原因)。NMF是一种非精确因式分解，分解成一个瘦正矩阵和一个短正矩阵。NMF的应用包括:<a class="ae jn" href="http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html" rel="noopener ugc nofollow" target="_blank">人脸分解</a>、<a class="ae jn" href="http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/" rel="noopener ugc nofollow" target="_blank">协同过滤、</a>和<a class="ae jn" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2623306/" rel="noopener ugc nofollow" target="_blank">基因表达</a>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/d919fd7cbfeadba992247e1c030b2e3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wNz54Y2BD0wK3zDD.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图取自NMF 的<a class="ae jn" href="https://perso.telecom-paristech.fr/essid/teach/NMF_tutorial_ICME-2014.pdf" rel="noopener ugc nofollow" target="_blank">教程</a></figcaption></figure><p id="7620" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们将使用sklearn的NMF实现。</p><pre class="iy iz ja jb fd ld le lf lg aw lh bi"><span id="751f" class="li km hi le b fi lj lk l ll lm">m,n=vectors.shape<br/>d=5  # num topics<br/>clf = decomposition.NMF(n_components=d, random_state=1)<br/><br/>W1 = clf.fit_transform(vectors)<br/>H1 = clf.components_<br/>show_topics(H1)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mc"><img src="../Images/6e9e9956f1df91d96d538865cb757051.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*ihD1RCUvzGwllyJUqR6fFg.png"/></div></figure><p id="a94d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">NMF既快又容易使用。然而，创造NMF需要多年的研究和专业知识。</p><p id="c5e0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">下次见！快乐编码…</p><h1 id="89fb" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">学分:</h1><p id="368b" class="pw-post-body-paragraph jo jp hi jq b jr lr ij jt ju ls im jw jx lt jz ka kb lu kd ke kf lv kh ki kj hb bi translated">这篇博客的内容灵感来自Coursera课程<em class="kk">Python中的文本挖掘应用</em>和<em class="kk"> Fast-ai NLP课程。</em></p></div></div>    
</body>
</html>