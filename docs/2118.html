<html>
<head>
<title>Multi-armed bandit for the selection of the landing page</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多臂土匪为登陆页的选择</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multi-armed-bandit-for-the-selection-of-the-landing-page-d533fc2c4e71?source=collection_archive---------15-----------------------#2021-04-05">https://medium.com/analytics-vidhya/multi-armed-bandit-for-the-selection-of-the-landing-page-d533fc2c4e71?source=collection_archive---------15-----------------------#2021-04-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="0d79" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章最初出现在<em class="jc">cienciadedatos.net</em>，你可以在这里找到西班牙语<a class="ae jd" href="https://www.cienciadedatos.net/documentos/py28-k-armed-bandit-evaluar-versiones-web-python.html" rel="noopener ugc nofollow" target="_blank">的原始版本。</a></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/0048ccf552d0c79bd4764065576c5d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uozq3a7d6OW2GCTWT3bFmw.jpeg"/></div></div></figure><p id="465c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我们将探讨<strong class="ig hi">强化学习</strong>，更准确地说是多臂土匪如何让我们减少必要的时间来评估一个新网站<strong class="ig hi">版本是否更有效</strong>来增加被号召行动的客户数量。</p></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h1 id="52bb" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">商业背景</h1><p id="28cb" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">营销的一个关键方面是将潜在客户对我们网站的访问转化为对我们主页的<strong class="ig hi"> <em class="jc"> CTA </em> </strong>(行动号召)。</p><p id="9592" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是为什么一个有吸引力的网站青睐这个<em class="jc"> CTA </em>，无论是注册成为用户，<strong class="ig hi">保持滚动</strong>，还是下载白皮书，都是极其重要的原因之一。</p><p id="c103" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是，当我们想要部署一个新版本，但又不确定其效率时，会发生什么呢？在这些情况下，我们通常做的是<strong class="ig hi">将一部分流量</strong>导向旧版本，一部分导向新版本，直到建立了有意义的用户样本，并且可以进行适当的评估。根据策略的不同，这种<strong class="ig hi"> A/B测试</strong>可以是平衡的(50 / 50)、金丝雀部署(新网站的1%)等。</p><p id="ff62" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种方法的问题在于它是事后的。在两个版本都上线一段时间后，我们正在承受<strong class="ig hi">机会成本</strong>(同时保持效率最低的版本在线)，在当前竞争日益激烈的环境下，这是不可取的，甚至是不可接受的。</p></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h1 id="16b4" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated"><strong class="ak">利用强化学习</strong></h1><p id="40c5" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">在这种情况下，我们想要比较我们的欢迎页面的2个以上版本，强化学习允许我们加快决策过程并减少机会成本，逐渐倾向于我们的客户反应更好的版本，并逐渐减少指向那些效率较低和令人满意的替代方案的流量。</p><p id="caef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在培训期间，我们将不得不在选择那个时间点的最佳选项或<strong class="ig hi">开发</strong>，以及将流量引导到那个时间效率较低的版本或<strong class="ig hi">探索</strong>之间找到平衡，以不断学习更多内容。</p></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h1 id="5696" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated"><strong class="ak">多臂土匪</strong></h1><p id="a6f1" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">在强化学习中，代理通过<strong class="ig hi">与环境</strong>交互来生成自己的训练数据。代理人必须通过反复试验来了解其行为的后果，而不是从该行为中获得先验知识。</p><p id="1e4c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在多臂强盗的特殊情况下，我们有k种不同的选择或行动。在选择了一个动作之后，然后<strong class="ig hi">代理收到</strong>来自概率分布的数字奖励。这种奖励必须是固定的，取决于所选择的行动。我们的目标是在预定的时间步数内，例如1000步，最大化总回报。</p><p id="a0a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们的问题中，每个行动在被选择后都有一个平均预期回报，称为行动价值或q。对于我们的任何行动:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es la"><img src="../Images/c07cd187d8cd0f1daebc1d8563e20520.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*HqviG-2x5rYmgy9BlJR6jA.png"/></div></figure><p id="015d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们事先知道每个动作的<strong class="ig hi">真实值</strong>，那么多臂强盗问题就变得简单了，因为期望值最高的动作将被选择。我们假设我们不能确定地知道它，尽管我们可以<strong class="ig hi">估计</strong>这些<em class="jc">Qt(a)</em>在采取行动<em class="jc"> a </em>后在时间戳<em class="jc"> t </em>的预期回报，并且我们将试图使它们尽可能接近<em class="jc">q</em>∫(<em class="jc">a</em>)。</p><p id="87f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有关<em class="jc">多臂土匪</em>及其实现所需伪代码的更多详细信息，请参考<em class="jc">强化学习:简介，第25–36页</em>。</p><p id="24b1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在每个时间点，我们将在概率为<em class="jc"> p </em>的探索和概率为<em class="jc"> (1-p) </em>的利用之间进行选择，选择到该时间点为止具有<strong class="ig hi">最高估计</strong> q值的动作。在强化学习中，这种类型的智能体被称为<em class="jc">ε贪婪</em>。</p></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h1 id="65b4" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated"><strong class="ak">用例形式化</strong></h1><p id="cdb8" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">我们将考虑以下因素:</p><ul class=""><li id="3cee" class="lb lc hh ig b ih ii il im ip ld it le ix lf jb lg lh li lj bi translated">三个不同版本的网站要进行评估。当前版本(0)和替代版本(1)和(2)。</li><li id="ffc4" class="lb lc hh ig b ih lk il ll ip lm it ln ix lo jb lg lh li lj bi translated">我们将假设有100，000名访问者到达我们的网站，并要求被重定向到任何版本(使用负载平衡器)。</li><li id="ad96" class="lb lc hh ig b ih lk il ll ip lm it ln ix lo jb lg lh li lj bi translated">如果访问者进行了CTA，则交互是成功的。</li></ul><h2 id="2f58" class="lp jy hh bd jz lq lr ls kd lt lu lv kh ip lw lx kl it ly lz kp ix ma mb kt mc bi translated"><strong class="ak">将其框架化为强化学习问题</strong></h2><ul class=""><li id="1417" class="lb lc hh ig b ih kv il kw ip md it me ix mf jb lg lh li lj bi translated">这组动作的基数为3:旧版本、新版本1和新版本2。</li><li id="5893" class="lb lc hh ig b ih lk il ll ip lm it ln ix lo jb lg lh li lj bi translated">如果CTA完成，奖励为1，否则为0。</li></ul><h2 id="9c3b" class="lp jy hh bd jz lq lr ls kd lt lu lv kh ip lw lx kl it ly lz kp ix ma mb kt mc bi translated">实验</h2><ul class=""><li id="927c" class="lb lc hh ig b ih kv il kw ip md it me ix mf jb lg lh li lj bi translated"><strong class="ig hi">实验一:</strong>评估一个探索概率为15%的智能体。</li><li id="fa42" class="lb lc hh ig b ih lk il ll ip lm it ln ix lo jb lg lh li lj bi translated"><strong class="ig hi">实验二:</strong>评估0%-50%之间不同探索概率的智能体。</li></ul><h2 id="8cc0" class="lp jy hh bd jz lq lr ls kd lt lu lv kh ip lw lx kl it ly lz kp ix ma mb kt mc bi translated">其他考虑</h2><ul class=""><li id="112b" class="lb lc hh ig b ih kv il kw ip md it me ix mf jb lg lh li lj bi translated">我们目前的版本有50%的效率，并将被建模为0和1之间的均匀分布。</li><li id="ab2c" class="lb lc hh ig b ih lk il ll ip lm it ln ix lo jb lg lh li lj bi translated">新版本1的CTA效率为75%，新版本2为25%。</li></ul><p id="3910" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，我们实验的<strong class="ig hi">累积预期回报</strong>将是50.000，与当前版本一致。(100.000个访问者x 50%的CTA概率)，其上限为75.000。我们将评估哪种<strong class="ig hi">学习策略</strong>提供最高的累积回报。</p></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h1 id="8f87" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">履行</h1><h2 id="eeca" class="lp jy hh bd jz lq lr ls kd lt lu lv kh ip lw lx kl it ly lz kp ix ma mb kt mc bi translated">平均累积报酬的理论范围</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mg"><img src="../Images/89871df28a8b3a681eef80105bec6484.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*4-CssrCDoyd2pk6Zl2_6OQ.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">平均累积奖励(超过100，000名访客)</figcaption></figure><h2 id="3305" class="lp jy hh bd jz lq lr ls kd lt lu lv kh ip lw lx kl it ly lz kp ix ma mb kt mc bi translated">环境</h2><p id="1ef3" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">我们实现了一个简单的环境，在这个环境中，每个行动都以一定的概率<em class="jc"> pa </em>取得成功</p><p id="b408" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们验证了行动0 的<strong class="ig hi">预期回报约为时间步长的50%，行动1的75%，行动2的25%。</strong></p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ml mm l"/></div></figure><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ml mm l"/></div></figure><h2 id="9ec6" class="lp jy hh bd jz lq lr ls kd lt lu lv kh ip lw lx kl it ly lz kp ix ma mb kt mc bi translated">代理人</h2><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ml mm l"/></div></figure></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h1 id="8455" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">实验1:具有15%探索率的代理</h1><p id="7ea2" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">在第一个实验中，我们将在200次模拟100，000个客户到来的过程中评估代理的行为。探索的概率将为15%，最大理论回报为0.75，对应于进行<em class="jc"> CTA </em>的四分之三客户的<strong class="ig hi">转化率</strong>。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ml mm l"/></div></figure><h2 id="0cf7" class="lp jy hh bd jz lq lr ls kd lt lu lv kh ip lw lx kl it ly lz kp ix ma mb kt mc bi translated">平均累积报酬</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mn"><img src="../Images/b4c9b2c02b97eb3dd7a446204f8b49e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1rKYURsleAjtWI5IkfH98A.png"/></div></div></figure><h2 id="cd97" class="lp jy hh bd jz lq lr ls kd lt lu lv kh ip lw lx kl it ly lz kp ix ma mb kt mc bi translated">选定的操作</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mo"><img src="../Images/2039754f964d296481d6692777bc5a7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*QT1a7xwf_t0LMkrnyha0HQ.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">实验1贪婪代理的行动选择</figcaption></figure><p id="b574" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们观察到代理人很快选择了<strong class="ig hi">行动1</strong>，这是回报最高的行动，尽管达到了71%的比率，比理论最大值低了4个点。无论如何，新版本提供了比当前登录页面至少21%的改进，而机会成本只有4%。</p><p id="f0eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">还可以验证，对应于更有效的新版本的动作1被选择，其频率比其他两个动作高几个<strong class="ig hi">数量级</strong>。</p><p id="9b5c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，可以检查每个估计动作值是否接近真实值:</p><pre class="jf jg jh ji fd mp mq mr ms aw mt bi"><span id="33b0" class="lp jy hh mq b fi mu mv l mw mx">action 1:0.501<br/>action 2:0.750<br/>action 3:0.246</span></pre></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h1 id="84a5" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">实验2:勘探和开采率</h1><p id="cd41" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">我们现在将进行第二次实验，使用不同的勘探/开采比率来评估是否有可能缩小机会成本差距。</p><p id="2517" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将比较0%(纯贪婪)和50%之间的五个不同选项，这是一个<strong class="ig hi">二分决策</strong>，其中每个行动都是在完全随机或当前最佳行动之间进行抛硬币选择的结果。</p><p id="bb31" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">找到利用最佳行动或探索新行动之间的平衡至关重要，因为低探索率将系统地提供次优选择，而高价值将带来代理人不稳定的<strong class="ig hi">行为，不提供真正的商业价值。</strong></p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ml mm l"/></div></figure><h2 id="58c3" class="lp jy hh bd jz lq lr ls kd lt lu lv kh ip lw lx kl it ly lz kp ix ma mb kt mc bi translated">平均累积报酬</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es my"><img src="../Images/51ef8676e1740afd6998f44dd8bb47f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cRUrDBEO8ClPYQERcmOLjQ.png"/></div></div></figure><h2 id="c46c" class="lp jy hh bd jz lq lr ls kd lt lu lv kh ip lw lx kl it ly lz kp ix ma mb kt mc bi translated">选定的操作</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mz"><img src="../Images/940c94707282f427ee32760258abf336.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*O4RFkI5EE9EQvtlxLULf5A.png"/></div></figure><p id="9cf3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这个实验中，我们可以检查参数<em class="jc"> ϵ </em>的<strong class="ig hi">不同值</strong>如何影响代理的学习。</p><p id="7fd1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个贪婪的代理很快收敛到0.6并被卡住，而这个<strong class="ig hi">随机</strong>探索的更高值，范围从p 0.4到p 0.5，获得略微优越的性能，但总是低于0.65。</p><p id="b67e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc"> ϵ </em>的最佳值在1%到10%之间，前者获得最高的平均累积回报，后者上升<strong class="ig hi">更快</strong>但收敛到理论最大值以下0.72 . 3%。</p><p id="af41" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在动作直方图中，我们可以看到代理如何探索动作0和2，但很快选择动作1作为最佳选择。</p></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h1 id="d1a7" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">先验知识整合</h1><p id="c89c" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">到目前为止，我们已经从每个版本的预期转换率考虑了<strong class="ig hi">没有先前的经验</strong>。</p><p id="f31f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种特殊情况下，这并不完全正确，因为我们必须继续跟踪初始版本及其度量，并且这种先验知识可以被<strong class="ig hi">利用</strong>来初始化每个动作<em class="jc"> Q0(a)的值。</em></p><p id="6937" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将以这个实验结束，用0.5作为每个版本的预期转化率来<strong class="ig hi">初始化</strong>所有的动作。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ml mm l"/></div></figure><h2 id="bba9" class="lp jy hh bd jz lq lr ls kd lt lu lv kh ip lw lx kl it ly lz kp ix ma mb kt mc bi translated">具有先验知识的平均累积报酬</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es my"><img src="../Images/4ca451a3b0e9a34e01a57726c5123714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lo5v40MNzolEGhsHNPm6tQ.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">先验知识epsilon贪婪代理奖励</figcaption></figure><p id="ca95" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种情况下，我们可以评估使用先验知识的<strong class="ig hi">潜力</strong>，这不仅可以提高大多数策略的平均累积回报，还可以加快学习过程。</p><p id="7d70" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">比较在有先验知识和没有先验知识的情况下的最佳代理，验证了尽管收敛到相同的值，但是利用先验知识的代理做得更快<strong class="ig hi"/>。</p><h2 id="7fd9" class="lp jy hh bd jz lq lr ls kd lt lu lv kh ip lw lx kl it ly lz kp ix ma mb kt mc bi translated">有(无)先验知识的平均累积报酬</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es my"><img src="../Images/e702ed2f7d3b08e91fa1ff4f3b0bea50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k82gQaOVZaGaCRqXoEGTOA.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">无论事先是否知情，都是最佳代理</figcaption></figure></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h1 id="80b1" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">结论</h1><p id="32de" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">在本文中，我们探讨了强化学习如何帮助我们决定哪个登录页面更适合我们的<strong class="ig hi">客户漏斗</strong>，从用户那里收集在线反馈。</p><p id="ac56" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用<em class="jc">多臂大盗</em>，我们已经看到了正确平衡探索和利用具有<strong class="ig hi">最高估计</strong>值的动作以及利用先验知识来获得每个动作的更快更准确的<em class="jc">q</em>∫(<em class="jc">a</em>)的影响。</p></div></div>    
</body>
</html>