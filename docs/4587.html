<html>
<head>
<title>Optimizers — Momentum and Nesterov momentum algorithms (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化器——动量和内斯特罗夫动量算法(下)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/welcome-to-the-second-part-on-optimisers-where-we-will-be-discussing-momentum-and-nesterov-c2698d5590e?source=collection_archive---------3-----------------------#2021-11-22">https://medium.com/analytics-vidhya/welcome-to-the-second-part-on-optimisers-where-we-will-be-discussing-momentum-and-nesterov-c2698d5590e?source=collection_archive---------3-----------------------#2021-11-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/3f309eb7725fa5a5de21a374481e9679.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*cZF62SbHrfhQ595O80w3nw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae iq" href="https://www.youtube.com/watch?v=CKLwvuKWQjo" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=CKLwvuKWQjo</a></figcaption></figure><p id="89e1" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">欢迎来到乐观者的第二部分，我们将讨论动量和内斯特罗夫加速梯度。如果你想快速回顾一下普通梯度下降算法及其变体，请阅读第一部分中的内容。在本系列的第3部分，我将详细解释RMSprop和Adam。</p><p id="1e59" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">梯度下降使用梯度来更新权重，这有时会有噪声。在小批量梯度下降中，当我们根据给定批次中的数据更新权重时，更新方向会有一些变化。这将降低训练速度并延迟收敛。</p><p id="1271" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">势头:</strong></p><p id="8c8f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">动量帮助我们不走那些不会引导我们趋同的方向。在动量中，引入了可变速度‘v’和动量‘β’。β'称为动量，通常赋值为0.9。v’被初始化为零，并且在随后的迭代中，使用以下公式来计算它，</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jp"><img src="../Images/0ac2e19a80187e86a164d9c5febe898f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l24ymTOHvflHPWIu"/></div></div></figure><p id="eaab" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">如果学习率按(1 — β)缩放，则上述等式可以写成</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jy"><img src="../Images/1fc03190f43a5101d6769d50b07126e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JZJpvD7DBHMe2WFX"/></div></div></figure><p id="5bc3" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">基本上，我们计算w的导数的移动平均值，这有助于减少振荡。</p><p id="cfc0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">具有小批量梯度下降的动量的Python代码</p><pre class="jq jr js jt fd jz ka kb kc aw kd bi"><span id="7e71" class="ke kf hi ka b fi kg kh l ki kj">def minibatchsgd_momentum(X, y, lr, epochs, batch_size, momentum):<br/>    m, b = 0.5, 0.5 # initial parameters<br/>    log, mse = [], [] # lists to store learning process<br/>    v_m = 0<br/>    v_b = 0<br/>    for epoch in range(epochs):<br/>        total_len = len(X)<br/>        for i in range(0, total_len, batch_size):<br/>            Xs = X[i:i+batch_size]<br/>            ys = y[i:i+batch_size]            <br/>            N = len(Xs)<br/>            f = ys - (m*Xs + b)</span><span id="7af2" class="ke kf hi ka b fi kk kh l ki kj">            v_m = (lr * (-2 * Xs.dot(f).sum() / N)) + (momentum * v_m)<br/>            v_b = (lr * (-2 * f.sum() / N)) + (momentum*v_b)<br/>            m = m - v_m<br/>            b = b - v_b<br/>            log.append((m, b))<br/>            mse.append(mean_squared_error(y, (m*X + b)))        <br/>    return m, b, log, mse</span></pre><p id="746c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">对于动量= 0.9，lr = 0.05 *(1-动量)，历元= 20，batch_size = 10，历元与MSE的关系曲线如下所示</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es kl"><img src="../Images/0a414a66f87b7c6ca4deb5bb5afd5c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pPC1STDWblxmvNkV"/></div></div></figure><p id="4d57" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">内斯特罗夫动量</strong>:在动量中，我们使用<code class="du km kn ko ka b">momentum * velocity</code>在正确的方向上推动参数，其中速度是前一时间步的更新。在内斯特罗夫动量中，不是计算参数W的梯度，而是计算(W - β * V t-1)的梯度。</p><p id="bc63" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">内斯特罗夫动量的公式如下。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jy"><img src="../Images/b25fd2082375a97cb79bed447b982001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MsP1fZrTRe0MfJGK"/></div></div></figure><p id="dcd3" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">内斯特罗夫势头的Python代码</strong></p><pre class="jq jr js jt fd jz ka kb kc aw kd bi"><span id="296b" class="ke kf hi ka b fi kg kh l ki kj">def minibatchsgd_nesterovmomentum(X, y, lr, epochs, batch_size, momentum):<br/>    m, b = 0.5, 0.5 # initial parameters<br/>    log, mse = [], [] # lists to store learning process<br/>    v_m = 0<br/>    v_b = 0<br/>    for epoch in range(epochs):<br/>        total_len = len(X)<br/>        for i in range(0, total_len, batch_size):<br/>            Xs = X[i:i+batch_size]<br/>            ys = y[i:i+batch_size]            <br/>            N = len(Xs)</span><span id="5239" class="ke kf hi ka b fi kk kh l ki kj">            m = m - (momentum * v_m)<br/>            b = b - (momentum * v_b)<br/>            # Gradients are calculated for <br/>            f = ys - (m*Xs +b)<br/>            v_m = (lr * (-2 * Xs.dot(f).sum() / N)) + (momentum * v_m)<br/>            v_b = (lr * (-2 * f.sum() / N)) + (momentum*v_b)<br/>            m = m - v_m<br/>            b = b - v_b<br/>            log.append((m, b))<br/>            mse.append(mean_squared_error(y, (m*X + b)))        <br/>    return m, b, log, mse</span></pre><p id="8baa" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">动量= 0.9、lr = 0.05 *(1-动量)、历元= 10和batch_size = 10时，历元与MSE的关系曲线如下所示。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es kp"><img src="../Images/e15dec4852f45ac347dc13f32c65eaaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cbbQb-LTUfJ29B8C"/></div></div></figure><p id="c057" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">注意</strong>:由于我们的数据集只是一个用于从头开始实施的玩具数据集，因此在本例中，我们看不到用于训练的时期数量有非常显著的增加。</p><p id="c5c3" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">感谢您的阅读。在本系列的下一部分，我们将讨论RMSprop和Adam。该代码可在<a class="ae iq" href="https://github.com/bhuvanakundumani/optimizers.git" rel="noopener ugc nofollow" target="_blank">https://github.com/bhuvanakundumani/optimizers.git</a>获得</p><p id="39ae" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">参考文献:</strong></p><p id="4a86" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><a class="ae iq" href="https://ruder.io/optimizing-gradient-descent/" rel="noopener ugc nofollow" target="_blank">https://ruder.io/optimizing-gradient-descent/</a><br/><a class="ae iq" href="https://cs231n.github.io/neural-networks-3/#sgd" rel="noopener ugc nofollow" target="_blank">https://cs231n.github.io/neural-networks-3/#sgd</a><br/><a class="ae iq" href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d" rel="noopener" target="_blank">https://towardsdatascience . com/random-gradient-descent-with-momentum-a 84097641 a5d</a><br/><a class="ae iq" rel="noopener" href="/analytics-vidhya/momentum-a-simple-yet-efficient-optimizing-technique-ef76834e4423">https://medium . com/analytics-vid hya/momentum-a-simple-yet-efficient-efficient-ef76834 e 4423</a><br/><a class="ae iq" href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d" rel="noopener" target="_blank">https://towardsdatascience . com/random-gradient-descent-descent</a></p></div></div>    
</body>
</html>