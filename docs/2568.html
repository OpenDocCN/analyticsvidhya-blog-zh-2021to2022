<html>
<head>
<title>Feature Selection — Filter Method</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择—过滤方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-selection-filter-method-43f7369cd2a5?source=collection_archive---------3-----------------------#2021-05-03">https://medium.com/analytics-vidhya/feature-selection-filter-method-43f7369cd2a5?source=collection_archive---------3-----------------------#2021-05-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/9eda4f1aa505acda9da902caf2049121.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gyE4PfM6Vp6bw_rUgcD-Ng.jpeg"/></div></div></figure><div class=""/><p id="b0fb" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了方便地研究数据、建立模型并获得好的结果，对数据进行预处理是很重要的，而最好的方法之一就是特征选择。</p><h2 id="eee7" class="jn jo hs bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated">什么是特征选择？</h2><p id="902f" class="pw-post-body-paragraph ip iq hs ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated">选择和保留最重要特征的过程称为<strong class="ir ht">特征选择。</strong>它有助于降低模型的噪声和计算成本，有时还能提高这些模型的性能。</p><p id="d1b4" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">特征选择有三种方法，即:<br/>滤波法；<br/>包装方法；<br/>嵌入式方法。</p><h2 id="2d6e" class="jn jo hs bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated">过滤方法:</h2><p id="70d9" class="pw-post-body-paragraph ip iq hs ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated">这种方法通常用作预处理步骤。对于这种方法，根据各种统计测试或者基于单变量度量(例如<br/>方差)来选择特征；<br/>相关性；<br/>卡方；<br/>相互信息。</p><p id="8fc8" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">存在不同的相关性计算方法，下面提供了其中一些常用的方法。</p><p id="720e" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="kn">皮尔逊相关</em>:该方法用于计算连续变量之间的相关性。</p><p id="be3c" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="kn">线性判别分析:</em>度量连续和分类特征之间的相关性。</p><p id="460b" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="kn">卡方:</em>计算两个或更多分类特征之间的相关性。</p><p id="00f7" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="kn"> ANOVA: </em>除了有两个独立的分类特征和一个连续的相关特征之外，与线性判别分析相同。</p><p id="c36b" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir ht">优点:</strong></p><p id="971c" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">计算速度非常快；<br/>避免过拟合；<br/>不依赖机型，只依赖特性；<br/>基于不同的统计方法。</p><p id="0248" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir ht">缺点:</strong></p><p id="f289" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">不要删除多重共线性；<br/>有时可能选择失败；</p></div><div class="ab cl ko kp go kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ha hb hc hd he"><p id="b87b" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">滤波方法可以分为两组，即:单变量滤波方法和多变量滤波方法。</p><p id="f4da" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在单变量过滤方法中，根据特定标准，例如fisher评分、互信息和方差，独立地观察每个特征。与此同时，还有一个重要的缺点。这种方法可以选择冗余特征，因为关系的无知。</p><p id="0aa0" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">另一方面，多元滤波方法可以处理冗余，同时用于去除重复和相关的特征。</p><h2 id="ab27" class="jn jo hs bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated">移除常量功能:</h2><p id="e251" class="pw-post-body-paragraph ip iq hs ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated">常数是仅包含一个值的特征，因此它们对分类和建模没有影响。建议删除。</p><p id="cf93" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">要删除Python中的常量，首先，必须将给定数据集分为训练数据集和测试数据集，然后按如下方式删除常量:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="eee5" class="jn jo hs la b fi le lf l lg lh">constant_features = [var for var in X_train.columns if X_train[var].std() == 0] <br/>X_train.drop(labels=constant_features, axis=1, inplace=True)<br/>X_test.drop(labels=constant_features, axis=1, inplace=True) X_train.shape, X_test.shape</span></pre><h2 id="4f53" class="jn jo hs bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated">移除准常数特征:</h2><p id="0747" class="pw-post-body-paragraph ip iq hs ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated">准常数、几乎常数要素对于数据集的大部分具有相同的值。这些值在预测期间没有用，并且阈值的方差没有被适当地定义。然而，一般来说，99%的常数重复自己可以被删除。</p><p id="263d" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">要删除Python中的准常量，首先要导入库，并将数据集拆分为训练集和测试集。之后，开始定义函数以移除准常数，如下所示:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="bf12" class="jn jo hs la b fi le lf l lg lh"># Define the threshold as 0.01<br/>q_remover = VarianceThreshold(threshold=0.01)</span><span id="a0ce" class="jn jo hs la b fi li lf l lg lh"># Find the values with low variance<br/>q_remover.fit(X_train) <br/>sum(q_remover.get_support())</span><span id="96e0" class="jn jo hs la b fi li lf l lg lh"># Apply to datasets<br/>X_train = q_remover.transform(X_train)<br/>X_test = q_remover.transform(X_test)<br/>X_train.shape, X_test.shape</span></pre><h2 id="2c0d" class="jn jo hs bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated"><strong class="ak">删除重复特征:</strong></h2><p id="0507" class="pw-post-body-paragraph ip iq hs ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated">重复要素是指重复多次的要素。这些值对数据集没有影响，但会延迟训练时间。因此，建议移除重复的特征。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="3d30" class="jn jo hs la b fi le lf l lg lh">duplFeatures = []<br/>for i in range(0, len(X_train.columns)):<br/>oneCol = X_train.columns[i]<br/>for othCol in X_train.columns[i + 1:]:<br/>        if X_train[oneCol].equals(X_train[othCol]):<br/>            duplFeatures.append(othCol)<br/>X_train.drop(labels=duplFeatures, axis=1, inplace=True)<br/>X_test.drop(labels=duplFeatures, axis=1, inplace=True)</span></pre><h2 id="59e8" class="jn jo hs bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated">移除相关特征:</h2><p id="e35c" class="pw-post-body-paragraph ip iq hs ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated">统计学和数据科学中的一个重要术语是相关性。如果特征在线性空间中是接近的，那么特征是相关的。因此，相关特征是重要的，应该被观察到。然而，有时这些特性也会造成冗余。这就是为什么，最好是删除它们。</p><p id="62ef" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了移除重复的特征，应该导入适当的库，并且应该将数据集分成训练数据集和测试数据集。然后，下面的代码可以用来删除重复的特性。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="5b39" class="jn jo hs la b fi le lf l lg lh">correl_Feat = set() <br/>correl_matrix = dataset.corr()<br/>    <br/>for i in range(len(corr_matrix.columns)):<br/>   for j in range(i):<br/>       if abs(correl_matrix.iloc[i, j]) &gt; 0.8:<br/>       colName = correl_matrix.columns[i]  <br/>       correl_Feat.add(colname)<br/>X_train.drop(labels=correl_Feat, axis=1, inplace=True)<br/>X_test.drop(labels=correl_Feat, axis=1, inplace=True)</span></pre><p id="298b" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上面的代码中，0.8是一个阈值，可以用任何方式定义。</p><p id="7e06" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">特征选择是机器学习中一个非常重要的术语，因为它在任何ML模型的性能和训练中起着巨大的作用。本文介绍了特征选择的过滤方法，并给出了一些实例。</p><p id="6548" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">完整的代码可以在我的<a class="ae lj" href="https://github.com/zaurrasulov/Data-Science---Python/blob/main/FilterMethod.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>个人资料中看到。</p></div></div>    
</body>
</html>