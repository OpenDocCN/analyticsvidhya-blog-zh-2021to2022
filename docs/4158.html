<html>
<head>
<title>ML Model — Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML模型—线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-8c4beb0e610b?source=collection_archive---------2-----------------------#2021-09-03">https://medium.com/analytics-vidhya/linear-regression-8c4beb0e610b?source=collection_archive---------2-----------------------#2021-09-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/1456a6a1018fb62fa287a7a5a82d5be2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RWgdkg91DkJgx7eYQF12zw.png"/></div></div></figure><div class=""/><p id="e17a" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">数据科学和所有相关领域的复杂结构是它令人愉快的原因。数据科学和机器学习中的一个主要术语是针对一系列问题训练ML模型。在这个博客中，线性回归，一个最流行和最简单的训练模型，将被描述。</p><p id="06e3" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">线性回归是一种对因变量和自变量之间的关系进行建模的线性方法。两种类型的线性回归是可行的，即:简单和多重。简单线性回归是一种训练模型，其中仅存在一个独立变量，而在多个模型中使用两个或多个独立变量。</p><h2 id="2c5b" class="jn jo hs bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated">数学定义:</h2><p id="ad37" class="pw-post-body-paragraph ip iq hs ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated">线性回归在寻找几个关系之间的关系时很有用，同时也用于机器学习中的预测。与其他模型相比，线性回归用于统计关系，而不是确定性的。</p><p id="3846" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">线性回归的数学定义如下:</p><figure class="ko kp kq kr fd hj er es paragraph-image"><div class="er es kn"><img src="../Images/6f63347aa638455312cbf0751b69badf.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*-gPff5g8BziCqJ9b-OEuiQ.jpeg"/></div></figure><p id="03db" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">y(hat)是预测值；<br/> n是特征的数量；<br/> x_i是第I个特征；<br/>theta _ j是第j个mod参数。而且，这些系数被称为权重；【thetha _ 0是偏置项。</p><p id="dbcd" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">以矢量化的形式重写上面的等式，</p><figure class="ko kp kq kr fd hj er es paragraph-image"><div class="er es ks"><img src="../Images/2de07a3bd194e6c764624ac713b7c558.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*5CK05X_fIGjoVSJ7Em-dSA.jpeg"/></div></figure><p id="2bd8" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">theta是参数向量，包括theta _ 0；<br/> X是特征向量；<br/>乘法就是点积。</p><p id="9fba" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先，值得一提的是，训练模型意味着设置适当的参数，以使模型非常适合。这就是为什么，好(或差)的模型性能应该进行研究。均方根误差(RMSE)函数是检查回归模型性能的常用函数。</p><figure class="ko kp kq kr fd hj er es paragraph-image"><div class="er es kt"><img src="../Images/f8aa9513f92830a8207e5465f5a73c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*mloyRJ0LWLcUBrRG7NfvYA.jpeg"/></div></figure><p id="8a01" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">根据RMSE的定义，需要找到使RMSE值最小的那个值。然而，为了易于实现，均方差(MSE)是一种寻找性能的替代方法。MSE将导致与RMSE相同的解决方案，因为最小化函数的值也将最小化它的根形式。</p><p id="e260" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">线性回归的MSE定义如下。</p><figure class="ko kp kq kr fd hj er es paragraph-image"><div class="er es ku"><img src="../Images/c48512ccafd3171eb60cc5820c1c0576.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*BM8xpWD09zXmQZmfh7UUmg.jpeg"/></div></figure><h2 id="ab0f" class="jn jo hs bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated">正态方程</h2><p id="b079" class="pw-post-body-paragraph ip iq hs ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated">寻找使成本函数最小化的θ值的另一种方法是正规方程，它是封闭形式的解。</p><p id="0f1e" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该函数的数学定义是:</p><figure class="ko kp kq kr fd hj er es paragraph-image"><div class="er es kv"><img src="../Images/964a9e4e033bfb969cf6dd1845d2bdb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*6MyZgCHVoGd0Q-GEMmtgcA.jpeg"/></div></figure><p id="10b9" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">θ(hat)是使成本函数最小化的θ的值<br/> y是目标值的向量。</p><h2 id="e202" class="jn jo hs bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated">Python实现:</h2><p id="092b" class="pw-post-body-paragraph ip iq hs ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated">在这一部分中，将描述Python中线性回归的实现。有几种方法可以找到这些问题的解决方案，但是这里将展示Scikit库。</p><pre class="ko kp kq kr fd kw kx ky kz aw la bi"><span id="fbf5" class="jn jo hs kx b fi lb lc l ld le">#defining linear Regression model and fitting it to train sets. <br/>lr = linear_model.LinearRegression()<br/>lr.fit(X_train,y_train)<br/>predict = lr.predict(X_test)</span><span id="d719" class="jn jo hs kx b fi lf lc l ld le">#checking the coefficient values<br/>print('The intercept : ', lr.intercept_)<br/>print('The coefficient: ', lr.coef_)</span><span id="ffe2" class="jn jo hs kx b fi lf lc l ld le">#checking the MSE value<br/>print('Mean Square Error', metrics.mean_squared_error(y_test, predict))</span></pre><p id="4bf2" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然而，在正规方程的情况下，代码是手动编写的。</p><pre class="ko kp kq kr fd kw kx ky kz aw la bi"><span id="7fe6" class="jn jo hs kx b fi lb lc l ld le">#finding the value of theta(hat)<br/>def theta_calc(df1, df2):<br/>    n_data = df1.shape[0]<br/>    bias_term = np.ones((n_data, 1))<br/>    df1_bias = np.append(bias_term, df1, axis=1)<br/>    theta_1 = np.linalg.inv(np.dot(df1_bias.T, df1_bias))<br/>    theta_2 = np.dot(theta_1, df1_bias.T)<br/>    theta = np.dot(theta_2, df2)<br/>    return theta</span></pre><p id="67b5" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在提到的两种方法之间通常观察到微小的差异。</p><p id="57ab" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">线性回归是最流行的机器学习模型之一，也是统计学和机器学习中最重要的术语之一。本文描述了该模型的两种不同方法。然而，也有不同的方法，这将在后面描述。</p><p id="6c96" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">全部代码都分享在我的<a class="ae lg" href="https://github.com/zaurrasulov/ML-modelling/blob/main/LinearRegression.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>个人资料上。</p></div></div>    
</body>
</html>