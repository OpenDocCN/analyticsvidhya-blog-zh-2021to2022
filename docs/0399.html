<html>
<head>
<title>Does text similarity metrics help in text classification problems?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本相似性度量对文本分类问题有帮助吗？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-similarity-approach-for-text-classification-f25e284d3e92?source=collection_archive---------7-----------------------#2021-01-15">https://medium.com/analytics-vidhya/text-similarity-approach-for-text-classification-f25e284d3e92?source=collection_archive---------7-----------------------#2021-01-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/81fa2c80691c277880a5183fabbf47dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eSIpvYBGiHMNvIzRystcZw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片由来自<a class="ae iu" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1204029" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae iu" href="https://pixabay.com/users/luboshouska-198496/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1204029" rel="noopener ugc nofollow" target="_blank"> Lubos Houska </a>拍摄</figcaption></figure><p id="acea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">曾经有一则新闻传遍网络，“消毒剂无助于对抗冠状病毒”。这条新闻在社交媒体上被分享，理由是因为冠状病毒是一种病毒，抗菌洗手液不会做任何事情。据FactCheck.org称，2020年3月1日，当一位自称“科学家”的人在推特上发布他们的理论时，谣言就开始了。然而，根据CDC的说法，如果没有肥皂和水，使用含至少60%酒精的洗手液是一种合适的替代品。</p><p id="493f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还有一个故事流传，“联邦政府已经下令工作场所关闭两周，以避免冠状病毒的传播”。然而，没有作出这样的授权。根据FactCheck.org的说法，这些被分享的帖子原本是一个笑话，当用户点击链接“阅读更多”时，会有一个笑点等着他们其他的帖子也把“政府”的提法换成了密歇根、佛罗里达、阿拉巴马和其他州的说法。该国一些地区已经宣布进入紧急状态，但并没有扩大到迫使工作场所关闭。这两个是有代表性的故事。我们每天都能在社交媒体平台上看到这类故事。因为这些故事制造了轰动效应，人们倾向于认同它并开始分享它。他们没有意识到的一点是，这些故事是假的。</p><p id="adf2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">尽管人们很容易接触到真实的事实和事实核查人员，但他们还是会相信假新闻。假新闻之所以成为一个问题，部分原因是人们确实会听信这些故事，而向他们展示事实无助于纠正这个问题。这是由于人类思维的一个特征，叫做认知偏差。认知偏差是推理、记忆或评估某件事情时的差距，会导致错误的结论。它们是普遍的。每个人都有。认知偏差影响我们使用信息的方式。四种类型的认知偏差与假新闻及其对社会的影响特别相关:首先，我们倾向于根据标题和标签采取行动，而不阅读与它们相关的文章。其次，社交媒体传达的信号会影响我们对信息受欢迎程度的感觉，从而让我们更容易接受。第三，假新闻利用了最常见的政治心理捷径:党派偏见。第四，有一种奇怪的趋势，虚假信息会一直存在，即使在它被纠正之后。</p><p id="cab2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">自然语言理解的主要问题之一是文本中语言的复杂性。因为这些文章都有不同的新闻写作方法，我们得出结论，在如此多样化的语言表达中应用相似性度量和概括结果不是一个好主意。因此，我们决定尽可能简化语言表示。项目的第四阶段集中于简化真实的新闻文章。在这个阶段，我们还试图通过改变文本数据中的一些词语，从那些真实的新闻中生成假新闻。前三篇，我们只是简单的把一个正面的词反过来变成一个负面的词，把一个真实的新闻转换成假新闻。例如，第三篇文章指出，“在2020年5月新冠肺炎冠状病毒疾病疫情期间，美国联邦和州政府正在努力解决的主要问题之一是保持社交距离和商业关闭限制以保护生命的权衡，以及对个人、企业和整个国家的持续(可能是永久)经济损害的权衡。”这个故事变成了一个虚假的故事，只不过是把下划线的单词改成了“我们没有摔跤”。</p><p id="021c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">同样，在最后四篇文章中，我们试图改变文章的结构，将其转换为一个虚假的故事，但保留了文章的上下文。例如，第六篇文章谈到奥巴马政府向中国科学院武汉病毒研究所提供了370万美元的拨款，冠状病毒的起源就是从那里推测的。我们添加了我们自己的额外信息，即奥巴马总统直接指示释放病毒以破坏唐纳德·特朗普总统任期，这是错误的。转换后的虚假故事如下。“2020年4月，有报道称，美国国家卫生研究院(NIH)在奥巴马总统在任期间直接下令向中国科学院武汉病毒研究所提供了370万美元的拨款。报告还指出，奥巴马总统明确指示泄露一种致命病毒，给即将上任的唐纳德·特朗普总统造成问题。这些报道支持了新型冠状病毒从这个病毒学实验室逃脱的传言。”</p><p id="5884" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我用python编程语言和google colab作为我们的工作空间。我们主要研究了三种相似性度量:Jaccardian相似性、余弦相似性和软余弦相似性。先详细讨论一下。</p><h2 id="6a5a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">Jaccardian相似性</h2><p id="21a0" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">Jaccard相似性(系数)，一个由Paul Jaccard创造的术语，用于度量集合之间的相似性。Jaccardian相似性可以针对两种表示来计算:基于集合的表示和基于向量的表示。它被定义为交集的大小除以两个数据的并集的大小。它比较两个数据集的成员，以查看哪些成员是共享的，哪些是不同的。这是对两组数据相似性的度量，范围从0%到100%。百分比越高，两个种群越相似。虽然它很容易解释，但它对小样本非常敏感，可能会给出错误的结果，特别是对于非常小的样本或缺失观测值的数据集。</p><p id="0bd3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看一个基于集合表示的Jaccard相似性的例子。考虑以下两句话:</p><p id="c44f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">哈里是一名学生。</p><p id="8e62" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上述两个集合Jaccard相似性的数学表示为:</p><p id="fb15" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，| A ⋂ B | = | {是，a，学生} | = 3和| A U B | = | {哈利，是，a，学生，他，学校，去} | = 7。</p><p id="8917" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，J(A，B)= | A ⋂ B | / | A U B | = 3/7 = 0.43</p><p id="9394" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们从向量表示的角度来分析这个例子</p><p id="60ee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑两个集合x={x1，x2，…，xn}和y={y1，y2，…..，yn}在向量空间表示中。那么它们之间的Jaccard系数为:</p><p id="95ef" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们考虑上面的两个句子A和b。在基于向量的表示中，我们在单词包(BoW)模型中表示这两个句子。在单词袋方法中，完整数据中的所有单词在列中表示，句子或文档在行中表示。现在，我们开始计算每个单词在所有文档/数据集中的出现次数，并填充这些值。下表显示了我们的两个例句的单词包表示。</p><p id="9b07" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据上面的公式，分子是由每一列中的最小值之和计算出来的。这导致分子= (0+1+0+0+1+0) = 3。类似地，分母通过每列中最大值的总和来计算。这导致分母= (1+1+1+1+1+1+1+ = 7，因此Jaccard相似性=分子/分母= 3/7 = 0.43</p><p id="f6e2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Jaccard相似性只对少量数据有用。考虑一个包含数千个文档的数据集。为如此大量的数据表示一个单词包模型可能不切实际。此外，当我们有一个相似的句子，但有一组不同的单词时，Jaccard相似性就失效了。考虑以下句子:</p><p id="dd88" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">A = {俄成功测试新冠肺炎疫苗} B = {冠状病毒解毒剂人体试验现已完成}</p><p id="410b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这两个句子推断出相似的意思，尽管B句中的信息可能不完整。但是如果我们分析这些句子之间的相似度，雅克第相似度将会是0，因为句子A和b之间没有共同的单词。</p><p id="6cff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">| A ⋂ B | = | { } | = 0因此Jaccard的系数(a，B ) = 0。</p><h2 id="34b9" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第一条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="d2e9" class="jt ju hi ky b fi lc ld l le lf">from __future__ import division<br/>import string<br/>import math<br/><br/>tokenize = lambda doc: doc.lower().split(" ")</span><span id="f7d1" class="jt ju hi ky b fi lg ld l le lf">document_1_0 = "Circulating on social networks a video that shows an excerpt from a Spanish television show, supposedly issued December 24, 2019, in which it appears a woman (who claims to be psychic) to make predictions. In this video, the woman describes a set of events that have been interpreted as a detailed forecast of Covid-19 pandemic that has hit the world. It is, however, a fake video, at least as regards the date of issue. The video has been being disseminated on the Internet with a date and not tampered with the real."<br/>document_1_1 = "Circulating on social networks a video that shows an excerpt from a Spanish television show, supposedly issued December 24, 2019, in which it is not appearing  a woman (who claims to be psychic) to make predictions. In this video, the woman does not describe a set of events that have not  been interpreted as a detailed forecast of Covid-19 pandemic that has not hit the world record. It is, however, a fake video, at least as regards the date of issue. The video has not been disseminated on the Internet with a date and not tampered with the real news."<br/><br/>document_2_0="On Feb. 28, 2020, the website PJ Media published an article claiming that U.S. President Barack Obama had waited until millions were infected and thousands were dead from swine flu, the H1N1 virus, before declaring a public health emergency in 2009. The article, which was presented as a    fact check,    got several simple details wrong."<br/>document_2_1="On Feb. 28, 2020, the website PJ Media published an article claiming that U.S. President Barack Obama has not waited until millions were not infected and thousands were not dead from swine flu, the H1N1 virus, before declaring a public health emergency in 2009. The article, which was not  presented as a    fact check,    got several simple details True."<br/><br/>document_3_0="One of the primary issues that the U.S. federal and state governments were wrestling with during the COVID-19 coronavirus disease pandemic in May 2020 was the trade-off of keeping social distancing and business closure restrictions in place to protect lives, versus the trade-off of ongoing (and possibly permanent) economic harm to individuals, businesses, and the country as a whole."<br/>document_3_1="One of the primary issues that the U.S. federal and state governments were not wrestling with during the COVID-19 coronavirus disease pandemic in May 2020 was not the trade-off of keeping social distancing and business closure restrictions in place to protect lives, versus the trade-off of ongoing (and possibly permanent) economic harm to individuals, businesses, and the country as a whole."<br/><br/>document_4_0="A number of countries have agreed to speed up the development of tests, drugs and vaccines against COVID-19 and share them globally. Countries like France, Germany and South Africa joined the virtual conference to launch a fight against  COVID-19. We are facing a common threat which we can only defeat with a common approach, WHO Director General Tedros Adhanom Ghebreyesus said pointing out that the tools should be equally available to all. European Commission President Ursula von der Leyen said that the objective would be to raise 8.1 billion dollars to speed up prevention, diagnostics and treatment. Despite the support from many other countries, the USA was not present in the virtual conference after the country discontinued regular funds to WHO claiming it was helping China to hide the information about the origin of the virus."<br/>document_4_1="The USA has been on the frontline to assist WHO in the fight against COVID-19. While many other countries like France, Germany and South Africa have not shown interest in this initiative, the USA has supported the objective to raise 50 billion dollars to speed up prevention. The WHO director  Tedros Adhanom Ghebreyesus stated that the treatment would be only provided to the country who contributes more to speed up diagnostics and prevention of the virus. European Commission President Ursula von der Leyen has strongly disagreed to the WHO director's statement saying that cure should be available to all equally. US officials have not responded in this matter."<br/><br/>document_5_0="The government of China has donated medical equipment to Rwanda for fighting the COVID-19 pandemic. The equipment including N95 face masks, surgical masks, protective clothing, infrared thermometers, and surgical gloves were handed over to the Ministry of Health. Ambassador of China to Rwanda said that the donation represents China’s determination on assisting other countries during this pandemic."<br/>document_5_1="The government of China has denied Rwanda’s request to assist the country with medical equipment to fight with the COVID-19 pandemic. The request was made during a diplomatic meeting between the Health Minister of Rwanda and Ambassador of China to Rwanda. The ambassador in response said that demand for medical equipment is huge in China and it cannot assist any other countries in the current situation."<br/><br/>document_6_0="In April 2020, reports started to circulate  that the National Institutes of Health (NIH) had provided the Wuhan Institute of Virology a $3.7 million grant in 2015, while former U.S. President Obama was still in office. These reports were often accompanied by the evidence-free suggestion that the novel coronavirus that causes COVID-19 had escaped from this lab."<br/>document_6_1="In April 2020, there were reports that the National Institute of Health(NIH) provided a grant of 3.7 million dollars to Wuhan Institute of Virology under direct order of President Barack Obama while he was in office. The reports also state that President Obama gave clear instructions to leak a deadly virus to cause problems for the upcoming president Donald Trump. These reports support rumors that the novel coronavirus has escaped from this virology lab."<br/><br/>document_7_0="The Food and Drug Administration issued a warning about the use of hydroxychloroquine, an antimalarial drug, after it became aware of heart risks. The drug has widely been encouraged by United States President Donald Trump as a potential treatment. FDA said that they became aware of reports of serious heart rhythm problems in patients with COVID-19 treated with hydroxychloroquine often in combination with azithromycin."<br/>document_7_1="The Food and Drug Administration has issued a press release encouraging health officials to use hydroxychloroquine, an antimalarial drug, as potential treatment of COVID-19. The press release states that the clinical trial is now complete and use of the drug cured COVID-19 patients of all age groups within a week. The drug was also encouraged by United States President Donald Trump after he claimed in a press conference that he was taking hydroxychloroquine as a preventive measure against COVID-19. This press release has now encouraged millions of health workers in the US and all around the world to go forward with the use of the drug as a cure against COVID-19."</span><span id="19db" class="jt ju hi ky b fi lg ld l le lf">all_documents = [document_1_0, document_1_1]<br/><br/>tokenized_documents = [tokenize(d) for d in all_documents] # tokenized docs<br/>all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])</span><span id="65d4" class="jt ju hi ky b fi lg ld l le lf">all_tokens_set</span><span id="18fe" class="jt ju hi ky b fi lg ld l le lf">{'',<br/> '(who',<br/> '2019,',<br/> '24,',<br/> 'a',<br/> 'an',<br/> 'and',<br/> 'appearing',<br/> 'appears',<br/> 'as',<br/> 'at',<br/> 'be',<br/> 'been',<br/> 'being',<br/> 'circulating',<br/> 'claims',<br/> 'covid-19',<br/> 'date',<br/> 'december',<br/> 'describe',<br/> 'describes',<br/> 'detailed',<br/> 'disseminated',<br/> 'does',<br/> 'events',<br/> 'excerpt',<br/> 'fake',<br/> 'forecast',<br/> 'from',<br/> 'has',<br/> 'have',<br/> 'hit',<br/> 'however,',<br/> 'in',<br/> 'internet',<br/> 'interpreted',<br/> 'is',<br/> 'is,',<br/> 'issue.',<br/> 'issued',<br/> 'it',<br/> 'least',<br/> 'make',<br/> 'networks',<br/> 'news.',<br/> 'not',<br/> 'of',<br/> 'on',<br/> 'pandemic',<br/> 'predictions.',<br/> 'psychic)',<br/> 'real',<br/> 'real.',<br/> 'record.',<br/> 'regards',<br/> 'set',<br/> 'show,',<br/> 'shows',<br/> 'social',<br/> 'spanish',<br/> 'supposedly',<br/> 'tampered',<br/> 'television',<br/> 'that',<br/> 'the',<br/> 'this',<br/> 'to',<br/> 'video',<br/> 'video,',<br/> 'which',<br/> 'with',<br/> 'woman',<br/> 'world',<br/> 'world.'}</span><span id="5483" class="jt ju hi ky b fi lg ld l le lf">def jaccard_similarity(query, document):<br/>    intersection = set(query).intersection(set(document))<br/>    union = set(query).union(set(document))<br/>    return len(intersection)/len(union)<br/><br/>jaccard_similarity(tokenized_documents[0],tokenized_documents[1])</span><span id="10b2" class="jt ju hi ky b fi lg ld l le lf">0.8108108108108109</span><span id="de10" class="jt ju hi ky b fi lg ld l le lf">all_documents = [document_1_0, document_2_0]<br/><br/>tokenized_documents = [tokenize(d) for d in all_documents] # tokenized docs<br/>all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])<br/><br/>jaccard_similarity(tokenized_documents[0],tokenized_documents[1])</span><span id="f602" class="jt ju hi ky b fi lg ld l le lf">0.09345794392523364</span></pre><p id="f1fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过简单地将真实新闻集中的正面词转换为负面词，我们不能通过雅克第相似性来区分它们</p><h2 id="2aa7" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第二条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="90b0" class="jt ju hi ky b fi lc ld l le lf">all_documents = [document_2_0, document_2_1]<br/><br/>tokenized_documents = [tokenize(d) for d in all_documents] # tokenized docs<br/>all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])<br/><br/>jaccard_similarity(tokenized_documents[0],tokenized_documents[1])</span><span id="f9c3" class="jt ju hi ky b fi lg ld l le lf">0.9090909090909091</span></pre><p id="d613" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">类似的情况在这里上面作为倒装句</p><h2 id="72be" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第三条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="061d" class="jt ju hi ky b fi lc ld l le lf">all_documents = [document_3_0, document_3_1]<br/><br/>tokenized_documents = [tokenize(d) for d in all_documents] # tokenized docs<br/>all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])<br/><br/>jaccard_similarity(tokenized_documents[0],tokenized_documents[1])</span><span id="cf83" class="jt ju hi ky b fi lg ld l le lf">0.9791666666666666</span></pre><h2 id="a641" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第四条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="3999" class="jt ju hi ky b fi lc ld l le lf">all_documents = [document_4_0, document_4_1]<br/><br/>tokenized_documents = [tokenize(d) for d in all_documents] # tokenized docs<br/>all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])<br/><br/>jaccard_similarity(tokenized_documents[0],tokenized_documents[1])</span><span id="3c17" class="jt ju hi ky b fi lg ld l le lf">0.35877862595419846</span></pre><p id="279e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，我们现在已经改变了这个词的上下文，因此我们现在看到了不同之处</p><h2 id="f8e6" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第五条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="a03d" class="jt ju hi ky b fi lc ld l le lf">all_documents = [document_5_0, document_5_1]<br/><br/>tokenized_documents = [tokenize(d) for d in all_documents] # tokenized docs<br/>all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])<br/><br/>jaccard_similarity(tokenized_documents[0],tokenized_documents[1])</span><span id="b634" class="jt ju hi ky b fi lg ld l le lf">0.2753623188405797</span></pre><h2 id="13fe" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第六条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="5b6d" class="jt ju hi ky b fi lc ld l le lf">all_documents = [document_6_0, document_6_1]<br/><br/>tokenized_documents = [tokenize(d) for d in all_documents] # tokenized docs<br/>all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])<br/><br/>jaccard_similarity(tokenized_documents[0],tokenized_documents[1])</span><span id="9899" class="jt ju hi ky b fi lg ld l le lf">0.3918918918918919</span></pre><h2 id="6056" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第七条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="97c6" class="jt ju hi ky b fi lc ld l le lf">all_documents = [document_7_0, document_7_1]<br/><br/>tokenized_documents = [tokenize(d) for d in all_documents] # tokenized docs<br/>all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])<br/><br/>jaccard_similarity(tokenized_documents[0],tokenized_documents[1])</span><span id="5970" class="jt ju hi ky b fi lg ld l le lf">0.3409090909090909</span></pre><h2 id="c705" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">我们可以说，如果假新闻中的词汇只是简单地与真实新闻中的词汇相对照，那么Jaccardian测度就无法发现差异。但是一旦我们改变了新闻文章的上下文，我们就会发现相似性的价值降低了</h2><h1 id="d9ce" class="lh ju hi bd jv li lj lk jz ll lm ln kd lo lp lq kg lr ls lt kj lu lv lw km lx bi translated">余弦相似性</h1><p id="5896" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">余弦相似性是一种度量，用于确定文档的相似程度，而不考虑它们的大小。在数学上，它测量的是在多维空间中投影的两个向量之间的角度余弦。在这个上下文中，我所说的两个向量是包含两个文档字数的数组。余弦相似度作为一种相似度度量，和常用词的数量有什么不同？当绘制在多维空间上时，其中每个维度对应于文档中的一个单词，余弦相似性捕捉文档的方向(角度)而不是大小。如果你想知道大小，计算欧几里得距离。余弦相似性是有利的，因为即使两个相似的文档由于大小而相距欧几里德距离很远，它们之间仍然可以有较小的角度。角度越小，相似度越高。</p><h2 id="5c8b" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第一条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="0797" class="jt ju hi ky b fi lc ld l le lf">documents = [document_1_0, document_1_1]<br/># Scikit Learn<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>import pandas as pd<br/><br/># Create the Document Term Matrix<br/>count_vectorizer = CountVectorizer(stop_words='english')<br/><br/>sparse_matrix = count_vectorizer.fit_transform(documents)<br/><br/># OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.<br/>doc_term_matrix = sparse_matrix.todense()<br/>df = pd.DataFrame(doc_term_matrix, <br/>                  columns=count_vectorizer.get_feature_names(), <br/>                  index=['true', 'false'])<br/>df</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="ab fe cl ly"><img src="../Images/342b8aba0138aab950caa2cd97072274.png" data-original-src="https://miro.medium.com/v2/format:webp/1*AM_DSKJdPyL_EUninlE9kQ.png"/></div></figure><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="5df9" class="jt ju hi ky b fi lc ld l le lf"># Compute Cosine Similarity<br/>from sklearn.metrics.pairwise import cosine_similarity<br/>print(cosine_similarity(df, df))</span><span id="3432" class="jt ju hi ky b fi lg ld l le lf">[[1.        0.9111977]<br/> [0.9111977 1.       ]]</span></pre><h2 id="cfbc" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第二条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="8492" class="jt ju hi ky b fi lc ld l le lf">documents = [document_2_0, document_2_1]<br/># Create the Document Term Matrix<br/>count_vectorizer = CountVectorizer(stop_words='english')<br/>count_vectorizer = CountVectorizer()<br/>sparse_matrix = count_vectorizer.fit_transform(documents)<br/><br/># OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.<br/>doc_term_matrix = sparse_matrix.todense()<br/>df = pd.DataFrame(doc_term_matrix, <br/>                  columns=count_vectorizer.get_feature_names(), <br/>                  index=['true', 'false'])<br/># Compute Cosine Similarity<br/>from sklearn.metrics.pairwise import cosine_similarity<br/>print(cosine_similarity(df, df))</span><span id="2129" class="jt ju hi ky b fi lg ld l le lf">[[1.         0.86279596]<br/> [0.86279596 1.        ]]</span></pre><h2 id="71b6" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第三条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="66db" class="jt ju hi ky b fi lc ld l le lf">documents = [document_3_0, document_3_1]<br/># Create the Document Term Matrix<br/>count_vectorizer = CountVectorizer(stop_words='english')<br/>count_vectorizer = CountVectorizer()<br/>sparse_matrix = count_vectorizer.fit_transform(documents)<br/><br/># OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.<br/>doc_term_matrix = sparse_matrix.todense()<br/>df = pd.DataFrame(doc_term_matrix, <br/>                  columns=count_vectorizer.get_feature_names(), <br/>                  index=['true', 'false'])<br/># Compute Cosine Similarity<br/>from sklearn.metrics.pairwise import cosine_similarity<br/>print(cosine_similarity(df, df))</span><span id="a162" class="jt ju hi ky b fi lg ld l le lf">[[1.         0.98319208]<br/> [0.98319208 1.        ]]</span></pre><h2 id="0b1b" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第四条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="8869" class="jt ju hi ky b fi lc ld l le lf">documents = [document_4_0, document_4_1]<br/># Create the Document Term Matrix<br/>count_vectorizer = CountVectorizer(stop_words='english')<br/>count_vectorizer = CountVectorizer()<br/>sparse_matrix = count_vectorizer.fit_transform(documents)<br/><br/># OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.<br/>doc_term_matrix = sparse_matrix.todense()<br/>df = pd.DataFrame(doc_term_matrix, <br/>                  columns=count_vectorizer.get_feature_names(), <br/>                  index=['true', 'false'])<br/># Compute Cosine Similarity<br/>from sklearn.metrics.pairwise import cosine_similarity<br/>print(cosine_similarity(df, df))</span><span id="da8a" class="jt ju hi ky b fi lg ld l le lf">[[1.        0.7974359]<br/> [0.7974359 1.       ]]</span></pre><h2 id="08da" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第五条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="cab0" class="jt ju hi ky b fi lc ld l le lf">documents = [document_5_0, document_5_1]<br/># Create the Document Term Matrix<br/>count_vectorizer = CountVectorizer(stop_words='english')<br/>count_vectorizer = CountVectorizer()<br/>sparse_matrix = count_vectorizer.fit_transform(documents)<br/><br/># OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.<br/>doc_term_matrix = sparse_matrix.todense()<br/>df = pd.DataFrame(doc_term_matrix, <br/>                  columns=count_vectorizer.get_feature_names(), <br/>                  index=['true', 'false'])<br/># Compute Cosine Similarity<br/>from sklearn.metrics.pairwise import cosine_similarity<br/>print(cosine_similarity(df, df))</span><span id="e4a7" class="jt ju hi ky b fi lg ld l le lf">[[1.         0.71691335]<br/> [0.71691335 1.        ]]</span></pre><h2 id="3fe1" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第六条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="edb2" class="jt ju hi ky b fi lc ld l le lf">documents = [document_6_0, document_6_1]<br/># Create the Document Term Matrix<br/>count_vectorizer = CountVectorizer(stop_words='english')<br/>count_vectorizer = CountVectorizer()<br/>sparse_matrix = count_vectorizer.fit_transform(documents)<br/><br/># OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.<br/>doc_term_matrix = sparse_matrix.todense()<br/>df = pd.DataFrame(doc_term_matrix, <br/>                  columns=count_vectorizer.get_feature_names(), <br/>                  index=['true', 'false'])<br/># Compute Cosine Similarity<br/>from sklearn.metrics.pairwise import cosine_similarity<br/>print(cosine_similarity(df, df))</span><span id="0911" class="jt ju hi ky b fi lg ld l le lf">[[1.         0.73389937]<br/> [0.73389937 1.        ]]</span></pre><h2 id="645a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第七条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="cd41" class="jt ju hi ky b fi lc ld l le lf">documents = [document_7_0, document_7_1]<br/># Create the Document Term Matrix<br/>count_vectorizer = CountVectorizer(stop_words='english')<br/>count_vectorizer = CountVectorizer()<br/>sparse_matrix = count_vectorizer.fit_transform(documents)<br/><br/># OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.<br/>doc_term_matrix = sparse_matrix.todense()<br/>df = pd.DataFrame(doc_term_matrix, <br/>                  columns=count_vectorizer.get_feature_names(), <br/>                  index=['true', 'false'])<br/># Compute Cosine Similarity<br/>from sklearn.metrics.pairwise import cosine_similarity<br/>print(cosine_similarity(df, df))</span><span id="723f" class="jt ju hi ky b fi lg ld l le lf">[[1.         0.62881668]<br/> [0.62881668 1.        ]]</span><span id="062f" class="jt ju hi ky b fi lg ld l le lf">documents = [document_4_0, document_5_0, document_6_0, document_7_0]<br/># Create the Document Term Matrix<br/>count_vectorizer = CountVectorizer(stop_words='english')<br/>count_vectorizer = CountVectorizer()<br/>sparse_matrix = count_vectorizer.fit_transform(documents)<br/><br/># OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.<br/>doc_term_matrix = sparse_matrix.todense()<br/>df = pd.DataFrame(doc_term_matrix, <br/>                  columns=count_vectorizer.get_feature_names(), <br/>                  index=['Set 4', 'Set 5', 'Set 6','Set 7'])<br/># Compute Cosine Similarity<br/>from sklearn.metrics.pairwise import cosine_similarity<br/>print(cosine_similarity(df, df))</span><span id="3ab4" class="jt ju hi ky b fi lg ld l le lf">[[1.         0.55025685 0.43467788 0.35961785]<br/> [0.55025685 1.         0.38752486 0.31582201]<br/> [0.43467788 0.38752486 1.         0.38250284]<br/> [0.35961785 0.31582201 0.38250284 1.        ]]</span><span id="22de" class="jt ju hi ky b fi lg ld l le lf">documents = [document_4_0, document_5_1, document_6_1, document_7_1]<br/># Create the Document Term Matrix<br/>count_vectorizer = CountVectorizer(stop_words='english')<br/>count_vectorizer = CountVectorizer()<br/>sparse_matrix = count_vectorizer.fit_transform(documents)<br/><br/># OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.<br/>doc_term_matrix = sparse_matrix.todense()<br/>df = pd.DataFrame(doc_term_matrix, <br/>                  columns=count_vectorizer.get_feature_names(), <br/>                  index=['Set 4', 'Set 5', 'Set 6','Set 7'])<br/># Compute Cosine Similarity<br/>from sklearn.metrics.pairwise import cosine_similarity<br/>print(cosine_similarity(df, df))</span><span id="e54d" class="jt ju hi ky b fi lg ld l le lf">[[1.         0.61857347 0.45259309 0.55249671]<br/> [0.61857347 1.         0.44449298 0.53149684]<br/> [0.45259309 0.44449298 1.         0.43816624]<br/> [0.55249671 0.53149684 0.43816624 1.        ]]</span></pre><h1 id="f3ac" class="lh ju hi bd jv li lj lk jz ll lm ln kd lo lp lq kg lr ls lt kj lu lv lw km lx bi translated">软余弦相似度</h1><p id="6946" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">两个向量之间的软余弦或(“软”相似性)考虑特征对之间的相似性。传统的余弦相似度认为向量空间模型(VSM)特征是独立的或完全不同的，而软余弦度量提出考虑VSM特征的相似度，这有助于推广余弦(和软余弦)的概念以及(软)相似的思想。例如，在自然语言处理(NLP)领域，特征之间的相似性是非常直观的。诸如单词、n元语法或句法n元语法之类的特征可能非常相似，尽管在形式上它们被认为是VSM中的不同特征。例如，单词“play”和“game”是不同的单词，因此映射到VSM的不同点；然而它们在语义上是相关的。在n元语法或句法n元语法的情况下，可以应用Levenshtein距离(事实上，Levenshtein距离也可以应用于单词)。为了计算软余弦，矩阵s用于指示特征之间的相似性。它可以通过Levenshtein距离、WordNet相似性或其他相似性度量来计算。然后我们乘以这个矩阵。</p><h2 id="c689" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第一条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="b493" class="jt ju hi ky b fi lc ld l le lf">import gensim<br/>documents = [document_1_0, document_1_1]<br/>from gensim.matutils import softcossim <br/>from gensim import corpora<br/>import gensim.downloader as api<br/>from gensim.utils import simple_preprocess<br/>fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')</span><span id="2929" class="jt ju hi ky b fi lg ld l le lf">[==================================================] 100.0% 958.5/958.4MB downloaded<br/><br/><br/>/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function<br/>  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL</span><span id="58bb" class="jt ju hi ky b fi lg ld l le lf"># Prepare a dictionary and a corpus.<br/>dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])<br/><br/># Prepare the similarity matrix<br/>similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)<br/>print(similarity_matrix)<br/><br/># Convert the sentences into bag-of-words vectors.<br/>sent_1 = dictionary.doc2bow(simple_preprocess(document_1_0))<br/>sent_2 = dictionary.doc2bow(simple_preprocess(document_1_1))<br/># Compute soft cosine similarity<br/>print(softcossim(sent_1, sent_2, similarity_matrix))</span><span id="1e49" class="jt ju hi ky b fi lg ld l le lf">(0, 0)	1.0<br/>  (1, 0)	0.057132058<br/>  (2, 0)	0.10670497<br/>  (3, 0)	0.12314428<br/>  (4, 0)	0.07843615<br/>  (5, 0)	0.17492798<br/>  (6, 0)	0.13644238<br/>  (7, 0)	0.16955109<br/>  (8, 0)	0.07055851<br/>  (9, 0)	0.05062564<br/>  (11, 0)	0.07319155<br/>  (12, 0)	0.008594441<br/>  (13, 0)	0.077509716<br/>  (14, 0)	0.17697306<br/>  (15, 0)	0.07584241<br/>  (16, 0)	0.035799537<br/>  (17, 0)	0.14308827<br/>  (18, 0)	0.079878114<br/>  (19, 0)	0.047306582<br/>  (20, 0)	0.10933455<br/>  (21, 0)	0.053965475<br/>  (22, 0)	0.07189872<br/>  (23, 0)	0.057720024<br/>  (24, 0)	0.10238695<br/>  (25, 0)	0.13781747<br/>  :	:<br/>  (40, 65)	0.08728125<br/>  (41, 65)	0.16906132<br/>  (42, 65)	0.13730161<br/>  (43, 65)	0.35975838<br/>  (44, 65)	0.24721034<br/>  (45, 65)	0.18764678<br/>  (46, 65)	0.14564325<br/>  (47, 65)	0.10063066<br/>  (48, 65)	0.14522117<br/>  (49, 65)	0.12855464<br/>  (50, 65)	0.15765215<br/>  (51, 65)	0.1680025<br/>  (52, 65)	0.1930339<br/>  (53, 65)	0.21249226<br/>  (54, 65)	0.16988975<br/>  (55, 65)	0.22988792<br/>  (56, 65)	0.15349543<br/>  (57, 65)	0.07669388<br/>  (58, 65)	0.18887833<br/>  (59, 65)	0.09025496<br/>  (60, 65)	0.16519293<br/>  (61, 65)	0.14934474<br/>  (62, 65)	0.18598081<br/>  (63, 65)	0.10408251<br/>  (64, 65)	0.15317246<br/>0.9907975401106488<br/><br/><br/>/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.<br/>  if np.issubdtype(vec.dtype, np.int):</span></pre><h2 id="8589" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第二条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="fe22" class="jt ju hi ky b fi lc ld l le lf">documents = [document_2_0, document_2_1]<br/># Prepare a dictionary and a corpus.<br/>dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])<br/><br/># Prepare the similarity matrix<br/>similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)<br/><br/># Convert the sentences into bag-of-words vectors.<br/>sent_1 = dictionary.doc2bow(simple_preprocess(document_2_0))<br/>sent_2 = dictionary.doc2bow(simple_preprocess(document_2_1))<br/># Compute soft cosine similarity<br/>print(softcossim(sent_1, sent_2, similarity_matrix))</span><span id="87c0" class="jt ju hi ky b fi lg ld l le lf">0.9825814076550279<br/><br/><br/>/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.<br/>  if np.issubdtype(vec.dtype, np.int):</span></pre><h2 id="08ee" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第三条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="da94" class="jt ju hi ky b fi lc ld l le lf">documents = [document_3_0, document_3_1]<br/># Prepare a dictionary and a corpus.<br/>dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])<br/><br/># Prepare the similarity matrix<br/>similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)<br/><br/># Convert the sentences into bag-of-words vectors.<br/>sent_1 = dictionary.doc2bow(simple_preprocess(document_3_0))<br/>sent_2 = dictionary.doc2bow(simple_preprocess(document_3_1))<br/># Compute soft cosine similarity<br/>print(softcossim(sent_1, sent_2, similarity_matrix))</span><span id="3bfe" class="jt ju hi ky b fi lg ld l le lf">0.9977103651306122<br/><br/><br/>/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.<br/>  if np.issubdtype(vec.dtype, np.int):</span></pre><h2 id="66b4" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第四条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="af2e" class="jt ju hi ky b fi lc ld l le lf">documents = [document_4_0, document_4_1]<br/># Prepare a dictionary and a corpus.<br/>dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])<br/><br/># Prepare the similarity matrix<br/>similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)<br/><br/># Convert the sentences into bag-of-words vectors.<br/>sent_1 = dictionary.doc2bow(simple_preprocess(document_4_0))<br/>sent_2 = dictionary.doc2bow(simple_preprocess(document_4_1))<br/># Compute soft cosine similarity<br/>print(softcossim(sent_1, sent_2, similarity_matrix))</span><span id="e833" class="jt ju hi ky b fi lg ld l le lf">/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.<br/>  if np.issubdtype(vec.dtype, np.int):<br/><br/><br/>0.8949769387024548</span></pre><h2 id="e947" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第五条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="f0d3" class="jt ju hi ky b fi lc ld l le lf">documents = [document_5_0, document_5_1]<br/># Prepare a dictionary and a corpus.<br/>dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])<br/><br/># Prepare the similarity matrix<br/>similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)<br/><br/># Convert the sentences into bag-of-words vectors.<br/>sent_1 = dictionary.doc2bow(simple_preprocess(document_5_0))<br/>sent_2 = dictionary.doc2bow(simple_preprocess(document_5_1))<br/># Compute soft cosine similarity<br/>print(softcossim(sent_1, sent_2, similarity_matrix))</span><span id="6709" class="jt ju hi ky b fi lg ld l le lf">0.9460392429972865<br/><br/><br/>/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.<br/>  if np.issubdtype(vec.dtype, np.int):</span></pre><h2 id="e30a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">第六条</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="e56a" class="jt ju hi ky b fi lc ld l le lf">documents = [document_6_0, document_6_1]<br/># Prepare a dictionary and a corpus.<br/>dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])<br/><br/># Prepare the similarity matrix<br/>similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)<br/><br/># Convert the sentences into bag-of-words vectors.<br/>sent_1 = dictionary.doc2bow(simple_preprocess(document_6_0))<br/>sent_2 = dictionary.doc2bow(simple_preprocess(document_6_1))<br/># Compute soft cosine similarity<br/>print(softcossim(sent_1, sent_2, similarity_matrix))</span><span id="135d" class="jt ju hi ky b fi lg ld l le lf">0.9642978369414005<br/><br/><br/>/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.<br/>  if np.issubdtype(vec.dtype, np.int):</span><span id="0e22" class="jt ju hi ky b fi lg ld l le lf">documents = [document_7_0, document_7_1]<br/># Prepare a dictionary and a corpus.<br/>dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])<br/><br/># Prepare the similarity matrix<br/>similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)<br/><br/># Convert the sentences into bag-of-words vectors.<br/>sent_1 = dictionary.doc2bow(simple_preprocess(document_7_0))<br/>sent_2 = dictionary.doc2bow(simple_preprocess(document_7_1))<br/># Compute soft cosine similarity<br/>print(softcossim(sent_1, sent_2, similarity_matrix))</span><span id="c56c" class="jt ju hi ky b fi lg ld l le lf">/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.<br/>  if np.issubdtype(vec.dtype, np.int):<br/><br/><br/>0.9505847949707352</span><span id="53fe" class="jt ju hi ky b fi lg ld l le lf">documents = [document_1_0, document_1_1,document_2_0, document_2_1,document_3_0, document_3_1]<br/># Prepare a dictionary and a corpus.<br/>dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])<br/><br/># Prepare the similarity matrix<br/>similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)<br/>print(similarity_matrix)<br/># Convert the sentences into bag-of-words vectors.<br/>sent_1 = dictionary.doc2bow(simple_preprocess(document_1_0))<br/>sent_2 = dictionary.doc2bow(simple_preprocess(document_1_1))<br/>sent_3 = dictionary.doc2bow(simple_preprocess(document_2_0))<br/>sent_4 = dictionary.doc2bow(simple_preprocess(document_2_1))<br/>sent_5 = dictionary.doc2bow(simple_preprocess(document_3_0))<br/>sent_6 = dictionary.doc2bow(simple_preprocess(document_3_1))<br/># Compute soft cosine similarity<br/>print('Similarity between set 1 real and set 1 fake: ',softcossim(sent_1, sent_2, similarity_matrix))<br/>print('Similarity between set 1 real and set 2 real: ',softcossim(sent_1, sent_3, similarity_matrix))<br/>print('Similarity between set 1 real and set 2 fake: ',softcossim(sent_1, sent_4, similarity_matrix))<br/>print('Similarity between set 1 real and set 3 real: ',softcossim(sent_1, sent_5, similarity_matrix))<br/>print('Similarity between set 1 real and set 3 fake: ',softcossim(sent_1, sent_6, similarity_matrix))</span><span id="1ae5" class="jt ju hi ky b fi lg ld l le lf">/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.<br/>  if np.issubdtype(vec.dtype, np.int):<br/><br/><br/>  (0, 0)	1.0<br/>  (53, 0)	0.269355<br/>  (1, 1)	1.0<br/>  (56, 1)	0.47188094<br/>  (58, 1)	0.38057277<br/>  (3, 1)	0.3184986<br/>  (25, 1)	0.35255837<br/>  (57, 1)	0.32696036<br/>  (130, 1)	0.25129354<br/>  (2, 2)	1.0<br/>  (5, 2)	0.43318766<br/>  (6, 2)	0.40915966<br/>  (28, 2)	0.43435976<br/>  (45, 2)	0.42125136<br/>  (61, 2)	0.5979465<br/>  (3, 2)	0.27391726<br/>  (3, 3)	1.0<br/>  (1, 3)	0.3184986<br/>  (2, 3)	0.27391726<br/>  (7, 3)	0.34076366<br/>  (24, 3)	0.28623286<br/>  (28, 3)	0.29178333<br/>  (31, 3)	0.2750326<br/>  (51, 3)	0.2794037<br/>  (56, 3)	0.28585052<br/>  :	:<br/>  (37, 123)	0.3409985<br/>  (119, 123)	0.34883913<br/>  (124, 124)	1.0<br/>  (3, 124)	0.2793383<br/>  (5, 124)	0.49403647<br/>  (7, 124)	0.41360554<br/>  (32, 124)	0.37957755<br/>  (48, 124)	0.43871945<br/>  (125, 125)	1.0<br/>  (126, 126)	1.0<br/>  (113, 126)	0.413245<br/>  (127, 127)	1.0<br/>  (128, 128)	1.0<br/>  (106, 128)	0.40852967<br/>  (111, 128)	0.46465328<br/>  (129, 129)	1.0<br/>  (102, 129)	0.43781716<br/>  (130, 130)	1.0<br/>  (1, 130)	0.25129354<br/>  (58, 130)	0.21334913<br/>  (131, 131)	1.0<br/>  (52, 131)	0.44526622<br/>  (53, 131)	0.40351504<br/>  (120, 131)	0.34769118<br/>  (132, 132)	1.0<br/>Similarity between set 1 real and set 1 fake:  0.9561104452967236<br/>Similarity between set 1 real and set 2 real:  0.6064600254922445<br/>Similarity between set 1 real and set 2 fake:  0.6401801333554172<br/>Similarity between set 1 real and set 3 real:  0.7197187246530167<br/>Similarity between set 1 real and set 3 fake:  0.7329004394401343</span><span id="9afa" class="jt ju hi ky b fi lg ld l le lf">documents = [document_4_0, document_4_1,document_5_0, document_5_1,document_6_0, document_6_1,document_7_0, document_7_1]<br/># Prepare a dictionary and a corpus.<br/>dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])<br/><br/># Prepare the similarity matrix<br/>similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)<br/>print(similarity_matrix)<br/># Convert the sentences into bag-of-words vectors.<br/>sent_1 = dictionary.doc2bow(simple_preprocess(document_4_0))<br/>sent_2 = dictionary.doc2bow(simple_preprocess(document_4_1))<br/>sent_3 = dictionary.doc2bow(simple_preprocess(document_5_0))<br/>sent_4 = dictionary.doc2bow(simple_preprocess(document_5_1))<br/>sent_5 = dictionary.doc2bow(simple_preprocess(document_6_0))<br/>sent_6 = dictionary.doc2bow(simple_preprocess(document_6_1))<br/>sent_7 = dictionary.doc2bow(simple_preprocess(document_7_0))<br/>sent_8 = dictionary.doc2bow(simple_preprocess(document_7_1))<br/># Compute soft cosine similarity<br/>print('Similarity between set 1 real and set 1 fake: ',softcossim(sent_1, sent_2, similarity_matrix))<br/>print('Similarity between set 1 real and set 2 real: ',softcossim(sent_1, sent_3, similarity_matrix))<br/>print('Similarity between set 1 real and set 2 fake: ',softcossim(sent_1, sent_4, similarity_matrix))<br/>print('Similarity between set 1 real and set 3 real: ',softcossim(sent_1, sent_5, similarity_matrix))<br/>print('Similarity between set 1 real and set 3 fake: ',softcossim(sent_1, sent_6, similarity_matrix))</span><span id="ba38" class="jt ju hi ky b fi lg ld l le lf">/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.<br/>  if np.issubdtype(vec.dtype, np.int):<br/><br/><br/>  (0, 0)	1.0<br/>  (6, 0)	0.34844103<br/>  (75, 0)	0.38856554<br/>  (93, 0)	0.3464246<br/>  (105, 0)	0.3964237<br/>  (129, 0)	0.37776864<br/>  (56, 0)	0.3360912<br/>  (107, 0)	0.33096197<br/>  (139, 0)	0.33437905<br/>  (237, 0)	0.3299397<br/>  (1, 1)	1.0<br/>  (2, 2)	1.0<br/>  (35, 2)	0.33962697<br/>  (84, 2)	0.31973574<br/>  (143, 2)	0.2928415<br/>  (3, 3)	1.0<br/>  (118, 3)	0.44548658<br/>  (125, 3)	0.542848<br/>  (254, 3)	0.37505317<br/>  (24, 3)	0.311034<br/>  (164, 3)	0.3202444<br/>  (4, 4)	1.0<br/>  (36, 4)	0.36854532<br/>  (56, 4)	0.34207016<br/>  (93, 4)	0.4323508<br/>  :	:<br/>  (49, 251)	0.35305354<br/>  (201, 251)	0.3053905<br/>  (249, 251)	0.3628248<br/>  (252, 252)	1.0<br/>  (129, 252)	0.40784162<br/>  (253, 253)	1.0<br/>  (239, 253)	0.3485727<br/>  (254, 254)	1.0<br/>  (3, 254)	0.37505317<br/>  (125, 254)	0.39192784<br/>  (255, 255)	1.0<br/>  (36, 255)	0.39253923<br/>  (45, 255)	0.5456714<br/>  (107, 255)	0.34599632<br/>  (139, 255)	0.42906368<br/>  (148, 255)	0.3516806<br/>  (164, 255)	0.26543584<br/>  (208, 255)	0.40348202<br/>  (237, 255)	0.43203592<br/>  (76, 255)	0.38168862<br/>  (93, 255)	0.34062588<br/>  (256, 256)	1.0<br/>  (257, 257)	1.0<br/>  (19, 257)	0.3910559<br/>  (20, 257)	0.5127834<br/>Similarity between set 1 real and set 1 fake:  0.8949769387024548<br/>Similarity between set 1 real and set 2 real:  0.7253979310460288<br/>Similarity between set 1 real and set 2 fake:  0.7909116399656686<br/>Similarity between set 1 real and set 3 real:  0.7092782752937434<br/>Similarity between set 1 real and set 3 fake:  0.7230690011066724</span></pre><p id="ab9d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是余弦和软余弦的主要区别。软余弦相似性还包括特征之间的相似性。如果所有特征之间的相似性为1，则软余弦将表现为余弦相似性方法。</p></div></div>    
</body>
</html>