<html>
<head>
<title>Beating Pong using Reinforcement Learning — Part 2 A2C and PPO</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用强化学习打败乒乓——第二部分A2C和PPO</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/beating-pong-using-reinforcement-learning-part-2-a2c-and-ppo-b83391dd3657?source=collection_archive---------12-----------------------#2021-02-23">https://medium.com/analytics-vidhya/beating-pong-using-reinforcement-learning-part-2-a2c-and-ppo-b83391dd3657?source=collection_archive---------12-----------------------#2021-02-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="314a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><em class="jc">继续我的强化学习之旅</em></h1><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/d929715eefd36a33309ae07b4efe1f8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/0*K2hoEd2fHB6ogvEl"/></div></figure><p id="326a" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><em class="kj">作者安东尼奥·李斯</em></p><h1 id="2ca8" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="643e" class="pw-post-body-paragraph jl jm hh jn b jo kk jq jr js kl ju jv jw km jy jz ka kn kc kd ke ko kg kh ki ha bi translated">大家好，这是我关于媒介的第二篇文章，是<a class="ae kp" rel="noopener" href="/analytics-vidhya/beating-pong-using-reinforcement-learning-part-1-dddqn-f7fbf5ad7768">上一篇</a>的续篇。我们将使用第一部分中开发的大量代码。所以如果你没有读过，我建议你先读一读。</p><p id="8f32" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在这篇文章中，我们将从头开始实现A2C和PPO来击败atari pong游戏，就像我们在第一部分中用DDDQN所做的那样。</p><h1 id="c70e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">激励A2C和PPO</h1><p id="684a" class="pw-post-body-paragraph jl jm hh jn b jo kk jq jr js kl ju jv jw km jy jz ka kn kc kd ke ko kg kh ki ha bi translated">在继续之前，我们需要讨论一下为什么我们关注这两个算法。首先，两者都属于策略梯度算法家族。当DQN学习q值函数并从中导出策略时，策略梯度算法直接改进策略。这意味着网络的权重将更新，以增加具有良好总回报的行动的概率，并减少不良结果。作为一个说明性的例子，策略输出将是n个动作中的每一个的概率向量:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kq"><img src="../Images/6e48b08c028cd0f76907bcac2763f635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/0*ZGF3F7mF9hVEWTFl"/></div></figure><p id="9245" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">一个显著的区别是政策梯度算法是随机的，而q值方法是确定性的。确定性方法将给出相同的输入，给出相同的输出，因此我们需要一些ε贪婪的政策来探索环境；这是典型的开发与勘探的权衡。给定相同的输入，梯度策略方法可以给出不同的输出。动作是从网络输出的分布中采样的。所以我们不需要任何贪婪的策略来探索环境，就像我们在DQN做的那样。</p><p id="6ac0" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">另一个关键的区别是，DQN方法是非策略的，这意味着它们评估和改进的策略不同于用来选择行动的策略。脱离策略的算法可以使用由单独的策略生成的观察来学习，基本上是上一篇文章中的缓冲类。</p><p id="bf66" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">策略梯度方法是基于策略的，这意味着它们评估和改进选择动作的相同策略。基于策略的方法不能使用由不同策略产生的过去的经验，因此它们在使用观察值时效率较低。</p><p id="bdef" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这是选择要使用的算法的关键点。如果环境很快，所以很容易获得观察值，那么政策上的方法是最佳选择。但是，如果很难获得观察结果，那么政策外的方法应该是首选。</p><p id="3e79" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">所有这些可能看起来有点神秘，但当我们深入研究代码时，就会变得更加清晰。所以事不宜迟，就这么办吧。</p><h1 id="54a0" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">环境包装</h1><p id="4e93" class="pw-post-body-paragraph jl jm hh jn b jo kk jq jr js kl ju jv jw km jy jz ka kn kc kd ke ko kg kh ki ha bi translated">我们将使用相同的包装器和所有技巧来处理图像，并将它们堆叠起来，这在上一篇文章中已经讨论过。所以这没什么新鲜的。我们去有趣的地方吧。</p><h1 id="35bc" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">网络</h1><p id="5521" class="pw-post-body-paragraph jl jm hh jn b jo kk jq jr js kl ju jv jw km jy jz ka kn kc kd ke ko kg kh ki ha bi translated">我们将利用DDDQN算法中使用的相同架构来处理输入图像，但我们需要对网络头进行一些更改。演员-评论家方法背后的思想是，我们有一个演员，用来选择行动，还有一个评论家，批评演员的行动。演员将输出动作空间中的概率分布，而评论家将输出估计值函数。</p><p id="ebec" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">一开始，演员不知道怎么玩，所以它会随机尝试一些动作。评论家观察所采取的行动，并根据估计值给出反馈。参与者将从其反馈中学习更新策略，并更好地玩那个游戏；同时，批评家也将更新其提供反馈的方式。</p><p id="a3f9" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们可以看到，演员-评论家的想法是有两个神经网络，演员和评论家。实际上，这两个网络部分重叠，主要是出于效率和融合的考虑。在我们的例子中，演员和评论家被实现为来自同一个公共主体的不同头部。</p><p id="24e5" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">它们将接受相同的输入，但是参与者将返回一个具有n个逻辑值的张量，其中n是动作的数量，而批评家将返回一个具有一个值的张量，该值表示状态的值:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kr"><img src="../Images/cb8ced274ae46ce1f02561d7b154c383.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/0*ROkZyyDuPgtJ5Ut7"/></div></figure><p id="9b2e" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">A2C和PPO在构建网络时使用相同的逻辑，因此这部分代码没有区别:</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="55bb" class="kx if hh kt b fi ky kz l la lb">import numpy as np<br/>import tensorflow as tf<br/>import tensorflow.keras.layers as kl<br/>from tensorflow.keras.initializers import VarianceScaling</span><span id="a860" class="kx if hh kt b fi lc kz l la lb"><em class="kj">class</em> ProbabilityDistribution(tf.keras.Model):<br/>    <em class="kj">def</em> call(self, logits, **kwargs):<br/>        # Random distribution<br/>        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)<br/></span><span id="dc7c" class="kx if hh kt b fi lc kz l la lb"><em class="kj">class</em> Model(tf.keras.Model):<br/>    <em class="kj">def</em> __init__(self, num_actions, hidden):<br/>        # Note: no tf.get_variable(), just simple Keras API!<br/>        <em class="kj">super</em>().__init__('mlp_policy')<br/>        self.normalize = kl.Lambda(lambda layer: layer / 255)    # normalize by 255<br/>        self.conv1 = kl.Conv2D(32, (8, 8), strides=4, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)<br/>        self.conv2 = kl.Conv2D(64, (4, 4), strides=2, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)<br/>        self.conv3 = kl.Conv2D(64, (3, 3), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)<br/>        self.conv4 = kl.Conv2D(hidden, (7, 7), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)<br/>        <br/>        self.flatten = kl.Flatten()<br/>        self.value = kl.Dense(1, kernel_initializer=VarianceScaling(scale=2.), name="value")<br/>        self.logits = kl.Dense(num_actions, kernel_initializer=VarianceScaling(scale=2.), name='policy_logits')<br/>        <br/>        self.dist = ProbabilityDistribution()</span><span id="6492" class="kx if hh kt b fi lc kz l la lb">    <em class="kj">def</em> call(self, inputs, **kwargs):<br/>        # Inputs is a numpy array, convert to a tensor.<br/>        x = tf.convert_to_tensor(inputs)<br/>        # Separate hidden layers from the same input tensor.</span><span id="71ba" class="kx if hh kt b fi lc kz l la lb">        x = self.normalize(x)<br/>        x = self.conv1(x)<br/>        x = self.conv2(x)<br/>        x = self.conv3(x)<br/>        x = self.conv4(x)<br/>        x = self.flatten(x)<br/>        return self.logits(x), self.value(x)</span></pre><h1 id="7a95" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">代理人</h1><p id="7c34" class="pw-post-body-paragraph jl jm hh jn b jo kk jq jr js kl ju jv jw km jy jz ka kn kc kd ke ko kg kh ki ha bi translated">我们现在可以实现代理的逻辑了。这里我们没有任何缓冲重放，因此我们可以从环境中获取观察值，并将它们馈送到神经网络以更新权重:</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="d6b8" class="kx if hh kt b fi ky kz l la lb"><em class="kj">class</em> Agent:<br/>    <em class="kj">def</em> __init__(self, model, save_path=PATH_SAVE_MODEL, load_path=PATH_LOAD_MODEL, lr=LR, gamma=GAMMA, value_c=VALUE_C,<br/>                 entropy_c=ENTROPY_C, clip_ratio=CLIP_RATIO, std_adv=STD_ADV, agent=AGENT, input_shape=INPUT_SHAPE,<br/>                 batch_size = BATCH_SIZE, updates=N_UPDATES):<br/>        # Coefficients are used for the loss terms.<br/>        self.value_c = value_c<br/>        self.entropy_c = entropy_c<br/>        # `gamma` is the discount factor<br/>        self.gamma = gamma<br/>        self.save_path = save_path<br/>        self.load_path = load_path<br/>        self.clip_ratio = clip_ratio<br/>        self.std_adv = std_adv<br/>        self.agent = agent<br/>        self.input_shape = input_shape<br/>        self.batch_size = batch_size<br/>        self.updates = updates<br/>        self.opt = opt.RMSprop(lr=lr)<br/>        <br/>        self.model = model</span><span id="7284" class="kx if hh kt b fi lc kz l la lb">        if load_path is not None:<br/>            print("loading model in {}".<em class="kj">format</em>(load_path))<br/>            self.load_model(load_path)<br/>            print("model loaded")<br/>            <br/>            <br/>    <em class="kj">def</em> train(self, wrapper):<br/>        # Storage helpers for a single batch of data.<br/>        actions = np.empty((self.batch_size), dtype=np.int32)<br/>        rewards, dones, values = np.empty((3, self.batch_size))<br/>        observations = np.empty((self.batch_size,) + self.input_shape)<br/>        old_logits = np.empty((self.batch_size, wrapper.env.action_space.n), dtype=np.float32)</span><span id="9960" class="kx if hh kt b fi lc kz l la lb">        # Training loop: collect samples, send to optimizer, repeat updates times.<br/>        ep_rewards = [0.0]<br/>        next_obs = wrapper.reset()<br/>        for update in tqdm(<em class="kj">range</em>(self.updates)):<br/>            start_time = time.time()<br/>            for step in <em class="kj">range</em>(self.batch_size):<br/>                observations[step] = next_obs.copy()<br/>                old_logits[step], actions[step], values[step] = self.logits_action_value(next_obs[None, :])<br/>                next_obs, rewards[step], dones[step] = wrapper.step(actions[step])<br/>                next_obs = wrapper.state<br/>                ep_rewards[-1] += rewards[step]<br/>                if dones[step]:<br/>                    ep_rewards.append(0.0)<br/>                    next_obs = wrapper.reset()<br/>                    wandb.log({'Game number': <em class="kj">len</em>(ep_rewards) - 1, '# Update': update, '% Update': <em class="kj">round</em>(update / self.updates, 2),<br/>                                "Reward": <em class="kj">round</em>(ep_rewards[-2], 2), "Time taken": <em class="kj">round</em>(time.time() - start_time, 2)})</span><span id="2908" class="kx if hh kt b fi lc kz l la lb">            _, _, next_value = self.logits_action_value(next_obs[None, :])</span><span id="32c3" class="kx if hh kt b fi lc kz l la lb">            returns, advs = self._returns_advantages(rewards, dones, values, next_value, self.std_adv)<br/></span><span id="52b6" class="kx if hh kt b fi lc kz l la lb">            # Performs a full training step on the collected batch.<br/>            # Note: no need to mess around with gradients, Keras API handles it.<br/>            with tf.GradientTape() as tape:<br/>                logits, v = self.model(observations, training=True)<br/>                if self.agent == "A2C":<br/>                    logit_loss = self._logits_loss_a2c(actions, advs, logits)<br/>                elif self.agent == "PPO":<br/>                    logit_loss = self._logits_loss_ppo(old_logits, logits, actions, advs, wrapper.env.action_space.n)<br/>                else:<br/>                    raise Exception("Sorry agent can be just A2C or PPO")<br/>                value_loss = self._value_loss(returns, v)<br/>                loss = logit_loss + value_loss<br/>            grads = tape.gradient(loss, self.model.trainable_variables)<br/>            self.opt.apply_gradients(<em class="kj">zip</em>(grads, self.model.trainable_variables))</span><span id="398c" class="kx if hh kt b fi lc kz l la lb">            if update % 5000 == 0 and self.save_path is not None:<br/>                print("Saving model in {}".<em class="kj">format</em>(self.save_path))<br/>                self.save_model(f'{self.save_path}/save_agent_{time.strftime("%Y%m%d%H%M") + "_" + <em class="kj">str</em>(update).zfill(8)}/model.tf')<br/>                print("model saved")<br/>                </span><span id="2426" class="kx if hh kt b fi lc kz l la lb">        return ep_rewards</span></pre><p id="4dfe" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这里有几件事需要注意:</p><ul class=""><li id="cf43" class="ld le hh jn b jo jp js jt jw lf ka lg ke lh ki li lj lk ll bi translated">我们使用优势的概念。</li><li id="43af" class="ld le hh jn b jo lm js ln jw lo ka lp ke lq ki li lj lk ll bi translated">我们正在存储旧的逻辑值。当我们看到损失函数时，你就会明白为什么了。</li><li id="de68" class="ld le hh jn b jo lm js ln jw lo ka lp ke lq ki li lj lk ll bi translated">A2C和PPO的唯一区别在于损失函数。</li></ul><h1 id="619f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">回报和优势</h1><p id="6eee" class="pw-post-body-paragraph jl jm hh jn b jo kk jq jr js kl ju jv jw km jy jz ka kn kc kd ke ko kg kh ki ha bi translated">要减少梯度的方差并提高策略梯度方法的稳定性，一个好主意是使用DDDQN算法中使用的相同概念，并将总回报视为状态值加上行动优势:</p><p id="5082" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">Q(s，a) = V(s) + A(s，A)</p><p id="54e3" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">因此，评论家将使用DQN方法中使用的相同训练过程来估计状态值，该训练过程携带贝尔曼步骤并最小化均方误差以改进V(s)近似。</p><p id="b21d" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">稳定模型的另一个有用的东西是标准化优点，但它并不总是有效，所以会有一个布尔变量来启用/禁用它。</p><p id="077f" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">为了获得优势，我们需要首先使用贴现因子gamma计算每一步的贴现总回报。</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="7bdd" class="kx if hh kt b fi ky kz l la lb"><em class="kj">def</em> _returns_advantages(self, rewards, dones, values, next_value, standardize_adv):<br/>        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)</span><span id="776c" class="kx if hh kt b fi lc kz l la lb">        # Returns are calculated as discounted sum of future rewards.<br/>        for t in <em class="kj">reversed</em>(<em class="kj">range</em>(rewards.shape[0])):<br/>            returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])<br/>        returns = returns[:-1]</span><span id="69d6" class="kx if hh kt b fi lc kz l la lb">        # Advantages are equal to returns - baseline (value estimates in our case).<br/>        advantages = returns - values<br/>        <br/>        if standardize_adv:<br/>            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-10)</span><span id="8939" class="kx if hh kt b fi lc kz l la lb">        return returns, advantages</span></pre><h1 id="1d09" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">损失函数</h1><p id="f097" class="pw-post-body-paragraph jl jm hh jn b jo kk jq jr js kl ju jv jw km jy jz ka kn kc kd ke ko kg kh ki ha bi translated">我们从A2C和PPO的价值损失开始。这是价值头的输出和收集的回报之间的经典均方误差:</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="fd35" class="kx if hh kt b fi ky kz l la lb"><em class="kj">def</em> _value_loss(self, returns, value):<br/>        # Value loss is typically MSE between value estimates and returns.<br/>        return self.value_c * kloss.mean_squared_error(returns, value)</span></pre><p id="68dc" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">然后我们来看看政策损失，这是A2C和PPO之间的真正区别。我们从A2C开始:</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="8891" class="kx if hh kt b fi ky kz l la lb"><em class="kj">def</em> _logits_loss_a2c(self, actions, advantages, logits):</span><span id="4b80" class="kx if hh kt b fi lc kz l la lb">        # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.<br/>        # `from_logits` argument ensures transformation into normalized probabilities.<br/>        weighted_sparse_ce = kloss.SparseCategoricalCrossentropy(from_logits=True)</span><span id="c58a" class="kx if hh kt b fi lc kz l la lb">        # Policy loss is defined by policy gradients, weighted by advantages.<br/>        # Note: we only calculate the loss on the actions we've actually taken.<br/>        actions = tf.cast(actions, tf.int32)<br/>        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)</span><span id="9d13" class="kx if hh kt b fi lc kz l la lb">        # Entropy loss can be calculated as cross-entropy over itself.<br/>        probs = tf.nn.softmax(logits)<br/>        entropy_loss = kloss.categorical_crossentropy(probs, probs)</span><span id="f1c7" class="kx if hh kt b fi lc kz l la lb">        # We want to minimize policy and maximize entropy losses.<br/>        # Here signs are flipped because the optimizer minimizes.<br/>        return policy_loss - self.entropy_c * entropy_loss</span></pre><p id="645f" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">首先，我们采取输入动作(从0到5的整数值)、优势和logits(策略的输出之一)。正如Keras模型类所期望的，我们将动作转换为int32。然后，我们将政策损失定义为采取行动的概率与利益加权的交叉熵。在底层，它接受动作向量并将其转换为分类或矢量化格式，如[0，0，0，1，0，0]。然后获取logits向量，使用softmax函数对其进行转换，获得概率向量。最后，它将概率向量的对数与行动和优势向量相乘。</p><p id="b322" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">正如我们所看到的，还有另一个最终损失，那就是熵损失。我们减去熵损失，以鼓励对模型的进一步探索，使极端分布，即概率值接近1的分布，不那么吸引人。</p><p id="fbe6" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">OpenAI团队于2017年在John Schulman等人名为<a class="ae kp" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank">proximity Policy Optimization Algorithms</a>的论文中介绍了PPO方法。铝..对经典A2C方法的核心改进是改变了它估计政策梯度的方式。PPO方法使用新旧保单之间的比率，按优势进行调整，而不是使用新保单的对数:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/1254535c980363079b4dadcc6eb492e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/0*HL4Z-z3Ny39iNIst"/></div></figure><p id="a07b" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这是通过<a class="ae kp" href="https://arxiv.org/pdf/1502.05477.pdf" rel="noopener ugc nofollow" target="_blank"> TRPO </a>算法实现的目标最大化(我们在这里不做介绍)，其约束条件是新旧策略之间的kull back-lei bler差异小于一个定义的delta。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/8e8a6352fcc18cea49b1b164e5c6441e.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/0*0QdVBJSJwm_lB2gx"/></div></figure><p id="bc27" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这个约束是必要的，因为没有它，L的最大化将导致巨大的政策更新。</p><p id="266d" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">PPO算法使用相同的逻辑，但方法更简单。它没有定义一个约束，只是裁剪了概率比。这样，新政策与旧政策不会有太大的不同:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/633d5144f643e8a5437766aeb3220e39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/0*iUA_VcC1TJoJnlxX"/></div></figure><p id="eaa6" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们看看这在代码中是如何翻译的:</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="71ea" class="kx if hh kt b fi ky kz l la lb"><em class="kj">def</em> _logits_loss_ppo(self, old_logits, logits, actions, advs, n_actions):<br/>        actions_oh = tf.one_hot(actions, n_actions)<br/>        actions_oh = tf.reshape(actions_oh, [-1, n_actions])<br/>        actions_oh = tf.cast(actions_oh, tf.float32)<br/>        actions_oh = tf.stop_gradient(actions_oh)<br/>        <br/>        new_policy = tf.nn.log_softmax(logits)<br/>        old_policy = tf.nn.log_softmax(old_logits)<br/>        old_policy = tf.stop_gradient(old_policy)<br/>        <br/>        old_log_p = tf.reduce_sum(old_policy * actions_oh, axis=1)<br/>        log_p = tf.reduce_sum(new_policy * actions_oh, axis=1)<br/>        ratio = tf.exp(log_p - old_log_p)<br/>        clipped_ratio = tf.clip_by_value(<br/>            ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)<br/>        advs = tf.stop_gradient(advs)<br/>        advs = tf.cast(advs, tf.float32)<br/>        surrogate = tf.minimum(ratio * advs, clipped_ratio * advs)<br/>        return -tf.reduce_mean(surrogate) - self.entropy_c * kloss.categorical_crossentropy(new_policy, new_policy)</span></pre><p id="a609" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在这里，我们将把动作转换成它们的矢量化格式，然后我们获取所采取的动作的旧的和新的逻辑，并将旧的和新的策略定义为逻辑的softmax。更准确地说，我们通过将比率定义为对数差，然后取指数值，从而使log softmax具有更多的数值稳定性。根据对数的商法则，数学上是等价的。最后，我们对比率进行裁剪，并取裁剪后的比率和标准比率之间的最小值作为优势权重。</p><p id="54c9" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们现在可以启动这两个算法来查看结果。</p><h1 id="875a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">培训和结果</h1><p id="40d4" class="pw-post-body-paragraph jl jm hh jn b jo kk jq jr js kl ju jv jw km jy jz ka kn kc kd ke ko kg kh ki ha bi translated">和往常一样，我们使用wandb来记录结果:</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="d8bb" class="kx if hh kt b fi ky kz l la lb">wandb.init(<br/>  project="tensorflow2_pong_{}".<em class="kj">format</em>(AGENT.lower()),<br/>  tags=[AGENT.lower(), "CNN", "RL", "atari_pong"],<br/>  config=CONFIG_WANDB,<br/>)</span></pre><p id="c8e6" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们定义环境，初始化网络和代理:</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="0c15" class="kx if hh kt b fi ky kz l la lb">pw = PongWrapper(ENV_NAME, history_length=4)<br/>model = Model(num_actions=pw.env.action_space.n, hidden=HIDDEN)<br/>agent = Agent(model)</span></pre><p id="f260" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">最后，我们有main()函数，它将调用agent.train方法。与往常一样，在中断培训时，有一个保存模型的try/except块，以便我们可以在另一个时间重新开始培训:</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="219a" class="kx if hh kt b fi ky kz l la lb"><em class="kj">def</em> main():<br/>    rewards_history = agent.train(pw)</span><span id="a419" class="kx if hh kt b fi lc kz l la lb">if __name__ == "__main__":<br/>    try:<br/>        main()<br/>    except KeyboardInterrupt:<br/>        # Save the model, I need this in order to save the networks, frame number, rewards and losses. <br/>        # if I want to stop the script and restart without training from the beginning<br/>        if PATH_SAVE_MODEL is None:<br/>            print("Setting path to ../model/{}".<em class="kj">format</em>(AGENT.lower()))<br/>            PATH_SAVE_MODEL = "../model/{}".<em class="kj">format</em>(AGENT.lower())<br/>        print('Saving the model in ' + f'{PATH_SAVE_MODEL}/save_agent_{time.strftime("%Y%m%d%H%M")}')<br/>        agent.save_model(f'{PATH_SAVE_MODEL}/save_agent_{time.strftime("%Y%m%d%H%M")}/model.tf')<br/>        print('Saved.')</span></pre><p id="d353" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们现在可以使用wandb的图来查看结果。</p><p id="70d2" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们从A2C开始:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/5635c7cd4971f1ba10abcbd652a9938a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*oGrgXrd17c1vO9U2"/></div></figure><p id="0813" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如您所见，由于没有对策略梯度的限制，策略更新中存在很高的可变性，并且算法在获得满意的结果后表现不佳。因此，我决定采用保存了大约1k步的模型，并使用低得多的学习速率再次开始训练，以避免在模型表现良好后进行大量的策略更新。让我们看看第二轮测试的结果:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/6de3bf3a0b4cbc0d60400a4f5c5ad58d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/0*4zsnDKqI-tZprxc_"/></div></figure><p id="4fb0" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如您所见，结果非常好，可变性非常低。有人可以从训练开始就想着把学习率定的低一点，但是适得其反，因为需要太多的时间去收敛。</p><p id="7079" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">开始时设定一个高的学习率，然后随着训练过程的推进而降低，这样更有意义。我们在DDDQN中用epsilon greedy策略做了一些类似的事情，在培训开始时有一个大的epsilon，在培训结束时有一个小的epsilon(以激励最初的探索和最后的开发)。这里我们采用了一种基本的方法来解决这个问题。一个更好的方法是将学习速度设定为步数的函数。但是训练需要一段时间，我需要训练PPO代理的时间，所以我决定用这种方式完成训练。</p><p id="427a" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们看看PPO的结果:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/f62cf22132785170cfb51664de1f21cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*6pQhtQIX2N4WY1Qe"/></div></figure><p id="efe2" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们使用了与A2C算法相同的起始学习速率，但是由于损失函数的削波机制，我们不需要在学习速率上做任何手脚。</p><p id="3ff3" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">你可以在我的<a class="ae kp" href="https://antonai.blog/?p=229#more-229" rel="noopener ugc nofollow" target="_blank">博客</a>上找到原文，我的<a class="ae kp" href="https://github.com/antonai91/reinforcement_learning/tree/master/a2c_ppo" rel="noopener ugc nofollow" target="_blank"> Github </a>上的所有代码。有任何问题，你可以通过<a class="ae kp" href="https://www.linkedin.com/in/lisiantonio/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我。</p><p id="8c98" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果你喜欢这篇文章，分享给你的朋友和同事吧！我会在下一篇文章中看到你。与此同时，要小心，保持安全，记住<em class="kj">不要成为另一块墙砖</em>。</p><p id="df32" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">Anton.ai</p></div></div>    
</body>
</html>