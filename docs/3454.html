<html>
<head>
<title>In-depth Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度主成分分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/in-depth-principal-component-analysis-3052479f5484?source=collection_archive---------6-----------------------#2021-07-01">https://medium.com/analytics-vidhya/in-depth-principal-component-analysis-3052479f5484?source=collection_archive---------6-----------------------#2021-07-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="69d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">主成分分析或PCA是一种无监督的学习方法，用于降维。在这篇博客中，我们将深入了解它，并学习如何使用它。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/0d9bd531eda083408665d6d3fc9a30cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uvixvDdwIIJkjCLyPjG55Q.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">可视化维度的诅咒。维数越高，处理难度越大。参考文献[1]</figcaption></figure></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="a119" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">了解PCA的先决条件:</strong></p><p id="01ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你需要知道什么是协方差矩阵，什么是特征值</p><p id="2ba8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以及特征向量来理解PCA的概念。</p><p id="48c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">本博客内容:</strong></p><ol class=""><li id="c954" class="jz ka hh ig b ih ii il im ip kb it kc ix kd jb ke kf kg kh bi translated">对认证后活动的需求</li><li id="f84d" class="jz ka hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated">什么是PCA？</li><li id="d0e2" class="jz ka hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated">PCA背后的数学</li><li id="b3b0" class="jz ka hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated">使用scikit-learn在Python中实现。</li></ol></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="e3e9" class="kn ko hh bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">对认证后活动的需求:</h1><p id="08ad" class="pw-post-body-paragraph ie if hh ig b ih ll ij ik il lm in io ip ln ir is it lo iv iw ix lp iz ja jb ha bi translated">当我们拥有大量数据集时，机器学习模型的工作令人惊讶，因为根据经验，拥有大量数据可以让我们建立更好的预测模型。但是，拥有高维(或拥有大量特征)数据会带来自身的问题。最大的缺点是维数灾难。由于特征的不一致性，高维数据很难处理，这将增加我们的计算时间，并使数据处理和EDA难以进行。</p><p id="6132" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">高维数据集在图像处理、自然语言处理等领域非常常见。因此，减少维数对于提高机器学习模型的性能至关重要。有各种技术可以做到这一点。在本文中，我们将学习一种叫做PCA的技术。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="629a" class="kn ko hh bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">什么是PCA:</h1><p id="8f18" class="pw-post-body-paragraph ie if hh ig b ih ll ij ik il lm in io ip ln ir is it lo iv iw ix lp iz ja jb ha bi translated">PCA是一种维度技术，它使我们能够识别数据集中的相关性和模式，以便可以将其转换为低维数据集，同时保留重要信息。换句话说，PCA帮助我们提取包含更多信息的特征集，并从数据集中移除高度相关的特征。例如，在一个图像处理问题中，使用PCA，我们可以将100维的数据集减少到20维，同时保留80%的信息。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="0092" class="kn ko hh bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">理解PCA背后的数学:</h1><p id="cae1" class="pw-post-body-paragraph ie if hh ig b ih ll ij ik il lm in io ip ln ir is it lo iv iw ix lp iz ja jb ha bi translated">现在，我们已经正式理解了PCA的定义，让我们来理解PCA在数学上是如何执行的。</p><p id="6a1b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PCA中遵循的步骤是</p><ol class=""><li id="8469" class="jz ka hh ig b ih ii il im ip kb it kc ix kd jb ke kf kg kh bi translated">标准化数据</li><li id="21f6" class="jz ka hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated">计算协方差矩阵</li><li id="bb51" class="jz ka hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated">计算特征值和特征向量</li><li id="1803" class="jz ka hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated">计算主成分</li><li id="3889" class="jz ka hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated">减少维度的数量</li></ol><p id="5de5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第一步:标准化数据:</strong></p><p id="e190" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">考虑一个电影数据集，从中提取两个特征，如下所示:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/d153c0cb8408503b5771788059113ba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*j_HXWGky5kH33PkhfzzfPw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">用于说明的数据集</figcaption></figure><p id="405f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们按原样将数据提供给机器学习模型，该模型将偏向特征2，因为它具有更高的值和更大的值范围。因此，我们需要使数据标准化。为此，我们使用以下公式:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/9a3380ff4967eee004498fb015225a0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*bbiTvYCV_MO6uGgrmOrcmA.png"/></div></figure><p id="8b1b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们有了标准化的值，让我们继续下一步。</p><p id="8aff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第二步:计算协方差矩阵:</strong></p><p id="00c3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">协方差矩阵为我们提供了数据集变量之间的相关性。它有助于我们识别高度相关的特征，这些特征彼此高度依赖，因此包含使ML模型的性能恶化的冗余信息。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ls"><img src="../Images/ad5466876d162d052ef9bda3b4cb930e.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*Q8FAgu6zSYzEUmYrnGPCfw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">协方差矩阵。图像参考[2]</figcaption></figure><p id="d503" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">协方差矩阵的重要要点是，协方差值表示–</p><p id="5ad1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lt"> 1。两个变量有多相关。</em></p><p id="3e15" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lt"> 2。负协方差表示成反比</em></p><p id="b0a0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lt"> 3。正协方差-正比。</em></p><p id="5e0e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第三步。计算特征值和特征向量:</strong></p><p id="2d5f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这一步解释了主成分分析的要点。</p><p id="4a92" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，我们从协方差矩阵计算特征值和相应的特征向量。这一步骤背后的主要思想是理解数据中的方差。众所周知，数据中的差异越大，意味着信息越多。所以，特征向量和特征值帮助我们找出数据在哪个方向有最大的方差。</p><p id="a499" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">利用它们，我们可以找出数据的主要成分。主成分是从原始变量集中获得的一组新变量。这些组件压缩并拥有大部分有用的信息。为了理解如何提取主成分，我们进入下一步。</p><p id="4759" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第四步。计算主成分:</strong></p><p id="5789" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，在我们已经计算了特征值和特征向量之后，我们将按照特征值的降序排列它们，因为具有最高特征值的特征向量是最重要的，并且形成第一主分量。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lu"><img src="../Images/7ad5ae27ca8d97615b48f6db1ef13080.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*5QXgAby2DExqSuvb_xfkYg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">主要成分。图像参考文献[3]</figcaption></figure><p id="f423" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如图所示，基因1和基因2是原始变量集，我们从中提取一组新的变量。因此，PCA第一维具有最高的方差，PCA第二维垂直于PC1并且具有第二高的方差，因此保留了最大的剩余信息。</p><p id="0a7e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第五步。减少数据集的维度:</strong></p><p id="303a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是我们用代表最重要信息的最终主成分重新排列数据集的最后一步。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="d8cd" class="kn ko hh bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">使用Python实现PCA</h1><p id="1c36" class="pw-post-body-paragraph ie if hh ig b ih ll ij ik il lm in io ip ln ir is it lo iv iw ix lp iz ja jb ha bi translated">我们将以图像处理数据集为例。</p><p id="9e09" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lt"> a .导入数据集</em></p><p id="600f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将使用scikit-learn库提供的标准图像数据集(在Wild face dataset中标记为Faces)。首先，按如下方式导入所有必需的库，并提取数据集:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/c9839e8479b78ded779056c00780082f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*BR3MT36jIOjdVcXkEARaIg.png"/></div></figure><p id="fd69" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据集有1140行或数据点和2914个要素。因此，这是一个很好的PCA图解数据集。</p><p id="85ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lt"> b .准备数据集:</em></p><p id="84d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们现在将特征和目标变量分开，以便进一步处理</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/f3dae83f1586cf995edba0df5904b304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*uYL9AzN-isQApp95tkFa2Q.png"/></div></figure><p id="34d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，在进行PCA之前，我们将使用测试大小= 0.30的测试序列分割来分割数据集</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/8c5d035706fa62faa36d83de662b47cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*WsQonzkUDAU8x8gdKcFBrA.png"/></div></figure><p id="f394" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lt"> c .应用PCA: </em></p><p id="6370" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先标准化数据集:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/d6c29c3404c5360c34d14f43a031467d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*9qN8gHphQteTwafRpZlntg.png"/></div></figure><p id="4304" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在应用PCA之前，我们的数据集中有2914个特征。现在，我们将使用scikit-learn库应用PCA，并将其缩减为100维数据集。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/09b1f3121046e0937a6a247c4e3633d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*WvRHLpm9ReJEVN3u3NK3Ew.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/7725fdefef14bb9ff86e70df0b2fac8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*xPxN7wzyowluEj_O7DoNYw.png"/></div></figure><p id="8fb7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上面的片段中，我们可以看到方差比率是递减的。现在，我们将最终把我们的数据集转换成一个简化的熊猫数据框架。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/f2efb4c1eb78c1544c21537fc8d861c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*honQQsT7sRQMNVbVmxxrmA.png"/></div></figure><p id="efe0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，精简的数据集已准备好输入到我们的机器学习模型中。</p><p id="257f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lt"> d .替代方法(应用主成分分析法减少维度以保留一定的方差百分比)</em></p><p id="970b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们还可以使用主成分分析来减少表示方差(或信息)的给定百分比(比如80%)的维数。</p><p id="66b2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们实现PCA来保留80%的方差:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lv"><img src="../Images/153662fd390de1de9dfcbac8194946e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*lGr4yFiNJS0l8YMf4ZgM5w.png"/></div></figure><p id="f6c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们在上图中看到的，数据集将给出代表80%数据集的维数，在我们的例子中是29。为了更好地理解，让我们绘制保留方差与维数的关系图。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/507ff19bdb4dd300f091407a9c1e0e29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*mCVJTNlgVQIuArbUlGPRWQ.png"/></div></figure><p id="4182" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以观察到初始成分包含更多的信息，而后面的主成分不包含太多的信息。</p><p id="cfa7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，最后，我们将转换数据集并减少维数。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lw"><img src="../Images/bf5af68b47590b3ba9b89d1e15ae6250.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*ADBvR_9EMiUtzNlnrTugZA.png"/></div></figure><p id="3100" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你想看工作代码，请<a class="ae lx" href="https://colab.research.google.com/drive/1ueXRLGgzuTKEbvv28drSun8gJLVoE2d-?usp=sharing" rel="noopener ugc nofollow" target="_blank">点击这里</a>打开colab笔记本。</p><p id="7638" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢你的耐心和阅读我的文章，我希望它给了你一些知识。如果你想联系我，你可以通过<a class="ae lx" href="https://www.linkedin.com/in/hrushikesh-shelar/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="6fbe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">图像参考:</p><p id="2dd4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[1]<a class="ae lx" href="https://www.i2tutorials.com/what-do-you-mean-by-curse-of-dimensionality-what-are-the-different-ways-to-deal-with-it/" rel="noopener ugc nofollow" target="_blank">https://www . I2 tutorials . com/what-do-you-mean-by-curse-of-dimensionality-the-different-way-to-deal-it/</a></p><p id="76b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2]<a class="ae lx" href="https://www.statlect.com/fundamentals-of-probability/covariance-matrix" rel="noopener ugc nofollow" target="_blank">https://www . stat lect . com/fundamentals-of-probability/协方差矩阵</a></p><p id="3bbb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3]<a class="ae lx" href="https://blog.bioturing.com/2018/06/14/principal-component-analysis-explained-simply/" rel="noopener ugc nofollow" target="_blank">https://blog . bio turing . com/2018/06/14/principal-component-analysis-explained-simplely/</a></p></div></div>    
</body>
</html>