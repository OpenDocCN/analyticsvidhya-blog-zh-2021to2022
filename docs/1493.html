<html>
<head>
<title>Feature Selection — Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择—机器学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-selection-in-machine-learning-ec1f5d053007?source=collection_archive---------8-----------------------#2021-03-04">https://medium.com/analytics-vidhya/feature-selection-in-machine-learning-ec1f5d053007?source=collection_archive---------8-----------------------#2021-03-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="f9ff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们将讨论<strong class="ig hi">特征选择过程</strong>、<strong class="ig hi">为什么需要</strong>和<strong class="ig hi">特征选择</strong>的不同类型。</p><p id="726a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么，让我们开始吧…</p><p id="20a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">什么是特征选择过程？</strong></p><ul class=""><li id="62f1" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">这是一个选择对输出变量有更大影响的所需特征的过程。</li><li id="c323" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">这意味着我们只需要选择那些与输出变量高度相关的特征(独立变量)。</li><li id="2e42" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">这是创建机器学习模型的最重要的过程。</li></ul><p id="51eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">为什么特性选择很重要？</strong></p><p id="3d1e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">考虑一下，我们有一个包含数千个要素的数据集。当我们用大量特征训练我们的模型时，模型的准确性会下降。我们称这个问题为<strong class="ig hi">维度诅咒</strong>。</p><p id="e94b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，为了解决这个问题，我们只需要使用那些对因变量(输出变量)有更大影响的特征。有许多技术可以用来衡量自变量对因变量的影响。</p><p id="a917" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">有哪些不同类型的特征选择？</strong></p><p id="b0e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图显示了不同类型的特征选择。</p><figure class="jr js jt ju fd jv er es paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="er es jq"><img src="../Images/abbe93459f41eb23dd8dd1b2cf0560dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k8272idmlQ5X6JEuhwXGqA.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx translated"><strong class="bd kg">特征选择技术。图1.1 </strong></figcaption></figure><p id="2140" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">我们将首先讨论过滤方法。</strong></p><ol class=""><li id="e4ac" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb kh ji jj jk bi translated">皮尔逊相关(线性)。</li><li id="df90" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb kh ji jj jk bi translated">斯皮尔曼的军衔。(单调)</li><li id="34c4" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb kh ji jj jk bi translated">ANOVA相关系数(线性)。</li><li id="e7d5" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb kh ji jj jk bi translated">肯德尔秩系数(非线性)。</li><li id="3969" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb kh ji jj jk bi translated">卡方检验(列联表)。</li><li id="0a4c" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb kh ji jj jk bi translated">相互信息。</li></ol><p id="34b9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图给出了何时使用哪种方法的想法:</p><figure class="jr js jt ju fd jv er es paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="er es ki"><img src="../Images/81fd244147cb03fa84611e6972f22ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bvo8IQg-vNsG-rMeU0PMpA.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx translated"><strong class="bd kg">如何选择过滤方法。图1.2 </strong></figcaption></figure><p id="c4ac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上图中，你会对何时使用哪种方法有更好的了解。</p><ul class=""><li id="7993" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><strong class="ig hi">输入变量数值，输出变量数值:</strong>这是一个回归问题。</li></ul><ol class=""><li id="df27" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb kh ji jj jk bi translated"><strong class="ig hi">皮尔逊相关</strong>为线性关系。</li><li id="69aa" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb kh ji jj jk bi translated"><strong class="ig hi">斯皮尔曼相关</strong>为单调关系。</li></ol><ul class=""><li id="6616" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><strong class="ig hi">输入变量数值，输出变量分类:</strong>这是一个分类问题。</li></ul><ol class=""><li id="65f0" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb kh ji jj jk bi translated"><strong class="ig hi"> ANOVA </strong>为线性关系。</li><li id="317e" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb kh ji jj jk bi translated"><strong class="ig hi">肯德尔的</strong>为非线性关系。</li></ol><ul class=""><li id="8e49" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><strong class="ig hi">输入变量分类，输出变量数值:</strong>这是一个回归问题。我们很少遇到这种问题。</li></ul><ol class=""><li id="2883" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb kh ji jj jk bi translated"><strong class="ig hi"> ANOVA </strong>为线性关系。</li><li id="f70b" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb kh ji jj jk bi translated"><strong class="ig hi">肯德尔的</strong>为非线性关系。</li></ol><ul class=""><li id="88f9" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><strong class="ig hi">输入变量分类，输出变量分类:</strong>这是一个分类问题。</li></ul><ol class=""><li id="5aa9" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb kh ji jj jk bi translated">Chie-Square检验。</li><li id="7787" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb kh ji jj jk bi translated"><strong class="ig hi">相互信息。</strong></li></ol><p id="f019" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Sklearn实现了以下方法。</p><ul class=""><li id="1261" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">皮尔逊相关系数:<a class="ae kj" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html" rel="noopener ugc nofollow" target="_blank"> f_regression() </a></li><li id="b991" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">方差分析:<a class="ae kj" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html" rel="noopener ugc nofollow" target="_blank"> f_classif() </a></li><li id="6762" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">卡方:<a class="ae kj" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html" rel="noopener ugc nofollow" target="_blank"> chi2() </a></li><li id="7614" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">互信息:<a class="ae kj" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html" rel="noopener ugc nofollow" target="_blank"> mutual_info_classif() </a>和<a class="ae kj" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html" rel="noopener ugc nofollow" target="_blank">mutual _ info _ regression()</a></li></ul><p id="1e73" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> SciPy实现了以下方法。</strong></p><ul class=""><li id="8652" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">肯德尔的tau: <a class="ae kj" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html" rel="noopener ugc nofollow" target="_blank">肯德尔tau() </a></li><li id="0e32" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">斯皮尔曼等级相关性:<a class="ae kj" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html" rel="noopener ugc nofollow" target="_blank">斯皮尔曼()</a></li></ul><p id="c023" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">总结:</strong></p><p id="75cb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇博客中，我们讨论了<strong class="ig hi">特征选择过程</strong>的重要性、<strong class="ig hi">为什么需要</strong>以及<strong class="ig hi">特征选择的不同类型</strong>。</p><p id="3f4a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们主要关注基于<strong class="ig hi">滤波器的特征选择</strong> <strong class="ig hi">方法</strong>及其类型。</p><p id="9824" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在接下来的文章中，我将向您展示如何实际实现这些方法，以及其他类型的特性选择过程。</p></div></div>    
</body>
</html>