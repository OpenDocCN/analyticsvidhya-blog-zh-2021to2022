<html>
<head>
<title>Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">小型工作台NLP:自然语言处理中单个小GPU训练模型的基准</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/small-bench-nlp-benchmark-for-small-single-gpu-trained-models-in-natural-language-processing-92433cf70f73?source=collection_archive---------7-----------------------#2021-10-20">https://medium.com/analytics-vidhya/small-bench-nlp-benchmark-for-small-single-gpu-trained-models-in-natural-language-processing-92433cf70f73?source=collection_archive---------7-----------------------#2021-10-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="526b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">自然语言处理领域的最新进展，如基于转换器的模型，为我们提供了几个最新的(SOTA)预训练模型。然后，可以针对特定任务在定制数据集上对这些大型预训练模型进行微调。值得注意的是，这些大型模型在数周/数月内在众多GPU/TPU上训练了数十亿个参数，在基准排行榜上处于领先地位。</p><p id="dc8b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">像伯特、T5、GPT-3等最先进的模型有数百万/数十亿个参数。自然，训练这些时间和计算密集型模型需要大量资金。这种机构在排行榜上与不断增加的计算密集型大型十亿参数模型竞争的趋势大大增加了财务成本和碳足迹。对于资源有限的研究人员来说，在标记化、预训练任务、架构、微调方法等方面尝试新颖和创新的想法确实非常困难。为了使自然语言处理(NLP)研究更具包容性，更容易被更大的社区所接受，我们发布了小型NLP基准，用于在单个GPU上训练的具有成本和时间效益的小型模型。研究人员和从业者有机会验证他们使用相对较小的计算资源构建的模型。</p><p id="12fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在单个GPU (V100- 16GB内存)上预训练了四个小型模型。我们在38GB的<a class="ae jc" href="https://skylion007.github.io/OpenWebTextCorpus/" rel="noopener ugc nofollow" target="_blank"> OpenTextWebCorpus </a>数据上对BERT、RoBERTa、DeBERTa和ELECTRADeBERTa模型架构进行了预训练。为了验证我们的模型，我们在<a class="ae jc" href="https://gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank">通用语言理解评估(GLUE) </a>基准中的八个数据集上对我们的模型进行了微调。GLUE由基于六个不同NLP任务的九个数据集组成——问题回答、语言可接受性、情感分析、文本相似性、释义检测和自然语言推理(NLI)。由于开发和测试发行版的问题，我们没有在WNLI数据集上进行评估。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/4f80e6fea53dfab56dff7c920bbbe466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QcPTdah7qV61PByoperHRg.png"/></div></div></figure><p id="ddc3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们观察到，我们的实验性ELECTRA-DeBERTa架构比BERT模型(BERT、RoBERTa和DeBERTa)具有显著的增益，这是由于ELECTRA的替代令牌检测任务和DeBERTa的解纠缠注意机制。表1显示了我们的模型在GLUE数据集上的结果。</p><p id="32a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将这些单GPU训练模型的性能与具有(110M参数)的BERTBase模型的性能进行了比较。我们观察到，在几天内用几百万个参数训练的这些模型的平均得分比BERT-Base的平均得分82.20低大约1.8到3.8分。我们的ELECTRA-DeBERTa架构平均得分为81.53，与BERT-Base模型的得分82.20相当。表2显示了性能比较。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jp"><img src="../Images/df5b1d149b120b1aaf71695c85827cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*DtyvO-BviQQ-HKLByyFu6g.png"/></div></figure><p id="faa7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们设立了一个排行榜来展示模特们的表现。小板凳NLP的代码和排行榜可在<a class="ae jc" href="https://github.com/smallbenchnlp" rel="noopener ugc nofollow" target="_blank">https://github.com/smallbenchnlp</a>获得。可以在https://huggingface.co/smallbenchnlp的<a class="ae jc" href="https://huggingface.co/smallbenchnlp" rel="noopener ugc nofollow" target="_blank">的huggingface下载模型。</a></p><p id="1551" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要了解更多的技术细节，请查看https://arxiv.org/pdf/2109.10847.pdf<a class="ae jc" href="https://arxiv.org/pdf/2109.10847.pdf" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="c52c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">引用我们的论文，</p><pre class="je jf jg jh fd jq jr js jt aw ju bi"><span id="48b4" class="jv jw hh jr b fi jx jy l jz ka"><a class="ae jc" href="http://twitter.com/misc" rel="noopener ugc nofollow" target="_blank">@misc</a>{kanakarajan2021smallbench, </span><span id="cfd1" class="jv jw hh jr b fi kb jy l jz ka">     title={Small-Bench NLP: Benchmark for small single GPU trained    <br/>     models in Natural Language Processing},<br/>     author={Kamal Raj Kanakarajan and Bhuvana Kundumani and    <br/>     Malaikannan Sankarasubbu},<br/>     year={2021},<br/>     eprint={2109.10847},<br/>     archivePrefix={arXiv},<br/>     primaryClass={cs.LG}<br/>}</span></pre><p id="7bd9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参考文献</strong></p><div class="kc kd ez fb ke kf"><a href="https://gluebenchmark.com/" rel="noopener  ugc nofollow" target="_blank"><div class="kg ab dw"><div class="kh ab ki cl cj kj"><h2 class="bd hi fi z dy kk ea eb kl ed ef hg bi translated">粘合基准</h2><div class="km l"><h3 class="bd b fi z dy kk ea eb kl ed ef dx translated">通用语言理解评估(GLUE)基准是一个培训、评估…</h3></div><div class="kn l"><p class="bd b fp z dy kk ea eb kl ed ef dx translated">gluebenchmark.com</p></div></div><div class="ko l"><div class="kp l kq kr ks ko kt jn kf"/></div></div></a></div><div class="kc kd ez fb ke kf"><a href="https://skylion007.github.io/OpenWebTextCorpus/" rel="noopener  ugc nofollow" target="_blank"><div class="kg ab dw"><div class="kh ab ki cl cj kj"><h2 class="bd hi fi z dy kk ea eb kl ed ef hg bi translated">[计] 下载</h2><div class="km l"><h3 class="bd b fi z dy kk ea eb kl ed ef dx translated">今天，我们宣布发布Open WebText的测试版——一个复制OpenAI的开源努力…</h3></div><div class="kn l"><p class="bd b fp z dy kk ea eb kl ed ef dx translated">skylion007.github.io</p></div></div></div></a></div></div></div>    
</body>
</html>