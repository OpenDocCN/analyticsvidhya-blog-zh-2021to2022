<html>
<head>
<title>A Super Simple Explanation to Random Forest Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林分类器的一个超简单解释</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-super-simple-explanation-to-random-forest-classifier-d73c9a3307ee?source=collection_archive---------18-----------------------#2021-02-21">https://medium.com/analytics-vidhya/a-super-simple-explanation-to-random-forest-classifier-d73c9a3307ee?source=collection_archive---------18-----------------------#2021-02-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="5bc6" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">目标</strong></h1><p id="deac" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这篇文章是<em class="ka">超级简单解释</em>系列的第二部分，旨在，你猜对了，对ML概念进行超级简单的解释。我尝试用一个手工制作的例子来做这件事，这个例子将引导您完成构建随机森林所涉及的决策过程，这样当您在Python上实现两行代码时，您就知道后端发生的神奇的确切性质。</p><h1 id="72d8" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">观众</strong></h1><p id="ff0a" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">任何数据科学新手/对ML基础感到生疏。</p><p id="cd01" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">什么是随机森林？</strong></p><p id="16c7" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">随机森林是一堆集合在一起的决策树。</p><p id="5f50" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">它们是对决策树的改进，因为它们更加灵活；不同的树可以说明不同类型的特征组合。此外，通过聚集来自众多决策树的结果，随机森林的预测更加稳健。</p><p id="5d43" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">随机森林的泛化能力通过两种方式实现:</p><p id="6e2f" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">1.为每棵树中的观察值赋予不同的权重(不同于决策树，决策树为所有观察值赋予相同的权重)</p><p id="bfc8" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">2.仅使用随机的特征子集来构建每棵树(不像决策树，它使用所有的特征来构建树)</p><h1 id="1f62" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">他们可以使用哪些类型的数据作为特征？</strong></h1><ul class=""><li id="3ad8" class="kg kh hh je b jf jg jj jk jn ki jr kj jv kk jz kl km kn ko bi translated">数字的</li><li id="eedd" class="kg kh hh je b jf kp jj kq jn kr jr ks jv kt jz kl km kn ko bi translated">类别<br/> -二进制<br/> -多类</li></ul><h1 id="7ce1" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">玩具示例</strong></h1><p id="f483" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">让我们继续我们的<a class="ae ku" href="https://lnkd.in/g8kyvcU" rel="noopener ugc nofollow" target="_blank">决策树博客</a>中的例子。下面讨论的构建个体决策树的确切机制可以通过参考前面提到的博客来理解。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es kv"><img src="../Images/588820f06ce9bbea9428394268479516.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*_6ggKL22UV-9mjQGRG4hoQ.png"/></div></figure><p id="eae3" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我们需要根据以下因素来预测一个人是适合(F)还是不适合(NF):</p><ul class=""><li id="d616" class="kg kh hh je b jf kb jj kc jn ld jr le jv lf jz kl km kn ko bi translated">他们的年龄(数字)</li><li id="abb2" class="kg kh hh je b jf kp jj kq jn kr jr ks jv kt jz kl km kn ko bi translated">他们是否经常吃披萨(分类-二元)</li><li id="78ac" class="kg kh hh je b jf kp jj kq jn kr jr ks jv kt jz kl km kn ko bi translated">他们居住在哪个城市(分类-多类)</li></ul><p id="313e" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">其中分类变量0 =否，1 =是。</p><p id="4810" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">第一步。对每棵树中的观察值赋予不同的权重</strong></p><p id="2d3f" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">这是通过创建引导数据集来实现的；也就是说，我们通过替换重采样来构建与原始数据集大小相同的数据集。这意味着在我们的引导数据集中会有重复的行。</p><p id="5478" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">乍一看，这似乎有点奇怪；我们的模型多次看到同一个观察值有什么意义？通过将我们的树暴露给相同观察的多个实例，我们给这些重复的样本更高的权重。你会问，我们为什么想要这个？</p><p id="e2fc" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">虽然在理想的情况下，我们的训练数据将是我们测试数据的完美表示，但这并不是我们实际处理的数据集中发生的情况。在我们处理的数据集中，我们的训练数据中的一些模式将比我们的测试数据中的更普遍/更不普遍。为了说明这种可能性，我们基于不同的数据集构建决策树，这些数据集为每棵树中的不同行(也称为模式)赋予更高的权重，从而允许学习更加一般化，并减少过度拟合的机会。</p><p id="28d4" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">尽管在真实的场景中，会构建数百个决策树，但为了便于说明，我们在示例中只构建两个树。让我们继续构建两个自举数据集和它们对应的决策树。</p><p id="6c4a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">该算法首先采用<strong class="je hi">N =</strong><strong class="je hi">sqrt(# features)</strong>(在我们的例子中= sqrt(5) ≈ 2)，然后尝试高于和低于该值的# features，以找到构建决策树的随机特征的最佳数量。让我们从构建具有两个随机特征的树开始:</p><p id="2054" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">构建第一棵决策树</strong></p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es lg"><img src="../Images/b2b4447acf638686cef90e83c8d82950.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*hJLJwKV-peWSC3drWRnHyg.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">第一个包含两个要素的随机子集的引导数据集。</figcaption></figure><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es ll"><img src="../Images/145bb38c5627bd283952b78e262e989e.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*-1X11FzimlGxpSkvlGjp8Q.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">我们可以看到新德里特征没有帮助我们区分适合和不适合，因此它没有用在我们的第一个决策树中。</figcaption></figure><p id="8ff1" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">构建第二棵决策树</strong></p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es lm"><img src="../Images/8c7313b2cf5a105ef73bf01da5d35994.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*XWYPbqp4ql5255G8fJl1zA.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">具有两个要素的随机子集的第二个引导数据集。</figcaption></figure><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es ln"><img src="../Images/a1a519d1d1df9b79f471416d33e048af.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*eHgLDoGrITP7QDx2qjknWg.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">看起来该决策树能够在不使用Bangalore功能的情况下区分适合和不适合。</figcaption></figure><p id="7c1b" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">第二步。来自树的聚合预测</strong></p><p id="d8b4" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">现在我们已经有了两个决策树，我们可以汇总它们的结果来得出我们的最终预测。这种<strong class="je hi"> b </strong>脱壳和<strong class="je hi">聚集</strong>的结合就是俗称的<strong class="je hi">装袋</strong>。</p><p id="0b7f" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">请注意，在我们的每个引导数据集中，有一行被遗漏了；第一种情况是第0行，第二种情况是第3行。这不是偶然的；一般来说，自举过程中会遗漏1/3的数据。这些行组合在一起构成了开箱数据集。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es lo"><img src="../Images/de446efe01d184f2a48342aac358f671.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*ay0XWqmYQfQhzsxfzV8NCw.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">袋外数据集</figcaption></figure><p id="9e3f" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">衡量我们的随机森林性能的一种方法是在我们的随机森林中运行Out-of-Bag数据集，看看它的表现如何。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es lm"><img src="../Images/b581613295cedbece4c8d1f647c17599.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*KrfDoEOigTXi3hDywPQxww.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">通过第一个决策树运行第0行，我们预测个人是合适的。</figcaption></figure><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es lp"><img src="../Images/407ccf4991cec8271a119f53f51b89f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*H_TOGb-ZygGRwgFOiLTfhQ.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">通过第二个决策树运行第0行，我们预测个人是合适的。</figcaption></figure><p id="d814" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">这两棵树一致预测个体是适合的，这是正确的。如果是平局，那么我们的随机森林会任意选择任何一个预测。这使得我们的OOB误差(OOB数据集被错误分类的比例)= 0。</p><p id="c361" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">顺便提一下，我们的OOB数据集中的两行将以相同的方式评分，因为我们的随机森林中使用的唯一两个要素是Pizza和Age，这两个要素对于两行都是相同的。这对我们来说很简单，不需要第二个例子。</p><p id="94aa" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">顺便提一下，使用两个特性的子集对我们来说效果很好。如果没有，我们将继续构建N=1，N=3，… N=5的两个树的集合，并在产生最小OOB误差的N上结束。</p><p id="16a7" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">您可以在这里找到我们手动构建随机森林的玩具示例的代码，以及如何使用实际的Python包构建随机森林的代码:<a class="ae ku" href="https://github.com/sreevidyaraman/Random-Forest" rel="noopener ugc nofollow" target="_blank">https://github.com/sreevidyaraman/Random-Forest</a>。</p><h1 id="bf5c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">结论</strong></h1><p id="4394" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">随机森林很棒。通过增加灵活性和减少过度拟合，它们是对决策树的巨大改进。正如我们在玩具示例中看到的，基本上有两个主要的超参数需要确定；树的数量和构建树的要素的数量。</p><p id="6679" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">像往常一样，Josh Starmer是这个深入，但(希望)简单解释的灵感。这个解释的一个可能更有趣的版本可以在:</p><figure class="kw kx ky kz fd la"><div class="bz dy l di"><div class="lq lr l"/></div></figure><h1 id="3b5b" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">参考文献</strong></h1><p id="8b7b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">[1] Starmer，与Josh一起进行StatQuest。StatQuest:随机森林第1部分——构建、使用和评估。YouTube，2018年2月5日，<a class="ae ku" href="https://www.youtube.com/watch?v=J4Wdy0Wc_xQ." rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=J4Wdy0Wc_xQ.</a></p></div></div>    
</body>
</html>