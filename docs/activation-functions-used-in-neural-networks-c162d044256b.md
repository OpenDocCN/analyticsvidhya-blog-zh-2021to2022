# 用于神经网络的激活函数

> 原文：<https://medium.com/analytics-vidhya/activation-functions-used-in-neural-networks-c162d044256b?source=collection_archive---------24----------------------->

激活函数在神经网络中起着至关重要的作用。它也被称为传递函数。其目的是引入非线性变换来学习数据中复杂的潜在模式。它应该是可微的，也应该遵循计算的廉价性。并且它的输出需要以零为中心，以便它将有助于计算出的梯度在相同的方向上并且偏移。

激活函数表示为 *f(x)* 其中 *x=(输入*权重)+偏差*。现在，让我们来看看常用的激活函数。

![](img/c0e05276a85894d059883381ccd076fe.png)

# **乙状结肠功能**

sigmoid 函数可以定义为

![](img/7bea9f5d9f16b5d9a5e48a33154847df.png)

1.  它在 0 和 1 之间缩放值。
2.  它有一个 S 形曲线。
3.  以 *0.5* 为中心。
4.  它是可微的和单调的。
5.  它也被称为逻辑功能。

![](img/bab311884e97f60c2ff5ac16a39f03ee.png)![](img/c2e638f5552d8e519192d01fa3c8d281.png)

# **双曲正切函数**

双曲正切函数可以定义为

![](img/881e17845ab346846a65c3a19faacae4.png)

1.  它将值调整到-1 到+1 之间。
2.  它也类似于 S 形曲线。
3.  它以 0 为中心。
4.  它是可微的和单调的。

![](img/23db989e722cbcbdee72ca99f779c25d.png)![](img/39e31af2350bef320b0c0a4043242c2f.png)

# **整流线性单元功能**

ReLU 函数表示为

![](img/f1c61b2c80740d037867e25dc2d7b866.png)

1.  它是一个分段函数。
2.  当 *x* 的值小于零时返回 0，否则返回 1。
3.  它的障碍是所有负值都为零，这是一个叫作“死 ReLU”的问题。

![](img/ab0b210a3021507e8100d8404a93632f.png)![](img/4fabce964af34895c1f01b52b8f63109.png)

# **漏 ReLU 功能**

该函数表示为

![](img/2d61e7b57db0a796a651b92d34747184.png)

1.  它是 ReLU 函数的变体。
2.  它对所有负值 *(α)* 都有一个小斜率。
3.  *α* 多为 0.01。
4.  参数 ReLU 函数:-这里，参数被发送到神经网络，该网络学习 *α* 的最佳值。
5.  随机化的 ReLU 函数:-这里设置了 a 的随机值。

![](img/03702bb2aef1459c2321ba9ab2878a3e.png)![](img/4fdd1f81c3e987b286bf6a490a9a4dce.png)

# **指数线性单位函数**

该函数可以表示如下

![](img/83c4c8c658498a57ae7bb18504a649cf.png)

1.  它类似于漏 ReLU 函数。
2.  负值的斜率很小。
3.  它以零为中心。

![](img/58069d4dea5498b385705a6a20260333.png)![](img/8ce50883915c89ebc8e8df72fdda9980.png)

# **嗖嗖功能**

swish 函数表示为

![](img/b7c87b7a015e98a24fdd6ca1ae43aa2c.png)

1.  它是由谷歌推出的。
2.  它比 ReLU 表现更好。
3.  它是非单调的。
4.  *α(x)* 是一个 sigmoid 函数。
5.  它可以被重新参数化如下。

![](img/991acca4bb1c42614bfb40707edb93e5.png)![](img/7103b964ca1040ec679603346780a48e.png)![](img/f011e2424f3f743f60d825a06ff95139.png)

# **Softmax 功能**

该功能可以定义为

![](img/d3146317a536b11fb5fb5d130435f09d.png)

1.  它是 sigmoid 函数的推广。
2.  主要应用于网络的最后一层和多类分类任务。
3.  softmax 值的总和始终为 1。
4.  它将它们的输入转换成概率，如下所示。

![](img/8b72a5b60d73f5c0ccf1bbe095d498b0.png)![](img/cc33ebf75bcb683936c8a9ae556cb6b6.png)