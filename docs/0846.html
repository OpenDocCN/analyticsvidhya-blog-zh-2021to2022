<html>
<head>
<title>RNNs and the Evolution of Attention</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RNNs与注意力的进化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/rnns-and-the-evolution-of-attention-f92deb76b51a?source=collection_archive---------18-----------------------#2021-02-02">https://medium.com/analytics-vidhya/rnns-and-the-evolution-of-attention-f92deb76b51a?source=collection_archive---------18-----------------------#2021-02-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/9eb9df31b4cdcd255f2212e8286272dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EtJezgo_PJS88VEfZw5Llw.jpeg"/></div></div></figure><p id="511f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">预测未来一直是实践中最令人着迷的课题之一，并且已经发展了一段时间。尤其是在这个信息时代，现代计算的发展和人工智能的进步使它比以往任何时候都更有趣。序列对序列建模一直是主要的研究领域之一，其中模型学习在一段时间内捕获的观察，并能够在给定的场景下生成这样的模式。虽然统计方法已经存在了很长一段时间，但神经网络特别是RNNs的最新进展在当今所有应用中抢尽了风头，如自然语言处理、语音转文本、时间序列分析等。我们将对rnn、它们的流行变体LSTM和GRU以及注意力机制的演变进行概述，注意力机制是当今最先进的网络(如GPT3)的核心。</p><p id="cee0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">递归神经网络(RNN): </strong></p><p id="c1c8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">众所周知，典型的全连接神经网络有三层，即输入层、隐藏层和输出层。这些在回归和分类问题方面非常出色，在这些问题中，输入不相互依赖，或者一个输入/观察值与另一个输入/观察值之间没有相关性。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es jn"><img src="../Images/95cd056290fd9dc951aa63649bcf316f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*EH3jOpHAku-s7F56rT6EWA.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><em class="jw">图1a——全连通网络</em></figcaption></figure><p id="bb34" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是在序列预测问题中，观察值通过时间彼此相关，因此全连接层通过设计不能对这种数据建模，因为它记住了前一步骤中发生的事情，这影响了当前的预测。这就是为什么RNNs被认为是解决这种存储器/环境问题的概念的确切原因。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es jx"><img src="../Images/55244bbb3a6bb958095052eecb7ff678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*N60cB7vp-MnbWNFMh2ZKBg.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><em class="jw">图1b —递归神经网络</em></figcaption></figure><p id="8761" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如上所述，通过简单地添加从输出到下一个输入的反馈回路来形成RNNs，这种设计向网络提供了关于先前观察是什么的想法，这固有地向RNNs提供了记住上下文的能力。让我们看看它是如何反向传播的。</p><p id="ed42" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">通过时间的反向传播:</strong></p><p id="2ace" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">与全连接层不同，输入通过rnn顺序馈入，因此每个时间步都会发生反向传播。因此权重更新或梯度流随时间反向传播。例如，在如下所示的FC(全连接)和RNN之间的输入转发之间的比较中，我们清楚地看到FC网络将有一次梯度更新，但在RNN的情况下，将有5次。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es jy"><img src="../Images/4ec5d1c57a27fd02aaad852ababc79ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*jtY7Ich4IvSdRojjczD4RQ.png"/></div></figure><p id="032d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">由于这种性质，rnn需要在每个时间步进行反向传播。如果序列很长，梯度可能会随着时间反向传播而呈指数收缩。这种现象被称为<strong class="ir hi">消失梯度</strong>问题，其中RNN无法在长序列中学习(更新权重)，或者被称为跨不同时间步长的上下文。为了理解这一点，让我们考虑一个电影评论，它说<em class="jz">“我很可能不会推荐这部电影”</em>，这是一个积极的评论。但由于RNNs无法记住长期依赖或短期记忆，他们可能只会看到<em class="jz">“不推荐这部电影”</em>，并将此归类为负面评论。</p><p id="57b2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了解决RNNs中的短期记忆问题，引入了RNNs的新变体，命名为<strong class="ir hi">长短期记忆(LSTM) </strong>和<strong class="ir hi">门控循环单元(GRU) </strong></p><p id="3664" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">长短期记忆(LSTM) </strong></p><p id="58b7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">想象一下，当你第一次坐过山车的时候，在每一次转弯、高峰和低谷的时候，你都会尖叫着死去，就好像这是你生命的最后一刻。但是当你习惯了这种乘坐，并且更频繁地乘坐时，那些尖叫声就变成了一种乐趣，现在你似乎很享受这种乐趣。这种转变的原因是因为你的大脑能够在一段时间内保持这种体验，并且在你脑海深处的某个地方，你知道这次旅行是安全和快乐的。这种在脑海深处的东西被称为背景！这正是LSTM正在努力做的。</p><p id="427f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">LSTMs具有保持沿着整个网络运行的长期依赖性的小区状态/上下文向量。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ka"><img src="../Images/627241c9f6682a331c14a0720c49d5e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oY7P3iYehq-rU34YAkjp3w.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><em class="jw">图:3a LSTM建筑</em></figcaption></figure><p id="169a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如上图所示，沿着网络顶部的水平线被称为保持记忆的单元状态。现在让我们看看不同的组件是如何一起工作的，以及它们存在的必要性。</p><p id="8951" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">LSTM的主要目标是维护和利用c。有门和激活功能可以做到这一点。</p><p id="f3b0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">遗忘门:</strong>遗忘门控制需要从存储器中删除的内容。这有助于网络消除干扰预测的不必要的噪声。</p><p id="8278" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输入门:</strong>输入门控制需要添加到存储器中的内容，这些内容在新输入中可能是必不可少的。这过滤了需要添加到单元状态c的候选存储器数据。</p><p id="a9c7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出门:</strong>输出门通过将当前输入与存储器c相乘产生新的隐藏状态。</p><p id="a4b2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">乙状结肠功能</strong></p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es kb"><img src="../Images/cbc8163ebfbb3df67b844a8b0ea71dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*UwjvpydW7THmH9IuAktw9g.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><em class="jw">图:3b — Sigmoid函数</em></figcaption></figure><p id="e89c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于任何给定的输入，sigmoid函数返回范围从0到1的值。由于这一特性，该函数被用作选择函数。它允许接近1的值通过，并过滤接近0的值。这就是为什么这个函数被用于LSTM单元的输入和遗忘门。</p><p id="c42c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">双曲正切函数</strong></p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es kc"><img src="../Images/58b41ed95bb9727860e58bec177a5f66.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*J6N38y6U5Su9VU-663zpwA.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图3c —双曲正切函数</figcaption></figure><p id="dae0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">双曲正切函数产生-1和+1之间的输出。这有助于输出一个较小的数字，否则当迭代次数增加时会爆炸。</p><p id="bb1f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">门控循环单元(GRU) </strong></p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es kd"><img src="../Images/6508bfd4d2bada8e80fde0022ff42a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*fHcbnhw-1weF5V5nhpbxRw.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><em class="jw">图4a GRU建筑</em></figcaption></figure><p id="cd1f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">gru是对LSTMs的一个微小的修改。</p><ul class=""><li id="4ff1" class="ke kf hh ir b is it iw ix ja kg je kh ji ki jm kj kk kl km bi translated">它将遗忘门和输入门合并成一个“更新门”。</li><li id="0a10" class="ke kf hh ir b is kn iw ko ja kp je kq ji kr jm kj kk kl km bi translated">它还合并了单元格状态和隐藏状态</li><li id="192c" class="ke kf hh ir b is kn iw ko ja kp je kq ji kr jm kj kk kl km bi translated">它更新存储器两次，第一次(使用旧状态和新输入，称为复位门)和第二次(作为最终输出)。</li><li id="46de" class="ke kf hh ir b is kn iw ko ja kp je kq ji kr jm kj kk kl km bi translated">旧单元状态或隐藏状态(具有输入)用于其自身的更新以及预测。</li></ul><p id="e286" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">RNNs中的编码器和解码器架构</strong></p><p id="255c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">与分类或回归问题不同，序列到序列预测问题具有挑战性，因为输入和输出观察值的数量是不确定的。为了缓解这个问题，引入了编码器-解码器架构。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es ks"><img src="../Images/6585fe8c9fcac5ff428d68b0e78b50b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*7AQOE0mBsBQMuUbwJTrlHQ.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><em class="jw">图5a编码器-解码器网络</em></figcaption></figure><p id="49fe" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这种架构中，C从编码器传递到解码器模块。这里的复杂性在于C向量需要有效地编码所有的输入。另一方面，解码器应该能够以非常小的误差对预测进行精确解码。并且在序列预测中，解码器可能不知道其他解码器的结果是什么。为了克服这些问题，引入了注意机制</p><p id="d324" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">注意</strong></p><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kt"><img src="../Images/2ce2165f3347a179d5243186fd7d74de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uPDudzh9G4g4SpJ1gTwCJg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><em class="jw">图6a编码器-解码器架构注意</em></figcaption></figure><p id="daf6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">关注层通过从所有解码器层中提取隐藏状态并为这些状态分配权重来解决这个问题。这些加权状态被传递到解码器层以预测y1。现在细胞状态S1和隐藏状态一起反馈到注意层。这一次，注意力层给隐藏状态分配不同的权重，因为它知道解码器层已经预测了什么。这在解码器端顺序发生，并且消除了从单个内容向量预测序列的限制。</p><p id="c43f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">注意力层只是一个完全连接的层，它在训练时学习注意力权重。这种机制被证明是非常强大的，为变压器及其变体奠定了基础。</p><p id="8808" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们正处于一个激动人心的时代，人工智能领域的日常研究数量正在增加。理解核心概念对于理解新的研究领域、应用和跟上时代是很重要的。感谢阅读！</p></div><div class="ab cl ku kv go kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="ha hb hc hd he"><p id="f158" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jz">最初发表于</em><a class="ae lb" href="https://www.linkedin.com/pulse/rnns-evolution-attention-saravana-alagar/" rel="noopener ugc nofollow" target="_blank"><em class="jz">【https://www.linkedin.com】</em></a><em class="jz">。</em></p></div></div>    
</body>
</html>