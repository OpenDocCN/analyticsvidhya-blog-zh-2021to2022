<html>
<head>
<title>Unsupervised Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无监督机器学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/unsupervised-machine-learning-e0b73b91eb28?source=collection_archive---------22-----------------------#2021-04-18">https://medium.com/analytics-vidhya/unsupervised-machine-learning-e0b73b91eb28?source=collection_archive---------22-----------------------#2021-04-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/fdb53af8566284f45e41afd51e9a5a2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MXvi9hqsMUTx8eRAvvFFcA.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图片来源— TechVidvan</figcaption></figure><h1 id="b438" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated"><strong class="ak">那么什么是无监督机器学习呢？</strong></h1><p id="cc88" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">在无监督学习中，没有提供正确的响应，因此我们试图找到输入之间的相似性，并将相似的输入分组在一起。</p><p id="b2af" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated"><strong class="jt hi">不同类型的无监督机器学习算法:</strong></p><h1 id="bf06" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">k均值聚类</h1><p id="1e24" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">当我们尝试使用KMeansClustering将相似类型的输入分组在一起时，这里指定了聚类的数量，并且模型将数据分成这些聚类。</p><h2 id="8a74" class="ku iu hh bd iv kv kw kx iz ky kz la jd kc lb lc jh kg ld le jl kk lf lg jp lh bi translated"><strong class="ak">k均值聚类遵循的步骤有:</strong></h2><ol class=""><li id="57f2" class="li lj hh jt b ju jv jy jz kc lk kg ll kk lm ko ln lo lp lq bi translated">选择k的值(聚类数)。</li><li id="3d72" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">从输入空间中选择k个随机位置，并将聚类中心分配给这些位置。</li><li id="b236" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">现在是学习部分，我们将重复下面的步骤直到步骤6，直到所有的聚类中心停止改变它们的位置。</li><li id="c4a3" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">将选择一个数据点，找出其与所有聚类中心的距离，并将其分配给聚类中心距离最小的聚类。</li><li id="e67d" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">对所有数据点重复步骤4。</li><li id="7e36" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">在将数据点分配给聚类之后，它计算该聚类的所有数据点的位置的平均值，并将聚类中心分配给计算出的平均值。</li><li id="e833" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">在模型完成了对任何新数据点的学习之后，我们将把它分配到一个聚类中心距离最小的聚类中。</li></ol><h2 id="2dbf" class="ku iu hh bd iv kv kw kx iz ky kz la jd kc lb lc jh kg ld le jl kk lf lg jp lh bi translated">【KMeansClustering面临的挑战:</h2><ol class=""><li id="2c9e" class="li lj hh jt b ju jv jy jz kc lk kg ll kk lm ko ln lo lp lq bi translated">它容易陷入局部最小值，这可以通过执行多次运行来防止。</li><li id="5372" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">如何选择正确的聚类数，因为k值非常低会导致<strong class="jt hi">过度泛化</strong>，而k值非常高会导致<strong class="jt hi">过度拟合。<em class="lw">(将误差平方和乘以k值可以防止过拟合)</em> </strong></li><li id="3f0f" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">为了处理噪声而不是计算用于决定聚类中心的平均值，它计算中值以不包括极值点，然而它在计算上更昂贵。</li></ol><h1 id="0c3f" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">k均值神经网络:</h1><p id="8a00" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">这里k个聚类中心由神经网络单层中的k个神经元表示。最接近当前输入的聚类中心被激活。神经元(聚类中心)和当前输入之间的距离使用以下公式计算。</p><p id="a10d" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated"><code class="du lx ly lz ma b">Location of neuron in weight space -Location of the current input.</code></p><p id="2b4e" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated"><strong class="jt hi">竞争学习</strong>即神经元竞相被激活，最接近当前输入的神经元获胜(被激活)。</p><h2 id="7dcc" class="ku iu hh bd iv kv kw kx iz ky kz la jd kc lb lc jh kg ld le jl kk lf lg jp lh bi translated">KMeans神经网络遵循的步骤是:</h2><ol class=""><li id="5e03" class="li lj hh jt b ju jv jy jz kc lk kg ll kk lm ko ln lo lp lq bi translated">选择k值(聚类数)。</li><li id="8eae" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">从输入空间中选择k个随机权重或根据给定的条件，这些将作为输出节点，即(聚类中心)。</li><li id="b961" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">将输入向量归一化为两个归一化向量的点积，可以得到它们之间的距离。</li><li id="9c0d" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">学习部分:对于每个输入，计算其与聚类中心(神经元)的权重的点积，点积最大的那个将是该输入的获胜者神经元。</li><li id="5e72" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">现在按照这个等式更新那个<strong class="jt hi">赢家神经元</strong>的权重:</li></ol><figure class="mc md me mf fd ii er es paragraph-image"><div class="er es mb"><img src="../Images/300ca3b91fc117a3a8d2f2050fca9558.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*PW04j9yYjLxcD1K2yXW-Yg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">winner神经元方程的权重更新</figcaption></figure><p id="ab00" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">继续重复步骤4和5，直到神经元的权重停止变化。</p><p id="a94f" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">6.现在，对于任何新的输入，我们可以使用两个聚类中心的权重计算其点积，并将其分配给点积最大的聚类。</p><h1 id="a3e5" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">自组织特征映射:</h1><p id="43da" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">→它基于这样一个事实，即由相似类型的输入所激发的神经元被放置在一起，反之亦然。也就是说，靠近的神经元代表靠近的输入，而远离的神经元代表远离的输入。</p><p id="7998" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">→因此，神经元以网格状结构存在，具有接近的概念，即，被相似类型的输入激发的那些神经元靠近在一起。如果一个输入刺激了一个神经元，那么在一定范围内，它一定也刺激了它相邻的神经元。</p><p id="a86a" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">→在这里，获胜的神经元试图组织其邻域来响应特定的输入。这里，我们不仅更新受激神经元的权重，还更新其邻居的权重。</p><p id="96c3" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">获胜的神经元试图在权重空间中将它的具有正连接的相邻神经元拉向它自己，并将在该范围之外的神经元从权重空间推出/排斥。而它只是忽略了那些完全远离的神经元。这种行为可以用下图来描述:</p><figure class="mc md me mf fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/47bd46ac672e8c9df5956024c1d19b4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*3aUBx4A9o4kT8MZFLfZi8w.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">要素映射的横向连接强度(墨西哥帽)</figcaption></figure><h2 id="f1fc" class="ku iu hh bd iv kv kw kx iz ky kz la jd kc lb lc jh kg ld le jl kk lf lg jp lh bi translated">Kohonen的SOM算法:</h2><p id="d3b7" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">它遵循以下步骤:</p><p id="fd62" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">步骤1我们有一个神经元的权重向量和一个输入向量。</p><p id="8937" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">步骤2计算每个输入和神经元权重之间的欧几里德距离。然后选择获胜的神经元，即权重最小的神经元。</p><p id="eed9" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated"><code class="du lx ly lz ma b">nb = min |x-wj|</code></p><p id="c5e8" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">步骤3现在使用下面的等式更新获胜神经元的权重:</p><figure class="mc md me mf fd ii er es paragraph-image"><div class="er es mh"><img src="../Images/2cac2b2d9f2427311b8220eb150b5c82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*bIuhO7G3z720Li2VP_Wy1A.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">获胜神经元的权值更新公式</figcaption></figure><p id="26c3" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">步骤4更新获胜神经元的邻居的权重。</p><figure class="mc md me mf fd ii er es paragraph-image"><div class="er es mi"><img src="../Images/0758979a64af4f41fa428ef22539bfa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*x8TIenHidphpszqJ_PiHMQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">邻居权重更新公式</figcaption></figure><p id="9b49" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">附近神经元的h(nb，t) → 1</p><p id="98a2" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">远处神经元的h(nb，t) → -1</p><p id="4941" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">对于非常远的神经元，h(nb，t) → 0。</p><p id="1c1f" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">通过迭代，我们将继续降低学习率和邻域函数。所以用这种方法，我们会为每个输入更新神经元的权重。</p><p id="60ca" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">文章结束。</p><p id="bbcb" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">希望有帮助:)</p><p id="c025" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">谢谢，祝你愉快。</p></div></div>    
</body>
</html>