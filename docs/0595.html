<html>
<head>
<title>Understanding Bert Usage</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解Bert用法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-bert-usage-31af2042be9e?source=collection_archive---------11-----------------------#2021-01-23">https://medium.com/analytics-vidhya/understanding-bert-usage-31af2042be9e?source=collection_archive---------11-----------------------#2021-01-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="fa0a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我将展示Bert的基本用法，并在没有任何培训的情况下可视化Bert嵌入。有很多很好的学习Bert的教程，我将尝试可视化一些方面，所以我假设你已经有了关于Bert的基本知识。在下一篇文章中，我将微调Bert，并尝试展示训练后的变化。(下一篇<a class="ae jc" href="https://celikkam.medium.com/bert-fine-tune-visualization-5b405991c84d" rel="noopener">链接</a>)</p><p id="4889" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">代码在github <a class="ae jc" href="https://github.com/mcelikkaya/medium_articles/blob/main/bert_usage_and_finetune.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a> <br/>用nbviewer看颜色更好。<a class="ae jc" href="https://nbviewer.jupyter.org/github/mcelikkaya/medium_articles/blob/main/bert_usage_and_finetune.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="6e96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们非常非常简要地总结一下伯特之路:</p><p id="c45a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">之前我们有生成固定嵌入的<strong class="ig hi">手套</strong>。(不管我们有什么句子，一个单词都有固定的嵌入。然后用<strong class="ig hi"> Word2vec </strong>我们开始在嵌入创建中使用上下文。随着ELMO(bi-LSTM)的出现，我们开始使用句子的上下文来嵌入单词。(左右串接嵌入)<br/>然后<strong class="ig hi"> Bert </strong>通过同时使用左右，同时使用变压器机构来实现。《变形金刚》是NLP的一大步。</p><p id="b2ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以非常高效地使用转换器进行语言翻译(英语-&gt;德语)、编码器处理输入(英语)、解码器处理(输入和输出)。Bert是一个语言模型，它是一个预先训练好的模型，用于生成在<strong class="ig hi">下游</strong>任务中使用的向量。所以我们只需要编码器部分。</p><p id="f184" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">伯特</strong>是一个<strong class="ig hi">蒙面</strong>语言模型。在<strong class="ig hi">自回归</strong>语言模型中，预测每个单词，以之前的单词为条件。在Bert 中，单词预测依赖于所有单词的剩余部分。Bert用特殊关键字<strong class="ig hi">【掩码】</strong>替换一些令牌，并尝试预测。现在，输出不是自回归的(左- &gt;右)，它是根据非屏蔽标记一次计算整个句子。(**这有一些缺点，因为屏蔽令牌被假定为独立的)</p><p id="f1c9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如何针对我们的问题使用训练有素的网络？<br/>  Bert是一个巨大的网络生成嵌入，我们能在任何任务中使用它吗？<br/> Bert在两个任务中接受训练，<strong class="ig hi">掩蔽语言模型</strong> ing和<strong class="ig hi">下一句预测</strong>，所以要时刻记住，如果你的任务不是这些中的任何一个，直接使用Bert输出不可能给出很好的结果。在本文中，我将直接使用它，在后面的文章中，我将对Bert进行微调。</p><p id="b641" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Bert architecture有一个BertEmbeddings层，它将“Embedding(30522，768，padding_idx=0)”作为第一个元素。30522是词汇大小，768 Bert标准嵌入向量大小。还有一个"<strong class="ig hi">位置_嵌入</strong>"和"<strong class="ig hi">令牌_类型_嵌入</strong>"作为输入。当我们提供这3个输入时，它为后面的层生成一个[batch_size x seq_len x 768]向量。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="ji jj l"/></div></figure><p id="633a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在第1层之后，Bert有12层称为“BertLayer”的变压器层。(<a class="ae jc" href="https://huggingface.co/transformers/_modules/transformers/modeling_bert.html" rel="noopener ugc nofollow" target="_blank">源代码</a>)。与任何神经网络一样，前几层学习基本特征，后几层学习训练目标的数据分布。下面我们将演示所有这些层的输出。基本上，我们可以想到BertLayers，连续创建大小为768的向量的层。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="ji jj l"/></div></figure><pre class="jd je jf jg fd jk jl jm jn aw jo bi"><span id="2384" class="jp jq hh jl b fi jr js l jt ju">BertLayer = BertAttention + BertIntermediate + BertOutput</span></pre><p id="bf34" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后我们有一个线性层<strong class="ig hi">池</strong>结束。这为我们分类问题提供了一个有用的向量。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="ji jj l"/></div></figure><p id="13ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面我们可以看到Bert模型本身有1.1亿个参数。在第二篇文章中，我们将结合1线性层到这个伯特，并再次计算参数值。您将看到它只会添加近2000个新参数。我们将训练网络，权重将根据我们的目标而变化。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="ji jj l"/></div></figure><p id="a07f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我在这里使用的数据是如上简单的36个英语句子。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="ji jj l"/></div></figure><p id="278f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">样本编码用Bert <br/> </strong>对于使用Bert的简单方法可以查看下面的代码。例句是“我吃苹果”。我们首先附加特殊的关键字。然后分裂成代币。在Bert词汇表中，将这些拆分更改为<strong class="ig hi"> indexed_tokens </strong>。然后我们创建分段向量。(对于简单的一句话编码，到处都是1。)</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="ji jj l"/></div></figure><p id="61a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上述代码的结果如下。你可以看到所有的输入向量和输出向量。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="ji jj l"/></div></figure><p id="14da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">用Bert嵌入单词<br/> </strong>当Bert生成向量时，它是用块词来做的，因为Bert的词汇量有限(这对低维30k来说很好，这对NLP来说很小)。因此，当你有一个输入错误或未知(罕见)的单词时，从Bert返回的数据将与你预期的不同。所以总是返回一个键向量对。</p><p id="1dba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于使用单个单词嵌入，你可以试着想象Bert为你的问题创建的嵌入。检查12个变压器层，检查它们的组合，或者全部使用(平均值或总和)。</p><p id="c412" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面我将尝试一个简单的单词嵌入可视化。由于Bert基于上下文生成向量，因此“我吃苹果”和“我吃面包”中“我”的向量必须不同。<br/>“苹果吃我”和“面包吃我”是不一样的。</p><p id="4d8c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">或者我们可以查一下“吃”，<br/>“第一单数人在苹果上的动作”<br/>“第一单数人在面包上的动作”<br/>即使这些都是同一个词“吃”，动作有不同的词向量。这意味着伯特正在捕捉上下文。(如果我们使用Glove，Glove将为“<strong class="ig hi">吃</strong>”返回相同的向量)</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="ji jj l"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="er es jv"><img src="../Images/df8ec719775739651d586124e4f0df0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rz7Qh-Spj0RflvDZRUvDDA.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx translated"><strong class="bd kg">吃矢量吃12句</strong></figcaption></figure><p id="9ff6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上面你可以看到成串的“<strong class="ig hi">可以</strong>”、“<strong class="ig hi">想要</strong>”和“<strong class="ig hi">动词只能</strong>”。你也可以看到“我”和“我们”的子集群。这意味着即使我们有12个句子的“吃”向量，向量也会根据句子而变化。</p><p id="8c8b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我用动词eat过滤了36个句子，我有12个句子。对于12个句子，我有12个“吃”的向量。我们可以检查这些向量的余弦相似性。下表显示了所有矢量之间的余弦值。你可以看到相同的情态动词或相同的主语句子得到最高分，因为它们有非常相似的语境。最差的是情态动词和普通动词句子(我可以吃面包，我们吃苹果)。如你所见，背景是非常不同的。(在图像模糊的新标签中打开，或检查github中的图像)</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="er es kh"><img src="../Images/d9feeb1248bcb166a2267401f080f5df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L0_MyGcwCVF1A_rONP_Wow.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx translated"><strong class="bd kg">每个矢量的余弦值</strong></figcaption></figure><p id="ca25" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">嵌入Bert的句子</strong></p><p id="028c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有很多方法可以为Bert生成句子嵌入。事实上，甚至有针对不同任务的实验。你可以在网上搜索它们。这里我试着测试一些实际的用例。</p><p id="94de" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> CLS </strong>:代表整句向量的特殊记号。为了使用它，我们必须检查模型返回的最后一层(最后一个隐藏状态)的第0维。<br/> <strong class="ig hi"> Pooler输出</strong>:这是Bert架构最后一层的向量，在网络的末端有一个用于分类任务的线性层。<br/> <strong class="ig hi">隐藏层数</strong> =第1个嵌入层数+ 12个隐藏层数。关于这些层的有趣的事情是，它们是为一个特殊的问题而训练的，可能并不完全适合你的问题。</p><p id="be2b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为在任何网络中，第一层捕获低级特征，后面的层捕获高级特征(语义、形状、组..)，BERT的最后几层需要根据任务进行微调。但是由于Bert是在一个大的语料库上训练的，它对于几乎所有的NLP问题仍然是一个非常好的矢量器。</p><p id="f912" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在互联网上，你通常会看到，极客使用Bert的最后4层。这4层有768个维度。您可以连接这些(768x4)或sum ( 768)或mean ( 768)。求和不是一个好主意，因为长句会产生更大的总和。</p><p id="d1bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你的问题如此复杂，事情对你不起作用，你可以试试CNN风格的单词向量卷积。它可以给出非常好的结果。就像图像是2D向量，文本是单词(行)x嵌入(列)2D结构。</p><p id="a93d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我将在我的小数据集中尝试这些。数据集是关于3个动词(吃、喝、读)和6个</p><p id="5e43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对象(吃-&gt;[面包，苹果]，喝-&gt;[水，啤酒]，读-&gt;[书，报纸])，带2个主语(我，我们)…</p><p id="fa6d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我将使用这个数据集生成单词和句子向量，并检查生成的向量的质量。正如我们所说的，创建向量有多种方法，下面我展示其中的3种。我认为“隐藏层句子”是这些句子的一个很好的分布。即使还没有训练，它也很好地把阅读和(吃，喝)分开了。(我不怪其他向量，他们不必按照这个任务分开，因为他们没有受过这方面的训练。)</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="ji jj l"/></div></figure><p id="a6f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上述代码的结果生成了3个图，每个图显示了上述句子生成方法的分布。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="er es ki"><img src="../Images/b6ac133f599edcccaa54da32dc123b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n6ioSEYlT_o2x4R32wabsA.png"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="er es ki"><img src="../Images/9f937311ea39e5f322b584bff7d7ac4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pefL2xwWiKdzDm9wSsxtxw.png"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="er es kj"><img src="../Images/5e33cfa3e020befcc4d06d3ddb3ff92e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e3eV2vVmWYgFAs-PAPbT1A.png"/></div></div></figure><p id="1eb7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">伯特隐藏层</strong></p><p id="a8c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们来检查一下所有中间步骤都学了什么。下面我转储嵌入层和12层变压器。如果您检查嵌入，Transformer1、Transformer2、Transformer3根据动词集群有很好的分布，但是然后组变成混合的(因为Bert没有为我的数据集训练)。这实际上显示了第一层网络如何进行特征分解。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="er es kk"><img src="../Images/88738bc5f3947e1c832122f08f0407d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rnVxEP4A-XdtDMiSkqasVQ.png"/></div></div></figure><p id="1739" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上面我们可以看到根据他们所学的伯特的各个层次。如果你把这个逻辑应用到你的问题中，你就会对Bert能为你做什么有一个基本的了解。</p><p id="78ea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我非常简单地总结了架构。然后我们复习一下<strong class="ig hi">字</strong>，<strong class="ig hi">句</strong>的嵌入。最后我们检查隐藏层学到了什么。在下一篇文章中，我将训练Bert，所以你可以看到当我们训练时发生了什么变化。</p></div></div>    
</body>
</html>