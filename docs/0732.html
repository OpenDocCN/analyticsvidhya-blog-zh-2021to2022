<html>
<head>
<title>A Beginner’s Guide to Natural Language Processing — Part 5</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理入门指南—第5部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-beginners-guide-to-natural-language-processing-part-5-10709cdc1d34?source=collection_archive---------16-----------------------#2021-01-28">https://medium.com/analytics-vidhya/a-beginners-guide-to-natural-language-processing-part-5-10709cdc1d34?source=collection_archive---------16-----------------------#2021-01-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/306b08b6ae11bafab9fb404b0222faca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jdx3cN5-8Yb6P3fotisNEw.jpeg"/></div></div></figure><p id="242f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在<a class="ae jn" rel="noopener" href="/analytics-vidhya/a-beginners-guide-to-natural-language-processing-part-4-734b995c53ea">第4部分</a>中，我们看到了什么是向量，以及如何使用TFIDF或简单计数来生成向量。我们有一个向量，表示整个语料库中唯一单词的数量。当我举一个例子时，只有4-10个独特的单词。因此，我的向量大小也是4-10个整数或浮点值。现在让我们来看一个只有100个独特单词的语料库。似乎可以接受，对吧？但是当我们开始转向更大的语料库时，向量大小将开始变得越来越麻烦。例如，如果在10，000单词的语料库中有10个单词的句子。我们的向量将有9990个0，只有10个非零值——整数或浮点数。这样的稀疏矩阵变得难以浏览。对于我们来说，使用小尺寸的向量会更容易。但是我们不能准确地决定，哪些词，我们将不得不作为向量的一部分来表示，哪些词，我们将不得不离开。</p><p id="4d25" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们在直接使用TF-IDF/Count矢量值时面临的另一个问题是，它们不能捕捉单词之间的关系。“男性”、“男人”、“警察”和“国王”这些词的向量之间可能没有任何相似之处。</p><p id="e553" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了避免这种选择问题，以及生成显示密切相关的单词之间的相似性的向量，我们使用单词嵌入的概念——固定长度的向量，也考虑句子中单词之间存在的语义关系。Word2Vec是生成单词嵌入最常用的概念之一。在我们实现Word2Vec之前，我们还必须讨论另外两个概念Skipgram模型和连续词袋(CBOW)模型。</p><h2 id="f0a7" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">Word2Vec是什么？</h2><p id="26cd" class="pw-post-body-paragraph ip iq hh ir b is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji kn jk jl jm ha bi translated">谷歌在2013年推出的深度学习模型，用于为单词生成连续的密集向量(没有太多零的向量)。创建的向量捕获单词之间的上下文和语义相似性。它是一个无监督的模型，可以获取一个巨大的语料库，其中有大量独特的单词，并返回一个代表整个词汇的密集向量空间。通常，用户决定向量的长度，称为尺寸。与我们的TF-IDF或计数向量相比，维度通常比词汇表中唯一单词的数量小得多。</p><figure class="kp kq kr ks fd ii er es paragraph-image"><div class="er es ko"><img src="../Images/a66b917efd8a5d44e51b5cf4c1db9392.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*bkrBASpteKfCaxZDEEeN6g.png"/></div></figure><h2 id="2f42" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">CBOW模型是什么？</h2><p id="a222" class="pw-post-body-paragraph ip iq hh ir b is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji kn jk jl jm ha bi translated">在Word2Vec的这个架构中，我们试图根据目标单词(t-n，.，…t-1，t+1，……，t+n)。n的值是我们根据所解决的问题选择的窗口大小。我们可以将CBOW架构建模为分类模型，其中我们使用上下文单词作为输入x，并预测目标单词y。</p><figure class="kp kq kr ks fd ii er es paragraph-image"><div class="er es kt"><img src="../Images/994d8cc0e31f94bae5dbf6f0dbce36c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*3xy5IOpScN0aQwwfFbCmGQ.png"/></div></figure><h2 id="35c8" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">什么是Skipgram模型？</h2><p id="ea77" class="pw-post-body-paragraph ip iq hh ir b is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji kn jk jl jm ha bi translated">在这个模型中，我们尝试并预测上下文单词(t-n，.，…t-1，t+1，……，t+n)在目标单词(t)周围。我们给skip-gram模型输入x和标签y作为对(x，y)。我们通过使用[(target，context)，1]值作为正样本来训练它(target是我们感兴趣的单词，context是出现在目标单词附近的上下文单词)。标签1表示这是一个上下文和目标彼此相关的正对。我们还给出[(target，random)，0]对作为负样本，其中目标是我们感兴趣的单词，而random只是从我们的词汇表中随机选择的与我们的结果没有关联的单词。通过这个，我们教会了模型哪些词对是相关的，哪些是不相关的。这允许模型为相似的单词生成相似的嵌入。</p><p id="0b1b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下面是一个带有非常小的数据集的Word2Vec实现的演示。精度较低，但可以让您了解如何处理大型数据集。</p><figure class="kp kq kr ks fd ii"><div class="bz dy l di"><div class="ku kv l"/></div></figure><p id="d322" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">至此，我们完成了“自然语言处理入门指南”系列。一旦生成了向量和嵌入，您就可以使用它们来解决NLP必须提供的几乎所有问题！</p></div><div class="ab cl kw kx go ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ha hb hc hd he"><p id="3545" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="ld">感谢您的阅读！让我知道你对这个系列的看法！如果你喜欢，一定要分享给一些初学者，也帮助他们！干杯！拍手，分享，指正！</em> ❤</p></div></div>    
</body>
</html>