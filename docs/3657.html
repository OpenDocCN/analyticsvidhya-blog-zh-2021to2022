<html>
<head>
<title>Topic Modelling using LDA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用LDA的主题建模</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/topic-modelling-using-lda-aa11ec9bec13?source=collection_archive---------0-----------------------#2021-07-16">https://medium.com/analytics-vidhya/topic-modelling-using-lda-aa11ec9bec13?source=collection_archive---------0-----------------------#2021-07-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/7e3be909cb7a8942180d5dc386f8834b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*h1FxCKeLMylp7tYW"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">照片由<a class="ae hv" href="https://unsplash.com/@edgr?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">edu·格兰德</a>在<a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><div class=""/><p id="c37e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">自然语言处理中的主题建模是一种基于出现的词将主题分配给给定语料库的技术。主题建模很重要，因为在这个充满数据的世界里，对文档进行分类变得越来越重要。例如，一家公司收到数百条评论，那么对该公司来说，知道哪些类别的评论更重要是很重要的，反之亦然。</p><p id="4ece" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将看到以下内容:</p><ol class=""><li id="0715" class="jt ju hy ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">皱胃向左移</li><li id="0158" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">LDA中的超参数</li><li id="30f3" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">Python中的LDA</li><li id="a849" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">LDA的缺点</li><li id="a3c8" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">供选择的</li></ol><p id="440a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">主题可以被认为是描述文档的<em class="kh">关键词</em>，例如，对于一个主题，我们脑海中浮现的单词是排球、篮球、网球、板球等。一个<strong class="ix hz">主题模型</strong>是一个模型，它可以根据文档中出现的单词自动检测主题。</p><p id="0c7e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">值得注意的是，主题建模不同于主题分类。主题分类是一种监督学习，而主题建模是一种非监督学习算法。</p><p id="c6ef" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一些众所周知的主题建模技术有</p><ol class=""><li id="08fd" class="jt ju hy ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">潜在语义分析(LSA)</li><li id="b395" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">概率潜在语义分析(PLSA)</li><li id="4766" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">潜在狄利克雷分配</li><li id="bf08" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">相关主题模型(CTM)</li></ol><p id="f47f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将重点讨论LDA</p><figure class="kj kk kl km fd hk er es paragraph-image"><div class="er es ki"><img src="../Images/c81b76f0362beaf04e0bae3292b9f8db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*O5UpBGT7jI25yQLL8JFJtg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">主题建模。图片来自<a class="ae hv" href="http://chdoig.github.io/pygotham-topic-modeling/#/" rel="noopener ugc nofollow" target="_blank"> pyGotham </a></figcaption></figure><h1 id="7d8d" class="kn ko hy bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">潜在狄利克雷分配</h1><p id="0cb1" class="pw-post-body-paragraph iv iw hy ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">LDA是潜在狄利克雷分配的缩写，是一种用于主题建模的技术。首先，让我们把这个词分解一下，理解LDA是什么意思。潜在的意思是隐藏的，尚未被发现的东西。狄利克雷指出模型<em class="kh">假设</em>文档中的主题和那些主题中的单词遵循狄利克雷分布。分配意味着给予一些东西，在这种情况下是话题。</p><figure class="kj kk kl km fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lq"><img src="../Images/24c7e8dc35db702c4ee9b2d358b01733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5nIOdn0AFbhhMhXopj1cnQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">艾达。图片由<a class="ae hv" href="https://www.researchgate.net/publication/336065245_Insider_Threat_Detection_Based_on_User_Behavior_Modeling_and_Anomaly_Detection_Algorithms" rel="noopener ugc nofollow" target="_blank">金等人</a>拍摄。</figcaption></figure><p id="cfb2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">LDA假设使用统计生成过程生成文档，使得每个文档是主题的混合，并且每个主题是单词的混合。</p><p id="f640" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下图中，文档由10个单词组成，可以分为3个不同的主题，这三个主题都有自己的描述词。</p><figure class="kj kk kl km fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lr"><img src="../Images/c08231db5bc4db2c0b21b00e90702b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1FO1L9hyVKnAib8C"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">文档生成假设。图片来自<a class="ae hv" href="https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/" rel="noopener ugc nofollow" target="_blank">我的大学</a>。</figcaption></figure><p id="591c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">LDA中的一般步骤如下</p><figure class="kj kk kl km fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ls"><img src="../Images/f8801565cc7dc9d8aec3bc85d6bb912b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FUB2WfIUKZ5r87e_"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图片来自我伟大的学识</figcaption></figure><h1 id="35d3" class="kn ko hy bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">LDA中的超参数</h1><p id="23ec" class="pw-post-body-paragraph iv iw hy ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">LDA中有三个超参数</p><ol class=""><li id="9b79" class="jt ju hy ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">α →文件密度系数</li><li id="9729" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">β →主题词密度因子</li><li id="7c01" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">K →所选主题的数量</li></ol><p id="b09d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> α </strong>超参数控制文档中期望的主题<em class="kh">的数量</em>。<strong class="ix hz"> β </strong>超参数控制文档中每个主题的单词分布，<strong class="ix hz"> K </strong>定义我们需要抽取多少个主题。</p><h1 id="a8b6" class="kn ko hy bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">Python中的LDA</h1><p id="67c0" class="pw-post-body-paragraph iv iw hy ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">让我们来看一个LDA的实现。我们将尝试从一组评论中提取主题。</p><p id="f750" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将要处理的数据集是一组评论，如下所示:</p><figure class="kj kk kl km fd hk er es paragraph-image"><div class="er es lt"><img src="../Images/00a0b1a444e9919e090efe5bf28236a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*VTDu5zZHSZBlcBdl8WzwUg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">资料组</figcaption></figure><h2 id="079a" class="lu ko hy bd kp lv lw lx kt ly lz ma kx jg mb mc lb jk md me lf jo mf mg lj mh bi translated">特征提取:</h2><p id="6b0e" class="pw-post-body-paragraph iv iw hy ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">这一步与LDA无关，请直接跳到矢量化。</p><p id="f19d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，我们将进行特征提取，以获得一些对数据有意义的见解。</p><figure class="kj kk kl km fd hk"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="eb95" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们提取了以下特征</p><ol class=""><li id="cbff" class="jt ju hy ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">文件中的字数</li><li id="3e23" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">文档中的字符数</li><li id="ac44" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">文档的平均单词长度</li><li id="c787" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">存在的停用词数量</li><li id="8f7e" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">数字字符的数量</li><li id="ceca" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">上限字符数</li><li id="aef0" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">两极情绪</li></ol><h2 id="acae" class="lu ko hy bd kp lv lw lx kt ly lz ma kx jg mb mc lb jk md me lf jo mf mg lj mh bi translated">数据清理和预处理:</h2><figure class="kj kk kl km fd hk"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="f72e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在数据清理和预处理中，我们做了以下工作</p><ol class=""><li id="dcc0" class="jt ju hy ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">使所有字符都变成小写</li><li id="7154" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">扩展了简短形式，如<em class="kh">我会→我会</em></li><li id="2921" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">删除了特殊字符</li><li id="1b8d" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">删除了多余的空格和尾随空格</li><li id="e3ec" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">删除了重音字符并用它们的替代字符替换它们</li><li id="ccf2" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">将单词词条化</li><li id="f7dc" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">删除了停用词</li></ol><h2 id="876d" class="lu ko hy bd kp lv lw lx kt ly lz ma kx jg mb mc lb jk md me lf jo mf mg lj mh bi translated">矢量化:</h2><p id="76af" class="pw-post-body-paragraph iv iw hy ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">由于LDA有一个内置的TF-IDF矢量器，我们将不得不使用计数矢量器。</p><figure class="kj kk kl km fd hk"><div class="bz dy l di"><div class="mi mj l"/></div></figure><h2 id="a38d" class="lu ko hy bd kp lv lw lx kt ly lz ma kx jg mb mc lb jk md me lf jo mf mg lj mh bi translated">潜在狄利克雷分配；</h2><p id="a701" class="pw-post-body-paragraph iv iw hy ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">在这个例子中，我们已经知道了主题的数量，所以我们不需要调整超参数<strong class="ix hz"> k </strong>，但是当我们不知道主题的数量时，我们可以使用网格搜索。</p><p id="4381" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这可以按如下方式完成</p><figure class="kj kk kl km fd hk"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="6b08" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">网格搜索如下所示</p><figure class="kj kk kl km fd hk"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="3842" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们模型的动机如下:</p><ul class=""><li id="1416" class="jt ju hy ix b iy iz jc jd jg jv jk jw jo jx js mk jz ka kb bi translated">由于我们知道主题的数量，我们将使用主题数量为12的潜在狄利克雷分配。</li><li id="1e74" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated">我们也不需要比较不同的模型来获得最佳数量的主题</li><li id="e5fe" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated">我们将使用<em class="kh"> random_state </em>，以便可以重现结果</li><li id="b121" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated">我们将把模型拟合到矢量化数据中，并对其进行转换</li><li id="9dae" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated">拟合模型后，我们将打印每个主题的前10个单词</li><li id="21f8" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated">获得主题后，我们将创建一个新的列并分配主题</li></ul><figure class="kj kk kl km fd hk"><div class="bz dy l di"><div class="mi mj l"/></div></figure><h2 id="d215" class="lu ko hy bd kp lv lw lx kt ly lz ma kx jg mb mc lb jk md me lf jo mf mg lj mh bi translated">主题作业:</h2><p id="fa45" class="pw-post-body-paragraph iv iw hy ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">要分配主题，我们可以执行以下操作:</p><ol class=""><li id="b98a" class="jt ju hy ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">查看每个主题的单词云</li><li id="42d4" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">查看前10个单词</li><li id="8cb7" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">寻找KERA →报道和文章的关键词提取</li></ol><p id="1268" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要做词云，我们可以简单的导入词云库。</p><figure class="kj kk kl km fd hk"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="a556" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要了解更多关于KERA的情况，可以参考Maiya等人的论文“高度异构的文献集合的探索性分析”这个<a class="ae hv" href="https://arxiv.org/abs/1308.2359" rel="noopener ugc nofollow" target="_blank">链接</a>，它在arXiv上。</p><p id="6dfc" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">摘要如下</p><blockquote class="ml mm mn"><p id="c58b" class="iv iw kh ix b iy iz ja jb jc jd je jf mo jh ji jj mp jl jm jn mq jp jq jr js hb bi translated">我们<!-- -->提出了一个有效的多方面系统，用于高度异构的文档集合的探索性分析。我们的系统是基于以完全自动化的方式智能地标记单个文档，并在强大的多面浏览框架中利用这些标记。所采用的标记策略包括基于机器学习和自然语言处理的无监督和有监督方法。作为我们的关键标记策略之一，我们引入了KERA算法(为报告和文章提取关键词)。KERA以一种完全无监督的方式从单个文档中提取主题代表术语，并且被证明比最先进的方法有效得多。最后，我们评估了我们的系统在帮助用户定位深埋在大量异构信息海洋中的军事关键技术相关文档方面的能力。</p></blockquote><h2 id="89d2" class="lu ko hy bd kp lv lw lx kt ly lz ma kx jg mb mc lb jk md me lf jo mf mg lj mh bi translated">模型中的问题:</h2><ul class=""><li id="2817" class="jt ju hy ix b iy ll jc lm jg mr jk ms jo mt js mk jz ka kb bi translated">我们必须用提供的主题手动分配主题，这可能会导致错误</li><li id="5454" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated">无法检查分配的主题是否正确</li><li id="52c0" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated">只分配一个主题，而理想情况下，它应该取决于什么最匹配。</li><li id="003f" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated">在一些文档中，所有的主题都有可能导致问题，因为我们只选择了最大值</li><li id="62e4" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated">有些词与主题无关，如<em class="kh">折扣</em>、<em class="kh">日期变更</em></li></ul><h1 id="30d2" class="kn ko hy bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">LDA的缺点:</h1><ol class=""><li id="212c" class="jt ju hy ix b iy ll jc lm jg mr jk ms jo mt js jy jz ka kb bi translated">LDA在小文本上表现不佳；我们的大部分数据都很短。</li><li id="44b0" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">由于评论不连贯，LDA发现确定主题更加困难</li><li id="9ae1" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">由于评论主要是基于上下文的，因此基于单词共现<br/>的模型失败了。</li></ol><h1 id="62dc" class="kn ko hy bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">备选方案:</h1><p id="5f6c" class="pw-post-body-paragraph iv iw hy ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">我们可以使用BERT来做更好的主题建模，这将在以后讨论:)</p><h1 id="8fee" class="kn ko hy bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">资源:</h1><ul class=""><li id="0580" class="jt ju hy ix b iy ll jc lm jg mr jk ms jo mt js mk jz ka kb bi translated"><a class="ae hv" href="https://investigate.ai/text-analysis/choosing-the-right-number-of-topics-for-a-scikit-learn-topic-model/" rel="noopener ugc nofollow" target="_blank">为scikit选择正确的主题数量-学习主题建模|新闻数据科学(investigate.ai) </a></li><li id="3588" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated"><a class="ae hv" href="https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032" rel="noopener ugc nofollow" target="_blank">上下文主题识别。为…确定有意义的主题|作者:Steve Shao | Insight(insightdatascience.com)</a></li><li id="b7af" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated"><a class="ae hv" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html" rel="noopener ugc nofollow" target="_blank">sk learn . decomposition . latentdirichletallocation—sci kit-learn 0 . 24 . 2文档</a></li><li id="aa85" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated"><a class="ae hv" href="https://www.youtube.com/watch?v=T05t-SqKArY&amp;t=13s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=T05t-SqKArY</a></li><li id="6927" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated"><a class="ae hv" href="https://www.youtube.com/watch?v=FLZvOKSCkxY&amp;t=21s" rel="noopener ugc nofollow" target="_blank">使用Python和NLTK p.1对单词和句子进行标记的自然语言处理— YouTube </a></li><li id="d5bf" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated"><a class="ae hv" href="https://www.youtube.com/watch?v=VyDmQggfsZ0" rel="noopener ugc nofollow" target="_blank"> NLP教程13 —完整文本处理|端到端NLP教程|面向所有人的NLP | KGP有声— YouTube </a></li><li id="29e7" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated"><a class="ae hv" rel="noopener" href="/comet-ml/organizing-machine-learning-projects-project-management-guidelines-2d2b85651bbd">组织机器学习项目:项目管理指南|作者Gideon Mendels | comet . ml | Medium</a></li><li id="f4f8" class="jt ju hy ix b iy kc jc kd jg ke jk kf jo kg js mk jz ka kb bi translated">和许多堆栈溢出问题。</li></ul><p id="d1a7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢您的阅读:)</p><figure class="kj kk kl km fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mu"><img src="../Images/e42fd29f21787aaa489d8c05a796f48f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iol0eHchg8hEL8Yp"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">由<a class="ae hv" href="https://unsplash.com/@kellysikkema?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Kelly Sikkema </a>在<a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure></div></div>    
</body>
</html>