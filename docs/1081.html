<html>
<head>
<title>Pipeline: Mongodb to Spark 3.0.1 with Kubernetes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">管道:Mongodb到Spark 3.0.1和Kubernetes</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pipeline-mongodb-to-spark-3-0-1-with-kubernetes-e84234ba5e3e?source=collection_archive---------9-----------------------#2021-02-13">https://medium.com/analytics-vidhya/pipeline-mongodb-to-spark-3-0-1-with-kubernetes-e84234ba5e3e?source=collection_archive---------9-----------------------#2021-02-13</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/38aba3b767d76f76ad278d25e185dff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rAgvm3BEcgRDylcW9yberw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">完整的管道模拟(测试环境)</figcaption></figure><p id="f723" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在本文中，我将只关注上面这个管道的一部分。我放上这张图片只是为了展示我的测试环境的全貌。但是，在第二张图中，有这篇文章的必要部分。</p><figure class="js jt ju jv fd ii er es paragraph-image"><div class="er es jr"><img src="../Images/fcbf0ab580e0f36209eff40f1303c1ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*Vp5YsA0GqkrXmHw27DLDrw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">物品管道</figcaption></figure><p id="c915" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我们的目标是展示如何在集群Kubernetes上用spark完成这个管道的第一部分。</p><p id="0f3a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我在标题里说我用的是Spark 3.0.1。在这个版本中，我需要一些包来使用“mongodb spark连接器”。我使用的是<strong class="iv hi">mongo-spark-connector _ 2.12–3 . 0 . 1 . jar</strong>连接器，然后我需要放入我的spark安装路径:“<strong class="iv hi"> /usr/local/spark/jars </strong>”，这个和另外两个库:<strong class="iv hi"> bson-4.2.0.jar </strong>和<strong class="iv hi">mongo-Java-driver-3 . 12 . 7 . jar</strong>。maven存储库中有这些包:</p><p id="bc7c" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><a class="ae jw" href="https://mvnrepository.com/artifact/org.mongodb/bson/4.2.0" rel="noopener ugc nofollow" target="_blank">https://mvnrepository.com/artifact/org.mongodb/bson/4.2.0</a>；<a class="ae jw" href="https://mvnrepository.com/artifact/org.mongodb/mongo-java-driver/3.12.7" rel="noopener ugc nofollow" target="_blank">https://mvn repository . com/artifact/org . MongoDB/mongo-Java-driver/3 . 12 . 7</a></p><p id="53ad" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在，可以创建spark应用程序了:从MongoDB获取数据并将其发送到S3 bucket。参见代码示例:</p><figure class="js jt ju jv fd ii"><div class="bz dy l di"><div class="jx jy l"/></div></figure><p id="23be" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在另一个spark作业中，从S3获取数据并发送给oracle:</p><figure class="js jt ju jv fd ii"><div class="bz dy l di"><div class="jx jy l"/></div></figure><p id="a650" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">还有一个细节:我创建了一个函数，因为在我的场景中，我有许多spark作业，并且我使用线程来并行运行不相关的作业。(根据另一篇文章)如果您将直接使用这段代码，您应该删除声明函数。</p></div><div class="ab cl jz ka go kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="ha hb hc hd he"><p id="4656" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">所以，现在我们想在集群Kubernetes中运行这个spark作业。请遵循以下步骤:</p><p id="4da5" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">步骤1: </strong>在spark文档中，我们找到了根据我们的版本创建docker映像的命令。</p><pre class="js jt ju jv fd kg kh ki kj aw kk bi"><span id="deac" class="kl km hh kh b fi kn ko l kp kq">./bin/docker-image-tool.sh<strong class="kh hi">-r</strong> &lt;repo&gt; <strong class="kh hi">-t</strong> my-tag <strong class="kh hi">-p</strong> ./kubernetes/dockerfiles/spark/bindings/python/Dockerfile build</span></pre><p id="6c06" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">步骤2: </strong>我们从上面的映像构建一个新的映像，其中Dockerfile运行python requirements.txt来创建spark作业代码，并将mongo_spark_s3.py复制到该映像。</p><p id="d92f" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">好了，现在我假设你有一个集群Kubernetes在运行，如果没有，你可以用kind在云或者其他引擎中创建。</p><p id="5df9" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">第三步:</strong>helm中有一个spark操作，可以帮助我们更轻松地运行spark应用。更多信息:<a class="ae jw" href="https://googlecloudplatform.github.io/spark-on-k8s-operator/" rel="noopener ugc nofollow" target="_blank">Spark-on-k8s-operator | Kubernetes operator，用于管理Kubernetes上Apache Spark应用程序的生命周期。(Google cloud platform . github . io)</a>。我们只需要安装舵:<code class="du kr ks kt kh b">helm install incubator/sparkoperator --namespace spark-operator</code></p><p id="87f2" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">步骤4: </strong>创建我们的yaml文件来运行我们的SparkApplication。</p><figure class="js jt ju jv fd ii"><div class="bz dy l di"><div class="jx jy l"/></div></figure><p id="5934" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">第5步:</strong>用下面的代码应用这个部署:<strong class="iv hi">ku bectl apply-f spark-operator . YAML</strong>，神奇的事情发生了。</p><p id="92f3" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我设置了3个实例，因此kubernetes将创建一个spark驱动程序和3个执行程序来运行这项工作。操作员将自动完成执行器盒，驾驶员将保持完成状态。然后，可以在驱动程序窗格中查看日志。</p><p id="2a49" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我跳过了一些安装，比如(helm和Kubernetes cluster ),并减少了一些步骤来简化这个过程。希望这段代码能帮到某个人。=)</p></div><div class="ab cl jz ka go kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="ha hb hc hd he"><p id="2f66" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">来源:</strong></p><p id="a1ed" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">[1]:<a class="ae jw" href="https://twitter.com/BradSheppard5" rel="noopener ugc nofollow" target="_blank">https://twitter.com/BradSheppard5</a>:他在这个实施过程中帮了大忙。</p><p id="bcb8" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">[2]:<a class="ae jw" href="https://github.com/Wesley-Bos/spark3.0-examples" rel="noopener ugc nofollow" target="_blank">Wesley-Bos/spark 3.0-示例:基本Spark示例。</a>(github.com):伟大的提示。</p></div></div>    
</body>
</html>