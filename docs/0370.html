<html>
<head>
<title>Invoice Information extraction using OCR and Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用OCR和深度学习的发票信息提取</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/invoice-information-extraction-using-ocr-and-deep-learning-b79464f54d69?source=collection_archive---------0-----------------------#2021-01-14">https://medium.com/analytics-vidhya/invoice-information-extraction-using-ocr-and-deep-learning-b79464f54d69?source=collection_archive---------0-----------------------#2021-01-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="e937" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">文档信息提取被认为是计算机视觉中的一个主要挑战，并且涉及场景中对象分类和对象定位的组合。深度学习中现代进步的出现导致了对象检测中的重大进步，大多数研究集中在设计越来越复杂的对象检测网络以提高准确性，例如SSD、R-CNN、Mask R-CNN和这些网络的其他扩展变体。该项目主要旨在使用最新的深度学习技术从发票中提取信息，可用于对象检测。这种深度卷积神经网络模型将被引入嵌入式对象检测。</p><h1 id="7d27" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">主要优势</strong></h1><blockquote class="kb kc kd"><p id="d8df" class="if ig ke ih b ii ij ik il im in io ip kf ir is it kg iv iw ix kh iz ja jb jc hb bi translated"><strong class="ih hj">降低成本</strong>:这有助于组织降低雇佣人力进行手工数据提取的成本。员工可以专注于其他生产性工作。</p><p id="a0b5" class="if ig ke ih b ii ij ik il im in io ip kf ir is it kg iv iw ix kh iz ja jb jc hb bi translated"><strong class="ih hj">减少错误:</strong>由于格式不同，从发票中提取信息很困难。此外，人为错误是另一个大问题，它会导致数据丢失和不准确。OCR有助于减少人为错误并使提取准确。</p><p id="b00f" class="if ig ke ih b ii ij ik il im in io ip kf ir is it kg iv iw ix kh iz ja jb jc hb bi translated"><strong class="ih hj">随时可用:</strong> OCR提取和验证过程不需要人工干预，一旦发票输入系统，它将从中提取文本，并以相同的流程将其推送到库存。</p><p id="cdb3" class="if ig ke ih b ii ij ik il im in io ip kf ir is it kg iv iw ix kh iz ja jb jc hb bi translated"><strong class="ih hj">安全性:</strong>完整的自动化提取流程为组织提供了数据级安全性，数据不容易被外界看到。</p></blockquote><h1 id="a609" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">研究当前文献</strong></h1><p id="7d10" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">深度学习中现代进步的出现导致了对象检测中的重大进步，大多数研究集中在设计越来越复杂的对象检测网络以提高准确性，例如SSD、R-CNN、Mask R-CNN和这些网络的其他扩展变体。本文主要研究利用卷积神经网络(CNN)进行图像中的目标检测。图像检测问题基于预定义的标签来预测图像的标签。基于假设，它选择图像中感兴趣的单个对象，并试图覆盖该图像上的重要部分。检测工作不仅仅是定义该对象的类别，而是定位该图像中对象的范围。</p><p id="7ef6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">先前用作对象检测方法的逐块方向直方图(SIFT或HOG)特征对对象的非常低级的特征进行编码，因此该技术适合于在不同标签之间进行正确区分。深度卷积神经网络成为图像中对象检测的最新技术。</p><h2 id="98b6" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">卷积神经网络架构</strong></h2><p id="fc6b" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">CNN架构类似于人脑中神经元的连接模式。CNN擅长使用不同的过滤器捕捉图像中的空间和时间依赖性。因此，为了理解图像的复杂性，可以使用CNN来训练网络。卷积网络由两个主要特征组成:特征学习(也称为隐藏层)包括卷积、ReLU和池化，以及分类层包括FC和Softmax。</p><p id="0026" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从技术上讲，在ConvNet中，每幅图像都经过一系列卷积层，这些卷积层具有多个内核或过滤器、池、全连接层，在网络的末端，它实现Softmax函数，使用概率值[0，1]对图像中的对象进行分类。图1描述了处理输入图像的CNN管道，并根据值对对象进行分类。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es lb"><img src="../Images/6429f2b8160c85f3d06275a50d391faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*5h0qSbokZlw5kD3SH2Vddg.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated"><em class="ln">CNN的架构(</em><a class="ae lo" href="https://www.datascience.com/blog/convolutional-neural-network" rel="noopener ugc nofollow" target="_blank">https://www . data science . com/blog/卷积神经网络</a> <em class="ln"> ) </em></figcaption></figure><h2 id="e231" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">基于区域的卷积网络(RCNN): </strong></h2><p id="5547" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">RCNN网络在图像中创建一堆边界框，并寻找这些框包含的任何对象的存在，而不是处理大量的区域。RCNN采用选择性搜索来创建包围盒或区域建议。选择性搜索在图像上抓取不同大小的窗口，并通过不同的比例、颜色、纹理和包围将相邻像素组合在一起。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es lp"><img src="../Images/114defc4f6c8d7940803392560448b22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*197tBmptFeInW6Ur1cQhVw.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">R-CNN架构(https://towardsdatascience . com/r-CNN-fast-r-CNN-faster-r-CNN-yolo-object-detection-algorithms-36d 53571365 e)</figcaption></figure><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es lq"><img src="../Images/8453eec5e0a26d160084548526026ab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*2VpVXQ1nsKbKs6oLD6eKzA.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">创建区域提案。(https://arxiv.org/abs/1311.2524。)</figcaption></figure><p id="962b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RCNN检测图像中对象的步骤如下:</p><ul class=""><li id="b51c" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated">迁移学习是深度学习范式中的一个关键概念。所以我们会考虑一个预训练的卷积神经网络，根据需要检测的类别重新训练模型的末端层。</li><li id="dcd9" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">在下一步中，我们将计算每个图像的感兴趣区域(ROI ),并对所有这些区域进行整形，以匹配CNN输入。</li><li id="3c4d" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">一旦计算出所有的区域，我们需要训练支持向量机(SVM ),以便以二元分类器的形式对目标和背景进行分类。</li><li id="6793" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">然后，我们将使用线性回归模型，以便为盒子输出更紧密的坐标。</li></ul><h2 id="08cc" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">更快-R-CNN (FRCN) </strong></h2><p id="8099" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">在更快的R-CNN上描述的最先进的对象检测框架，其基于深度卷积网络，并且包括区域提议网络(RPN)和对象检测网络。RPN和R-FCN网络都被训练为共享卷积层以进行快速计算和测试。RPN利用检测网络产生完整图像卷积特征，该检测网络实现几乎无成本的区域提议，其中每个对象提议具有作为输出的对象性分数。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es mf"><img src="../Images/b0e50ef90dab291e5fc607cdf54a480b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*jzcpX1Wd77pFZCz1cTE6hw.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">FRCNN架构—(https://towardsdatascience . com/deep-learning-for-object-detection-a-comprehensive-review-73930816 d8d 9)</figcaption></figure><h2 id="7516" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated">区域提案网络</h2><ul class=""><li id="bb20" class="lr ls hi ih b ii ki im kj iq mg iu mh iy mi jc lw lx ly lz bi translated">RNP取一个3×3的滑动窗口流过特征图并映射到一个较低的维度。</li><li id="0862" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">然后，它为每个滑动窗口位置生成k个不同形状大小的固定锚盒。</li><li id="3e44" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">一旦锚框生成，RPN计算锚框是对象的softmax概率。</li><li id="ed7f" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">然后，为了更好地拟合对象，执行边界框回归以调整锚。</li></ul><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es mj"><img src="../Images/841a0ab25a8d9b7bd6739d6232890522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*OaeJZrrkBTrhF2VZGuaiXQ.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">RPN架构—<a class="ae lo" href="https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/10/a-分步介绍-基础-对象-检测-算法-第一部分/ </a></figcaption></figure><p id="2dac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于我们有了所有的建议区域，下一步是直接馈送到快速R-CNN网络，该网络由一个汇集层、一些全连接层以及最后的softmax分类层和边界框回归器组成。</p><h2 id="ec55" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">单发多盒探测器(SSD) </strong></h2><p id="4c0f" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">2016年底Christian Szegedy在对象检测领域提出了单次多盒检测器，在COCO和PascalVOC标准数据集上的平均精度为74%。在训练期间，SSD只需要每个对象的输入图像和地面真相盒。它基于前馈卷积网络。它生成限定大小的边界框集合和这些框中存在的对象类实例的相应分数，以及产生最终检测结果的非最大值抑制步骤。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es mo"><img src="../Images/c974bcc680d3171f96fff39b15879207.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dxhv93OhhYpZyZDsUMA9mg.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">单发多箱探测器架构(【https://arxiv.org/pdf/1512.02325.pdf】T4</figcaption></figure><ul class=""><li id="fe1d" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated"><strong class="ih hj">架构:</strong> VGG-16是图像分类领域最强的网络之一，具有高性能和高质量。因此，基于VGG-16架构，Christian Szegedy设计了SSD架构，但从架构中丢弃了完全连接的层。为了逐渐减小每个后续层的输入大小，实现了一组辅助卷积层。下面提到的VGG-16架构。</li></ul><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es mp"><img src="../Images/385de4a9955b2926e5cfeca04ab94c90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*C6-bUnlSpQ-0D7z9fbKuBw.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">输入为224x224x3的VGG架构</figcaption></figure><ul class=""><li id="da57" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated"><strong class="ih hj">多盒检测器:</strong>基于Christian Szegedy多盒方法，设计了SSD的包围盒回归。它获得具有p个通道的尺寸为m×n的特征层(m×n×p)。<strong class="ih hj"> </strong>对于每个位置，我们有k个不同大小和长宽比的边界框。例如，人的垂直边界框和汽车的水平边界框。然后，它计算类分数和相对于原始地面真实边界框形状的4个坐标偏移。</li><li id="1224" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">MultiBox的损失函数有两个关键部分进入SSD，<strong class="ih hj">置信度损失:</strong>交叉熵用于测量多个类别置信度的softmax损失，以及<strong class="ih hj">定位损失:</strong>平滑L1用于计算预测框和地面真实框之间的损失，包括边界框的中心点(cx，cy)、宽度(w)和高度(h)的偏移</li></ul><blockquote class="kb kc kd"><p id="9380" class="if ig ke ih b ii ij ik il im in io ip kf ir is it kg iv iw ix kh iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="hi"> multibox_loss =置信度_loss + alpha *位置_loss </em> </strong></p></blockquote><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es mq"><img src="../Images/f36003844f66422a32cf45ba2c43e56c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nAeoZr6Q9SBDSzvpry9MBQ.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">用于定位和置信度的多个边界框(<a class="ae lo" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1512.02325.pdf</a>)</figcaption></figure><ul class=""><li id="cac2" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated"><strong class="ih hj">并集上的交集(IOU): </strong> IOU是一种评估技术，用于测量对象检测模型的准确性。我们需要找到IOU的组件是作为对象检测模型的输出的真实边界框和预测边界框。实际上，预测的边界框坐标不可能完全符合真实边界框坐标。因此，为了找到与基本事实框高度重叠的边界框，我们设置了一个IOU阈值。这确保了我们预测的边界框尽可能接近地匹配基本事实框。</li></ul><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es mr"><img src="../Images/34c95619b9bb7e529c6ae0b2a04af078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*AQ5DNZ7OxArh7b6eDRrwIg.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">Union上的交集(https://www . pyimagesearch . com/2016/11/07/intersection-Over-Union-iou-for-object-detection/)</figcaption></figure><ul class=""><li id="77ec" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated"><strong class="ih hj">非最大值抑制:</strong>在推断阶段，使用NMS修剪由模型生成的大部分边界框。通过设置置信度阈值0.01和IOU阈值0.5，我们可以过滤掉大部分框，仅保留N个预测。它有助于在推理过程中优化模型输出，并消除噪声较大的预测。</li></ul><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es ms"><img src="../Images/0cb9d3b068b5067446085a183efcd361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*svYr9cagVwmJSMuC1bcmww.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">非最大抑制示例(https://medium . com/@ yusuken/object-detection-1-NMS-ed 00d 16 fdcf 9)</figcaption></figure><h1 id="9df0" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">管道设计</strong></h1><h1 id="a48b" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">高层架构</strong></h1><p id="87ee" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">在这个用例的开始，我们使用了一些计算机视觉技术来识别PDF或图像文件中的表格结构，其中我们通过应用两个内核1来尝试边缘检测的形态学操作。内核检测水平线2。内核来检测垂直线。这种方法在表格不包含任何水平线或垂直线的情况下会失败。如今，结构提取是深度学习领域中的关键研究领域之一，因此我们决定进一步使用深度神经网络作为我们的用例。我们将此用例分为三个不同的部分1。使用深度学习技术的区域检测，2 .使用OCR工具从检测区域中提取文本。3.实施文本分析以识别提取的文本之间的关系，并将其转储到存储库中。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es mt"><img src="../Images/be49ccb9b1ee10083b9f3d47f09ab6e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kfjxujNv8njA0zJLtB3VNg.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">流水线架构</figcaption></figure><p id="f817" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">图像预处理:</strong>这里的图像是为训练和测试过程准备的。首先，我们使用(600x600x3)和300 DPI将PDF发票转换为JPG，然后使用第[6]节中提到的不同预处理技术。一旦所有图像都被收集和处理，它将被解析以用于深度学习模型的训练(正在使用FRCN和SSD)。</p><p id="a562" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">使用深度学习的结构提取:</strong>我们将提取部分分为两类，如下所述。</p><ul class=""><li id="2f16" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated"><strong class="ih hj">检测模型:</strong>一旦数据集准备好，我们将把它传递给检测模型，以便识别输入图像中的表格、段落和表格。我们目前正在处理FRCN和SSD模型，根据准确性，我们将选择最终使用案例的模型。所有的测量细节和比较将在最终文件中展示。</li><li id="587e" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">文本提取:管道中的下一个组件是文本提取。我们目前正在使用名为Tesseract-OCR的开源OCR工具从检测到的区域中提取文本。这是要提到的，提取模块正在进行中，我们可能会转移到其他方法，以获得更好的性能和准确性。</li></ul><p id="8045" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">文本分类:文本分类由两部分组成。1.使用自然语言处理技术将用于识别提取的文本作为键-值对和2之间的关系。用于计算最终输出精度的分类算法。</p><blockquote class="kb kc kd"><p id="ad5e" class="if ig ke ih b ii ij ik il im in io ip kf ir is it kg iv iw ix kh iz ja jb jc hb bi translated"><strong class="ih hj">开发环境:</strong> Anaconda3，Google云平台<strong class="ih hj">，</strong> protobuf-compiler，python-pil，python-lxml，python-tk</p><p id="7ffb" class="if ig ke ih b ii ij ik il im in io ip kf ir is it kg iv iw ix kh iz ja jb jc hb bi translated"><strong class="ih hj"> Python包:</strong> Cython，matplotlib，PIL (pillow)，tensorflow-gpu，keras，LabelImg，Imgaug，spaCy</p></blockquote><h1 id="2f52" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">训练数据集的准备</strong></h1><h2 id="bdc8" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">图像采集和准备</strong></h2><p id="4fdd" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">近1K的图像是从不同的来源收集的，如Google、Being和一些供应商发票，30%的图像用于测试和验证模型。为了加强模型的准确性，采用了图像增强技术。</p><ul class=""><li id="f59e" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated"><strong class="ih hj">图像归一化:</strong>图像归一化技术用于改变像素强度值的范围，以提高图像的对比度。直方图均衡或对比度拉伸机制广泛用于图像归一化。在这个用例中，我们采用了对比拉伸技术。</li></ul><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es mu"><img src="../Images/1f44d6d0da959a2a600876227c25cf78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qepZWWCAusW0eZmMGZVeiA.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图像标准化。原始拉伸与对比拉伸</figcaption></figure><ul class=""><li id="f835" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated"><strong class="ih hj">图像大小调整:</strong>我们用600x600像素的图像来训练深度神经网络。PIL python模块的Resize函数已经被用来调整图像的大小。</li><li id="c2ef" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><strong class="ih hj">图像转换:</strong>将所有图像转换为RGB(通道3)格式并编码为JEPG。OpenCV包用于转换。</li><li id="25d5" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><strong class="ih hj">每英寸点数(DPI)转换</strong>:在整个项目中，我们致力于高强度的图像，将所有图像转换为300 DPI。</li></ul><h2 id="455e" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">图像标注</strong></h2><p id="f562" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">图像标记是目标检测领域最重要的任务之一。我们已经使用基于GUI的标签工具<a class="ae lo" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank"> labelImg </a>手动标注了1000张图片，分为三类:段落、表格和表单。所有的注释都作为XML文件保存在不同的目录中，并且使用PascalVOC格式。创建一个自定义python脚本来准备所有XML文件的CSV文件，包含文件名、图像大小、边界框坐标和类，这些将在TFRecord创建期间使用。</p><p id="5350" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">XML文件包含关于图像和注释的所有细节。</p><ul class=""><li id="18d4" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated">图像位置。</li><li id="bdaf" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">尺寸和通道(RGB)</li><li id="359a" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">类别(段落、表格、表单)</li><li id="aac9" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">边界框坐标为xmin，ymin，xmax，ymax。</li></ul><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es mv"><img src="../Images/c3f4f135421fe55fa0d2a6ef0f7bafbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fN4DWTG0WOCGLlAqUfKz-w.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">使用标签的图像注释</figcaption></figure><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es mw"><img src="../Images/94da01f55d2a06fe7d81e05466125a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*7Aga3eCvaU1q6keygZg_Bg.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">带有边界框坐标的XML文件</figcaption></figure><h2 id="fc9a" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">图像增强</strong></h2><p id="e97c" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">为了在深度网络中取得良好的性能，我们需要大量的数据进行训练。由于我们只收集了1000幅图像，因此采用了图像增强技术来提高我们网络的性能。python库<a class="ae lo" href="https://github.com/aleju/imgaug" rel="noopener ugc nofollow" target="_blank"> imgaug </a>帮助我们生成增强图像以及边界框移动。在我们的使用案例中使用了以下增强器:</p><ul class=""><li id="ad40" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated"><strong class="ih hj">仿射平移:</strong>使用仿射平移技术，我们将图像在x轴上平移40像素，在y轴上平移60像素，并缩放至其原始大小的50–70%。这影响了边界框的位置，因此我们移动了增强图像的边界框坐标。</li><li id="3e70" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><strong class="ih hj">亮度:</strong>为了让图像更亮，我们将所有像素乘以(1.2，1.5)。</li><li id="563f" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><strong class="ih hj">高斯模糊</strong>:高斯内核用于以1.5的西格玛增强图像。</li><li id="d338" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><strong class="ih hj">水平翻转:</strong>与缩放1.0一起使用的增强器翻转器，用于水平翻转图像。</li></ul><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es mx"><img src="../Images/bc604ceed74a3b71da33f8ce9f727147.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRudLoTs8kW1gOQa9PoNmQ.png"/></div></div></figure><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es my"><img src="../Images/656212cd955c882d395319efd6fdaff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KaZbTqnhGd7NJf_wsIsTFw.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图像增强</figcaption></figure><h1 id="edcc" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">模型结构</h1><p id="ba12" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">如前所述，Fast-RCNN是RPN而不是选择性搜索和Fast-RCNN框架的结合。选择性搜索采用SIFT和HOG描述符来生成对象提议，在CPU上每幅图像需要2秒，其中，使用VGGnet或ResNext作为后端网络，RCNN的工作速度更快，为5fps(每秒帧数)。</p><p id="9e5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Keras提供不同的预训练模型，通过加载现有重量来生成定制模型。预训练模型在1000个类和25，636，712个参数的ImageNet数据集上训练。我们已经使用VGG16和ResNet50网络为我们的定制模型实现了作为独立特征提取器和权重初始化器的迁移学习，并且已经从互联网下载了预训练模型。这个模型可以在包括inception_v3的另一个网络上进行训练。</p><h2 id="bd64" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">优化</strong></h2><p id="492d" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">损失函数可以针对所有锚点进行优化，但是由于图像的最大部分不包含任何对象，因此它会偏向负样本。正如FRCN论文中提到的，我们随机抽样了256个锚，以计算正负锚比例高达1:1的小批量的损失函数。我们使用标准差为0.01的零均值高斯分布来初始化其他层。在PASCAL数据集上，我们使用0.001的初始学习率直到23k小批量，以及0.0001用于接下来的20k小批量。</p><h2 id="17bf" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">超参数</strong></h2><ul class=""><li id="8041" class="lr ls hi ih b ii ki im kj iq mg iu mh iy mi jc lw lx ly lz bi translated">初始学习率:前23k步0.001</li><li id="63c7" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">预定学习率:接下来20k步为0.0001。</li><li id="97e4" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">动量优化器值:0.9</li><li id="168f" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">使用移动平均线:假</li><li id="1b98" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">按规范进行渐变剪裁:10.0</li><li id="15c4" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">标准偏差:0.01</li><li id="00f4" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">纪元:25年</li><li id="bdf6" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">步骤/时期:1000</li><li id="f686" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">nms_iou_threshold : 0.8</li></ul><h2 id="36cd" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">最终损失计算</strong></h2><p id="9865" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">如前所述，我们已经用每个1000步的25个时期训练了该模型，并且最终损失被计算为RPN损失和检测损失。对于包围盒分类器，我们实现了88.9%的准确率。</p><h2 id="0779" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">激活层输出的可视化</strong></h2><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es mz"><img src="../Images/4263fc1b8cd4f17745119e389cfb449d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uOLjJuZpcmLwJJHcaO15DQ.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">激活层的可视化</figcaption></figure><h2 id="493b" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated">最终分割图像</h2><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es na"><img src="../Images/c342cab05e1f8f8e9f9a47fbf2ab9048.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*zJo_dFHasFnRg2W2PeJP6g.jpeg"/></div></figure><h1 id="8ba1" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">使用宇宙魔方提取文本:</strong></h1><p id="9c7b" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">光学字符识别是近二十年来计算机科学领域的热门话题之一。OCR检测图像上的文本内容，并将图像翻译成机器编码的文本供计算机处理。OCR涉及的步骤如下所述。</p><ul class=""><li id="133f" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated">图像被扫描并转换成位图，位图是黑白点的矩阵。</li><li id="8dea" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">为了提高精度，必须对图像进行亮度和对比度调整的预处理。</li><li id="0171" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">然后使用分割算法找到图像中文本所在的感兴趣区域。</li><li id="1321" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">现在感兴趣的区域可以被进一步分割成线条、单词和字符。然后，使用比较和检测算法进行字符匹配。</li></ul><p id="5848" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们选择了谷歌的<a class="ae lo" href="https://en.wikipedia.org/wiki/Tesseract_(software)" rel="noopener ugc nofollow" target="_blank"> Tesseract-OCR </a>引擎，该引擎最初是由惠普在20世纪80年代开发的，谷歌在2006年接管了它。Tesseract-OCR是基于深度学习的开源软件，它支持130种语言和超过35种脚本。我们使用的<a class="ae lo" href="https://pypi.org/project/pytesseract/" rel="noopener ugc nofollow" target="_blank">pytesseracat</a>是用于文本提取的Tesseract-OCR引擎的python包装器。</p><h2 id="5bd4" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">图像预处理</strong></h2><p id="7651" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">为了提高Tesseract-OCR的精度，需要对输入图像进行处理。有几种技术可用于图像的预处理，其中我们采用了一些基本技术，即灰度转换、大小调整和亮度调整。<strong class="ih hj"> </strong>我们使用python包Pillow(是Python图像库(PIL))进行图像预处理</p><h2 id="a123" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">内容提取</strong></h2><p id="d386" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">FRCN模型推理图接受图像并生成检测框、检测分数和检测类作为python字典，其中检测框保存边界框坐标。我们正在转换坐标并计算xmin、ymin、xmax、ymax和python OpenCV，以裁剪边界框并将其传递给<a class="ae lo" href="https://pypi.org/project/pytesseract/" rel="noopener ugc nofollow" target="_blank">pytesserac</a>进行文本提取。</p><blockquote class="kb kc kd"><p id="429e" class="if ig ke ih b ii ij ik il im in io ip kf ir is it kg iv iw ix kh iz ja jb jc hb bi translated">使用的Python包:Pytesseract，Numpy，Matplotlib，OpenCV。</p></blockquote><h1 id="9811" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">使用命名实体识别的文本分类</strong></h1><p id="a3d1" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">命名实体识别(<a class="ae lo" href="https://spacy.io/usage/training/#ner" rel="noopener ugc nofollow" target="_blank"> NER </a>)是自然语言处理中使用的文本分类技术之一，也称为实体识别、实体分块和实体提取。它旨在定位非结构化数据中描述的命名实体并将其分类为预定义的类别，如名称、国家、组织、货币等。NLTK和<a class="ae lo" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> SpaCy </a>是NER最常用的python包。我们使用SpaCy构建了NER模型，该模型支持英语、西班牙语、法语、意大利语、荷兰语和多种语言的NER。我们采用了OCR的输出，并训练了一个定制的NER模型，以便对以下实体进行分类。</p><ol class=""><li id="086e" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc nb lx ly lz bi translated">发票号</li><li id="56ce" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc nb lx ly lz bi translated">发票日期</li><li id="962d" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc nb lx ly lz bi translated">货币</li><li id="0816" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc nb lx ly lz bi translated">总数</li><li id="fba7" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc nb lx ly lz bi translated">采购订单</li></ol><h2 id="c1bc" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">数据集准备</strong></h2><p id="3e1b" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">我们使用了<a class="ae lo" href="https://github.com/ManivannanMurugavel/spacy-ner-annotator" rel="noopener ugc nofollow" target="_blank"> spaCy NER注释器</a>工具来标记从训练图像中提取的所有文本，它生成一个合并。包含所有注释的json文件。spaCy接受训练过程的元组列表，因此我们创建了python代码来将json数据转换为SpaCy格式。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es nc"><img src="../Images/71bf976beddeed486d79632a11b51809.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*By75LvDFk_3X6BM4oQAxrw.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">用于数据预处理的NER标注工具</figcaption></figure><h2 id="8560" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">培训NER模特</strong></h2><p id="2582" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">NER培训从创建一个空白模型开始，并将其添加到内置管道中。SpaCy小批量组件用于训练批次。因为我们正在训练一个新的模型，因此随机重置和初始化权重。模型经过200次迭代训练，下降0.5，小批量复合学习率为(4.0，32.0，1.001)。</p><h2 id="6ea2" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated">NER模型评估</h2><p id="0ded" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">我们已经使用spaCy内置类黄金和scorer API来评估出NER模型。精确度得分为93.33，召回率为94.27，F1得分为94.12。</p><h2 id="247d" class="kn je hi bd jf ko kp kq jj kr ks kt jn iq ku kv jr iu kw kx jv iy ky kz jz la bi translated"><strong class="ak">测试和验证</strong></h2><p id="2772" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">训练后，它将模型存储到预定义的目录中。经过训练的模型对从FRCN推理图中提取的输入文本进行分类，并随着标记化对上述实体进行分类。<a class="ae lo" href="https://spacy.io/usage/visualizers" rel="noopener ugc nofollow" target="_blank"> Displacy </a>组件已用于可视化分类输出。提取的文本存储到字典中，同样可以用于存储到CSV文件或直接推送到SAP系统。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es mt"><img src="../Images/1207ecbc42ff950c255758303020929c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oO9BMoJIO-kJ-z7jQl315g.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">可视化最终输出</figcaption></figure><h1 id="1d66" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">结论和未来工作</strong></h1><p id="3103" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">本文涵盖了为期16周的工作，其中我们研究了深度神经网络的不同先进模型，并选择了其中的几个用于我们的用例，我们已经完成了培训活动所需的所有数据预处理。我们已经建立了一个现实生活的数据集来训练模型。我们已经训练了用于区域检测的所有三个模型，并且基于其准确性和时间复杂性，决定继续使用FRCN。作为未来的范围，我们正在探索图形卷积神经网络，以更好地把握表格标题、行和单元格之间的关系，并建立一个图形模型以获得更好的准确性。</p><h1 id="1d53" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="e1b1" class="lr ls hi ih b ii ki im kj iq mg iu mh iy mi jc lw lx ly lz bi translated">并集上的广义交集(<a class="ae lo" href="https://arxiv.org/pdf/1902.09630.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></li><li id="f18f" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">SSD:单次多盒探测器(<a class="ae lo" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank">来源</a>)</li><li id="e721" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">用于精确对象检测和语义分割的丰富特征层次(<a class="ae lo" href="https://arxiv.org/abs/1311.2524" rel="noopener ugc nofollow" target="_blank">来源</a>)</li><li id="d662" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">更快的R-CNN:用区域提议网络实现实时目标检测(<a class="ae lo" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">来源</a></li><li id="d260" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">用于大规模图像识别的深度卷积网络(<a class="ae lo" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank">来源</a></li><li id="bc04" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">快速R-CNN ( <a class="ae lo" href="https://arxiv.org/abs/1504.08083" rel="noopener ugc nofollow" target="_blank">来源</a></li><li id="c4ab" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">用于语义分割的全卷积网络— ( <a class="ae lo" href="https://ieeexplore.ieee.org/document/7478072" rel="noopener ugc nofollow" target="_blank">来源</a>)</li><li id="42e1" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">SegNet:一种用于图像分割的深度卷积编码器-解码器架构</li><li id="74f5" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">深度学习模型中命名实体识别的最新进展综述(<a class="ae lo" href="https://www.aclweb.org/anthology/C18-1182.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></li><li id="cc8e" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">科学:生物医学自然语言处理的快速和健壮模型(<a class="ae lo" href="https://arxiv.org/pdf/1902.07669.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></li><li id="114f" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><a class="ae lo" rel="noopener" href="/@fractaldle/guide-to-build-faster-rcnn-in-pytorch-95b10c273439">https://medium . com/@ fractal dle/guide-to-build-faster-rcnn-in-py torch-95b 10 c 273439</a></li><li id="6d08" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><a class="ae lo" href="https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da" rel="noopener" target="_blank">https://towards data science . com/named-entity-recognition-with-nltk-and-spacy-8 C4 a7d 88 E7 da</a></li><li id="a067" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><a class="ae lo" rel="noopener" href="/@manivannan_data/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6">https://medium . com/@ manivannan _ data/how-to-train-ner-with-custom-training-data-using-spacy-188 E0 e 508 c 6</a></li><li id="3f56" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><a class="ae lo" href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6" rel="noopener" target="_blank">https://towards data science . com/activation-functions-neural-networks-1 CBD 9 F8 d 91d 6</a></li><li id="1ddc" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><a class="ae lo" href="https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/</a></li><li id="e20c" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><a class="ae lo" href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" rel="noopener" target="_blank">https://towards data science . com/review-fcn-semantic-segmentation-EB 8 c 9 b 50d 2d 1</a></li><li id="8830" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><a class="ae lo" href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" rel="noopener ugc nofollow" target="_blank">https://people . eecs . Berkeley . edu/~ Jon long/long _ shelhamer _ fcn . pdf</a></li><li id="e321" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><a class="ae lo" href="https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96" rel="noopener" target="_blank">https://towards data science . com/review-seg net-semantic-segmentation-e 66 F2 e 30 FB 96</a></li><li id="5479" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><a class="ae lo" href="https://nanonets.com/blog/how-to-do-semantic-segmentation-using-deep-learning/" rel="noopener ugc nofollow" target="_blank">https://nano nets . com/blog/how-do-semantic-segmentation-using-deep-learning/</a></li><li id="85f8" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><a class="ae lo" href="http://cvlab.postech.ac.kr/~bhhan/class/cse703r_2016s/csed703r_lecture6.pdf" rel="noopener ugc nofollow" target="_blank">http://cvlab . postech . AC . kr/~ bhhan/class/CSE 703 r _ 2016s/csed 703 r _ lecture 6 . pdf</a></li><li id="9496" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><a class="ae lo" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2016/11/07/intersection-over-union-iou-for-object-detection/</a></li></ul></div></div>    
</body>
</html>