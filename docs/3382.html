<html>
<head>
<title>Evolving with BERT: Introduction to RoBERTa</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">与伯特一起进化:罗伯塔简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/evolving-with-bert-introduction-to-roberta-5174ec0e7c82?source=collection_archive---------0-----------------------#2021-06-28">https://medium.com/analytics-vidhya/evolving-with-bert-introduction-to-roberta-5174ec0e7c82?source=collection_archive---------0-----------------------#2021-06-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if"><p id="9dd2" class="ig ih hi bd ii ij ik il im in io ip dx translated">再好，也总可以变得更好，这才是激动人心的地方。</p></blockquote><p id="c38b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ip hb bi translated">在这篇文章中，我将讨论“令人兴奋的部分”，即<a class="jn jo ge" href="https://medium.com/u/25aae929dbb1?source=post_page-----5174ec0e7c82--------------------------------" rel="noopener" target="_blank">脸书研究</a>人工智能机构如何修改现有<a class="jn jo ge" href="https://medium.com/u/be36e94a7e47?source=post_page-----5174ec0e7c82--------------------------------" rel="noopener" target="_blank">谷歌</a> BERT的培训程序，向世界证明总有改进的空间。</p><p id="91cd" class="pw-post-body-paragraph iq ir hi is b it jp iv iw ix jq iz ja jb jr jd je jf js jh ji jj jt jl jm ip hb bi translated">让我们看看一种用于预训练自然语言处理(NLP)系统(RoBERTa)的鲁棒优化方法的开发。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es ju"><img src="../Images/2a3a90232ed265db691ab587c3c1cc61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*E_2thD-NBl8stZ4P.jpg"/></div></div></figure><h1 id="aa5b" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">Google开源的BERT</h1><p id="2dd7" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">来自Transformers的双向编码器表示，或称为BERT，是谷歌在2018年发布的一种自我监督方法<strong class="is hj">。</strong></p><blockquote class="if"><p id="f540" class="ig ih hi bd ii ij lj lk ll lm ln ip dx translated">伯特是历史上最理解语言的工具/模型。它是免费的，而且非常通用，因为它可以解决大量与语言任务相关的问题。你在不知情的情况下利用了伯特。</p><p id="4757" class="ig ih hi bd ii ij lj lk ll lm ln ip dx translated">如果你用过谷歌搜索，你就已经用过BERT了</p></blockquote><h1 id="4ab0" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr lo kt ku kv lp kx ky kz lq lb lc ld bi translated">建筑:</h1><h2 id="4a18" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">变压器模型BERT的基础概念</h2><p id="7a5f" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">BERT基于变压器模型架构</p><p id="eb4f" class="pw-post-body-paragraph iq ir hi is b it jp iv iw ix jq iz ja jb jr jd je jf js jh ji jj jt jl jm ip hb bi translated">检查模型，就好像它是一个单一的黑匣子，机器翻译应用程序会把一种语言的句子翻译成另一种语言。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es mf"><img src="../Images/c18b31b01c1a12365177f75a86673763.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IJiWAHP7JDZFkWO_.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">执行机器翻译任务的变压器[ <a class="ae mk" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><ul class=""><li id="899f" class="ml mm hi is b it jp ix jq jb mn jf mo jj mp ip mq mr ms mt bi translated">基本Transformer由一个读取文本输入的编码器和一个为任务产生预测的解码器组成。</li></ul><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es mu"><img src="../Images/15bb802131d951090e74d14b7361a790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YjoWwblcP0y9IOTq.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">变压器的基本结构<a class="ae mk" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><ul class=""><li id="df30" class="ml mm hi is b it jp ix jq jb mn jf mo jj mp ip mq mr ms mt bi translated">由于BERT的目标是生成语言表示模型，所以只需要编码器部分。因此，<strong class="is hj"> BERT基本上是一个经过训练的变压器编码器堆栈</strong></li></ul><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es mv"><img src="../Images/b0e1fbe3eb21e2f988bb633ff806156a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*e-QE-c3jZewtMubA.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">编码器模块的基本结构<a class="ae mk" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="d72a" class="pw-post-body-paragraph iq ir hi is b it jp iv iw ix jq iz ja jb jr jd je jf js jh ji jj jt jl jm ip hb bi translated">要理解其机制的进一步实现，请参考我之前的博客</p><h2 id="26ef" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">伯特的训练</h2><p id="3fb4" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">在预训练期间，BERT使用两个目标:掩蔽语言建模和下一句预测。</p><ul class=""><li id="5bab" class="ml mm hi is b it jp ix jq jb mn jf mo jj mp ip mq mr ms mt bi translated"><strong class="is hj">屏蔽语言建模(</strong> <code class="du mw mx my mz b"><strong class="is hj">MLM</strong></code> <strong class="is hj"> ) </strong>基本上<strong class="is hj">屏蔽随机选择的15%的输入标记中的80%</strong>并使用其他标记来尝试预测屏蔽(丢失的单词)。</li></ul><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es na"><img src="../Images/ed9f668d86a1e50256546ca46a56dda1.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/0*7fxNNDYzIVPSZoTa.png"/></div></figure><ul class=""><li id="0b2a" class="ml mm hi is b it jp ix jq jb mn jf mo jj mp ip mq mr ms mt bi translated"><strong class="is hj">下一句预测(</strong> <code class="du mw mx my mz b"><strong class="is hj">NSP</strong></code> <strong class="is hj"> ) </strong>是一种<strong class="is hj">二元分类损失</strong>，用于预测两个片段是彼此跟随还是来自不同的文档以创建语义。</li></ul><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es nb"><img src="../Images/ae19d9d49716302b734094859c0fa126.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/0*fIJ71CLrjyhh799K.png"/></div></figure><h1 id="384c" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">B</strong>BERT优化的开始:RoBERTa简介</h1><h2 id="9e15" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated"><strong class="ak">BERT的改进空间</strong></h2><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es nc"><img src="../Images/cb684fd97bfb82b0c2e9933cab8d5c5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*5e3Y7sqvfCnG5Qk4.gif"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">BERT中改进的学习范围</figcaption></figure><h2 id="1600" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">BERT明显训练不足，以下区域属于修改范围。</h2><h2 id="c103" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated"><strong class="ak"> 1。BERT训练中的掩蔽:</strong></h2><p id="57bb" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">在数据预处理期间，屏蔽只进行一次，从而产生单个静态屏蔽。因此，相同的输入掩码在每个时期都被输入到模型中。</p><h2 id="f3b6" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated"><strong class="ak"> 2。下一句预测:</strong></h2><ul class=""><li id="1622" class="ml mm hi is b it le ix lf jb nd jf ne jj nf ip mq mr ms mt bi translated">BERT中使用的原始输入格式是<strong class="is hj">段对+NSP损耗。</strong></li><li id="6c3d" class="ml mm hi is b it ng ix nh jb ni jf nj jj nk ip mq mr ms mt bi translated">在这种情况下，每个输入都有一对片段，每个片段可以包含多个自然句子，但是总的组合长度必须少于512个标记。</li><li id="ea43" class="ml mm hi is b it ng ix nh jb ni jf nj jj nk ip mq mr ms mt bi translated">值得注意的是，单个句子会损害下游任务的性能，根据假设，这是因为该模型无法学习长期依赖关系，因此作者可以通过删除/添加NSP损失来实验，以查看模型性能的影响。</li></ul><h2 id="1633" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">3.文本编码:</h2><ul class=""><li id="2cd4" class="ml mm hi is b it le ix lf jb nd jf ne jj nf ip mq mr ms mt bi translated">最初的BERT实现使用大小为30K的字符级BPE词汇表。</li><li id="e937" class="ml mm hi is b it ng ix nh jb ni jf nj jj nk ip mq mr ms mt bi translated">BERT使用WordPiece方法，这是一种基于语言建模的字节对编码变体。</li></ul><h2 id="ac87" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">4.培训批量:</h2><p id="fd3b" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">最初，BERT被训练用于具有256个序列的批量大小的<strong class="is hj"> 1M步，这显示了在掩蔽语言建模目标的复杂度方面的改进空间。</strong></p><h1 id="efd5" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">改变培训程序:</h1><h2 id="4c3a" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">1.用动态屏蔽代替静态屏蔽:</h2><p id="1f01" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">为了避免多次屏蔽同一个单词，脸书使用了动态屏蔽；训练数据被重复10次，每一次，被屏蔽的单词将会不同，这意味着句子将是相同的，但是被屏蔽的单词将会不同。</p><h2 id="e166" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated"><strong class="ak"> 2。移除NSP : </strong></h2><p id="c57c" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated"><strong class="is hj">测试1:喂食以下交替训练形式。</strong></p><p id="4b6e" class="pw-post-body-paragraph iq ir hi is b it jp iv iw ix jq iz ja jb jr jd je jf js jh ji jj jt jl jm ip hb bi translated"><strong class="is hj"> 2.1。保留NSP损失:</strong></p><ul class=""><li id="9b6c" class="ml mm hi is b it jp ix jq jb mn jf mo jj mp ip mq mr ms mt bi translated"><strong class="is hj">句子对+NSP: </strong>每个输入包含一对自然句子，从一个文档或单独文档的连续部分取样。NSP损失得以保留。</li></ul><p id="75cf" class="pw-post-body-paragraph iq ir hi is b it jp iv iw ix jq iz ja jb jr jd je jf js jh ji jj jt jl jm ip hb bi translated"><strong class="is hj"> 2.2。移除NSP损失:</strong></p><ul class=""><li id="d49a" class="ml mm hi is b it jp ix jq jb mn jf mo jj mp ip mq mr ms mt bi translated"><strong class="is hj">完整句子:</strong>每个输入都包含从单个或交叉文档中连续采样的完整句子，因此总长度最多为512个标记。我们消除了NSP损失。</li><li id="1ee9" class="ml mm hi is b it ng ix nh jb ni jf nj jj nk ip mq mr ms mt bi translated"><strong class="is hj">文档句子</strong>:输入的构造类似于完整句子，除了它们不能跨越文档边界。在文档末尾附近采样的输入可能少于512个标记，因此我们在这些情况下动态增加批量大小，以实现与完整句子相似的标记总数。我们消除了NSP损失。</li></ul><h2 id="27eb" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">结果—</h2><ul class=""><li id="1346" class="ml mm hi is b it le ix lf jb nd jf ne jj nf ip mq mr ms mt bi translated">与具有NSP损耗的原始BERT相比，去除NSP损耗匹配或稍微改善了下游任务性能。</li><li id="44e8" class="ml mm hi is b it ng ix nh jb ni jf nj jj nk ip mq mr ms mt bi translated">如<em class="nl">表1所示，来自单个文档的序列(文档句子)比来自多个文档的打包序列(完整句子)表现稍好。</em></li></ul><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es nm"><img src="../Images/0f577efc5fa61e847974875d3998080a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/0*cJEC3OtdCe5B2ZjD.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">表1:有无NSP损耗模型的性能比较(图片取自<a class="ae mk" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">论文</a></figcaption></figure><h2 id="8789" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">3.大批量小批量培训:</h2><p id="68e5" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">值得注意的是，用大的小批量训练模型改善了MLM目标的复杂性和最终准确性。</p><blockquote class="if"><p id="ccae" class="ig ih hi bd ii ij lj lk ll lm ln ip dx translated">1M步骤，批量大小为256与31K步骤，批量大小为8K的计算成本相当。</p></blockquote><p id="5dea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ip hb bi translated">通过分布式并行训练，大批量也更容易并行化。</p><h2 id="2e3c" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">4.字节对编码:</h2><ul class=""><li id="b884" class="ml mm hi is b it le ix lf jb nd jf ne jj nf ip mq mr ms mt bi translated">这里对原始字节使用字节对编码，而不是Unicode字符。</li><li id="bacb" class="ml mm hi is b it ng ix nh jb ni jf nj jj nk ip mq mr ms mt bi translated">BPE的子词词汇量减少到50K(仍然大于伯特的词汇容量)单位。</li></ul><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es nn"><img src="../Images/1b97e9ded1a3ac9cbb08245e2e8b1295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/0*5qI5ioaSVV5YhU9-"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">字节对编码的一个简单例子(BPE) [ <a class="ae mk" href="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQCZAAiwMGZi20JrcMwe9Nn-L4Vn2enNZj4lA&amp;usqp=CAU" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="92bc" class="pw-post-body-paragraph iq ir hi is b it jp iv iw ix jq iz ja jb jr jd je jf js jh ji jj jt jl jm ip hb bi translated">尽管在某些情况下会降低最终任务的性能，但这种方法还是用于编码，因为它是一种通用的编码方案，不需要任何预处理和标记化规则。</p><h2 id="8053" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">5.增加训练数据:</h2><p id="c539" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">据观察，在更大的数据集上训练BERT极大地提高了它的性能。因此，训练数据增加到160GB的未压缩文本。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="ab fe cl no"><img src="../Images/7c07439a7fdd06be9c584a1e8a674472.png" data-original-src="https://miro.medium.com/v2/format:webp/0*FWXcIIW7vFqLsBRa.jpeg"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">谷歌的伯特看到修改后！<a class="ae mk" href="https://www.google.com/search?q=bert+meme+face&amp;tbm=isch&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwi4uv7VnrrxAhU-xHMBHRwTCGIQrNwCKAJ6BAgBEHE&amp;biw=1519&amp;bih=722#imgrc=3M9gWEsbSbq_NM&amp;imgdii=nOdlVxygRt-izM" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure></div><div class="ab cl np nq gp nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="hb hc hd he hf"><h1 id="3ff4" class="kg kh hi bd ki kj nw kl km kn nx kp kq kr ny kt ku kv nz kx ky kz oa lb lc ld bi translated">脸书的RoBERTa:一种预训练自监督NLP系统的优化方法</h1><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es ob"><img src="../Images/06f62bcab79f64d654073f4aaace4ee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*yZ7gFAzfRFa3Cua87DJE7w.gif"/></div></div></figure><p id="2192" class="pw-post-body-paragraph iq ir hi is b it jp iv iw ix jq iz ja jb jr jd je jf js jh ji jj jt jl jm ip hb bi translated">上面讨论的问题是由脸书人工智能研究所(FAIR)发现的，因此，他们提出了一个“优化”和“稳健”的BERT版本。</p><blockquote class="if"><p id="97fc" class="ig ih hi bd ii ij lj lk ll lm ln ip dx translated">罗伯塔实际上在NLU任务中表现强劲，脸书的绝对天才实际上做到了，这不是一个点击诱饵！</p></blockquote><blockquote class="oc od oe"><p id="f41d" class="iq ir nl is b it iu iv iw ix iy iz ja of jc jd je og jg jh ji oh jk jl jm ip hb bi translated">RoBERTa是脸书不断致力于推进自我监督系统发展的一部分，这种系统可以减少对时间和资源密集型数据标签的依赖。</p></blockquote><p id="946f" class="pw-post-body-paragraph iq ir hi is b it jp iv iw ix jq iz ja jb jr jd je jf js jh ji jj jt jl jm ip hb bi translated">它的行为更像是给输入的其余部分提供上下文(实际的上下文)，这就是为什么RoBERTa比实际上在诸如SQUAD等任务上经过微调的模型工作得更好的原因。</p><h1 id="ed72" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">为什么罗伯塔很重要？</h1><p id="9b8c" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">世界欣赏谷歌，因为他们让这个自然语言处理程序向世界开源。通过RoBERTA，我们看到BERT开源的举动给NLP带来了巨大的变化。</p><p id="4842" class="pw-post-body-paragraph iq ir hi is b it jp iv iw ix jq iz ja jb jr jd je jf js jh ji jj jt jl jm ip hb bi translated">该研究证明了充分预训练的模型如何在扩展到极限时导致琐碎任务的改善。</p><blockquote class="oc od oe"><p id="bb9c" class="iq ir nl is b it jp iv iw ix jq iz ja of jr jd je og js jh ji oh jt jl jm ip hb bi translated"><strong class="is hj">提高BERT的潜力，对市场和经济前景产生巨大影响。</strong></p></blockquote><ul class=""><li id="3567" class="ml mm hi is b it jp ix jq jb mn jf mo jj mp ip mq mr ms mt bi translated">BERT和RoBERTa用于<br/>NLP任务的改进，因为他们利用了上下文丰富的嵌入向量空间。</li><li id="eef7" class="ml mm hi is b it ng ix nh jb ni jf nj jj nk ip mq mr ms mt bi translated">使用RoBERTa对数据进行预处理，对于所有小型产品到大型跨国公司来说都是一个重大进步，因为主要工作是整合数据进行分析以提取信息。</li></ul><p id="8e68" class="pw-post-body-paragraph iq ir hi is b it jp iv iw ix jq iz ja jb jr jd je jf js jh ji jj jt jl jm ip hb bi translated">这是很重要的，因为作为这些研究、实验和发展的结果，我们正越来越接近NLP模型的更大挑战，即实现人类水平的语言理解！</p><h1 id="f94d" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">罗伯塔和伯特有什么不同</h1><h2 id="c325" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">罗伯塔的作者认为，伯特在很大程度上训练不足，因此，他们提出了以下相同的改进。</h2><ul class=""><li id="5bb4" class="ml mm hi is b it le ix lf jb nd jf ne jj nf ip mq mr ms mt bi translated">更多<strong class="is hj">训练数据</strong> (16G对160G)</li><li id="cca6" class="ml mm hi is b it ng ix nh jb ni jf nj jj nk ip mq mr ms mt bi translated">使用<strong class="is hj">动态屏蔽模式</strong>代替<strong class="is hj">静态屏蔽模式。</strong></li><li id="b4c2" class="ml mm hi is b it ng ix nh jb ni jf nj jj nk ip mq mr ms mt bi translated">将<strong class="is hj">下一句预测</strong>目标替换为<strong class="is hj">不带NSP的完整句子。</strong></li><li id="cfa3" class="ml mm hi is b it ng ix nh jb ni jf nj jj nk ip mq mr ms mt bi translated">关于<strong class="is hj">更长序列的训练。</strong></li></ul><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es oi"><img src="../Images/e41b7aba46d882be1bd8e37f79563a5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/0*z_N15lPs46CDIxpb"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">不同模型的精度与训练步骤数的关系图。[ <a class="ae mk" href="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRLIXLOFlu96piyukpXw3zCBlt5h5ohpuU4tw&amp;usqp=CAU" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="ab fe cl no"><img src="../Images/3f5344f6fcc2fea0f25dcbed87a01485.png" data-original-src="https://miro.medium.com/v2/format:webp/0*wJraZazbjkr2avUc.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">BERT、RoBERTa、DistilBERT和XLNet在培训策略方面的比较</figcaption></figure><h1 id="cc3a" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">和罗伯塔一起零距离学习</h1><h2 id="9d5b" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">零射击学习:</h2><p id="5dd2" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">这是一种机器学习技术，其中模型的使用不需要对特定任务进行微调。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es oj"><img src="../Images/335a2bddd658b486291f259f037ae639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*62JP-PqRYhycsUKu.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">解释零射击学习</figcaption></figure><h2 id="615d" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">您可以使用下面的示例来实现任何文本序列分类任务(一次性分类),只需按照以下步骤操作即可。它也广泛用于序列回归任务。</h2><h2 id="753d" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated"><strong class="ak">从torch.hub加载RoBERTa</strong></h2><p id="0885" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">使用Pytorch加载预训练的RoBERTa大型模型</p><pre class="jv jw jx jy fd ok mz ol om aw on bi"><span id="feb7" class="lr kh hi mz b fi oo op l oq or">import torch<br/>roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')<br/>roberta.eval()</span></pre><h2 id="ab47" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">Roberta用于序列分类:</h2><p id="290f" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">RoBERTa Model transformer在顶部有一个序列分类/回归头(在合并输出的顶部有一个线性层),例如用于粘合任务。</p><h2 id="d5bd" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">1.下载已经为MNLI微调的RoBERTa</h2><p id="0d31" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">RoBERTa还接受了MNLI(多体裁自然语言推理是一个由433k个句子对组成的众包集合，其中标注了文本蕴涵信息)的预训练。</p><pre class="jv jw jx jy fd ok mz ol om aw on bi"><span id="5c12" class="lr kh hi mz b fi oo op l oq or">roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')<br/>roberta.eval()  # disable dropout for evaluation</span></pre><h2 id="5195" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">2.对一对句子进行编码，并做出预测</h2><pre class="jv jw jx jy fd ok mz ol om aw on bi"><span id="325f" class="lr kh hi mz b fi oo op l oq or">tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')<br/>roberta.predict('mnli', tokens).argmax()</span></pre><h2 id="0850" class="lr kh hi bd ki ls lt lu km lv lw lx kq jb ly lz ku jf ma mb ky jj mc md lc me bi translated">3.编码另一对句子</h2><pre class="jv jw jx jy fd ok mz ol om aw on bi"><span id="952b" class="lr kh hi mz b fi oo op l oq or">tokens = roberta.encode(‘Roberta is a heavily optimized version of BERT.’, ‘Roberta is based on BERT.’)<br/>roberta.predict(‘mnli’, tokens).argmax()</span></pre><h1 id="1396" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">进一步讨论</h1><p id="0bae" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm ip hb bi translated">变形金刚非常占用内存。因此，在训练更大的模型或更长的时期时，我们很有可能会耗尽内存或超出运行时间限制。</p><p id="e081" class="pw-post-body-paragraph iq ir hi is b it jp iv iw ix jq iz ja jb jr jd je jf js jh ji jj jt jl jm ip hb bi translated">因此，在我的下一篇博客中，我将讨论和实施一些<strong class="is hj">有前途的、众所周知的、有影响力的开箱即用的策略</strong> <strong class="is hj">，用优化策略来加速transformer，以减少培训时间。</strong></p></div><div class="ab cl np nq gp nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="hb hc hd he hf"><h1 id="4af4" class="kg kh hi bd ki kj nw kl km kn nx kp kq kr ny kt ku kv nz kx ky kz oa lb lc ld bi translated">一起创造纽带、故事和魔法！</h1><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es os"><img src="../Images/5495a580c18e7b99b33118501fb2a4d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XTN2BDeDBcKTDkWX.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">那是我通过屏幕与你联系！<a class="ae mk" href="https://blog.textnow.com/blog/2020/04/23/distance-learning-how-textnow-is-helping-teachers-stay-connected-pt-2/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><blockquote class="if"><p id="b7c1" class="ig ih hi bd ii ij ik il im in io ip dx translated">我意识到技术正在发展，人们的思维变得更加好奇，结果也变得更加有趣。也正因为如此，我们才不断的学习和成长！</p></blockquote><p id="db98" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ip hb bi translated">祝贺你坚持到博客的最后，那就是我！</p><p id="cb79" class="pw-post-body-paragraph iq ir hi is b it jp iv iw ix jq iz ja jb jr jd je jf js jh ji jj jt jl jm ip hb bi translated">如果你今天喜欢学习RoBERTa，请在下面的评论中告诉我你使用的是什么技术，并随时回答任何问题。</p></div></div>    
</body>
</html>