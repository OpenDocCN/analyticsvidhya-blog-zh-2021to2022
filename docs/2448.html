<html>
<head>
<title>Basic Natural Language Processing in 30 min (NO BS)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">30分钟基本自然语言处理(无学士学位)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/basic-natural-language-processing-in-30-min-no-bs-38046abd919d?source=collection_archive---------8-----------------------#2021-04-24">https://medium.com/analytics-vidhya/basic-natural-language-processing-in-30-min-no-bs-38046abd919d?source=collection_archive---------8-----------------------#2021-04-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/087026c92ac90593a422bc0f39503804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A3V7EuHaS2Ey8pKV"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">由<a class="ae it" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马库斯·斯皮斯克</a>在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="8c86" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">数据来源:</strong> IMDB <strong class="iw hi"> </strong>烂番茄数据集</p><p id="90e3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae it" href="https://www.kaggle.com/ashirwadsangwan/imdb-dataset" rel="noopener ugc nofollow" target="_blank"> IMDb数据集| Kaggle </a></p><p id="c96a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">任务:</strong>开发一个分类器，根据给电影的评论判断电影是新鲜的还是烂片。</p><p id="3b83" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">预测模型:</strong>朴素贝叶斯</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="6652" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里最重要的支持功能之一是<strong class="iw hi"> <em class="jz">计数矢量器</em> </strong>，它将原始文本转换成‘单词包’矢量。</p><p id="6b77" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">根据定义，“BOW”向量是一种数据结构，它告诉你一个特定的单词在广告中出现了多少次。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="dd04" class="kj kk hh kf b fi kl km l kn ko">#demonstration on CountVectorizer and BOW output <br/>from sklearn.feature_extraction.text import CountVectorizer</span><span id="c557" class="kj kk hh kf b fi kp km l kn ko">text = ['machine learning rocks', 'machine learning rules', 'rocks rocks rules']</span><span id="d970" class="kj kk hh kf b fi kp km l kn ko">print("Original text is:-\n", '\n'.join(text))<br/>print()</span><span id="6b82" class="kj kk hh kf b fi kp km l kn ko">vectorizer = CountVectorizer(min_df=0)</span><span id="f34c" class="kj kk hh kf b fi kp km l kn ko"># call `fit` to build the vocabulary<br/>vectorizer.fit(text)</span><span id="28f0" class="kj kk hh kf b fi kp km l kn ko"># call `transform` to convert text to a bag of words<br/>x = vectorizer.transform(text)</span><span id="fa2f" class="kj kk hh kf b fi kp km l kn ko"># CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to <br/># convert back to a "normal" numpy array<br/>x = x.toarray()</span><span id="e18c" class="kj kk hh kf b fi kp km l kn ko">print("Transformed text vector is \n", x)<br/>print()<br/># `get_feature_names` tracks which word is associated with each column of the transformed x<br/>print("Words for each feature:-")<br/>print(vectorizer.get_feature_names())</span></pre><p id="788f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">运行上面的函数，预期的输出是:</p><figure class="ka kb kc kd fd ii er es paragraph-image"><div class="er es kq"><img src="../Images/71c4917896ae700e4a293d7501335df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*Byd1pCLQ6eiwAYiV6ALLDQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">“一袋单词”</figcaption></figure></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="0d0b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，假设我们有X矩阵(nreview，nwords)和y向量(nreview)输入，两者都是数组:</p><ul class=""><li id="6e80" class="kr ks hh iw b ix iy jb jc jf kt jj ku jn kv jr kw kx ky kz bi translated">x:每行对应于一个评论(输入)的一个单词包</li><li id="25ca" class="kr ks hh iw b ix la jb lb jf lc jj ld jn le jr kw kx ky kz bi translated">y:编码评论是新的(1)还是坏的(0)(输出)</li></ul><p id="c826" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="jz">注意</em> </strong> <em class="jz">:确保X和y在行上有相同的尺寸(nreview) </em></p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="d841" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下一步是像这样执行<strong class="iw hi">测试列车分割</strong> (80%/20%):</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="03ef" class="kj kk hh kf b fi kl km l kn ko">X_train, X_test, y_train, y_test= train_test_split(X, Y, test_size= .2, random_state = 42, stratify= Y)</span></pre></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="3139" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">实现<strong class="iw hi"> <em class="jz">多项式的时间</em> </strong>(只使用训练集训练你的模型)，这里我们使用<code class="du lf lg lh kf b">accuracy_score</code>作为目标函数。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="ac0f" class="kj kk hh kf b fi kl km l kn ko">from sklearn.metrics import accuracy_score</span><span id="0508" class="kj kk hh kf b fi kp km l kn ko">clf = MultinomialNB()<br/>clf.fit(X_train, y_train)</span><span id="0d5e" class="kj kk hh kf b fi kp km l kn ko">#predictions<br/>y_train_pred = clf.predict(X_train)<br/>y_test_pred = clf.predict(X_test)</span><span id="5cc2" class="kj kk hh kf b fi kp km l kn ko">#accuracy score <br/>train_pred_score = accuracy_score(y_train, y_train_pred)<br/>test_pred_score = accuracy_score(y_test, y_test_pred)</span><span id="d031" class="kj kk hh kf b fi kp km l kn ko">print('Training Set Accuracy Score: \n', (100 * train_pred_score))<br/>print('Testing Set Accuracy Score: \n', (100 * test_pred_score))</span></pre><figure class="ka kb kc kd fd ii er es paragraph-image"><div class="er es li"><img src="../Images/0a22ee77f4faa380a86ea7f84581c635.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*OoO6AYwHPimO3y92u_bhnQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">clf模型的输出</figcaption></figure><p id="6ef8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">瞧，这简直太简单了，不是吗！</p><p id="3569" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">但是，我们希望优化模型。我们的方法有几个<strong class="iw hi">超参数</strong>，这些是我们想要使用<strong class="iw hi">交叉验证</strong>来调整的值。</p><ol class=""><li id="b5e0" class="kr ks hh iw b ix iy jb jc jf kt jj ku jn kv jr lj kx ky kz bi translated"><code class="du lf lg lh kf b">min_df</code>在<em class="jz">计数矢量器中，</em>忽略出现在少于<em class="jz"> min_df </em>部分评论中的单词。</li><li id="43e0" class="kr ks hh iw b ix la jb lb jf lc jj ld jn le jr lj kx ky kz bi translated"><code class="du lf lg lh kf b">alpha </code>在<em class="jz">多项式</em>中，被称为“平滑参数”——增加该值会降低对任何单个特征的敏感度，并倾向于将预测概率拉近50%</li></ol></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="4f28" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在我们进行交叉验证之前，我们需要理解什么定义了一个“更好”的参数值。</p><p id="05aa" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">目标:</strong>找出使我们的数据的对数似然最大化的值。</p><figure class="ka kb kc kd fd ii er es paragraph-image"><div class="er es lk"><img src="../Images/399546702bc5ff21858d6b4f3b8f4e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*DG_vE5ojy4Nm-zGGNjQZfQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">最大后验估计</figcaption></figure><p id="ed85" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这种情况下，我们有目标函数:</p><p id="37ff" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">L = Sum _ fresh(logP(fresh))+Sum _ rotate(logP(rotate))</strong></p><blockquote class="ll lm ln"><p id="2d65" class="iu iv jz iw b ix iy iz ja jb jc jd je lo jg jh ji lp jk jl jm lq jo jp jq jr ha bi translated">其中<code class="du lf lg lh kf b">Sum_fresh</code>表示所有新评论的总和，<br/>和<code class="du lf lg lh kf b">Sum_rotten</code>表示所有烂评论的总和</p></blockquote><p id="75d1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">为什么使用log_likelihood: </strong></p><p id="98b8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因为我们计算的是一堆非常小的数字(又名。概率)，我们假设所有的事件都是相互独立的(乘法)；<strong class="iw hi">对数转换将使计算更容易，并避免最终结果过于接近0。</strong></p><p id="1037" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有了这种直觉，我们需要一个类似log _的函数，它就在这里！</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="3de0" class="kj kk hh kf b fi kl km l kn ko">def log_likelihood(model, x, y):<br/>    prob = model.predict_log_proba(x)<br/>    rotten = y == 0<br/>    fresh = ~rotten<br/>    return prob[rotten, 0].sum() + prob[fresh, 1].sum()</span><span id="8407" class="kj kk hh kf b fi kp km l kn ko"># output the likelihood of test data <br/>log_likelihood(clf, X_test, y_test)</span></pre></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="91d1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在你有了所有的部分，是时候给你看一些代码了！</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="03af" class="kj kk hh kf b fi kl km l kn ko">#potential tuning values for hyper parameters<br/>alphas = [0, 1, 5, 10, 50]<br/>min_dfs = [0.0001,0.001,0.01, 0.1, 0.2]</span><span id="d918" class="kj kk hh kf b fi kp km l kn ko">#Find the best value for alpha and min_df, and the best classifier<br/>best_alpha = None<br/>best_min_df = None<br/>max_loglike = -np.inf</span><span id="34ae" class="kj kk hh kf b fi kp km l kn ko">for alpha in alphas:<br/>    <br/>    print('alpha is : ' + str(alpha))<br/>    <br/>    for min_df in min_dfs:<br/>        <br/>        print('min_df is : ' + str(min_df))<br/>        <br/>        #initialize vecotizer <br/>        vectorizer = CountVectorizer(min_df = min_df)<br/>        X = vectorizer.fit_transform(x)<br/>        <br/>        X = X.toarray()<br/>        Y = y.to_numpy()       <br/>        <br/>        #train test split<br/>        X_train, X_test, y_train, y_test= train_test_split(X, Y, test_size= .2, random_state = 42, stratify= Y)<br/>        <br/>        #initialize NB model<br/>        clf = MultinomialNB(alpha = alpha)</span><span id="3f42" class="kj kk hh kf b fi kp km l kn ko">        #cross validating the NB model using 5 cv<br/>        score = cross_val_score(clf, X_train, y_train, cv=5, scoring = log_likelihood)<br/><br/>        print('log scores: {:.2f}'.format(np.mean(score)))<br/>        <br/>        #updating the best parameters<br/>        if np.mean(score) &gt; max_loglike:<br/>            max_loglike = np.mean(score)<br/>            best_alpha = alpha<br/>            best_min_df = min_df</span><span id="3004" class="kj kk hh kf b fi kp km l kn ko">print("best alpha, best min_dfs: ",best_alpha, best_min_df)</span></pre><figure class="ka kb kc kd fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/68395cf4ca6f55c96dc0bedc27157194.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*Yv63A3JGCbKTmjqOEjTs2A.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">部分输出</figcaption></figure><p id="fa36" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，我们可以使用<code class="du lf lg lh kf b">best_alpha</code>、<code class="du lf lg lh kf b">best_min_df</code>、<em class="jz">、</em>训练最终模型，并重新评估训练集和测试集的准确性。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="5767" class="kj kk hh kf b fi kl km l kn ko">best_alpha = 5<br/>best_min_df = 0.001</span><span id="06cb" class="kj kk hh kf b fi kp km l kn ko">#final vectorizer model and X, Y data<br/>vectorizer_final = CountVectorizer(min_df = best_min_df)</span><span id="01a1" class="kj kk hh kf b fi kp km l kn ko">X = vectorizer_final.fit_transform(X)<br/>X = X.toarray()<br/>Y = y.to_numpy()</span><span id="8fa0" class="kj kk hh kf b fi kp km l kn ko">#train-test split<br/>X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(X, Y, test_size= .2, random_state = 42, stratify= Y_final)</span><span id="7e04" class="kj kk hh kf b fi kp km l kn ko">#fit NB model on the training datasets<br/>clf_final = MultinomialNB(alpha = best_alpha)<br/>clf_final.fit(X_train_final, y_train_final)</span><span id="02b5" class="kj kk hh kf b fi kp km l kn ko">#predict y on training data<br/>y_train_pred = clf_final.predict(X_train_final)</span><span id="edb0" class="kj kk hh kf b fi kp km l kn ko">#predict y on testing data<br/>y_test_pred = clf_final.predict(X_test_final)</span><span id="d2e7" class="kj kk hh kf b fi kp km l kn ko">#accuracy score for training and testing predictions<br/>test_pred_score = accuracy_score(y_test_final, y_test_pred)<br/>train_pred_score = accuracy_score(y_train_final, y_train_pred)</span><span id="63ae" class="kj kk hh kf b fi kp km l kn ko">print('Training set Accuracy Score: \n', (100 * train_pred_score))<br/>print('Testing set Accuracy Score: \n', (100 * test_pred_score))</span></pre><figure class="ka kb kc kd fd ii er es paragraph-image"><div class="er es ls"><img src="../Images/ca7ca9c057d46bec50d49749ea788f08.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*AeFqF3c-Fm5GBgDZHUcL4g.png"/></div></figure></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="54a0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，我们有了基本的朴素贝叶斯模型，这是一个简单但有效的模型。</p><blockquote class="ll lm ln"><p id="213e" class="iu iv jz iw b ix iy iz ja jb jc jd je lo jg jh ji lp jk jl jm lq jo jp jq jr ha bi translated">寓意是，总是先尝试简单的事情。</p></blockquote><p id="6b3f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以尝试传入一个评论，并要求模型预测电影是“烂”还是“新”。</p><p id="033c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这个例子中，我们使用的测试评论是:<em class="jz">“这部电影在任何方面都不显著、不感人或不精彩”</em></p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="ad5f" class="kj kk hh kf b fi kl km l kn ko">#predict the probability of a given review</span><span id="6b96" class="kj kk hh kf b fi kp km l kn ko">def predict_prob(s, model=clf_final):<br/> s_tf = vectorizer_final.transform([s]).toarray()<br/> proba_s = model.predict_proba(s_t)<br/> <br/> return proba_s</span><span id="f1dd" class="kj kk hh kf b fi kp km l kn ko">predict_prob("This movie is not remarkable, touching, or superb in any way")</span></pre><figure class="ka kb kc kd fd ii er es paragraph-image"><div class="er es lt"><img src="../Images/b1748df00eed0b663e40885ce1a357c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*noPdIxb1Rx3iXSAgDSi9WA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">输出概率</figcaption></figure><p id="3960" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">给定概率通讯员【烂，鲜】，这个模型实际上输出了一个错误的答案；因为正面词的情绪压倒了负面关键词‘不’。像“但是”、“不是”等词语。行动起来否定文字的情绪。</p><p id="d496" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然而，因为朴素贝叶斯将每个单词分开处理，所以它不能捕捉这种单词交互。</p><blockquote class="ll lm ln"><p id="f4fa" class="iu iv jz iw b ix iy iz ja jb jc jd je lo jg jh ji lp jk jl jm lq jo jp jq jr ha bi translated">属性之间的依赖不可避免地降低了朴素贝叶斯辨别正在发生什么的能力。</p></blockquote></div></div>    
</body>
</html>