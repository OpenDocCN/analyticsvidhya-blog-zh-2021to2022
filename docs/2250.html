<html>
<head>
<title>Multi-Dimensional Data— boon or bane?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多维数据——好事还是坏事？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multi-dimensional-data-boon-or-bane-3de73c9650bb?source=collection_archive---------12-----------------------#2021-04-14">https://medium.com/analytics-vidhya/multi-dimensional-data-boon-or-bane-3de73c9650bb?source=collection_archive---------12-----------------------#2021-04-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="a447" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">PCA的‘为什么，什么&amp;如何)</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/b88a7ae2604184c29747d4c63dd77397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*_yZEWcT7VBtNM2PLXg1-ww.gif"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:——【megaputer.com】</figcaption></figure><p id="605d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据的可用性无需介绍。不用说，如今数据驱动着商业。对于给定的业务问题，我们捕获的特征越多，分析就越好。不是吗？或者是？</p><p id="9737" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果，…</p><blockquote class="ju jv jw"><p id="af42" class="ie if jc ig b ih ii ij ik il im in io jx iq ir is jy iu iv iw jz iy iz ja jb ha bi translated">1.维数/特征比观测数多？</p><p id="ccae" class="ie if jc ig b ih ii ij ik il im in io jx iq ir is jy iu iv iw jz iy iz ja jb ha bi translated">2.维度太多，有时候会碰到上百个甚至更多？</p></blockquote><p id="2af9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上面的场景定义了一种叫做<strong class="ig hi"> <em class="jc">维度诅咒</em> </strong>的特殊现象，简单来说就是太多的特性实际上可能是一个问题。但是，怎么做呢？</p><p id="6c93" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">比方说，我们试图找出一个员工是否会流失。我们有员工的年龄、教育程度、经验、当前工作级别、月收入等信息。在这里，我们已经知道，年龄越高，经验越多；经验越多，职位级别越高，月收入越高。因此，如果我们说，初级管理层的员工容易流失，这也意味着，经验较少的员工可能会流失。正确吗？类似地，加薪百分比和绩效等级也是相关的；工作满意度和在同一家公司工作的年数也可能相关等等。这种特征(或自变量)之间的相关性，即特征之间的相互关系，称为<strong class="ig hi"><em class="jc"/></strong>。</p><p id="fe2f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">特征(指数据集中的列)，也称为独立/预测变量，用于预测目标/因变量/响应变量。上例中的目标变量是“员工是否会流失—流失(是|否)”。</em></p><p id="fb72" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果一些特征传达了相同的信息，我们能不能把它们作为多余的东西丢掉？<em class="jc">(只保留一列，去掉多余的其他列)。</em>在某些情况下，是的，我们可以——丢弃它们不会(极大地)影响模型的预测。准确地说，我们只选择最相关的特征。这就是所谓的，<strong class="ig hi"> <em class="jc">【特征选择】</em> </strong>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ka"><img src="../Images/23cf0df8263ef1551eb9a1bccbb52aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*cBQ4Ckkqg7xiaA2cTxblbw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><strong class="bd kb"> <em class="kc">关联热图</em> </strong></figcaption></figure><p id="ff3e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">左侧图像显示了样本数据中特征(X1至X13)的相关性。<strong class="ig hi"> <em class="jc">相关系数范围从-1到+1，其中极值表示最大相关，0表示不相关。</em> </strong></p><p id="9d4e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PS:-相关不代表因果关系。更多的睡眠有助于你表现得更好；更多的睡眠和表现可能相关；当一个增加时，另一个也可能增加。但是，更多的睡眠不会导致(有助于)更好的表现。这同样适用于上述示例。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kd"><img src="../Images/699a0032e58980b6d9553c6ce46b038e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xcb0LrXn_8TT7ejUYQ2XYA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:——【quantdare.com/】</figcaption></figure><p id="a496" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于特征选择，我们应该知道哪些特征是不重要的，哪些是贡献最小的。也有可能在删除它们之后，多重共线性仍然存在。那么，如果我们能创造一组新的衍生变量，是原始变量的线性组合，会怎么样呢？这样的技术叫做<strong class="ig hi"> <em class="jc">【特征提取】</em> </strong>。</p><blockquote class="ju jv jw"><p id="fcc8" class="ie if jc ig b ih ii ij ik il im in io jx iq ir is jy iu iv iw jz iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="hh"> PCA(主成分分析)</em> </strong>是一种用于特征提取的无监督算法。派生特征是以彼此不相关的方式创建的，因此，每个特征都传达唯一的信息。虽然目的之一是避免信息中的冗余，但PCA也旨在减少维度，即减少要分析的特征的数量。</p></blockquote><p id="c1a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个回归的例子，其中我们试图预测的值是连续的<em class="jc">(不像上面例子中的离散标签)。我们想预测给定地区的房价(目标变量)。影响这一点的因素可能是地块大小、污染程度、可达性等。</em></p><p id="b4d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">原始方程将是</p><p id="e5ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">房价= w0 + w1地块面积+ w2污染程度+ w3可达性+ …。</em></p><p id="a9cb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中，w1、w2、w3 —每个特征的相应系数/权重。在其他条件不变的情况下，每增加一个单位的土地面积，房价就会上涨1倍。</p><p id="cd66" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在PCA之后，相同的等式变成</p><p id="43f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">房价=β0+β1 PC1+β2 PC2+β3 PC3+……</em></strong></p><p id="b730" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在哪里，</p><p id="3238" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PC1、PC2、PC3 —主要组件或新功能</p><p id="ac8a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">β1，β2，β3 —主成分的权重/系数</p><blockquote class="ju jv jw"><p id="bfe8" class="ie if jc ig b ih ii ij ik il im in io jx iq ir is jy iu iv iw jz iy iz ja jb ha bi translated">无论有无PCA，预测价格变化不大。但是，如果没有PCA，其中一个特征的单位增加可能会影响所有相关变量的权重，因此，不可能确定特征的重要性，即每个特征对目标变量的贡献大小。特征的可解释性降低。</p></blockquote></div><div class="ab cl ke kf go kg" role="separator"><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj"/></div><div class="ha hb hc hd he"><h1 id="15d2" class="kl km hh bd kb kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">PCA的概念:-</h1><p id="2043" class="pw-post-body-paragraph ie if hh ig b ih li ij ik il lj in io ip lk ir is it ll iv iw ix lm iz ja jb ha bi translated"><em class="jc">主成分建立在数据中的方差代表数据中的信息这一事实上。</em>方差为零的特征，不携带任何信息，可以认为是常数。</p><p id="4e36" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上面的例子中，随着工作级别的增加，月收入也会增加。我们假设月收入没有方差。这样，月收入不会因工作级别的不同而变化，因此，无论工作级别如何，月收入都可以被视为一个常数。</p><p id="9c17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正是由于数据中存在这种固有的差异，才能进行分析并提取信息。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ln"><img src="../Images/38e487330a8ba068b07000f511484a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*VRJFmqJJbwotrCiUKsezww.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:——<a class="ae jt" href="https://docs.tibco.com" rel="noopener ugc nofollow" target="_blank">docs.tibco.com</a></figcaption></figure><p id="2e2d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PCA捕捉数据的最大方差。具有n个维度/特征的数据集将产生n个主成分。获得最大方差的组件是PC1，方差最小的组件是PCn。主成分形成为彼此正交，即它们是不相关的<em class="jc">(无线性关系)</em>，因此，每个成分携带关于数据的有用信息。<em class="jc">由于目标之一是降低维度，我们选择k个主成分，使得k &lt; n和它捕获数据</em>中总方差的至少70%和高达90%。</p></div><div class="ab cl ke kf go kg" role="separator"><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj"/></div><div class="ha hb hc hd he"><h1 id="d4d9" class="kl km hh bd kb kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">PCA的步骤:-</h1><ol class=""><li id="dce7" class="lo lp hh ig b ih li il lj ip lq it lr ix ls jb lt lu lv lw bi translated">标准化功能:-</li><li id="8f9f" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">构建协方差矩阵:-</li><li id="e19b" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">特征分解:-</li><li id="7b25" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">对特征对排序:-</li><li id="ee02" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">选择最佳主成分:-</li></ol><h1 id="81f1" class="kl km hh bd kb kn mc kp kq kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh bi translated">1.标准化功能:-</h1><p id="7344" class="pw-post-body-paragraph ie if hh ig b ih li ij ik il lj in io ip lk ir is it ll iv iw ix lm iz ja jb ha bi translated">由于PCA捕捉数据中的方差，特定特征中固有的巨大方差可能会误导。</p><p id="4b76" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">比方说，年龄在30到60岁之间。然而，月收入可能从10K卢比到2L卢比不等。月收入的方差明显高于年龄的方差。如果特征没有被标准化，月收入自动成为携带最大差异的特征<em class="jc">(在年龄&amp;月收入)</em>。也是出于同样的原因，离群值将被删除(或在下一步之前处理)。初级管理层中50岁以上的人或更年轻的人成为首席执行官(因此，薪水更高，等等)。)被认为是该数据中的异常值。</p><p id="0577" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc"> PCA偏向方差较大的特征，得到的主成分将不能传达原始信息。</em>T11】</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mh"><img src="../Images/576d61b5f6d4e8f9e0d644a457899aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rwxCNbVlcO4hFYoOL37oMQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:——<a class="ae jt" href="https://cybernetist.com" rel="noopener ugc nofollow" target="_blank">cybernetist.com</a></figcaption></figure><blockquote class="ju jv jw"><p id="5252" class="ie if jc ig b ih ii ij ik il im in io jx iq ir is jy iu iv iw jz iy iz ja jb ha bi translated">使用<strong class="ig hi"> <em class="hh"> z得分</em> </strong>对特征进行标准化，其中从观察值中减去平均值，以使数据围绕轴居中，然后除以标准偏差，使它们处于相同的范围内。</p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mi"><img src="../Images/862ece88387f2671c5bbb02e9aff2c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*bhXuaoqHrvHK_jezl-j8MA.png"/></div></figure><p id="b06c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中，x-特征的观测值</p><p id="74f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">—特征的平均值</p><p id="0f7d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">σ —平均值周围的标准偏差</p><h1 id="8de3" class="kl km hh bd kb kn mc kp kq kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh bi translated">2.协方差矩阵:-</h1><p id="851e" class="pw-post-body-paragraph ie if hh ig b ih li ij ik il lj in io ip lk ir is it ll iv iw ix lm iz ja jb ha bi translated"><em class="jc">协方差矩阵显示了特征之间的相互作用。它告诉我们一个特性的可变性对另一个特性的影响有多大。</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mj"><img src="../Images/65b28024deba745d69861f2f10837104.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*4GcOOcDshMIkgQ9QjSC1dQ.png"/></div></figure><p id="a847" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中，Z-标准化数据，ZT-标准化数据的转置。</p><p id="39c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">为什么协变而不是相关？:- </strong></p><p id="e78b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">协方差衡量两个变量之间线性关系的方向，可以是成正比(正)或成反比(负)。另一方面，相关性不仅衡量方向，还衡量两个变量之间线性关系的强度。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mk"><img src="../Images/8e2ae05d554acc0ce95090bc9fc65a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*Ez7WxjdQSG4VLBnCRXJk2A.png"/></div></figure><p id="0ba6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">协方差是方差的一种扩展，其中考虑了两个变量相对于各自均值的偏差和积，而不是观测值相对于均值的偏差平方和。它以平方单位来度量，因此范围在-∞和+∞之间。比例的变化会影响协方差。当实际值被标准化时，协方差降低到-1 &lt; cov &lt; 1.</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/1ce95f41ccbfa2f2a533a19fafecce12.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*PxB8plUtzlyhla-vF4olVA.png"/></div></figure><p id="97df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Correlation is already standardized, because the calculation involves dividing covariance by the product of standard deviations of the variables. Correlation does not have any units and ranges between -1 to 1. Hence, <strong class="ig hi">相关性，标准化变量的协方差将为(大约。)一样。</strong></p><p id="38fb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"><em class="jc">【COV(缩放数据)= CORR(缩放数据)= CORR(未缩放数据)</em> </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mm"><img src="../Images/a3f038a92ac80278a27d507a201da548.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*Ru-gTXde1_j7QA6uq6n5OQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><strong class="bd kb">协方差矩阵</strong></figcaption></figure><p id="bf78" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">COV(X，Y) <em class="jc">(在左图中)</em>表示X &amp; Y一起变化了多少。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mn"><img src="../Images/a50b5d62daaaa70d84caabc193eca151.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*ERI2DmvX36aMAmrY5XHTnw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:——<a class="ae jt" href="https://www.analyticsvidhya.com" rel="noopener ugc nofollow" target="_blank">analyticsvidhya.com</a></figcaption></figure><p id="1414" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"><em class="jc">【COV(X，Y) = COV(Y，X) </em> </strong></p><blockquote class="ju jv jw"><p id="6535" class="ie if jc ig b ih ii ij ik il im in io jx iq ir is jy iu iv iw jz iy iz ja jb ha bi translated">协方差矩阵的<strong class="ig hi">对角线表示特征的方差——它是同一特征与其自身或特征与其自身相互作用的协方差，并且<strong class="ig hi">始终为1。</strong>由于对角元素始终为1，因此<strong class="ig hi">数据中的总方差(信息)=协方差矩阵中对角元素的总和=数据中变量/特征/维度的数量。</strong></strong></p></blockquote><h1 id="d3c9" class="kl km hh bd kb kn mc kp kq kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh bi translated">3.特征分解:-</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mo"><img src="../Images/0b41a7f8d338a16ec28e79352fd7ab4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V7jTcG0Vfp1jYdDLBZWaew.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:cybernetist.com</figcaption></figure><p id="dbee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">先前转换为协方差矩阵的原始缩放和居中数据中的信息绕轴旋转，以形成数据的主分量(新维度)。主成分又称为<strong class="ig hi"> <em class="jc">特征向量</em> </strong>，决定最大方差的方向。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mp"><img src="../Images/cd024d9487c0eddd72450919beec59db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*vPrXiibNsELirZurYoEOXQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:——<a class="ae jt" href="https://www.analyticsvidhya.com" rel="noopener ugc nofollow" target="_blank">analyticsvidhya.com</a></figcaption></figure><blockquote class="ju jv jw"><p id="8d88" class="ie if jc ig b ih ii ij ik il im in io jx iq ir is jy iu iv iw jz iy iz ja jb ha bi translated"><em class="hh">最大方差(x-max — x-min)又称为</em> <strong class="ig hi">信号</strong>(如左图)<em class="hh">形成最大方差的方向，作为PC1。该信号指示捕获的变化量，也称为</em> <strong class="ig hi"> <em class="hh">特征值</em> </strong> <em class="hh">。</em></p></blockquote><p id="ba2f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">残差方差(y-max — y-min)称为<strong class="ig hi"> <em class="jc">噪声</em> </strong>。信噪比(SNR)如下所示。更好的SNR表明捕捉最大方差(数据中的信息)的能力，因此，会产生更好的模型。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mq"><img src="../Images/5572bcc5c42cca70a7810375831ea51f.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*Zp8Udzdb31Uq-f-nwr9y0g.png"/></div></figure><p id="ebb6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然残差方差被归类为PC1的噪声，但它包含关于其余数据的信息(方差),并成为第二主分量的信号。数据中n个特征/维度的所有n个主成分都以相同的方式构建。</p><h1 id="16da" class="kl km hh bd kb kn mc kp kq kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh bi translated">4.对特征对排序:-</h1><p id="ab08" class="pw-post-body-paragraph ie if hh ig b ih li ij ik il lj in io ip lk ir is it ll iv iw ix lm iz ja jb ha bi translated">本征向量和本征值总是成对出现，分别表示方向和大小。特征值将按降序排序，以获得从最高到最低的方差，各个特征向量给出主分量。</p><p id="9a14" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">解释了&amp;累计方差:- </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mr"><img src="../Images/0d657e39dc7849da4ff7b8438ccd7c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*3ZWn7KFIuWI0Nwi_4t7Wpg.png"/></div></figure><p id="2cfe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">解释方差</em> </strong>是每个主成分捕捉到的变异性的百分比。在<strong class="ig hi"> <em class="jc">碎石图</em> </strong> <em class="jc">(绘制在左图所示的样本数据上)</em>中，第一主成分捕获了大约45%的可变性。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ms"><img src="../Images/6208b00599129cef0422514ef689364b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lMYef1kPs7tzIax5I00ayQ.png"/></div></div></figure><p id="e306" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">累积方差</em> </strong>是从PC1捕捉到的各个主成分的总方差百分比。最后一个PC捕获的累积方差表示所有PC的方差百分比之和，即它捕获了数据中的总方差，即100%。</p><h1 id="d804" class="kl km hh bd kb kn mc kp kq kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh bi translated">5.最佳主成分:-</h1><p id="3d98" class="pw-post-body-paragraph ie if hh ig b ih li ij ik il lj in io ip lk ir is it ll iv iw ix lm iz ja jb ha bi translated">选择的<strong class="ig hi"> <em class="jc">件数&lt;总尺寸(特征数)</em> </strong>。这个想法是用较少数量的个人计算机来解释数据中的大量差异。</p><p id="d0ac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">I .如果k是所选择的主成分的数量，那么PC(k)预期捕获数据中总方差的至少70%和高达90%。但是，这可能会随着数据或域的变化而变化。</p><p id="d538" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">二。另一种确定主成分数量的方法是找出两个连续成分之间累积方差的增量。如果k和k+1个分量的两个累积方差之差小于10%，则取k个分量，否则取k+1个分量。</p><p id="f549" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">三。还有一种方法是在“碎石图”中寻找一个拐点，超过这个拐点，线就变得近似水平。</p><blockquote class="ju jv jw"><p id="b59e" class="ie if jc ig b ih ii ij ik il im in io jx iq ir is jy iu iv iw jz iy iz ja jb ha bi translated">在上图中，为了解释大约70%的方差，选择四个主成分就足够了。然而，为了解释90%的方差，要选择7个主成分。</p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mt"><img src="../Images/de5931fc42d73c7a278ffa36aed3b3a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*Lb35Ur_VdbR-LoZVlQkS4Q.png"/></div></figure><ol class=""><li id="2f22" class="lo lp hh ig b ih ii il im ip mu it mv ix mw jb lt lu lv lw bi translated">最初的相关变量<em class="jc">(如上面的相关热图所示)</em>现在被转换成不相关的主成分。</li><li id="5144" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">特征的数量从13个减少到7个，每个特征携带关于数据的唯一信息。</li></ol><p id="b035" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">python中<em class="jc"> (google colab) </em>对<a class="ae jt" href="https://www.kaggle.com/heptapod/uci-ml-datasets" rel="noopener ugc nofollow" target="_blank"> <em class="jc">波士顿房价数据集</em> </a>的PCA实现，在这里 可用<a class="ae jt" href="https://colab.research.google.com/drive/1VWikUA30KvZrzUzdESa21UUagX6IetKQ#scrollTo=SpftnOsaq0Yf" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl ke kf go kg" role="separator"><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj"/></div><div class="ha hb hc hd he"><h1 id="56c1" class="kl km hh bd kb kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">最后的想法:-</h1><p id="4733" class="pw-post-body-paragraph ie if hh ig b ih li ij ik il lj in io ip lk ir is it ll iv iw ix lm iz ja jb ha bi translated">由于特征高度相关，减少了特征数量后，PCA有助于显著提高模型性能。然而，解释主要成分(衍生特征)并不简单。重要的是要记住，在进行PCA之前，必须对数据进行标准化。必须小心选择主成分的正确数量，以便在丢失的信息量和模型精度之间有一个可接受的折衷。</p><p id="097a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然PCA可以解决这个问题，但是当数据具有多个维度时，将数据收集限制在相关/最佳的特征上也同样重要，这样可以有效地利用资源。但是，要收集多少数据(观察值)？这是另一个博客的故事。敬请期待！</p></div></div>    
</body>
</html>