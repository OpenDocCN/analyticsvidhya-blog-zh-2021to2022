<html>
<head>
<title>It’s All About Regression — Ordinary Least Squares (OLS)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这都是关于回归——普通最小二乘法(OLS)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/its-all-about-regression-part-1-c002fcaa8a55?source=collection_archive---------13-----------------------#2021-02-04">https://medium.com/analytics-vidhya/its-all-about-regression-part-1-c002fcaa8a55?source=collection_archive---------13-----------------------#2021-02-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9ba8" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">在这个由多个博客组成的系列中，我们将研究回归模型。这将与你可能在网上看到的许多其他回归教程/讲座不同，因为我们将从多个角度讨论这个主题</p><p id="2068" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">本系列与众不同之处在于<br/> -真实世界示例和应用<br/> -预测分析<br/> -推理分析(几乎总是被遗忘的东西)<br/> -此外，我们将研究普通最小二乘法之外的回归(OLS)</p><p id="01da" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">因此，没有任何进一步的麻烦，让我们开始吧。</p><h1 id="e654" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">线性回归</strong></h1><p id="fa04" class="pw-post-body-paragraph ig ih hi ii b ij kc il im in kd ip iq ir ke it iu iv kf ix iy iz kg jb jc jd hb bi translated">线性回归是一种统计模型，它映射了预测因子/特征和响应变量之间的关系。它表示如下</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/6f6642ef68f9e9fc83438dfe6b5e3be6.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*4TDnI-Kc-UNKxGTX9-JpUQ.png"/></div></figure><p id="ee47" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">在哪里</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kp"><img src="../Images/24c4a432580b12bbf993baa2a39c10af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*Uvwa-OsFQPYUYF4UfU3g5w.png"/></div></figure><p id="93c3" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">出于本教程的目的，将误差项<em class="kq">ε</em>视为与<em class="kq"> E </em>(残差)相同。两者在被观测(E，属于可观测样本，可约化)和不被观测(ε，属于总体，常被称为不可约误差)方面是不同的。</p><p id="7ca5" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">要获得更具体的理解，请点击下面的链接。<a class="ae kr" href="https://en.wikipedia.org/wiki/Errors_and_residuals" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Errors_and_residuals</a></p><p id="90bb" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">线性回归模型试图通过降低成本函数来找到betas的最优值。成本函数可以基于不同的方法/算法而变化。要获得详细的解释，请查看这个关于最常用成本函数的博客。</p><h1 id="ed50" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">普通最小二乘法</h1><p id="99a4" class="pw-post-body-paragraph ig ih hi ii b ij kc il im in kd ip iq ir ke it iu iv kf ix iy iz kg jb jc jd hb bi translated">普通最小二乘法，或OLS，是一种估计回归模型参数的方法。它试图通过降低成本函数来估计贝塔系数；即响应变量和由模型做出的预测之间的平方距离之和。</p><p id="b000" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">OLS的成本函数定义为</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ks"><img src="../Images/2300d7d5be8ea547ea30e5d6b33c4d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZiTFgXN--N8Wvi-pzDWXbQ.png"/></div></div></figure><p id="5bb4" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">在哪里，</p><p id="6c13" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">n =观察次数</p><p id="a7be" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">p =特征数量</p><p id="4e0e" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">为了说明的目的，让我们假设我们有一个数据集如下</p><pre class="ki kj kk kl fd kx ky kz la aw lb bi"><span id="e9ed" class="lc jf hi ky b fi ld le l lf lg">X = [1,2,3,4,5,6,7,8,9]</span><span id="ffc3" class="lc jf hi ky b fi lh le l lf lg">Y = [3,5,7,9,11,13,15,17,19]</span></pre><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es li"><img src="../Images/8ea1231215c805fe7f7de977825b0cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ChLO0271PzJmMdvAoYk0fg.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">输入数据</figcaption></figure><p id="27ac" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">我们需要找到X和y之间的关系，让我们看看能否用OLS来做这件事。</p><p id="9105" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">由于线性回归属于所谓的参数族，我们需要定义一组我们希望OLS过程估计的参数(或者X和Y之间的关系)。因此，一般来说，由于我们有一个特征X和一个响应Y，我们需要找到X和Y之间的关系，这样我们就可以估计给定X时Y的值。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ln"><img src="../Images/872c2e231dd01964a6e9ed1993318f2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*_yktxUDux8B9Fr6xcQ5kLQ.png"/></div></figure><p id="c483" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">在这里，我们希望OLS估计beta_{0}和beta_{1}的值，这样我们就可以绘制X和y之间的关系</p><p id="0c92" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">OLS试图用一种叫做梯度下降的方法来估计贝塔系数。在这个博客的范围内，我们关注线性回归的应用方面，因此不会深入研究梯度下降算法。现在把梯度下降想象成一种算法，其中你给出方程(成本函数),它给出一个β值，对于任何特定的关系，它把成本函数减到最小。为了更好地理解梯度下降，看看这个博客。</p><p id="946f" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">在python中，我们利用<em class="kq"> statsmodel </em>包来使用OLS。它负责成本函数及其背后的数学的所有优化，我们需要给出的只是输入特征和响应变量。OLS函数将拟合模型并估计参数。</p><pre class="ki kj kk kl fd kx ky kz la aw lb bi"><span id="026e" class="lc jf hi ky b fi ld le l lf lg">model = sm.OLS(Y,sm.add_constant(X)).fit()</span></pre><p id="83f4" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">我们符合OLS模型，让我们来看看摘要。</p><pre class="ki kj kk kl fd kx ky kz la aw lb bi"><span id="1706" class="lc jf hi ky b fi ld le l lf lg">model.summary()</span></pre><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lo"><img src="../Images/5afb4c549ec7d342f133f7e3fe51c735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X2iluOMCe8wnisCYu79Ikg.png"/></div></div></figure><p id="6e8e" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">这里我们可以在<strong class="ii hj">系数</strong>(系数)中看到，模型估计为</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lp"><img src="../Images/402144ed31ce8a4bb3e2b463beadd2d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:180/format:webp/1*UeLQ2IPDOkoaqZzvAi-Yjw.png"/></div></div></figure><p id="91b8" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">因此映射如下:</p><pre class="ki kj kk kl fd kx ky kz la aw lb bi"><span id="fff9" class="lc jf hi ky b fi ld le l lf lg">Y = 1 + ( 2 * X )</span></pre><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lq"><img src="../Images/e0b4fa8c72d3263b8384318d0db6cdcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pdbW9_IlKTLoUeCBMc-tBw.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">预测与输入数据</figcaption></figure><p id="9776" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">在给定X的情况下，模型在预测Y的值方面似乎做得非常好，所以现在如果我们给模型一个x = 67的值，这是模型从未见过的，它应该会得出一个值<br/> 1 + (67 * 2) = 135</p><h1 id="153a" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">预言；预测；预告</h1><p id="e6dc" class="pw-post-body-paragraph ig ih hi ii b ij kc il im in kd ip iq ir ke it iu iv kf ix iy iz kg jb jc jd hb bi translated">假设当X为[67，45，555，123]时，您想要获得Y的预测值</p><pre class="ki kj kk kl fd kx ky kz la aw lb bi"><span id="35b9" class="lc jf hi ky b fi ld le l lf lg">X_pred = [67,45,555,123]<br/>Y_pred = model.predict(sm.add_constant(X_pred))</span></pre><p id="e8ee" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">上面命令的输出将是一个数组[ 135，91，1111，247]</p><p id="4f3e" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="kq">恭喜</em> </strong>，那是你的第一个机器学习模型。<br/>尽管线性回归是一个非常简单的模型，但如果它所依据的假设成立，它就会变得非常强大。</p><h1 id="60b1" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">假设</h1><p id="63a4" class="pw-post-body-paragraph ig ih hi ii b ij kc il im in kd ip iq ir ke it iu iv kf ix iy iz kg jb jc jd hb bi translated">无论模型多么简单，它都需要满足以下假设才能按预期工作<br/> -线性:模型假设特征X和响应Y之间存在线性关系<br/> -残差的正态性:残差/误差是独立的且呈正态分布<br/> -恒定可变性:残差的方差是恒定的，同质性</p><p id="538c" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">接下来，我们将深入探讨成本函数和梯度下降算法。在这里查看<a class="ae kr" href="https://vachan15.medium.com/its-all-about-regression-gradient-descent-f827de988036" rel="noopener"/>。</p><p id="18e9" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">在那之前，继续摇摆吧！</p></div></div>    
</body>
</html>