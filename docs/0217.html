<html>
<head>
<title>Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-c6625caf9e8e?source=collection_archive---------22-----------------------#2021-01-08">https://medium.com/analytics-vidhya/linear-regression-c6625caf9e8e?source=collection_archive---------22-----------------------#2021-01-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="4f71" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">单个和多个因变量</p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/f3e79389adccdf0dd1c519670626f804.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*o6pEqb0O3zLYL0P5.jpg"/></div></figure><p id="83a6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">回归基本上意味着当你想预测一个连续的数据。回归分析是预测建模技术的一种形式，它研究因变量和自变量之间的关系。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jr"><img src="../Images/a80cd25e8c392a6314921e50fc5ca769.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WtiS5iLDy9JvvN6X.jpeg"/></div></div></figure><p id="d455" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">当对自变量集合𝐱 = (𝑥₁，…，𝑥ᵣ)上的某个因变量𝑦实施线性回归时，其中𝑟是预测值的数量，您假设𝑦和𝐱: 𝑦之间存在线性关系=θ₀+θ+θ。这个方程就是<strong class="ik hi">回归方程</strong>。θ₀、θ₁、…、θᵣ为<strong class="ik hi">回归系数</strong>，𝜀为<strong class="ik hi">随机误差</strong>。</p><p id="f30f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">这是一种监督学习的形式，因为出于训练目的，我们将得到x和y。</p><blockquote class="ie if ig"><p id="da8b" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">例如，我们希望在给定学生学习时间(单一特征)的情况下预测他们的分数，或者在给定许多特征(如面积、离市场的远近、医院和许多其他特征)的情况下预测房价。</p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jw"><img src="../Images/52409635046c557d8a320007f189b885.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/0*agiAHIke3d6BXFKf.png"/></div></figure><p id="1463" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">我们的任务是在给定大量数据点的情况下找到最佳拟合线。等式<strong class="ik hi"> <em class="ij"> y=mx+c </em> </strong> <em class="ij"> </em>其中<strong class="ik hi"> y是我们的因变量</strong>，它将借助于<strong class="ik hi">自变量即x进行预测</strong>这里，我们的梯度/斜率表示为m - <em class="ij">是x沿直线单位变化的y变化，截距表示为c </em> <em class="ij">是直线与y轴相交点处的y值。</em></p><p id="0134" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">我们如何学习参数？</p><ul class=""><li id="28d3" class="jx jy hh ik b il im ip iq jo jz jp ka jq kb jf kc kd ke kf bi translated">我们使用训练数据来训练我们的模型</li><li id="245b" class="jx jy hh ik b il kg ip kh jo ki jp kj jq kk jf kc kd ke kf bi translated">然后我们学习一种算法或假设</li><li id="c7d1" class="jx jy hh ik b il kg ip kh jo ki jp kj jq kk jf kc kd ke kf bi translated">利用这个假设，我们完成了预测的目标。</li></ul><p id="80fe" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">H</span>T22】ow可以是选择最佳路线吗？</p><p id="f6d4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated"><strong class="ik hi">可能有许多可能的线，并且每条线将具有不同的θ。</strong></p><blockquote class="ie if ig"><p id="9b12" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">θ=[ θ₀ θ₁ ……θᵣ]其中，θ₀为c(截距)，θ₁为m(斜率)，其他θ值用于其他要素。</p></blockquote><p id="05b5" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">最佳拟合可以通过减少误差(损失)来获得，误差可以通过预测点和实际点之间的差异来测量。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es ku"><img src="../Images/31f43a102539abdd48728adc51a839a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*B0RVNAiyOUNrnzy1.jpg"/></div></div></figure><p id="156b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">我们可以用均方差来求成本函数。</p><p id="bc0b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">所以，我们的任务将是减少这个误差(损失)函数。我们用J(θ)来表示。</p><h1 id="827a" class="kv kw hh bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">梯度下降</h1><p id="7a6a" class="pw-post-body-paragraph ih ii hh ik b il lt in io ip lu ir is jo lv iv iw jp lw iz ja jq lx jd je jf ha bi translated">我们将使用梯度下降法来最小化J(θ)。一般来说，当我们想找到最小点时，我们会找到函数的导数。所以用简单的语言来说，梯度下降就是寻找最小值的迭代方法。</p><blockquote class="ie if ig"><p id="f248" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated"><strong class="ik hi">梯度下降</strong>是一种寻找可微函数局部极小值的一阶迭代优化算法。其思路是在当前点的函数的<strong class="ik hi">梯度</strong>(或近似<strong class="ik hi">梯度</strong>)的反方向重复步进，因为这是<strong class="ik hi">最陡下降</strong>的方向。</p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es ly"><img src="../Images/f2f28ab782905b7d390c96b032491c8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fhw-aMgGEFbpAYCH.jpg"/></div></div></figure><p id="9091" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">所以我们将取一个<em class="ij">学习率，它是步长</em> <strong class="ik hi"> α </strong>(它是一个常数)，并继续减去它，直到我们达到最小值。在我们达到最小值之后，偏导数将变为零。当函数是凸的时，初值无关紧要。而在非凸函数中，初值也会起作用。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lz"><img src="../Images/33646b001225106c6a9ef9630cc9a516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/0*Ao8IiCYH8tjFi8IF.jpg"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">来源:GeeksForGeeks</figcaption></figure><p id="f301" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">程序:</p><ol class=""><li id="197f" class="jx jy hh ik b il im ip iq jo jz jp ka jq kb jf me kd ke kf bi translated">随机初始化θ，或者我们可以用0来初始化</li><li id="c989" class="jx jy hh ik b il kg ip kh jo ki jp kj jq kk jf me kd ke kf bi translated">不断更新θ，直到我们得到最佳拟合。</li><li id="69f7" class="jx jy hh ik b il kg ip kh jo ki jp kj jq kk jf me kd ke kf bi translated">应用梯度下降，得到θ</li><li id="4a73" class="jx jy hh ik b il kg ip kh jo ki jp kj jq kk jf me kd ke kf bi translated">应用假设获得预测值。</li></ol><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="d0bc" class="mk kw hh mg b fi ml mm l mn mo">def hypothesis(x,theta):<br/>    y_ = 0.0<br/>    n = x.shape[0]<br/>    for i in range(n):<br/>        y_ += (theta[i]*x[i])<br/>    return y_</span><span id="294a" class="mk kw hh mg b fi mp mm l mn mo">def error(X,y,theta):<br/>    e=0.0<br/>    m = X.shape[0]<br/>    <br/>    for i in range(m):<br/>        y_ = hypothesis(X[i],theta)<br/>        e += (y[i] - y_)**2<br/>    return e/m</span><span id="5fe4" class="mk kw hh mg b fi mp mm l mn mo">def gradient(X,y,theta):<br/>    m,n = X.shape<br/>    grad = np.zeros((n,))<br/>    # for all values of j<br/>    for j in range(n):<br/>        # sum over all examples<br/>        for i in range(m):<br/>            y_ = hypothesis(X[i],theta)<br/>            grad[j] += (y_ - y[i])*X[i][j]<br/>    # out of loops<br/>    return grad/m</span><span id="7b70" class="mk kw hh mg b fi mp mm l mn mo">def gradient_descent(X,y,learning_rate=0.1,max_epochs=300):<br/>    m,n = X.shape<br/>    theta = np.zeros((n,))<br/>    error_list = []<br/>    <br/>    for i in range(max_epochs):<br/>        e = error(X,y,theta)<br/>        error_list.append(e)<br/>        <br/>        # Gradient Descent<br/>        grad = gradient(X,y,theta)<br/>        for j in range(n):<br/>            theta[j] = theta[j] - learning_rate*grad[j]<br/>    return theta,error_list</span></pre><blockquote class="mq"><p id="08a0" class="mr ms hh bd mt mu mv mw mx my mz jf dx translated">这个方法相当慢！！我们可以通过在代码中进行矢量化来改善这一点。</p></blockquote><pre class="na nb nc nd ne mf mg mh mi aw mj bi"><span id="d95e" class="mk kw hh mg b fi ml mm l mn mo">def hypothesis(X,theta):<br/>    return np.dot(X,theta)</span><span id="4d85" class="mk kw hh mg b fi mp mm l mn mo">def error(X,y,theta):<br/>    error =0.0<br/>    y_ = hypothesis(X,theta)<br/>    m = X.shape[0]<br/>    error = np.sum((y-y_)**2)<br/>    <br/>    return error/m<br/>def gradient(X,y,theta):<br/>    y_ = hypothesis(X,theta)<br/>    grad = np.dot(X.T,(y_-y))<br/>    m = X.shape[0]<br/>    <br/>    return grad/m</span><span id="81d7" class="mk kw hh mg b fi mp mm l mn mo">def gradient_descent(X,y,learning_rate=0.1,max_iter=300):<br/>    n = X.shape[1]<br/>    theta = np.zeros((n,))<br/>    error_list = []<br/>    <br/>    for i in range(max_iter):<br/>        e = error(X,y,theta)<br/>        error_list.append(e)<br/>        <br/>        # Gradient Descent<br/>        grad = gradient(X,y,theta)<br/>        theta = theta - learning_rate*grad<br/>        <br/>    return theta,error_list</span></pre><p id="b742" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">这里我们执行相同的操作，但是使用numpy函数来提高代码的速度。</p><h1 id="5dbb" class="kv kw hh bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">R2分数</h1><blockquote class="ie if ig"><p id="72d1" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">什么是线性模型的拟合优度？— R2Score</p></blockquote><p id="da69" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">r平方是数据与拟合回归线接近程度的统计度量。它也被称为决定系数，或多元回归的多重决定系数。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nf"><img src="../Images/90eb85c1abe8f4f27daccd3657d88b6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/0*5gdQGz-cAGOMk8qH.png"/></div></figure><p id="1ec6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">r平方始终介于0和100%之间:</p><ul class=""><li id="29ab" class="jx jy hh ik b il im ip iq jo jz jp ka jq kb jf kc kd ke kf bi translated">0%表示该模型不能解释响应数据在其平均值附近的任何可变性。</li><li id="f675" class="jx jy hh ik b il kg ip kh jo ki jp kj jq kk jf kc kd ke kf bi translated">100%表示模型解释了响应数据围绕其平均值的所有可变性。</li></ul><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="5d9d" class="mk kw hh mg b fi ml mm l mn mo">def r2Score(y,y_):<br/> num = np.sum((y-y_)**2)<br/> deno = np.sum((y-y.mean())**2)<br/> score = (1-num/deno)<br/> return score*100</span></pre><p id="8cdc" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">一般来说，R平方越高，模型就越符合您的数据。</p><p id="92d6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jo iu iv iw jp iy iz ja jq jc jd je jf ha bi translated">希望这篇文章给你解释了大部分的线性回归。</p></div></div>    
</body>
</html>