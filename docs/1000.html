<html>
<head>
<title>Using DeepStack for Sign Language Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用DeepStack进行手语对象检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/using-deepstack-for-sign-language-object-detection-4478a6e7a2e6?source=collection_archive---------17-----------------------#2021-02-09">https://medium.com/analytics-vidhya/using-deepstack-for-sign-language-object-detection-4478a6e7a2e6?source=collection_archive---------17-----------------------#2021-02-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/9d5fb64a5ea0e7b0b2a7e2ee9a5fcee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*87ZljnpTL78vytS1WgIB1Q.gif"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">使用DeepStack的手语对象检测</figcaption></figure><p id="48c7" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我最近看了一个由Nicholas Renotte制作的关于如何使用T2 tensor flow 2手语物体检测API的视频。</p><p id="9571" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">使用Tensorflow 2，我重新创建了那个项目，你可以在我的<a class="ae jr" href="https://github.com/youngsoul/sign-language-detection-with-tfod2" rel="noopener ugc nofollow" target="_blank"> Github repo </a>上读到它。</p><p id="d42b" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在我研究如何执行自定义对象检测的过程中，我浏览了一个中型博客，“用DeepStack 检测任何自定义对象”。</p><p id="1f7e" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">你可以在这里找到更多关于DeepStack的信息。DeepStack的文档可以在<a class="ae jr" href="https://docs.deepstack.cc" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="d592" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我对他们使用Docker容器的方法非常感兴趣，这些容器要么具有预配置的功能，要么可以根据自己的数据进行训练。</p><p id="41c2" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">下面是我使用DeepStack框架创建你在上面看到的手语物体检测器的步骤总结。</p><h1 id="5214" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">注释数据集</h1><p id="fd44" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">首先，我用pascal-voc注释了用于TFOD的图像，并用YOLO格式重新注释了它们。你可以在我的<a class="ae jr" href="https://github.com/youngsoul/sign-language-dataset" rel="noopener ugc nofollow" target="_blank">手语数据集报告</a>中找到两个数据集的报告。</p><p id="6c51" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我用了LabelImg，但是任何你喜欢的工具都可以。你只需要使用YOLO格式。</p><h1 id="6635" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">压缩数据集</h1><p id="6d64" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">创建一个包含测试和训练目录的zip文件。在每个测试和训练目录中都有YOLO格式的注释文件。</p><h1 id="74cc" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">将数据集上传到Google Colab</h1><p id="4f1e" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">如果你有GPU，你可以在你的计算机上进行本地训练，或者按照DeepStack的建议，你可以使用Google Colab环境进行训练。他们通过以下链接提供了入门笔记本。</p><div class="kv kw ez fb kx ky"><a href="https://colab.research.google.com/drive/1gbTr_4xpDk3cpnbAVbMVxtyp-3XuUPix?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="kz ab dw"><div class="la ab lb cl cj lc"><h2 class="bd hi fi z dy ld ea eb le ed ef hg bi translated">谷歌联合实验室</h2><div class="lf l"><h3 class="bd b fi z dy ld ea eb le ed ef dx translated">编辑描述</h3></div><div class="lg l"><p class="bd b fp z dy ld ea eb le ed ef dx translated">colab.research.google.com</p></div></div><div class="lh l"><div class="li l lj lk ll lh lm in ky"/></div></div></a></div><p id="d778" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">所有这些信息都可以在“自定义模型”下的DeepStack文档中找到。</p><p id="a17d" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">当你第一次访问Google Colab链接时，页面看起来像这样:</p><figure class="lo lp lq lr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ln"><img src="../Images/8b1c0215cac222b18ec1f7141c131e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8bxi0NUlrgKxDvf9iLUHuQ.png"/></div></div></figure><p id="4a99" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">选择左上角的upload按钮，并导航到您创建测试和培训目录的zip文件的位置。</p><figure class="lo lp lq lr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ln"><img src="../Images/a03cc7fac3c37fff14582fa769ba6925.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qa7oAi-b8fKr-XdrhLLZyw.png"/></div></div></figure><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es ls"><img src="../Images/e67bc8776018d2e10851548af5a213a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*PWM34X3yXZw2dX6zc8v2sA.png"/></div></figure><h1 id="e6b4" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">克隆DeepStack训练器</h1><p id="5b32" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">运行代码单元并克隆DeepStack Trainer repo。你会得到一个警告，说该笔记本不是由谷歌创作的。尽管跑吧。</p><h1 id="dae3" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">解压缩归档文件</h1><p id="c7a5" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">打开一个新的代码单元格，并键入以下内容:</p><blockquote class="lt lu lv"><p id="4360" class="it iu lw iv b iw ix iy iz ja jb jc jd lx jf jg jh ly jj jk jl lz jn jo jp jq ha bi translated">！unzip/content/archive . zip-d/content/sign _ dataset</p></blockquote><p id="39aa" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">完成后，左边的导航应该看起来像这样:</p><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es ma"><img src="../Images/16dc394ae0489fbc1fb80b40353777a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*h7MQD2I0Nox9UEKX5ygPwA.png"/></div></figure><h1 id="a141" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">训练模型</h1><p id="545d" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">打开一个新的代码单元格，要定型模型，您可以键入:</p><pre class="lo lp lq lr fd mb mc md me aw mf bi"><span id="f88e" class="mg jt hh mc b fi mh mi l mj mk">python3 train.py --dataset-path "/content/sign_dataset"</span></pre><p id="5c38" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">但是，文档中强调了一些您可能需要注意的参数。(直接来自他们的文档)。</p><p id="5f7a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">当我训练这个模型的时候，我只是使用了上面的命令，但是我注意到在大约110个时期之后，精确度并没有提高。我可以把它剪短，但我让它跑了整整300个时代。</p><p id="9cf8" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">培训过程约1.5小时。</p><h1 id="b52d" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">重要参数</h1><p id="a617" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">可以设置以下参数来优化您的模型，使其更适合您</p><ul class=""><li id="0235" class="ml mm hh iv b iw ix ja jb je mn ji mo jm mp jq mq mr ms mt bi translated"><code class="du mu mv mw mc b">`--model`</code> DeepStack训练器支持五种模型类型，按照精度递增的顺序分别是<code class="du mu mv mw mc b">`"yolov5s"`</code>、<code class="du mu mv mw mc b">`"yolov5m"`</code>、<code class="du mu mv mw mc b">`"yolov5l"`</code>、<code class="du mu mv mw mc b">`"yolov5x"`</code>。默认值是yolov5m，最高精度的yolov5l和yolov5x要慢得多，需要部署更高端的计算。如果部署在nvidia jetson上，强烈建议使用最快的yolov5s。</li><li id="bb9e" class="ml mm hh iv b iw mx ja my je mz ji na jm nb jq mq mr ms mt bi translated"><code class="du mu mv mw mc b">`--batch-size`</code>这是一次处理的图像数量，如果您的gpu内存允许，您可以将其设置为较高的数字，如32或64，如果您使用内存较低的gpu，如果遇到内存问题，您可以将其设置为较低的数字，如8或更少。默认值是16</li><li id="a96a" class="ml mm hh iv b iw mx ja my je mz ji na jm nb jq mq mr ms mt bi translated"><code class="du mu mv mw mc b">`epochs`</code>这是整个数据集的迭代次数，默认值是300。您总是可以用更少的时期或更多的时期运行，准确性随着您运行更多的时期而增加。</li></ul><h1 id="9782" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">下载最佳模型</h1><p id="906a" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">训练结束后。您将在目录中找到最佳型号:</p><blockquote class="lt lu lv"><p id="7dab" class="it iu lw iv b iw ix iy iz ja jb jc jd lx jf jg jh ly jj jk jl lz jn jo jp jq ha bi translated">deep stack-trainer/train-runs/sign _ dataset/exp/weights/best . pt</p></blockquote><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es nc"><img src="../Images/4f3e2aa24b515c020f1208d2e4e4b3ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*XNQNMQC9UZ1Zh6Zgw2F6TQ.png"/></div></figure><h1 id="b6a1" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">设置DeepStack Docker容器</h1><p id="ee46" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">有许多方法可以设置DeepStack docker容器，您应该查看它们的文档以获得完整的可能性列表。我将讲述我是如何在MacOS上运行的。</p><p id="2ba0" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">假设您已经安装了Docker。</p><p id="9d28" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在终端窗口中</p><pre class="lo lp lq lr fd mb mc md me aw mf bi"><span id="a318" class="mg jt hh mc b fi mh mi l mj mk">docker pull deepquestai/deepstack</span></pre></div><div class="ab cl nd ne go nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ha hb hc hd he"><h1 id="f00c" class="js jt hh bd ju jv nk jx jy jz nl kb kc kd nm kf kg kh nn kj kk kl no kn ko kp bi translated">使用新模型运行DeepStack Docker容器</h1><p id="c62a" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">在终端窗口中运行以下命令:</p><blockquote class="lt lu lv"><p id="c4e0" class="it iu lw iv b iw ix iy iz ja jb jc jd lx jf jg jh ly jj jk jl lz jn jo jp jq ha bi translated">docker run-v/path/to/directory/containing/downloaded/model/sign language-model:/model store/detection-p 5000<br/>:5000 deepquestai/deep stack</p></blockquote><p id="3226" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我使用5000作为本地计算机端口号，而不是他们推荐的80，因为我有太多的东西可能要使用端口80。</p><h1 id="b506" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">运行脚本以捕获视频图像进行推断</h1><p id="1f15" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">我编写了一个脚本，可以捕获视频帧，将它们编码为jpg图像，并向运行新训练模型的运行DeepStack Docker容器发出所需的post请求。</p><p id="c282" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">你可以在我的<a class="ae jr" href="https://gist.github.com/youngsoul/2e6a64dabbf9303103bc48b8b2ab3617" rel="noopener ugc nofollow" target="_blank">要点报告</a>中找到那个脚本</p><p id="fc8b" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">请注意，您需要从客户端脚本访问的url如下所示:</p><blockquote class="lt lu lv"><p id="69f1" class="it iu lw iv b iw ix iy iz ja jb jc jd lx jf jg jh ly jj jk jl lz jn jo jp jq ha bi translated"><a class="ae jr" href="http://localhost:5000/v1/vision/custom/sign" rel="noopener ugc nofollow" target="_blank">http://localhost:5000/v1/vision/custom/sign</a></p></blockquote><p id="3116" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">其中5000是你上面公开的端口。url的“符号”部分是下载的模型的基本名称。在我的例子中，我把这个模型叫做‘sign . pt’</p><p id="b054" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">要使用这个客户端脚本，您需要创建一个Python虚拟环境并安装OpenCV，因为我用它来捕捉视频和显示图像。当然，你也可以选择任何你喜欢的框架。</p><h1 id="b2a3" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">结果</h1><p id="0afe" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">你可以看到上面的结果。非常好的准确性，没有错误的预测。</p><p id="4fb3" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">使用“yolov5m”模型，运行推理模型每帧大约需要250毫秒。所以不完全是实时的，但在我看来相当不错。我会试试‘yolov 5s’，看看这个型号的性能特点是什么。</p><p id="1eb1" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">总的来说，我对工作流程和创建自定义YOLOV5对象检测模型的简单性印象深刻，只需安装一个Docker容器。</p></div></div>    
</body>
</html>