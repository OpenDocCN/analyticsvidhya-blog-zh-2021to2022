# 嵌入——什么、为什么和如何嵌入？

> 原文：<https://medium.com/analytics-vidhya/embeddings-the-what-the-why-and-the-how-15a6a3c99ce8?source=collection_archive---------12----------------------->

![](img/e4edd8132ab314cf5ea9e7da56ff640e.png)

来源:[https://www.youtube.com/watch?v=wvsE8jm1GzE&ab _ channel = Google developers](https://www.youtube.com/watch?v=wvsE8jm1GzE&ab_channel=GoogleDevelopers)

> 我喜欢注意别人看不到的细节——匿名

我们大多数稍微熟悉机器学习和深度学习技术的人都在某个时候遇到过“嵌入”这个术语。将这一相位作为实现更大目标的第一步或中间步骤是很常见的。

在这篇文章中，我们将试图揭开嵌入的世界，并意识到当我们放大它们时，它们是多么有效和有影响力。我的主要目标是能够在一个非常抽象的层次上理解一个概念，以便任何专业水平的人都能够容易地理解其内容。因此，文章的某些部分可能看起来非常简单！

## 动机

想象一个场景，你的教授想把抄袭的作业和原始作业分开；-)或者你想根据你和暗恋对象的短信对话来发现，你是否有了一个真正的竞争对手；-)，或者更严肃地说，您想要突出显示一组给定文档中的模式，执行文本内容的语义聚类，等等。

在上述所有场景中，如果我们可以避免逐字逐句地扫描整个文本文档来得出结论，那不是更好吗？这正是我们使用嵌入所能实现的。

## 什么

简而言之，嵌入是像文本、图像、声音等数据形态的健壮表示。本质上，它们是相对较低维度的向量，可以捕捉语义*。*

## 为什么

如前所述，如果我们使用数据的嵌入表示来操作数据，事情会变得容易得多。向量可以以这样一种方式构建，即它可以捕获大量与其自身相关的语义。一个著名的例子是“国王”与“男人”(同性)相关，“女王”与“女人”相关。有各种各样的方法来构造嵌入。我们将在这里参观一些著名的技术。

## 怎么做

我打算在这里介绍的一些技术包括:

1.  一键编码
2.  矩阵分解
3.  Word2Vec
4.  手套

**一键编码**

作为所有技术中最简单的一种，独热编码表示使用稀疏二进制向量来表示词汇表中的每个单词。它将给定文本中每个单词的位置转换成一个二进制向量。

例句:对于句子“我爱分析”向量的表示形式可以是这样的

![](img/23de1c01721635592e57a88d2e6b4c47.png)

这种方法的一个缺点是，它没有真正捕捉到文本中每个单词相对于另一个单词的更深层次的语义含义，因为所有向量彼此等距。

此外，在上述情况下，当 vocab 大小为 3 时，单词“love”由向量[0 1 0 ]表示，但是当 vocab 大小增加到例如 6 时，“love”将必须由具有 6 维的向量表示，因此所有单词所需的空间随着 vocab 大小的增加而呈指数增加。也许这种方法可以通过使用[单词包](https://en.wikipedia.org/wiki/Bag-of-words_model)的概念来改进，但是，在这种情况下，单词的顺序可能会丢失。

## 矩阵分解

这是一种广泛应用于推荐系统领域的技术，如电子商务门户网站，用于向客户推荐产品、预测用户对电影的评价等。

在将此与嵌入可能联系起来之前，我们可以简单地讨论一下这种方法是如何操作的。考虑用户与电影评级的用例。考虑矩阵

![](img/ab27eb2d9416534a2b56982d4892a6f8.png)

这些行代表不同用户对每部电影的评级，其中一些用户是未知的。人们想出了一些聪明的方法，利用一些数学技术来计算这些评级。其中一种方法是使用矩阵分解( [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) 就是这样一种技术)，目标是将一个大矩阵分解成多个小矩阵的乘积。这减少了所需的空间，多倍。如果这个主矩阵的维数是(1000，2000)，这可以分解为(1000，100) (100，100) (100，2000)矩阵的乘积，即我们可以将它解释为

(1000,2000) → (1000,100) * (100,100) * (100,2000)

(电影，用户)→(电影，**流派** ) * ( **流派**，**流派** ) * ( **流派**，用户)

这允许将每个用户或电影作为一个较小的维度向量，然后可以用于预测评级。关于这个的更多细节在这个[视频](https://www.youtube.com/watch?v=ZspR5PZemcs&t=265s&ab_channel=LuisSerrano)中给出。

在文本内容的上下文中，我们可以通过构建一个称为共现矩阵的东西，将一堆文档转换为一个巨大的矩阵，即对于每个单词，该单词在给定的窗口大小内与另一个单词共现的次数。对于大小为 2 的窗口，以下是句子“我喜欢数据”的共现矩阵。我喜欢数据科学”。

![](img/0b301ce9c4724c56e88d2a9c92cce63a.png)

或者也可以使用类似于文档中每个单词的 [TF-IDF](https://monkeylearn.com/blog/what-is-tf-idf/#:~:text=TF%2DIDF%20is%20a%20statistical,across%20a%20set%20of%20documents.) score 这样的函数作为矩阵中的元素。TF-IDF 捕捉文档中每个单词的重要性，同时也考虑它在文档中的出现频率。

一旦我们有了如此庞大的矩阵，我们就可以使用矩阵分解技术来分解这个矩阵，从而得到一个降维的矩阵。这将允许我们将每个单词表示为一个低维向量，同时捕获更多的语义，而不仅仅是每个单词在文档中的同现。

也就是说，我们有维度矩阵:

(文字，文档)→(文字，**话题** ) * ( **话题，话题** ) * ( **话题**，文档)

然后，我们可以使用上述任何较小的矩阵作为每个单词的嵌入。然而，这种技术可能需要我们有大量的内存来分解矩阵。

## Word2Vec

这项技术属于神经网络(NN)领域。为了向不熟悉的人简要介绍 NNs，它有以下步骤:

*   神经网络接受一个向量作为它的输入——这个向量可以代表任何数据形式——文本、图像、数字、声音等等。
*   然后，它对向量进行一系列矩阵乘法，并对结果进行一些非线性函数处理(双曲正切函数、sigmoid 函数等)
*   最后吐出结果，并将结果与实际情况(误差函数)进行比较
*   将来自步骤(3)的损失反向传播到系统的开始，并修改来自步骤(2)的矩阵中的权重/元素，以带来改进的预测。

有了上面的见解，对于我们的用例，我们更感兴趣的是如何使用神经网络来构建嵌入。特别地，我们对来自步骤(2)的那些中间矩阵感兴趣。

Word2Vec 是一种神经网络，它要么接受一个单词并预测周围的单词(这称为 Skip-Gram 模型)，要么接受周围的单词并预测主单词(这称为 CBOW 模型)。作为输入，神经网络可以接受前面讨论过的独热编码向量或每个单词的任何其他等效的矢量化表示。考虑这个例子:我刚买了水果和蔬菜。

![](img/5cc735f930a44b48a6a79a789abaf380.png)

以上作品的论文可以在[这里](https://arxiv.org/abs/1301.3781)找到。

因此，如上所述，我们对黄色和绿色方框更感兴趣，因为它们将高维向量压缩到低维，然后将较小维的向量扩展到原始维。在这个过程中，人们相信这些方框(又称为矩阵，又称为查找表)捕获了所涉及的必要语义。

最后，我们应该能够捕捉这样的语义:

![](img/082076cb9aad3a126952b4e9ea4613d1.png)

也就是说，男性和女性之间性别差异的概念被捕捉到了，这很酷！

有一些令人惊奇的库可以让我们传递文本，它们的 API 可以捕捉单词之间的“语义”紧密度。

作为一个练习，我有一个 [colab](https://colab.research.google.com/drive/1gr5ulYVeNWm5_iGObjlraZSVQXGk1JE8?usp=sharing) 笔记本，我试图将这篇文章中的一段文本作为输入传递给 word2vec 模型:
，并获得了一些见解:

![](img/a17355ec25f3b3fe8ce6762c12859681.png)

如上所述，与“信息”、“语言”等词相比，我似乎将“维度”、“更小”、“空间”等词放在一起使用。

## *Glove——单词表示的全局向量*

> *这种技术利用了前面讨论过的单词共现表示法，但方式更酷！在这里，他们分析巨大的文本语料库，计算单词向量，从而得出单词的共现概率。他们在* [*论文*](https://nlp.stanford.edu/pubs/glove.pdf) 中讨论技巧
> 
> *为了获得这个想法的本质，考虑他们论文中讨论的例子*
> 
> *例如:从热力学角度来看，单词“固体”与单词“冰”一起出现的概率高于它与“蒸汽”一起出现的概率:即 P(固体|冰)=(“冰”与固体一起出现的次数)/(任何单词与“固体”一起出现的次数)将高于 P(固体|蒸汽)，同样，我们可以填充整个矩阵:*

![](img/a1340dcf96ae4d46e30b85f86fc27a5b.png)

> *这种类型的矩阵构造捕捉了语料库中每个单词的全局影响，而不仅仅是它对其近邻的影响。*
> 
> *他们使用这种共现概率矩阵设置来得出损失函数，然后该损失函数可以反向传播到神经网络，以最终为每个单词构建语义感知向量。*

## *应用*

> *如前所述，嵌入在几乎所有机器学习相关的任务中无处不在。自近代以来，越来越多的强有力的方法被开发出来，它们捕获的信息比我们所能想象的要多得多，因此它们被用于各种各样的领域。*
> 
> *我个人很喜欢这个发表在本* [*期刊*](https://www.pnas.org/content/115/16/E3635) *上的特殊例子。*
> 
> 在这里，研究人员利用 Word2Vec 和 GloVe 单词嵌入来分析超过 100 年的大量文本！从像谷歌图书的历史美国英语，美国人口普查数据等语料库中捕捉历史趋势和性别和种族刻板印象如何随着时间的推移而演变。
> 
> *简单描述他们的过程-*

*   对于每一个用例，他们将单词分成不同的目标组，比如男人、他、先生……|女人、她、女士..白人、亚洲人、西班牙人..等等
*   然后他们考虑与职业相关的中性词(水手，司机..)或形容词(好的、苛刻的、坏的……)并计算目标群体与中性词的距离。

**G1** — d1 → **中性词**←——D2——**G2**

*   用这样的中性词比较目标群体的距离。如果 d1 — d2 为负，那么 G1 组更接近中性词，因此存在嵌入偏差。

> *通过这种分析，他们能够捕捉到令人惊讶的偏差，比如-*

![](img/8bd49ca99a8760852102d80547477bcc.png)

> *(一)捕捉职业中的性别偏见。*
> 
> *(二)对妇女的态度以及历史事件的发生——20 世纪 60 年代妇女运动的高潮也是对妇女的态度发生变化的时间点。*
> 
> *(iii)抓住了亚裔美国人的刻板印象，以及当他们推出《移民和国籍法》时，对亚裔美国移民的态度是如何转变的。*
> 
> *附加分析可参见*[](https://www.pnas.org/content/115/16/E3635)**。**
> 
> *我认为嵌入在现代世界中可以扮演一个强大的角色，来捕捉像上面讨论的那样的洞察力。*

## **结论和未来范围**

> *因此，这篇文章试图更详细地描述嵌入式世界，同时确保该领域的新手能够理解。我们简单地基于一些基本技术来构建/表示单词，以便计算机可以理解所传达内容背后的语义或上下文。*
> 
> *我们从基本的一键向量编码开始，接着是矩阵分解、基于神经网络的 Word2Vector 以及手套嵌入技术。接下来是一个应用程序的简要概述，该应用程序使用单词嵌入来量化历史趋势。*
> 
> *已经讨论的只是冰山一角，在嵌入领域已经有了广泛的研究。基于 Transformer 的模型，如[*【GPT-3】*](https://en.wikipedia.org/wiki/GPT-3)*[*基于 Google 的 T5*](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)*[*基于 BERT*](https://en.wikipedia.org/wiki/BERT_(language_model)#:~:text=Bidirectional%20Encoder%20Representations%20from%20Transformers,pre%2Dtraining%20developed%20by%20Google.&text=Both%20models%20are%20pre%2Dtrained,Wikipedia%20with%202%2C500M%20words.) *的模型是一些强大的语言模型，可以进一步探索。我们也可以选择更进一步，不仅仅是坚持文字，而是超越封装事实信息的三元组。这个领域被称为* [*知识嵌入*](https://towardsdatascience.com/summary-of-translate-model-for-knowledge-graph-embedding-29042be64273) *来捕捉所谓的知识图(KGs)。简单描述一下 KG，它有节点和边，节点代表现实世界的实体(句子中的主语和宾语)，边代表节点之间的连接(谓词)。然后，我们可以将这样的节点建模为嵌入，然后可以用于执行操作，例如能够从事实信息中进行推断，预测新信息等等！天空是无限的！:)****