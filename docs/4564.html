<html>
<head>
<title>Optimizers — Gradient descent algorithms ( Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化器—梯度下降算法(第一部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/optimizers-gradient-descent-algorithms-part-1-ac9baf446df1?source=collection_archive---------1-----------------------#2021-11-16">https://medium.com/analytics-vidhya/optimizers-gradient-descent-algorithms-part-1-ac9baf446df1?source=collection_archive---------1-----------------------#2021-11-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="8d24" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">嘿大家好！欢迎来到我的博客！在这篇博客中，我们将看到一些基本优化算法的实现。在机器学习中，权重和偏差是机器学习/深度学习模型的可学习参数(Theta θ)。优化器是用于修改模型参数(权重和偏差)以最小化损失函数的算法。通常在模型训练的开始，权重和偏差被随机初始化。在训练期间，损失函数被计算，并且这些参数被优化器更新，使得损失函数的值最小。</p><p id="70ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在第1部分中，我们将从头开始学习并使用python实现以下算法。</p><ol class=""><li id="c583" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">梯度下降</li><li id="9058" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">随机梯度下降</li><li id="215d" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">批量梯度下降</li><li id="a0a5" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">小批量随机梯度下降</li></ol><p id="a002" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将针对具有单一特征的线性回归模型讨论这些算法。让我们考虑这个等式</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="c6ab" class="jz ka hh jv b fi kb kc l kd ke">y = 2.5X + 1 (1)</span></pre><p id="4f51" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中m = 2.5，b = 1</p><p id="88c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的X具有100个数据点的单一特征。我们将用-1和1之间的随机值填充X。然后，我们可以使用上面所示的等式(1)计算y。</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="48d0" class="jz ka hh jv b fi kb kc l kd ke">import numpy as np<br/>import random<br/># set seed for reproducibilty<br/>np.random.seed(1) <br/>num_samples = 100<br/>X = np.random.uniform(-1.,1.,num_samples)<br/>m = 2.5<br/>b = 1<br/>y = m*X +b</span></pre><p id="5eb1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们来看看由等式(1)表达的这个函数的可视化</p><figure class="jq jr js jt fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kf"><img src="../Images/e10668b33a75bc0aa08a09b6d7a23cae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5NYv8B7_mcUJAqxNa0ouiQ.jpeg"/></div></div></figure><p id="bbe5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将看到计算m和b的各种优化器算法的实现。我们将使用下图中等式描述的均方误差(MSE)作为目标函数(L)。基本上，我们将使用这些优化算法来最小化损失函数。</p><figure class="jq jr js jt fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kn"><img src="../Images/e63f265a2a4c83bcf87d3c750628e911.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*onW7-1MleyWM7Dle1LFrWQ.png"/></div></div></figure><p id="4cb0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">梯度下降:</p><p id="41a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在梯度下降中，计算目标函数(L)相对于参数theta (θ)的梯度，并且在目标函数梯度的相反方向上更新参数。学习率α决定了达到局部最小值所要采取的步长。</p><figure class="jq jr js jt fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es ko"><img src="../Images/74ca101275cec705d606a07d987a28f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ehL7TmDRy-PONXqbBakgA.jpeg"/></div></div></figure><p id="0645" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基于多少数据用于计算权重更新的梯度，我们有不同的变量，我们将详细讨论这些变量</p><p id="b0c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">批量梯度下降:</strong></p><p id="7483" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在批量梯度下降中，对于每个时期，我们为整个数据集的目标函数计算梯度<a class="ae kp" href="http://w.r.to/" rel="noopener ugc nofollow" target="_blank"> w.r .到</a>参数。因此，参数的更新每个时期发生一次。批量梯度下降也称为香草梯度下降。</p><p id="1c7f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于我们的MSE目标函数，关于m和b的梯度如下所示。如果你有兴趣了解如何计算梯度，请点击<a class="ae kp" href="https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc" rel="noopener" target="_blank">这里</a></p><figure class="jq jr js jt fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kq"><img src="../Images/35f1f08e6cbef245662cb365d4085246.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VYYxEqprD5dl52Ycs8sGoA.jpeg"/></div></div></figure><figure class="jq jr js jt fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kr"><img src="../Images/7b5b58923b1f787489a1fa26abd1a75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Goxd7ZDDhErnpAN7dcRYUg.jpeg"/></div></div></figure><p id="09ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">批量梯度下降的Python代码</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="577e" class="jz ka hh jv b fi kb kc l kd ke">import numpy as np<br/>from sklearn.metrics import mean_squared_error</span><span id="ffc5" class="jz ka hh jv b fi ks kc l kd ke">def batch_gradient_descent(X, y, lr, epochs): <br/>    m, b = 0.33, 0.48 # initialising the parameters<br/>    # log stores m and b values for differnt updates <br/>    # mse stores the error<br/>    log, mse = [], [] # lists to store learning process <br/>    N = len(X) # number of samples</span><span id="802b" class="jz ka hh jv b fi ks kc l kd ke">for _ in range(epochs):               <br/>        f = y - (m*X + b)   <br/>        # Updating m and b<br/>        m -= lr * (-2 * X.dot(f).sum() / N)<br/>        b -= lr * (-2 * f.sum() / N)</span><span id="f1e6" class="jz ka hh jv b fi ks kc l kd ke">log.append((m, b))<br/>        mse.append(mean_squared_error(y, (m*X + b)))        <br/>    return m, b, log, mse<br/></span></pre><p id="e79e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图显示了lr = 0.1和时期= 75时的MSE批次梯度下降。</p><figure class="jq jr js jt fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kt"><img src="../Images/27be6c57477efdedb10d21055b48a382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nr51aige7lfWK94cxEw-NQ.jpeg"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">注意:计算整个数据集的梯度后，参数会发生变化。如果数据集很大。许多功能，这是非常耗时的。它还需要更多的内存来计算整个数据集的梯度。</figcaption></figure><p id="f69d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">随机梯度下降:</strong></p><p id="c1fb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在随机梯度下降中，从一个时期的整个集合中随机选择一个样本。计算该特定样品的梯度，并更新样品和重量。</p><p id="f933" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机梯度下降的Python代码；</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="5b36" class="jz ka hh jv b fi kb kc l kd ke">import numpy as np<br/>from sklearn.metrics import mean_squared_error<br/>def SGD(X, y, lr, epochs):<br/>    m, b = 0.5, 0.5  # initial parameters<br/>    log, mse = [], [] # lists to store learning process</span><span id="7b79" class="jz ka hh jv b fi ks kc l kd ke">for _ in range(epochs):<br/>        indexes = np.random.randint(0, len(X)) # random sample<br/>        Xs = np.take(X, indexes)<br/>        ys = np.take(y, indexes)<br/>        N = len(X)<br/>        f = ys - (m*Xs + b)</span><span id="2f81" class="jz ka hh jv b fi ks kc l kd ke"># Updating parameters m and b<br/>        m -= lr * (-2 * Xs*(f).sum() / N)<br/>        b -= lr * (-2 * f.sum() / N)</span><span id="4270" class="jz ka hh jv b fi ks kc l kd ke">log.append((m, b))<br/>        mse.append(mean_squared_error(y, m*X+b))</span><span id="1155" class="jz ka hh jv b fi ks kc l kd ke">return m, b, log, mse</span></pre><p id="d300" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图显示了lr = 0.85和历元= 500时MSE与历元的关系。我们可以观察到，对于我们的例子，我们能够以更高的学习率和更多的时期数达到局部最小值。</p><figure class="jq jr js jt fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es ky"><img src="../Images/c8a86af427674861689ff1b4afdec37c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CWV5xCurJSrV_uPWC5gWjw.jpeg"/></div></div></figure><p id="f6b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">小批量梯度下降:</strong></p><p id="3e3a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在小批量梯度下降中，对小批量样品进行更新。在我们的例子中，我们有100个样本。因此，批量大小为10时，我们在10个时期内有100次更新。小批量梯度下降是训练神经网络时选择的算法。</p><p id="6784" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">用于小批量梯度下降的Python代码</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="4543" class="jz ka hh jv b fi kb kc l kd ke">def minibatchgd(X, y, lr, epochs, batch_size):<br/>    m, b = 0.5, 0.5 # initial parameters<br/>    log, mse = [], [] # lists to store learning process<br/>    for _ in range(epochs):<br/>        total_len = len(X)<br/>        for i in range(0, total_len, batch_size):<br/>            Xs = X[i:i+batch_size]<br/>            ys = y[i:i+batch_size]<br/>            N = len(Xs)<br/>            f = ys - (m*Xs + b)<br/>            # Updating parameters m and b<br/>            m -= lr * (-2 * Xs.dot(f).sum() / N)<br/>            b -= lr * (-2 * f.sum() / N)<br/>            log.append((m, b))<br/>            mse.append(mean_squared_error(y, m*X+b))</span><span id="c548" class="jz ka hh jv b fi ks kc l kd ke">return m, b, log, mse</span></pre><p id="0fce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于时期= 10，lr = 0.5，batch_size = 10，小批量梯度下降图如下所示。</p><p id="c42f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">小批量梯度下降也称为小批量SGD，其中数据被随机打乱，然后选择小批量数据。</p><figure class="jq jr js jt fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kz"><img src="../Images/4f9bfc50f7d19c5bb979d2c60d78cd15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aNDD856MIVXB3yl_1q1s8g.jpeg"/></div></div></figure><p id="7230" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总而言之，对于“m”个样本的训练数据集，如果</p><ol class=""><li id="8d35" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">最小批量= 'm '。—批量梯度下降(一个时期的所有数据)</li><li id="7d66" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">最小批量= '1' —随机梯度下降(一个时期所有数据中的一个样本)</li><li id="e637" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">小批量= ' m '/100-小批量梯度下降(一个时期取m/100个样品)</li></ol><p id="503f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">重要注意事项:</strong></p><ol class=""><li id="83c4" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">必须改变超参数学习速率、时期、批次大小，以使MSE达到最小值(理想情况下为0)。</li><li id="449a" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">我们可以观察到，小批量梯度下降中的时期数显著低于批量梯度下降和随机梯度下降。这意味着用更少的训练时间达到最小值。</li><li id="7703" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">学习速度必须仔细选择，这是一项相当具有挑战性的任务。</li></ol><p id="74c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下一部分，我们将讨论一些复杂的优化算法。该代码可在<a class="ae kp" href="https://github.com/bhuvanakundumani/optimizers.git" rel="noopener ugc nofollow" target="_blank">https://github.com/bhuvanakundumani/optimizers.git</a>获得</p><p id="99f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参考文献</strong>:<br/><a class="ae kp" href="https://ruder.io/optimizing-gradient-descent/" rel="noopener ugc nofollow" target="_blank">https://ruder.io/optimizing-gradient-descent/</a><br/><a class="ae kp" href="https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html" rel="noopener ugc nofollow" target="_blank">https://www . kdnugges . com/2020/12/optimization-algorithms-neural-networks . html</a><br/><a class="ae kp" href="https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc" rel="noopener" target="_blank">https://towards data science . com/gradient-descent-from-scratch-E8 b 75 fa 986 cc</a></p></div></div>    
</body>
</html>