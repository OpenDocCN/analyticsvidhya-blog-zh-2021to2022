<html>
<head>
<title>Fundamentals Of Machine Learning Part 3 — Regularization In Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习基础第3部分—回归中的正则化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fundamentals-of-machine-learning-part-3-regularization-in-regression-f75afd726250?source=collection_archive---------7-----------------------#2021-03-19">https://medium.com/analytics-vidhya/fundamentals-of-machine-learning-part-3-regularization-in-regression-f75afd726250?source=collection_archive---------7-----------------------#2021-03-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="1027" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们将在线性模型/回归中实现正则化技术。</p><h2 id="ce99" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">先决条件:-</h2><ul class=""><li id="1164" class="jx jy hh ig b ih jz il ka ip kb it kc ix kd jb ke kf kg kh bi translated"><strong class="ig hi">线性回归</strong></li><li id="972c" class="jx jy hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated"><strong class="ig hi">成本函数</strong></li><li id="917e" class="jx jy hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated"><strong class="ig hi">梯度下降</strong></li></ul><p id="4647" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果您不熟悉这些术语，那么可以考虑通读这篇文章:-</p><p id="0a4e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae kn" rel="noopener" href="/analytics-vidhya/fundamentals-of-machine-learning-part-2-linear-regression-b7f87d804028">机器学习基础第二部分-线性回归|作者阿克谢·库玛尔·雷|分析维迪亚| 2021年3月|媒体</a></p><p id="1e35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们开始讨论线性模型或一般回归中的正则化之前。我们需要熟悉这些术语:-</p><h2 id="7cb0" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">过度拟合:-</h2><blockquote class="ko kp kq"><p id="27c0" class="ie if kr ig b ih ii ij ik il im in io ks iq ir is kt iu iv iw ku iy iz ja jb ha bi translated">简而言之，当你的模型开始吸收东西而不是从中学习时。</p></blockquote><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es kv"><img src="../Images/ecaa45d24304a5c1120d1c9c820fc74c.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*ywcSuVd3yHVXpx0zvJOTFQ.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated">这个模型甚至捕捉到了噪音。</figcaption></figure><p id="bf3f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当模型开始记忆训练数据而不是学习和理解数据的潜在趋势时，就会发生过度拟合。这导致模型在新的独立数据上表现不佳。在线性回归中，成本函数是预测值和实际值之间的平方差之和，我们的目标是降低成本函数。但是在过拟合的情况下，预测值等于实际值，因此<strong class="ig hi">“成本函数”的值为零</strong>。这个模型对我们的训练集非常有效，但是当引入新的数据集时，这个模型将给出一个可怕的预测。</p><h2 id="f781" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">欠配:-</h2><blockquote class="ko kp kq"><p id="5431" class="ie if kr ig b ih ii ij ik il im in io ks iq ir is kt iu iv iw ku iy iz ja jb ha bi translated">简而言之，该模型既不从数据集学习，也不吸收任何东西。</p></blockquote><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es lh"><img src="../Images/2095e8bd2b72389ba2beee0a075b45c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*BI13IIHWM3iKWpKuLVvfhw.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated">模型太简单了。</figcaption></figure><p id="1e65" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">欠拟合模型对训练数据和测试数据都不太好。</p><p id="824b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">模型无法理解/分析数据的潜在趋势/结构，无法从训练数据中获得关键信息。这通常发生在没有足够的训练数据或者模型做出非常简单的假设/前提时。当我们试图在非线性数据上建立线性模型时，可能会出现拟合不足的情况。</p><h2 id="3074" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated"><strong class="ak">偏差:- </strong></h2><p id="c706" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip li ir is it lj iv iw ix lk iz ja jb ha bi translated">偏差被称为ML模型的预测值和正确值之间的差异。偏置过高会在训练和测试数据中产生较大的误差。建议算法应始终为<strong class="ig hi">低偏置</strong>，以避免<strong class="ig hi">欠拟合</strong>的问题。<br/>通过<strong class="ig hi">高偏差</strong>，预测的数据为直线格式，因此不能准确拟合数据集中的数据。这种拟合被称为数据的<strong class="ig hi">欠拟合。</strong></p><h2 id="d771" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">差异:-</h2><p id="702c" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip li ir is it lj iv iw ix lk iz ja jb ha bi translated">给定数据点的模型预测的可变性告诉我们数据的分布，称为模型的方差。具有<strong class="ig hi">高方差</strong>的模型对训练数据的拟合非常复杂，因此无法准确拟合之前未见过的数据。因此，这种模型在训练数据上表现得非常好，但是在测试数据上有很高的错误率。<br/>当一个模型在方差上<strong class="ig hi">高时，则称之为<strong class="ig hi">数据的过度拟合</strong>。</strong></p><h2 id="2c3c" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">如何克服过拟合或高方差？</h2><ul class=""><li id="3e0d" class="jx jy hh ig b ih jz il ka ip kb it kc ix kd jb ke kf kg kh bi translated">向数据集引入更多数据。</li><li id="1270" class="jx jy hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated"><strong class="ig hi">降低模型复杂度。</strong></li><li id="7686" class="jx jy hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated"><strong class="ig hi">山脊正则化(L2正则化)、拉索正则化(L1正则化)和弹性网正则化。</strong></li></ul><h2 id="4f8f" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">岭回归(L2归一化):-</h2><blockquote class="ko kp kq"><p id="564f" class="ie if kr ig b ih ii ij ik il im in io ks iq ir is kt iu iv iw ku iy iz ja jb ha bi translated"><strong class="ig hi">使用岭正则化(L2归一化)的回归模型称为岭回归。</strong></p></blockquote><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es ll"><img src="../Images/8f1f6744438ec5e289799326e9aefed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DVGD2WLWy9A_SUmlOVJP-g.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">引入一些偏差来管理高方差。</figcaption></figure><p id="25d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我们有一个数据集，该数据集只有一个要素和两个示例，如果我们尝试绘制一条连接这两个示例的线，该线将是线性的，两个数据点将<strong class="ig hi">位于线</strong>上，因此<strong class="ig hi">成本函数将为零。</strong></p><p id="eb9a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但如果我们试图引入新的数据点，并要求模型做出预测，预测结果可能会非常可怕。因为模型具有<strong class="ig hi">高方差或模型在训练数据上过度拟合。</strong></p><p id="bc82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们<strong class="ig hi">引入一些偏差</strong>来管理高方差，我们可以使这个模型成为一个可行的模型。这样，<strong class="ig hi">成本函数将不会为零</strong>，并且模型将能够对新的数据点做出一些有效的预测。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es lq"><img src="../Images/d002f9791b9bba9751575c24a8cf965a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5z45dPzomeoghC_HCaw3qA.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">岭回归的新代价函数。</figcaption></figure><p id="e334" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">m-&gt;#训练示例</strong></p><p id="6f9b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> θ - &gt;【参数】</strong></p><p id="33bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> y- &gt;【真实值】</strong></p><p id="0115" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">h(x)——&gt;“预测值”</strong></p><p id="9c90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> λ- &gt;超参数/惩罚</strong></p><ul class=""><li id="c6d8" class="jx jy hh ig b ih ii il im ip lr it ls ix lt jb ke kf kg kh bi translated">如果<strong class="ig hi"> λ=0，那么就变成线性回归。</strong></li><li id="9d45" class="jx jy hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated">我们使用10倍交叉验证找到了<strong class="ig hi"> λ的最佳值。</strong></li><li id="a221" class="jx jy hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated"><strong class="ig hi"> λ的值越高，我们添加到模型中的偏差就越大。</strong></li><li id="92a3" class="jx jy hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated">如果我们对<strong class="ig hi"> λ使用更高的值，那么<strong class="ig hi"> </strong>线的斜率</strong>将会是<strong class="ig hi"> </strong>渐近地接近零，但决不会是零。</li></ul><h2 id="1a09" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">实施:-</h2><figure class="kw kx ky kz fd la"><div class="bz dy l di"><div class="lu lv l"/></div></figure><h2 id="800d" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated"><strong class="ak">套索回归(L1归一化):- </strong></h2><blockquote class="ko kp kq"><p id="3145" class="ie if kr ig b ih ii ij ik il im in io ks iq ir is kt iu iv iw ku iy iz ja jb ha bi translated"><strong class="ig hi">使用套索正则化(L1归一化)的回归模型被称为套索回归。</strong></p></blockquote><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es lw"><img src="../Images/869938e66d14d8aad74dd37367efbf81.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*8FrMe0fLwRKVWc339DpxMw.jpeg"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated">Lasso回归的成本函数。</figcaption></figure><p id="e556" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">m-&gt;#训练示例</strong></p><p id="7e0f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> θ - &gt;【参数】</strong></p><p id="73d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> y- &gt;【真实值】</strong></p><p id="99f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> h(x)- &gt;【预测值】</strong></p><p id="6b2a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> λ- &gt;超参数/惩罚</strong></p><ul class=""><li id="c028" class="jx jy hh ig b ih ii il im ip lr it ls ix lt jb ke kf kg kh bi translated">如果<strong class="ig hi"> λ=0，那么就变成线性回归。</strong></li><li id="a60f" class="jx jy hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated">我们使用10倍交叉验证找到了<strong class="ig hi"> λ的最佳值。</strong></li><li id="a499" class="jx jy hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated"><strong class="ig hi"> λ的值越高，我们添加到模型</strong>的偏差越大，如果我们使用<strong class="ig hi">更大的λ值，那么直线将变得平行于X轴，即斜率变为零。</strong></li></ul><p id="0038" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如你所见，在Lasso回归中，我们只添加了参数绝对值的总和<strong class="ig hi">。</strong></p><h2 id="e586" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">实施:-</h2><figure class="kw kx ky kz fd la"><div class="bz dy l di"><div class="lu lv l"/></div></figure><h2 id="ac67" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">弹性净回归:-</h2><blockquote class="ko kp kq"><p id="13aa" class="ie if kr ig b ih ii ij ik il im in io ks iq ir is kt iu iv iw ku iy iz ja jb ha bi translated"><strong class="ig hi">弹性网回归是脊和套索的结合。</strong></p></blockquote><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es lx"><img src="../Images/a5773907b94bb88c9cf874fd6caeec43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NNJnj6pNGGVRal0DTc1tBw.jpeg"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">弹性网络的新成本函数。它是脊和套索的结合。</figcaption></figure><p id="fde4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">m-&gt;#训练实例</strong></p><p id="ccb4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> θ - &gt;【参数】</strong></p><p id="8e82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> y- &gt;【真实值】</strong></p><p id="0879" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">h(x)——&gt;“预测值”</strong></p><p id="a4a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> λ- &gt;(超参数/罚值)L1、L2归一化的两个不同的λ。</strong></p><ul class=""><li id="abe6" class="jx jy hh ig b ih ii il im ip lr it ls ix lt jb ke kf kg kh bi translated">如果<strong class="ig hi"> λ1和λ2 =0，则变为线性回归。</strong></li><li id="d61f" class="jx jy hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated">我们使用10倍交叉验证找到了<strong class="ig hi"> λ的最佳值。</strong></li></ul><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es ly"><img src="../Images/d646d70ca27caa5667c98279915143a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g0cZXQaQG654ubRYJi8Ipg.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">这也是弹性网成本函数的一种写法。</figcaption></figure><p id="000c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于这个公式，我们有一个<strong class="ig hi"> λ和一个附加项α。</strong></p><ul class=""><li id="8809" class="jx jy hh ig b ih ii il im ip lr it ls ix lt jb ke kf kg kh bi translated">如果<strong class="ig hi"> α=0，则成为岭回归。</strong></li><li id="ad26" class="jx jy hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated">如果<strong class="ig hi"> α=1，则变成套索回归。</strong></li></ul><h2 id="e694" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">实施:-</h2><figure class="kw kx ky kz fd la"><div class="bz dy l di"><div class="lu lv l"/></div></figure><h2 id="9daf" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">什么时候使用脊，套索或弹性网？</h2><blockquote class="ko kp kq"><p id="f98a" class="ie if kr ig b ih ii ij ik il im in io ks iq ir is kt iu iv iw ku iy iz ja jb ha bi translated"><strong class="ig hi">岭回归:- </strong>当我们有一个小数据集，并且所有特征都非常重要和相关时。增加一些偏差有助于减少方差。</p><p id="663d" class="ie if kr ig b ih ii ij ik il im in io ks iq ir is kt iu iv iw ku iy iz ja jb ha bi translated"><strong class="ig hi">套索回归:- </strong>当我们有一个包含大量特征的小数据集，并且并非所有特征都有用时。通过排除无用的特征，它有助于减少差异。</p><p id="1641" class="ie if kr ig b ih ii ij ik il im in io ks iq ir is kt iu iv iw ku iy iz ja jb ha bi translated"><strong class="ig hi">弹性网络回归:- </strong>当我们有数百万个特征，并且我们事先不知道哪些是有用的/无用的特征，那么你应该选择弹性网络回归，而不是选择套索或山脊。它在处理相关参数方面也做得更好。</p></blockquote></div></div>    
</body>
</html>