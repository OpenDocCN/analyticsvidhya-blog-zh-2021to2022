<html>
<head>
<title>5 Question Series — Data Science &amp; AI — 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">5个问题系列—数据科学和人工智能— 4</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/5-question-series-data-science-ai-4-3070be5632a2?source=collection_archive---------7-----------------------#2021-08-12">https://medium.com/analytics-vidhya/5-question-series-data-science-ai-4-3070be5632a2?source=collection_archive---------7-----------------------#2021-08-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="9381" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我将写关于降维..为什么它很重要，降低特征维数的方法有哪些？</p><p id="e46b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Q1。我们所说的术语<strong class="ig hi"> <em class="jc">【维数灾难】</em> </strong>是什么意思，什么是降维？</p><p id="7392" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们需要解决问题的任何数据集中，我们都有独立和相关的特征。独立特征有助于根据以前的记录预测从属值。现在最重要的是选择哪些独立的特性，以及我们使用了多少。有时不必要的和不重要的特征会降低模型的准确性，因此选择对预测贡献更大的有价值的特征是非常重要的。增加特征的数量将增加模型的准确性，但它也有其局限性，在某个时间点之后，增加特征对模型准确性没有太大贡献，而是降低了准确性，因为如果我们增加不重要的特征，将导致较差的准确性，并且进一步增加会使准确性变差。这就是所谓的<strong class="ig hi"> <em class="jc">维度之祸。为了摆脱这个魔咒，我们降低了特征的维数，也就是说，我们减少了在预测中不重要的特征的数量，因此降维的概念出现了…</em></strong></p><p id="b456" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Q2。特征降维有哪些不同的方法？</p><p id="dac8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有几种方法:-</p><blockquote class="jd je jf"><p id="2a79" class="ie if jc ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">单变量选择</p><p id="1b16" class="ie if jc ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">特征重要性</p><p id="ec9c" class="ie if jc ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">相关热图矩阵</p><p id="af03" class="ie if jc ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">包装方法</p></blockquote><p id="4caf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1.预选</p><p id="fb71" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.反向选择</p><p id="543a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.递归特征消除。</p><p id="979a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们首先讨论包装方法:</p><p id="3aa2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正向选择-这是一种迭代选择模型，倾向于在每次迭代中选择一个额外的要素，并检查模型的准确性。循环继续，直到模型精度的饱和点。</p><p id="ec3b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">A — AB — ABC — ABCD — ABCDE</p><p id="3064" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">反向选择-它首先考虑所有的特征，并进行类似卡方检验的统计测试来确定准确度，并在每次迭代中继续减少一个特征，直到准确度继续增加。</p><p id="8917" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">ABCDE — ABCD — ABC — AB — A</p><p id="f346" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">递归特征消除——首先选择所有对目标变量影响最大的特征，然后添加另一个最重要的特征，并继续这样做，直到得到对目标变量影响较小的特征。</p><p id="16eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">注意:—包装方法仅用于较小的数据集。</em>T11】</strong></p><p id="66fb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">嵌入式选择-它使用单个特征或特征子集的随机选择，并检查准确性。它遵循排列组合法选择特征。选择具有最大精确度的特征子集作为模型。</p><p id="5072" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">单变量选择— </strong>在这种情况下，我们在python库的帮助下，根据重要性排序随机选择最佳特征，如<strong class="ig hi"> <em class="jc"> SelectKBest(信息增益)</em> </strong>和<strong class="ig hi"> <em class="jc"> Chi2(卡方)</em> </strong>，我们根据需要多少顶级特征来设置k值。</p><p id="c5af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">特征重要性— </strong>在这种情况下，我们为数据集中的每个特征获得分数，然后根据分数进行排名。我们根据可以预测大多数结果的分数来选择这些特征。我们对这些特征的预测得分越高。我们在此使用的各种算法，即额外的树分类器，主成分分析，线性判别分析，t-SNE，UMAP。大多数情况下，我们使用PCA或集成技术(额外的树分类器)来选择特征。</p><p id="86db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">关联热图— </strong>在该热图中，我们找出哪些独立特征彼此高度关联，然后移除这些关联特征中的一个关联特征。我们这样做是因为如果两个特征彼此高度相关，那么它们都将对因变量产生相同的影响或关系，从而降低模型的准确性或性能。我们使用皮尔逊相关和Spearmann等级法来找出相关性。在获得相关分数后，我们通过VIF(变化膨胀因子)或通过手动设置阈值来移除特征。</p><p id="7252" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc"> IMP:跟随克里希纳伊克视频对</em> </strong> <a class="ae jj" href="https://www.youtube.com/watch?v=k-EpAMjw6AE&amp;list=PLZoTAELRMXVPgjwJ8VyRoqmfNs2CJwhVH&amp;index=7" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="jc">功能选择</em> </strong> </a> <strong class="ig hi"> <em class="jc">进行详细讲解。也可以按照</em> </strong> <a class="ae jj" href="https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="jc">这篇文章</em> </strong> </a> <strong class="ig hi"> <em class="jc">对各种方法进行数学解释。</em> </strong></p><p id="b5ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Q3。什么是信息增益和互信息？</p><p id="547e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">信息增益用于计算数据的熵，熵(干扰或方差)是指在<strong class="ig hi">决策树模型</strong>中数据是如何分割的。它有助于测量分裂的纯度。信息增益降低了熵，有助于决策树的构建。分裂的熵介于0到1之间。o表示纯分裂，熵更小，其中1表示不纯分裂，是精确度方面的最差情况。为了有效分类，我们计算特征分裂的熵和信息增益，并选择给出较小熵和较高信息增益的特征分裂模式。信息增益也用于特征选择，并且经常用于计算关于目标变量的增益；被称为相互信息。互信息计算一个自变量对另一个自变量的依赖性。它也用于寻找两个变量之间的相关性。互信息的范围从0到1，意味着值越大，特征越相互依赖。</p><p id="bc6a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注:关于信息增益和互信息的数学细节，请阅读这篇关于机器学习的<a class="ae jj" href="https://machinelearningmastery.com/information-gain-and-mutual-information/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">信息增益和互信息</strong> </a> <strong class="ig hi"> </strong>的文章，并关注关于<a class="ae jj" href="https://www.youtube.com/watch?v=FuTRucXB9rA" rel="noopener ugc nofollow" target="_blank">信息增益</a>的<strong class="ig hi">克里斯·纳伊克</strong>视频。</p><p id="966d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">熵的计算方法是:</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es jk"><img src="../Images/a3a7049353b1a3b205c92338a3657915.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*HRFYGHR1BKKVtYYKs6E8aA.png"/></div></figure><p id="a8b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中H(s)是熵，P-是负面事件的概率，P+是正面事件的概率。熵的范围在(0到1)之间。</p><p id="49e7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">信息增益:</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es js"><img src="../Images/6e67e1a79920a3afc527b3118d24dae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*GKV7kSMK-y9YCLaf-97YoA.png"/></div></figure><p id="5522" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中，S是主要或第一特征，Sv是分割后的特征。</p><p id="a119" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Q4。Kullback — Leibler散度还是KL散度？</p><p id="d604" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">统计学中的KL散度用于比较两种不同的概率分布，实际的或观察到的概率分布。这也用于计算互信息和熵。这种比较可以通过计算两个分布之间的统计距离或者通过计算两个分布之间的散度来完成。KL散度使用散度方法来比较两个分布。如果一个随机变量有两个分布P和Q，那么P和Q之间的散度可以计算为:P的每个事件的概率之和乘以事件P的概率的对数除以事件Q的概率。</p><ul class=""><li id="c2c9" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">KL(P | | Q)= sum X in X P(X)* log(P(X)/Q(X))</li></ul><p id="a952" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果P中事件的概率比Q中事件的概率大，那么散度就更大，反之亦然。</p><p id="8c24" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">注:我们可以跟随Jason Brownlee写的这篇关于</em> </strong> <a class="ae jj" href="https://machinelearningmastery.com/divergence-between-probability-distributions/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="jc"> KL发散</em> </strong> </a> <strong class="ig hi"> <em class="jc">的漂亮而详细的文章。</em>T13】</strong></p><p id="bc4b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Q5。詹森-香农散度和它与KL散度有什么不同？？</p><p id="9976" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">JS散度也用于比较概率分布之间的差异。JS散度和KL散度的区别在于，在KL散度中，P与Q的散度和Q与P的散度是不同的，或者我们可以说，KL散度的散度比较是不对称的，这在JS散度中转化为对称的。即，在KL散度中，我们可以写成:</p><ul class=""><li id="9f14" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">KL(P || Q)！= KL(Q || P)</li></ul><p id="ea7c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这在JS发散上是对称的；我们可以把它写成:</p><ul class=""><li id="3d43" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">JS(P || Q) == JS(Q || P)</li></ul><p id="e8ca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中JS散度计算如下:</p><ul class=""><li id="295b" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">JS(P | | Q)= 1/2 * KL(P | | M)+1/2 * KL(Q | | M)</li><li id="bc94" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">M = 1/2 * (P + Q)</li></ul><p id="a498" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">注:方程取自杰森·布朗利(Jason Brownlee)写的</em></strong><em class="jc"/><a class="ae jj" href="https://machinelearningmastery.com/divergence-between-probability-distributions/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"><em class="jc">KL发散</em> </strong> </a> <strong class="ig hi"> <em class="jc">。</em>T29】</strong></p><p id="83ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">额外问题:</p><p id="5d1e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">6.关于统计数据，p值意味着什么？</p><p id="c8a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">7.什么是假设和假设检验？</p><p id="952e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">8.什么是混淆矩阵和错误类型？</p><p id="1abb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要了解以上问题，请阅读我在以下网站上的文章:</p><div class="kh ki ez fb kj kk"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/confusion-matrix-a2db318ef1f5"><div class="kl ab dw"><div class="km ab kn cl cj ko"><h2 class="bd hi fi z dy kp ea eb kq ed ef hg bi translated">混淆矩阵</h2><div class="kr l"><h3 class="bd b fi z dy kp ea eb kq ed ef dx translated">我们在学校和大学的课堂上都学习过矩阵和向量。井矩阵是一种N…</h3></div><div class="ks l"><p class="bd b fp z dy kp ea eb kq ed ef dx translated">medium.com</p></div></div><div class="kt l"><div class="ku l kv kw kx kt ky jq kk"/></div></div></a></div><div class="kh ki ez fb kj kk"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/probability-and-likelihood-b62f015b65ce"><div class="kl ab dw"><div class="km ab kn cl cj ko"><h2 class="bd hi fi z dy kp ea eb kq ed ef hg bi translated">概率和可能性</h2><div class="kr l"><h3 class="bd b fi z dy kp ea eb kq ed ef dx translated">概率是某些事件的确切结果。你可能知道一件事情发生的结果是什么</h3></div><div class="ks l"><p class="bd b fp z dy kp ea eb kq ed ef dx translated">medium.com</p></div></div><div class="kt l"><div class="kz l kv kw kx kt ky jq kk"/></div></div></a></div><p id="ce1a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于假设检验:</p><div class="kh ki ez fb kj kk"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/linear-regression-and-fitting-a-line-to-a-data-6dfd027a0fe2"><div class="kl ab dw"><div class="km ab kn cl cj ko"><h2 class="bd hi fi z dy kp ea eb kq ed ef hg bi translated">线性回归和对数据进行直线拟合</h2><div class="kr l"><h3 class="bd b fi z dy kp ea eb kq ed ef dx translated">线性回归是预测连续值输出的监督机器学习算法。在线性…</h3></div><div class="ks l"><p class="bd b fp z dy kp ea eb kq ed ef dx translated">medium.com</p></div></div><div class="kt l"><div class="la l kv kw kx kt ky jq kk"/></div></div></a></div></div><div class="ab cl lb lc go ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ha hb hc hd he"><p id="dfca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">希望你喜欢。如果你想让我添加任何东西或纠正任何东西，请在评论中提及，并指导我更多类似的问题。我的大部分作品我都是从<a class="ae jj" href="https://www.youtube.com/channel/UCNU_lfiiWBdtULKOw6X0Dig" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="jc">克里希·纳伊克</em> </strong> </a>先生的视频和从<a class="ae jj" href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"><em class="jc">stat quest</em></strong></a>中借鉴而来。这是YouTube上最具生产力和最棒的两个数据科学频道。</p></div></div>    
</body>
</html>