<html>
<head>
<title>Deep Learning Fundamental Concept with Keras Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有Keras代码的深度学习基本概念</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-learning-fundamental-concept-with-keras-code-8293640699b?source=collection_archive---------14-----------------------#2021-03-23">https://medium.com/analytics-vidhya/deep-learning-fundamental-concept-with-keras-code-8293640699b?source=collection_archive---------14-----------------------#2021-03-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="2f1f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Keras代码实施— Abhishek Adarsh Mishra</p><div class="jc jd je jf fd ab cb"><figure class="jg jh ji jj jk jl jm paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><img src="../Images/1db151b4ed588d98c966cc1ddfb8fd7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*5hdZPOD7FzAlsliRKlTI_w.png"/></div></figure><figure class="jg jh jt jj jk jl jm paragraph-image"><img src="../Images/06c6a682bf19d8089718fed11d2f951f.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*f5JvGwbcvG2OmESkMm6RNA.png"/><figcaption class="ju jv et er es jw jx bd b be z dx jy di jz ka translated">深度神经网络与Keras代码实现</figcaption></figure></div><h1 id="a871" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">机器学习</strong></h1><p id="134e" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">机器学习是使用算法分析数据，从数据中学习，然后决定或预测新数据的实践。</p><ul class=""><li id="475c" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">从这些数据中学习。</li></ul><p id="03bf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该算法分析给定的媒体数据，并学习对负面文章和正面文章进行分类的特征。</p><h1 id="86c4" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">深度学习</h1><p id="a652" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">深度学习是机器学习的一个子领域，它使用受大脑神经网络结构和功能启发的算法。</p><h1 id="680a" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">人工神经网络</strong></h1><p id="db26" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">人工神经网络是一个计算系统，它由一组称为神经元的连接单元组成，这些单元被组织成我们所说的层。</p><ul class=""><li id="fd62" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">人工神经网络是用我们称之为神经元的东西构建的。</li><li id="3c7f" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">人工神经网络中的神经元被组织成我们所说的层。</li><li id="ccc4" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">ANN中的层(除了输入和输出层之外的所有层)称为隐藏层。</li><li id="eec5" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">如果一个ANN有一个以上的隐藏层，则称该ANN为深层ANN</li></ul><p id="59a7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">节点被组织成我们所说的层。在最高级别，每个人工神经网络中有三种类型的层:</p><ul class=""><li id="ba09" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">输入层</li><li id="893b" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">隐藏层</li><li id="de40" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">输出层</li></ul><p id="4649" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">深度学习Keras序列模型</strong></p><blockquote class="ls lt lu"><p id="3a18" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.models导入序列</p><p id="fb96" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.layers导入密集，激活</p><p id="6234" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">#生成一个神经网络</p><p id="0536" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型=顺序([</p><p id="e9bf" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(5，input_shape=(3，)，activation='relu ')，#隐藏层，shape是输入层</p><p id="49de" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(2，activation='softmax ')，#输出层</p><p id="2c6e" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">])</p></blockquote><h1 id="4633" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">神经网络中的层</strong></h1><p id="383b" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">然而，存在不同类型的层。一些例子包括:</p><ul class=""><li id="9fbb" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">密集(或完全连接)层</li><li id="7fc0" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">卷积层</li><li id="06aa" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">池层</li><li id="a72e" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">循环层</li><li id="0931" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">标准化图层</li></ul><p id="2591" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">层权重</strong></p><p id="bb94" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">两个节点之间的每个连接都有一个关联权重，它只是一个数字。</p><p id="8ff2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个权重代表两个节点之间的连接强度。当网络在输入层的给定节点接收到输入时，该输入通过连接传递到下一个节点，并且该输入将乘以分配给该连接的权重。</p><blockquote class="ls lt lu"><p id="8858" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">节点输出=激活(输入的加权和)</p></blockquote><p id="b5ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">前向传递神经网络</strong></p><p id="0459" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦我们获得给定节点的输出，所获得的输出就是作为输入传递给下一层中的节点的值。</p><p id="3d3d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">寻找最佳权重</strong></p><p id="ea3c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随着模型的学习，所有连接处的权重都会更新和优化，以便输入数据点映射到正确的输出预测类</p><blockquote class="ls lt lu"><p id="c83e" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">将numpy作为np导入</p><p id="0924" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从scipy导入ndimage</p><p id="d199" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">将matplotlib.pyplot作为plt导入</p><p id="cbc6" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">%matplotlib内联</p><p id="86e7" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">img = NP . expand _ dims(ndimage . im read(' NN。PNG ')、0)</p><p id="cadf" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">plt.imshow(img[0])</p></blockquote><h1 id="ec91" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">激活功能</h1><p id="47d7" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">在人工神经网络中，激活函数是将节点的输入映射到其相应输出的函数。</p><p id="e5f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">乙状结肠激活功能</strong></p><p id="55a7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Sigmoid接收输入并执行以下操作:</p><ul class=""><li id="0f88" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">对于大多数负输入，sigmoid会将输入转换为非常接近0的数字。</li><li id="b0ae" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">对于大多数正输入，sigmoid会将输入转换为非常接近1的数字。</li><li id="7708" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">对于相对接近0的输入，sigmoid会将输入转换为0到1之间的某个数字。</li></ul><blockquote class="ls lt lu"><p id="75f2" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">sigmoid(x) = e^x/(e^x + 1)</p></blockquote><p id="7fa8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">激活功能直觉</strong></p><p id="466f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">嗯，激活功能是由我们大脑中的活动激发的，不同的神经元被不同的刺激激活。</p><p id="44f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，如果你闻到一些令人愉快的味道，比如新鲜出炉的饼干，你大脑中的某些神经元就会被激活。如果你闻到不愉快的味道，比如变质的牛奶，这将导致你大脑中的其他神经元放电。</p><blockquote class="ls lt lu"><p id="8e89" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated"><strong class="ig hi"> relu </strong>激活功能=整流器线性单元</p><p id="abdc" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated"><strong class="ig hi"> relu(x) </strong> = max(0，x)</p></blockquote><p id="41a7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">带激活功能的代码</strong></p><blockquote class="ls lt lu"><p id="9dae" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.models导入序列</p><p id="e22c" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.layers导入密集，激活</p><p id="4f0e" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型=顺序([</p><p id="1a46" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">密集(单位=5，输入形状=(3，)，激活='relu ')</p><p id="3d51" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">])</p><p id="10da" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型=顺序()</p><p id="001b" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">model.add(Dense(units=5，input_shape=(3，))</p><p id="45b3" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">model.add(激活(' relu '))</p></blockquote><h1 id="d1dc" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">训练人工神经网络</h1><p id="db50" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated"><strong class="ig hi">优化算法</strong></p><p id="cc85" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用我们称之为优化算法的方法来优化权重。</p><p id="b45f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最广为人知的优化器被称为<strong class="ig hi">随机梯度下降</strong>，或者更简单地说，SGD。</p><p id="3804" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们有任何优化问题时，我们必须有一个优化目标，所以现在让我们考虑SGD在优化模型权重方面的目标是什么。</p><p id="86f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SGD的目标是最小化某个给定的函数，我们称之为损失函数。</p><p id="aebe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">损失函数</strong></p><p id="90e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个常见的损失函数是均方误差(MSE ),但我们可以用几个损失函数来代替它</p><p id="9139" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">损失是网络对图像的预测与图像的真实标签之间的误差或差异，SGD将尝试最小化该误差，以使我们的模型尽可能准确地进行预测。</p><h1 id="f105" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">解释了神经网络如何学习</strong></h1><p id="5193" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">一个<strong class="ig hi">时期</strong>指的是在训练期间整个数据集到网络的一次传递。</p><p id="c7eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">20个历元意味着传递了20次数据</p><p id="52c7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">损失函数的梯度</strong></p><p id="d140" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">计算损失后，为网络中的每个权重计算该损失函数的梯度。</p><p id="ac5e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们已经计算了单个输出的损失，并计算了单个选定重量的损失梯度。这个计算是使用一种叫做**反向传播，* *的技术来完成的</p><p id="cb56" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">学习率</strong></p><p id="3f00" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们将梯度值乘以一个叫做学习率的东西。学习率是一个很小的数字，通常在0.01和0.0001之间，但实际值可以变化。</p><p id="5706" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">学习率告诉我们在最小值的方向上应该走多大的<strong class="ig hi">步</strong>。(更新)</p><p id="27fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">更新权重</strong></p><p id="066b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好的，我们用学习率乘以梯度，然后从权重中减去这个乘积，这将给出这个权重的新的更新值。</p><blockquote class="ls lt lu"><p id="e7c6" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">新wt=旧wt —(学习率*梯度)</p></blockquote><p id="5d93" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">代号</strong></p><blockquote class="ls lt lu"><p id="b4c7" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">导入keras</p><p id="a47b" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">将numpy作为np导入</p><p id="d0d4" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.models导入序列</p><p id="b353" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.layers导入激活</p><p id="fc0a" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.layers.core导入密集</p><p id="fe06" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">优化程序导入Adam</p><p id="6f2a" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.metrics导入分类交叉熵</p><p id="f3c3" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型=顺序([</p><p id="1839" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(units=16，input_shape=(1，)，activation='relu ')，</p><p id="cc82" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">密集(单位=32，激活='relu ')，</p><p id="9724" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">密集(单位=2，激活= ' s形')</p><p id="d7d8" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">])</p><p id="36f2" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型.编译(</p><p id="5c03" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">optimizer = Adam(learning _ rate = 0.0001)，</p><p id="8d48" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">loss= '稀疏分类交叉熵'，</p><p id="6c37" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">度量=['准确性']</p><p id="42b6" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">)</p><p id="ce59" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">train_samples = np.array([</p><p id="099b" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">[150, 67],</p><p id="c0a8" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">[130, 60],</p><p id="504c" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">[200, 65],</p><p id="f57d" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">[125, 52],</p><p id="7326" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">[230, 72],</p><p id="a92d" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">[181, 70]</p><p id="5ae7" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">])</p><p id="1fa2" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated"># 0:男性</p><p id="a620" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated"># 1:女性</p><p id="c6e0" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">train_labels = np.array([1，1，0，1，0，0])</p><p id="6106" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">#scaled_train_samples是由训练样本组成的numpy数组。</p><p id="de09" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">#train_labels是一个numpy数组，由训练样本的相应标签组成。</p><p id="833d" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型. fit(</p><p id="3318" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">x =训练样本，</p><p id="5a3e" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">y =火车标签，</p><p id="5850" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">batch_size=3</p><p id="dd6c" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">纪元=10，</p><p id="26ca" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">洗牌=真，</p><p id="105f" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">详细=2，</p><p id="c273" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">)</p></blockquote><h1 id="1630" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">神经网络中的损失函数</h1><blockquote class="ls lt lu"><p id="dd05" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">误差=输出-实际值</p><p id="fb4a" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">平均平方误差=总和((输出-实际) )</p></blockquote><p id="d80a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以看到指定的损失函数<strong class="ig hi">loss = ' sparse _ category _ cross entropy '</strong>。</p><p id="fcf7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">代码</strong></p><blockquote class="ls lt lu"><p id="8c69" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型=顺序([</p><p id="a7ce" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(16，input_shape=(1，)，activation='relu ')，</p><p id="7779" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(32，激活='relu ')，</p><p id="566e" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(2，激活='sigmoid ')</p><p id="fd85" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">])</p><p id="9d2f" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型.编译(</p><p id="c5d2" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">亚当(learning_rate=.0001)，</p><p id="1f36" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">损失= '稀疏分类交叉熵'，#损失函数</p><p id="a3da" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">度量=['准确性']</p><p id="8e40" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">)</p></blockquote><p id="0d43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当前可用于Keras的损失函数如下:</p><ul class=""><li id="9722" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">均方误差</li><li id="0334" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">平均绝对误差</li><li id="15e4" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">平均值绝对百分比误差</li><li id="7b4e" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">均方对数误差</li><li id="197e" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">方形铰链</li><li id="2d07" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">关键</li><li id="7e61" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">分类铰链</li><li id="ef14" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">对数曲线</li><li id="cedd" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">分类交叉熵</li><li id="022a" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">稀疏分类交叉熵</li><li id="cc27" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">二元交叉熵</li><li id="4c2d" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">库尔贝克_莱布勒_散度</li><li id="d87e" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">泊松</li><li id="6fa8" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">余弦近似</li></ul><h1 id="63b6" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">喀拉的学习率</h1><blockquote class="ls lt lu"><p id="d0fc" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型=顺序([</p><p id="8ec4" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(units=16，input_shape=(1，)，activation='relu ')，</p><p id="173d" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(units=32，activation='relu '，kernel _ regulator = regulators . L2(0.01))，</p><p id="37ea" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">密集(单位=2，激活= ' s形')</p><p id="478a" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">])</p><p id="0b8e" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型.编译(</p><p id="ee4d" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">optimizer = Adam(learning _ rate = 0.0001)，</p><p id="0517" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">loss= '稀疏分类交叉熵'，</p><p id="1457" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">度量=['准确性']</p><p id="2c5f" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">)</p></blockquote><h1 id="eefa" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">解释了培训、测试和验证集</h1><p id="3cb1" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated"><strong class="ig hi">训练集</strong></p><p id="dd52" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">训练集就像它听起来的那样。这是用来训练模型的数据集。在每个时期，我们的模型将在我们的训练集中对相同的数据进行一次又一次的训练，它将继续学习这些数据的特征。</p><p id="6864" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">验证设置</strong></p><p id="b40e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">验证集是一组独立于训练集的数据，用于在训练期间验证我们的模型。这一验证过程有助于提供有助于我们调整超参数的信息。</p><p id="525c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们需要验证集的主要原因之一是确保我们的模型不会<strong class="ig hi">过度适应</strong>训练集中的数据。</p><p id="5e3b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">验证集让我们可以看到模型在训练过程中概括得有多好。</p><p id="1dda" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">测试设置</strong></p><p id="e743" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">测试集是在模型已经被训练之后用于测试模型的一组数据。测试集独立于训练集和验证集。</p><p id="c044" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在将模型部署到生产环境之前，测试集提供了对模型泛化能力的最终检查。</p><p id="0a6a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">代号</strong></p><blockquote class="ls lt lu"><p id="5539" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型. fit(</p><p id="b0d9" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">x=scaled_train_samples，#scaled_train_samples是由训练样本组成的numpy数组。</p><p id="ed40" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">y=train_labels，#train_labels是一个numpy数组，由训练样本的相应标签组成。</p><p id="39c9" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">验证数据=有效设置，</p><p id="3fb0" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">batch_size=10</p><p id="0ab3" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">纪元=20，</p><p id="3691" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">洗牌=真，</p><p id="8715" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">详细=2</p><p id="e08a" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">)</p></blockquote><h1 id="a808" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">用神经网络预测</h1><p id="a0f0" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">预测是基于模型在训练中所学到的东西。</p><p id="64ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">传递无标签的样品</p><p id="b3ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">在现实世界中部署模型(生产)</strong></p><p id="a454" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">除了在我们的测试数据上运行预测，我们还可以让我们的模型在现实世界的数据上预测，一旦它被部署来服务于它的实际目的。</p><blockquote class="ls lt lu"><p id="b883" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">预测=模型.预测(</p><p id="cb55" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">x =缩放测试样本，</p><p id="8164" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">batch_size=10</p><p id="a005" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">详细=0</p><p id="ac4f" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">)</p></blockquote><h1 id="47e4" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">过拟合</strong></h1><p id="6947" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">当我们的模型变得非常擅长对包含在训练集中的数据进行分类或预测，但不擅长对未经训练的数据进行分类时，就会发生过度拟合。因此，从本质上讲，模型过度拟合了训练集中的数据。</p><p id="7b11" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果在训练期间，模型的指标很好，但是当我们使用模型预测测试数据时，它不能准确地对测试集中的数据进行分类，我们也可以认为我们的模型是过度拟合的。</p><p id="68b9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">减少过拟合</strong></p><ul class=""><li id="05ac" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">向训练集添加更多数据</li><li id="b3ff" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">数据扩充(通过裁剪、旋转、翻转来添加新数据或修改旧数据</li><li id="a4cd" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">降低模型的复杂性(去除神经元层)</li><li id="a151" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">丢弃一些神经元(数据)</li></ul><h1 id="12b8" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">欠拟合</h1><p id="6f0b" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">当一个模型不能对它被训练的数据进行分类时，它就被认为是不适合的。</p><p id="c3f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当给定的训练数据的度量很差时，我们可以知道模型是欠拟合的，这意味着模型的训练精度低和/或训练损失高。</p><p id="8d22" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">减少欠配合</strong></p><ul class=""><li id="f261" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">增加模型的复杂性</li><li id="15ec" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">增加模型中的层数。</li><li id="1c78" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">增加每层中神经元的数量。</li><li id="93c0" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">改变我们使用的图层类型和位置。</li><li id="283d" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">向输入样本添加更多要素(更多列)</li><li id="2d69" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">降低辍学率</li></ul><h1 id="0857" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">监督学习</strong></h1><ul class=""><li id="6bdc" class="le lf hh ig b ih kz il la ip lz it ma ix mb jb lj lk ll lm bi translated">标记数据</li><li id="e0b5" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">标签用于监督或指导学习过程。</li></ul><h1 id="47aa" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">无监督学习</h1><p id="8718" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">无监督学习发生在未标记的数据上。</p><p id="f985" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它将尝试从数据中学习某种类型的结构，并从这些数据中提取有用的信息或特征。</p><p id="1799" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它将学习如何创建从给定输入到特定输出的映射，这是基于它对没有任何标签的数据结构的学习。</p><p id="06c8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">无监督学习示例</p><ul class=""><li id="9f54" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">聚类算法</li><li id="9a1f" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">自动编码器</li></ul><p id="f7c9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">自动编码器是一种人工神经网络，它接收输入，然后输出该输入的重构。</p><h1 id="fdf2" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">半监督学习</h1><p id="ae66" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">半监督学习使用监督和非监督学习技术的组合，这是因为，在我们使用半监督学习的场景中，我们将有标记和未标记数据的组合。</p><ul class=""><li id="6d86" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">标记数据上的第一列</li><li id="9b37" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">然后用先前训练的标记数据预测的伪标记来训练剩余数据。</li></ul><p id="4219" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">伪标记</strong>允许我们在大得多的数据集上进行训练。</p><h1 id="9d23" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">数据扩充</h1><p id="895b" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">当我们根据对现有数据的修改创建新数据时，就会发生这种情况。</p><p id="b62c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所有这些都是常见的数据扩充技术。</p><ul class=""><li id="fb68" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">水平翻转</li><li id="f5cf" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">垂直翻转</li><li id="ae81" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">旋转</li><li id="8719" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">放大</li><li id="3439" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">缩小</li><li id="90c4" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">种植</li><li id="8837" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">颜色变化</li></ul><p id="09dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">用于在过度拟合的情况下增加数据数量。</p><h1 id="57d6" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">一键编码</h1><p id="db10" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">冷热值</p><p id="df96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一种广泛用于用数值对分类数据进行编码的编码类型被称为一次性编码。</p><p id="c24a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一键编码将我们的分类标签转换成0和1的向量。这些向量的长度是我们的模型期望分类的类或类别的数量。</p><p id="2cf3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">0和1的向量</p><p id="c6d9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们对图像进行分类，是狗还是猫，那么我们对应于这些类别的独热编码向量的长度都是2，反映了这两个类别。</p><p id="764a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们添加另一个类别，如蜥蜴，这样我们就可以分类图像是狗、猫还是蜥蜴，那么我们对应的独热编码向量的长度都将是3，因为我们现在有三个类别。</p><ul class=""><li id="7b7b" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">标签索引-0索引-1索引-2</li><li id="1aae" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">第1 0 0类</li><li id="6efb" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">狗0 1 0</li><li id="9f49" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">蜥蜴0 0 1</li></ul><p id="c28e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — -</p><ul class=""><li id="c247" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">标签向量</li><li id="1c51" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">猫[1，0，0，0]</li><li id="9513" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">狗[0，1，0，0]</li><li id="5040" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">蜥蜴</li><li id="52f0" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">美洲驼[0，0，0，1]</li></ul><h1 id="0971" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">卷积神经网络</h1><p id="e444" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">CNN是一种人工神经网络，具有某种类型的专门化，能够挑选或检测模式。这种模式检测使得CNN对图像分析如此有用。</p><p id="40ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">CNN有被称为卷积层的隐藏层，这些层构成了CNN，嗯…CNN！</p><p id="6ae6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">滤波器和卷积运算</p><p id="04a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如前所述，卷积神经网络能够检测图像中的模式。</p><p id="6714" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于每个卷积层，我们需要指定该层应该具有的滤波器数量。这些过滤器实际上是用来检测模式的。</p><p id="af4f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是我们所说的模式。</p><ul class=""><li id="f608" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">优势</li><li id="ef33" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">形状</li><li id="4964" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">口感</li><li id="e9b5" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">曲线</li><li id="e099" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">目标</li><li id="db1c" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">颜色；色彩；色调</li></ul><figure class="jc jd je jf fd jh er es paragraph-image"><div class="er es mc"><img src="../Images/30a6bd4b9c68671505376c765353d817.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*usZoVVjpv6HRoX8zKEetAA.png"/></div></figure><h1 id="38df" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">可视化卷积滤波器</h1><blockquote class="ls lt lu"><p id="bd25" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">#使用ImageNet权重构建VGG16网络</p><p id="01b8" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">model = VGG16(weights='imagenet '，include_top=False)</p><p id="dc65" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">打印(“模型已加载”)</p><p id="d573" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型.摘要()</p><p id="e596" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">#输入图像的占位符</p><p id="bdbf" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">输入_输入=模型.输入</p><p id="419c" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">#获取每个“关键”层的符号输出(我们给出了唯一的名称)</p><p id="3366" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">layer_dict = dict([(layer.name，layer)for layer in model . layers[1:]])</p></blockquote><p id="e949" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">处理</strong></p><blockquote class="ls lt lu"><p id="de85" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">model = VGG16(weights='imagenet '，include_top=False)</p><p id="3cb7" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">#我们建立了一个最大化激活的损失函数</p><p id="983e" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">所考虑层的第n个过滤器的数量</p><p id="48a1" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">层输出=层字典[层名称]。输出</p><p id="a8bc" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">if k . image _ data _ format()= = ' channels _ first ':</p><p id="8209" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">loss = K.mean(layer_output[:，filter_index，:))</p><p id="a8f8" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">否则:</p><p id="12a8" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">loss = K.mean(图层输出[:，:，:，过滤器索引])</p><p id="4b14" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">#我们计算输入图像在这种损失下的梯度</p><p id="61fd" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">grads = K.gradients(loss，input_img)[0]</p><p id="0d27" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">#我们从带有随机噪声的灰度图像开始</p><p id="1439" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">if k . image _ data _ format()= = ' channels _ first ':</p><p id="f8ec" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">input _ img _ data = NP . random . random((1，3，img_width，img_height))</p><p id="f196" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">否则:</p><p id="34da" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">input _ img _ data = NP . random . random((1，img_width，img_height，3))</p><p id="11f4" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">input _ img _ data =(input _ img _ data—0.5)* 20+128</p><p id="4aee" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">save _ img(' stitched_filters _ % dx % d . png ' %(n，n)，stitched _ filters)</p></blockquote><h1 id="9307" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">零填充</h1><p id="911e" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">图像尺寸通过过滤而减小。</p><p id="4196" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了便于形象化，让我们来看一个更小的例子。这里我们有一个大小为4 x 4的输入，然后是一个3 x 3的滤波器。让我们看看这个滤波器可以对输入进行多少次卷积，以及最终的输出大小是多少。</p><figure class="jc jd je jf fd jh er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es md"><img src="../Images/7826266da00391b3b038fa0e3821df2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dpx_T7ZdQUBCBYV3UPi97w.png"/></div></div></figure><blockquote class="ls lt lu"><p id="375a" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">图像= (n* <em class="hh"> n)滤镜= (f* </em> f)输出= (n-f+1) * (n-f+g)</p></blockquote><p id="7b5b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以角落丢了。</p><p id="e13f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">什么是零填充？</strong></p><p id="327a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们现在知道零填充反对什么问题，但它实际上是什么？</p><p id="02ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们在输入图像的边缘周围添加一个像素值都为零的边界时，就会出现零填充。这在图像外部添加了一种零填充，因此被称为零填充。回到前面的小例子，如果我们用零值像素填充输入，让我们看看卷积输入后的输出大小。</p><figure class="jc jd je jf fd jh er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es me"><img src="../Images/cdf5948ca5583e66f2d7d6f05d60c311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1JXv4EQlhic8NZGmKEBXFg.png"/></div></div></figure><ul class=""><li id="4697" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">填充-类型-描述影响</li><li id="2ae4" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">“有效”-无填充-尺寸减小</li><li id="ba5f" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">“相同”-边缘周围的零-尺寸保持不变</li></ul><p id="a4dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">使用Keras中的代码</strong></p><blockquote class="ls lt lu"><p id="7cf6" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">导入keras</p><p id="ad4f" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.models导入序列</p><p id="5fa6" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.layers导入激活</p><p id="3c6a" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.layers.core导入密集、展平</p><p id="585c" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">来自keras.layers .卷积导入*</p><p id="c52c" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">model_valid =顺序([</p><p id="96e7" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(16，input_shape=(20，20，3)，activation='relu ')，</p><p id="dc39" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Conv2D(32，kernel_size=(3，3)，activation='relu '，padding='valid ')，</p><p id="3a00" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Conv2D(64，kernel_size=(5，5)，activation='relu '，padding='valid ')，</p><p id="2560" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Conv2D(128，kernel_size=(7，7)，activation='relu '，padding='valid ')，</p><p id="c89a" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Flatten()，</p><p id="9494" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(2，activation='softmax ')</p><p id="c18c" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">])</p><p id="7e67" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型_有效.摘要()</p></blockquote><h1 id="cbf0" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">最大池化</h1><p id="c535" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">它减小了矩阵的大小</p><p id="5379" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">为什么要使用Max Pooling？</strong></p><ul class=""><li id="e57e" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">减少计算量和参数</li><li id="46d7" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">减少过度拟合</li></ul><figure class="jc jd je jf fd jh er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mf"><img src="../Images/439cdb5eabbf5b6e10dcb85462bd26f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6WaBu6qktfGkHR6IzQCb8A.png"/></div></div></figure><p id="8f4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">代码</strong></p><blockquote class="ls lt lu"><p id="cd82" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">导入keras</p><p id="e12c" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.models导入序列</p><p id="e730" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.layers导入激活</p><p id="f8f7" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.layers.core导入密集、展平</p><p id="d4b9" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">来自keras.layers .卷积导入*</p><p id="e8b4" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.layers.pooling导入*</p><p id="5786" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">model_valid =顺序([</p><p id="1724" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(16，input_shape=(20，20，3)，activation='relu ')，</p><p id="3d76" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Conv2D(32，kernel_size=(3，3)，activation='relu '，padding='same ')，</p><p id="5cd0" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">MaxPooling2D(pool_size=(2，2)，strides=2，padding='valid ')，</p><p id="4ad7" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Conv2D(64，kernel_size=(5，5)，activation='relu '，padding='same ')，</p><p id="1628" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Flatten()，</p><p id="7606" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(2，activation='softmax ')</p><p id="3ef8" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">])</p><p id="83af" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型_有效.摘要()</p></blockquote><h1 id="0e01" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">神经网络中的反向传播</h1><ul class=""><li id="1888" class="le lf hh ig b ih kz il la ip lz it ma ix mb jb lj lk ll lm bi translated">随机梯度下降</li><li id="a6cb" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">正向传播</li><li id="d7b3" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">计算损失</li></ul><p id="6340" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">反向传播是梯度下降用来计算损失函数的梯度的工具。</p><p id="a258" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">反向传播直觉</strong></p><p id="3d5d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了更新权重，梯度下降将从查看来自输出节点的激活输出开始。</p><figure class="jc jd je jf fd jh er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mg"><img src="../Images/ed1333bd2a6772b29abb15a1ae58f1a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28aLGfiKJkgb2yRgiOJQ8Q.png"/></div></div></figure><figure class="jc jd je jf fd jh er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es md"><img src="../Images/77c5909cf0ffed370356a089b7219bcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-um9CLX2OGdpPu5ns7R3fQ.png"/></div></div></figure><p id="0322" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">反向传播的数学观测值</strong></p><ul class=""><li id="7974" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">观察我们如何表达损失函数</li><li id="dd4e" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">观察我们如何表达给定节点的输入和输出。</li><li id="728c" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">观察我们如何区分损失函数。</li></ul><h1 id="4895" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">消失和爆炸梯度解释|由反向传播引起的问题</h1><p id="840f" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated"><strong class="ig hi">什么是渐变消失问题？</strong></p><p id="8213" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通常，消失梯度问题是在训练神经网络时引起主要困难的问题。更具体地说，这是一个涉及网络早期层中权重的问题。</p><p id="79da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，有时网络前几层的权重梯度变得很小，几乎为零。因此，消失梯度。</p><p id="603d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">爆炸梯度</strong>现在考虑计算相同重量的梯度，但不是很小的项，如果它们很大呢？我们所说的“大”是指不止一个。</p><p id="3f72" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们把一堆都大于1的项相乘，我们会得到大于1的数，甚至可能比1大很多。</p><h1 id="ab84" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">重量初始化</h1><p id="10b2" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">具体来说，每当我们构建和编译一个网络时，权重的值将被设置为随机数。每个重量一个随机数。典型地，这些随机数将是正态分布的，使得这些数的分布具有0的平均值和1的标准偏差。</p><p id="49d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">随机初始化的问题</strong></p><p id="2c3f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们的激活函数的期望输出位于饱和的相反侧，那么在训练期间，当SGD更新权重以试图影响激活输出时，它只会对该激活输出的值进行非常小的改变，甚至几乎不会在正确的方向上递增。</p><p id="d3fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> Xavier初始化</strong></p><p id="a16b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">事后看来，我们应该能够回过头来看看我们已经讨论过的问题，并追溯到这些问题是由加权和的方差大于或小于1引起的。所以要解决这个问题，我们能做的就是迫使这个方差变小。</p><blockquote class="ls lt lu"><p id="e34e" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">var(wt)= 2/n</p><p id="9707" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">更新-</p><p id="4e6f" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">重量=根(2/n)</p><p id="bcbe" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">var(wt) = 2/((n)in + (n)out)</p></blockquote><p id="e57f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">代码</strong></p><blockquote class="ls lt lu"><p id="ad73" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.models导入序列</p><p id="605c" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.layers导入密集，激活</p><p id="56a7" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型=顺序([</p><p id="c27f" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(16，input_shape=(1，5)，activation='relu ')，</p><p id="57b9" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(32，activation='relu '，kernel _ initializer = ' glorot _ uniform ')，# glorot _ uniform是xavier初始化</p><p id="50ae" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(2，activation='softmax ')</p><p id="18e3" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">])</p></blockquote><h1 id="36e4" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">偏见</h1><ul class=""><li id="6421" class="le lf hh ig b ih kz il la ip lz it ma ix mb jb lj lk ll lm bi translated">首先，当我们谈论偏见时，我们是在每个神经元的基础上谈论的。我们可以认为每个神经元都有自己的偏置项，因此整个网络将由多个偏置项组成。</li><li id="65b3" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">现在，分配给这些偏差的值是可以学习的，就像权重一样。</li><li id="1ae4" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">增加模型的灵活性。</li><li id="3eb9" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">g(w1x1 + w2x2 + ……+ wnxn + B)</li></ul><p id="a469" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">b是偏差项</p><h1 id="360b" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">人工神经网络中的可学习参数</h1><p id="2876" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">由网络在训练期间学习的参数</p><blockquote class="ls lt lu"><p id="270c" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated"><strong class="ig hi">可学习参数</strong> =输入+输出+偏置</p></blockquote><h1 id="48fb" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">CNN中的可学习参数</h1><blockquote class="ls lt lu"><p id="6f25" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">输入*输出+偏差</p></blockquote><p id="59b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在哪里，</p><p id="36d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">输入=(如果最后一项密集，则输入是节点的编号)或(如果最后一层是conv，则输入是过滤器的编号)</p><blockquote class="ls lt lu"><p id="bfb7" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">输出=(过滤器数量)+(过滤器尺寸)</p><p id="c5d5" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">偏差=过滤器数量</p></blockquote><h1 id="0f32" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">正规化</h1><ul class=""><li id="40b7" class="le lf hh ig b ih kz il la ip lz it ma ix mb jb lj lk ll lm bi translated">正则化是一种通过惩罚复杂性来帮助减少过度拟合或减少我们网络中的变化的技术。</li><li id="dbee" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">减轻大重量。</li><li id="6896" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">损失+ x</li><li id="78e3" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">其中x对于大wt是不利的</li></ul><p id="aebb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> L2正规化</strong></p><p id="780c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最常见的正则化技术称为L2正则化。我们知道，正则化基本上包括在损失函数中增加一项，对大权重进行惩罚。</p><p id="00d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">损失+ x</p><p id="706c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里x = summation(1，n){||w[f]|| * lamda/2m}</p><ul class=""><li id="a4ab" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">n =层数</li><li id="4179" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">w[f] =第j层的wt矩阵</li><li id="3cdf" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">m =输入数量</li></ul><h1 id="2e54" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">密码</h1><blockquote class="ls lt lu"><p id="370c" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.models导入序列</p><p id="dc07" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.layers导入密集，激活</p><p id="4e32" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型=顺序([</p><p id="992c" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(16，input_shape=(1，5)，activation='relu ')，</p><p id="a46e" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(32，activation='relu '，kernel _ regulator = regular iser . L2(0.01))，# l2 regulariser</p><p id="edc4" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">Dense(2，activation='softmax ')</p><p id="a3e8" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">])</p></blockquote><h1 id="1271" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">神经网络中的批量大小</h1><p id="3719" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">批量大小是一次传递到网络的样本数量</p><blockquote class="ls lt lu"><p id="5716" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">巴克。=纪元</p><p id="7b7e" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">历元中的批次=训练集大小/ batch_size</p></blockquote><ul class=""><li id="a49d" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">批量越大=训练越快，但质量下降。</li></ul><blockquote class="ls lt lu"><p id="8d01" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型. fit(</p><p id="fa1a" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">x =缩放的训练样本，</p><p id="e3d3" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">y =火车标签，</p><p id="e86f" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">验证数据=有效设置，</p><p id="7548" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">batch_size=10，#批量大小</p><p id="accf" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">纪元=20，</p><p id="5a62" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">洗牌=真，</p><p id="5dcb" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">详细=2</p><p id="c1fe" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">)</p></blockquote><h1 id="cf7e" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">微调神经网络</h1><p id="3ac3" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">微调与术语迁移学习密切相关</p><p id="ccf1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们使用从解决一个问题中获得的知识，并将其应用于一个新的但相关的问题时，迁移学习就发生了。</p><p id="bb9a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为什么要使用微调？假设原始任务与新任务相似，使用已经设计和训练好的人工神经网络可以让我们利用模型已经学习到的东西，而不必从头开始开发。</p><p id="cc9c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，我们必须选择使用多少层，使用什么类型的层，以什么顺序放置层，每层包含多少节点，决定使用多少正则化，将我们的学习速率设置为什么，等等。</p><ul class=""><li id="2d1b" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">层数</li><li id="9815" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">层的类型</li><li id="1b4a" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">层的顺序</li><li id="77c6" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">每层中的节点数</li><li id="1662" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">使用多少正则化</li><li id="304f" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">学习率</li></ul><h1 id="7a31" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">批量标准化(“批量标准”)</h1><p id="b3fe" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated"><strong class="ig hi">标准化技术</strong></p><p id="a046" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">z= (x-m)/s</p><p id="18c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">为什么？</strong></p><p id="0d45" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些非标准化数据集中较大的数据点会导致神经网络不稳定，因为相对较大的输入会通过网络中的层向下级联，这可能导致不平衡的梯度，从而可能导致著名的<strong class="ig hi">爆炸梯度问题</strong>。</p><p id="0e4b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将批次定额应用于图层</p><ul class=""><li id="df46" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated">批处理规范化应用于图层。</li><li id="4372" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">标准化激活函数的输出x</li><li id="ea69" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">z=(x-m)/s</li><li id="8803" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">将归一化输出z乘以任意参数g。</li><li id="8c8e" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">z*g</li><li id="d893" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">将任意参数b添加到结果乘积(z*g)中。</li><li id="7b74" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated">(z*g)+b</li></ul><p id="4b2d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">代码</strong></p><blockquote class="ls lt lu"><p id="64f2" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.models导入序列</p><p id="efc3" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">从keras.layers导入密集、激活、批量标准化#导入批量标准化</p><p id="5506" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">模型=顺序([</p><p id="878f" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">密集(units=16，input_shape=(1，5)，activation='relu ')，</p><p id="07bd" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">密集(单位=32，激活='relu ')，</p><p id="3143" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">批次正常化(轴=1)，#批次正常化</p><p id="d395" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi translated">密集(单位=2，激活='softmax ')</p><p id="345b" class="ie if lv ig b ih ii ij ik il im in io lw iq ir is lx iu iv iw ly iy iz ja jb ha bi">])</p></blockquote></div></div>    
</body>
</html>