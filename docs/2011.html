<html>
<head>
<title>Looking to start a ML model building project? This one stop guide is all you will need! (Includes code)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">想开始一个ML模型构建项目吗？这个一站式指南是你所需要的！(包括代码)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/looking-to-start-a-ml-model-building-project-23b08444d8b3?source=collection_archive---------13-----------------------#2021-03-31">https://medium.com/analytics-vidhya/looking-to-start-a-ml-model-building-project-23b08444d8b3?source=collection_archive---------13-----------------------#2021-03-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="2081" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">为数据科学领域的任何人理解重要的ML算法以及模型构建和实际使用案例的足智多谋和简洁的指南。</h2></div></div><div class="ab cl iw ix go iy" role="separator"><span class="iz bw bk ja jb jc"/><span class="iz bw bk ja jb jc"/><span class="iz bw bk ja jb"/></div><div class="ha hb hc hd he"><blockquote class="jd je jf"><p id="7a6b" class="jg jh ji jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">本文将重点理解四种广泛使用的ML算法，以及它们的性能指标和超参数调整，这是模型构建的一个组成部分。这个想法是用简化的例子和简洁的描述使这个学习过程变得清晰。但是，假设你对机器学习中使用的术语有基本的了解。</p><p id="412c" class="jg jh ji jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">这篇文章可能看起来比平时长了一点，但是请耐心听我说，我已经尝试广泛地涵盖每个主题，所以它绝对值得一读。欢迎随时加入，关于我如何改进我的内容的任何建议和评论。</p></blockquote><h1 id="67c5" class="kd ke hh bd kf kg kh ki kj kk kl km kn in ko io kp iq kq ir kr it ks iu kt ku bi translated"><strong class="ak">机器学习算法:</strong></h1><h2 id="00de" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">1.随机森林和决策树</h2><p id="f8e2" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">我们通常会听到很多关于决策树(DT)和随机森林(RF)的说法。简单来说，决策树是随机森林的子集。您可以将它们称为“<strong class="jj hi"><em class="ji">”</em></strong>，这意味着DT从以行和列的形式提供给它的数据子集中学习，并转换成RF。</p><p id="11c3" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">RF通常由一系列<strong class="jj hi"> <em class="ji">行采样(行)+特征采样(列)</em> </strong>组成，这些采样被提供给每个DT用于训练目的。</p><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://www.analyticsvidhya.com/blog/2020/05/decision-tree-vs-random-forest-algorithm/"><div class="er es lr"><img src="../Images/737b642815e6b314e038b0f59df29cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*kzFq1P5gTnT2nLujo8Dnbg.png"/></div></a><figcaption class="lz ma et er es mb mc bd b be z dx translated">用于数据分类的3个DT的多数票</figcaption></figure><p id="6dea" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">现在，当测试数据被提供给它进行测试时，<strong class="jj hi"> <em class="ji">各种DT的多数投票被检查并被分类</em> </strong>为输出属于哪一类。</p><p id="ad8a" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">关于DT的一个重要事实是，它们中的每一个都具有<strong class="jj hi"> <em class="ji">低偏差和</em> </strong>高方差。这意味着，当我们创建DT到其完整深度时，由于使用的树的数量，它给我们一个最小的训练误差(即低偏差)。在高方差的情况下，当引入一个测试数据时，容易给出更多的误差。</p><p id="720b" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><em class="ji">但是现在当你把这些DT组合在一起的时候</em> <strong class="jj hi"> <em class="ji">高方差&gt;低方差</em> </strong> <em class="ji">由于多数票是从多个DT中考虑的。</em></p><blockquote class="md"><p id="17ba" class="me mf hh bd mg mh mi mj mk ml mm kc dx translated">综上所述，您认为数据的任何变化/减少都会影响RF模型的准确性吗？</p></blockquote><p id="f2c0" class="pw-post-body-paragraph jg jh hh jj b jk mn ii jm jn mo il jp lc mp js jt lf mq jw jx li mr ka kb kc ha bi translated">肯定不会，因为由于训练发生在如此多的DT上，RF模型变成了理解记录细微差别的专家，因此给出了非常低的准确度方差误差wrt。</p><h2 id="0827" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">2.神经网络</h2><p id="0e34" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">如果你完全是ML的新手，这个主题是巨大的，有点复杂，但是当你深入研究它的时候会非常有趣。很可能，我和大多数数据科学家最喜欢的建模算法仅仅是因为它的多样性。我将在这里链接一篇文章<a class="ae ms" href="https://towardsdatascience.com/neural-networks-for-beginners-by-beginners-6bfc002e13a2" rel="noopener" target="_blank">，这篇文章极大地帮助了我深入理解这个话题。</a></p><p id="4a26" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">在这里，我的目标是为您简化这个过程，以便明确关键概念。</p><p id="999e" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">我们经常听说<strong class="jj hi">人工神经网络(安的)</strong>是人脑的复制品。就像人脑有神经元一样，人工神经网络也由神经元组成，这显然是人工的。它包含三个明显的特征:</p><p id="b04f" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">1.输入(例如:图像、文本、视频、音频)</p><p id="5b4e" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">2.隐藏层(用于映射目的)</p><p id="4a69" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">3.输出(对数据进行分类)</p><p id="d25e" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">当一个输入被提供给网络的节点时，它们被乘以一个权重函数'<em class="ji"> w' </em>，该权重函数用于训练输入并从提供给它的数据中学习。</p><p id="1577" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">神经网络中的<strong class="jj hi"> <em class="ji">激活函数(AF) </em> </strong>取输入<em class="ji">‘x’</em>乘以权重<em class="ji">‘w’</em>。偏置允许您通过在输入中添加一个常数(即给定的偏置)来改变激活函数。神经网络中的偏差与线性函数中常数的作用是同义的。</p><p id="b906" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">现在激活函数基本上就是在<strong class="jj hi"> 0和1 <em class="ji">之间转换O/P数据的函数。</em> </strong></p><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://deepai.org/machine-learning-glossary-and-terms/weight-artificial-neural-network"><div class="er es mt"><img src="../Images/9c65c54f49d7aa476e8b497627b1a3a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rkbWtSkaOcaZaj8P.png"/></div></a><figcaption class="lz ma et er es mb mc bd b be z dx translated">所有神经网络的基本方程</figcaption></figure><p id="7e69" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><em class="ji">例如:当你把一个热的物体放在你的手掌上时，大脑中的神经元被激活，并向你的大脑发送信号，之后你对刺激做出反应。</em></p><p id="a2ca" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">激活函数通过计算加权和并进一步加上偏差来决定是否激活一个神经元。激活函数的目的是将非线性(学习和执行复杂任务的能力)引入神经元的输出。</p><p id="d146" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">现在，有三个主要的房颤应该被认识到。</p><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec"><div class="er es mu"><img src="../Images/c9dead028c99173d4c2b6db9c7e97915.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*wHDW_sI09v81CgZ-1jGzNQ.png"/></div></a></figure><p id="fdfb" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi">我<em class="ji">。整流线性单元</em> </strong>:为输入(+ve)提供正值，否则为0。基本上，它消除了所有的负面影响，也解决了消失梯度(VG)的问题。</p><p id="3148" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><em class="ji">如果你对VG不熟悉，那么我在这里</em>  <em class="ji">附上一个链接</em> <a class="ae ms" href="https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484" rel="noopener" target="_blank"> <em class="ji">。</em></a></p><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://medium.com/@gabriel.mayers/sigmoid-function-explained-in-less-than-5-minutes-ca156eb3049a"><div class="er es mv"><img src="../Images/665f3684e484645b67af0b2ad61efd6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*iA0hszPEsYPJ5YkF8M4xbA.png"/></div></a><figcaption class="lz ma et er es mb mc bd b be z dx translated">Sigmoid函数和公式</figcaption></figure><p id="7a47" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi">二</strong>。<strong class="jj hi"> <em class="ji"> Sigmoid </em> </strong>:它是深度学习中最常用的AF之一，通常用于二进制分类，因为输出值绑定在0-1之间，这使每个神经元的输出标准化。</p><p id="6236" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">Sigmoid的一些缺点包括，在梯度下降的情况下，它们容易出现VG问题&amp;函数输出不是以零为中心的(即，更多的计算时间，这意味着更多的时间达到全局最小值)。<strong class="jj hi"> <em class="ji">其中x: wx+b(输入)</em> </strong></p><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://developer.apple.com/documentation/accelerate/bnns/activationfunction/softmax"><div class="er es mw"><img src="../Images/f9a11bccfd62deddda7e9b3f75bb5cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*QTCja199-iytylNipMqWyg.png"/></div></a><figcaption class="lz ma et er es mb mc bd b be z dx translated">SoftMax函数</figcaption></figure><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://deepai.org/machine-learning-glossary-and-terms/softmax-layer"><div class="er es mx"><img src="../Images/34d0d57dfd046118121ae31facb4f9bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*-zaeGCr40BrvfwVcndXY0w.png"/></div></a><figcaption class="lz ma et er es mb mc bd b be z dx translated">分类示例</figcaption></figure><p id="58eb" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi">三世<em class="ji">。SoftMax </em> </strong>:当你的输出层有两个以上的类别(多类别分类)时，使用SoftMax功能。它基本上给出了每个类的概率列表，这样，无论哪个概率最高，都将成为o/p类。</p><p id="7db8" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">SoftMax和Sigmoid AF都用于神经网络的最后一层。</p><p id="9f23" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi"> <em class="ji">在这种情况下，输出属于猫类，因为概率为0.71。</em>T11】</strong></p><h2 id="f394" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated"><strong class="ak">权重初始化技术</strong></h2><p id="cb8e" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">权重只负责学习和归纳与给予神经元的各种输入特征不同的东西，以进一步执行复杂的任务。因此，在为神经网络选择权重时，有各种各样的特征。</p><p id="ecb1" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><em class="ji">砝码尺寸应较小。(如果太小，可能会出现VG问题。)</em></p><p id="32fa" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><em class="ji">权重不应相同</em></p><p id="f0c6" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><em class="ji">他们应该有一个好的方差</em>。</p><p id="2292" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi"><em class="ji">~ Xavier/Glorot:Xavier-Normal或Xavier-Uniform，</em> </strong>配合Sigmoid或SoftMax AF效果最佳。</p><p id="3eeb" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi"> <em class="ji"> ~均匀分布:</em> </strong>与乙状结肠AF配合良好。</p><p id="c735" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi"> <em class="ji"> ~ He初始化:He-Uniform或He-Normal </em> </strong>通常与Relu AF一起使用，以获得所需的结果。</p><h2 id="727d" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">优化者</h2><p id="4c8c" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">这个主题本身就是一篇文章，如果我说我会涵盖你需要知道的关于优化器的一切，这是不公平的，但显然不是。</p><p id="44c4" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">首先，需要优化器来减少在输出端获得的损失函数。当使用反向传播更新权重时，会计算新的权重，因此在特定时期后损失会减少。</p><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://towardsai.net/p/machine-learning/improving-artificial-neural-network-with-regularization-and-optimization"><div class="er es my"><img src="../Images/36b4ddc583d3f323ff9f2281eaec4956.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*g8ryfCC5z2dXMVJnC3Ir5Q.png"/></div></a><figcaption class="lz ma et er es mb mc bd b be z dx translated">权重更新公式</figcaption></figure><p id="65f5" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><em class="ji">其中n =学习率，E=均方误差。</em></p><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9"><div class="er es mz"><img src="../Images/3adc80288545be920139cb5932bd349e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ApDAKJOj9n9B4cPnhJGwdg.png"/></div></a></figure><p id="bba6" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">如果你想知道这些术语的实际含义，我会在这里链接一篇解释这些术语的文章<a class="ae ms" href="https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c" rel="noopener" target="_blank"><em class="ji"/></a><em class="ji">，所以不用担心。</em></p><p id="3713" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">我们将讨论每个ML工程师应该辨别的3个最重要的优化器，即—</p><h2 id="bbd7" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">a.梯度下降算法:</h2><p id="2562" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">正如我前面提到的，这个过程从正向传播开始，包括使用激活函数计算预测值，从而计算误差。反向传播是一种通过使用上述公式更新权重来减少损失/误差的算法。</p><p id="e885" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><em class="ji">是计算导数的过程，梯度下降是通过梯度下降的过程，即通过损失函数调整模型的参数向下。</em></p><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://wiki.tum.de/display/lfdv/Adaptive+Learning+Rate+Method"><div class="er es na"><img src="../Images/0f8d8c2e8bf74f60f2b386fe3cc68748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*cPKwXgyleIOP6JYOrtum2A.png"/></div></a><figcaption class="lz ma et er es mb mc bd b be z dx translated">权重试图达到GD中的局部最小值</figcaption></figure><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://datascience.stackexchange.com/questions/44703/how-does-gradient-descent-and-backpropagation-work-together"><div class="er es lr"><img src="../Images/f3a40229a9c739579271269b83752274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*20eRXI9j-ZBANt7JI0lw4Q.png"/></div></a><figcaption class="lz ma et er es mb mc bd b be z dx translated">正向和反向传播传播</figcaption></figure><p id="2cf1" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><em class="ji">在GD中需要记住的一个事实是，整个数据集都经过了前向和后向传播处理(即经过了“x”个历元)</em>。因此，在经过一定数量的时期后，损失函数减小。</p><h2 id="d09a" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">b.随机梯度下降；</h2><p id="4695" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">这个优化器的工作方式与GD完全相同，只是有一点小小的不同，那就是传递的记录数量。这里，前后传播<strong class="jj hi"> </strong>一次只传递一条记录来更新权重参数<strong class="jj hi">。</strong></p><p id="1365" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">例如:如果你有10，000条记录，时段1将有10，000次前后传播迭代。</p><p id="6c70" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">这意味着需要更多的计算时间来达到全局最小值(收敛点)。</p><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://xzz201920.medium.com/gradient-descent-stochastic-vs-mini-batch-vs-batch-vs-adagrad-vs-rmsprop-vs-adam-3aa652318b0d"><div class="er es nb"><img src="../Images/f15de5e2b8ff2fd3e57f1b90ab26a8e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RpygpRU673wWDWq6.png"/></div></a><figcaption class="lz ma et er es mb mc bd b be z dx translated">噪声存在时到达收敛点的路径。</figcaption></figure><p id="111f" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">为了解决这个问题，推出了小批量SGD 。</p><p id="12e1" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">这仅意味着数据以<strong class="jj hi">小批量</strong>传递，以便更快地到达汇聚点，但由于存在噪音，到达汇聚点的路线可能不太顺畅。(因为取批量)。尽管Mini SGD已经被证明使用较少的资源并且在计算上是最便宜的。</p><h2 id="3968" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">c.Adam优化器:</h2><p id="33c8" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">迄今为止最有效的发明之一，ADAM是自适应矩估计的首字母缩写。结合使用两个优化器，即动量和均方根道具。</p><p id="1c34" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi"> <em class="ji">动量&gt;用于平滑到汇聚点的路线。</em> </strong></p><p id="ff17" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi"> <em class="ji"> RMS Prop &gt;平滑高效地改变学习率。</em> </strong></p><p id="cd0b" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">结合RMS Prop和动量平滑的优点，形成了Adam优化器。</p><p id="6fb7" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">还有一些其他的优化器，比如SGD with Momentum，Adagrad，Adadelta和RMS Prop，我在这里没有介绍，但是如果你想深入阅读，我会放一个链接。</p><div class="nc nd ez fb ne nf"><a href="https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6" rel="noopener follow" target="_blank"><div class="ng ab dw"><div class="nh ab ni cl cj nj"><h2 class="bd hi fi z dy nk ea eb nl ed ef hg bi translated">训练神经网络的各种优化算法</h2><div class="nm l"><h3 class="bd b fi z dy nk ea eb nl ed ef dx translated">正确的优化算法可以成倍地减少训练时间。</h3></div><div class="nn l"><p class="bd b fp z dy nk ea eb nl ed ef dx translated">towardsdatascience.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt lx nf"/></div></div></a></div><h2 id="3d6f" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">3.k-最近邻(KNN分类)</h2><p id="d4f6" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">这是一种监督学习算法，主要基于K值。<em class="ji">(注意:K值为奇数时效果最佳，因为如果取偶数，则两边的类别数量相等)。</em></p><figure class="ls lt lu lv fd lw er es paragraph-image"><div class="er es nu"><img src="../Images/3daabdf2c54b578fcc8acb9122e7a05d.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*_kdRO7jSzzP0zGt9MkWW1g.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">作者图片:KNN K = 3分类</figcaption></figure><p id="5781" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">无论哪一个类别在该附近具有最大数量的邻居，数据点都属于通过称为<a class="ae ms" rel="noopener" href="/@kunal_gohrani/different-types-of-distance-metrics-used-in-machine-learning-e9928c5e26c7"> <strong class="jj hi"> <em class="ji">欧几里得</em> </strong> </a> <strong class="jj hi"> <em class="ji">或曼哈顿</em> </strong>距离的度量来计算的那个类别。</p><p id="410e" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">在回归用例中，取所有最近邻的平均值来给数据点赋值。(<em class="ji">适用于大多数算法</em>)</p><p id="127f" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">KNN的主要缺点包括离群值的存在，这可能会极大地影响分类用例。不平衡的数据集也可能对您的数据集构成威胁，但可以使用采样技术来应对。</p><h2 id="c570" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated"><em class="nv"> 4。K-均值聚类</em></h2><p id="ef02" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">该算法用于无监督的机器学习问题，以利用数据点之间的相似性形成聚类。“K”值或质心取决于所形成的簇。(如果k=2，即2个质心/簇)</p><ul class=""><li id="7c94" class="nw nx hh jj b jk jl jn jo lc ny lf nz li oa kc ob oc od oe bi translated"><em class="ji">随机初始化质心，并使用欧几里德/曼哈顿距离计算数据点之间的距离。</em></li><li id="51ed" class="nw nx hh jj b jk of jn og lc oh lf oi li oj kc ob oc od oe bi translated"><em class="ji">形成一组数据点，并计算各组的平均值。</em></li><li id="d100" class="nw nx hh jj b jk of jn og lc oh lf oi li oj kc ob oc od oe bi translated"><em class="ji">基于平均值，数据点被移动到质心。</em></li><li id="c4e9" class="nw nx hh jj b jk of jn og lc oh lf oi li oj kc ob oc od oe bi translated"><em class="ji">同样，该过程继续进行，直到数据点中没有移动，并且形成固定的组/簇。这意味着您有两个不同的集群/组。</em></li></ul><figure class="ls lt lu lv fd lw er es paragraph-image"><div class="er es ok"><img src="../Images/2275d438283eb1966412657c05078e55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*rVtUuEInlK1EE9bPpf9O4w.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">作者图片</figcaption></figure><blockquote class="md"><p id="a0b4" class="me mf hh bd mg mh ol om on oo op kc dx translated">如果你想知道K=2的值是如何出现在图片中的，这里是它是如何出现的..</p></blockquote><blockquote class="jd je jf"><p id="10aa" class="jg jh ji jj b jk mn ii jm jn mo il jp jq mp js jt ju mq jw jx jy mr ka kb kc ha bi translated">K值是用这种叫做<strong class="jj hi">‘肘法’</strong>(字面意思)的方法测量的</p></blockquote><figure class="ls lt lu lv fd lw er es paragraph-image"><div class="er es oq"><img src="../Images/71b56671439c785a134cae6c9525885d.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*Y478KRLY5eqReSwaVhU46A.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">作者提供的图片:WCSS计算方法和肘形曲线的图示</figcaption></figure><ul class=""><li id="ebd8" class="nw nx hh jj b jk jl jn jo lc ny lf nz li oa kc ob oc od oe bi translated"><em class="ji">假设从K=1到20运行一个循环，同时执行K均值聚类过程。</em></li><li id="0acd" class="nw nx hh jj b jk of jn og lc oh lf oi li oj kc ob oc od oe bi translated">随机初始化一个质心。</li><li id="ba61" class="nw nx hh jj b jk of jn og lc oh lf oi li oj kc ob oc od oe bi translated"><em class="ji">现在，质心和所有数据点之间的距离是使用一个称为</em><strong class="jj hi"><em class="ji">‘聚类平方和内’或WCSS </em> </strong> <em class="ji">的度量来计算的。</em></li><li id="d4b2" class="nw nx hh jj b jk of jn og lc oh lf oi li oj kc ob oc od oe bi translated">现在要知道的一个主要事实是，对于K=1，WCSS的值会非常非常高，因为只有一个质心。</li><li id="52b7" class="nw nx hh jj b jk of jn og lc oh lf oi li oj kc ob oc od oe bi translated">随着K值的增加，WCSS会减小。这会给你肘部曲线。</li></ul><p id="b8cf" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi">从这里你可以推断出，K的值是图表经历突然/突然减少的值。</strong></p><h1 id="fd0b" class="kd ke hh bd kf kg kh ki kj kk kl km kn in ko io kp iq kq ir kr it ks iu kt ku bi translated">分类问题中的性能度量</h1><blockquote class="jd je jf"><p id="c508" class="jg jh ji jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">在从事任何ML项目时，我们经常不断地听到这些度量术语。所以我宁愿向您展示实现代码，而不是解释每一个术语。然而，我在这里链接了一篇文章<a class="ae ms" rel="noopener" href="/@MohammedS/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b">，这篇文章会让你对这些术语有一个全面的了解。</a></p></blockquote><h2 id="a667" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">混淆矩阵:</h2><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62"><div class="er es or"><img src="../Images/b84e601add415fa30eb5e9dd8d1247b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/0*kutbi3X1xUhsg9pg.png"/></div></a><figcaption class="lz ma et er es mb mc bd b be z dx translated">混乱</figcaption></figure><p id="3e9d" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">它是一个2 x 2矩阵，由顶部的实际值和左侧的预测值组成。</p><p id="f47f" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">它有4个符号或字段:</p><p id="09d3" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi"> <em class="ji">真阳性、假阳性、假阴性、真阴性。</em> </strong></p><p id="d0ab" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">在任何分类用例中应该减少的两类错误:</p><ul class=""><li id="04f2" class="nw nx hh jj b jk jl jn jo lc ny lf nz li oa kc ob oc od oe bi translated">类型1错误:假阳性率(FPR) <strong class="jj hi"> <em class="ji">其中FPR: FP/(FP+TN) </em> </strong></li><li id="d74a" class="nw nx hh jj b jk of jn og lc oh lf oi li oj kc ob oc od oe bi translated">第二类错误:假阴性率(FNR) <strong class="jj hi"> <em class="ji">其中FNR: FN/(FN+TP) </em> </strong></li></ul><h2 id="0662" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">精确度:</h2><p id="d73e" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">平衡数据集的理想选择。原因如下..</p><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://medium.com/@erika.dauria/accuracy-recall-precision-80a5b6cbd28d"><div class="er es os"><img src="../Images/2d5c8cf1bf0c11e77ae31bd048a4f2e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ND4tm4TfJmlnM8Q1.png"/></div></a></figure><p id="3904" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">在不平衡数据集(80-20比率)的情况下，仅使用精度来验证模型会使花费在模型构建上的所有努力白费，因为它会误导模型性能，因此精度和召回是必需的。</p><h2 id="28ca" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">精确度和召回率:</h2><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9"><div class="er es mv"><img src="../Images/6677c4948734b7cddf19db35b20e1e80.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/0*uOHw3MjSRRqrCnNm.png"/></div></a></figure><p id="c425" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi"> Precision </strong>表示，在做出的所有正面预测中，有多少比例被您的分类器正确预测。又称为<strong class="jj hi"> <em class="ji">正预测值。</em>T19】</strong></p><p id="28dc" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">著名的垃圾邮件检测用例是理解这一措施的最佳方式。</p><p id="0261" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi">回想一下</strong>简单来说就是，实际正值中有多少百分比被您的分类器正确分类为正值。回忆也称为<strong class="jj hi"> <em class="ji">真阳性率/灵敏度。</em> </strong></p><p id="0b03" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">以今天的情况为例，如果一名患者的Covid测试结果为阴性，但实际上他/她是Covid阳性。这是一个灾难性的想法，但却是理解回忆的一个很好的例子。</p><h2 id="6579" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">F1分数:</h2><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826"><div class="er es ot"><img src="../Images/d1a7b871679b79cfe9f3bd23a41efbaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kO8eh-ki_LW-tZzY.png"/></div></a></figure><p id="2bb3" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">在问题陈述中，需要精确和回忆的功能，使用F1分数。这就是精度和召回的'<em class="ji">调和平均值</em>。这种方法适用于不平衡的数据集。</p><p id="66f9" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">F1分数中的“1”来自于:</p><figure class="ls lt lu lv fd lw er es paragraph-image"><div class="er es ou"><img src="../Images/b23b284de9cd83749b690d5ee0c227f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/0*g2wkliTA1G4vCvn9"/></div></figure><p id="5bbe" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">因为FP和FN在你的用例中都扮演着重要的角色。</p><p id="a09c" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">在您的假阳性(第一类错误)比假阴性(第二类错误)影响更大的情况下，您的beta值在0-1之间减少(通常为0.5)。</p><p id="a5f6" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">同样，如果假阴性起了更大的作用，β值会增加(在2-10之间)。</p><h2 id="d502" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">ROC曲线和AUC曲线:</h2><p id="8b6b" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">这些曲线是模型的证明，反映了模型对提供给它的数据的表现。<strong class="jj hi"> ROC曲线</strong> ( <strong class="jj hi">接收器操作特性曲线</strong>)是显示分类模型在所有分类阈值的性能的图表。该曲线绘制了两个参数:</p><ul class=""><li id="9958" class="nw nx hh jj b jk jl jn jo lc ny lf nz li oa kc ob oc od oe bi translated">真实阳性率</li><li id="630a" class="nw nx hh jj b jk of jn og lc oh lf oi li oj kc ob oc od oe bi translated">假阳性率</li></ul><p id="a995" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi"> AUC </strong>代表“<strong class="jj hi">ROC曲线下面积</strong>”也就是说，AUC测量从(0，0)到(1，1)的整个ROC曲线下的整个二维面积。</p><figure class="ls lt lu lv fd lw er es paragraph-image"><a href="https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c"><div class="er es ov"><img src="../Images/6e349ba13f10e335455d3817a9408746.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/0*v1567DB-kmufGnm2.png"/></div></a><figcaption class="lz ma et er es mb mc bd b be z dx translated">ROC和AUC曲线</figcaption></figure><p id="1eef" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">AUC应该至少直到中间的虚线，否则模型性能被认为是非常低的。</p><p id="26a8" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><strong class="jj hi">AUC越大，模型性能越好！！</strong></p><p id="9ced" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">当访问ROC曲线时，通常需要更高的TPR。因此，这取决于你选择什么样的阈值，以及模型实际需要什么样的AUC。</p><h1 id="1e3a" class="kd ke hh bd kf kg kh ki kj kk kl km kn in ko io kp iq kq ir kr it ks iu kt ku bi translated">超参数调谐</h1><p id="8689" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">从我们对超参数的了解来看，它们是最佳参数数量的<em class="ji">字典</em>，往往最适合模型，目的是实现高模型精度和性能。</p><p id="3152" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">优化超级参数可以通过多种方式完成，但通常对我有用的是基于这两个从<code class="du ow ox oy oz b">sklearn.model_selection</code>导入的库，称为<a class="ae ms" rel="noopener" href="/@senapati.dipak97/grid-search-vs-random-search-d34c92946318"> <strong class="jj hi">网格搜索CV </strong>或<strong class="jj hi">随机搜索CV </strong> </a>。虽然随机CV被证明是两者中更快的技术。</p><h2 id="68a3" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">I)随机森林:</h2><p id="9a43" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">该模型使用随机森林回归器，基于某些独立特征从<a class="ae ms" href="https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho" rel="noopener ugc nofollow" target="_blank"> <em class="ji">车辆数据集</em> </a>中预测汽车销售价格，使用随机搜索CV选择超参数。</p><pre class="ls lt lu lv fd pa oz pb pc aw pd bi"><span id="2c1b" class="kv ke hh oz b fi pe pf l pg ph">from sklearn.ensemble import <strong class="oz hi">RandomForestRegressor</strong><br/>regressor=RandomForestRegressor()</span><span id="8b6f" class="kv ke hh oz b fi pi pf l pg ph">n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]<br/>print(n_estimators)</span><span id="1481" class="kv ke hh oz b fi pi pf l pg ph">Output<strong class="oz hi">:[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200]</strong></span><span id="67c9" class="kv ke hh oz b fi pi pf l pg ph">from sklearn.model_selection import <strong class="oz hi">RandomizedSearchCV</strong></span><span id="ef90" class="kv ke hh oz b fi pi pf l pg ph"><em class="ji"># Number of trees in random forest</em><br/>n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]<br/><em class="ji"># Number of features to consider at every split</em><br/>max_features = [‘auto’, ‘sqrt’]<br/><em class="ji"># Maximum number of levels in tree</em><br/>max_depth = [int(x) <strong class="oz hi">for</strong> x <strong class="oz hi">in</strong> np.linspace(5, 30, num = 6)]<br/><em class="ji"># max_depth.append(None)</em><br/><em class="ji"># Minimum number of samples required to split a node</em><br/>min_samples_split = [2, 5, 10, 15, 100]<br/><em class="ji"># Minimum number of samples required at each leaf node</em><br/>min_samples_leaf = [1, 2, 5, 10]</span></pre><p id="7147" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">在下面的代码中，<code class="du ow ox oy oz b">random_grid</code>被用作所有超参数组合的框架，之后，通过运行<code class="du ow ox oy oz b">best_params_.</code>来决定最终的参数列表</p><pre class="ls lt lu lv fd pa oz pb pc aw pd bi"><span id="3cab" class="kv ke hh oz b fi pe pf l pg ph"><em class="ji"># Create the random grid</em><br/>random_grid = {'n_estimators': n_estimators,<br/>               'max_features': max_features,<br/>               'max_depth': max_depth,<br/>               'min_samples_split': min_samples_split,<br/>               'min_samples_leaf': min_samples_leaf}<br/>print(random_grid)</span><span id="a2fd" class="kv ke hh oz b fi pi pf l pg ph">Output: <strong class="oz hi">{'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200], 'max_features': ['auto', 'sqrt'], 'max_depth': [5, 10, 15, 20, 25, 30], 'min_samples_split': [2, 5, 10, 15, 100], 'min_samples_leaf': [1, 2, 5, 10]}</strong></span></pre><p id="24bf" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><code class="du ow ox oy oz b">scoring</code> : ' <em class="ji">负均方误差</em>'和'<em class="ji">平均绝对误差</em>'在回归用例验证模型的情况下用作性能度量。</p><p id="17bd" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><code class="du ow ox oy oz b">cv=5</code>:交叉验证分割比率</p><p id="998d" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><code class="du ow ox oy oz b">verbose=2</code>:提供运行时间和代码运行时间的描述，同时生成程序代码的所有细节。</p><p id="c105" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><code class="du ow ox oy oz b">random_state=42</code>:用于多次运行一个代码时检查和验证数据。将<code class="du ow ox oy oz b">random_state</code>设置为固定值将保证每次运行代码时生成相同的随机数序列。除非在这个过程中存在一些其他的随机性，否则产生的结果总是一样的。这有助于验证输出。</p><p id="e2d1" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated"><code class="du ow ox oy oz b">n_jobs</code>:这是你的机器中为了实现你的代码而必须运行的所有CPU核心的指示。</p><p id="d77c" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">此外，<code class="du ow ox oy oz b">rf_random</code>网格适合于训练数据(<em class="ji"> X_train </em>，<em class="ji"> y_train </em>)，以便他们从中学习和概括，然后<code class="du ow ox oy oz b">predictions </code> ( <em class="ji"> y_pred </em>)发生在<em class="ji"> X_test </em>数据上，然后这些数据对照诸如准确性、以(<em class="ji"> y_pred，y_test </em>形式的分类报告等性能度量进行测试。</p><pre class="ls lt lu lv fd pa oz pb pc aw pd bi"><span id="6247" class="kv ke hh oz b fi pe pf l pg ph"><em class="ji"># Use the random grid to search for best hyperparameters</em><br/><em class="ji"># First create the base model to tune</em><br/>rf = RandomForestRegressor()</span><span id="fcd6" class="kv ke hh oz b fi pi pf l pg ph"><em class="ji"># Random search of parameters, using 3 fold cross validation, </em><br/><em class="ji"># search across 100 different combinations</em><br/>rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)</span><span id="a975" class="kv ke hh oz b fi pi pf l pg ph">rf_random.fit(X_train,y_train)</span><span id="e691" class="kv ke hh oz b fi pi pf l pg ph">Output<strong class="oz hi">: RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=1 param_distributions={'max_depth': [5, 10, 15, 20, 25, 30], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 5, 10],<br/>'min_samples_split': [2, 5, 10, 15,100],'n_estimators': [100, 200, 300, 400,500, 600, 700, 800,900, 1000, 1100,1200]}, random_state=42, scoring='neg_mean_squared_error',verbose=2)</strong></span><span id="4615" class="kv ke hh oz b fi pi pf l pg ph">rf_random.best_params_</span><span id="e182" class="kv ke hh oz b fi pi pf l pg ph"><strong class="oz hi">{'n_estimators': 1000,<br/> 'min_samples_split': 2,<br/> 'min_samples_leaf': 1,<br/> 'max_features': 'sqrt',<br/> 'max_depth': 25}</strong></span><span id="0f53" class="kv ke hh oz b fi pi pf l pg ph">rf_random.best_score_</span><span id="6feb" class="kv ke hh oz b fi pi pf l pg ph"><strong class="oz hi">-3.983773356232129</strong></span><span id="52ad" class="kv ke hh oz b fi pi pf l pg ph">predictions=rf_random.predict(X_test)</span><span id="3d55" class="kv ke hh oz b fi pi pf l pg ph"><strong class="oz hi">#Performance Metrics for Regression Use case</strong></span><span id="4fe3" class="kv ke hh oz b fi pi pf l pg ph">from sklearn import metrics<br/>print('MAE:', metrics.mean_absolute_error(y_test, predictions))<br/>print('MSE:', metrics.mean_squared_error(y_test, predictions))<br/>print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))</span><span id="3bb8" class="kv ke hh oz b fi pi pf l pg ph">Output:</span><span id="cbed" class="kv ke hh oz b fi pi pf l pg ph"><strong class="oz hi">MAE: 0.8849978021977988<br/>MSE: 3.9544237722813156<br/>RMSE: 1.9885733007061408</strong></span><span id="979c" class="kv ke hh oz b fi pi pf l pg ph"><strong class="oz hi">#Performance Metrics for Classification Use Case</strong></span><span id="a38e" class="kv ke hh oz b fi pi pf l pg ph">from sklearn.ensemble import RandomForestClassifier<br/>rf = RandomForestClassifier(n_estimators=500, random_state=101)<br/>rf.fit(X_train,y_train)<br/>y_pred= rf.predict(X_test)<br/>print(accuracy_score(y_test,y_pred))<br/>print(confusion_matrix(y_test,y_pred))<br/>print(classification_report(y_test,y_pred))</span><span id="79cb" class="kv ke hh oz b fi pi pf l pg ph"><strong class="oz hi">Output: <br/>0.9506974337261543<br/>[[  251   786    10]<br/> [  271 39763   266]<br/> [   10   753   403]]<br/>              precision    recall  f1-score   support</strong></span><span id="601b" class="kv ke hh oz b fi pi pf l pg ph"><strong class="oz hi">    1 - High       0.47      0.24      0.32      1047<br/>  2 - Medium       0.96      0.99      0.97     40300<br/>     3 - Low       0.59      0.35      0.44      1166</strong></span><span id="6ddc" class="kv ke hh oz b fi pi pf l pg ph"><strong class="oz hi">    accuracy                           0.95     42513<br/>   macro avg       0.68      0.52      0.58     42513<br/>weighted avg       0.94      0.95      0.94     42513</strong></span></pre><h2 id="6906" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">ii)神经网络:</h2><p id="3702" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">在这段代码中，我对一个<a class="ae ms" href="https://www.kaggle.com/shrutimechlearn/churn-modelling?select=Churn_Modelling.csv" rel="noopener ugc nofollow" target="_blank"> <em class="ji">客户流失建模</em> </a>数据集使用了一个序列模型，该数据集预测了未来可能离开银行的客户。该模型使用一个<code class="du ow ox oy oz b">enumerate</code>函数，该函数在3个隐藏的<code class="du ow ox oy oz b">layers</code>和两个激活函数<code class="du ow ox oy oz b">sigmoid</code>和<code class="du ow ox oy oz b">relu.</code>之间计数。<code class="du ow ox oy oz b">param_grid</code>就像是一个你想要包含的所有参数的框架，用来描述你的模型，并且通常在<em class="ji">键:值</em>对中给出。</p><p id="c271" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">该模型运行30个时期，最后GridSearch CV的<code class="du ow ox oy oz b">'best_score_'</code>和<code class="du ow ox oy oz b">'best_params_'</code>为我们提供了优化模型的参数。最后，计算混淆矩阵和准确度分数等性能度量来验证模型。</p><pre class="ls lt lu lv fd pa oz pb pc aw pd bi"><span id="dd53" class="kv ke hh oz b fi pe pf l pg ph">#Importing necessary libraries<br/>from keras.wrappers.scikit_learn import KerasClassifer<br/>from sklearn,model_selection import GridSearchCV<br/>from keras.models import Sequentialfrom keras.layers import Dense, Activation,Dropout<br/>from keras.activations import relu, sigmoid</span><span id="d9bf" class="kv ke hh oz b fi pi pf l pg ph">#Creating a Neural Model<br/>def create_model(layers, activation): <br/>model = Sequential()    <br/>for i, nodes in enumerate(layers):      <br/> if i==0:            model.add(Dense(nodes,input_dim=X_train.shape[1])) #1st input layer <br/>model.add(Activation(activation))          <br/>model.add(Dropout(0.3)) #Specifying the ratio of how many neurons to be dropped out to avoid overfitting     <br/> else:           <br/>model.add(Dense(nodes))           <br/>model.add(Activation(activation))            <br/>model.add(Dropout(0.3))               <br/> model.add(Dense(units = 1, kernel_initializer= 'glorot_uniform', activation = 'sigmoid'))<strong class="oz hi"> </strong># Note: no activation beyond this point<strong class="oz hi"> </strong>       </span><span id="e911" class="kv ke hh oz b fi pi pf l pg ph">model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])<br/> <br/>return model</span><span id="99c2" class="kv ke hh oz b fi pi pf l pg ph">model = KerasClassifier(build_fn=create_model, verbose=0) </span><span id="b2f4" class="kv ke hh oz b fi pi pf l pg ph">layers = [[20], [40, 20], [45, 30, 15]]<br/>activations = ['sigmoid', 'relu']<br/>param_grid = dict(layers=layers, activation=activations, batch_size = [128, 256], epochs=[30])</span><span id="0214" class="kv ke hh oz b fi pi pf l pg ph">grid = GridSearchCV(estimator=model, param_grid=param_grid,cv=5)<br/>grid_result = grid.fit(X_train, y_train)</span><span id="08d7" class="kv ke hh oz b fi pi pf l pg ph">[grid_result.best_score_,grid_result.best_params_]</span><span id="1160" class="kv ke hh oz b fi pi pf l pg ph"><strong class="oz hi">[0.8387499995157123]</strong></span><span id="df1d" class="kv ke hh oz b fi pi pf l pg ph"><strong class="oz hi"> {'activation': 'relu',<br/>  'batch_size': 256,<br/>  'epochs': 30,<br/>  'layers': [45, 30, 15]}</strong></span><span id="1e30" class="kv ke hh oz b fi pi pf l pg ph">pred_y = grid.predict(X_test)</span><span id="67b2" class="kv ke hh oz b fi pi pf l pg ph"><br/>#Performance Metric Calculation<br/>from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred)</span><span id="fad6" class="kv ke hh oz b fi pi pf l pg ph"><strong class="oz hi">array([[1595,    0],<br/>       [ 405,    0]], dtype=int64)</strong></span><span id="4fd9" class="kv ke hh oz b fi pi pf l pg ph">from sklearn.metrics import accuracy_score<br/>score=accuracy_score(y_test,y_pred)</span><span id="a0c9" class="kv ke hh oz b fi pi pf l pg ph"><strong class="oz hi">0.7975</strong></span></pre><h2 id="ae9e" class="kv ke hh bd kf kw kx ky kj kz la lb kn lc ld le kp lf lg lh kr li lj lk kt ll bi translated">iii) K-最近邻:</h2><p id="462c" class="pw-post-body-paragraph jg jh hh jj b jk lm ii jm jn ln il jp lc lo js jt lf lp jw jx li lq ka kb kc ha bi translated">该模型使用事件影响预测问题陈述将影响分为3类。“k”的最佳值是通过使用调谐从2–50以3的步长运行一个循环找到的。导入KNN分类器<code class="du ow ox oy oz b">from sklearn.neighbors import KNeighborsClassifier as KNC</code>后，对k=3进行分类，并基于该邻域中最近邻类别的最大值进行分类。</p><p id="6c6f" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">从下面的代码可以看出，<code class="du ow ox oy oz b">training accuracy: 0.975 and test accuracy: 0.96667</code>是用最优值<code class="du ow ox oy oz b">'k' as 9</code>实现的。</p><pre class="ls lt lu lv fd pa oz pb pc aw pd bi"><span id="8b99" class="kv ke hh oz b fi pe pf l pg ph">#Classification for 3 neighbors<br/>knn = KNC(n_neighbors= 3)<br/>knn.fit(predictor, y_train.values.ravel())<br/>y_train_pred= knn.predict(predictor)<br/>knn.score(predictor, y_train) # 89.4%<br/>print(confusion_matrix(y_train, y_train_pred))<br/>print(classification_report(y_train, y_train_pred)) # accuracy = 89%</span><span id="ed48" class="kv ke hh oz b fi pi pf l pg ph"># fit test data<br/>y_test_pred = knn.predict(pred_test)<br/>print(confusion_matrix(y_test, y_test_pred)) <br/>print(classification_report(y_test, y_test_pred))  # accuracy = 79%</span><span id="94a3" class="kv ke hh oz b fi pi pf l pg ph"># getting optimal "k" value<br/>a= []<br/>for i in range (2,50,3):<br/>    knn = KNC(n_neighbors=i)<br/>    knn.fit(predictor, y_train.values.ravel())<br/>    train_acc = knn.score(predictor, y_train)<br/>    test_acc = knn.score(pred_test, y_test)<br/>    a.append([train_acc,test_acc])</span></pre><p id="e93e" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">输出:</p><pre class="ls lt lu lv fd pa oz pb pc aw pd bi"><span id="2640" class="kv ke hh oz b fi pe pf l pg ph">[[105923     80     72]<br/> [  2299  83959   2577]<br/> [    72    109 105485]]<br/>              precision    recall  f1-score   support<br/><br/>           0       0.98      1.00      0.99    106075<br/>           1       1.00      0.95      0.97     88835<br/>           2       0.98      1.00      0.99    105666<br/><br/>    accuracy                           0.98    300576<br/>   macro avg       0.98      0.98      0.98    300576<br/>weighted avg       0.98      0.98      0.98    300576</span><span id="de1f" class="kv ke hh oz b fi pi pf l pg ph">[[26367    42    31]<br/> [ 1077 20078  1061]<br/> [   27    49 26412]]<br/>              precision    recall  f1-score   support<br/><br/>           0       0.96      1.00      0.98     26440<br/>           1       1.00      0.90      0.95     22216<br/>           2       0.96      1.00      0.98     26488<br/><br/>    accuracy                           0.97     75144<br/>   macro avg       0.97      0.97      0.97     75144<br/>weighted avg       0.97      0.97      0.97     75144</span></pre></div><div class="ab cl iw ix go iy" role="separator"><span class="iz bw bk ja jb jc"/><span class="iz bw bk ja jb jc"/><span class="iz bw bk ja jb"/></div><div class="ha hb hc hd he"><p id="3605" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">谢谢你看到这篇文章的结尾。这是我作为一名刚毕业的工科学生的第一篇文章，来自我在从事学术/实践项目时所掌握的一点点知识。我将很快发表这篇文章的第2部分，关于Flask上模型的部署。所以，敬请关注！！如果对这篇文章有任何疑问，请告诉我。</p><p id="e3c8" class="pw-post-body-paragraph jg jh hh jj b jk jl ii jm jn jo il jp lc jr js jt lf jv jw jx li jz ka kb kc ha bi translated">联系我:rushanbhag@gmail.com。</p></div></div>    
</body>
</html>