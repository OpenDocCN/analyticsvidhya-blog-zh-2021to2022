<html>
<head>
<title>Distributed Training in PyTorch (Distributed Data Parallel)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch中的分布式培训(分布式数据并行)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/distributed-training-in-pytorch-part-1-distributed-data-parallel-ae5c645e74cb?source=collection_archive---------1-----------------------#2021-04-17">https://medium.com/analytics-vidhya/distributed-training-in-pytorch-part-1-distributed-data-parallel-ae5c645e74cb?source=collection_archive---------1-----------------------#2021-04-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d12f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">今天，我们将讨论PyTorch中的分布式数据并行，它可用于跨GPU分布数据，以训练具有多个GPU的模型。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/fda68312e82c9a30ebf60aa866855e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fucE8mNBFRpluEBX"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">泰勒·维克在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="2ba8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就像我们在程序中使用线程以并行方式执行任务，以尽可能最快的时间完成任务一样，我们可以使用类似的技术在PyTorch中跨GPU并行训练深度学习模型。我们所需要的是一个有多个GPU的系统，或者每个系统中有多个GPU的多个系统。让我们看看如何在训练模型时实现并行性。</p><p id="c24a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了开始这样做，我们需要让自己习惯于一些我们可能会在前面的文章中用到的关键词。我们将按照给定的顺序讨论这些要点。</p><ol class=""><li id="9210" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">PyTorch中的数据并行</li><li id="723f" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">全局解释器锁(GIL)</li><li id="1acb" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">PyTorch中的分布式数据并行</li><li id="434f" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">关键词</li><li id="7796" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">履行</li></ol></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><h1 id="d213" class="kp kq hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">PyTorch中的数据并行</h1><p id="7d43" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated">如果我们想快速开始分布式训练，我们可以在PyTorch中使用数据并行，py torch使用线程来实现并行训练。</p><p id="085b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们所需要做的就是在你的脚本中添加一行代码，PyTorch会为我们处理并行性。我们基本上将在模型上添加一个包装器，让PyTorch知道它需要并行化。</p><pre class="je jf jg jh fd ls lt lu lv aw lw bi"><span id="1f0e" class="lx kq hi lt b fi ly lz l ma mb">model = torch.nn.DataParallel(model)</span></pre><p id="5c64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于数据并行使用线程来实现并行性，它遇到了一个众所周知的主要问题，该问题是由Python中的全局解释器锁(GIL)引起的。按照Python解释器的设计方式，使用线程不可能在Python中实现完美的并行性。让我们看看GIL是什么。</p><h1 id="a11c" class="kp kq hi bd kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li mg lk ll lm bi translated">全局解释器锁(GIL)</h1><p id="7158" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated">正如我前面提到的，Python解释器的实现方式，使用线程实现完美的并行是非常困难的。这是由于一种叫做全局解释器锁的东西。</p><h2 id="49cd" class="lx kq hi bd kr mh mi mj kv mk ml mm kz iq mn mo ld iu mp mq lh iy mr ms ll mt bi translated">GIL</h2><blockquote class="mu mv mw"><p id="0876" class="if ig mx ih b ii ij ik il im in io ip my ir is it mz iv iw ix na iz ja jb jc hb bi translated">Python全局解释器锁或GIL，简而言之，是一个互斥体(或锁)，只允许一个线程控制Python解释器。在任何时间点，只有一个线程可以处于执行状态。</p></blockquote><h2 id="055e" class="lx kq hi bd kr mh mi mj kv mk ml mm kz iq mn mo ld iu mp mq lh iy mr ms ll mt bi translated">互斥（体）…</h2><blockquote class="mu mv mw"><p id="c7c5" class="if ig mx ih b ii ij ik il im in io ip my ir is it mz iv iw ix na iz ja jb jc hb bi translated">互斥体是一个互斥对象，用于同步对资源的访问。它是在程序开始时用唯一的名称创建的。互斥体是一种锁定机制，确保一次只有一个线程可以获得互斥体并进入临界区。</p></blockquote><p id="1318" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这基本上违背了使用线程的初衷。这就是为什么我们在PyTorch中有一些可以用来实现完美并行的东西。</p><h1 id="f30d" class="kp kq hi bd kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li mg lk ll lm bi translated">PyTorch中的分布式数据并行</h1><p id="5260" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated">PyTorch中的DDP做了同样的事情，但方式更加熟练，在实现完美并行的同时也给了我们更好的控制。DDP使用多处理而不是线程，并通过模型作为每个GPU的不同进程来执行传播。DDP跨多个GPU复制模型，每个GPU由一个进程控制。这里的进程可以称为在您的系统上运行的脚本。通常我们会生成进程，这样每个GPU都有一个单独的进程。</p><p id="3348" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的每个流程都执行相同的任务，但使用不同的数据批次。每个过程与其他过程通信以共享梯度，该梯度需要在优化步骤期间被全部减少。在优化步骤结束时，每个过程都有平均梯度，确保模型权重保持同步。</p><h1 id="860a" class="kp kq hi bd kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li mg lk ll lm bi translated">关键词</h1><blockquote class="mu mv mw"><p id="ed52" class="if ig mx ih b ii ij ik il im in io ip my ir is it mz iv iw ix na iz ja jb jc hb bi translated"><strong class="ih hj">【节点】</strong>是你分布式架构中的一个系统。用外行人的话来说，拥有多个GPU的单个系统可以称为一个节点。</p><p id="7cc3" class="if ig mx ih b ii ij ik il im in io ip my ir is it mz iv iw ix na iz ja jb jc hb bi translated"><strong class="ih hj">“全局等级”</strong>是我们架构中每个节点的唯一标识号。</p><p id="fbe4" class="if ig mx ih b ii ij ik il im in io ip my ir is it mz iv iw ix na iz ja jb jc hb bi translated"><strong class="ih hj">“本地等级”</strong>是每个节点中流程的唯一标识号。</p><p id="a66f" class="if ig mx ih b ii ij ik il im in io ip my ir is it mz iv iw ix na iz ja jb jc hb bi translated"><strong class="ih hj">“world”</strong>是上述所有内容的联合，它可以有多个节点，每个节点产生多个流程。(理想情况下，每个GPU一个)</p><p id="0231" class="if ig mx ih b ii ij ik il im in io ip my ir is it mz iv iw ix na iz ja jb jc hb bi translated"><strong class="ih hj">“世界尺寸”</strong>等于<code class="du nb nc nd lt b">number of nodes * number of gpus</code></p></blockquote><h1 id="d6d3" class="kp kq hi bd kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li mg lk ll lm bi translated">履行</h1><p id="7f68" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated">让我们实现一个简单的示例，并浏览在跨GPU的分布式架构中训练模型所需的重要更改！我们将实现一个简单的分类器，该分类器使用非常著名的MNIST数据集对手写数字进行分类。我不会详细讲述编写神经网络的所有细节和其他基本内容，因为这篇文章假设您对在PyTorch中训练一个简单模型有一个基本的概念。事不宜迟，我们开始吧！</p><p id="fa17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，导入必要的库。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="ne nf l"/></div></figure><p id="54ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们编写一个简单的convnet，它可以将给定的输入分成十类。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="ne nf l"/></div></figure><p id="11e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们实现<code class="du nb nc nd lt b">main</code>函数，它将接受定义节点id、GPU数量、等级和其他内容的命令行参数。我们需要这些参数来确保每个进程可以与主节点通信，以减少梯度并获得平均梯度。每个进程都需要知道使用哪个GPU，以及它在所有正在运行的进程中的排名。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="ne nf l"/></div></figure><p id="2f1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将传递这样的参数——带有<strong class="ih hj"> <em class="mx"> '-n' </em> </strong>的节点id，带有<em class="mx">'</em><strong class="ih hj"><em class="mx">-g '</em></strong>的GPU数量，带有<strong class="ih hj"> <em class="mx"> '-nr' </em> </strong>的全局秩，以及带有<strong class="ih hj"> <em class="mx"> ' — epochs '的历元数。</em>T29】</strong></p><p id="e12c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du nb nc nd lt b">world_size</code>是通过将<em class="mx">的GPU数量和节点数量相乘计算出来的。</em><code class="du nb nc nd lt b">MASTER_ADDRESS</code>和<code class="du nb nc nd lt b">MASTER_PORT</code>被设置为环境变量，可以被节点上的所有进程直接访问。</p><p id="399f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们生成了<code class="du nb nc nd lt b">train</code>函数，它执行脚本中最重要的部分，即训练模型。<code class="du nb nc nd lt b">nprocs</code>定义了要派生的进程数，等于<code class="du nb nc nd lt b">args.gpus</code>。默认情况下，生成train函数时，它会获得一个参数，用于从所有生成的进程中标识单个进程。这个数字是一个从<code class="du nb nc nd lt b">0 to args.gpus — 1</code>开始的整数。该编号可用于识别GPU编号以及同一范围内的设备编号。</p><p id="4807" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，让我们实现<code class="du nb nc nd lt b">train</code>函数，看看我们如何广泛地使用这些参数。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="ne nf l"/></div></figure><p id="9235" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最初，我们使用<code class="du nb nc nd lt b">args.nr * args.gpus + gpu</code>计算工作者的等级。如果这是第二个节点上具有2个GPU的第一个工作者，这里的数字将是:<code class="du nb nc nd lt b">args.nr = 1</code>因为它是第二个节点，<code class="du nb nc nd lt b">args.gpus = 2</code>因为在该节点上有2个GPU，并且<code class="du nb nc nd lt b">gpu = 0</code>因为这是第一个工作者在第二个节点上使用第一个GPU。因此，这里计算的等级将等于2，这就是我们在所有进程中的<strong class="ih hj">局部等级</strong>。</p><p id="5d91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们用<a class="ae jt" href="https://developer.nvidia.com/nccl" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">nccl</strong></a><strong class="ih hj"/>初始化进程组，这是NVIDIA集体通信库，实现了针对NVIDIA GPUs和网络优化的多GPU和多节点通信原语。我们还传入了<code class="du nb nc nd lt b">init_method = 'env://'</code>，它基本上是说，像<code class="du nb nc nd lt b">MASTER_ADDRESS</code>和<code class="du nb nc nd lt b">MASTER_PORT</code>这样的信息应该从环境变量中获取。我们指定我们计算的<code class="du nb nc nd lt b">world_size</code>和<code class="du nb nc nd lt b">local_rank</code>。</p><p id="d7ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第21行，我们用PyTorch的<code class="du nb nc nd lt b">DistributedDataParallel</code>类包装我们的模型，它负责模型克隆和并行训练。</p><p id="85d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第31行，我们初始化了一个采样器，它可以在不重复任何批处理的情况下，处理不同GPU上的批处理的分布式采样。这是使用<code class="du nb nc nd lt b">DistributedSampler</code>完成的。我们还在下一行中向数据加载器传递这个采样器，数据加载器可以在批处理数据时使用这个采样器。</p><p id="601b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是我们如何在多个GPU上用分布式数据训练我们的模型。你可以在这个<a class="ae jt" href="https://gist.github.com/Praneet9/185fd5c50ea337e2ca5a3e3bf29334cc" rel="noopener ugc nofollow" target="_blank">链接</a>上获得完整的代码！如果我需要阐述任何观点或需要任何更正，请评论下来。</p><p id="93ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你也可以参考neptune.ai <a class="ae jt" href="https://neptune.ai/blog/distributed-training-frameworks-and-tools" rel="noopener ugc nofollow" target="_blank">这里</a>的一篇关于分布式培训可用的各种框架和工具的博客。</p><h1 id="b35a" class="kp kq hi bd kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li mg lk ll lm bi translated"><strong class="ak">参考文献</strong></h1><div class="ng nh ez fb ni nj"><a href="https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab dw"><div class="nl ab nm cl cj nn"><h2 class="bd hj fi z dy no ea eb np ed ef hh bi translated">分布式数据并行- PyTorch 1.8.1文档</h2><div class="nq l"><h3 class="bd b fi z dy no ea eb np ed ef dx translated">class torch . nn . parallel . distributed data parallel(module，device_ids=None，output_device=None，dim=0…</h3></div><div class="nr l"><p class="bd b fp z dy no ea eb np ed ef dx translated">pytorch.org</p></div></div></div></a></div><div class="ng nh ez fb ni nj"><a href="https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab dw"><div class="nl ab nm cl cj nn"><h2 class="bd hj fi z dy no ea eb np ed ef hh bi translated">Pytorch中的分布式数据并行训练</h2><div class="nq l"><h3 class="bd b fi z dy no ea eb np ed ef dx translated">2019年10月18日编辑:我们需要在每个过程中设置随机种子，以便用相同的…</h3></div><div class="nr l"><p class="bd b fp z dy no ea eb np ed ef dx translated">yangkky.github.io</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx jn nj"/></div></div></a></div><div class="ng nh ez fb ni nj"><a href="https://wiki.python.org/moin/GlobalInterpreterLock" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab dw"><div class="nl ab nm cl cj nn"><h2 class="bd hj fi z dy no ea eb np ed ef hh bi translated">GlobalInterpreterLock-Python Wiki</h2><div class="nq l"><h3 class="bd b fi z dy no ea eb np ed ef dx translated">在CPython中，全局解释器锁或GIL是一个互斥体，它保护对Python对象的访问，防止多个…</h3></div><div class="nr l"><p class="bd b fp z dy no ea eb np ed ef dx translated">wiki.python.org</p></div></div></div></a></div><div class="ng nh ez fb ni nj"><a href="https://www.tutorialspoint.com/mutex-vs-semaphore" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab dw"><div class="nl ab nm cl cj nn"><h2 class="bd hj fi z dy no ea eb np ed ef hh bi translated">互斥与信号量</h2><div class="nq l"><h3 class="bd b fi z dy no ea eb np ed ef dx translated">互斥和信号量都提供同步服务，但它们并不相同。关于互斥体和…</h3></div><div class="nr l"><p class="bd b fp z dy no ea eb np ed ef dx translated">www.tutorialspoint.com</p></div></div><div class="ns l"><div class="ny l nu nv nw ns nx jn nj"/></div></div></a></div><div class="ng nh ez fb ni nj"><a href="https://neptune.ai/blog/distributed-training-frameworks-and-tools" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab dw"><div class="nl ab nm cl cj nn"><h2 class="bd hj fi z dy no ea eb np ed ef hh bi translated">分布式培训:框架和工具- neptune.ai</h2><div class="nq l"><h3 class="bd b fi z dy no ea eb np ed ef dx translated">深度学习的最新发展已经带来了一些令人着迷的最新成果，特别是在以下领域……</h3></div><div class="nr l"><p class="bd b fp z dy no ea eb np ed ef dx translated">海王星. ai</p></div></div><div class="ns l"><div class="nz l nu nv nw ns nx jn nj"/></div></div></a></div></div></div>    
</body>
</html>