<html>
<head>
<title>The distance-based algorithms in data mining</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据挖掘中基于距离的算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-distance-based-algorithms-in-data-mining-9752be4ab9d4?source=collection_archive---------1-----------------------#2021-09-12">https://medium.com/analytics-vidhya/the-distance-based-algorithms-in-data-mining-9752be4ab9d4?source=collection_archive---------1-----------------------#2021-09-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="36df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些算法用于测量每个文本之间的距离并计算分数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/bbebde65bffcfe68fb7bb157219fe5a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*shjyDzr4XORzKWe0.jpeg"/></div></div></figure><p id="a634" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">距离度量在机器学习中起着重要的作用</p><p id="8119" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它们为许多流行和有效的机器学习算法提供了基础，如用于监督学习的KNN(K-最近邻)和用于非监督学习的K-均值聚类。</p><p id="a963" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">必须根据数据类型选择和使用不同的距离度量，因此，了解如何实施和计算一系列不同的流行距离度量以及对结果分数的直觉非常重要。</p><p id="c6d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个博客中，我们将发现机器学习中的距离度量。</p><h2 id="5aa4" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated"><strong class="ak">概述:</strong></h2><ol class=""><li id="1258" class="kk kl hi ih b ii km im kn iq ko iu kp iy kq jc kr ks kt ku bi translated">距离测量的作用</li><li id="e7bc" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">汉娩距</li><li id="d45e" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">欧几里得距离</li><li id="ab17" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">曼哈顿距离(出租车或城市街区)</li><li id="df7e" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">闵可夫斯基距离</li><li id="27ca" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">马哈拉诺比斯距离</li><li id="8970" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">余弦相似性</li></ol><h1 id="6c2d" class="la jq hi bd jr lb lc ld jv le lf lg jz lh li lj kc lk ll lm kf ln lo lp ki lq bi translated"><strong class="ak">距离测量的作用</strong></h1><p id="d15d" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">距离度量在机器学习中起着重要的作用</p><p id="f73c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">距离度量是一个客观分数，它概括了问题域中两个对象之间的相对差异。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/42dee69b896d27ec5c5c30c15f96dd4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*qaMYet89rhEWoz_Q"/></div></figure><p id="1f80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最常见的是，这两个对象是描述一个主题(比如一个人、一辆车或一栋房子)或一个事件(比如购买、索赔或诊断)的数据行</p><p id="c555" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">也许，我们最有可能遇到距离度量的方式是当我们使用特定的机器学习算法时，该算法以距离度量为核心。最著名的算法是KNN—[K-最近邻算法]</p><h1 id="850e" class="la jq hi bd jr lb lc ld jv le lf lg jz lh li lj kc lk ll lm kf ln lo lp ki lq bi translated"><strong class="ak"> KNN </strong></h1><p id="4cd3" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">通过计算训练数据集中新样本集和所有现有样本集之间的距离，对新样本进行分类或回归预测。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/02b754c256dce9fdd50c16494a007f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*MZKG8sTIdSNv6TXB"/></div></figure><p id="9afd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后选择训练数据集中具有最小距离的K个示例，并通过对结果(类别标签的模式或回归的真实值的平均值)进行平均来进行预测</p><p id="ba56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KNN属于一个更广泛的算法领域，称为基于案例或基于实例的学习，其中大多数以类似的方式使用距离度量。另一种使用距离测量的流行的基于实例的算法是学习矢量量化或LVQ，该算法也可以被认为是一种神经网络。</p><p id="f493" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们有自组织映射算法，即SOM，这是一种也使用距离度量的算法，可用于监督和非监督学习算法，其核心是K-means聚类算法。</p><p id="7cde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在基于实例的学习中，训练样本被逐字存储，并且使用距离函数来确定训练集中的哪个成员最接近未知的测试实例。一旦定位了最近的训练实例，就为测试实例预测其类别。</p><p id="f260" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一些更流行的机器学习算法的核心是使用距离度量</p><ol class=""><li id="20e2" class="kk kl hi ih b ii ij im in iq lw iu lx iy ly jc kr ks kt ku bi translated">k-最近邻(KNN)</li><li id="05c6" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">学习矢量量化(LVQ)</li><li id="bccf" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">自组织映射(SOM)</li><li id="5ac6" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">k均值聚类</li></ol><p id="e003" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有许多基于核的方法也可以被认为是基于距离的算法。</p><p id="134c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">也许最广为人知的核方法是支持向量机算法(SVM)</p><blockquote class="lz ma mb"><p id="31f1" class="if ig mc ih b ii ij ik il im in io ip md ir is it me iv iw ix mf iz ja jb jc hb bi translated">当计算两个示例或数据行之间的距离时，可能会对示例集的不同列使用不同的数据类型。</p></blockquote><p id="80da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">示例集可能有实值、布尔值、分类值和序数值。</p><blockquote class="lz ma mb"><p id="9e50" class="if ig mc ih b ii ij ik il im in io ip md ir is it me iv iw ix mf iz ja jb jc hb bi translated">对于每一个可能需要不同的距离测量，这些距离测量被加在一起成为单个距离分数。</p></blockquote><p id="688f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数值可能有不同的标度。这可能会极大地影响距离度量的计算，通常在计算距离度量之前对数值进行归一化或标准化是一种好的做法。</p><p id="8ea5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回归问题中的数值误差也可以被认为是距离。例如，预期值和预测值之间的误差是一维距离度量，可以对测试集中的所有示例求和或求平均值，以给出数据集中预期结果和预测结果之间的总距离。</p><p id="86db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">误差的计算，例如均方误差或平均绝对误差，可以类似于标准的距离测量。</p><p id="de60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所见，距离度量在机器学习中起着重要的作用，</p><p id="940e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">机器学习中最常用的距离度量是</p><ol class=""><li id="fcf8" class="kk kl hi ih b ii ij im in iq lw iu lx iy ly jc kr ks kt ku bi translated">汉娩距</li><li id="c1e7" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">欧几里得距离</li><li id="1020" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">曼哈顿距离</li><li id="e28b" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">闵可夫斯基距离</li><li id="7515" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">马哈拉诺比斯</li></ol><p id="9589" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最重要的是知道在从头实现算法时如何计算这些距离度量，以及在使用利用这些距离度量的算法时对计算内容的直觉。</p><h1 id="11d3" class="la jq hi bd jr lb lc ld jv le lf lg jz lh li lj kc lk ll lm kf ln lo lp ki lq bi translated"><strong class="ak">海明距离</strong></h1><p id="df1e" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">汉明距离计算两个二进制向量(也称为二进制串或位串)之间的距离</p><p id="9d7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们对数据的分类列进行一次性编码时，我们很可能会遇到二进制字符串。</p><p id="cc7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">举个例子，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mg"><img src="../Images/7c6b12c8484958a75e741c3ae6c7ea7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*YkORoIwatCCe5BdpQIMyhQ.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">示例集</figcaption></figure><p id="ba0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在一次热编码之后</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/5d0fce01c80e66fede961b4641253676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*pE0ogeOxhwrjFvefFAH9yA.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">具有独热编码的示例集</figcaption></figure><p id="6fd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">红色和绿色之间的距离可以计算为两个位串之间的位差的和或平均数。这是海明距离。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/88d8798a0023f9d73dd28caf43d102a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*nnsjsKSGolon-O5p.png"/></div></figure><p id="60f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于一个独热编码的字符串，总结字符串之间的位差总和可能更有意义，它将始终是0或1。</p><ul class=""><li id="02e7" class="kk kl hi ih b ii ij im in iq lw iu lx iy ly jc mm ks kt ku bi translated">汉明距离= I至N abs之和(v1[i] — v2[i])</li></ul><p id="c1a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于可能具有许多1位的位串，更常见的是计算位差的平均数，以给出介于0(相同)和1(全部不同)之间的汉明距离分数。</p><ul class=""><li id="5e55" class="kk kl hi ih b ii ij im in iq lw iu lx iy ly jc mm ks kt ku bi translated">汉明距离=(I与N的绝对值之和(v1[I]—v2[I])/N</li></ul><p id="3340" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以用一个计算两个位串之间的汉明距离的例子来证明这一点，如下所示。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="4af4" class="jp jq hi mo b fi ms mt l mu mv"># calculating hamming distance between bit string<br/># calculate hamming distance<br/>def hamming_distance(a,b):<br/>    return sum(abs(e1-e2) for e1, e2 in zip(a,b)) / len(a)</span><span id="458b" class="jp jq hi mo b fi mw mt l mu mv"># define data<br/>row1 = [0,0,0,0,0,1]<br/>row2 = [0,0,0,0,1,0]</span><span id="a3d8" class="jp jq hi mo b fi mw mt l mu mv"># calculate distance<br/>dist = hamming_distance(row1, row2)<br/>print(dist)</span></pre><p id="1e02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，字符串之间有两个差异，即6位差异中的2位，其平均值(2/6)约为1/3或0.33。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="10aa" class="jp jq hi mo b fi ms mt l mu mv">0.33333333333333</span></pre><p id="3b04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还可以使用SciPy中的hamming()函数执行相同的计算。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="70aa" class="jp jq hi mo b fi ms mt l mu mv"># calculating hamming distance between bit strings<br/>from scipy.spatial.distance import hamming</span><span id="07fb" class="jp jq hi mo b fi mw mt l mu mv"># define data<br/>row1 = [0, 0, 0, 0, 0, 1]<br/>row2 = [0, 0, 0, 0, 1, 0]</span><span id="5ac1" class="jp jq hi mo b fi mw mt l mu mv"># calculate distance<br/>dist = hamming(row1, row2)<br/>print(dist)</span></pre><p id="2455" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们可以确认我们的例子得到相同的结果，确认我们的手动实现</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="d939" class="jp jq hi mo b fi ms mt l mu mv">0.33333333333333</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mx"><img src="../Images/86da309ca349f958ff5b7e4d5048b6fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/0*-i65UaK-6qKNV9LE.png"/></div></figure><h1 id="6328" class="la jq hi bd jr lb lc ld jv le lf lg jz lh li lj kc lk ll lm kf ln lo lp ki lq bi translated"><strong class="ak">欧几里德距离</strong></h1><p id="85fc" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">欧几里德距离计算两个实值向量之间的距离。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es my"><img src="../Images/9e221c7f1833666d9db0bab30eadf901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*txt6ZH-FL-xTIixq.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">摘自汗学院的距离公式教程</figcaption></figure><p id="40d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了计算数据点之间的距离，A和B勾股定理考虑了x轴和y轴的长度。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mz"><img src="../Images/cc99a2c1f0b07e38466f51822fb1f0a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*8mDqyhw8bPWWvZGj.png"/></div></figure><p id="753f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在计算具有数值(如浮点或整数值)的两行数据之间的距离时，您最有可能使用欧几里德距离。</p><p id="84cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果列中的值具有不同的刻度，通常在计算欧几里德距离之前对所有列中的数值进行规范化或标准化。否则，具有大值的列将主导距离度量。</p><p id="12b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">欧几里德距离的计算方法是两个向量的平方差之和的平方根。</p><ul class=""><li id="bd48" class="kk kl hi ih b ii ij im in iq lw iu lx iy ly jc mm ks kt ku bi translated">欧几里德距离= sqrt(I到N的和(v1[i] — v2[i]))</li></ul><p id="6be6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果距离计算要执行数千次或数百万次，为了加速计算，通常要去掉平方根运算。在这种修改之后，得到的分数将具有相同的相对比例，并且仍然可以在机器学习算法中有效地用于寻找最相似的例子。</p><ul class=""><li id="1a34" class="kk kl hi ih b ii ij im in iq lw iu lx iy ly jc mm ks kt ku bi translated">欧几里德距离= I到N的和(v1[i] — v2[i])</li></ul><p id="0e63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种计算与L2向量范数有关，如果加上平方根，则相当于平方和误差和平方和误差的平方根。</p><p id="e882" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以用一个计算两个实值向量之间欧几里德距离的例子来证明这一点，如下所示。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="4829" class="jp jq hi mo b fi ms mt l mu mv"># calculating euclidean distance between vectors<br/>from math import sqrt</span><span id="5ec9" class="jp jq hi mo b fi mw mt l mu mv"># calculate euclidean distance<br/>def euclidean_distance(a, b):<br/>return sqrt(sum((e1-e2)**2 for e1, e2 in zip(a,b)))</span><span id="3d37" class="jp jq hi mo b fi mw mt l mu mv"># define data<br/>row1 = [10, 20, 15, 10, 5]<br/>row2 = [12, 24, 18, 8, 7]</span><span id="1711" class="jp jq hi mo b fi mw mt l mu mv"># calculate distance<br/>dist = euclidean_distance(row1, row2)<br/>print(dist)</span></pre><p id="fd39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行该示例会报告两个向量之间的欧几里德距离。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="bb7c" class="jp jq hi mo b fi ms mt l mu mv">6.082762530298219</span></pre><p id="1e73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们也可以使用SciPy中的<a class="ae na" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html" rel="noopener ugc nofollow" target="_blank">欧几里德()函数</a>来执行相同的计算。下面列出了完整的示例。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="74fd" class="jp jq hi mo b fi ms mt l mu mv"># calculating euclidean distance between vectors<br/>from scipy.spatial.distance import euclidean</span><span id="6150" class="jp jq hi mo b fi mw mt l mu mv"># define data<br/>row1 = [10, 20, 15, 10, 5]<br/>row2 = [12, 24, 18, 8, 7]</span><span id="ecb4" class="jp jq hi mo b fi mw mt l mu mv"># calculate distance<br/>dist = euclidean(row1, row2)<br/>print(dist)</span></pre><p id="56fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行这个例子，我们可以看到我们得到了相同的结果，证实了我们的手动实现。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="ddbe" class="jp jq hi mo b fi ms mt l mu mv">6.082762530298219</span></pre><h1 id="fb9b" class="la jq hi bd jr lb lc ld jv le lf lg jz lh li lj kc lk ll lm kf ln lo lp ki lq bi translated">曼哈顿距离(出租车或城市街区距离)</h1><p id="0bed" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated"><a class="ae na" href="https://en.wikipedia.org/wiki/Taxicab_geometry" rel="noopener ugc nofollow" target="_blank">曼哈顿距离</a>，也称为出租车距离或城市街区距离，计算两个实值向量之间的距离。</p><p id="922a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于在统一网格上描述对象的矢量，比如棋盘或城市街区，它可能更有用。测量值的出租车名称指的是测量值计算结果的直觉:出租车在城市街区之间行驶的最短路径(网格上的坐标)。</p><p id="cd69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于整数特征空间中的两个向量，计算曼哈顿距离而不是欧几里德距离可能是有意义的。</p><p id="8e0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">曼哈顿距离计算为两个向量之间的绝对差之和。</p><ul class=""><li id="2f7c" class="kk kl hi ih b ii ij im in iq lw iu lx iy ly jc mm ks kt ku bi translated">Manhattan distance = sum for I to N sum | v1[I]—v2[I]|</li></ul><p id="7022" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">曼哈顿距离与L1向量范数以及绝对误差和平均绝对误差度量有关。</p><p id="5394" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以用一个计算两个整数向量之间的曼哈顿距离的例子来证明这一点，如下所示。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="ffa6" class="jp jq hi mo b fi ms mt l mu mv"># calculating manhattan distance between vectors<br/>from math import sqrt</span><span id="d629" class="jp jq hi mo b fi mw mt l mu mv"># calculate manhattan distance<br/>def manhattan_distance(a, b):<br/>return sum(abs(e1-e2) for e1, e2 in zip(a,b))</span><span id="98c0" class="jp jq hi mo b fi mw mt l mu mv"># define data<br/>row1 = [10, 20, 15, 10, 5]<br/>row2 = [12, 24, 18, 8, 7]</span><span id="8856" class="jp jq hi mo b fi mw mt l mu mv"># calculate distance<br/>dist = manhattan_distance(row1, row2)<br/>print(dist)</span></pre><p id="1aaa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行该示例将报告两个向量之间的曼哈顿距离。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="9105" class="jp jq hi mo b fi ms mt l mu mv">13</span></pre><p id="c27e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还可以使用SciPy中的<a class="ae na" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cityblock.html" rel="noopener ugc nofollow" target="_blank"> cityblock()函数</a>执行相同的计算。下面列出了完整的示例。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="49fb" class="jp jq hi mo b fi ms mt l mu mv"># calculating manhattan distance between vectors<br/>from scipy.spatial.distance import cityblock</span><span id="5f8d" class="jp jq hi mo b fi mw mt l mu mv"># define data<br/>row1 = [10, 20, 15, 10, 5]<br/>row2 = [12, 24, 18, 8, 7]</span><span id="f7d1" class="jp jq hi mo b fi mw mt l mu mv"># calculate distance<br/>dist = cityblock(row1, row2)<br/>print(dist)</span></pre><p id="cbcb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行这个例子，我们可以看到我们得到了相同的结果，证实了我们的手动实现。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="d28a" class="jp jq hi mo b fi ms mt l mu mv">13</span></pre><h1 id="d0b2" class="la jq hi bd jr lb lc ld jv le lf lg jz lh li lj kc lk ll lm kf ln lo lp ki lq bi translated">闵可夫斯基距离</h1><p id="89b0" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated"><a class="ae na" href="https://en.wikipedia.org/wiki/Minkowski_distance" rel="noopener ugc nofollow" target="_blank">闵可夫斯基距离</a>计算两个实值向量之间的距离。</p><p id="c738" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它是欧几里德和曼哈顿距离度量的推广，并添加了一个参数，称为“<em class="mc">阶</em>或“<em class="mc"> p </em>”，允许计算不同的距离度量。</p><p id="98b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">闵可夫斯基距离度量的计算如下:</p><ul class=""><li id="965d" class="kk kl hi ih b ii ij im in iq lw iu lx iy ly jc mm ks kt ku bi translated">欧几里德距离=(I到n的和(abs(v1[i] — v2[i]))^p)^(1/p)</li></ul><p id="cdc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中“<em class="mc"> p </em>”为顺序参数。</p><p id="577e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当p设置为1时，计算结果与曼哈顿距离相同。当p设置为2时，它与欧几里德距离相同。</p><ul class=""><li id="77c1" class="kk kl hi ih b ii ij im in iq lw iu lx iy ly jc mm ks kt ku bi translated"><em class="mc"> p=1 </em>:曼哈顿距离。</li><li id="eb1a" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc mm ks kt ku bi translated"><em class="mc"> p=2 </em>:欧氏距离。</li></ul><p id="5ae9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">中间值提供了两种度量之间的受控平衡。</p><p id="f903" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当实现使用距离度量的机器学习算法时，通常使用Minkowski距离，因为它通过可以调整的超参数“<em class="mc"> p </em>”来控制用于实值向量的距离度量的类型。</p><p id="cfe4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以用下面列出的计算两个实向量之间的闵可夫斯基距离的例子来演示这种计算。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="b606" class="jp jq hi mo b fi ms mt l mu mv"># calculating minkowski distance between vectors<br/>from math import sqrt</span><span id="9932" class="jp jq hi mo b fi mw mt l mu mv"># calculate minkowski distance<br/>def minkowski_distance(a, b, p):<br/>return sum(abs(e1-e2)**p for e1, e2 in zip(a,b))**(1/p)</span><span id="d3f9" class="jp jq hi mo b fi mw mt l mu mv"># define data<br/>row1 = [10, 20, 15, 10, 5]<br/>row2 = [12, 24, 18, 8, 7]</span><span id="5f68" class="jp jq hi mo b fi mw mt l mu mv"># calculate distance (p=1)<br/>dist = minkowski_distance(row1, row2, 1)<br/>print(dist)</span><span id="0320" class="jp jq hi mo b fi mw mt l mu mv"># calculate distance (p=2)<br/>dist = minkowski_distance(row1, row2, 2)<br/>print(dist)</span></pre><p id="3770" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行该示例时，首先计算并打印Minkowski距离，将<em class="mc"> p </em>设置为1以给出曼哈顿距离，然后将<em class="mc"> p </em>设置为2以给出欧几里得距离，匹配对前面部分的相同数据计算的值。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="f232" class="jp jq hi mo b fi ms mt l mu mv">13.0<br/>6.082762530298219</span></pre><p id="e48b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们也可以使用SciPy中的<a class="ae na" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.minkowski_distance.html" rel="noopener ugc nofollow" target="_blank"> minkowski_distance()函数</a>来执行相同的计算。下面列出了完整的示例。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="642f" class="jp jq hi mo b fi ms mt l mu mv"># calculating minkowski distance between vectors<br/>from scipy.spatial import minkowski_distance</span><span id="73fb" class="jp jq hi mo b fi mw mt l mu mv"># define data<br/>row1 = [10, 20, 15, 10, 5]<br/>row2 = [12, 24, 18, 8, 7]</span><span id="d795" class="jp jq hi mo b fi mw mt l mu mv"># calculate distance (p=1)<br/>dist = minkowski_distance(row1, row2, 1)<br/>print(dist)</span><span id="3dd6" class="jp jq hi mo b fi mw mt l mu mv"># calculate distance (p=2)<br/>dist = minkowski_distance(row1, row2, 2)<br/>print(dist)</span></pre><p id="dd2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行这个例子，我们可以看到我们得到了相同的结果，证实了我们的手动实现。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="a4ad" class="jp jq hi mo b fi ms mt l mu mv">13.0<br/>6.082762530298219</span></pre><h1 id="579a" class="la jq hi bd jr lb lc ld jv le lf lg jz lh li lj kc lk ll lm kf ln lo lp ki lq bi translated">马哈拉诺比斯距离</h1><p id="c158" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">Mahalanobis距离是一种有效的多元距离度量，用于测量点(向量)和分布之间的距离。这是一个非常有用的度量，在多变量异常检测、高度不平衡数据集的分类和单类分类中有很好的应用。</p><p id="1400" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它在多元异常检测、高度不平衡数据集上的分类和单类分类以及更多未开发的用例中具有出色的应用。</p><p id="650e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑到其极其有用的应用，这一指标很少在stats或ML工作流中讨论或使用。这篇文章解释了使用马氏距离的原因和时间，然后解释了直觉和有用的数学应用。</p><p id="da71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">马氏距离是一个点和一个分布之间的距离。而不是在两个不同的点之间。它实际上是欧几里得距离的多元等价物</p><ol class=""><li id="f946" class="kk kl hi ih b ii ij im in iq lw iu lx iy ly jc kr ks kt ku bi translated">它将列转换成不相关的变量</li><li id="07a2" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">缩放列，使它们的方差等于1</li><li id="8431" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">最后，它计算欧几里德距离</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nb"><img src="../Images/6b8ade218295a7ada56d605c4e911257.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*R3ZaRSEhx72-IDWXnzYcMQ.png"/></div></figure><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="8e16" class="jp jq hi mo b fi ms mt l mu mv">where, <br/> - D^2 is the square of the Mahalanobis distance. <br/> - x is the vector of the observation (row in a dataset), <br/> - m is the vector of mean values of independent variables (mean of each column), <br/> - C^(-1) is the inverse covariance matrix of independent variables.</span></pre><p id="dd97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算马氏距离</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="33dc" class="jp jq hi mo b fi ms mt l mu mv">import pandas as pd<br/>import scipy as sp<br/>import numpy as np<br/><br/>filepath = 'local/input<a class="ae na" href="https://raw.githubusercontent.com/selva86/datasets/master/diamonds.csv" rel="noopener ugc nofollow" target="_blank">.csv</a>'<br/>df = pd.read_csv(filepath).iloc[:, [0,4,6]]<br/>df.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nc"><img src="../Images/3a1fbdb254a987d7cf9d046d52875626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*DGXIfGtrRZ4hQC5UFtiSvA.png"/></div></figure><p id="186b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们编写计算Mahalanobis距离的函数:</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="b9ea" class="jp jq hi mo b fi ms mt l mu mv">def mahalanobis(x=None, data=None, cov=None):<br/>    """Compute the Mahalanobis Distance between each row of x and the data  <br/>    x    : vector or matrix of data with, say, p columns.<br/>    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.<br/>    cov  : covariance matrix (p x p) of the distribution. If None, will be computed from data.<br/>    """<br/>    x_minus_mu = x - np.mean(data)<br/>    if not cov:<br/>        cov = np.cov(data.values.T)<br/>    inv_covmat = sp.linalg.inv(cov)<br/>    left_term = np.dot(x_minus_mu, inv_covmat)<br/>    mahal = np.dot(left_term, x_minus_mu.T)<br/>    return mahal.diagonal()<br/><br/>df_x = df[['A', 'B', 'C']].head(500)<br/>df_x['maha_dist'] = mahalanobis(x=df_x, data=df[['A', 'B', 'C']])<br/>df_x.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nd"><img src="../Images/4a04f089f70bf83628d1745dafbe440e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*nNepnOiHfhaiaXy-TQ3dvQ.png"/></div></figure><h1 id="e6aa" class="la jq hi bd jr lb lc ld jv le lf lg jz lh li lj kc lk ll lm kf ln lo lp ki lq bi translated">余弦距离:</h1><p id="22e5" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">余弦距离度量主要用于发现不同文档之间的相似性。在余弦度量中，我们测量两个文档/向量之间的角度(作为度量收集的不同文档中的术语频率)。当向量之间的大小无关紧要但方向重要时，使用这种特定的度量。</p><p id="9524" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">余弦相似性公式可以从点积公式中导出</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ne"><img src="../Images/2dd01cbf8a0fc97b31640c2bd00458c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/0*qvMYPEQHncGJtBRh.png"/></div></figure><p id="36b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，你一定在想，余弦角的哪个值会有助于找出相似点。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nf"><img src="../Images/b0aabead1ed0274a6a27c769c7ce7669.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/0*_D1MypdunQmRYxWw.png"/></div></figure><p id="b25d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了用来衡量相似性的值，我们需要知道1，0和-1分别代表什么。</p><p id="cf33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里余弦值1用于指向相同方向的向量，即在文档/数据点之间存在相似性。正交向量为零，即不相关(发现一些相似性)。指向相反方向的向量的值为-1(无相似性)。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="b7d4" class="jp jq hi mo b fi ms mt l mu mv">sklearn.metrics.pairwise.<strong class="mo hj">cosine_similarity</strong>(<em class="mc">X</em>, <em class="mc">Y=None</em>, <em class="mc">dense_output=True</em>)</span></pre><p id="3f0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">余弦相似性示例</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="0c99" class="jp jq hi mo b fi ms mt l mu mv">from scipy import spatial<br/><br/>dataSetI = [3, 45, 7, 2]<br/>dataSetII = [2, 54, 13, 15]<br/>result = 1 - spatial.distance.cosine(dataSetI, dataSetII)</span></pre><p id="c772" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个基于Numpy的版本</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="3b56" class="jp jq hi mo b fi ms mt l mu mv">from numpy import dot<br/>from numpy.linalg import norm<br/><br/>cos_sim = dot(a, b)/(norm(a)*norm(b))</span></pre><p id="d914" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">定义余弦相似度函数</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="d5d9" class="jp jq hi mo b fi ms mt l mu mv">import math<br/>def cosine_similarity(v1,v2):<br/>    "compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)"<br/>    sumxx, sumxy, sumyy = 0, 0, 0<br/>    for i in range(len(v1)):<br/>        x = v1[i]; y = v2[i]<br/>        sumxx += x*x<br/>        sumyy += y*y<br/>        sumxy += x*y<br/>    return sumxy/math.sqrt(sumxx*sumyy)<br/><br/>v1,v2 = [3, 45, 7, 2], [2, 54, 13, 15]<br/>print(cosine_similarity(v1,v2))</span></pre><p id="0263" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行这个例子，我们可以看到我们得到了相同的结果，证实了我们的手动实现。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="7d9b" class="jp jq hi mo b fi ms mt l mu mv">0.972284251712</span></pre><h1 id="4dad" class="la jq hi bd jr lb lc ld jv le lf lg jz lh li lj kc lk ll lm kf ln lo lp ki lq bi translated">进一步阅读</h1><p id="fb8d" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">如果您想更深入地了解这个主题，本节提供了更多的资源。</p><h1 id="558f" class="la jq hi bd jr lb lc ld jv le lf lg jz lh li lj kc lk ll lm kf ln lo lp ki lq bi translated">书</h1><ul class=""><li id="7601" class="kk kl hi ih b ii km im kn iq ko iu kp iy kq jc mm ks kt ku bi translated"><a class="ae na" href="https://amzn.to/383LSNQ" rel="noopener ugc nofollow" target="_blank">数据挖掘:实用机器学习工具与技术</a>，2016年第4版。</li></ul><h1 id="9c23" class="la jq hi bd jr lb lc ld jv le lf lg jz lh li lj kc lk ll lm kf ln lo lp ki lq bi translated">蜜蜂</h1><ul class=""><li id="bc24" class="kk kl hi ih b ii km im kn iq ko iu kp iy kq jc mm ks kt ku bi translated"><a class="ae na" href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html" rel="noopener ugc nofollow" target="_blank">距离计算(科学空间距离)</a></li><li id="a227" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc mm ks kt ku bi translated"><a class="ae na" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.hamming.html" rel="noopener ugc nofollow" target="_blank">scipy . spatial . distance . hamming API</a>。</li><li id="2af1" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc mm ks kt ku bi translated"><a class="ae na" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html" rel="noopener ugc nofollow" target="_blank"> scipy.spatial.distance .欧几里德API </a>。</li><li id="49d1" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc mm ks kt ku bi translated"><a class="ae na" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cityblock.html" rel="noopener ugc nofollow" target="_blank">scipy . spatial . distance . city block API</a>。</li><li id="62b2" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc mm ks kt ku bi translated"><a class="ae na" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.minkowski_distance.html" rel="noopener ugc nofollow" target="_blank">scipy . space . Minkowski _ distance API</a>。</li></ul><h1 id="6f96" class="la jq hi bd jr lb lc ld jv le lf lg jz lh li lj kc lk ll lm kf ln lo lp ki lq bi translated">文章</h1><ul class=""><li id="2f96" class="kk kl hi ih b ii km im kn iq ko iu kp iy kq jc mm ks kt ku bi translated"><a class="ae na" href="https://en.wikipedia.org/wiki/Instance-based_learning" rel="noopener ugc nofollow" target="_blank">基于实例的学习，维基百科</a>。</li><li id="02f8" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc mm ks kt ku bi translated"><a class="ae na" href="https://en.wikipedia.org/wiki/Hamming_distance" rel="noopener ugc nofollow" target="_blank">海明距离，维基</a>。</li><li id="802d" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc mm ks kt ku bi translated"><a class="ae na" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧几里得距离，维基</a>。</li><li id="daee" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc mm ks kt ku bi translated"><a class="ae na" href="https://en.wikipedia.org/wiki/Taxicab_geometry" rel="noopener ugc nofollow" target="_blank">出租车几何，维基百科</a>。</li><li id="7fc1" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc mm ks kt ku bi translated"><a class="ae na" href="https://en.wikipedia.org/wiki/Minkowski_distance" rel="noopener ugc nofollow" target="_blank">闵可夫斯基距离，维基</a>。</li></ul><h1 id="9db3" class="la jq hi bd jr lb lc ld jv le lf lg jz lh li lj kc lk ll lm kf ln lo lp ki lq bi translated">摘要</h1><p id="4b64" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">在这篇博文中，你发现了机器学习中的距离度量。</p><p id="7eef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">具体来说，您学到了:</p><ul class=""><li id="bced" class="kk kl hi ih b ii ij im in iq lw iu lx iy ly jc mm ks kt ku bi translated">距离度量在机器学习算法中的作用和重要性。</li><li id="5743" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc mm ks kt ku bi translated">如何实现和计算汉明，欧几里德，曼哈顿距离测量，余弦相似性。</li><li id="7da2" class="kk kl hi ih b ii kv im kw iq kx iu ky iy kz jc mm ks kt ku bi translated">如何实现和计算推广欧几里德和曼哈顿距离度量的闵可夫斯基距离。</li></ul><p id="489d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你有什么问题吗？<br/>在下面的评论里提出你的问题，我会尽力回答。</p><p id="dceb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">快乐学习！</p></div></div>    
</body>
</html>