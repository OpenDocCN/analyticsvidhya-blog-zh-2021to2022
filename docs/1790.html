<html>
<head>
<title>BERT for identifying disasters from tweets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特从推特上识别灾难</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bert-for-identifying-disasters-from-tweets-50eeb6844302?source=collection_archive---------6-----------------------#2021-03-18">https://medium.com/analytics-vidhya/bert-for-identifying-disasters-from-tweets-50eeb6844302?source=collection_archive---------6-----------------------#2021-03-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="598f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">詹妮弗·波切内克、约瑟夫·拉森、姚世波和杨一凡</p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><p id="0281" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">利用Kaggle竞赛的数据(<a class="ae jj" href="https://www.kaggle.com/c/nlp-getting-started/overview" rel="noopener ugc nofollow" target="_blank">自然语言处理灾难推文</a>)，我们的团队使用BERT模型来预测推文是否包含真实的或非真实的灾难。</p><p id="c822" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">想想真实和非真实灾难之间的区别，比如“主街上的房子着火了，一家三口都死了”和“我的家人围在火堆旁真好”，或者“瓦斯爆炸造成数千人损失，没有人受伤”和“不要错过比特币爆炸！”作为人类，我们可以很容易地解读每一句话背后的情绪和意图，并识别出哪些是真正的灾难，哪些不是，但计算机在这方面不太熟练。</p><p id="bf60" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然我们在检查这个数据集的过程中创建了许多新的变量(这是将来另一篇媒体文章的主题)，但选择的BERT模型只使用了干净的tweet文本。下面提供的文字云显示了真实和非真实灾难中经过清理的推文文本。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jk"><img src="../Images/1e863021792ad1dbb4f21412251ca822.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cRZ7Xx-XILZhXCDJ530LXA.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">图一。真实灾难的文本(仅限训练集)</figcaption></figure><p id="af8b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">两个词云(顶部和底部)使用相同的种子制作，因此相似频率的词将以相同的位置、大小和颜色显示，以方便两者之间的比较。正如我们在顶部看到的，最常见的词是“新”，在底部的图像中也是如此。此后结果出现分歧。这些差异是任何使用的机器学习或深度学习都需要挑选和识别的部分。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jk"><img src="../Images/1420d098baea6f037dcb7038b93ad309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0au4inCg0ZFWA1MGZaPI2A.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">图二。非真实灾难推文</figcaption></figure><p id="7abd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">BERT是如何工作的，为什么它在NLP中如此出色，这可以通过专门的文章得到最好的解释(参见<a class="ae jj" href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" rel="noopener" target="_blank"> Horev </a>、<a class="ae jj" href="https://towardsdatascience.com/bert-why-its-been-revolutionizing-nlp-5d1bcae76a13" rel="noopener" target="_blank"> Wei </a>或<a class="ae jj" href="https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/" rel="noopener ugc nofollow" target="_blank"> Rizvi </a>中一些精彩的解释，我们也参考这些解释来更好地理解和编程我们的BERT模型)。我们选择使用BERT uncased英语模型，该模型位于:<a class="ae jj" href="https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1" rel="noopener ugc nofollow" target="_blank">https://tfhub . dev/tensor flow/BERT _ en _ un cased _ L-24 _ H-1024 _ A-16/1</a></p><p id="5533" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">自从使用它以来，模型已经更新，但是我们将继续使用链接中的那个来匹配我们的初始结果。该模型使用24个隐藏层，隐藏大小为1024，有16个注意头。这个模型是在维基百科和图书语料库上为英语预先训练的。输入类型是不区分大小写的，这意味着在标记化之前文本必须被设置为小写，并且任何重音符号都必须被去除，以便正确地利用这个模型。如果您的输入混合了大写和小写，或者如果重音符号很重要，那么还有其他预先训练好的BERT模型可供您使用。</p><p id="6cdf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然我们也使用NLTK对文本进行了标记化，但我们选择对未标记的文本使用BERT的标记化器，以更好地匹配预训练模型中的语言。这里有tokenizer:<a class="ae jj" href="https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py" rel="noopener ugc nofollow" target="_blank">https://raw . githubusercontent . com/tensor flow/models/master/official/NLP/Bert/tokenization . py</a></p><p id="bda9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中一个步骤是构建我们的助手函数:</p><pre class="jl jm jn jo fd ka kb kc kd aw ke bi"><span id="536e" class="kf kg hh kb b fi kh ki l kj kk">def bert_encode(texts, tokenizer, max_len=512):<br/>  all_tokens = [] <br/>  all_masks = []<br/>  all_segments = []<br/>  for text in texts:<br/>     text = tokenizer.tokenize(text)<br/>     text = text[:max_len-2]<br/>     input_sequence = ["[CLS]"] + text + ["[SEP]"]<br/>     pad_len = max_len - len(input_sequence)<br/>     tokens = tokenizer.convert_tokens_to_ids(input_sequence)<br/>     tokens += [0] * pad_len<br/>     pad_masks = [1] * len(input_sequence) + [0] * pad_len<br/>     segment_ids = [0] * max_len<br/>     all_tokens.append(tokens)<br/>     all_masks.append(pad_masks)<br/>     all_segments.append(segment_ids)<br/>  return np.array(all_tokens), np.array(all_masks), np.array(all_segments)</span><span id="3b35" class="kf kg hh kb b fi kl ki l kj kk">def build_model(bert_layer, max_len=512):<br/>  input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")<br/>   input_mask = Input(shape=(max_len,), dtype=tf.int32, name="input_mask")<br/>   segment_ids = Input(shape=(max_len,), dtype=tf.int32, name="segment_ids")<br/>   _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])<br/>   clf_output = sequence_output[:, 0, :]<br/>   out = Dense(1, activation='sigmoid')(clf_output)<br/>   model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)<br/>   model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])<br/>   return model</span></pre><p id="3f0b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们需要设置我们的层:</p><pre class="jl jm jn jo fd ka kb kc kd aw ke bi"><span id="169d" class="kf kg hh kb b fi kh ki l kj kk">module_url = "https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1"<br/>bert_layer = hub.KerasLayer(module_url, trainable=True)<br/>vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()<br/>do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()<br/>tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)</span></pre><p id="b4ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后建立模型:</p><pre class="jl jm jn jo fd ka kb kc kd aw ke bi"><span id="d3d6" class="kf kg hh kb b fi kh ki l kj kk">model = build_model(bert_layer, max_len=160)<br/>model.summary()</span></pre><p id="c015" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请参见下面的模型摘要。这是一个相当简单的BERT设置。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es km"><img src="../Images/5eb07de3524c5cbfbecdd3ad58cec6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*M6ROf4-OeWXxfi0A4egkTg.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">图3。模型摘要</figcaption></figure><p id="be3f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Colab TPU的总训练时间为3小时，我们确定3个时期是合适的，因为超过3个时期会导致模型过度适应训练集(验证集的性能下降)。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es kn"><img src="../Images/c6951fcc16efe6bf81bd1da3dd105ea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*Ixe3KcRmkfkIt1uS-YhJZA.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">图4。伯特的训练成绩</figcaption></figure><p id="b98c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的最终模型性能如下:</p><ul class=""><li id="aa36" class="ko kp hh ig b ih ii il im ip kq it kr ix ks jb kt ku kv kw bi translated">精确度:0.831</li><li id="78ed" class="ko kp hh ig b ih kx il ky ip kz it la ix lb jb kt ku kv kw bi translated">召回率:0.766</li><li id="81b3" class="ko kp hh ig b ih kx il ky ip kz it la ix lb jb kt ku kv kw bi translated">F1: 0.796</li><li id="d14e" class="ko kp hh ig b ih kx il ky ip kz it la ix lb jb kt ku kv kw bi translated">精度:0.827</li></ul><p id="3c33" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">标准化的混淆矩阵如下。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lc"><img src="../Images/ae559d0d8b72499aa182c16463ceb0c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*t_4QjQtZzE4Z2E16ytf2tg.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">图5。归一化混淆矩阵。</figcaption></figure><p id="b9ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然该模型在预测非灾害方面比实际灾害更好，但它的表现优于在同一数据集上使用的其他模型。总的来说，我们使用了多项式朴素贝叶斯、支持向量机、K近邻、梯度推进和BERT。其中，伯特在预测一条推文是否涉及真实或非真实的灾难方面表现最佳。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ld"><img src="../Images/d1f5289368f9ac89d72b90520b0f41ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*gXohmP9CoghdFSvDXViJ2Q.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">表1。结果我们的ML方法，伯特是最好的</figcaption></figure></div></div>    
</body>
</html>