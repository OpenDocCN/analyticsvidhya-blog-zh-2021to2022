<html>
<head>
<title>NF-Nets : Normalizer Free Nets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NF-网:规格化自由网</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nf-nets-normalizer-free-nets-95648703109a?source=collection_archive---------30-----------------------#2021-03-07">https://medium.com/analytics-vidhya/nf-nets-normalizer-free-nets-95648703109a?source=collection_archive---------30-----------------------#2021-03-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="5afc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">败落为批量正常化的开始？</em></p><h1 id="5bd2" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">介绍</h1><p id="4a2b" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi kg translated"><span class="l kh ki kj bm kk kl km kn ko di"> D </span> eepMind发布了一系列先进的图像分类网络，远远超过了之前的最佳产品——<a class="ae kp" href="https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html" rel="noopener ugc nofollow" target="_blank">efficient net</a>。新架构背后的主要思想是使用无规格化器神经网络或NF-net来训练网络，而不是批量规格化，批量规格化由于其丰富的优点仍然广泛用于几乎所有其他神经网络。<strong class="ig hi">NF-Net-F1的精确度与EfficientNet-B7相当，但训练速度却快了8.7倍！</strong>下面给出的是ImageNet数据集上用于图像分类的最新最先进架构的准确性与延迟的关系图。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es kq"><img src="../Images/d6b4d5ac10deb06b7432980551f96e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/0*zGkALdvhSUrboDl0.jpeg"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">正如我们在这里看到的，NFNet-F0的精确度几乎与EfficientNet-B5相当，并凭借其NFNet-F5全面击败了EfficientNet-B7。图片鸣谢— NFNet <a class="ae kp" href="https://arxiv.org/pdf/2102.06171v1.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure><blockquote class="lc"><p id="bba4" class="ld le hh bd lf lg lh li lj lk ll jb dx translated">将会有一个向神经网络的范式转变，不再使用批处理规范化</p></blockquote><h1 id="200a" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo lm jq jr js ln ju jv jw lo jy jz ka bi translated">背景</h1><p id="f566" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">几乎所有的神经网络，尤其是在计算机视觉-图像分类领域，都严重依赖于批量标准化来训练深度网络。可能是损失的平滑，减少协变量偏移，或者是正则化效应，批处理正则化在训练神经网络中一直具有前所未有的优势，直到现在。这就要改变了！NF-Net的作者表明，我们可以在不进行批量归一化的情况下训练深度残差网络，方法是用一些其他技术代替它，这些技术将导致比启用批量归一化的网络更快的训练和更好的准确性。神经网络的训练速度更快，批量可以达到4096个！但是在深入新的替代方案和技术的细节之前，让我们先通过一个小的批处理规范化演练来设置<strong class="ig hi">上下文</strong>。</p><h1 id="d68b" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">批量标准化</h1><p id="511d" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">简单回顾一下，批量标准化是一种通过标准化每个小批量的输入来训练像ResNet这样的深度网络的方法。观察到一个称为内部协变量转移的问题，这是各层之间输入分布的变化，它看起来像是网络朝着移动的目标进行训练。这是因为神经网络对初始权重和算法非常敏感。批量标准化在初始阶段很大程度上解决了这一问题，它通过计算均值和方差参数来移动输入，从而平滑损失情况并稳定训练。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lp"><img src="../Images/c6d2a3c8fbcc17cec6feb219591579f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4npPWeCwGtvnHcSX.jpeg"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">批量标准化图解。</figcaption></figure><p id="176c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图显示了每个小批量的批量标准化过程中的各个步骤和公式。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es lu"><img src="../Images/c5a59a0ab15f5bc61206e68f78613718.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*GqOEZ30d3qCLiUqb.jpeg"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated"><em class="lv">批量归一化操作中执行的连续步骤——均值、方差、归一化和标度&amp;移动。γ和β参数是神经网络中的可学习参数。图片来自</em> <a class="ae kp" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">论文</em> </a> <em class="lv">。</em></figcaption></figure><h2 id="de8b" class="lw je hh bd jf lx ly lz jj ma mb mc jn ip md me jr it mf mg jv ix mh mi jz mj bi translated">批处理规范化的优点</h2><ol class=""><li id="7825" class="mk ml hh ig b ih kb il kc ip mm it mn ix mo jb mp mq mr ms bi translated">当训练由1000个层组成的深度ResNet(ResNet、ResNeXt等)时，它在<strong class="ig hi">缩小隐藏激活</strong>方面非常成功。怎么会？当在剩余分支上使用时，批量标准化在按比例缩小激活输出方面是有效的。另一方面，当对跳过分支应用批处理规范化时，它会导致对剩余分支的轻微偏向。这确保了初始训练是稳定的，并导致有效的优化。</li><li id="c246" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated"><strong class="ig hi">消除了ReLU &amp; GELU等函数输出非零均值激活时的均值漂移效应</strong>。主要问题是，随着网络越来越深入，平均激活变得越来越大，越来越积极，这导致网络预测所有训练样本的标签相同，训练不稳定。批量标准化通过确保所有层的平均激活为零来解决这一问题。</li><li id="b23e" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">它有很好的正则化效果，在训练神经网络时，它可以代替Dropout作为正则化因子。用批量标准化训练的网络不容易过度拟合，并且对于任何给定的数据样本都概括得很好。这主要是由于将有噪声的批次统计数据标准化。此外，通过调整批次大小可以提高验证的准确性。</li><li id="930e" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">它可以<strong class="ig hi">以更大的批量和更大的学习率训练神经网络</strong>。由于损失前景是平滑的，因此可以使用更大且稳定的学习率来进行训练。对于较小的批量，这可能效率不高。此外，与非批量标准化神经网络相比，它以更少的步骤实现了相同的精度，从而提高了训练速度。</li></ol><h2 id="1ec0" class="lw je hh bd jf lx ly lz jj ma mb mc jn ip md me jr it mf mg jv ix mh mi jz mj bi translated">批处理规范化的缺点</h2><ol class=""><li id="547e" class="mk ml hh ig b ih kb il kc ip mm it mn ix mo jb mp mq mr ms bi translated">在一些网络中，由于计算平均值和缩放参数并存储它们以用于反向支持步骤，因此计算量可能很大。</li><li id="7db4" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">在训练和测试期间，网络行为可能会有差异。在训练时，网络可能已经学习并训练了某些批次，这使得网络依赖于该批次方式的设置。因此，当在推理中只提供一个例子时，它可能表现不好。</li><li id="9e52" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">批处理规范化<strong class="ig hi">打破了批处理中实例之间的独立性。</strong>这意味着在一批中选择的例子是有意义的，并导致我们有两个更多的前景——<strong class="ig hi">批量大小问题</strong>和低效的<strong class="ig hi">分布式训练</strong>，这将导致网络欺骗损失函数。</li><li id="afd8" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">批量大小很重要，因为在小批量中，平均近似值会有噪声，而在大批量中，近似值可能值得考虑。据观察，较大的批量导致稳定和有效的训练。<strong class="ig hi">此外，如果训练时统计数据的方差很大，批量归一化网络的性能也会下降。</strong></li><li id="5859" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">关于欺骗损失函数、困难的分布式训练和在另一个硬件中复制结果的第二点，当我们在并行流中分布训练时，每个流将接收一批的一部分或碎片，并且应用向前传递。当批标准化层之间没有通信时，即所有流将独立计算均值和方差参数时，就会出现差异。这是错误的，因为参数不是对整批都成立，而是对每个碎片都成立，这导致<strong class="ig hi">欺骗</strong>损失函数。</li></ol><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lp"><img src="../Images/6367ef9773c2a6efe9a246c7fa4f21f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fBJP07l8_vvWSkub.png"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">神经网络中出现批量标准化时所需的分布式训练的图示。</figcaption></figure><h1 id="c60b" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">神经网络的背景</h1><p id="5863" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">从以上对利弊的讨论中，我们了解到，尽管批处理规范化在训练深度网络中是有帮助的，但是存在主要的缺点，并且作者提出了走向无批处理规范化的神经网络的新方向的想法。实现这一点的方法是通过抑制剩余分支上的隐藏激活来替换批量范数。这篇[ <a class="ae kp" href="https://arxiv.org/pdf/2101.08692.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的作者通过在初始化时抑制剩余分支并使用<strong class="ig hi">比例权重标准化实现了一个无规格化器的ResNet。</strong>权重标准化在卷积层中单独控制每个输出通道的权重的一阶和二阶矩。权重标准化以可区分的方式标准化权重，其目的是在反向传播期间标准化梯度。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es my"><img src="../Images/ed54696a886acf293565b9b10d36317b.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/0*UaZ2uWb0CUVao3JV.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">称重标准化。</figcaption></figure><blockquote class="lc"><p id="c168" class="ld le hh bd lf lg lh li lj lk ll jb dx translated">其中，μ和σ指权重的均值和标准差，N指卷积层的f-in</p></blockquote><p id="34ba" class="pw-post-body-paragraph ie if hh ig b ih mz ij ik il na in io ip nb ir is it nc iv iw ix nd iz ja jb ha bi translated">这些网络(<strong class="ig hi"> NF-ResNet </strong>)能够匹配批量标准化ResNet的精度，但在批量较大的情况下难以匹配，无法匹配当前最先进的EfficientNet。NF-Nets建立在这项研究工作的基础上。</p><h2 id="7fac" class="lw je hh bd jf lx ly lz jj ma mb mc jn ip md me jr it mf mg jv ix mh mi jz mj bi translated">研究的主要贡献</h2><ol class=""><li id="c38a" class="mk ml hh ig b ih kb il kc ip mm it mn ix mo jb mp mq mr ms bi translated">作者提出了一种新的方法，<strong class="ig hi">自适应梯度裁剪，</strong>，该方法可用于基于梯度范数与参数范数的单位比<strong class="ig hi"><em class="jc"/></strong>来裁剪梯度，从而允许训练具有更大批量和更强数据扩充的神经网络。</li><li id="518e" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">它引入了一个<strong class="ig hi">系列的无规格化器网络</strong>NF-Nets，它超越了之前最先进架构EfficientNets的结果。最大的NF-Net模型在不使用额外数据的情况下实现了86.5%的顶级准确性(最新技术水平)!</li><li id="15f4" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">它表明<strong class="ig hi"> NF-Nets在ImageNet上微调时，在验证准确性方面优于批量归一化网络</strong>。微调后的最高精度为89.2%</li></ol><p id="636f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">让我们了解这篇论文的主要亮点，即自适应梯度裁剪</strong></p><h1 id="bb32" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">自适应渐变裁剪</h1><p id="313e" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">在进入自适应梯度削波或简称ADC之前，什么是梯度削波？渐变裁剪是一种限制渐变值发生巨大变化的方法，无论是正变化还是负变化。简单来说，我们不希望梯度在寻找全局最小值时发生大的跳跃。当梯度值太大时，我们简单地剪掉它。但是，我们还必须适应这样的情况，即梯度必须足够大，以便在穿过损失景观时从局部最小值中出来或修正其路线。如果合成的路径足够好，我们肯定会再次得到它，但另一方面，如果它是一个坏的梯度，我们希望限制它的影响。</p><p id="adaf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者假设，梯度裁剪将使训练的神经网络具有较大的批量和较大的学习率。这是因为以前的语言建模工作T12论文T13稳定了训练，并且它还允许使用更大的学习率，从而加速了训练。标准梯度剪裁T14纸张T15由下式给出:</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es ne"><img src="../Images/8bf99a450305439308a7f406425258ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*Z2pRCUeitvExPGTL.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated"><em class="lv">标准渐变裁剪。</em></figcaption></figure><p id="d586" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中g是梯度向量，G = ∂L/∂θ，其中l表示损失，θ表示所有模型参数的向量。此剪辑在更新θ之前完成。λ是必须调整的限幅阈值超参数。作者观察到，这个超参数对训练的影响非常大(训练稳定性对选择的限幅阈值非常敏感)，因此需要非常精细的调整。为了应对这一点，他们引入了<strong class="ig hi">自适应梯度削波或ADC </strong>。</p><p id="5f62" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">ADC的要点是，它以这个特定的比率(梯度范数G^ <em class="jc"> l </em>与层<em class="jc"> l </em>的权重范数W^ <em class="jc"> l </em>(梯度有多大/梯度作用的权重有多大)来裁剪梯度。这将给出改变原始权重的梯度步长变化的度量。具体地，第<em class="jc"> l- </em>层的梯度的每个单位<em class="jc">I</em>—G(<em class="jc">l)(I)</em>由下式给出:</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es nf"><img src="../Images/b3360b21ea48087d8d07eed2fa29ed37.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/0*XxKSemD24i_6uYHB.png"/></div></figure><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es ng"><img src="../Images/b19a1d704c9b7c0253cded8f9f3b67d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/0*xCrec5wsHUpBoWgB.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">自适应渐变裁剪公式。</figcaption></figure><p id="a5bf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中ε= 10^-3，以防止零初始化的参数被削波回0。随着AGC的使用，它能够以4096的批量稳定地训练神经网络，这对于一批来说是相当大的数量。λ是限幅参数，它的设置取决于优化器、学习率和批处理大小。下图描述了较大批量的NF-net的缩放。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es nh"><img src="../Images/0d0e40f58854fc0e0eceb5267f873d4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/0*mY8q07lqmWPxwmrU.jpeg"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated"><em class="lv">正如我们所看到的，它很好地支持到4096年，但是最大观测精度有它的<br/>批量在2048年左右。图片摘自</em> <a class="ae kp" href="https://arxiv.org/pdf/2102.06171v1.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">论文</em> </a> <em class="lv">。</em></figcaption></figure><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es ni"><img src="../Images/20eb10f0b8cc329e793c361a5572e2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/0*RaSUgaoWT4Ujr1Aw.jpeg"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated"><em class="lv">0.02的限幅阈值保持良好，并给出最大可能的精度。<br/>图片取自</em> <a class="ae kp" href="https://arxiv.org/pdf/2102.06171v1.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">论文</em> </a> <em class="lv">。</em></figcaption></figure><p id="7442" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据经验，我们发现<strong class="ig hi">较低的削波阈值适用于较大的批量。</strong>考虑批量较大的情况，此时批次统计数据没有那么嘈杂。在这里，剪辑必须设置为低，否则它会崩溃。同样，如果批处理大小非常小，批处理统计中可能会有更多的噪声，我们可以使用更高的限幅阈值。</p><h1 id="58e2" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">体系结构</h1><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lp"><img src="../Images/09b547991aa8b1d266e47d20ffaeaa5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AA0XrwYcH8DHkDYR.jpeg"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated"><em class="lv"> NF-Net架构。图片取自</em> <a class="ae kp" href="https://arxiv.org/pdf/2102.06171v1.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">纸</em> </a> <em class="lv">。左图描绘了过渡块，右图描绘了非过渡块。</em></figcaption></figure><p id="2b0d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">说到架构，NF-Net是SE-ResNeXt-D [ <a class="ae kp" href="https://arxiv.org/pdf/1709.01507.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> ]的修改版。该模型的初始“主干”由16通道的3×3步距-2卷积、分别32通道和64通道的两个3×3步距-1卷积以及128通道的最终3×3步距-2卷积组成。这里使用的激活函数是格律[ <a class="ae kp" href="https://arxiv.org/pdf/1606.08415.pdf" rel="noopener ugc nofollow" target="_blank">纸</a> ]，其性能不亚于雷律&amp;路斯。</p><p id="c8c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">剩余阶段由两种类型的块组成，从过渡块开始，接着是非过渡块。所有模块都采用预激活ResNe(X)t瓶颈模式，并在瓶颈内添加3×3分组卷积。在最后一个1×1卷积之后是挤压和激励层，其全局平均汇集激活，将两个线性层与交错缩放的非线性应用于汇集的激活，应用sigmoid，然后通过两倍于该sigmoid的值按通道重新缩放张量。</p><p id="20fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在所有剩余阶段之后，我们应用1×1扩展卷积，使通道数加倍。当使用非常薄的网络时，该层主要是有帮助的，因为通常希望最终激活向量(分类器层接收的)的维数大于或等于类的数量。到了最后一层，它输出一个带有可学习偏差的1000向类向量。该层的权重初始化为0.01，而不是0。</p><p id="fd55" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">NF-Nets的<strong class="ig hi">重要</strong>特征是<strong class="ig hi">“无规格化器”。</strong>残差块主路径的输入乘以1/β，其中β是初始化时该块方差的预测值，该块的输出乘以标量超参数α，通常设置为小值，如α = 0.2。这些标量α和β非常有助于实现无规格化器的实现。下面给出了公式，</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es nj"><img src="../Images/610545bd00e148bdfb5102ea94922433.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/0*EEZeM-WGjB7od9Gf.png"/></div></figure><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es nk"><img src="../Images/25c6f4176061cfc907d375e8795dc556.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/0*Jn4Myx6zlGZWv-tq.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated"><em class="lv">每个阶段开始时的单位信号方差。</em></figcaption></figure><p id="0185" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该模型及其配置相当复杂，如果您正在复制/实现NF-net，通读本文将会有所帮助。这些实验也有点长，为了在这篇博客中保持简洁，我没有包括实验和消融研究。</p><h1 id="9c77" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">基准和结果</h1><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es nl"><img src="../Images/ca9805aeb0d27f28597f509419a41f36.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/0*4_p634ALJY5IQO2q.jpeg"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated"><em class="lv">NF-ResNet和BN-ResNet(批量归一化-ResNet)在各种图像大小(像素)下的比较。图片摘自</em> <a class="ae kp" href="https://arxiv.org/pdf/2102.06171v1.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">论文</em> </a> <em class="lv">。</em></figcaption></figure><p id="80c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们所看到的，NF-Nets优于它们的批处理规范化对应物。<strong class="ig hi">NF-Net-F5模型实现了86%的顶级验证准确率，超过了之前最先进的结果。看，我们有了NF-Net作为新的最先进的网络</strong>，而NF-Net-F1模型与EfficientNet-B7的84.7%的分数相匹配(所有这些都在下表中描述)。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lp"><img src="../Images/97d92e8717bccac924702584268ec06a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Abhg6OdP9tBt6rze.jpeg"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated"><em class="lv">各种架构与训练次数的对比。图片摘自</em> <a class="ae kp" href="https://arxiv.org/pdf/2102.06171v1.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">论文</em> </a> <em class="lv">。</em></figcaption></figure><h1 id="bb2a" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">可能尚未解决的问题</h1><ol class=""><li id="324a" class="mk ml hh ig b ih kb il kc ip mm it mn ix mo jb mp mq mr ms bi translated">如果我们观察<strong class="ig hi">实现</strong>是如何完成的，那么训练示例仍然隐含地依赖于该批次:在该批次的平均操作之后进行削波。</li><li id="5032" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">训练和测试时间中不同行为的观察被引用为批量标准化的问题之一。但是在这里，在实现中，他们使用了Dropout，这在训练和测试时也有不同的行为。</li></ol><h1 id="26e3" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">结论</h1><p id="dca4" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">总之，在这篇论文中有很多东西在进行，但是没有几个对无规范器神经网络的成功开发是非常重要的。引入的NF-Nets的性能超过了最新的图像分类技术(不使用额外的数据)，并且训练速度也更快。还表明NF-Nets家族比批量标准化变体更适合于对大型数据集进行微调。</p><h1 id="f893" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">参考</h1><ol class=""><li id="d508" class="mk ml hh ig b ih kb il kc ip mm it mn ix mo jb mp mq mr ms bi translated">https://arxiv.org/pdf/2102.06171v1.pdf<a class="ae kp" href="https://arxiv.org/pdf/2102.06171v1.pdf" rel="noopener ugc nofollow" target="_blank"/></li><li id="50b7" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">效率网:【https://arxiv.org/pdf/1905.11946.pdf】T4</li><li id="8f2f" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">批量归一化:<a class="ae kp" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1502.03167.pdf</a></li><li id="5baa" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">ImageNet:<a class="ae kp" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank">http://www.image-net.org/</a></li><li id="b8ae" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">雷斯内特:<a class="ae kp" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1512.03385.pdf</a></li><li id="f6e7" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">https://arxiv.org/pdf/1811.03378.pdf</li><li id="cae9" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">葛鲁:<a class="ae kp" href="https://arxiv.org/pdf/1606.08415.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1606.08415.pdf</a></li><li id="bf3a" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">非标准化ResNet:<a class="ae kp" href="https://arxiv.org/pdf/2101.08692.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2101.08692.pdf</a></li><li id="94af" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">规范和优化https://arxiv.org/pdf/1708.02182.pdf的LSTM语言模型:<a class="ae kp" href="https://arxiv.org/pdf/1708.02182.pdf" rel="noopener ugc nofollow" target="_blank"/></li><li id="1ecf" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">渐变裁剪和正则化:<a class="ae kp" href="https://arxiv.org/pdf/1211.5063.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1211.5063.pdf</a></li><li id="b972" class="mk ml hh ig b ih mt il mu ip mv it mw ix mx jb mp mq mr ms bi translated">SE-ResNeXt-D(压缩和激励网络):<a class="ae kp" href="https://arxiv.org/pdf/2102.06171v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2102.06171v1.pdf</a></li></ol></div><div class="ab cl nm nn go no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ha hb hc hd he"><p id="a79b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">原载于2021年3月7日http://appliedsingularity.com</em><a class="ae kp" href="https://appliedsingularity.com/2021/03/07/nf-nets-normalizer-free-nets/" rel="noopener ugc nofollow" target="_blank"><em class="jc"/></a><em class="jc">。</em></p></div></div>    
</body>
</html>