<html>
<head>
<title>Fundamentals Of Machine Learning Part 5- KNN(K Nearest Neighbors)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习基础第5部分- KNN(K近邻)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fundamentals-of-machine-learning-part-5-knn-k-nearest-neighbors-905e66229a0b?source=collection_archive---------20-----------------------#2021-04-05">https://medium.com/analytics-vidhya/fundamentals-of-machine-learning-part-5-knn-k-nearest-neighbors-905e66229a0b?source=collection_archive---------20-----------------------#2021-04-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="cdf8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我们将讨论一个非常著名的机器学习算法，它既用于分类，也用于回归。</p><blockquote class="jc jd je"><p id="3099" class="ie if jf ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated"><strong class="ig hi">‘物以类聚’</strong></p></blockquote><p id="a2b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNN算法基于这样的假设，即具有相似输入的数据将具有相似的输出。KNN算法假设相似的事物存在于附近。换句话说，相似的东西彼此更接近。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es jj"><img src="../Images/de9ac7f3e2a82a4a6a863c8eb4f091bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*ON8e6J-mt_vTqhcwnrC4Qw.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">相似数据点彼此保持接近。</figcaption></figure><p id="6067" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">现在的问题是如何衡量这些点之间的相似性？</strong></p><p id="b837" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有很多方法可以找到它，你可能对其中一些很熟悉。但是用的比较多的是点与点之间的距离。</p><h1 id="6791" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">测距:- </strong></h1><p id="9629" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb ha bi translated">找到相似数据点的基本方法之一是找到它们之间的距离，如果它们很接近，距离会更小，这表明它们属于同一类。相反，如果距离很大，这意味着他们不相似，他们不属于同一类。</p><p id="ce53" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">现在如何测量两点之间的距离？</strong></p><h2 id="b6d1" class="ky jw hh bd jx kz la lb kb lc ld le kf ip lf lg kj it lh li kn ix lj lk kr ll bi translated">欧几里德距离:-</h2><p id="100a" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb ha bi translated">等等，我们以前没用过这个吗？是的，你是对的，我们已经用了很久了。这是最基本和最短的距离查找技术，定义如下:</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es lm"><img src="../Images/23d756be43620c45cf0671abff06ff8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*C7MtAuGthFefQbQGlrUvHA.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">p和q是n维平面中的数据点。</figcaption></figure><p id="fa36" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它对大的差异高度敏感。因为它对具有较大差异的距离求平方，并对计算产生巨大影响。</p><h2 id="aa15" class="ky jw hh bd jx kz la lb kb lc ld le kf ip lf lg kj it lh li kn ix lj lk kr ll bi translated">曼哈顿距离:-</h2><p id="baad" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb ha bi translated">它也被称为<em class="jf">城市街区、出租车距离、蛇形距离或L1范数</em>，它计算点坐标之间的绝对差之和。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es ln"><img src="../Images/bcca68bdad197b59c51078e1f051e132.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*JycSMYyIUqe7HBwzotvn0Q.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">p和q是n维平面中的数据点。</figcaption></figure><h2 id="fe33" class="ky jw hh bd jx kz la lb kb lc ld le kf ip lf lg kj it lh li kn ix lj lk kr ll bi translated"><strong class="ak"> <em class="lo">闵可夫斯基距离:- </em> </strong></h2><p id="c64b" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb ha bi translated">用于找出任意给定两点<strong class="ig hi"> <em class="jf"> p </em> </strong>和<strong class="ig hi"> <em class="jf"> q </em> </strong>之间的距离的通用形式如下所示，也称为<strong class="ig hi"> <em class="jf">【闵可夫斯基距离】</em> </strong>:</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lp"><img src="../Images/1910c3f07a301a49eb779a55a12c5250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_1xUqHFfSbEEzH_KnuQ3gQ.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">p和q是n维平面中的数据点。</figcaption></figure><p id="f9a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种形式看起来不像欧几里德和曼哈顿吗？</p><p id="56de" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以称这个公式为求两点间距离的根公式。欧几里得和曼哈顿是这个根公式的子公式。</p><p id="cca6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">如果a =1那么，叫做曼哈顿距离。</strong></p><p id="32c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">若a =2则称之为欧氏距离。</strong></p><p id="2f2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">若a =∞则称之为切比雪夫距离。</strong></p><h2 id="80be" class="ky jw hh bd jx kz la lb kb lc ld le kf ip lf lg kj it lh li kn ix lj lk kr ll bi translated">切比雪夫距离:-</h2><p id="1e73" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb ha bi translated">对于切比雪夫距离，两点之间的距离是它们沿任意坐标维度的最大差值。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lu"><img src="../Images/4355789847e63d33b7de80c4ec67b94e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*5xj2HGtIJlLjBTx6u_ZMUg.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">p和q是n维平面中的数据点。</figcaption></figure><h1 id="f3c1" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">KNN算法:-</h1><ul class=""><li id="b8f6" class="lv lw hh ig b ih kt il ku ip lx it ly ix lz jb ma mb mc md bi translated">在具有n个特征和不同类别/标签的训练集中加载具有m个示例的数据集。</li><li id="d9e4" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ma mb mc md bi translated">初始化K(超参数)，邻居的数量。</li><li id="581b" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ma mb mc md bi translated">计算测试数据点和训练数据点之间的距离。</li><li id="e079" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ma mb mc md bi translated">将示例的距离和索引添加到有序集合中。</li><li id="cc83" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ma mb mc md bi translated">按距离从小到大(按升序)对距离和索引的有序集合进行排序。</li><li id="5c6d" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ma mb mc md bi translated">从排序的集合中挑选前K个条目。</li><li id="a90d" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ma mb mc md bi translated">获取所选K个条目的标签。</li><li id="577b" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ma mb mc md bi translated">如果是回归，返回K个标签的平均值。</li><li id="43a4" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ma mb mc md bi translated">如果分类，返回K个标签的模式(拥有最多票数的类别)。</li></ul><h1 id="5c2d" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">如何挑选K的最佳值？</h1><ul class=""><li id="8edf" class="lv lw hh ig b ih kt il ku ip lx it ly ix lz jb ma mb mc md bi translated">如果K = 1，则查看邻居并将数据点映射到与邻居相同的类(用于分类)/值(用于回归)。</li><li id="3250" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ma mb mc md bi translated">如果K = 100那么，这将使模型<strong class="ig hi"> <em class="jf">过于一般化</em> </strong>，而<strong class="ig hi"> <em class="jf">偏高</em> </strong>和<strong class="ig hi"> <em class="jf">欠拟合。</em> </strong>测试和训练数据的性能都不好。</li><li id="216e" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ma mb mc md bi translated">如果K = m(数据集中的样本数)，则将所有数据点映射到数据集中最大的类。</li></ul><h2 id="8ba7" class="ky jw hh bd jx kz la lb kb lc ld le kf ip lf lg kj it lh li kn ix lj lk kr ll bi translated">调整参数K:-</h2><ul class=""><li id="07fb" class="lv lw hh ig b ih kt il ku ip lx it ly ix lz jb ma mb mc md bi translated">总是尝试为k选择奇数值。这有助于打破类之间的平局。</li><li id="0378" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ma mb mc md bi translated">尝试根据不同的k值绘制验证数据点的<strong class="ig hi">错误率</strong></li></ul><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mj"><img src="../Images/edeb755170e10ea56d1b7b9e70f7fd5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*abjAL1gxlZhSmwWOCWiZcQ.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">K =3似乎很适合算法。</figcaption></figure><p id="72ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当您绘制不同K值的错误率之间的图表时，您会发现在某个点之后，错误率<strong class="ig hi">将不会显示任何显著下降。</strong></p><ul class=""><li id="ecf6" class="lv lw hh ig b ih ii il im ip mk it ml ix mm jb ma mb mc md bi translated">你也可以试试<strong class="ig hi"> K =floor(sqrt(m)) </strong>其中m是数据集中的例子数。这只是一个神奇的数字，似乎在大多数情况下都有效。</li></ul><h2 id="e182" class="ky jw hh bd jx kz la lb kb lc ld le kf ip lf lg kj it lh li kn ix lj lk kr ll bi translated">重要事项:-</h2><p id="d6a3" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb ha bi translated">在使用KNN之前，确保你的数据集是完全缩放的。</p><h2 id="7d81" class="ky jw hh bd jx kz la lb kb lc ld le kf ip lf lg kj it lh li kn ix lj lk kr ll bi translated">优点:-</h2><ol class=""><li id="64ec" class="lv lw hh ig b ih kt il ku ip lx it ly ix lz jb mn mb mc md bi translated">KNN是一种简单的数据分类方法。</li><li id="5dde" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb mn mb mc md bi translated">KNN既可用于分类，也可用于回归。</li><li id="01af" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb mn mb mc md bi translated">这是一种非参数学习。它不会对基础分布做出任何假设，也不会尝试对其进行估计。</li></ol><h2 id="ee60" class="ky jw hh bd jx kz la lb kb lc ld le kf ip lf lg kj it lh li kn ix lj lk kr ll bi translated">反对意见:-</h2><ol class=""><li id="bfbf" class="lv lw hh ig b ih kt il ku ip lx it ly ix lz jb mn mb mc md bi translated">容易受到异常值或不平衡数据集的影响。</li><li id="f13f" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb mn mb mc md bi translated">计算开销很大。</li><li id="72c2" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb mn mb mc md bi translated">在大型数据集上表现不佳。</li></ol><p id="287e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">够了，让我们编码吧。</p><h1 id="d098" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">实施:-</h1><figure class="jk jl jm jn fd jo"><div class="bz dy l di"><div class="mo mp l"/></div></figure></div></div>    
</body>
</html>