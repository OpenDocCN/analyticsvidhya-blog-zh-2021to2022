<html>
<head>
<title>Paper summary — BERT: Bidirectional Transformers for Language Understanding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文摘要——BERT:语言理解的双向转换器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/paper-summary-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-861456fed1f9?source=collection_archive---------4-----------------------#2021-05-16">https://medium.com/analytics-vidhya/paper-summary-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-861456fed1f9?source=collection_archive---------4-----------------------#2021-05-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="e272" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Jacob Devlin等人的<a class="ae jd" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>论文是在大型NLP模型兴起期间，第一个<a class="ae jd" href="https://openai.com/blog/language-unsupervised/" rel="noopener ugc nofollow" target="_blank"> GPT </a>模型发表后不久的2018年发表的。在发表的时候，它在许多重要的NLP基准上取得了显著的进步，比如GLUE。从那时起，他们的想法影响了许多语言理解的最新模式。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/8de791fb00851a31c3dff222f8c543f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F0vZSDGYGkc-CpkA1pzogQ.png"/></div></div></figure><p id="cbbf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇论文摘要的目的是让你对BERT论文中最重要的观点有一个概述，这样你就可以自己决定是否要阅读所有的细节。</p><h1 id="1728" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">简短的历史</h1><p id="2291" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">2018年是真正的NLP年，与2012年随着AlexNet的发布而成为计算机视觉年非常相似。最初的<a class="ae jd" href="https://openai.com/blog/language-unsupervised/" rel="noopener ugc nofollow" target="_blank"> GPT </a>论文是第一个非常成功地在无监督的情况下预训练大型变压器网络，然后对其进行微调以达到NLP基准的最先进水平的模型。GPT模型由原始变压器的堆叠解码器块组成，他们在一个大型文本数据集上进行预训练，任务是根据前一个单词预测下一个单词。在GPT发表后不久，BERT基于相似的想法发表了他们的模型，他们一起改变了NLP研究的许多方面。</p><h1 id="9c0f" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">为什么伯特与GPT相比是独一无二的？</h1><p id="edc1" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">有两个主要的想法使得伯特论文如此成功。首先，它建立在GPT提出的想法之上，即在一个巨大的文本语料库上预先训练一个转换器模型，然后针对特定的NLP任务对其进行微调。第二个重要的想法是，它使用了双向转换器架构，将原来的<a class="ae jd" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">转换器</a>中的编码器堆叠在一起。</p><p id="f6a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与为特定语言任务训练的传统模型不同，伯特和GPT都在大型文本数据集上半监督地预训练了他们的模型，如维基百科或总字数超过30亿的图书语料库。然后，BERT在带标签的数据集上进行微调，用于NLP任务，如情感分析、问题回答或命名实体识别，并在许多著名的基准测试中远远超过了以前的SOTA。</p><p id="5cb9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在GPT模型中，他们使用原始<a class="ae jd" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">转换器</a>的解码器层来执行自回归任务，其中每个令牌都是从文本中的先前单词生成的。该模型经过预训练，可以在大量文本数据上，根据给定的前一个单词预测下一个单词。在BERT中，他们使用了相同的预训练方法，但他们认为，通过让模型拥有感兴趣的单词之前和之后的上下文，可以大大提高许多NLP任务的性能。因此，他们使用双向变压器来构建他们的模型。他们所说的双向变压器实际上是原始变压器的编码器部分，输入端没有任何屏蔽。</p><h1 id="05ee" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">模型和预培训</h1><p id="7a11" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">BERT由堆叠的变压器编码器组成，他们在论文中为不同的变化提出了不同的层数。然而，重要的是要明白，使用编码器意味着像GPT那样预先训练伯特会导致目标泄漏。如下图所示，GPT架构与输入文本中的未来单词和先前单词没有任何联系。由于BERT使用本质上是双向的编码器-变换器架构，然而，如果我们训练模型来预测输入文本中的下一个模型，将会有来自未来单词的目标泄漏。BERT的作者通过使用两种不同的预训练方案来解决这一问题:训练以预测句子中间的屏蔽词，训练以预测两个输入句子是否连续。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kt"><img src="../Images/fb58479178331032e3f19a5344ac62f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*EjNBVA1W_c4n53LBMpXb0A.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">图来自伯特<a class="ae jd" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">论文</a>。BERT(左)中使用编码器模块，GPT(右)中使用解码器模块。</figcaption></figure><p id="1c2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一种类型的预培训在出版时没有广泛用于变压器。举一个例句:<em class="ky">那边的草更绿</em>，他们会用以下方法之一屏蔽15 %的单词:屏蔽标记(80 %)、随机单词(10 %)或原始单词(10 %)，其中指定的概率是他们发现最成功的。然后，模型的输入可以例如看起来像:<em class="ky">草在另一侧被掩蔽</em>，然后他们将训练模型在其词汇表中正确地分类被掩蔽的单词。</p><h1 id="88f0" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">微调伯特</h1><p id="532e" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">预训练的伟大之处在于，它允许模型使用几乎无限的数据资源来加深对语言结构的理解。尽管训练前的任务本身并不十分有用。相反，GPT和伯特的目的是在特定的语言任务上进行微调。本文将简要描述如何将预训练的BERT模型用于分类、问答和命名实体识别任务，这些任务涵盖了测试BERT的大多数数据集。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kz"><img src="../Images/88a91824a63f6a6f9bcde565a2269118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*Ws-zNi158-cWZfcFPS4Ijg.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">图来自BERT <a class="ae jd" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">论文</a></figcaption></figure><h1 id="c208" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">句子分类任务</h1><p id="de46" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">该任务涉及具有一个或几个输入句子的数据，其中模型应该进行分类，例如情感分析或判断两个句子是否同义。对于这种类型的任务，BERT的输入是结构化的，如上图所示，用SEP-token分隔每个输入句子。在BERT的预训练期间，他们总是在输入的开始使用一个CLS令牌以及一个相应的输出节点，该输出节点意在聚集用于分类任务的输入的上下文。如果您要微调用于句子分类的BERT，那么您将从该节点获取输出，并通过一个线性层将其传递给数据集中的多个类。</p><h1 id="5ead" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">问题回答</h1><p id="5423" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">我一直认为问答的任务是指一个模型可以像聊天机器人一样对一个提出的问题产生一个现实的答案。然而，在这种情况下(以及大多数其他情况下),这意味着给定一个问题和一个包含答案的参考文本，模型将识别参考段落中给出答案的位置的开始和结束索引。为了为此任务训练BERT，您将添加两个线性层，一个用于预测开始索引，另一个用于预测结束索引，并输入来自与段落的输入标记相对应的BERT模型的最后一个编码器块的输出。然后，您将使用交叉熵损失(w. softmax)来训练开始和结束索引的分类。</p><h1 id="eb4f" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">命名实体识别</h1><p id="0c9c" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">NER-任务包括将单词分类，如人，组织，时间等。为了对NER任务进行微调，你可以通过一个线性层将与输入句子中的每个单词相对应的BERT输出映射到类别数。</p><h1 id="0383" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">伯特的局限性</h1><p id="7191" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">在许多方面，BERT可以说是彻底改变了NLP，但是这个体系结构有一些限制。通常，对于自回归任务(在推断期间一次预测一个表征)，没有训练BERT的明确方法，因为它是双向的，并且在训练期间输入目标会导致目标泄漏。为了保持它的细微差别，有论文提出了解决方法，但是BERT架构在这些任务中的任何一个都不是SOTA(据我所知)。</p><p id="3bad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于上面提到的微调任务以及其他NLP分类或理解任务，类似BERT的模型在今天(2021年5月)仍然是一些最先进的模型。</p><h1 id="165c" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">结论</h1><p id="26e1" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">除了展示当时SOTA的结果，伯特还为NLP研究中的许多后续模型铺平了道路。与GPT模型一起，BERT还传播了在极其庞大的数据集上训练大型变压器模型的想法，这个基本上改变了NLP中进行研究的方式。</p><p id="ad71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望这个总结能让你对BERT模型的基本原理有所了解。如果你有什么想法或者你认为我遗漏了什么重要的东西，请留下你的评论！</p></div></div>    
</body>
</html>