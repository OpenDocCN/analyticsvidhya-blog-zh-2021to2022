<html>
<head>
<title>PCA explained by Elon Musk</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">埃隆·马斯克解释的PCA</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pca-explained-by-elon-musk-b019717a60e3?source=collection_archive---------6-----------------------#2021-07-06">https://medium.com/analytics-vidhya/pca-explained-by-elon-musk-b019717a60e3?source=collection_archive---------6-----------------------#2021-07-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/4194c8b1a84873f3c7355c2fbe9d4e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RVAPzkV2EU9XGWpLt2XAkg.jpeg"/></div></div></figure><p id="f8a6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你点击了这篇文章，很可能你要么对机器学习算法着迷，要么像我一样也是埃隆·马斯克的忠实粉丝。不管怎样，我想你会喜欢读这篇文章的。正如我所承诺的，你将会看到Elon解释PCA。现在，就像Elon说的几乎所有其他事情一样，一开始听起来可能很神秘或令人困惑，一旦你理解了他所指的基础和首要原则，事情就会变得更清楚。所以在埃隆解释之前。我们先把Pca的基础知识弄下来。</p><h1 id="63ec" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">什么是PCA？</h1><p id="4390" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">主成分分析(PCA)是一种降维方法，通常用于降低大型数据集的维数，方法是将一个大型变量集转换为一个较小的变量集，该变量集仍包含大型数据集中的大部分信息。减少数据集的变量数量自然会以牺牲准确性为代价，但降维的诀窍是用一点准确性换取简单性。因为较小的数据集更容易探索和可视化，并使机器学习算法分析数据更容易和更快，而无需处理无关变量。主成分分析通过将数据几何投影到更低的维度上来减少数据，这些维度又被称为主成分(PC)。该方法的目标是通过使用最少的主成分找到我们数据的最佳摘要。通过选择我们的主成分，我们最小化原始数据与其在主成分上的投影值之间的距离。作为最小化距离的结果，我们最大化投影点的方差，我们类似地对所有其他主分量这样做，同时这样做我们的主分量不应该与先前的分量相关。通过不相关，我们确保主成分的数量等于数据集的变量或特征的数量，以较小者为准。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kq"><img src="../Images/383a4f1fb160016efb6460a63a21b7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*eAAhbT9TF9R50HM5YkW1CA.gif"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">数据点在主成分上的投影。来源:<a class="ae kz" href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" rel="noopener ugc nofollow" target="_blank">栈交换</a></figcaption></figure><p id="107d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">综上所述，PCA的思想很简单——减少数据集的变量数量，同时保留尽可能多的信息。</strong></p><h1 id="f779" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">警告:数学和代码在前面</h1><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es la"><img src="../Images/304d562f3a9eca3a85bf7ac3230d6ab5.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*Gubct7M7HDwwHTALUPdOuQ.png"/></div></figure><p id="a123" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，如果你对数学或代码方面还不熟悉，可以跳到<a class="ae kz" href="#76a3" rel="noopener ugc nofollow">这部分</a>，这是PCA的更高级别。</p><p id="54a3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">一个例子</strong></p><p id="f6e1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在假设你想预测美国2017年的国内生产总值(GDP)是多少。你有很多可用的信息:2017年第一季度的美国GDP，2016年全年的美国GDP，2015年等等。你有任何公开可用的经济指标，如失业率、通货膨胀率等等。你有2010年的美国人口普查数据，估计每个行业中有多少美国人工作，以及在每次人口普查之间更新这些估计的<a class="ae kz" href="https://www.census.gov/programs-surveys/acs/about.html" rel="noopener ugc nofollow" target="_blank">美国社区调查</a>数据。你知道每个政党有多少名众议员和参议员。你可以收集股票价格数据，一年中发生的<a class="ae kz" href="https://en.wikipedia.org/wiki/Initial_public_offering" rel="noopener ugc nofollow" target="_blank">首次公开募股</a>的数量，以及<a class="ae kz" href="https://www.nytimes.com/2017/03/09/business/bloomberg-iger-business-executives-president.html?_r=0" rel="noopener ugc nofollow" target="_blank">有多少CEO</a><a class="ae kz" href="http://www.latimes.com/business/technology/la-fi-tn-zuckerberg-president-20170120-story.html" rel="noopener ugc nofollow" target="_blank">似乎正在准备竞选公职</a>。尽管有大量的变量需要考虑，但这个<em class="lb">只是触及了表面</em>。</p><p id="5686" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">关键是你要考虑很多可变因素。</p><p id="2948" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你可能会问这样一个问题，“我如何把我收集的所有变量集中在其中的几个上？”用专业术语来说，你想要“减少你的特征空间的维数”通过减少特征空间的维度，您需要考虑的变量之间的关系更少，并且您不太可能过度拟合您的模型。(注意:这并不立即意味着过度拟合等。不再令人担忧，但我们正朝着正确的方向前进！)</p><p id="0caf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">不出所料，<strong class="ir hi"> <em class="lb">降维</em> </strong>特征空间的<strong class="ir hi"><em class="lb"/></strong>称为<strong class="ir hi"> <em class="lb">降维</em> </strong>有许多方法可以实现降维，但这些技术大多属于以下两类之一:</p><ul class=""><li id="8eb6" class="lc ld hh ir b is it iw ix ja le je lf ji lg jm lh li lj lk bi translated">特征消除</li><li id="c8d7" class="lc ld hh ir b is ll iw lm ja ln je lo ji lp jm lh li lj lk bi translated">特征抽出</li></ul><p id="ea42" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">特征消除</strong>顾名思义:我们通过消除特征来减少特征空间。在上面的GDP例子中，除了我们认为最能预测美国国内生产总值的三个变量之外，我们可能会放弃所有变量，而不是考虑每一个变量。特征消除方法的优点包括简单性和维护变量的可解释性。</p><p id="d9ef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然而，作为一个缺点，你不能从你丢弃的变量中获得任何信息。如果我们只使用去年的国内生产总值、根据最新的美国社区调查数据计算的制造业就业人口比例以及失业率来预测今年的国内生产总值，我们就错过了任何可能对我们的模型有所贡献的变量。通过消除功能，我们也消除了这些变量可能带来的任何好处。</p><p id="0a34" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">特征提取</strong>不过，不会碰到这个问题。假设我们有十个独立变量。在特征提取中，我们创建十个“新”独立变量，其中每个“新”独立变量是十个“旧”独立变量的组合。然而，我们以特定的方式创建这些新的自变量，并根据它们预测因变量的程度对这些新变量进行排序。</p><p id="ef48" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你可能会说，“降维在哪里发挥作用？”好吧，我们保留尽可能多的新的独立变量，但我们放弃了“最不重要的”因为我们根据新变量对因变量的预测程度对它们进行了排序，所以我们知道哪个变量最重要，哪个最不重要。但是——这就是问题所在——因为这些新的独立变量是旧变量的组合，所以我们仍然保留旧变量中最有价值的部分，即使我们放弃了一个或多个“新”变量！</p><p id="6089" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">主成分分析是一种用于<em class="lb">特征提取</em>的技术——因此它以一种特定的方式组合我们的输入变量，然后我们可以丢弃“最不重要”的变量，同时仍然保留所有变量中最有价值的部分！<em class="lb">一个额外的好处是，PCA之后的每个“新”变量都是相互独立的。</em>这是一个好处，因为线性模型的<a class="ae kz" href="http://people.duke.edu/~rnau/testing.htm" rel="noopener ugc nofollow" target="_blank">假设要求我们的独立变量相互独立。如果我们决定用这些“新”变量拟合一个线性回归模型(见下面的“主成分回归”)，这个假设必然会得到满足。</a></p><p id="f4a1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们来看看这个过程的数学公式:</p><p id="3e7c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了在k≤n的情况下将数据的维度从n减少到k，我们按照离差递减的顺序对轴列表进行排序，并取其中最高的kk个轴。</p><p id="efba" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们从计算初始特征的离差和协方差开始。这通常用协方差矩阵来完成。根据协方差定义，两个要素的协方差计算如下:</p><p id="3b4d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="lb"> cov(Xi，Xj)= E[(XiμI)(Xjμj)]= E[XiXj]μIμj</em></p><p id="e08d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">其中，μiμi是第I个特性的期望值。值得注意的是，协方差是对称的，向量与自身的协方差等于其离差。</p><p id="2604" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，协方差矩阵是对称的，对角线上对应特征的分散。非对角线值是相应特征对的协方差。就矩阵而言，其中<strong class="ir hi"> X </strong>是观察值的矩阵，协方差矩阵如下:</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/d02e086ffd5c2f5fce437333c0676486.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*94PFczITwUoIHelVmWgScw.png"/></div></figure><p id="0f90" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">快速回顾:矩阵作为线性算子，有特征值和特征向量。它们非常方便，因为它们描述了我们的空间中不旋转的部分，当我们对它们应用线性算子时，它们只会拉伸；特征向量保持相同的方向，但是被相应的特征值拉伸。形式上，一个特征向量为wiwi，特征值为λi的矩阵MM满足这个方程:Mwi=λiwi。</p><p id="ae76" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">样本的协方差矩阵<strong class="ir hi"> X </strong>可以写成<strong class="ir hi">X</strong>T<strong class="ir hi">X</strong>的乘积。根据<a class="ae kz" href="https://en.wikipedia.org/wiki/Rayleigh_quotient" rel="noopener ugc nofollow" target="_blank">瑞利商</a>，我们样本的最大变化沿着这个矩阵的特征向量，并且与最大特征值一致。因此，我们旨在从数据中保留的主成分只是对应于矩阵的前k个最大特征值的特征向量。</p><p id="3ed7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来的步骤更容易消化。我们将数据矩阵<strong class="ir hi"> X </strong>乘以这些分量，得到数据在所选分量的正交基上的投影。如果组件的数量小于初始空间维度，请记住，在应用此转换时，我们将丢失一些信息。</p><p id="25b1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们看看代码中的一个例子。</p><pre class="kr ks kt ku fd lr ls lt lu aw lv bi"><span id="4186" class="lw jo hh ls b fi lx ly l lz ma">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns; sns.set(style='white')<br/>%matplotlib inline<br/>%config InlineBackend.figure_format = 'retina'<br/>from sklearn import decomposition<br/>from sklearn import datasets<br/>from mpl_toolkits.mplot3d import Axes3D<br/><br/><em class="lb"># Loading the dataset</em><br/>iris = datasets.load_iris()<br/>X = iris.data<br/>y = iris.target<br/><br/><em class="lb"># Let's create a beautiful 3d-plot</em><br/>fig = plt.figure(1, figsize=(6, 5))<br/>plt.clf()<br/>ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)<br/><br/>plt.cla()<br/><br/>for name, label <strong class="ls hi">in</strong> [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:<br/>    ax.text3D(X[y == label, 0].mean(),<br/>              X[y == label, 1].mean() + 1.5,<br/>              X[y == label, 2].mean(), name,<br/>              horizontalalignment='center',<br/>              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))<br/><em class="lb"># Change the order of labels, so that they match</em><br/>y_clr = np.choose(y, [1, 2, 0]).astype(np.float)<br/>ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y_clr, <br/>           cmap=plt.cm.nipy_spectral)<br/><br/>ax.w_xaxis.set_ticklabels([])<br/>ax.w_yaxis.set_ticklabels([])<br/>ax.w_zaxis.set_ticklabels([]);</span></pre><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mb"><img src="../Images/5ccfd09d91fc9eaff4e8f136dfaab0b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0xle5YD2YS8RobUO_FWzJA.png"/></div></div></figure><p id="de43" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们看看PCA将如何改进一个简单模型的结果，该模型不能正确地拟合所有的训练数据</p><pre class="kr ks kt ku fd lr ls lt lu aw lv bi"><span id="cff3" class="lw jo hh ls b fi lx ly l lz ma">from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score, roc_auc_score<br/><br/><em class="lb"># Train, test splits</em><br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3,stratify=y,random_state=42)<br/><br/><em class="lb"># Decision trees with depth = 2</em><br/>clf = DecisionTreeClassifier(max_depth=2, random_state=42)<br/>clf.fit(X_train, y_train)<br/>preds = clf.predict_proba(X_test)<br/>print('Accuracy: <strong class="ls hi">{:.5f}</strong>'.format(accuracy_score(y_test, <br/>                                                preds.argmax(axis=1))))</span><span id="bb11" class="lw jo hh ls b fi mc ly l lz ma"><strong class="ls hi">Accuracy: 0.88889</strong></span></pre><p id="7cb4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们再试一次，但是，这一次，让我们把维度减少到2维:</p><pre class="kr ks kt ku fd lr ls lt lu aw lv bi"><span id="0e1b" class="lw jo hh ls b fi lx ly l lz ma"><em class="lb"># Using PCA from sklearn PCA</em><br/>pca = decomposition.PCA(n_components=2)<br/>X_centered = X - X.mean(axis=0)<br/>pca.fit(X_centered)<br/>X_pca = pca.transform(X_centered)<br/><br/><em class="lb"># Plotting the results of PCA</em><br/>plt.plot(X_pca[y == 0, 0], X_pca[y == 0, 1], 'bo', label='Setosa')<br/>plt.plot(X_pca[y == 1, 0], X_pca[y == 1, 1], 'go', label='Versicolour')<br/>plt.plot(X_pca[y == 2, 0], X_pca[y == 2, 1], 'ro', label='Virginica')<br/>plt.legend(loc=0);</span></pre><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es md"><img src="../Images/67d889ff3c9afeffa305bd07c9fd0548.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3cWOE5OZpyf60E82Nk1HPg.png"/></div></div></figure><pre class="kr ks kt ku fd lr ls lt lu aw lv bi"><span id="7417" class="lw jo hh ls b fi lx ly l lz ma"><em class="lb"># Test-train split and apply PCA</em><br/>X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=.3, <br/>                                                    stratify=y, <br/>                                                    random_state=42)<br/><br/>clf = DecisionTreeClassifier(max_depth=2, random_state=42)<br/>clf.fit(X_train, y_train)<br/>preds = clf.predict_proba(X_test)<br/>print('Accuracy: <strong class="ls hi">{:.5f}</strong>'.format(accuracy_score(y_test, <br/>                                                preds.argmax(axis=1))))</span><span id="7385" class="lw jo hh ls b fi mc ly l lz ma">Accuracy: 0.91111</span><span id="2181" class="lw jo hh ls b fi mc ly l lz ma">for i, component <strong class="ls hi">in</strong> enumerate(pca.components_):<br/>    print("<strong class="ls hi">{}</strong> component: <strong class="ls hi">{}% o</strong>f initial variance".format(i + 1, <br/>          round(100 * pca.explained_variance_ratio_[i], 2)))<br/>    print(" + ".join("<strong class="ls hi">%.3f</strong> x <strong class="ls hi">%s</strong>" % (value, name)<br/>                     for value, name <strong class="ls hi">in</strong> zip(component,<br/>                                            iris.feature_names)))</span></pre><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es me"><img src="../Images/ddc3b892c129354945230fd217d83b06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LzORqMsiN6iGpFW86D78bQ.png"/></div></div></figure><p id="76a3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">激动人心的部分</strong></p><p id="5092" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">既然你已经了解了PCA是什么，让我们来谈谈大脑是如何执行PCA的，或者至少是它的一种变体。我们的感官每天都在接收如此多的信息。就像如果我们认为我们的大脑是一个处理单元，如果我们没有一种过滤器来处理它，我们每天接受的信息量就会太大而难以理解。这就是为什么我相信大脑执行一种降维技术，使我们能够处理它认为对我们有用的信息，这些信息反过来是我们感官接收的信息的一小部分，这在某种程度上似乎类似于主成分算法所做的。</p><p id="1c5f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">埃隆详细解释的时间到了</strong></p><p id="a468" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在视频中，Elon解释了人工神经网络如何使用PCA等压缩算法来降低数据集的维度，以将生成的数据集应用于机器学习算法，从而在训练算法的同时减少计算时间。他用我们的大脑将现实还原为一个向量空间或一种与我们相关的形式的过程作为类比，让我们能够执行日常决策，而不会被我们感官每天遇到的其他事情所困扰。</p><figure class="kr ks kt ku fd ii"><div class="bz dy l di"><div class="mf mg l"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">埃隆解释的PCA</figcaption></figure></div></div>    
</body>
</html>