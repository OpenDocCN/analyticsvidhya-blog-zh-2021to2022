<html>
<head>
<title>Overview of loss functions for Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习损失函数综述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/overview-of-loss-functions-for-machine-learning-61829095fa8a?source=collection_archive---------8-----------------------#2021-02-17">https://medium.com/analytics-vidhya/overview-of-loss-functions-for-machine-learning-61829095fa8a?source=collection_archive---------8-----------------------#2021-02-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="bd16" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">机器学习本质上是一个优化问题。对于任何优化问题，我们都需要计算我们的预测离事实有多远的方法，以确定我们需要模型朝哪个方向改变。</p><p id="8996" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们通过最小化模型的损失和成本函数来做到这一点。</p><p id="9fa0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">损失函数获取预测值，并通过输出误差度量将预测值与实际值或数据标签进行比较。这个误差决定了神经元的权重应该如何变化。成本函数是整个数据和预测样本的平均损失。</p><p id="2bc6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">分类问题和回归问题由于其输出的性质而具有不同类型的损失函数。分类问题是模型必须正确地将输入标注为一个类。回归函数预测数字。基于这两个示例，没有任何损失函数适用于每种类型的输入数据。</p><h1 id="e026" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">回归损失</h1><p id="7b34" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated"><strong class="ig hi"> MSE(均方误差/平方误差损失)</strong></p><p id="864e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">也称为机器学习中的L2(岭)损失</p><p id="f13a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">就像它的名字一样，它是实际值和预测值之间距离的平方之和。线性回归使用MSE。</p><p id="bba6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果数据容易出现异常值，MSE可能不是合适的选择，因为对大的异常值求平方会使损失非常大。不成比例的大数字会影响梯度下降的稳定性。</p><p id="484d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">MSE也不适合逻辑回归，因为数据可能不是完全凸的。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kf"><img src="../Images/0be9edcc80e3167d9db40bbb698b2226.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pJlGspC71HaM-8IA"/></div></div></figure><p id="3e6c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> MAE(平均绝对误差)</strong></p><p id="5dff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">——也被称为L1(拉索)损失</p><p id="a57b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它是预测值和实际值之间距离的绝对值。</p><p id="59c9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它也比MSE更稳健，但在其他数学方程中使用绝对值并不容易。(这会使导数变得复杂)</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es kr"><img src="../Images/83c4d076209f4bfdd31918454db8c502.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/0*eR4OGGMpm9bL_cx1.gif"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">平方损失与绝对误差损失</figcaption></figure><p id="76c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如该图所示，当误差低于1时，MSE梯度小于MAE梯度。</p><p id="2614" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">胡伯损失/平滑平均绝对误差</strong></p><p id="3d37" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">组合MSE和MAE，以便函数对小误差进行平方运算，对大误差进行线性运算(偏移δ)</p><p id="6120" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当δ接近零时，它接近绝对误差，当δ接近无穷大时，它接近均方误差。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kw"><img src="../Images/5ba51f23ac35013453f057542dc16983.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CZl72yWeVBav7id2"/></div></div></figure><p id="abd0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然Huber loss结合了MSE和MAE的最佳方面，但delta成为了另一个需要训练的超参数。</p><p id="5166" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它用于稳健回归、M-估计、加法建模以及分类</p><p id="4d38" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">测井曲线损失</strong></p><p id="8f3e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对数余弦损失的工作原理类似于抵抗异常值的均方误差。与Huber不同，它也是连续二次可微的——这是一些ML模型在计算中所要求的。</p><p id="534b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它很难处理价值巨大的不正确预测。</p><p id="a5f2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">分位数损失</strong></p><p id="8bcc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用预测范围来预测间隔。它不需要恒定方差或正态分布来获得信息预测区间。</p><p id="7f06" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它根据自变量的值估计因变量的分位数。当分位数为第50个百分位时，它成为MAE函数。</p><p id="3596" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据您选择的分位数，它会惩罚高估或低估。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es kx"><img src="../Images/73da548da9477ce9889c4824c862bcb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/0*HLLGPWaEGQI1YuCx"/></div></figure><h1 id="f6d1" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">二元分类损失</h1><p id="1361" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">我们将一个对象分配给两个类中的一个。二元分类的一个例子是将图像标记为包含或不包含猫的图像。</p><p id="9b46" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">二元交叉熵损失(对数损失)</strong></p><p id="8c00" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为我们将输出映射到概率空间，所以我们可以通过模型表示输入在某个类中的概率来测量损失。</p><p id="f627" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用来自P和Q的事件概率计算交叉熵，如下所示:</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es ky"><img src="../Images/7c301f583ffbf3ab8e2494aa196b3f44.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/0*NDcfFoT-L7y_SHT6"/></div></figure><p id="42ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">大熵意味着模型中有更多的不确定性。我们希望最小化这个函数，因为熵度量的是给定分布的不确定性。但是我们不知道我们预测的实际分布——kull back-lei bler散度根据一个假设的分布计算损失。</p><p id="b7e7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">二元交叉熵损失通常与对数损失归为一类。其核心是，它还使用伯努利分布的负对数来解决最大化问题。它在计算上类似于最小化负对数似然，尽管这并不意味着对数损失和交叉熵损失完全相同。</p><p id="0d63" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种损失所使用的激活器通常是Sigmoid函数，因为其输出在[0，1]之间，这很容易分成两个类别→高于0.5是一个类别，低于0.5是另一个类别。</p><p id="51c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">铰链损耗</strong></p><p id="7807" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">铰链损失用于支持向量机，并用-1和1而不是0和1分类。因此，网络的最后一个激活层必须是双曲正切Tanh。</p><p id="175e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个损失函数的作用类似于二元交叉熵，只是它也惩罚了正确标记但不可信的预测。(回归概率低，但只到某一点)</p><p id="54d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是通过计算数据的实际值和预测的线性边界之间的距离来实现的。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es kz"><img src="../Images/0679239187ae2dfc4c2917564c346817.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*24diiBb75oqKZPzl.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">参考文献5中的图片</figcaption></figure><p id="2288" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，一旦预测准确可信，铰链误差为零，否则线性增加。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es la"><img src="../Images/1bd4f0dfbd113c7868df2a9ad41f8c98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/0*MLsHVh2Wk7sKMwzC.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">参考文献5中的图片</figcaption></figure><p id="95b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">直观上，它使用分类符号的差异来计算损失。如果预测值与实际值符号相反，误差会更大。</p><p id="9362" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种损耗还有一种扩展，称为平方铰链损耗和立方铰链损耗。这使误差函数变得平滑，并使计算微分在数值上更容易，尽管它可能不会执行得更好。</p><p id="56a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">尽管有其特点，这种损失并不可靠地执行优于交叉熵。</p><h1 id="fb20" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">多类分类损失</h1><h2 id="f118" class="lb jd hh bd je lc ld le ji lf lg lh jm ip li lj jq it lk ll ju ix lm ln jy lo bi translated">分类/多类交叉熵损失</h2><p id="626e" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">分类交叉熵使用一个热点向量从二进制交叉熵损失中进行归纳。</p><p id="c691" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个类都有一个概率为1的类标签，所有其他类标签的概率为0。</p><p id="2a1a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当有几千或者几十万个标签，没有那么多数据的时候，这就变成了一个稀疏多类交叉熵问题。在这种情况下，为了节省内存，目标变量有时不是为训练而一次性编码的。</p><p id="a75e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它与Softmax activator一起使用——这意味着类不是独立的，因为这个activator的输出不是独立的。</p><h2 id="9b52" class="lb jd hh bd je lc ld le ji lf lg lh jm ip li lj jq it lk ll ju ix lm ln jy lo bi translated">KL-散度</h2><p id="c49f" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">KL散度衡量一个概率分布与另一个分布的不同程度。较低的散度意味着分布更接近实际分布。</p><p id="eff2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它更接近复杂的功能，而不是分类，例如必须学习密集特征的自动编码器。</p><p id="00f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与交叉熵损失相比，它衡量的是两个概率分布之间的相对差异，而不是分布之间的总熵——这就是为什么KL散度有时也被称为<em class="lp">相对熵。</em></p><p id="1a2f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它也用于深度生成模型，如变分自动编码器。(VAE)</p><p id="b952" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">参考资料:</p><ol class=""><li id="95e8" class="lq lr hh ig b ih ii il im ip ls it lt ix lu jb lv lw lx ly bi translated"><a class="ae lz" href="https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0" rel="noopener ugc nofollow" target="_blank">(回归函数)https://heart beat . fritz . ai/5-Regression-loss-Functions-all-machine-learners-should-know-4fb 140 e 9 D4 b 0</a></li><li id="024c" class="lq lr hh ig b ih ma il mb ip mc it md ix me jb lv lw lx ly bi translated"><a class="ae lz" href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a" rel="noopener" target="_blank">(交叉熵损失/KL)https://towards data science . com/understanding-binary-Cross-Entropy-log-Loss-a-visual-explain-a3ac 6025181 a</a></li><li id="051d" class="lq lr hh ig b ih ma il mb ip mc it md ix me jb lv lw lx ly bi translated"><a class="ae lz" href="https://liyanxu.blog/2018/10/28/review-of-cross-entropy/" rel="noopener ugc nofollow" target="_blank">https://liyanxu.blog/2018/10/28/review-of-cross-entropy/</a></li><li id="f9ca" class="lq lr hh ig b ih ma il mb ip mc it md ix me jb lv lw lx ly bi translated"><a class="ae lz" href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/cross-entropy-for-machine-learning/</a></li><li id="e89c" class="lq lr hh ig b ih ma il mb ip mc it md ix me jb lv lw lx ly bi translated">(铰链损耗)<a class="ae lz" href="https://math.stackexchange.com/questions/782586/how-do-you-minimize-hinge-loss" rel="noopener ugc nofollow" target="_blank">https://math . stack exchange . com/questions/782586/how-do-you-minimize-Hinge-Loss</a></li></ol></div></div>    
</body>
</html>