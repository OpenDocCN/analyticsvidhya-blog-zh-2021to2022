<html>
<head>
<title>Web Scrapping: The Art of Collecting Real World Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">网络报废:收集真实世界数据的艺术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/web-scrapping-the-art-of-collecting-real-world-data-868b7e356299?source=collection_archive---------24-----------------------#2021-04-18">https://medium.com/analytics-vidhya/web-scrapping-the-art-of-collecting-real-world-data-868b7e356299?source=collection_archive---------24-----------------------#2021-04-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/d9dc4c93682accb90d4c04fbafd6acd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dANo96D2pXoSf7uL.jpg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://www.google.com/search?q=scrapping&amp;sxsrf=ALeKk03Iyf5CRIkhbBOk_O83uV3ScVX7tg:1616766095572&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwjg0r6Gi87vAhXjwjgGHZecD0gQ_AUoAXoECAEQAw&amp;biw=1366&amp;bih=625#imgrc=FjYOG8MKEAfKIM" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><blockquote class="iu iv iw"><p id="a97d" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hi">什么是Web报废？</strong></p></blockquote><p id="c7c7" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">在客户端和服务器(网站)之间建立连接以解析特定网站的数据的过程称为web报废。假设你是一名NLP工程师，为一家特定的公司工作，该公司希望了解客户对他们在市场上提供的产品的看法，为此他们没有给你csv、excel或json格式的数据，而是给你发布评论的网站的身份验证。在这种情况下，你必须使用web scrapping从web中提取数据，然后根据你的方便将所有这些数据保存到数据库或csv文件格式中，以便进一步操作。</p><p id="6de5" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">换句话说，我们可以说web报废是一个从第三方来源提取信息的前数据科学过程。作为一名优秀的数据科学家，我们应该了解网络报废。</p><p id="1b5c" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">这个博客将帮助你从头开始建立你的自定义评论剪贴簿</p><p id="c5ef" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi">在这个博客中，我们将实施端到端的网络报废项目。让我们开始吧……</strong>………</p><blockquote class="iu iv iw"><p id="3c3f" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hi">请求</strong></p></blockquote><p id="8f77" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">我们可以使用<code class="du jz ka kb kc b">import requests</code>导入请求库。request的主要任务是建立客户端和服务器之间的连接。如果连接将被建立，那么它将在<code class="du jz ka kb kc b">200–299</code>范围内打印<code class="du jz ka kb kc b">status_code</code>输出。假设我想解析来自flipkart网站的信息，如果该网站允许解析相关信息，那么我们将在<code class="du jz ka kb kc b">200–220</code>之间获得状态代码。</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="adcf" class="kl km hh kc b fi kn ko l kp kq">import requests<br/>r=requests.get("https://www.flipkart.com/")<br/>print(r.text)  #print all the textual data present in website<br/>print(r.status_code) #print result between 200-299 if website            <br/>                     #allowed to parse the data<br/>print(r.headers["content-type"]) #print text/html; charset=utf-8<br/>print(r.encoding)      #print utf-8</span></pre><blockquote class="iu iv iw"><p id="5d14" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">美味的汤</p></blockquote><p id="406a" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">Beautiful Soup是一个python库，用于解析html页面中的文本数据。假设我们想要解析关于联想笔记本电脑的所有评论，那么这些事情可以使用beautiful soup来完成。它可以很容易地从<code class="du jz ka kb kc b">bs4</code>包装中导入</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="9ced" class="kl km hh kc b fi kn ko l kp kq">from bs4 import BeautifulSoup as bs</span></pre><blockquote class="iu iv iw"><p id="8365" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hi">美人汤里的</strong> <code class="du jz ka kb kc b"><strong class="ja hi">find</strong></code> <strong class="ja hi">和</strong> <code class="du jz ka kb kc b"><strong class="ja hi">find_all</strong></code> <strong class="ja hi">有什么区别？</strong></p></blockquote><p id="542b" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">假设我们想要删除一个有10个<code class="du jz ka kb kc b"> h5</code>标签的网页，如果我们应用<code class="du jz ka kb kc b">find(”h5")</code>，那么我们将只得到第一个出现的<code class="du jz ka kb kc b">h5</code>标签的数据。但是在<code class="du jz ka kb kc b">find_all(“h5”)</code>中，我们将获得网页中所有<code class="du jz ka kb kc b">h5</code>标签的数据。</p><blockquote class="iu iv iw"><p id="2b9e" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">在下面的任务中，我将使用两个库<strong class="ja hi">请求</strong>和<strong class="ja hi"> urllib </strong></p></blockquote><p id="7e38" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi"> urllib :- </strong>它访问URL地址并试图读取该地址，但是<strong class="ja hi">请求</strong>做两件事首先它读取URL地址，其次它对该URL进行编码</p><h2 id="b266" class="kl km hh bd kr ks kt ku kv kw kx ky kz jw la lb lc jx ld le lf jy lg lh li lj bi translated">任务1:-取消对iphone 7的评论</h2><p id="c1aa" class="pw-post-body-paragraph ix iy hh ja b jb lk jd je jf ll jh ji jw lm jl jm jx ln jp jq jy lo jt ju jv ha bi translated">在这里，我们将为您想要从flipkart中删除的任何产品评论构建一站式解决方案。</p><p id="6d87" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi">步骤1:导入基本库</strong></p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="f2d1" class="kl km hh kc b fi kn ko l kp kq">import requests <br/>from bs4 import BeautifulSoup<br/>from urllib.request import urlopen as uReq<br/>import pandas as pd<br/>import numpy as np</span></pre><p id="819e" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><code class="du jz ka kb kc b">requests</code>这个库将读取URL并对其中的数据进行编码。<code class="du jz ka kb kc b"> uReq</code>将要读取的URL地址。<code class="du jz ka kb kc b">Pandas</code>因为我们将把数据存储成csv格式，因此我们需要熊猫库。<code class="du jz ka kb kc b">bs4</code>用于解析html标签中的所有数据。</p><p id="a9d7" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi">步骤2 :-建立一个自动链接，用于提取用户选择的评论</strong></p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="1c3d" class="kl km hh kc b fi kn ko l kp kq">import requests<br/>from bs4 import BeautifulSoup<br/>from urllib.request import urlopen as uReq<br/>import pandas as pd<br/>import numpy as np<br/>product=input("Enter the product name \n") #enter the product name whose review you want to scrap<br/>url="https://www.flipkart.com/search?q="+product.replace(" ","")<br/>uClient=uReq(url)  #requesting the url<br/>url=uClient.read() #reading the url<br/>uClient.close()    #closing the connection<br/>soup=BeautifulSoup(url,"html.parser") #parsing the html page<br/>data_class=soup.find_all("div",{"class":"_1AtVbE col-12-12"})<br/>data=data_class[2]<br/># here are the following things we are going to scrap<br/>ratings = []<br/>comment_headings = []<br/>user_comments = []<br/>user_names = []<br/>number_of_likes = []<br/>locations = []<br/>for page in range(1,41):    #prininting the reviews of first 30 pages<br/>    url = "https://www.flipkart.com" + data.div.div.div.a["href"].format(page)  # link<br/>    req = requests.get(url)  # setting the connection with given website<br/>    soup_updated = BeautifulSoup(req.text, "html.parser")  # parse the data<br/>    comments = soup_updated.find_all("div", {"class": "col _2wzgFH"})<br/>    # upper code will print all the data present in comment class<br/>    for comment in comments:<br/>        rating = comment.find("div", class_="_3LWZlK _1BLPMq")<br/>        if rating is not None:<br/>            ratings.append(rating.text)<br/>        else:<br/>            ratings.append(np.nan)<br/>        comment_heading = comment.find("p", class_="_2-N8zT")<br/>        if rating is not None:<br/>            comment_headings.append(comment_heading.text)<br/>        else:<br/>            comment_headings.append(np.nan)<br/>        user_comment = comment.find("div", class_="t-ZTKy")<br/>        if user_comment is not None:<br/>            user_comments.append(user_comment.text)<br/>        else:<br/>            user_comments.append(np.nan)<br/>        user_name = comment.find("p", class_="_2sc7ZR _2V5EHH")<br/>        if user_name is not None:<br/>            user_names.append(user_name.text)<br/>        else:<br/>            user_names.append(np.nan)<br/>        location = comment.find("p", class_="_2mcZGG")<br/>        if location is not None:<br/>            locations.append(location.text)<br/>        else:<br/>            locations.append(np.nan)<br/>        likes_count = comment.find("span", class_="_3c3Px5")<br/>        if likes_count is not None:<br/>            number_of_likes.append(likes_count.text)<br/>        else:<br/>            number_of_likes.append(np.nan)<br/>#taking all these scrapped information into dictionary format<br/>dict1={"users":user_names,"ratings":ratings,    "heading":comment_headings,"reviews":user_comments, "location":locations,"likes count":number_of_likes}<br/>#converting all the scrapped data into dataframe<br/>df_scrapped=pd.DataFrame(dict1)<br/>print(df_scrapped.head())<br/>df_scrapped.to_csv("scrapped_df.csv")</span></pre><blockquote class="iu iv iw"><p id="ede5" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hi">为什么我们在将值追加到列表</strong>时使用了 <code class="du jz ka kb kc b"><strong class="ja hi">if else</strong></code> <strong class="ja hi">条件</strong></p></blockquote><p id="9981" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">如果我们不使用if else条件，那么我们将得到<strong class="ja hi">属性错误:- None类型对象不是文本。为了克服这一点，我们应用了<code class="du jz ka kb kc b">if else</code>条件，让我们更准确地理解它。</strong></p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="7876" class="kl km hh kc b fi kn ko l kp kq">if rating is not None:<br/>    ratings.append(rating)</span></pre><p id="72db" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">如果html标签的rating类中不存在任何空值，那么我们将把这些值追加到我们已经获取的<code class="du jz ka kb kc b">ratings</code>列表中。</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="258e" class="kl km hh kc b fi kn ko l kp kq">else:<br/>    ratings.append(np.nan)</span></pre><p id="130c" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">如果在这种情况下我们没有任何评级(html类标签不包含任何值)，我们将使用<code class="du jz ka kb kc b">Nan</code>填充空值。</p><blockquote class="iu iv iw"><p id="b644" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hi">为什么我们要用</strong> <code class="du jz ka kb kc b"><strong class="ja hi">Nan </strong></code> <strong class="ja hi">填充</strong> <code class="du jz ka kb kc b"><strong class="ja hi">None</strong></code> <strong class="ja hi">值为什么我们要额外添加</strong> <code class="du jz ka kb kc b"><strong class="ja hi"> else </strong></code> <strong class="ja hi">块</strong></p></blockquote><p id="a5a5" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">正如我们所知，在构建数据框时，每一列都应该具有相同的大小，否则我们将无法创建数据框，因此我们采用了额外的<code class="du jz ka kb kc b">else </code>块，每当我们获得<code class="du jz ka kb kc b">None</code>值作为结果时，该块将返回<code class="du jz ka kb kc b">Nan</code>值。</p><p id="11f6" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi lp translated"><span class="l lq lr ls bm lt lu lv lw lx di"> N </span>注:——取<code class="du jz ka kb kc b">if else</code>条件总是明智的，它不会在废弃任何一种产品评论时给出任何属性错误。</p><p id="6108" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi">第三步:生成csv文件</strong></p><p id="31c2" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">点击此<a class="ae it" href="https://github.com/Akasonal/Review-Scrapper/blob/main/scrapped_df.csv" rel="noopener ugc nofollow" target="_blank"> <strong class="ja hi">链接</strong> </a> <strong class="ja hi"> </strong>查看报废数据框。</p><p id="c6b6" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi">结论:- </strong></p><p id="6cd5" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">这些都是我的观点，如果你对博客的进一步改进有任何建议，请在下面评论。别走开，我们将一起学习更多的话题。<strong class="ja hi">不断学习不断探索……..</strong></p></div></div>    
</body>
</html>