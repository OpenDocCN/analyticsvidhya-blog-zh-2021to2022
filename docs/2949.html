<html>
<head>
<title>Summarizing NLP Research Papers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP研究论文综述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/summarizing-nlp-research-papers-dbd12965aa0a?source=collection_archive---------9-----------------------#2021-05-27">https://medium.com/analytics-vidhya/summarizing-nlp-research-papers-dbd12965aa0a?source=collection_archive---------9-----------------------#2021-05-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="3d9d" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">NLP研究论文摘要博客和视频的阅读列表</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/04b6af94c4c743bb768156587616c402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cQjah9sV04PIXA1bO1uJWw.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">图片来自<a class="ae jm" href="https://unsplash.com/photos/f2Bi-VBs71M" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="c29f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi kj translated"><span class="l kk kl km bm kn ko kp kq kr di"> T </span>他的博客是一个前哨，目的是组织我到目前为止在媒体上为<strong class="jp hi">解释NLP研究论文</strong>写的所有博客。同样为了便于搜索，我将所有的博客归类到一个共同的高层次主题下。</p><p id="4c3c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在你开始批评我没有在这篇文章中注明原创作者之前😠— <em class="ks">与作者、组织和论文相关的所有信息都出现在各自的博客帖子中</em>。😊<em class="ks">所以</em> <em class="ks">快乐阅读…… </em></p><p id="4035" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> P.S. </strong>我会继续更新这个博客，当我添加更多的研究论文摘要时。<strong class="jp hi"> <em class="ks">(最近更新时间:2021年6月12日)—日/月/年</em> </strong></p><h1 id="20fc" class="kt ku hh bd kv kw kx ky kz la lb lc ld in le io lf iq lg ir lh it li iu lj lk bi translated">到目前为止涵盖的主题…</h1><ol class=""><li id="588e" class="ll lm hh jp b jq ln jt lo jw lp ka lq ke lr ki ls lt lu lv bi translated">设备上NLP <em class="ks"> (1张纸)</em></li><li id="7476" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated">文本相似度<em class="ks"> (2篇论文)</em></li><li id="1fcc" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated">文本摘要<em class="ks"> (5篇论文)</em></li><li id="63ad" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated">关键词提取<em class="ks"> (11篇论文)</em></li><li id="699f" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated">查询扩展<em class="ks"> (2篇论文)</em></li><li id="28db" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated">聊天机器人和对话系统<em class="ks"> (1篇论文)</em></li><li id="d4da" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated">NLP中的数据扩充<em class="ks"> (3篇论文)</em></li><li id="b8fb" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated">问题解答<em class="ks"> (1篇论文)</em></li><li id="35e4" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated">其他<em class="ks"> (6篇论文)</em></li></ol><p id="5d26" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">论文总数= 32 ……还有很多要走……</strong></p></div><div class="ab cl mb mc go md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ha hb hc hd he"><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mi"><img src="../Images/d71f06711dad928a4a767fb60d77856f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_OHqK6modnfEvOhWRDTZaw.png"/></div></div></figure><p id="0acd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">设备上的自然语言处理</strong>指的是经过优化的小型精确自然语言处理模型，可以直接部署在移动设备上。<em class="ks">下面是我迄今为止总结的一些博客</em></p><ol class=""><li id="f798" class="ll lm hh jp b jq jr jt ju jw mj ka mk ke ml ki ls lt lu lv bi translated"><em class="ks">高效系统用于</em> <strong class="jp hi"> <em class="ks">移动设备上的</em></strong><em class="ks"/><strong class="jp hi"/>(<a class="ae jm" rel="noopener" href="/mlearning-ai/efficient-system-for-grammar-error-correction-on-mobile-devices-3a207105b7a3">博客</a> / <a class="ae jm" href="https://www.youtube.com/watch?time_continue=343&amp;v=3rVn14m8zaM" rel="noopener ugc nofollow" target="_blank">视频</a>)</li></ol><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mm"><img src="../Images/f81b964b27d102620833b68b93b575d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*neNUHTZL2I3NlEcOXd80lQ.png"/></div></div></figure><p id="bd95" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">文本相似度</strong>的任务是测量任意两个文本片段之间的<strong class="jp hi">接近度</strong> <strong class="jp hi">值</strong>。它们可以在句法层次<strong class="jp hi"/><em class="ks">(面向句法)</em><strong class="jp hi">以及语义层次</strong> <em class="ks">(面向意义)进行计算。以下是我迄今为止总结的一些博客— </em></p><ol class=""><li id="ec61" class="ll lm hh jp b jq jr jt ju jw mj ka mk ke ml ki ls lt lu lv bi translated"><em class="ks">一种基于</em> <strong class="jp hi"> <em class="ks">图的文本相似度</em> </strong> <em class="ks">方法与自然语言处理</em> ( <a class="ae jm" rel="noopener" href="/mlearning-ai/a-graph-based-text-similarity-method-with-named-entity-information-in-nlp-abc7f1201d96">博客</a>)中的命名实体信息</li><li id="ba5e" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><strong class="jp hi"> <em class="ks">基于方面的文档相似度</em> </strong> <em class="ks">使用变形金刚</em> ( <a class="ae jm" href="https://link.medium.com/Vom69zlZMgb" rel="noopener">博客</a>，<a class="ae jm" href="https://www.youtube.com/watch?v=ZO6QWG7-Ye0" rel="noopener ugc nofollow" target="_blank">视频</a>)</li></ol><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mn"><img src="../Images/fc1247b3b1d468363d18d65008748dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1oq-NVLA_lM0BVNTfc4nNg.png"/></div></div></figure><p id="6216" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">文本摘要的任务是缩短一组文档？，创建一个代表原始文档中最重要或最相关信息的子集？。下面是我迄今为止总结的一些博客</p><ol class=""><li id="790b" class="ll lm hh jp b jq jr jt ju jw mj ka mk ke ml ki ls lt lu lv bi translated"><strong class="jp hi"> <em class="ks">飞马</em> </strong> <em class="ks">:利用提取的间隙句进行抽象概括的预训练</em> ( <a class="ae jm" rel="noopener" href="/analytics-vidhya/pegasus-pre-training-with-extracted-gap-sentences-for-abstractive-summarization-acb238aa1096">博客</a> / <a class="ae jm" href="https://www.youtube.com/watch?v=QY8oZxS0txs" rel="noopener ugc nofollow" target="_blank">视频</a>)</li><li id="82ea" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><em class="ks">借力</em> <strong class="jp hi"> <em class="ks">伯特</em> </strong> <em class="ks">对讲座</em> ( <a class="ae jm" rel="noopener" href="/analytics-vidhya/leveraging-bert-for-extractive-text-summarization-on-lectures-294feb643486">博客</a> / <a class="ae jm" href="https://www.youtube.com/watch?v=JU6eSLsp6vI" rel="noopener ugc nofollow" target="_blank">视频</a>)</li><li id="dca8" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><em class="ks">了解</em> <strong class="jp hi"> <em class="ks"> T5型号</em> </strong> <em class="ks">:文字转文字转换变压器型号</em> ( <a class="ae jm" href="https://towardsdatascience.com/understanding-t5-model-text-to-text-transfer-transformer-model-69ce4c165023" rel="noopener" target="_blank">博客</a> / <a class="ae jm" href="https://www.youtube.com/watch?v=91iLu6OOrwk" rel="noopener ugc nofollow" target="_blank">视频</a>)</li><li id="4b32" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><em class="ks">实体级</em> <strong class="jp hi"> <em class="ks">事实一致性</em> </strong> <em class="ks">抽象文本摘要</em> ( <a class="ae jm" href="https://prakhar-mishra.medium.com/entity-level-factual-consistency-in-abstractive-text-summarization-cb19e8a48397" rel="noopener">博客</a> / <a class="ae jm" href="https://www.youtube.com/watch?v=P9wr8IBfDQs" rel="noopener ugc nofollow" target="_blank">视频</a>)</li><li id="bc1a" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><strong class="jp hi"> <em class="ks">多句压缩</em> </strong> <em class="ks">:在Word图中寻找最短路径</em> ( <a class="ae jm" href="https://prakhar-mishra.medium.com/multi-sentence-compression-finding-shortest-paths-in-word-graphs-be02c9065bdc" rel="noopener">博客</a>)</li></ol><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mo"><img src="../Images/f9b566b8fe162fc6e1e0e14aee597b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*52-J0uFegA7hdpp8Hhp8WA.png"/></div></div></figure><p id="bacb" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">关键词提取</strong>是识别最能描述内容的术语/短语的任务。它们可以是提取性的，也可以是抽象的。<em class="ks">以下是我迄今为止总结的一些博客— </em></p><ol class=""><li id="9779" class="ll lm hh jp b jq jr jt ju jw mj ka mk ke ml ki ls lt lu lv bi translated"><strong class="jp hi"> <em class="ks">自然语言处理中十大热门</em> </strong> <em class="ks">关键词提取算法</em> ( <a class="ae jm" rel="noopener" href="/mlearning-ai/10-popular-keyword-extraction-algorithms-in-natural-language-processing-8975ada5750c">博客</a>)</li><li id="7f5f" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><strong class="jp hi"><em class="ks">embebed</em></strong><em class="ks">:使用句子嵌入的简单无监督关键短语提取</em> <strong class="jp hi"> </strong> ( <a class="ae jm" href="https://towardsdatascience.com/embedviz-simple-unsupervised-keyphrase-extraction-using-sentence-embeddings-97ed5e16ad00" rel="noopener" target="_blank">博客</a> / <a class="ae jm" href="https://www.youtube.com/watch?v=ykClwtoLER8" rel="noopener ugc nofollow" target="_blank">视频</a>)</li></ol><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mm"><img src="../Images/139e2d4bb9a8bfe5a32889dca63f0706.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*liIsoA-AG6BLHnzvbYbGPA.png"/></div></div></figure><p id="c97b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">查询扩展</strong>是在信息检索系统中重构给定查询以提高检索性能的<strong class="jp hi"> </strong>过程。下面是我迄今为止总结的一些博客</p><ol class=""><li id="b62c" class="ll lm hh jp b jq jr jt ju jw mj ka mk ke ml ki ls lt lu lv bi translated"><strong class="jp hi"> <em class="ks">伯特-QE: </em> </strong> <em class="ks">用于文档重排序的情境化查询扩展</em> ( <a class="ae jm" rel="noopener" href="/nerd-for-tech/bert-qe-contextualized-query-expansion-for-document-re-ranking-4f0f421840b9">博客</a> / <a class="ae jm" href="https://www.youtube.com/watch?v=WAv6LsIJZbs" rel="noopener ugc nofollow" target="_blank">视频</a>)</li><li id="65eb" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><em class="ks">神经查询扩展为</em> <strong class="jp hi"> <em class="ks">代码搜索</em> </strong> ( <a class="ae jm" href="https://towardsdatascience.com/neural-query-expansion-for-code-search-3d60ebe8b751" rel="noopener" target="_blank">博客</a> / <a class="ae jm" href="https://www.youtube.com/watch?v=QpTZ_-6uio8" rel="noopener ugc nofollow" target="_blank">视频</a>)</li></ol><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mp"><img src="../Images/e05ee7e08df197c8abf0cf51831f6f7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-ahVUfxjZYWjvrQpXM0uQ.png"/></div></div></figure><p id="628b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">会话智能体</strong>是一个用自然语言理解并与人类交流对话的对话系统。下面是我迄今为止总结的一些博客</p><ol class=""><li id="7d82" class="ll lm hh jp b jq jr jt ju jw mj ka mk ke ml ki ls lt lu lv bi translated"><strong class="jp hi"> <em class="ks"> DialoGPT: </em> </strong> <em class="ks">会话回应生成的大规模生成性预训练</em> ( <a class="ae jm" href="https://towardsdatascience.com/dialogpt-large-scale-generative-pre-training-for-conversational-response-generation-5ceb783428dc" rel="noopener" target="_blank">博客</a> / <a class="ae jm" href="https://www.youtube.com/watch?v=Zo679MYoJns" rel="noopener ugc nofollow" target="_blank">视频</a>)</li></ol><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mq"><img src="../Images/b5d5edc83125a7aae2e2cb4123c7a0de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AooWn6CSiQE4Wrf57eYjiQ.png"/></div></div></figure><p id="edd5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">数据扩充</strong>是一种广泛使用的技术，通过添加稍微修改的副本来增加现有数据的数据量。<em class="ks">以下是我迄今为止总结的一些博客— </em></p><ol class=""><li id="3161" class="ll lm hh jp b jq jr jt ju jw mj ka mk ke ml ki ls lt lu lv bi translated"><strong class="jp hi"> <em class="ks"> EDA:简单的数据扩充</em> </strong> <em class="ks">提高文本分类任务性能的技术</em> ( <a class="ae jm" href="https://towardsdatascience.com/eda-easy-data-augmentation-techniques-for-boosting-performance-on-text-classification-tasks-3e61a56d1332" rel="noopener" target="_blank">博客</a> / <a class="ae jm" href="https://www.youtube.com/watch?v=-1unNLkwImw" rel="noopener ugc nofollow" target="_blank">视频</a>)</li><li id="3525" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><em class="ks">使用预先训练好的变压器模型进行数据扩充(</em> <strong class="jp hi"> <em class="ks"> BERT、GPT2等</em> </strong> <em class="ks"> ) </em> ( <a class="ae jm" href="https://www.youtube.com/watch?v=9O9scQb4sNo&amp;list=PLsAqq9lZFOtUg63g_95OuV-R2GhV1UiIZ" rel="noopener ugc nofollow" target="_blank">视频</a>)</li><li id="0fd0" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><em class="ks">通过</em> <strong class="jp hi"> <em class="ks">利用NLP云API，如spaCy、SyntaxNet、WordNet、NMT </em> </strong> <em class="ks"> </em> ( <a class="ae jm" href="https://www.youtube.com/watch?v=6-fPy5j-Uzg&amp;list=PLsAqq9lZFOtUg63g_95OuV-R2GhV1UiIZ" rel="noopener ugc nofollow" target="_blank">视频</a>)简化文本数据增强</li></ol><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mr"><img src="../Images/3ccaccf1b4bec2671d9fc9b5c73cd971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qp7ZInoRETkjdJaDcBIbeg.png"/></div></div></figure><p id="462c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">问题回答(QA) 是关于建立自动回答人类用自然语言提出的问题的系统。下面是我迄今为止总结的一些博客</p><ol class=""><li id="45eb" class="ll lm hh jp b jq jr jt ju jw mj ka mk ke ml ki ls lt lu lv bi translated"><em class="ks">训练</em> <strong class="jp hi"> <em class="ks">问答</em> </strong> <em class="ks">模型来自合成数据</em> ( <a class="ae jm" href="https://prakhar-mishra.medium.com/training-question-answering-models-from-synthetic-data-research-paper-summary-2220186703f" rel="noopener">博客</a> / <a class="ae jm" href="https://youtu.be/_fNYVuFrgP8" rel="noopener ugc nofollow" target="_blank">视频</a>)</li></ol><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ms"><img src="../Images/2383ce4ba2d9b0ab906a307f793a2e25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Td5Zqqjj54QPcEaS8ob7Sg.png"/></div></div></figure><ol class=""><li id="69fd" class="ll lm hh jp b jq jr jt ju jw mj ka mk ke ml ki ls lt lu lv bi translated"><strong class="jp hi"> <em class="ks"> ByT5 </em> </strong> <em class="ks">:用预先训练好的字节到字节模型走向无令牌的未来</em> ( <a class="ae jm" href="https://towardsdatascience.com/byt5-towards-a-token-free-future-with-pre-trained-byte-to-byte-models-3638791a44b2" rel="noopener" target="_blank">博客</a>)</li><li id="9e93" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><em class="ks">深度自然语言处理为</em> <strong class="jp hi"> <em class="ks"> LinkedIn搜索系统</em> </strong> <em class="ks"> </em> ( <a class="ae jm" href="https://prakhar-mishra.medium.com/deep-natural-language-processing-for-linkedin-search-systems-6d136978bcfe" rel="noopener">博客</a> / <a class="ae jm" href="https://www.youtube.com/watch?v=l3O7bCn1JI0" rel="noopener ugc nofollow" target="_blank">视频</a>)</li><li id="f4ac" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><strong class="jp hi"> <em class="ks">预训练、提示、预测</em> </strong> <em class="ks">:自然语言处理中提示方法的系统综述(</em> <a class="ae jm" rel="noopener" href="/mlearning-ai/the-idea-of-prompt-based-learning-in-natural-language-processing-4055c77002fa"> <em class="ks">博客</em> </a> <em class="ks"> / </em> <a class="ae jm" href="https://www.youtube.com/watch?v=K3MasIU25Zw" rel="noopener ugc nofollow" target="_blank"> <em class="ks">视频</em> </a> <em class="ks"> ) </em></li><li id="c442" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><em class="ks">基于情感词典的特征为</em> <strong class="jp hi"> <em class="ks">情感分析</em> </strong> <em class="ks">短文本(</em> <a class="ae jm" rel="noopener" href="/mlearning-ai/features-for-short-text-sentiment-classification-432dc2cb556c"> <em class="ks">博客</em> </a> <em class="ks"> ) </em></li><li id="d469" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><strong class="jp hi"> <em class="ks">无监督的话题分割</em> </strong> <em class="ks">与伯特的会面</em> <a class="ae jm" href="https://www.youtube.com/watch?v=uIdqcGNoI_o" rel="noopener ugc nofollow" target="_blank"> <em class="ks">视频</em> </a> <em class="ks"> / </em> <a class="ae jm" href="https://prakhar-mishra.medium.com/unsupervised-topic-segmentation-of-meetings-with-bert-embeddings-summary-46e1b7369755" rel="noopener"> <em class="ks">博客</em> </a> <em class="ks"> ) </em></li><li id="f539" class="ll lm hh jp b jq lw jt lx jw ly ka lz ke ma ki ls lt lu lv bi translated"><strong class="jp hi"><em class="ks">BERT score</em></strong><em class="ks">:用BERT ( </em> <a class="ae jm" href="https://www.youtube.com/watch?v=Nq4VKXhumSY" rel="noopener ugc nofollow" target="_blank"> <em class="ks">视频</em> </a> <em class="ks"> / </em> <a class="ae jm" href="https://prakhar-mishra.medium.com/bertscore-evaluating-text-generation-with-bert-beb7b3431300" rel="noopener"> <em class="ks">博客</em> </a> <em class="ks"> ) </em></li></ol></div><div class="ab cl mb mc go md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ha hb hc hd he"><blockquote class="mt"><p id="ca1f" class="mu mv hh bd mw mx my mz na nb nc ki dx translated">我希望你喜欢读这篇文章。如果你愿意支持我成为一名作家，可以考虑注册<a class="ae jm" href="https://prakhar-mishra.medium.com/membership" rel="noopener">成为一名媒体会员</a>。每月只需5美元，你就可以无限制地使用Medium</p></blockquote><p id="c89e" class="pw-post-body-paragraph jn jo hh jp b jq nd ii js jt ne il jv jw nf jy jz ka ng kc kd ke nh kg kh ki ha bi translated">非常感谢您的宝贵时间。我希望你觉得这个前哨站有用。请把它分享给任何你认为可能从这个 🥰中受益的人</p></div></div>    
</body>
</html>