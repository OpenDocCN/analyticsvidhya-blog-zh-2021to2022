<html>
<head>
<title>A Quick Start With Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树快速入门</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-quick-start-with-decision-tree-e75487e0696b?source=collection_archive---------13-----------------------#2021-06-07">https://medium.com/analytics-vidhya/a-quick-start-with-decision-tree-e75487e0696b?source=collection_archive---------13-----------------------#2021-06-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="dec7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">决策树</strong>是基于特定条件做出决策的各种方法的图解。这些条件通常是if-then语句，或者我们一般可以说是或否。树越健壮，条件越困难。每种情况都是一个问题，它的解决方案将帮助我们量化问题。</p><p id="c7d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们用一个实时例子来说明这一点</p><ul class=""><li id="22ec" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">假设我们想在某一天打羽毛球，比如周六，你将如何决定是否打。</li><li id="3ad9" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">假设你出去检查天气是热还是冷，检查风速和湿度，天气如何，例如是晴天、多云还是下雨。你要考虑所有这些因素来决定你是否想玩。</li></ul><p id="16d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">树状结构将代表以下属性:</p><ul class=""><li id="326f" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">决策节点:它定义了对单个属性的测试。</li><li id="4c15" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">叶节点:显示目标属性的值。</li><li id="5992" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">边缘:它是一个属性的分裂。</li><li id="8f58" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">路径:这是一个析取的测试，以作出最后的决定。</li></ul><p id="8624" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">节点杂质</strong>是节点内的相关性。如果事例有多个响应值，则节点是不纯的。如果所有实例都具有相同的响应值或目标变量或杂质值= 0，则节点是纯的。</p><p id="551f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是两种最常用的测量节点杂质的方法:</p><ul class=""><li id="c222" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">熵</li><li id="24f9" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">基尼</li></ul><p id="4625" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">熵</strong></p><p id="4077" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在决策树中，熵是一种无序或不确定性。它是对一堆数据中杂质、无序或不确定性的度量。</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="5f68" class="jz ka hh jv b fi kb kc l kd ke">def get_entropy(data):<br/>label_col = data[:, -1]<br/>a, counts = np.unique(label_col, return_counts=True)<br/>prob = counts / counts.sum()<br/>entropy = sum(probabilities * -np.log2(probabilities))</span><span id="0640" class="jz ka hh jv b fi kf kc l kd ke">return entropy</span></pre><p id="62d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">熵的一个简单例子:</p><p id="24ac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设有一个包描述了两种不同的场景:</p><ul class=""><li id="b9c7" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">袋子A有100个绿色的球。彼得想从这个包里选一个绿色的球。这里，袋A的熵为0，因为它意味着0杂质或总纯度。</li><li id="dd78" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">我们用红球替换A袋中的40个绿球，同样，我们用黑球替换10个绿球。现在，约翰想从这个包里选择一个绿色的球。在这种情况下，由于袋子杂质的增加，抽取绿色球的概率将从1.0下降到0.5。</li></ul><p id="7cc6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">信息增益</strong></p><p id="0858" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它是决策树算法建立决策树所接受的主键。决策树将改进以最大化信息增益。具有最高信息增益的属性将首先被测试或分割。</p><p id="473a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">基尼指数</strong></p><p id="16cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">像熵一样，基尼指数也是决策树中计算信息增益的一种标准。决策树使用信息增益来分割节点，而基尼系数测量节点的杂质。基尼系数的范围在0到0.5之间。与熵相比，基尼系数更适合于选择最佳特征。</p><p id="0dcc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">希望它能帮助你理解决策树的基础&amp;与决策树相关的一些其他方法。像熵计算这样的方法越来越少，需要克服的问题也越来越少，比如过度拟合，我会在我即将到来的博客中解释。</p><p id="413e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">快乐学习！</p><p id="ff4c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参考资料:</strong>【https://scikit-learn.org/stable/modules/tree.html】T2</p></div><div class="ab cl kh ki go kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="ha hb hc hd he"><p id="3503" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ko">原载于2021年6月7日https://www.numpyninja.com</em><a class="ae kg" href="https://www.numpyninja.com/post/a-quick-start-with-decision-tree" rel="noopener ugc nofollow" target="_blank"><em class="ko"/></a><em class="ko">。</em></p></div></div>    
</body>
</html>