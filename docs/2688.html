<html>
<head>
<title>Learning Optimization(SGD) Through Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过示例学习优化(SGD)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/learning-optimization-sgd-through-examples-2459f1a0715?source=collection_archive---------22-----------------------#2021-05-12">https://medium.com/analytics-vidhya/learning-optimization-sgd-through-examples-2459f1a0715?source=collection_archive---------22-----------------------#2021-05-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="3527" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/5bcad12bb5b19f9e076374fbf16867e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/0*yDsxdTYy0sH4WBeR.png"/></div></figure><p id="6c71" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">优化的整个目的是最小化<a class="ae ki" href="https://srinivask-bits.medium.com/loss-functions-part-1-17b2601031c1" rel="noopener">成本函数</a>。我们将在本文的后面部分了解更多关于优化的内容。</p><h1 id="a13e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">批量梯度下降</h1><p id="77b2" class="pw-post-body-paragraph jk jl hh jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh ha bi translated">这里，我们在执行权重或参数的更新时，对每次迭代的所有示例进行总结。因此，对于权重的每次更新，我们需要对所有示例求和。基于梯度和学习速率(n)更新权重和偏差。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ko"><img src="../Images/948273dc5a4fc66b9027873247a2b7b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*xRkYaDfkfK-X_ySv.png"/></div></figure><p id="d2fd" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">主要优势在于有一条朝向最小值的直线轨迹，并且在训练期间具有对梯度的无偏估计和固定的学习速率。当我们使用向量实现时是不利的，因为我们必须一次又一次地检查所有的训练集。当我们浏览所有数据时，学习就发生了，即使一些例子是还原剂，对更新没有贡献。</p><h1 id="aa23" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">随机梯度下降</h1><p id="1de5" class="pw-post-body-paragraph jk jl hh jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh ha bi translated">这里不同于分支梯度下降，我们更新每个例子上的参数，因此学习发生在每个例子上。所以它比分批梯度下降收敛得更快，因为权重更新得更频繁。这里，发生了大量的振荡，导致更新具有更高的方差。这些嘈杂的更新有助于找到新的和更好的局部最小值。由于频繁的波动，它将会在期望的精确最小值附近保持超调。</p><p id="b891" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">让我们用例子来理解梯度下降。</p><h1 id="ba76" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">示例1</h1><blockquote class="kp kq kr"><p id="e7b8" class="jk jl ks jm b jn jo jp jq jr js jt ju kt jw jx jy ku ka kb kc kv ke kf kg kh ha bi translated"><strong class="jm hi">将了解:我们将如何达到最小成本点。</strong></p></blockquote><p id="5bc7" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">让我们取一个成本函数f(x)和梯度df(x)给出如下</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kw"><img src="../Images/aad11703054649875768a97728da37ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/0*UKGWqRdazRXVq8im.PNG"/></div></figure><p id="b0d5" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我们的目标是找到最低成本。所以我们来微分，等于0。导数是2x+1。所以最小值出现在x = -0.5。让我们使用下图来理解这一点。x范围从-3到3。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es kx"><img src="../Images/e37c20b809e3da477e16dc31e015460d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/0*bRe-eo5ju75nSkOU.PNG"/></div></div></figure><p id="319e" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我们可以画出曲线和导数。这里我们已经知道二重导数是正的。现在使用梯度下降，我们可以运行500次，如果在50次之前达到最小值，它将停止。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lc"><img src="../Images/d20e19d3f65827a58450a640e424b7b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*EsZuQ37oK3kYNmcT.PNG"/></div></figure><p id="a83d" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我们可以看到下图中的点是如何向最小成本移动的，因为我们从x=-1/2开始，我们得到了最小成本。当我们从x=3开始，学习率为0，我们要求精度为0.0001。在最后一张图中，我们可以看到渐变下降的特写镜头。</p><h1 id="6774" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">示例2</h1><blockquote class="kp kq kr"><p id="b7bc" class="jk jl ks jm b jn jo jp jq jr js jt ju kt jw jx jy ku ka kb kc kv ke kf kg kh ha bi translated"><strong class="jm hi">将了解:如果超过一个极小值会发生什么。</strong></p><p id="6d07" class="jk jl ks jm b jn jo jp jq jr js jt ju kt jw jx jy ku ka kb kc kv ke kf kg kh ha bi translated"><strong class="jm hi">将了解:起点重要吗？。</strong></p><p id="30b6" class="jk jl ks jm b jn jo jp jq jr js jt ju kt jw jx jy ku ka kb kc kv ke kf kg kh ha bi translated"><strong class="jm hi">会了解:学习率有作用吗？。</strong></p><p id="55cd" class="jk jl ks jm b jn jo jp jq jr js jt ju kt jw jx jy ku ka kb kc kv ke kf kg kh ha bi translated"><strong class="jm hi">会了解:梯度下降的限制。</strong></p></blockquote><p id="afab" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">让我们取一个成本函数f(x)和梯度df(x)如下给出。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ld"><img src="../Images/05472e1fbdfc58b80b206be21937b88f.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/0*AV6E6qpnzyBUFM5T.PNG"/></div></figure><p id="c75b" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我们的目标是找到最低成本。让我们使用下图来理解这一点。x范围是从-2到2。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es le"><img src="../Images/13bd2fdda2305958310fafb65615eb9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/0*LVGPMtaXKipCX0TR.PNG"/></div></figure><p id="e0f6" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">从上图中我们可以看到，成本函数有两个最小值。成本函数的斜率在3点处为零。</p><p id="d5a4" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">起点对渐变后代重要吗？</p><p id="c2e5" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">是的。下图显示了我们根据起始位置收敛到不同的最小值。现在让我们从x = 1开始。我们可以看到我们收敛到了正确的最小值。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lf"><img src="../Images/1be734f3f4558ecf5b6559416a957e4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/0*OnChBBT3CPfd-1ei.PNG"/></div></figure><p id="3a8c" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">现在让我们从x = -1开始。我们可以看到我们收敛到左极小值。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es le"><img src="../Images/6c320b2d5d9af04a489d4005df4f0b65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/0*Bbi2soPmNodhSNYT.PNG"/></div></figure><p id="786a" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">通过上面的图，我们可以清楚地说明，我们开始的初始位置决定了我们收敛的最小值。当我们仔细观察上述内容时，我们会发现斜率在3点处为零。当我们从X=0开始时，我们看到我们在第一次迭代时停止，因为X=0处的斜率为0，但实际上，它不是最小值。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lg"><img src="../Images/2708f333bc4a64c9eb5cd260e5340188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/0*z87VgOzP6y8-Vs45.PNG"/></div></figure><p id="a27e" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">另一个限制可以是，如果存在两个最小值，并且当初始点在局部最大值的右侧并且没有发现全局最小值时。</p><p id="3aae" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">学习率对梯度后代重要吗？</p><p id="f634" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我做了一张图来解释学习率对梯度下降法的影响。这张图片解释了不同的学习速度会发生什么。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lh"><img src="../Images/29ba8b45bf54d5b943ea2d33821f4673.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/0*Qjckf1TP6Wx53iBN.png"/></div></figure><p id="4fa4" class="pw-post-body-paragraph jk jl hh jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">粉色线代表最高的学习率，绿色线代表最低的学习率，蓝色线代表介于粉色和绿色之间的学习率。从下图可以看出，高学习率收敛得很快，而低学习率收敛得很慢。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es li"><img src="../Images/459b5c5a7d06e89e829fef0c2edf3332.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/0*pozOk1lhjDP3W3rp.PNG"/></div></figure><h2 id="7af8" class="lj if hh bd ig lk ll lm ik ln lo lp io jv lq lr is jz ls lt iw kd lu lv ja lw bi translated">神经网络梯度下降</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lx"><img src="../Images/27fd838faa2de1b0998c8e9df844949e.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/0*xfASb07An5nuvGVN.jpg"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ly"><img src="../Images/c092f69d528b56967bf2f9a545347e0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/0*1rYgyJcf3W2jDtAZ.jpg"/></div></figure></div></div>    
</body>
</html>