<html>
<head>
<title>Linear Model Features Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性模型特征选择</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-model-features-selection-5c6a62f0f4ad?source=collection_archive---------4-----------------------#2021-07-11">https://medium.com/analytics-vidhya/linear-model-features-selection-5c6a62f0f4ad?source=collection_archive---------4-----------------------#2021-07-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="45ca" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">选择重要的功能</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/49353be0ff3bd04152c71d523e5b85e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AsUNbj8OgNzB-R2CLQHu7g.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">鸣谢:图片来自<a class="ae jm" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2584713" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的格尔德·奥特曼</figcaption></figure><p id="7dfd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">有时，即使是最简单的算法，线性回归也可能会被认为用大量的特征X来预测响应变量y有点过于拥挤。</p><p id="79e2" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">有了高维特征，模型<strong class="jp hi">可能会失去其<em class="kj">可解释性</em> </strong>，并且可能会发现很难解释导致响应变量y变化的特征</p><p id="d1dd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">对于高维特征，模型<strong class="jp hi">可能会因以下任一因素而失去其对测试数据的<em class="kj">预测能力</em></strong>—<em class="kj">特征间的多重共线性</em>、响应变量的<em class="kj">不相关特征</em>。</p><p id="a753" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这就引出了我们使用特征选择的原因。</p><h1 id="fbdf" class="kk kl hh bd km kn ko kp kq kr ks kt ku in kv io kw iq kx ir ky it kz iu la lb bi translated">特征选择</h1><p id="d467" class="pw-post-body-paragraph jn jo hh jp b jq lc ii js jt ld il jv jw le jy jz ka lf kc kd ke lg kg kh ki ha bi translated"><em class="kj">特征选择是从特征池中选择特征子集的过程。</em></p><p id="b9f2" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这是删除不相关和冗余数据的有效方法，从而可以减少计算时间，提高统计效率，提高准确性，并为推理目的提供更好的理解。</p><p id="2bb6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在让我们讨论一下实现这一点的几种方法。</p><h2 id="5742" class="lh kl hh bd km li lj lk kq ll lm ln ku jw lo lp kw ka lq lr ky ke ls lt la lu bi translated">1.最佳子集选择:</h2><p id="cf42" class="pw-post-body-paragraph jn jo hh jp b jq lc ii js jt ld il jv jw le jy jz ka lf kc kd ke lg kg kh ki ha bi translated">这是最基本和计算量最大的特征选择方法。它的工作原理是用X特征的每一种可能的组合来拟合每个模型。</p><p id="5131" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">考虑到有p个特征，这将生成符合每个可能组合的2ᴾ模型的总数。对于P = 2的情况，将有2 = 4个模型<em class="kj"> </em> <strong class="jp hi"> <em class="kj"> (Y = B₀，Y = B₀ + B₁X₁，Y = B₀ + B₂X₂，Y = B₀ + B₁X₁ + B₂X₂).</em>T25】</strong></p><p id="9d98" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们将2ᴾ模型分成具有相同数量特征的模型组，用<strong class="jp hi"> Mᵢ </strong>表示。根据上面的例子，它是这样的——</p><p id="6799" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> Mₒ : <em class="kj"> {Y = B₀} </em> </strong></p><p id="c9fa" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> M₁ : <em class="kj"> {Y = B₀ + B₁X₁，y =b₀+b₂x₂}</em>t35】</strong></p><p id="0371" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">m₂:<em class="kj">{ y =b₀+b₁x₁+b₂x₂}</em>t39】</strong></p><p id="76fd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">第一个过滤器:</strong>现在我们已经将所有的2ᴾ模型分成组(m)，基于RSE或r指标，从每组<strong class="jp hi"> Mᵢ </strong>中选择一个模型。这给了我们总共P个模型。</p><p id="75c9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">第二个过滤器:</strong>在这一步中，从第一个过滤器中获得P个模型后，我们将使用以下指标之一选择一个最佳模型——AIC、BIC、adjusted-R、交叉验证。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lv"><img src="../Images/2ae24619a6bb76aa1a15323b587013dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*weZMxhk5dLCobWGWnq5RbA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated"><strong class="bd km">最佳子集算法</strong>(鸣谢:James，g .，Witten，d .，Hastie，t .，&amp; Tibshirani，R. (2013)。<em class="lw">学习入门</em>(第1版。)[PDF]。斯普林格。)</figcaption></figure><p id="b446" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">此外，从统计学的角度来看，由于随着模型搜索空间的增加，它也增加了找到尽管在训练数据上表现良好但在测试数据上没有足够预测能力的模型的可能性。</p></div><div class="ab cl lx ly go lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ha hb hc hd he"><h2 id="6a40" class="lh kl hh bd km li lj lk kq ll lm ln ku jw lo lp kw ka lq lr ky ke ls lt la lu bi translated">附加内容(关于指标)</h2><p id="ea2f" class="pw-post-body-paragraph jn jo hh jp b jq lc ii js jt ld il jv jw le jy jz ka lf kc kd ke lg kg kh ki ha bi translated">交叉验证是估计测试误差的直接方法。而Cp、AIC、BIC和调整后的R是估计检验误差的间接方法。他们将惩罚加到训练误差上，以调整训练误差低估测试误差的事实。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es me"><img src="../Images/41a43c5f3a4f1b8dc7228a87148dc0b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*jWx6sAYUuri5xStLlCqXqQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">RSS:残差平方和，d:特征数量，σ:与模型相关的误差方差(E)</figcaption></figure><p id="b362" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">RSS是模型解释的差异量。</p><p id="35e4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> Akiake信息标准(AIC) </strong>使用最大似然估计(对数似然)作为拟合度的衡量标准。在具有高斯误差(E)的线性模型的情况下。最大似然法和线性最小二乘法是相同的，AIC评估是使用上述方程完成的。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mf"><img src="../Images/566afb568eae49c35e95ed2be804ad50.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*yTJg-OlZPpF8fDV410KAlA.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">n:数据点数</figcaption></figure><p id="4289" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">贝叶斯信息准则(BIC) </strong>来源于贝叶斯观点。它通常对有许多变量的模型处以更重的惩罚。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mg"><img src="../Images/041e4dc3ecdc8cab07d86bf5010d8098.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*GGpdRtsjAXwxXPWAGn35mw.png"/></div></figure><p id="c7f0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">不像AIC、BIC和C，我们更喜欢低价值的模型。在调整R的情况下，我们更喜欢具有较高值的模型，对原始R的调整是为了考虑增加不必要的变量，这不会增加RSS的实质性减少。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mh"><img src="../Images/c9c0e2dd956314b25436f07b040064cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*FzPo8DlWeA1P36y1cC6nJA.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated"><strong class="bd km"> TSS:总平方误差(Y方差)</strong>(鸣谢:James，g .，Witten，d .，Hastie，t .，&amp; Tibshirani，R. (2013)。统计学习导论(第1版。)[PDF]。斯普林格。)</figcaption></figure></div><div class="ab cl lx ly go lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ha hb hc hd he"><h2 id="9b4b" class="lh kl hh bd km li lj lk kq ll lm ln ku jw lo lp kw ka lq lr ky ke ls lt la lu bi translated">2.逐步选择</h2><p id="e259" class="pw-post-body-paragraph jn jo hh jp b jq lc ii js jt ld il jv jw le jy jz ka lf kc kd ke lg kg kh ki ha bi translated">逐步选择探索了一组限制性很强的模型，因此它可能无法找到最佳解决方案，而只能满足于接近最佳的解决方案。作为交换，它享有计算和统计效率。以下是类型-</p><p id="12a3" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> a)正向逐步选择:</strong></p><p id="cd3e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">它始于一个不含预测因子的模型。此外，我们将有一个所有功能的池。</p><p id="f906" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在每一步中，我们将寻找库中剩余的每一个特性，并(从库中)挑选出在RSS或R指标上表现最好的特性添加到模型中。添加的特征不能从模型中删除。</p><p id="3850" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这种情况会持续下去，直到没有任何特征需要挑选，并且所有特征都已经添加到模型中。</p><p id="c18a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在检查和添加特征的同时，我们在每个步骤存储模型。(所有步骤的总P模型)</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mi"><img src="../Images/c057d47bfb9c995ae378a166684d326f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*3caf9bhkJ0p9Z48E.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated"><strong class="bd km">功能-3被选择，接着是下一步的功能-1</strong></figcaption></figure><p id="f773" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">与最佳子集选择方法相比，我们有2ᴾ模型来选择最佳模型。在这种方法中，我们只有P个模型，每个模型都有不同数量的特征。</p><p id="c445" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">另一个好处是，我们可以将它用于n &lt; p (n : number of datapoints, p : number of features), by considering only top (n-1) features in the model. This could not have attained a unique solution otherwise with n ≤ p using the least square method alone.</p><p id="cfe3" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> b)向后逐步选择:</strong>的情况</p><p id="48ce" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">它的工作方向与正向选择相反。在向前选择中，我们一次添加一个特征，而在向后选择中，我们一次消除一个特征。</p><p id="bc91" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">已删除的功能无法重新添加。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mi"><img src="../Images/6815f60e7ba06fea156e6911559bdd0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*2DxY4uMEbpPRW-zx.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated"><strong class="bd km">从所有特征开始，我们在每一步消除一个特征</strong></figcaption></figure><p id="992f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在计算方面，它几乎与向前消去法相同，并且都比子集选择快得多。</p><p id="1faf" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> c)混合逐步方法:</strong></p><p id="ebb6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在这种方法中，我们使用向前和向后两种方法。</p><p id="d1bd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在使用前向选择时，如果我们添加了一个后来证明没有意义的特性，我们可以使用后向选择将其删除。或者，如果我们删除了一个后来被证明是有用的特征，可以再添加回来。</p><p id="1704" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">因此，该方法在选择最佳特征方面挑战了子集选择方法，同时保留了向前和向后方法的计算优势。</p></div><div class="ab cl lx ly go lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ha hb hc hd he"><h2 id="fdd0" class="lh kl hh bd km li lj lk kq ll lm ln ku jw lo lp kw ka lq lr ky ke ls lt la lu bi translated">3.收缩方法</h2><p id="c26c" class="pw-post-body-paragraph jn jo hh jp b jq lc ii js jt ld il jv jw le jy jz ka lf kc kd ke lg kg kh ki ha bi translated">上述方法集中于选择特征子集以提高准确性。该方法考虑了所有的特征，并应用了<em class="kj">约束</em>或<em class="kj">正则化</em>，将特征系数向零收缩，因为收缩系数减少了方差。</p><p id="a552" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">收缩法可能比上述子集选择和逐步选择更快。对于(λ)的固定值，它只需要适合单个模型，而在逐步的情况下，我们需要P(特征数量)模型。</p><p id="c558" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">然而在实践中，我们需要使用交叉验证来训练和检查(λ)的多个值。</p><p id="a204" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> a)岭回归(L正则化):</strong></p><p id="8bb7" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">该方法将范数惩罚加入到早期的代价函数(RSS)中。损失是系数平方的总和乘以拉格朗日系数(λ)。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mj"><img src="../Images/8e4864b02c74699409b13be8a52543de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*M9v79jIunmfLzYWHg-N1PQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">β:特征系数</figcaption></figure><blockquote class="mk ml mm"><p id="d775" class="jn jo kj jp b jq jr ii js jt ju il jv mn jx jy jz mo kb kc kd mp kf kg kh ki ha bi translated">这种方法将特征系数收缩到零，原因在于受到的约束——β₁+β₂≤s</p></blockquote><p id="3981" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">岭回归可以在贝叶斯意义上解释，其中惩罚项可以通过将参数上的<em class="kj">正态分布</em>视为<em class="kj">先验概率来导出。</em></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mq"><img src="../Images/97c9d1b9ab5b0e5d2d830bcde875b6a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*4Rss9YX-GvH4wdFaU9bP5w.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">岭回归(图片取自“统计学习的要素”)</figcaption></figure><blockquote class="mr"><p id="cbb0" class="ms mt hh bd mu mv mw mx my mz na ki dx translated">如图所示，椭圆表示成本函数(RSS)的轮廓，圆形表示范数约束。它们相交的点不在任何轴上。</p></blockquote><p id="1e12" class="pw-post-body-paragraph jn jo hh jp b jq nb ii js jt nc il jv jw nd jy jz ka ne kc kd ke nf kg kh ki ha bi translated"><strong class="jp hi"> b)套索回归(L正则化):</strong></p><p id="14e4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">该方法还将范数惩罚添加到早期成本函数(RSS)中。损失是系数绝对值的总和，乘以拉格朗日系数(λ)。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ng"><img src="../Images/6aaf7d28e49817e111c346bc501ddbe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*U0O_us-b3az-PfO7UYH5_Q.png"/></div></figure><blockquote class="mk ml mm"><p id="7f2b" class="jn jo kj jp b jq jr ii js jt ju il jv mn jx jy jz mo kb kc kd mp kf kg kh ki ha bi translated">套索回归可能将系数缩小到零，原因还是在于受到的约束——|β₁|+|β₂|≤s</p></blockquote><p id="998b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">套索回归可以在贝叶斯意义上解释，其中惩罚项可以通过将参数上的<em class="kj">拉普拉斯分布</em>视为<em class="kj">先验概率来导出。</em></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nh"><img src="../Images/7533740f9a561cbe9cf87d1c6931cc1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*pWkRevAwf3p0khmdKz1oCQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">拉索回归(图片取自“统计学习的要素”)</figcaption></figure><blockquote class="mr"><p id="7b28" class="ms mt hh bd mu mv mw mx my mz na ki dx translated">如图所示，椭圆表示成本函数(RSS)的轮廓，菱形表示范数约束。它们相交的点可能在轴上。</p></blockquote><h2 id="36ed" class="lh kl hh bd km li ni lk kq ll nj ln ku jw nk lp kw ka nl lr ky ke nm lt la lu bi translated">山脊vs套索:</h2><p id="1dd7" class="pw-post-body-paragraph jn jo hh jp b jq lc ii js jt ld il jv jw le jy jz ka lf kc kd ke lg kg kh ki ha bi translated">岭享有闭合形式的表达成本函数最小化。而套索约束使解在响应变量Y方面呈非线性，优化起来有些棘手。</p><p id="41e1" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">Lasso与ridge不同，它通过将要素缩小到零来减少要素的数量，从而提供更好的可解释性。</p><p id="3f09" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在所有特性同等重要的情况下，Ridge可以表现得更好。Lasso在某些特征比其他特征更好地解释响应变量Y的情况下表现更好。</p></div><div class="ab cl lx ly go lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ha hb hc hd he"><h2 id="6450" class="lh kl hh bd km li lj lk kq ll lm ln ku jw lo lp kw ka lq lr ky ke ls lt la lu bi translated">4.降维方法:</h2><p id="28f7" class="pw-post-body-paragraph jn jo hh jp b jq lc ii js jt ld il jv jw le jy jz ka lf kc kd ke lg kg kh ki ha bi translated">上述方法通过选取变量子集或缩小变量系数来控制方差，在这些方法中，我们使用了最初的预测因子X₁、X₂、X₃ …Xp。我们现在将看到首先转换原始预测值，然后用这些转换后的预测值拟合模型的方法。</p><p id="aabb" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> a)主成分回归(PCR): </strong></p><p id="cf6d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这是一种无监督的技术，通过将原始高维(P)投影到低维(K)来降低数据的维度，其中K &lt; P.</p><p id="fdbd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">We start with selecting an arbitrary axis along which data has maximum variance and call it the <em class="kj">是第一主成分。</em>然后我们寻找垂直于它的第二轴，打包最大方差，从而选择<em class="kj">第二主成分。</em>该过程持续进行，直到达到某些停止标准。</p><blockquote class="mk ml mm"><p id="d2bd" class="jn jo kj jp b jq jr ii js jt ju il jv mn jx jy jz mo kb kc kd mp kf kg kh ki ha bi translated">例如:停止标准很可能是继续寻找主成分，直到我们解释了数据中99%的差异。</p></blockquote><p id="885f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在，我们简单地对新获得的低维数据应用线性回归。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nn"><img src="../Images/3b1f0acb430947ff118a631e8ed0f349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*NM3AkvE2QlHqBj3Y1jXAYw.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">图片摘自《用Scikit-learn和Tensorflow进行机器学习的实践》一书</figcaption></figure><blockquote class="mr"><p id="8362" class="ms mt hh bd mu mv mw mx my mz na ki dx translated">在这个图中，X₂的X₁是原始轴，C₂的C₁是主要组成部分。在右图中，我们可以看到每个轴上的变化。C₁包含最大的方差，并且按照选择C₂，我们选择垂直于它的具有最大方差的轴。</p></blockquote><p id="ef70" class="pw-post-body-paragraph jn jo hh jp b jq nb ii js jt nc il jv jw nd jy jz ka ne kc kd ke nf kg kh ki ha bi translated">注意:右图中的第二行包含第二个最大方差，但不被视为C₂，因为它不垂直于C₁.</p><p id="0f74" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> b)偏最小二乘法</strong></p><p id="cc4d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">PCR有一个缺点——它不能保证最好地解释预测因子的方向也能最好地预测反应</p><p id="9058" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">PLS是一种受监督的方法。该过程与PCR相同，寻找转换的特征并对其应用线性回归。除了在变换特征时使用响应变量y。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es no"><img src="../Images/f9ce496fe9bf5dbbd7e66d202596305b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*-85QRlJmVQETB765fa-5Xw.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated"><strong class="bd km">PCR/PLA的通用算法</strong>(鸣谢:James，g .，Witten，d .，Hastie，t .，&amp; Tibshirani，R. (2013)。<em class="lw">统计学习导论</em>(第1版。)[PDF]。斯普林格。)</figcaption></figure><p id="962e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在计算新方向时，PLS将权重(ϕ)放在与响应变量的相关性成比例的预测变量上。</p></div><div class="ab cl lx ly go lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ha hb hc hd he"><p id="482c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">文章到此结束。希望对某人有益！！</p></div></div>    
</body>
</html>