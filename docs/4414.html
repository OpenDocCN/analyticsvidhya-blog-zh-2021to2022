<html>
<head>
<title>What makes BERT special?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特有什么特别之处？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-makes-bert-special-a4b85c1ea68e?source=collection_archive---------1-----------------------#2021-10-09">https://medium.com/analytics-vidhya/what-makes-bert-special-a4b85c1ea68e?source=collection_archive---------1-----------------------#2021-10-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="ebb1" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">为什么BERT在NLP中这么特别？我对这个开创性的NLP变压器模型的理解。</h2></div><p id="b86b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">众所周知，大多数基于自然语言处理的模型受益于大量的训练数据。但是，获得更多的训练数据一直是一项具有挑战性的任务，大多数特定于任务的数据集只包含几千行人类标记的训练样本。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es js"><img src="../Images/516a7b695c827f8134dcf61f14c10b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ma_3gNSSaMmSAEi7xWX0IA.png"/></div></div></figure><p id="228d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了帮助缩小训练数据可用性的差距，我们现在有几个NLP预训练模型，如ELMO，GPT，可以在特定的语言相关任务上进行微调。BERT就是其中之一，它在情感分析、问题回答等11个自然语言理解任务中的突破性表现似乎已经在NLP中创造了突破。</p><h1 id="5783" class="ke kf hh bd kg kh ki kj kk kl km kn ko in kp io kq iq kr ir ks it kt iu ku kv bi translated"><strong class="ak">伯特是什么？</strong></h1><p id="b438" class="pw-post-body-paragraph iw ix hh iy b iz kw ii jb jc kx il je jf ky jh ji jj kz jl jm jn la jp jq jr ha bi translated">2018年，谷歌开发了一种基于变压器的NLP预训练模型，称为BERT或变压器的双向编码器表示。它只不过是一个<a class="ae lb" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">转换器语言模型</a>，有多个编码器层和自我关注头。随着BERT的发布，世界上任何人现在都可以在单个云TPU上在大约30分钟内，或者使用单个GPU在几个小时内训练自己的问题回答模型、情感分析或任何其他语言模型。</p><p id="ed39" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">酷…对吧？</p><p id="cc81" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在让我们看看它在NLP中有什么特别之处。</p><h1 id="b76a" class="ke kf hh bd kg kh ki kj kk kl km kn ko in kp io kq iq kr ir ks it kt iu ku kv bi translated">伯特有什么特别之处？</h1><p id="af0f" class="pw-post-body-paragraph iw ix hh iy b iz kw ii jb jc kx il je jf ky jh ji jj kz jl jm jn la jp jq jr ha bi translated">BERT是一个深度双向、无监督的语言表示模型，与以前的模型不同，它能够处理与句子中所有其他单词相关的单词。它可以通过同时查看单词前后的单词来考虑单词的完整上下文。</p><p id="4553" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如，考虑下面的句子，</p><p id="b033" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我的<strong class="iy hi">背</strong>疼。我会在5分钟后回来。</p><p id="9325" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里的<strong class="iy hi"> back </strong>是指上面句子中两种不同的语境。先前可用的上下文无关模型对于在上面的句子中单词<strong class="iy hi">后面</strong>的出现将具有相同的向量表示。而BERT，由于其双向性，比单向语言模型具有更深的语言上下文感，因此将上述单词<strong class="iy hi">存储为基于句子上下文而不同的不同向量。</strong></p><h1 id="45e9" class="ke kf hh bd kg kh ki kj kk kl km kn ko in kp io kq iq kr ir ks it kt iu ku kv bi translated">伯特是如何工作的？</h1><h2 id="ad48" class="lc kf hh bd kg ld le lf kk lg lh li ko jf lj lk kq jj ll lm ks jn ln lo ku lp bi translated">预处理:</h2><p id="013a" class="pw-post-body-paragraph iw ix hh iy b iz kw ii jb jc kx il je jf ky jh ji jj kz jl jm jn la jp jq jr ha bi translated">BERT中的预训练包括两个步骤，</p><p id="ffaa" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">蒙面LM(MLM)</p><p id="5c00" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下一句预测(NSP)</p><h2 id="ba61" class="lc kf hh bd kg ld le lf kk lg lh li ko jf lj lk kq jj ll lm ks jn ln lo ku lp bi translated">蒙面LM(MLM):</h2><p id="f437" class="pw-post-body-paragraph iw ix hh iy b iz kw ii jb jc kx il je jf ky jh ji jj kz jl jm jn la jp jq jr ha bi translated">在将单词序列输入BERT之前的这一步中，每个序列中15%的单词被替换为一个[MASK]标记。然后，该模型将被训练成通过查看序列中其他非屏蔽词的上下文来预测屏蔽词。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es lq"><img src="../Images/a5df23dbb6a8dc0114a5f96dc683eec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c0-6AzGNrK3mksSbi9AcGw.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">来源</figcaption></figure><h2 id="986e" class="lc kf hh bd kg ld le lf kk lg lh li ko jf lj lk kq jj ll lm ks jn ln lo ku lp bi translated">下一句预测(NSP):</h2><p id="44fe" class="pw-post-body-paragraph iw ix hh iy b iz kw ii jb jc kx il je jf ky jh ji jj kz jl jm jn la jp jq jr ha bi translated">在训练过程的这一步，BERT接收一对句子作为输入，并学习预测第二个句子是否是前一个句子的下一个句子。在训练期间，50%的输入是一对，其中第二个句子是原始文档中的下一个句子，而在另外50%中，从语料库中随机选择一个句子作为第二个句子。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es lv"><img src="../Images/941a84b3c4d7ab4c902e485e07d250d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cP6mfWjp6K6DwbXMaVWQcw.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated"><a class="ae lb" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="028c" class="lc kf hh bd kg ld le lf kk lg lh li ko jf lj lk kq jj ll lm ks jn ln lo ku lp bi translated">输入表示:</h2><p id="246f" class="pw-post-body-paragraph iw ix hh iy b iz kw ii jb jc kx il je jf ky jh ji jj kz jl jm jn la jp jq jr ha bi translated">使用以上两个步骤，在预训练阶段，我们创建了输入的密集表示，如下所示。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es lw"><img src="../Images/2267417a4b512c618159cb2ed984a579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qnO4uGeiFDOYIKv_rt36g.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated"><a class="ae lb" href="https://nlp.stanford.edu/seminar/details/jdevlin.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="1f94" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如上所示，句子对被打包成一个输入序列。输入序列在第一个句子的开头插入了一个“分类器”[CLS]标记，在每个句子的结尾插入了一个[分离]标记。每个标记具有标记嵌入、指示句子A或句子B的段嵌入和指示每个标记在序列中的位置的位置嵌入。然后，每个令牌被视为所有这三个嵌入的总和。</p><p id="6f8f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在训练BERT模型时，掩蔽LM和下一句预测一起训练，使得两种策略的组合损失函数最小。</p><p id="2cea" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">注意:</strong>对于预训练语料库，BERT使用了图书语料库(800M单词)和英语维基百科(2,500M单词)。</p><h2 id="7214" class="lc kf hh bd kg ld le lf kk lg lh li ko jf lj lk kq jj ll lm ks jn ln lo ku lp bi translated">微调:</h2><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es lx"><img src="../Images/b1a76e19f14940717cd1e12be0a5f75a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ezjnwkqrx6C9lHs4wtzUgw.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated"><a class="ae lb" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="2ea5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">正如您所看到的，除了输出层，预训练和微调都使用了相同的架构。经过预训练后，我们只需将特定于任务的输入和输出插入BERT，并端到端地微调所有参数。</p><p id="5229" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如，在问题回答的情况下，输入序列将是两个句子，句子A代表问题，下一个句子B代表其相应的答案。</p><p id="62b8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，在输出端，记号表示被馈送到用于记号级问题回答任务的输出层，并且[CLS]表示被馈送到用于分类任务(例如情感分析)的输出层。</p><h2 id="253f" class="lc kf hh bd kg ld le lf kk lg lh li ko jf lj lk kq jj ll lm ks jn ln lo ku lp bi translated">总结:</h2><p id="b66d" class="pw-post-body-paragraph iw ix hh iy b iz kw ii jb jc kx il je jf ky jh ji jj kz jl jm jn la jp jq jr ha bi translated">所以总结一下，</p><p id="e6a5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们首先将任务的输入输入到BERT中，这涉及到MLM和下一句预测的组合，一旦经过预训练，我们就可以针对特定语言的任务进行微调，方法是在末尾添加一个额外的输出层，将预测转换为特定任务的答案。</p><p id="e3b2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">请记住，这种微调的成本相对较低，所有上述过程在单个云TPU上最多只需1小时，在GPU上只需几个小时。</p><p id="f4c0" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你对这篇文章感兴趣，请在下面的评论中告诉我。请关注我，获取更多与NLP相关的文章。</p><p id="c736" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">感谢您的阅读！</p><h2 id="e70d" class="lc kf hh bd kg ld le lf kk lg lh li ko jf lj lk kq jj ll lm ks jn ln lo ku lp bi translated">参考资料:</h2><p id="06d1" class="pw-post-body-paragraph iw ix hh iy b iz kw ii jb jc kx il je jf ky jh ji jj kz jl jm jn la jp jq jr ha bi translated"><a class="ae lb" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">https://jalammar.github.io/illustrated-transformer/</a></p><p id="2b3a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae lb" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2018/11/open-sourcing-Bert-state-of-art-pre . html</a></p><p id="2e6c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">【https://arxiv.org/pdf/1810.04805.pdf T4】</p></div></div>    
</body>
</html>