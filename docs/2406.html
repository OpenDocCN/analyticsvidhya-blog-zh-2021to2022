<html>
<head>
<title>Visualizing Spotify Top 200 data in Tableau &amp; implementing a fast Python web-scraper</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Tableau中可视化Spotify前200名数据&amp;实现一个快速Python web-scraper</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/visualizing-spotify-top-200-data-in-tableau-implementing-a-fast-python-web-scraper-88a562495ad8?source=collection_archive---------3-----------------------#2021-04-22">https://medium.com/analytics-vidhya/visualizing-spotify-top-200-data-in-tableau-implementing-a-fast-python-web-scraper-88a562495ad8?source=collection_archive---------3-----------------------#2021-04-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/84617aaa3b81322fbe2c0656cb67f343.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z3C9BtbNc3JCD-yA7jBIMg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:作者的Spotify账户</figcaption></figure><p id="9ec9" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi jr translated">在音乐流媒体平台诞生之前很久，像大多数印度人一样，我也是通过在一些随机网站上盗录或盗版歌曲，或者在Youtube上合法观看歌曲来获得歌曲修复(<em class="ka"> Napster在我居住的地方并不流行</em>)。苹果音乐当时确实存在，但作为一个主要使用Windows的青少年，我几乎不知道如何使用苹果音乐，更不知道它实际上是一项付费服务。现在，除了苹果，你还有亚马逊、Youtube、Pandora、Soundcloud和其他多家竞争音乐流媒体行业的公司。</p><p id="08f2" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">但自成立以来，Spotify在流媒体音乐方面已经成为手机和电脑用户的主要选择。它在苹果和安卓用户中广受欢迎，不像亚马逊音乐或苹果音乐，有些人可能会说卡特更倾向于苹果或亚马逊产品。</p><blockquote class="kb"><p id="52e9" class="kc kd hh bd ke kf kg kh ki kj kk jq dx translated">音乐本身就是一种世界语言，它将世界各地的人们联系在一起&amp;歌曲只是艺术家用旋律描述的故事。</p></blockquote><p id="1267" class="pw-post-body-paragraph it iu hh iv b iw kl iy iz ja km jc jd je kn jg jh ji ko jk jl jm kp jo jp jq ha bi translated">作为一名Spotify的老用户，我想到了分析音乐故事背后的数据并将这个数据驱动的故事可视化该有多有趣。</p><h1 id="58f9" class="kq kr hh bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">通过webscraper自动收集数据</h1><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lo"><img src="../Images/384db47d881694b97dd492922b80e998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q9ecOt_7n9RZqUlZM55feA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图片来自<a class="ae lt" href="https://www.webharvy.com/articles/what-is-web-scraping.html" rel="noopener ugc nofollow" target="_blank"> Webharvy </a></figcaption></figure><p id="f493" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi jr translated"><span class="l js jt ju bm jv jw jx jy jz di"> F </span>首先，什么是<strong class="iv hi">网页抓取</strong>？网络抓取是一种技术，用于从网站或网站的一部分提取或抓取数据，然后以可读格式保存，如数据库或JSON文件。当某个网站没有API或者您对API的访问受到限制时，它是API的一个很好的替代品。Web抓取工具有助于利用网站上的丰富数据，这些数据可以用来获取新数据或增强现有数据。网页抓取可以通过网页抓取软件轻松完成，这使任务变得简单，或者可以通过编写一段代码以编程方式完成，这有时会非常复杂，具体取决于网站结构和要抓取的数据。Web scrapers本质上帮助从具有相同结构的网页中重复收集相似结构数据的过程自动化。web抓取的好例子包括:从电子商务商店提取产品数据，或者提取股票价格数据以进行更好的预测。</p><blockquote class="lu lv lw"><p id="1a3c" class="it iu ka iv b iw ix iy iz ja jb jc jd lx jf jg jh ly jj jk jl lz jn jo jp jq ha bi translated">网络抓取是一种技术，用于从网站或网站的一部分提取或抓取数据，然后以可读格式保存，如数据库或JSON文件。</p></blockquote><h2 id="689d" class="ma kr hh bd ks mb mc md kw me mf mg la je mh mi le ji mj mk li jm ml mm lm mn bi translated">合法吗？</h2><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mo"><img src="../Images/98ab8c42575a30d1ca6247061e1b7e48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mRCRv4tWW_qr37qf.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图片来自<a class="ae lt" href="https://prowebscraper.com/blog/is-web-scraping-legal/" rel="noopener ugc nofollow" target="_blank"> Prowebscraper </a></figcaption></figure><p id="f83e" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">当你<strong class="iv hi"> <em class="ka">利用</em> </strong>或<strong class="iv hi"> <em class="ka">滥用数据用于商业目的或商业用途或经济利益</em> </strong>时，网络抓取被视为<strong class="iv hi">非法</strong>。</p><ul class=""><li id="bd3b" class="mp mq hh iv b iw ix ja jb je mr ji ms jm mt jq mu mv mw mx bi translated">所有网站都有一定的指导方针，规定网站的哪些部分可以废弃，哪些部分不可以废弃。只要网页抓取是在网站所有者允许或允许抓取的部分进行的，你就是安全的。</li><li id="fa15" class="mp mq hh iv b iw my ja mz je na ji nb jm nc jq mu mv mw mx bi translated">此外，只要你留在公共领域，不回避被禁止的数据，刮被认为是合法的。</li><li id="1fa6" class="mp mq hh iv b iw my ja mz je na ji nb jm nc jq mu mv mw mx bi translated">作为一个人，你大概可以在一分钟内访问或点击10页左右。但是软件或程序有能力在一分钟内访问100或1000个页面。所以，重要的是要注意你的抓取率，也就是说，你不希望频繁地重复请求网站，以至于服务器无法处理它们，从而导致拥塞或使其认为有攻击正在进行。</li></ul><h2 id="f98f" class="ma kr hh bd ks mb mc md kw me mf mg la je mh mi le ji mj mk li jm ml mm lm mn bi translated">我如何访问网站的指南？</h2><p id="af26" class="pw-post-body-paragraph it iu hh iv b iw nd iy iz ja ne jc jd je nf jg jh ji ng jk jl jm nh jo jp jq ha bi translated">阅读每个网站提供的<strong class="iv hi"> robots.txt </strong>文件是一个很好的做法。该文件包含网站的哪些部分可以访问，哪些部分不可以访问，以及可以以什么速度访问的数据。<br/>如何访问<strong class="iv hi"> robots.txt </strong>文件？在您的浏览器的URL框中键入您想要抓取的网站URL，并将一个<strong class="iv hi"> /robots.txt </strong>附加到URL的末尾。<strong class="iv hi"> <em class="ka">为</em> <em class="ka">例</em></strong>——https://google.com/robots.txt</p><p id="6158" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">robots.txt通常返回像用户代理、不允许、允许和爬行延迟这样的字段。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ni"><img src="../Images/3c41b29e21af9e52307d5e429b2b8094.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ggzzMu44iDizA8_W2sLP_Q.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:Buzzfeed的robots.txt示例</figcaption></figure><p id="4baf" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">例如，见上图。Buzzfeed希望msnbot在抓取每个页面之前等待120秒，并禁止msnbot抓取列出的任何URL字符串。</p><p id="1bc6" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">以下是robots.txt结果的几个例子及其含义:</strong></p><ul class=""><li id="e271" class="mp mq hh iv b iw ix ja jb je mr ji ms jm mt jq mu mv mw mx bi translated"><em class="ka">阻止所有网络爬虫抓取所有内容</em></li></ul><pre class="lp lq lr ls fd nj nk nl nm aw nn bi"><span id="084b" class="ma kr hh nk b fi no np l nq nr">User-agent: * Disallow: /</span></pre><ul class=""><li id="f579" class="mp mq hh iv b iw ix ja jb je mr ji ms jm mt jq mu mv mw mx bi translated"><em class="ka">允许所有网络爬虫访问所有内容</em></li></ul><pre class="lp lq lr ls fd nj nk nl nm aw nn bi"><span id="0ef1" class="ma kr hh nk b fi no np l nq nr"><em class="ka"> </em>User-agent: * Disallow:</span></pre><ul class=""><li id="bc19" class="mp mq hh iv b iw ix ja jb je mr ji ms jm mt jq mu mv mw mx bi translated"><em class="ka">阻止特定文件夹中的特定网络爬虫</em></li></ul><pre class="lp lq lr ls fd nj nk nl nm aw nn bi"><span id="b049" class="ma kr hh nk b fi no np l nq nr">User-agent: Googlebot Disallow: /example-subfolder/</span></pre><p id="0773" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在我想收集和可视化的数据是Spotify Charts网站。</p><p id="7fda" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi jr translated"><span class="l js jt ju bm jv jw jx jy jz di"> L </span> <strong class="iv hi"> <em class="ka">对我来说很幸运的是，Spotify排行榜的robots.txt允许所有爬虫访问所有内容！！所以我没有做任何违法的事情！</em> </strong></p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ns"><img src="../Images/53632943c84e689b671169d7027e85e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PX4AHPBGXyfP1ZY1NrrjXw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:Spotify Charts robots.txt文件的结果—合法快乐</figcaption></figure><h2 id="520f" class="ma kr hh bd ks mb mc md kw me mf mg la je mh mi le ji mj mk li jm ml mm lm mn bi translated">网页抓取过程</h2><p id="b60c" class="pw-post-body-paragraph it iu hh iv b iw nd iy iz ja ne jc jd je nf jg jh ji ng jk jl jm nh jo jp jq ha bi translated">我刮到的数据是<a class="ae lt" href="https://spotifycharts.com/regional/" rel="noopener ugc nofollow" target="_blank"><strong class="iv hi">Spotify 200强排行榜</strong> </a>。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nt"><img src="../Images/b3fca584be3d623c569945777a312da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m8b0rGVmgAzpArVdN3EC4w.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:Spotify 200强排行榜</figcaption></figure><p id="e414" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我从Spotify charts网站上搜集了所有地区和该地区所有日期的数据。构建我的web scraper的两个基本库是:</p><ol class=""><li id="023e" class="mp mq hh iv b iw ix ja jb je mr ji ms jm mt jq nu mv mw mx bi translated"><a class="ae lt" href="https://pypi.org/project/beautifulsoup4/" rel="noopener ugc nofollow" target="_blank"><em class="ka">【bs4】</em></a>用于处理从网页源代码(HTML和CSS)中提取文本的库</li><li id="96b2" class="mp mq hh iv b iw my ja mz je na ji nb jm nc jq nu mv mw mx bi translated"><a class="ae lt" href="https://pypi.org/project/requests/" rel="noopener ugc nofollow" target="_blank"> <em class="ka">请求</em> </a>库用于处理与网页的交互(使用HTTP请求)</li></ol><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nv"><img src="../Images/bc94857d28748c8b3cda4f9df662a999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OUuh34k4uEOblOnrTwg-Uw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:网页抓取过程</figcaption></figure><h2 id="bb19" class="ma kr hh bd ks mb mc md kw me mf mg la je mh mi le ji mj mk li jm ml mm lm mn bi translated"><strong class="ak">自动收集地区和日期</strong></h2><p id="c434" class="pw-post-body-paragraph it iu hh iv b iw nd iy iz ja ne jc jd je nf jg jh ji ng jk jl jm nh jo jp jq ha bi translated">Spotify提供每日和每周图表，我想为每日数据制作一个仪表板。</p><p id="0873" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">此外，我想收集所有地区和该地区所有可能日期的数据。<strong class="iv hi"> <em class="ka">现在我该怎么做？</em>T13】</strong></p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nw"><img src="../Images/f0aebb384e2270d9050e6de4af9268be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0BQlSrhFjIKO4x5CkyujbQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:不同日期的印度(左)和阿拉伯联合酋长国(右)</figcaption></figure><p id="524d" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">Spotify图表网站有许多不同的地区和日期，你可以根据这些地区和日期过滤数据。<br/>从上面两张图片可以看出，URL会根据您选择的地区和日期而变化。地区甚至有特定的代码，<em class="ka">印度</em>有代码<em class="ka">在</em>阿联酋有代码<em class="ka"> ae。所以我需要所有地区和日期的代码来访问所有地区和日期的数据。</em></p><p id="53da" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在，我可以很容易地用Python硬编码一个包含所有地区和日期名称的列表。但是这里有大约60个地区，这意味着我需要浏览全部60页才能看到每个国家的地区代码。此外，值得注意的是，每个地区都有不同的数据可用日期。例如，美国有截至2017年1月1日的可用数据，但另一方面，印度只有截至2017年2月27日的可用数据。我可以创建一个循环，收集从今天到一年前的数据。</p><p id="9b21" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">但是为什么要手动创建列表呢？你可以通过抓取网站的HTML代码，以编程的方式获取所有区域、它们的代码以及这些区域可用的日期。 </p><p id="cebe" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">如果在浏览器中单击鼠标右键并按“检查元素”,然后使用浏览器检查器单击区域过滤器，则可以在该下拉列表中看到所有可用的区域。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nx"><img src="../Images/c6692c6b94ccb8420b0ea399aedc4e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LNJXvSgYZ106CV2oaOPIvA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:使用浏览器的inspect元素查看网站HTML代码</figcaption></figure><p id="8c51" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">从上图中，您可以看到下拉列表中所有可用区域的名称，甚至它们对应的标记为data-value的代码。您也可以使用Inspect元素来查看日期列表。现在我们知道了我们想要从哪些HTML标签中提取数据。</p><p id="861a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">首先，我为我的剪贴簿提供了一个基本的URL，这将帮助我抓取所有地区名称及其代码的网站，并将其保存到一个列表中，并将代码和地区名称压缩为一个字典。将这个压缩的字典保存到一个CSV文件中，在可视化过程中使用别名时会很有用。</p><pre class="lp lq lr ls fd nj nk nl nm aw nn bi"><span id="cab5" class="ma kr hh nk b fi no np l nq nr">###get all content on home_page <br/>home_page='<a class="ae lt" href="https://spotifycharts.com/viral/global/daily'" rel="noopener ugc nofollow" target="_blank">https://spotifycharts.com/viral/global/daily'</a><br/>hsession = requests.Session() <br/>hresponse = hsession.get(home_page)<br/>soup=BeautifulSoup(hresponse.content.decode('utf-8'),"html.parser")</span><span id="2f86" class="ma kr hh nk b fi ny np l nq nr">###parse through the home_page to get all the region names and their codes<br/>regions=[item['data-value'] for item in soup.find('div', {'data-type':'country'}).find_all('li', attrs={'data-value' : True})] <br/>region_fullname=[]<br/>for li in soup.find('div', {'data-type':'country'}).find_all('li', attrs={'data-value' : True}): #run alone no probs<br/>    region_fullname.append((li.get_text()))<br/>region_dictionary=dict(zip(regions, region_fullname))</span></pre><p id="3101" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">接下来，我想提取每个地区的所有日期。为此，我需要每个地区的主要网址。正如您所记得的，URL中只有一部分根据地区而变化，其余部分保持不变。为此，我创建了一个列表来存储每个地区的所有基本URL。</p><pre class="lp lq lr ls fd nj nk nl nm aw nn bi"><span id="5cd1" class="ma kr hh nk b fi no np l nq nr">#create a list of URLS for the landing page of each region collected earlier<br/>URLS=['<a class="ae lt" href="https://spotifycharts.com/viral/'+region+'/daily/'" rel="noopener ugc nofollow" target="_blank">https://spotifycharts.com/viral/'+region+'/daily/'</a> for region in regions]</span></pre><p id="3268" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在，我们希望访问URL列表中的每个URL，获取该URL的所有可用日期，并创建一个新列表，该列表将区域URL和日期附加在一起，以形成每个区域的新URL和我们希望访问的区域的可用日期组合。</p><pre class="lp lq lr ls fd nj nk nl nm aw nn bi"><span id="e0c6" class="ma kr hh nk b fi no np l nq nr">####function for getting all dates avalilable for each region<br/>new_URLS=[]<br/>def downloadPage(URL):                                                       #pass the landing page of a region<br/>  s1 = requests.Session() <br/>  response1 = s1.get(URL)       <br/>  soup_region=BeautifulSoup(response1.content.decode("utf-8"),"html.parser")  #get content of that page<br/>  time.sleep(0.25)<br/>  all_dates=[]                                                               #list for holding all dates on that region's page<br/>  for li in soup_region.find('div', {'data-type':'date'}).find_all('li'):   <br/>    all_dates.append((dt.datetime.strptime(li.get_text(), '%m/%d/%Y').date()))  <br/>  [new_URLS.append(URL+str(date)+'/download') for date in all_dates]       </span></pre><h2 id="0b24" class="ma kr hh bd ks mb mc md kw me mf mg la je mh mi le ji mj mk li jm ml mm lm mn bi translated">要收集的实际数据</h2><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/5604b37dc8a7fdd4989ca04a570ea19a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K3W6F-T6l-XRuws6BURkWQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:抓取表格数据</figcaption></figure><p id="bc58" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在我们有了所有地区及其相应日期的列表，我们可以前进到实际的web抓取，提取web浏览器上显示的前200个表。上图显示的是我们实际想要抓取的数据。我们要刮高光表。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nt"><img src="../Images/40158c094be5bf70789053c70062f7c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*asu3JkVi7NMziDChCFb02A.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:抓取表中的表数据标签</figcaption></figure><p id="ac51" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">上图显示了表格中所有可用的数据。它包含歌曲图像、歌曲位置、歌曲趋势图标、歌曲所属曲目以及歌曲已接收的流数量的数据。</p><p id="5527" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在，这里有一个巧妙的小骗局。我们可以直接从<strong class="iv hi"> <em class="ka">【下载到CSV】</em></strong>按钮提取数据，而不是抓取表格中的每个单元格。CSV文件以CSV格式提供浏览器中显示的整个表格数据；浏览器上显示的图像和趋势图标除外。</p><h2 id="cbec" class="ma kr hh bd ks mb mc md kw me mf mg la je mh mi le ji mj mk li jm ml mm lm mn bi translated">构建web刮刀</h2><p id="41f9" class="pw-post-body-paragraph it iu hh iv b iw nd iy iz ja ne jc jd je nf jg jh ji ng jk jl jm nh jo jp jq ha bi translated">现在我们可以解析每个HTML页面(<em class="ka">每个唯一的区域和日期组合</em>)来获取CSV数据，并将其存储在一个临时数据帧中。我们甚至将URL附加到数据框中，因为我们的CSV文件不提供区域和日期数据，而且在可视化时必须知道数据属于哪个日期和区域。这是收集数据进行分析的实际web抓取代码。</p><pre class="lp lq lr ls fd nj nk nl nm aw nn bi"><span id="90ef" class="ma kr hh nk b fi no np l nq nr">#function to get actual top 200 data from new_URL having date and region<br/>def downloadData(new_URL):<br/>    s2 = requests.Session() <br/>    response2 = s2.get(new_URL)   <br/>    soup_data=BeautifulSoup(response2.content,"html.parser")     #get page content for that particular date and region<br/>temp_df = pd.read_csv(io.StringIO(soup_data.decode("utf-8")))</span><span id="0a1f" class="ma kr hh nk b fi ny np l nq nr"> #save csv data to a temporary data frame<br/>    temp_df['region_date'] = new_URL      <br/> <br/>#add a new column for date(so we can extact later) and save the URL of that page<br/>    return temp_df<br/>  </span></pre><h2 id="bf98" class="ma kr hh bd ks mb mc md kw me mf mg la je mh mi le ji mj mk li jm ml mm lm mn bi translated">通过多线程实现更快的网页抓取</h2><p id="c9c0" class="pw-post-body-paragraph it iu hh iv b iw nd iy iz ja ne jc jd je nf jg jh ji ng jk jl jm nh jo jp jq ha bi translated">但每个地区都有大约2到3年的数据，这意味着每个地区都有大约1000个日期可供选择。<strong class="iv hi"> <em class="ka"> </em> </strong> <em class="ka">这意味着我们需要去</em> <strong class="iv hi"> <em class="ka"> 70个地区X 1000个日期= 70000页</em> </strong> <em class="ka">！</em></p><p id="6e17" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">解析70，000页可不是闹着玩的，因为你的计算机需要那么大的计算能力。由于这不是一个商业项目，我没有巨大的计算能力。一个接一个地逐个解析每个链接非常耗时！从 <strong class="iv hi"> <em class="ka">到</em></strong><em class="ka"/><strong class="iv hi"><em class="ka">100或更多的网页</em> </strong> <em class="ka">，web解析器将花费大约2-3个小时或更多的时间。</em>这意味着70，000页需要一天或更长时间。</p><blockquote class="kb"><p id="d0d9" class="kc kd hh bd ke kf kg kh ki kj kk jq dx translated">我需要一个更快的网络刮刀。<strong class="ak">回车，多线程。</strong></p></blockquote><p id="a674" class="pw-post-body-paragraph it iu hh iv b iw kl iy iz ja km jc jd je kn jg jh ji ko jk jl jm kp jo jp jq ha bi translated">想象一下，在你的浏览器上打开了多个标签，你同时在所有的标签上下载数据。这就是多线程。</p><p id="c756" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">您有多个并行执行同一任务的线程，而不是顺序执行同一任务，这意味着等待时间更长。由于我们正在执行相同的任务，但只有我们的URL发生了变化，多线程就派上了用场。我已经使用了<strong class="iv hi"> concurrent.futures </strong>线程库来执行多线程。</p><pre class="lp lq lr ls fd nj nk nl nm aw nn bi"><span id="2a05" class="ma kr hh nk b fi no np l nq nr">#use concurrent.futures threading to simultaneously obtain data for all days in a region<br/>executor=concurrent.futures.ThreadPoolExecutor(max_workers=1510)          #max webpages being downlaoded simultaneously is 1510<br/>futures = [executor.submit(downloadData, new_URL) for new_URL in new_URLS] #save the data in a list<br/>wait(futures, timeout=60, return_when=ALL_COMPLETED)              </span></pre><p id="5395" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">本质上，上面的代码块传递函数(一个我们想要再次执行的<em class="ka">任务&amp;再次</em>)和我们想要执行任务的URL列表。执行器并行运行线程。运行多个线程时，wait函数的超时时间为60秒。该函数在后台持续运行您的线程，只有在检索到所有线程/网页的数据后才会停止。未运行的线程保持挂起模式。</p><p id="62b5" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi"> <em class="ka">多线程显著减少了1000个网页的抓取时间，从2-3小时减少到大约15-20分钟！</em> </strong></p><p id="2fe0" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">接下来，只需将临时数据框中的所有数据合并到最终数据框中，即可获得所有地区和日期的前200个数据，并将其保存在CSV或Excel文件中，以备可视化使用。</p><h1 id="5d55" class="kq kr hh bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">使用Tableau可视化数据</h1><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nz"><img src="../Images/64fde4d407a7067d3d6ca0c1635e6355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZAl-2V6DdNCzFrEldp1z6w.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图片由作者提供:Spotify每日流媒体仪表盘，显示前200名数据(点击此处查看功能齐全的仪表盘<a class="ae lt" href="https://public.tableau.com/profile/anu1001#!/vizhome/SpotifyDailyStreamingDashboardTop200ChartData/SpotifyStreaming" rel="noopener ugc nofollow" target="_blank"/>)</figcaption></figure><p id="0a4a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">通过Tableau可视化是超直观的。</p><h2 id="26e2" class="ma kr hh bd ks mb mc md kw me mf mg la je mh mi le ji mj mk li jm ml mm lm mn bi translated">数据清理</h2><p id="603c" class="pw-post-body-paragraph it iu hh iv b iw nd iy iz ja ne jc jd je nf jg jh ji ng jk jl jm nh jo jp jq ha bi translated">将存储的CSV/Excel文件导入Tableau。接下来，分解我们通过自定义拆分附加的new_URL字段，以获得区域和日期字段。并删除由于拆分而创建的所有不必要的字段，并隐藏URL字段。</p><p id="7f36" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我决定制作4张图表:</p><ol class=""><li id="171b" class="mp mq hh iv b iw ix ja jb je mr ji ms jm mt jq nu mv mw mx bi translated">随时间推移的流</li><li id="675f" class="mp mq hh iv b iw my ja mz je na ji nb jm nc jq nu mv mw mx bi translated">最受欢迎的曲目</li><li id="399b" class="mp mq hh iv b iw my ja mz je na ji nb jm nc jq nu mv mw mx bi translated">最受欢迎的艺术家</li><li id="5d5c" class="mp mq hh iv b iw my ja mz je na ji nb jm nc jq nu mv mw mx bi translated">按区域划分的河流图</li></ol><p id="417e" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">下面是如何创建它们的快速分解。</p><h2 id="a984" class="ma kr hh bd ks mb mc md kw me mf mg la je mh mi le ji mj mk li jm ml mm lm mn bi translated">随时间推移的流</h2><p id="0d50" class="pw-post-body-paragraph it iu hh iv b iw nd iy iz ja ne jc jd je nf jg jh ji ng jk jl jm nh jo jp jq ha bi translated">在新的工作表中，将连续的<strong class="iv hi">周(日期)</strong>拖到列中，将<strong class="iv hi">和(流)</strong>拖到行中，并过滤出<strong class="iv hi">全局</strong>数据。根据需要设置工作表的格式。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es oa"><img src="../Images/4a33655264b82ad385f331b2bc9ac592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9P0pXVsjDR22wCYiCcHi6Q.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:时间长河</figcaption></figure><h2 id="fa0b" class="ma kr hh bd ks mb mc md kw me mf mg la je mh mi le ji mj mk li jm ml mm lm mn bi translated">热门曲目-嵌入Spotify URL</h2><p id="2073" class="pw-post-body-paragraph it iu hh iv b iw nd iy iz ja ne jc jd je nf jg jh ji ng jk jl jm nh jo jp jq ha bi translated">在新的工作表中，将<strong class="iv hi"> SUM(Streams) </strong>拖到列中，将<strong class="iv hi"> Track URL </strong>拖到行中。对行进行排序并添加<strong class="iv hi">轨道名称</strong>。现在你可以在图表上看到相邻的曲目名称和曲目URL，但是我们不需要<strong class="iv hi">曲目URL </strong>，所以隐藏它。</p><p id="031d" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">Spotify曲目链接的格式为:https://open.spotify.com/track/<strong class="iv hi">0 a1 hocfmlkiagvhwkkucja</strong></p><p id="8acd" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我们只需要track/之后的字符，即<strong class="iv hi"> 0A1hoCfMLkiAgvhWkkucJa </strong></p><p id="beca" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">要嵌入Spotify URL，格式如下:https://embed.spotify.com/?uri=spotify%3Atrack%3A<strong class="iv hi">PASTEHERE。</strong>所以我们只需要将/track之后的部分粘贴到这个虚拟嵌入URL。如果你需要嵌入一个播放列表，你需要做的就是把<em class="ka">% 3A音轨%3A </em>改成<em class="ka">% 3A播放列表%3A </em></p><p id="a586" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在创建一个名为<strong class="iv hi">的计算字段，嵌入URL </strong>。我们将使用该字段在最终仪表板中嵌入一个播放器，该仪表板显示在该图表中选择的歌曲。将此公式添加到嵌入的URL:<strong class="iv hi"><em class="ka">"https://embed.spotify.com/？uri = Spotify % 3A Track % 3A "+SPLIT([Track URL]，'/'，5) </em> </strong></p><p id="6416" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">因为我们只需要track/之后的部分，所以我们使用split函数来拆分Track URL，使用“/”作为分隔符。根据需要设置工作表的格式。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ob"><img src="../Images/e0699783e87cf269fd0830bcb94ea46d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oYMPyUYdwZP1QrfyyvJ1Bw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:流行曲目</figcaption></figure><h2 id="5dd3" class="ma kr hh bd ks mb mc md kw me mf mg la je mh mi le ji mj mk li jm ml mm lm mn bi translated">流行艺术家</h2><p id="382f" class="pw-post-body-paragraph it iu hh iv b iw nd iy iz ja ne jc jd je nf jg jh ji ng jk jl jm nh jo jp jq ha bi translated">在新表单中，将<strong class="iv hi">曲目URL </strong>拖至列，并将度量值更改为计数(Distinct)，然后将<strong class="iv hi">艺术家</strong>字段拖至行。根据需要设置工作表的格式。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es oc"><img src="../Images/6d903ee160ee8847b3be77d1bee0a0b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hXOtLPTNrggMiOKmhlYBqA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:流行艺术家</figcaption></figure><h2 id="4769" class="ma kr hh bd ks mb mc md kw me mf mg la je mh mi le ji mj mk li jm ml mm lm mn bi translated">按区域映射的流-批量重新混淆</h2><p id="4580" class="pw-post-body-paragraph it iu hh iv b iw nd iy iz ja ne jc jd je nf jg jh ji ng jk jl jm nh jo jp jq ha bi translated">首先，如果您直接在工作表中添加国家代码。你会看到它给出了18个未知数。你可以手动编辑这些未知数并修正它们，但我们甚至可以自动修正它们。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es od"><img src="../Images/9daa2eb0abac1dd7205882c28282e7a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y3ywFzFbRe8Z4x0vzDcm2A.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:图表中的国家代码-给出18个未知数</figcaption></figure><p id="4298" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">打开CSV文件，其中包含我们之前在Tableau中进行web抓取时创建的区域代码和区域名称的压缩字典。</p><p id="d0cb" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在转到<strong class="iv hi">数据&gt; &gt;编辑混合关系</strong>，点击自定义并从两个数据源中选择国家代码进行混合。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es oe"><img src="../Images/062149b6fb878b8023cc9f497aa62d2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*HsHjR75CrFvLiUDVOmIirA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:数据混合</figcaption></figure><p id="66bc" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在，从原始数据集中拖动国家代码<strong class="iv hi">到行中，从压缩的国家代码数据集中抓取国家名称</strong>到行中。然后右键点击<strong class="iv hi">国家名称</strong>字段，点击<strong class="iv hi">编辑主要别名</strong>并点击确定。现在，所有的国家代码将被映射到网站上的实际国家名称。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es of"><img src="../Images/2b46cc26baf9fcc41ff585c31d8c4351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zv1udnf8qG0kMVvcp1k2FQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">按作者分类的图像:批量创建主要别名</figcaption></figure><p id="bf10" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在在一个新的工作表中拖动，<strong class="iv hi">国家代码</strong>到空白处，你应该只看到<strong class="iv hi"> <em class="ka"> 1未知-全局</em> </strong>。通过将<strong class="iv hi">国家代码</strong>添加到过滤卡中将其过滤掉。将<strong class="iv hi"> SUM(Streams) </strong>添加到颜色中，创建一个填充的贴图。根据需要设置工作表的格式。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es og"><img src="../Images/ea423b9538e901b4937fe4f8e2398455.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f5vgnVZiVTBgQCFf8D81YQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">按作者分类的图像:按区域分类的流图</figcaption></figure><p id="ca67" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">最后，创建一个仪表板和其中的所有工作表，并为每个图表添加操作过滤器。向仪表板添加一个web页面对象，并向其添加一个URL操作，选择source sheet作为常用跟踪表，选择action作为select，URL作为计算字段:<strong class="iv hi">嵌入的URL </strong>，这是我们之前创建的。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es oh"><img src="../Images/12c39dea6fbc0515174712cd9bd89ce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*v3fbD7BuepaEjvCX5BT65g.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片:向对象添加嵌入的Spotify URL</figcaption></figure></div><div class="ab cl oi oj go ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ha hb hc hd he"><p id="01ce" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">对我有帮助的资源:<br/> 1。受点播网上研讨会启发:<a class="ae lt" href="https://www.tableau.com/learn/webinars/how-visualize-your-love-music" rel="noopener ugc nofollow" target="_blank">如何可视化自己对音乐的热爱</a> <strong class="iv hi"> <br/> 2。</strong> <a class="ae lt" href="https://www.thedataschool.co.uk/louise-le/how-to-embed-spotify-in-your-vizzes" rel="noopener ugc nofollow" target="_blank">如何将Spotify嵌入你的Tableau Vizzes </a> <br/> <strong class="iv hi"> 3。</strong> <a class="ae lt" href="https://www.tableau.com/learn/tutorials/on-demand/cleaning-data-bulk-re-aliasing" rel="noopener ugc nofollow" target="_blank">如何在Tableau中使用不同的数据源</a> <br/> <strong class="iv hi"> 4 </strong>。<a class="ae lt" href="https://beckernick.github.io/faster-web-scraping-python/" rel="noopener ugc nofollow" target="_blank">Python中更快的网页抓取</a></p></div></div>    
</body>
</html>