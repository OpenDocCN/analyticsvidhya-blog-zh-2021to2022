<html>
<head>
<title>Autosuggestion like Gmail using Attention Based Seq2Seq Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用基于注意力的Seq2Seq模型的自动建议，如Gmail</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/autosuggestion-like-gmail-using-seq2seq-attention-model-2ee00c604304?source=collection_archive---------6-----------------------#2021-04-13">https://medium.com/analytics-vidhya/autosuggestion-like-gmail-using-seq2seq-attention-model-2ee00c604304?source=collection_archive---------6-----------------------#2021-04-13</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/d8fa3b528974faa0de22fa0516d67da1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*3oK3xV0YbW6vZQ44CYbegQ.jpeg"/></div></figure><h1 id="9dd7" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="ak">目录:</strong></h1><p id="a08e" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">1.商业问题</p><p id="663d" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">2.数据采集</p><p id="4253" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">3.基础EDA</p><p id="58d7" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">4.数据预处理</p><p id="d506" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">5.了解基本编码器和解码器N/W</p><p id="04a5" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">6.定义我们的模型</p><p id="9b43" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">7.模型比较</p><p id="4755" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">8.注意力模型简介</p><p id="c225" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">9.Flask API上的最终部署</p><p id="c0aa" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">10.未来的工作</p><p id="06f8" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">11.如何联系我</p><p id="f77d" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">12.参考</p><h1 id="3f62" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="ak"> 1。业务问题</strong></h1><p id="1a4f" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">2018年，谷歌为gmail推出了智能撰写功能。用于帮助用户舒适地撰写各自的邮件，如下图所示。</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es km"><img src="../Images/4a74a2b4d27721847fafc794e5f60dce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4_SzLXQlUesSaFzO8ms7Fw.png"/></div></div></figure><p id="3bf0" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">目的是预测接下来的几个单词，从而在基于用户的初始单词实时编写的同时向用户提供建议。</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es kv"><img src="../Images/97b7b75cada0a1efe50e1680dae37597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S0BLn05rVCOyjjwHMN5-Xg.png"/></div></div></figure><h1 id="b667" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="ak"> 2。数据采集:</strong></h1><p id="69b5" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">我们将在这项任务中使用的数据集是Kaggle<a class="ae kw" href="https://www.kaggle.com/wcukierski/enron-email-dataset" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/wcukierski/enron-email-dataset</a>上提供的“安然电子邮件数据集”。安然电子邮件数据集包含大约500，000封由安然公司员工生成的电子邮件。这是联邦能源管理委员会在调查安然公司倒闭时获得的。数据集采用制表符分隔文件的形式，如下所示:</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es kx"><img src="../Images/cc1dc155c847c0d63759790a7ccd9ba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mG_JJkZydEKiZ6nj0ofGeQ.png"/></div></div></figure><p id="c2c8" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">如上图所示，每条消息都由一个file_id引用。我们将使用python库“电子邮件”来进一步处理数据。</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es ky"><img src="../Images/e856bae6b12d876a13dd7c9025e14804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lEqXgjbLIaaA15UjYyzNyg.png"/></div></div></figure><p id="531f" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">该消息包含所有细节，如消息id、日期、发送者、接收者等。我们对从消息文件中提取消息部分感兴趣，为此我们将使用电子邮件库中的有效载荷函数。下面是一个消息示例。</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es kz"><img src="../Images/9484101c79ba7298e19c37945dd41ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QhfVTd0lDOnaV-UqV3cbww.png"/></div></div></figure><h1 id="b377" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="ak"> 3。基础EDA : </strong></h1><p id="5e9d" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">为了更好地理解底层数据，我们将执行一些基本的EDA来收集一些关于数据的基本参数的信息。为了衡量安然员工之间分享的信息长度，我们将绘制一个信息长度分布图。</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es la"><img src="../Images/7da1c8132203d124ebc15eb5ac031f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dz0DnqOQ3h9VAaEF1KOu-g.png"/></div></div></figure><p id="2342" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">消息长度是右偏的，该图没有提供太多信息，因此我们将绘制CDF以获得更大的清晰度。</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lb"><img src="../Images/cca8ad44b079010214ec2937773a62f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qw0oDVqPA8u78rfZhDfAMA.png"/></div></div></figure><p id="2733" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">所以99 %的消息长度都在500k以下。</p><p id="00a0" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">下图显示了发送消息最多的员工。</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div class="er es lc"><img src="../Images/5d84a07acf736d4d391185bba83e5415.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*8TWeI2UDqqQp3ehKTcnfsg.png"/></div></figure><h1 id="098c" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="ak"> 4。数据预处理:</strong></h1><p id="617c" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">为了进一步处理，我们必须预处理电子邮件数据，因为数据中有许多特殊字符需要删除，以清理数据并将其转换为所需的格式，以便进一步处理。我们将使用下面的函数去压缩文本数据并清理它</p><figure class="kn ko kp kq fd ii"><div class="bz dy l di"><div class="ld le l"/></div></figure><p id="0efe" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">最后，是时候以一种格式构建我们的数据，以提供给我们的编码器-解码器模型了</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lf"><img src="../Images/dca3ec1bf60b5438eaad08035a324bb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HZUMu0opmM5EMH3EpGPtSw.png"/></div></div></figure><p id="4560" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">想法是预测标记为建议提供的初始文本的文本，其中“cls”和“sep”是开始和结束标记。现在，我们将继续对数据进行标记化。使用下面的代码片段。</p><figure class="kn ko kp kq fd ii"><div class="bz dy l di"><div class="ld le l"/></div></figure><h1 id="3e21" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="ak"> 5。了解基本编码器和解码器N/W </strong></h1><p id="0fda" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">首先，我们将尝试使用简单的seq2seq编码器-解码器模型来完成下一个单词/句子预测的任务。让我们首先理解seq2seq模型背后的直觉</p><p id="08d4" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated"><strong class="jl hi"> 5.1编码器:</strong></p><ul class=""><li id="de23" class="lg lh hh jl b jm kh jq ki ju li jy lj kc lk kg ll lm ln lo bi translated">编码器和解码器都是LSTM模型(或者有时是GRU模型)</li><li id="e658" class="lg lh hh jl b jm lp jq lq ju lr jy ls kc lt kg ll lm ln lo bi translated">编码器读取输入序列，并在所谓的<strong class="jl hi">内部状态向量</strong>或<strong class="jl hi">上下文向量</strong>中总结信息(在LSTM的情况下，这些被称为隐藏状态和单元状态向量)。我们丢弃编码器的输出，只保留内部状态。该上下文向量旨在封装所有输入元素的信息，以便帮助解码器做出准确的预测。</li><li id="2ffb" class="lg lh hh jl b jm lp jq lq ju lr jy ls kc lt kg ll lm ln lo bi translated">隐藏状态<em class="lu"> h_i </em>使用以下公式计算:</li></ul><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lv"><img src="../Images/f7a3459addec3edd6ed291f426ce1304.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/0*ofETP1ssG7UiMBzF.png"/></div></div></figure><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lw"><img src="../Images/bbc5b7a5bbd62fdb2d256135c1daa380.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n8kjkcNXWfiaLTQvD2KWeQ.png"/></div></div></figure><p id="7d21" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">他LSTM一个接一个地读取数据。因此，如果输入是一个长度为“t”的序列，我们说LSTM以“t”个时间步长读取它。</p><p id="dcb0" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">1.Xi =时间步长I的输入序列</p><p id="8223" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">2.hi和ci = LSTM在每个时间步长保持两种状态(“h”代表隐藏状态，“c”代表单元状态)。这些组合在一起，就是LSTM在时间步I的内部状态</p><p id="4e62" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">3.Yi =时间步长I的输出序列。Yi实际上是通过使用softmax激活生成的整个词汇的概率分布。因此，每个Yi是代表概率分布的大小为“vocab_size”的向量。</p><p id="eae9" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated"><strong class="jl hi"> 5.2解码器:</strong></p><ul class=""><li id="b72d" class="lg lh hh jl b jm kh jq ki ju li jy lj kc lk kg ll lm ln lo bi translated">解码器是LSTM，其初始状态被初始化为编码器LSTM的最终状态，即编码器最终单元的上下文向量被输入到解码器网络的第一个单元。利用这些初始状态，解码器开始产生输出序列，并且这些输出也被考虑用于将来的输出。</li><li id="2087" class="lg lh hh jl b jm lp jq lq ju lr jy ls kc lt kg ll lm ln lo bi translated">几个LSTM单元的堆栈，其中每个单元在时间步长t预测一个输出y_t</li><li id="7524" class="lg lh hh jl b jm lp jq lq ju lr jy ls kc lt kg ll lm ln lo bi translated">每个递归单元接受来自前一个单元的隐藏状态，并产生和输出它自己的隐藏状态。</li><li id="5e5b" class="lg lh hh jl b jm lp jq lq ju lr jy ls kc lt kg ll lm ln lo bi translated">任何隐藏状态<em class="lu"> h_i </em>使用以下公式计算:</li></ul><figure class="kn ko kp kq fd ii er es paragraph-image"><div class="er es lx"><img src="../Images/7acf055824405bc7f9f8c9e9c7cd77e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/0*RjTCB8nV96bjTRJo.png"/></div></figure><ul class=""><li id="727c" class="lg lh hh jl b jm kh jq ki ju li jy lj kc lk kg ll lm ln lo bi translated">使用以下公式计算时间步长<em class="lu"> t </em>的输出<em class="lu"> y_t </em>:</li></ul><figure class="kn ko kp kq fd ii er es paragraph-image"><div class="er es ly"><img src="../Images/4543bc5327627edaface33b002729828.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*_JI3MMimdmT1CyzO.png"/></div></figure><ul class=""><li id="7b9a" class="lg lh hh jl b jm kh jq ki ju li jy lj kc lk kg ll lm ln lo bi translated">我们使用当前时间步长的隐藏状态以及相应的权重W(S)来计算输出。<a class="ae kw" href="https://www.youtube.com/watch?v=LLux1SW--oM" rel="noopener ugc nofollow" target="_blank"> Softmax </a>用于创建一个概率向量，该向量将帮助我们确定最终输出(如问答问题中的单词)。</li></ul><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lz"><img src="../Images/8f806a57ff46fc4062ae083a947b14ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fJVamDT3e2X-5kk-dMayBA.png"/></div></div></figure><h1 id="5a2c" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="ak"> 6。定义我们的模型</strong></h1><p id="e69d" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">现在，我们将编码器-解码器模型定义如下:</p><figure class="kn ko kp kq fd ii"><div class="bz dy l di"><div class="ld le l"/></div></figure><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es ma"><img src="../Images/ece7fe4091533a3e917d42e36295f4d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P3xmZ_GX6u_sC9aBKUApiQ.png"/></div></div></figure><p id="5f67" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">编码器N/W由一个嵌入层和一个LSTM层组成。编码器的输出状态作为初始状态传递给解码器。解码器N/W包括一个嵌入层、一个LSTM层和使用softmax激活的时间分布密集层，输出为output_vocab_size。</p><p id="0f7a" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">在对模型进行了几个时期的训练后，现在是时候评估我们的模型了。为此，我们需要通过加载保存的编码器和解码器模型来构建推理模型</p><figure class="kn ko kp kq fd ii"><div class="bz dy l di"><div class="ld le l"/></div></figure><p id="4211" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated"><strong class="jl hi">损失图</strong></p><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es mb"><img src="../Images/28607bf561509214a91067cd2dbcc937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JBSPdgFAevQs3UmY_rySng.png"/></div></div></figure><p id="2248" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated"><strong class="jl hi"> 6.1训练模型的推理功能</strong></p><p id="e6a4" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">最后，我们定义我们的推理函数来预测我们的编码器-解码器模型的输出。</p><figure class="kn ko kp kq fd ii"><div class="bz dy l di"><div class="ld le l"/></div></figure><p id="29b6" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated"><strong class="jl hi">预测6.2蓝分</strong></p><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es mc"><img src="../Images/fa5c87ead5a69357a98f12fc863c086a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*93hspPHU1JS3l-pyJd_kCw.png"/></div></div></figure><p id="17b7" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">简单的编码器解码器模型做了一个体面的工作，就像上面看到的自动建议。但是简单的seq2seq模型在处理非常长的句子时有局限性，因此在这种情况下不是很健壮。为了克服这个缺点，我们将通过在我们的编码器-解码器模型中添加注意层来使用注意机制。</p><h1 id="4925" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="ak"> 7。注意力模型简介:</strong></h1><p id="1fb5" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">在注意机制中，编码器是双向LSTM，而解码器是单向LSTM。与仅将编码器的隐藏状态作为输入传递给解码器相反，我们传递由如下等式表示的上下文向量</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es md"><img src="../Images/c368a549066a7bbe2c0e6dc89093f5a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qDu-cVFh5sTBPD09Chx2pw.png"/></div></div></figure><p id="22c4" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">这里“<strong class="jl hi"> <em class="lu"> Ci </em> </strong>”是通过将编码器的隐藏状态“<strong class="jl hi"> <em class="lu"> hi </em> </strong>”与给定的“<strong class="jl hi"> <em class="lu"> alpha i </em> </strong>”相乘而生成的上下文向量，并且“<strong class="jl hi"> <em class="lu"> Tx </em> </strong>”是决定这样的词的数量以创建上下文向量的hyper参数。基本上，我们尝试创建一个单词窗口，它可能出现在我们的目标/输出单词之前或之后，如下图所示:</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div class="er es me"><img src="../Images/e33e18f8a627ee534e60f93376bc46b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*gJdAmgpnVrM3n8LCKjsg1w.png"/></div></figure><p id="f39a" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated"><strong class="jl hi"><em class="lu">【aplha I】</em></strong>的值在0和1之间，由当前解码器的<strong class="jl hi"><em class="lu">【eij】</em></strong>和<strong class="jl hi"><em class="lu">【St-1】</em></strong>隐藏状态<strong class="jl hi"><em class="lu">【HJ’</em></strong>给出。编码器架构如下所示:</p><figure class="kn ko kp kq fd ii"><div class="bz dy l di"><div class="ld le l"/></div></figure><p id="c127" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">编码器由一个lstm层和一个具有“glorot uniform”初始化的嵌入层组成。</p><p id="7c27" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">解码器架构定义如下:</p><figure class="kn ko kp kq fd ii"><div class="bz dy l di"><div class="ld le l"/></div></figure><p id="c778" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">解码器架构由一个Lstm层、一个嵌入层和一个关注层组成。这里我们使用tensorflow插件库来定义我们的注意力层。我们在这个模型中使用了“休闲”注意力层。</p><h1 id="92ea" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="ak"> 8。型号对比:</strong></h1><p id="3fb8" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">编码器-解码器模型和具有注意机制的编码器-解码器模型的比较如下:</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/5adc1cf45f1e0e05356bacf6527a88d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*8-EBsc9Dkc4lLbC2X8UwyA.png"/></div></figure><p id="0bbf" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">显然，注意力模型比没有注意力的模型好得多。我们将继续部署具有注意机制的模型。</p><h1 id="a042" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="ak"> 9。烧瓶API的最终部署</strong></h1><p id="3b78" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">在对模型进行几个时期的训练并使用<strong class="jl hi"> flask api </strong>将模型部署到我们的本地服务器上之后，模型的最终输出如下面的视频所示:</p><figure class="kn ko kp kq fd ii"><div class="bz dy l di"><div class="mg le l"/></div></figure><h1 id="d6fa" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="ak"> 10。未来工作:</strong></h1><p id="7587" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">我想用基于变压器的模型来训练数据。</p><h1 id="43c4" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="ak"> 11。如何联系我:</strong></h1><p id="c6bf" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">我的Linkedin个人资料<strong class="jl hi"> </strong> <a class="ae kw" href="https://www.linkedin.com/in/akash-anande-it/" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hi">链接</strong> </a> <strong class="jl hi"> </strong>和代码链接<a class="ae kw" href="https://github.com/Akashaanande" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hi"> Github链接</strong> </a></p><h1 id="758f" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="ak"> 12。参考文献:</strong></h1><p id="304e" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated"><a class="ae kw" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank">https://blog . keras . io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html</a></p><p id="830c" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated"><a class="ae kw" href="https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/addons/tutorials/networks _ seq 2 seq _ NMT</a></p><p id="4b0e" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated"><a class="ae kw" href="https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/4149/live-encoder-decoder-models/8/module-8-neural-networks-computer-vision-and-deep-learning" rel="noopener ugc nofollow" target="_blank">https://www . applied ai course . com/lecture/11/applied-machine-learning-online-course/4149/live-encoder-decoder-models/8/module-8-neural-networks-computer-vision-and-deep-learning</a></p><p id="0038" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated"><a class="ae kw" href="https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/4150/attention-models-in-deep-learning/8/module-8-neural-networks-computer-vision-and-deep-learning" rel="noopener ugc nofollow" target="_blank">https://www . applied ai course . com/lecture/11/applied-machine-learning-online-course/4150/attention-models-in-deep-learning/8/module-8-neural-networks-computer-vision-and-deep-learning</a></p></div></div>    
</body>
</html>