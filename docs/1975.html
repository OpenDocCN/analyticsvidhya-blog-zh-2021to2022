<html>
<head>
<title>Decision Tree — My Interpretation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树——我的解读</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-tree-my-interpretation-part-i-e730aed60cd3?source=collection_archive---------13-----------------------#2021-03-29">https://medium.com/analytics-vidhya/decision-tree-my-interpretation-part-i-e730aed60cd3?source=collection_archive---------13-----------------------#2021-03-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e3f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在做决定的时候，我们倾向于假设很多“如果-但是”的情况，然后得出结论。机器学习中的决策树也以类似的概念工作。从视觉上看，它是一个类似流程图结构，由父节点、分支节点和叶节点组成。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/65f9e13add7cd3308a947681edefaed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/0*2dRmlEiYuI2igKuL.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图1.0</figcaption></figure><p id="f8d9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">参考上图<strong class="ig hi">图1.0 </strong>、<strong class="ig hi">年龄</strong>为父节点、<strong class="ig hi">吃披萨</strong> &amp; <strong class="ig hi">练习</strong>为分支节点、<strong class="ig hi">适合</strong> &amp; <strong class="ig hi">不适合</strong>为叶节点。它告诉我们，如果你的年龄不到30岁，你吃比萨饼，你是不健康的。这样的if和else链形成了一棵决策树。</p></div><div class="ab cl jo jp go jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="ha hb hc hd he"><h1 id="14d6" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">ML中的决策树</h1><p id="359b" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb ha bi translated">我们拥有的是历史数据，当我们对其应用决策树时，决策树将数据分成越来越小的块来创建树。决策树对数据应用一个函数来划分数据。根据函数，数据属于标签类的分离段或桶，例如:<strong class="ig hi">是</strong> &amp; <strong class="ig hi">否</strong>。当数据被分成更小的块或桶时，目标列变得更加<strong class="ig hi">同质</strong>。直到树达到完美的同质桶，它继续进一步划分树。当我们划分树时，桶(分支)的数量增加。</p><blockquote class="ky kz la"><p id="e290" class="ie if lb ig b ih ii ij ik il im in io lc iq ir is ld iu iv iw le iy iz ja jb ha bi translated"><strong class="ig hi">同质桶</strong>指由所有相同标签组成的桶</p></blockquote><p id="18b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">自变量和因变量之间的关系通过用于划分树的函数来表达<strong class="ig hi">。这意味着<strong class="ig hi">决策树</strong>是<strong class="ig hi">非参数算法</strong>。这意味着模型<strong class="ig hi">不返回模型参数</strong>。因此，在每次迭代中，数据s被分成<strong class="ig hi">两个子集</strong>，这些子集被称为<strong class="ig hi">分支</strong>。</strong></p><p id="5942" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们对训练数据应用决策树时，算法产生非常大和复杂的树，即它将具有许多分支、叶子、节点、桶，使得在最后一个桶中，你的目标列是完全同质的。但是，您可能在每个叶节点中只找到一条记录。这种树被称为过拟合树，因此我们需要正则化树。</p><blockquote class="ky kz la"><p id="e836" class="ie if lb ig b ih ii ij ik il im in io lc iq ir is ld iu iv iw le iy iz ja jb ha bi translated"><strong class="ig hi">规则化</strong>是指在树被整枝时控制树的生长。</p></blockquote><p id="e852" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当决策树变得非常大时，它们会溢出。我们规范这样的树，也就是说，树不能充分发挥它的潜力，它会受到限制。因此，您最终可能会得到目标并不完全相同的叶节点。因此，我们计算桶中每个类的概率。测试记录属于桶中概率高的那一类。这个概率叫做<strong class="ig hi">后验概率。</strong></p><p id="1a13" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">决策树</strong>有一个问题，我们遗漏了一些数据点的组合，这些组合要么不是训练数据的一部分，要么被遗漏了。因此，决策树不会错过分类。</p><p id="5ff9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">构建决策树的步骤:</p><ul class=""><li id="530a" class="lf lg hh ig b ih ii il im ip lh it li ix lj jb lk ll lm ln bi translated">在决策树中，原始数据集代表<strong class="ig hi">根节点</strong>。</li><li id="ec1d" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">根节点被分成两个桶，在对根节点应用一些函数之后，这些桶被称为<strong class="ig hi">分支节点</strong>。</li><li id="5ee2" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">除非正则化，否则当分支节点具有同类目标时，它们不会被进一步分裂。这些最后的节点被称为<strong class="ig hi">叶节点</strong>。</li></ul><p id="7310" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，问题出现了，如何决定决策树应该根据哪一列及其阈值将节点拆分到其分支，以及如何决定决策树应该将节点拆分到其分支的阈值？</p><ul class=""><li id="7fe4" class="lf lg hh ig b ih ii il im ip lh it li ix lj jb lk ll lm ln bi translated">决策树使用称为<strong class="ig hi">损失函数的学习机制。</strong>损失函数代表减少目标柱中杂质的一种方式。</li><li id="5614" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">为了计算任何特定节点中的杂质，我们使用:<br/> a)熵<br/> b)基尼</li><li id="e5c7" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">你的叶子越不均匀，就有越多的不确定性，即错误分类的高概率。</li></ul><p id="8003" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> a)熵:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lt"><img src="../Images/3d23e23be6c1d408002d664701b5b5f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/0*wWJP74fJr4rqQxPq.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图1.1</figcaption></figure><p id="53e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">熵是概率和杂质之间的关系。X轴表示概率，Y轴表示杂质。你可以看到在概率= 0.5时，你有最大的不确定性。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lu"><img src="../Images/40efd23f89fc14d7fd7126d7f61d3683.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*JM6mrQ1kAXuqZ41JsiaUrA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图1.2</figcaption></figure><p id="b372" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上图显示的是熵的公式。log2 pi具有以下属性:</p><ul class=""><li id="ab3e" class="lf lg hh ig b ih ii il im ip lh it li ix lj jb lk ll lm ln bi translated">log2(pi=1) = 0</li><li id="aa16" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">log2(pi=.5) = 1</li><li id="6f24" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">log2(pi=0) =无穷大，</li></ul><p id="f973" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，当pi=0时，熵曲线将向无穷大移动，但熵的范围在0–1之间，因此我们将pi乘以log2(pi ),这样曲线不是向无穷大移动，而是以0结束。</p><p id="63e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">熵状态:</p><ul class=""><li id="50f4" class="lf lg hh ig b ih ii il im ip lh it li ix lj jb lk ll lm ln bi translated">每当一个事件的概率，即P(X=1)是0.5，就有最大的不确定性。</li><li id="5085" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">当事件的概率为0或1时，不确定性为0。</li></ul><p id="8355" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">公式中出现负号的原因是log2(pi)返回负数。</p><p id="0cd7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">与决策树相关的熵:</strong></p><ul class=""><li id="5993" class="lf lg hh ig b ih ii il im ip lh it li ix lj jb lk ll lm ln bi translated">决策树找到一个独立的属性&amp;在该属性中，它还会找到一个阈值，这样当算法对给定的列应用函数时，在给定的阈值上，它会将数据分成两个节点。</li><li id="7ba8" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">当创建子分支时，子分支的总熵应该小于父节点的熵。下降越多，获得的信息越多。因此，为了分裂，选择给出最大熵降的柱。<br/>信息增益=前一节点熵-当前节点熵</li></ul><p id="d338" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> b)基尼</strong></p><ul class=""><li id="de97" class="lf lg hh ig b ih ii il im ip lh it li ix lj jb lk ll lm ln bi translated">选择熵和基尼两者中的任何一个，产出都不会有巨大的差异。</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lv"><img src="../Images/402d4555a58afcb6a598f001a4630887.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/0*GliFXiKf5i9pv7ZM.jpeg"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图1.3</figcaption></figure><p id="a75e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上图展示了基尼系数公式。</p></div><div class="ab cl jo jp go jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="ha hb hc hd he"><h1 id="f587" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">信息增益</strong></h1><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lw"><img src="../Images/687d7bc6b7b787f2de46292ae9f2f46d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*-sjFEjyICAMB77uqlK0yFA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图1.4</figcaption></figure><ul class=""><li id="39ed" class="lf lg hh ig b ih ii il im ip lh it li ix lj jb lk ll lm ln bi translated">H(X):它是根节点和每个分裂节点的熵</li><li id="59df" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">|Xv|:分支节点中的样本数</li><li id="639a" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">|X|:样本总数</li><li id="f1e1" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">H(Xv):分支节点的熵</li><li id="55fd" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">G(X，A):信息增益</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lx"><img src="../Images/340b55dd08fb0767d7534f60bec671f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*ku-Aepiy1c4ltdbKerumSQ.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图1.5</figcaption></figure><p id="8070" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">h(F1)=-[(9/14)* log2(9/14)+(5/14)* log2(5/14)]= . 91</p><p id="2947" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">h(F2)=-[(6/8)* log2(6/8)+(2/8)* log2(2/8)]= . 81</p><p id="98f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">h(F3)=-[(3/6)* log2(3/6)+(3/6)* log2(3/6)]= . 1</p><p id="ec7d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">增益= . 91-[(8/14)* . 81]-[(6/14)* . 1]]=<strong class="ig hi">. 0049</strong></p><blockquote class="ky kz la"><p id="0093" class="ie if lb ig b ih ii ij ik il im in io lc iq ir is ld iu iv iw le iy iz ja jb ha bi translated">节点中记录的数量决定了它们对整体数据的影响程度。</p></blockquote></div><div class="ab cl jo jp go jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="ha hb hc hd he"><h1 id="b1d4" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">关键外卖:</strong></h1><ul class=""><li id="41f1" class="lf lg hh ig b ih kt il ku ip ly it lz ix ma jb lk ll lm ln bi translated">将熵设置为标准会因为日志操作而减慢计算</li><li id="b3d9" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">将拆分器设置为“随机”时，使用超级参数，如:max_features，random_state。这样就有机会使用不重要的列，这些列不会给出太多的信息。这导致更深和更不精确的树。但是它很快，不太容易因为随机性而过度拟合，并且在每次分割之前，您不需要计算最佳分割</li><li id="8b03" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">当你有几个功能时，你可以用“最佳”作为拆分器</li><li id="cf42" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">在max_depth设置为默认值的情况下，如果您为min_samples_leaf选择默认值，叶节点将只有一个标签</li><li id="cdd3" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">如果指定min_samples_split，节点将扩展，直到所有叶子包含的样本数小于max_depth的最小数目。算法将选择提供最大深度的超参数。</li><li id="0a9d" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">您允许树增长得越深，模型就变得越复杂，因为您将有更多分裂，它将捕获更多关于训练数据信息。由于这种情况，会发生过度配合</li><li id="857a" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">如果模型过拟合，请减小max_depth</li><li id="c363" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">让您的模型默认决定max_depth，根据训练和验证集的分数增加或减少max_depth</li><li id="e3a7" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">min_samples_split的理想值:1到40。它控制过度拟合。较高的值会阻止模型学习关系。</li><li id="918f" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">如果类别不平衡，请定义min_weight_fraction_leaf超参数。给不平衡的类更高的权重。</li></ul></div><div class="ab cl jo jp go jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="ha hb hc hd he"><h1 id="d3bc" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">注意事项</h1><p id="322f" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb ha bi translated">采油树的复杂性通过以下方式衡量:</p><ul class=""><li id="591f" class="lf lg hh ig b ih ii il im ip lh it li ix lj jb lk ll lm ln bi translated">节点数量</li><li id="b40d" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">叶片数</li><li id="8fd1" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">深度</li><li id="937d" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">属性数量</li></ul><p id="3bb8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">停止标准:</p><ul class=""><li id="11be" class="lf lg hh ig b ih ii il im ip lh it li ix lj jb lk ll lm ln bi translated">最大深度</li><li id="fd8b" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">最小_样本_分割</li><li id="9791" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">最小样本叶</li></ul><p id="049e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我建议你阅读关于修剪方法。</p><p id="c358" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我希望这些内容能帮助你理清关于决策树的概念。如果对你有帮助，就善意的分享给同行，增长链条。</p></div></div>    
</body>
</html>