<html>
<head>
<title>Hybrid Variational Autoencoder-based Models for Fraud Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于混合变分自动编码器的欺诈检测模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/hybrid-variational-autoencoder-based-models-for-fraud-detection-c801d06b1794?source=collection_archive---------1-----------------------#2021-01-06">https://medium.com/analytics-vidhya/hybrid-variational-autoencoder-based-models-for-fraud-detection-c801d06b1794?source=collection_archive---------1-----------------------#2021-01-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/6641e611c5a86b958e2f51f895421377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*tNdxr8tH45O7EbnI3gkGeg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">使用堆叠值的异常检测</figcaption></figure><p id="0161" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">简介</em> </strong></p><p id="1f94" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这项工作的目标是使用Keras/Tensorflow API开发深度学习模型，以检测异常信用卡交易并对欺诈进行分类。典型的异常检测涉及高度不平衡的数据集。我们在一个无人监管的环境中使用堆叠变分自动编码器(VAE)来有效地对欺诈交易进行分类。使用Kaggle信用卡欺诈数据集对模型进行了测试。其他基准测试也是使用KDD cup 99–10%数据集完成的。</p><p id="38c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">堆叠变分自动编码器(VAEs)用于通过仅用“正常”数据训练它们来学习“正常”信用卡交易的潜在空间表示。通过使用训练的VAE网络计算重构误差来识别异常交易。异常高的重建误差表示异常交易/欺诈交易。我们确定了重建误差的最佳阈值，超过该阈值的交易被标记为欺诈，作为产生最高模型准确性的阈值(根据F1分数和ROC下面积)。</p><p id="2ebe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们还探索了不同的混合模型，包括VAEs与监督学习模型(如随机森林分类器)的组合，以提高分类精度。在混合工作流中，堆叠值仅用作生成模型，以增加欠采样数据(即异常)。基于VAE的数据扩充用于提高随机森林分类器的性能。</p><p id="dd32" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">数据集描述</em> </strong></p><p id="e99b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们使用两个数据集进行基准测试，Kaggle信用卡欺诈数据集(<a class="ae jp" href="https://www.kaggle.com/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/mlg-ulb/creditcardfraud</a>)和1999年KDD杯10%数据集(<a class="ae jp" href="http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html" rel="noopener ugc nofollow" target="_blank">http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html</a>)。虽然我们主要使用Kaggle信用卡欺诈数据集，但kddcup99数据集仅用于额外的基准测试。在本节中，我们将简要探讨Kaggle数据集。</p><p id="83ea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Kaggle信用卡欺诈数据集有284807笔信用卡交易，其中492笔是欺诈交易(类别标签= 1)，其余284315笔是正常交易(类别标签= 0)。数据集高度不平衡，欺诈交易(异常)仅占所有交易的0.172%。</p><p id="f4e9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">数据集有31列:</p><p id="62dd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">['时间'，' V1 '，' V2 '，' V3 '，' V4 '，' V5 '，' V6 '，' V7 '，' V8 '，' V9 '，' V10 '，' V11 '，' V12 '，' V13 '，' V14 '，' V15 '，' V16 '，' V17 '，' V18 '，' V19 '，' V20 '，' V21 '，' V22 '，' V23 '，' V24 '，' V25 '，' V26 '，' V27 '，' V28 '，'数量'，'类']</p><p id="cfda" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">特征['V1 '，' V2 '，' V3 '，' V4 '，' V5 '，' V6 '，' V7 '，' V8 '，' V9 '，' V10 '，' V11 '，' V12 '，' V13 '，' V14 '，' V15 '，' V16 '，' V17 '，' V18 '，' V19 '，' V20 '，' V21 '，' V22 '，' V23 '，' V24 '，' V25 '，' V26 '，' V27 '，' V28']是用PCA获得的。由于保密问题，未提供原始功能。特征“时间”是第一次交易和当前交易之间的时间差，以秒为单位。特征“金额”是交易金额。</p><p id="ece9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">数据集中没有缺失值/空值。作为预处理步骤的一部分，我们使用scikit-learn的MinMaxScaler或StandardScaler缩放所有数字特征。</p><p id="f7cd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">KDD杯1999-10%数据集是为第三届国际知识发现和数据挖掘工具竞赛准备的。它包含局域网的TCP转储数据。数据集中的每一行指的是源IP和目标IP地址之间的连接，即在特定时间开始和结束的TCP数据包序列。连接被标记为正常或恶意(攻击)。这些攻击被进一步分为4个主要类别。我们把所有的攻击作为一个类别，把网络入侵检测问题作为二分类问题来处理。KDD杯数据集有494020个数据点，其中97277个(20%)是正常连接，结果是异常(攻击连接)。它有41个特征，其中7个是分类特征。我们对这些分类特征进行热编码。数据集中没有空值/缺失值。</p><p id="90ec" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于“正常”连接的数据点数较少(仅占所有数据的20%)，我们在异常数据点(即“攻击”连接)上训练VAE，然后预测“正常”连接。</p><p id="5802" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">使用堆叠变分自动编码器的异常检测</em> </strong></p><p id="602a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们首先展示堆叠VAE的Tensorflow实现。Kaggle数据集中有29个要素。我们开发了具有以下架构的堆叠式VAE。</p><p id="2149" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">车型详情</em> </strong></p><p id="5ea4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输入层的维度为29</p><p id="89b1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">编码器有两个隐藏层，分别有20和15个神经元。</p><p id="c4b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">编码层/潜在层有5个神经元</p><p id="6823" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">解码器有两个隐藏层，分别有15个和20个神经元。</p><p id="ed4a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输出尺寸为29</p><p id="7250" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">损失功能和训练</em> </strong></p><p id="4c7a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们使用RMSProp优化器训练VAE 100个时期，学习率为0.001，批量大小为256。对于这个问题，我们使用不同的损失函数。在简短的文献回顾和相当多的优化以获得高F1分数之后，我们在模型中使用以下损失函数，</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es jq"><img src="../Images/4239649c5c6714ea17e37ad7da016b6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*HiBBtXpV7BLguCf3cnxqyw.png"/></div></figure><p id="9912" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一项是均方重建损失，第二项是后验和高斯先验之间的加权KL散度。我们可以将α固定为常数，或者在训练过程中给出一个时间表。我们固定α= 1，其他著作使用了不同的α值(理解β-VAE中的解纠缠，Burgess P. C .等人arXiv:1804.03599)或者甚至使用了随时期改变α的时间表。我们的Tensorflow实现将alpha作为输入，并允许所有这些变化。</p><p id="1dc3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">异常情况的测定</em> </strong></p><p id="ccd6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们跟随作者，安，Jinwon，和Sungzoon赵。(“使用重建概率的基于变分自动编码器的异常检测”IE 2专题讲座(2015):1–18。)用于确定异常。然而，我们对他们的算法进行了修改。在网络被训练之后，交易被确定为正常或异常，如下所述，</p><p id="81bc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.给定输入x，我们使用编码器网络确定均值(z_mean)和标准差(z_log_var)。</p><p id="d9c4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.我们使用z_mean和z_log_var抽取L个样本。我们在Tensorflow实现中固定L = 10，在Keras实现中固定L = 1。结果变化不大。</p><p id="98ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.我们获得了所有L个样本的平均重建误差，</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es jv"><img src="../Images/43380bf61e262d5d38be28aeff5f4971.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*NapCrEUgh_ta4kKFMlBhjQ.png"/></div></figure><p id="adf8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.如果R_e &gt; R_threshold，则交易被确定为异常或欺诈，否则被确定为正常</p><p id="ba24" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>我们不使用类别标签来确定异常。虽然提供了类别标签，但我们完全将此视为无监督学习问题。通常情况下，数据集中没有标注异常。</p><p id="9f95" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们在TensorFlow中展示了一段简单的代码来演示这个想法。</p><pre class="jr js jt ju fd jw jx jy jz aw ka bi"><span id="b441" class="kb kc hi jx b fi kd ke l kf kg">tf.reset_default_graph()<br/># stacked VAE<br/># encoder has 2 hidden layers with 20 and 15 neurons<br/># coding layer/latent layer has 5 neurons<br/># decoder has 2 hidden layers with 15 and 20 neurons</span><span id="670e" class="kb kc hi jx b fi kh ke l kf kg">n_inputs = X_train.shape[1]<br/>n_hidden1 = 20<br/>n_hidden2 = 15<br/>n_hidden3 = 5  # codings<br/>n_hidden4 = n_hidden2<br/>n_hidden5 = n_hidden1<br/>n_outputs = n_inputs<br/>learning_rate = 0.001</span><span id="cda7" class="kb kc hi jx b fi kh ke l kf kg"># initialize weights of network<br/>initializer = tf.contrib.layers.variance_scaling_initializer()</span><span id="5ac3" class="kb kc hi jx b fi kh ke l kf kg">my_dense_layer = partial(tf.layers.dense,<br/>                         activation=tf.nn.relu,<br/>                         kernel_initializer=initializer)</span><span id="b669" class="kb kc hi jx b fi kh ke l kf kg"># X is the input data<br/>X = tf.placeholder(tf.float32, [None, n_inputs])</span><span id="d067" class="kb kc hi jx b fi kh ke l kf kg"># alpha is weight parameter for KL divergence<br/># this determines the regularization of posterior<br/># we fix it to 1 here<br/># this can be tuned refer: Understanding disentangling in beta-VAE arXiv:1804.03599<br/>alpha = tf.placeholder(tf.float32)<br/></span></pre><p id="426d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">编码器网络</em> </strong></p><pre class="jr js jt ju fd jw jx jy jz aw ka bi"><span id="c97c" class="kb kc hi jx b fi kd ke l kf kg">with tf.name_scope("Encoder"):<br/>    hidden1 = my_dense_layer(X, n_hidden1)<br/>    hidden2 = my_dense_layer(hidden1, n_hidden2)<br/>    hidden3_mean = my_dense_layer(hidden2, n_hidden3, activation=None)<br/>    hidden3_sigma = my_dense_layer(hidden2, n_hidden3, activation=None)<br/>    noise = tf.random_normal(tf.shape(hidden3_sigma), dtype=tf.float32)<br/>    hidden3 = hidden3_mean + tf.exp(0.5*hidden3_sigma) * noise</span></pre><p id="ba32" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">解码器网络</em> </strong></p><pre class="jr js jt ju fd jw jx jy jz aw ka bi"><span id="0b82" class="kb kc hi jx b fi kd ke l kf kg">with tf.name_scope("Decoder"):<br/>    hidden4 = my_dense_layer(hidden3, n_hidden4)<br/>    hidden5 = my_dense_layer(hidden4, n_hidden5)<br/>    x_hat = my_dense_layer(hidden5, n_outputs, activation=None)</span></pre><p id="87fb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">损失函数</em> </strong></p><pre class="jr js jt ju fd jw jx jy jz aw ka bi"><span id="3844" class="kb kc hi jx b fi kd ke l kf kg">with tf.name_scope("Loss"):<br/>    reconstruction_loss = tf.reduce_mean(tf.square(x_hat - X))<br/>    eps = 1e-10 # smoothing term to avoid computing log(0) <br/>    latent_loss = 0.5 *(tf.reduce_sum(tf.exp(hidden3_sigma) + tf.square(hidden3_mean) - 1 - hidden3_sigma))<br/>    <br/>    loss = tf.reduce_mean(reconstruction_loss + alpha*latent_loss)</span></pre><p id="f1a8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">训练</em> </strong></p><pre class="jr js jt ju fd jw jx jy jz aw ka bi"><span id="41a9" class="kb kc hi jx b fi kd ke l kf kg">with tf.name_scope("Training"):<br/>    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)<br/>    training_op = optimizer.minimize(loss)</span></pre><p id="7e4d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">重建错误</em> </strong></p><pre class="jr js jt ju fd jw jx jy jz aw ka bi"><span id="c2d8" class="kb kc hi jx b fi kd ke l kf kg">with tf.name_scope("ReconstructionError"):<br/>    rError = tf.Variable(0.0, name = "ReconstructionError")<br/>    init_error = tf.variables_initializer([rError])<br/>    # take average of 10 samples drawn in the neighborhood X_test sample<br/>    for i in range(10):<br/>        rError = rError + tf.reduce_mean(tf.reduce_sum(tf.square(X-x_hat),axis=1))<br/>    rError = rError/10<br/>init = tf.global_variables_initializer()<br/>saver = tf.train.Saver()</span></pre><p id="ec84" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们训练网络100个历元，批量大小为256</p><pre class="jr js jt ju fd jw jx jy jz aw ka bi"><span id="0cb6" class="kb kc hi jx b fi kd ke l kf kg">nepochs = 100<br/>batch_size = 256<br/>nbatches = X_train.shape[0] // batch_size<br/>with tf.Session() as sess:<br/>    init.run()<br/>    for i in range(nepochs):<br/>        Tloss = 0<br/>        RCloss = 0<br/>        Latloss = 0<br/>        print ("----------------------------------")<br/>        print ("Epoch # = {}".format(i+1))<br/>        print ("----------------------------------")</span><span id="75fb" class="kb kc hi jx b fi kh ke l kf kg">for j in range(nbatches):<br/>            X_batch = X_train[j*batch_size:(j+1)*batch_size,:]<br/>            totloss, rcloss, latloss, _ = sess.run([loss, <br/>                                                    reconstruction_loss, <br/>                                                    latent_loss,<br/>                                                    training_op], feed_dict = {X:X_batch, alpha: 1.0})<br/>            Tloss += totloss<br/>            RCloss += rcloss<br/>            Latloss += latloss<br/>        <br/>        # we monitor average loss every epoch<br/>        print ("Average Total Loss for epoch = {:.6f}".format(Tloss/nbatches))<br/>        print ("Average Latent loss for epoch = {:.6f}".format(Latloss/nbatches))<br/>        print ("Average Reconstruction loss for epoch = {:.10f}".format(RCloss/nbatches))<br/>    <br/>    # calculating the reconstruction error for test data<br/>    rloss_test = []<br/>    for i in range(X_test.shape[0]):<br/>        X_batch = X_test[i].reshape(-1,29)<br/>        init_error.run()<br/>        rloss = sess.run([rError], feed_dict = {X:X_batch})<br/>        rloss_test.append(rloss[0])</span></pre><p id="a03a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们看到，在100个周期后，总的训练损失在要求的容限内减少了。</p><pre class="jr js jt ju fd jw jx jy jz aw ka bi"><span id="4d0a" class="kb kc hi jx b fi kd ke l kf kg">- - - - - - - - - - - - - - - - - <br/>Epoch # = 98<br/> - - - - - - - - - - - - - - - - - <br/>Average Total Loss for epoch = 0.001899<br/>Average Latent loss for epoch = 0.000231<br/>Average Reconstruction loss for epoch = 0.0016675723<br/> - - - - - - - - - - - - - - - - - <br/>Epoch # = 99<br/> - - - - - - - - - - - - - - - - - <br/>Average Total Loss for epoch = 0.001900<br/>Average Latent loss for epoch = 0.000232<br/>Average Reconstruction loss for epoch = 0.0016675660<br/> - - - - - - - - - - - - - - - - - <br/>Epoch # = 100<br/> - - - - - - - - - - - - - - - - - <br/>Average Total Loss for epoch = 0.001898<br/>Average Latent loss for epoch = 0.000231<br/>Average Reconstruction loss for epoch = 0.0016675662</span></pre><p id="6729" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们定义了效用函数来获得诸如F1分数、精确度、召回率、ROC下面积</p><pre class="jr js jt ju fd jw jx jy jz aw ka bi"><span id="d437" class="kb kc hi jx b fi kd ke l kf kg">def tf_get_metrics(rloss, threshold):<br/>    f1score = np.zeros(threshold.shape)<br/>    precision = np.zeros(threshold.shape)<br/>    recall = np.zeros(threshold.shape)<br/>    rocauc = np.zeros(threshold.shape)</span><span id="b699" class="kb kc hi jx b fi kh ke l kf kg">for i in range(threshold.shape[0]):<br/>        anomaly = pd.DataFrame({'TrueClass':y_test.values, 'ReconstructionLoss':rloss})</span><span id="95bd" class="kb kc hi jx b fi kh ke l kf kg">anomaly["PredictedClass"] = (anomaly['ReconstructionLoss']&gt;threshold[i])*1</span><span id="dc97" class="kb kc hi jx b fi kh ke l kf kg">y_true = anomaly['TrueClass'].values<br/>        y_pred = anomaly['PredictedClass'].values<br/>        f1score[i] = f1_score(y_true, y_pred)<br/>        precision[i] = precision_score(y_true, y_pred)<br/>        recall[i] = recall_score(y_true, y_pred)<br/>        rocauc[i] = roc_auc_score(y_true, y_pred)<br/>    return f1score, precision, recall, rocauc</span></pre><p id="da4d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了找到重建误差的最佳阈值(R_threshold ),我们改变阈值并确定结果F1分数。我们选择R_threshold，使F1分数和ROC下的面积最大化(图1)。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es ki"><img src="../Images/b47fdb1261dac296bc62ae58523be2b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*u352OXp0nD05OBH9dfKnhw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><strong class="bd kj">图一</strong>。我们改变阈值重建误差(R_threshold ),并确定结果精度、召回率、F1分数和ROC下面积。最佳R_threshold是使F1分数和ROC下面积(即ROC AUC分数)最大化的阈值。最佳R_threshold为0.3774，最大F1值为0.467。</figcaption></figure><p id="52c7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">与隔离林的比较</em> </strong></p><p id="9749" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们用隔离森林作为我们最好的F1分数(0.467)的基准(图2)。隔离森林是另一种无监督的机器学习技术。它使用决策树中从根到叶的路径长度作为异常值。发现异常具有较小的路径长度。使用树的集合来确定分数。由于我们的异常检测模型是一个无监督的机器学习模型，我们将我们的结果与另一个众所周知的无监督机器学习模型进行比较。尽管我们没有对隔离林进行严格的超参数调整，但我们尽力使用GridSearchCV方法找到最佳参数。为了简洁起见，我们没有显示这些结果。我们在隔离森林模型中使用半调优超参数。</p><p id="e5da" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将污染(异常数量)固定为0.00172，因为0.172%的数据是欺诈性的。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es kk"><img src="../Images/b4390249bdb0b3bf0f1ec89b0a4b874f.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*Fnf8rdBRdOIY02n80F2zKg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">图二。与隔离林的比较</figcaption></figure><p id="4eda" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"><em class="jo">KDD cup 99–10%数据集的堆积VAE</em></strong></p><p id="349a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们在<strong class="is hj"> <em class="jo"> Keras功能API </em> </strong>中开发模型。对于这个数据集，我们获得了非常令人鼓舞的结果。事实上，我们能够获得与文献中获得的一些顶级F1分数类似的结果(宗，b .，宋，q .，闵，M.R .，程，w .，卢美扎努，c .，赵，d .和陈，h .，2018)。用于无监督异常检测的深度自动编码高斯混合模型。)</p><p id="42a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们开发了一个隐藏层数比Kaggle数据集多的堆叠VAE。我们的网络包括:</p><p id="27f7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">维度为121的输入层</p><p id="8658" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">包含具有神经元100、80、60、40和20的5个隐藏层的编码器</p><p id="85e3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">具有10个神经元的潜在层</p><p id="8529" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">解码器包含具有神经元20、40、60、80和100的5个隐藏层。</p><p id="b390" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输出的维度为121。</p><p id="31d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们使用Keras functional API来构建模型。我们在下面展示代码片段。</p><pre class="jr js jt ju fd jw jx jy jz aw ka bi"><span id="efe1" class="kb kc hi jx b fi kd ke l kf kg">K.clear_session()<br/>input_dim = X_train.shape[1]<br/>encoding_dim = [100, 80, 60, 40, 20]<br/>coding_vector_size = 10<br/>decoding_dim = [20, 40, 60, 80, 100]</span><span id="ce93" class="kb kc hi jx b fi kh ke l kf kg">x = Input(shape = (input_dim, ))</span></pre><p id="383d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">编码器</em> </strong></p><pre class="jr js jt ju fd jw jx jy jz aw ka bi"><span id="0056" class="kb kc hi jx b fi kd ke l kf kg">encoder_layer =  Dense(encoding_dim[0], activation = tf.nn.tanh)(x)<br/>for i in range(1, len(encoding_dim)):<br/>    encoder_layer = Dense(encoding_dim[i], activation =tf.nn.tanh)(encoder_layer)</span></pre><p id="f532" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">潜层</em> </strong></p><pre class="jr js jt ju fd jw jx jy jz aw ka bi"><span id="a0c7" class="kb kc hi jx b fi kd ke l kf kg">#coding layer<br/>z_mean = Dense(coding_vector_size)(encoder_layer)<br/>z_log_var = Dense(coding_vector_size)(encoder_layer)</span><span id="51b7" class="kb kc hi jx b fi kh ke l kf kg">def sampling(args): <br/>    z_mean, z_log_var = args<br/>    batch = K.shape(z_mean)[0]<br/>    dim = K.int_shape(z_mean)[1]<br/>    epsilon = K.random_normal(shape=(batch, dim))<br/>    return z_mean + K.exp(0.5 * z_log_var) * epsilon</span><span id="8e0b" class="kb kc hi jx b fi kh ke l kf kg"># reference: https://keras.io/examples/variational_autoencoder/<br/>z = Lambda(sampling, output_shape=(coding_vector_size,), name='z')([z_mean, z_log_var])</span></pre><p id="f57b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">解码器</em> </strong></p><pre class="jr js jt ju fd jw jx jy jz aw ka bi"><span id="e9d2" class="kb kc hi jx b fi kd ke l kf kg">decoder_layer = Dense(decoding_dim[0], activation = tf.nn.tanh)(z)<br/>for i in range(1, len(decoding_dim)):<br/>    decoder_layer = Dense(decoding_dim[i], activation = tf.nn.tanh)(decoder_layer)<br/>    <br/>x_hat = Dense(input_dim)(decoder_layer)</span></pre><p id="7577" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们使用256的批量大小训练25个时期(图3a)。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es kl"><img src="../Images/3c79b6f0b570a1d51d7d9c5577ea2db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*pABZMPKC2amx-VM874IbvA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">图3a。培训损失与#个时期</figcaption></figure><p id="da15" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们获得最佳F1分数0.942，最佳AUROC分数0.983(图3b)。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es km"><img src="../Images/17bebd3998e1a1efeaf29a50c3f87de7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*eTKsHCLw_bK12eAy4B4LwA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><strong class="bd kj">图3b。</strong>使用Keras functional API对KDD杯99–10%数据集进行了与图1和图2相同的分析。最佳阈值= 0.262，最佳F1分数= 0.942，最佳AUROC = 0.983</figcaption></figure><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es kn"><img src="../Images/6fbeaec4e27271c0c7d7489f8e031495.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*AM65wsUQHLKEh34rwyPTOw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><strong class="bd kj">图3c。</strong>异常和正常数据点的重建误差，我们观察到在重建误差方面正常和异常数据点之间的明显分离。因此使用这种方法获得了高F1分数。</figcaption></figure><p id="c0bb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，堆叠VAE (F1得分= 0.942)的表现比隔离森林(F1得分= 0.488)好得多。</p><p id="5efd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">用于欺诈检测的混合非监督/监督学习模型</em> </strong></p><p id="b820" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们试图探索堆叠VAE是否可以与监督机器学习模型(如随机森林、梯度推进树或XGBoost)结合使用，以帮助提高异常检测准确性(即F1得分)。</p><p id="eb01" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们制定了两种开发混合模型的方法。在第一种方法中，我们使用正态数据(如在第2章中)训练堆叠VAE，并开发用于监督分类模型的新特征。我们开发了两个特性，</p><p id="5687" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.余弦-在输入数据和重建数据之间计算的相似性</p><p id="ecfc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.输入数据和重建数据之间的重建误差</p><p id="9b9c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们使用这些特性中的一个作为我们的监督模型中的附加特性，以探索任何性能优势。我们将我们的结果与没有这个附加特征的基线随机森林模型进行比较。</p><p id="3e60" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在第二种方法中，我们使用异常/欺诈交易数据来训练我们的堆叠VAE。对欺诈交易数据学习潜在空间压缩。然后，我们通过从学习到的潜在空间特征中取样来生成更多的欺诈性数据。因此，我们使用堆叠VAE来增加欺诈性数据。我们生成大约20，000个欺诈数据样本(类别标签= 1)。这被附加到原始数据集，并且结果数据集被用于训练随机森林模型。为了测试数据扩充的好处，我们将我们的结果与未经数据扩充训练的基线随机森林模型进行了比较。</p><p id="b511" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以下是实验总结:</p><ol class=""><li id="ee17" class="ko kp hi is b it iu ix iy jb kq jf kr jj ks jn kt ku kv kw bi translated">我们得到基线F1分数为0.809，AUROC为0.857。我们试图使用堆叠VAE来提高这一基线模型的性能</li><li id="f6df" class="ko kp hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">通过使用堆叠VAE开发的附加特征，我们观察到我们获得了0.802的F1分数和0.851的AUROC。因此，它无助于改进基线模型。</li><li id="61e2" class="ko kp hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">我们看到使用VAE的数据增强已经改善了基线RF分类器的性能。在数据扩充后，我们得到F1分数为0.843，AUROC为0.882，而基线F1分数为0.809，基线AUROC为0.857。因此，使用叠加VAE的数据扩充使F1得分提高了4.08%，AUROC提高了3.03%</li></ol><p id="5aea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">观想潜空间</em> </strong></p><p id="8be0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了可视化2D空间中的潜在向量，我们将编码向量大小减少到2。我们在KDD Cup99数据集中可视化“正常”连接和“攻击/恶意”连接的潜在空间表示</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/27fad9210edee68679bb69e4009fc0c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*UxVhlbOpRch5IV9cX7BWfA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><strong class="bd kj">图四。</strong>在2D可视化潜在空间表征“攻击”连接(深蓝色小圆点)和“正常”(较大的黄色圆圈)。为此，我们使用KDD cup 99–10%数据集。我们看到在“正常”和“攻击”连接之间有一个明显的界限。这可能是我们的堆叠VAE在KDD cup 99–10%数据集上获得更高F1分数的原因。这个堆叠的VAE的编码向量大小只有2，为了可视化的目的，我们减少了它。</figcaption></figure><p id="00e2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">结论与未来工作</em> </strong></p><p id="dd4b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们证明了使用重建误差作为度量的堆叠VAE可以用于检测数据中的异常。此外，堆叠VAE还可以用于生成更多异常数据，以减少监督分类中的类别不平衡。这可以提高模型性能。</p><p id="11f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这项工作的过程中，我们还尝试了生成敌对网络(GANs)。我们将在未来几周跟进这项工作。此外，在文献中很少有使用GANs进行异常检测的研究，这是一个值得进一步探索的有趣课题。</p><p id="289a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">参考文献</em> </strong></p><p id="a295" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[1] Zong，b .，Song，q .，Min，M.R .，Cheng，w .，Lumezanu，c .，Cho，d .和Chen，h .，2018年。用于无监督异常检测的深度自动编码高斯混合模型。</p><p id="513d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[2]理解解开——VAE，arXiv:1804.03599</p><p id="b4ea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[3]用于新颖性检测的生成式深度模型真的更好吗？，奇工场，SIGKDD，2019</p><p id="510d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[4]安、金元和赵成宗。“使用重建概率的基于变分自动编码器的异常检测”IE 2专题讲座(2015):1–18。</p></div></div>    
</body>
</html>