<html>
<head>
<title>AI Writer ✍: Text Generation Using GPT-2 &amp; 🤗Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能作家✍:使用GPT-2 &amp;🤗变形金刚(电影名)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ai-writer-text-generation-using-gpt-2-transformers-4c33d8c52d5a?source=collection_archive---------1-----------------------#2021-04-10">https://medium.com/analytics-vidhya/ai-writer-text-generation-using-gpt-2-transformers-4c33d8c52d5a?source=collection_archive---------1-----------------------#2021-04-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if"><p id="2b18" class="ig ih hi bd ii ij ik il im in io ip dx translated">“我们的智力是我们成为人类的原因，而人工智能是这种素质的延伸。”</p><p id="de3c" class="ig ih hi bd ii ij iq ir is it iu ip dx translated">纽约大学教授Yann LeCun</p></blockquote><figure class="ix iy iz ja jb jc er es paragraph-image"><div class="er es iw"><img src="../Images/cee31d5234b82a6f3e23468c72acf9f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SQoI1cQcVbdNJprzCUz6Bw.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">演职员表:<a class="ae iv" href="https://www.canva.com/" rel="noopener ugc nofollow" target="_blank"> Canva </a></figcaption></figure><blockquote class="jj jk jl"><p id="80d1" class="jm jn jo jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj ip hb bi translated"><strong class="jp hj">T5】概述:T7】</strong></p></blockquote><p id="635b" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">这篇博客是关于人工智能如何从给定的输入序列中生成一串文本行的。对于文本生成，我们在python中使用了两样东西。作为语言模型，我们使用GPT-2大型预训练模型，对于文本生成管道，我们使用拥抱面部变形器管道。</p><blockquote class="jj jk jl"><p id="2807" class="jm jn jo jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj ip hb bi translated">GPT-2 : </p></blockquote><p id="fd05" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">GPT-2是一个基于大型<a class="ae iv" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">转换器</a>的语言模型，拥有15亿个参数，在800万个网页的数据集上进行训练。GPT-2训练有一个简单的目标:预测下一个单词，给定一些文本中的所有以前的单词。数据集的多样性导致这个简单的目标包含跨不同领域的许多任务的自然发生的演示。GPT-2是GPT的直接放大版，参数超过10倍，数据量超过10倍。</p><p id="9ae9" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">基本上，GPT-2是一种用于从给定文本生成下一个文本的语言模型。</p><p id="a074" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">点击阅读更多<a class="ae iv" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"/></p><blockquote class="jj jk jl"><p id="4859" class="jm jn jo jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj ip hb bi translated"><strong class="jp hj">抱脸变形金刚:</strong></p></blockquote><p id="04b5" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">🤗(拥抱脸)Transformers(以前称为PyTorch-transformers和PyTorch-pretrained-bert)为自然语言理解(NLU)和自然语言生成(NLG)提供通用架构(bert、GPT-2、罗伯塔、XLM、DistilBert、XLNet…)，拥有100多种语言中超过32个预训练模型以及TensorFlow 2.0和PyTorch之间的深度互操作性。</p><p id="9d80" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">基本上拥抱面部变形是大型python包，它有一些预定义或预训练的函数、管道和模型。我们可以用它来完成自然语言处理任务。</p><p id="431a" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">GPT-2记号赋予器和模型也包括在内🤗变形金刚。</p><p id="ae13" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">点击此处阅读更多<a class="ae iv" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank"/></p><blockquote class="jj jk jl"><p id="e3e6" class="jm jn jo jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj ip hb bi translated"><strong class="jp hj"> <em class="hi">代号:</em> </strong></p></blockquote><p id="d0fa" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated"><strong class="jp hj">第一步</strong>:首先，我们导入GPT2LMHeadModel进行文本生成，导入GPT2Tokenizer进行文本分词。</p><pre class="kn ko kp kq fd kr ks kt ku aw kv bi"><span id="c29b" class="kw kx hi ks b fi ky kz l la lb">from transformers import GPT2LMHeadModel , GPT2Tokenizer</span></pre><p id="1878" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated"><strong class="jp hj">第二步:</strong>现在我们将模型加载到Jupyter笔记本中。</p><pre class="kn ko kp kq fd kr ks kt ku aw kv bi"><span id="e01e" class="kw kx hi ks b fi ky kz l la lb">tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large') <br/>model = GPT2LMHeadModel.from_pretrained('gpt2-large' , <br/>pad_token_id = tokenizer.eos_token_id)</span></pre><p id="1f59" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">步骤3: 对于文本生成，我们必须首先向我们模型提供一些文本，然后从文本模型生成文本。我们输入到模型中的文本首先要进行预处理。所以在第三步中，我们对文本进行标记。</p><figure class="kn ko kp kq fd jc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lc"><img src="../Images/c08cf5285e8d15e3522849bfccc2b46e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mxWkNsMqKcxZ-VicN01EyA.jpeg"/></div></div></figure><p id="a7a2" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">在第一行中，我们对文本进行编码并返回火炬张量。“pt”表示PyTorch张量。我们的文字转换成了数字的索引。如你所见，解码功能会将这些数字解码成文本。</p><p id="609f" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated"><strong class="jp hj">第四步:</strong>我们使用GPT2LMHeadModel的generate函数生成文本。</p><pre class="kn ko kp kq fd kr ks kt ku aw kv bi"><span id="6391" class="kw kx hi ks b fi ky kz l la lb">output = model.generate(input_ids, <br/>max_length = 10000, <br/>num_beams = 5,<br/>no_repeat_ngram_size  = 2,<br/>early_stopping = True)</span></pre><blockquote class="jj jk jl"><p id="e3f5" class="jm jn jo jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj ip hb bi translated"><strong class="jp hj">论据:</strong></p></blockquote><p id="1163" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated"><strong class="jp hj"> <em class="jo"> max_length: </em> </strong>生成文本的最大字数。</p><p id="550f" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated"><strong class="jp hj"> <em class="jo"> num_beams: </em> </strong>波束搜索通过在每个时间步长保持最可能的<code class="du lh li lj ks b">num_beams</code>个假设，并最终选择具有总体最高概率的假设，降低了遗漏隐藏的高概率单词序列的风险。</p><p id="5651" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated"><em class="jo">波束搜索将总是以比贪婪搜索更高的概率找到输出序列，但不能保证找到最可能的输出。</em></p><p id="8e5a" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated"><strong class="jp hj"><em class="jo">no _ repeat _ ngram _ size:</em></strong>虽然结果可以说更加流畅，但输出仍然包含相同单词序列的重复。一个简单的补救方法是引入<em class="jo">n-grams</em>(<em class="jo">a . k . a</em>n个单词的单词序列)惩罚，如<a class="ae iv" href="https://arxiv.org/abs/1705.04304" rel="noopener ugc nofollow" target="_blank"> Paulus et al. (2017) </a>和<a class="ae iv" href="https://arxiv.org/abs/1701.02810" rel="noopener ugc nofollow" target="_blank"> Klein et al. (2017) </a>所介绍的。最常见的<em class="jo">n-gram</em>惩罚是通过手动将可能创建已经看到的<em class="jo"> n-gram </em>的下一个单词的概率设置为0，来确保没有<em class="jo"> n-gram </em>出现两次。</p><p id="0bc2" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated"><strong class="jp hj"><em class="jo">early _ stopping:</em></strong><code class="du lh li lj ks b">early_stopping=True</code>以便当所有波束假设到达EOS令牌时生成完成。</p><p id="bac3" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">点击阅读更多关于参数<a class="ae iv" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank">的信息</a></p><p id="3707" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">作为回报，我们从该函数中获得令牌id。在解码令牌id之后，我们得到我们的结果。</p><p id="92d3" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">下面的段落是我们的模特写的。</p><p id="f9ac" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">“爱是什么？”是输入序列。</p><pre class="kn ko kp kq fd kr ks kt ku aw kv bi"><span id="fbef" class="kw kx hi ks b fi ky kz l la lb">What is Love?<br/><br/>Love is a state of mind, a feeling, an emotion, or a mental state. It is the feeling of being in love with someone or something. The word "love" is derived from the Latin word for "to love," "lēgēre," which means to feel, to be attracted to, and to love. Love is an emotional state, not a physical one. A person can feel love for another person, but it is not the same thing as having physical feelings for that person. For example, if a man and a woman are in a relationship, the man may feel a strong attraction to the woman. However, he may not feel any physical attraction toward her. This is because he does not have any feelings of love or attraction for her, nor does he have a desire to have physical relations with her at this point in time. He may, however, feel an intense emotional attachment to her as a result of her being a person of interest to him. In other words, she is someone who he is interested in and he wants to get to know more about her and her life. If he were to become romantically involved with that woman, it would not be love at first sight, for he would still have no desire or desire for physical contact with the person he loves. Instead, his love would be based on the fact that he has an interest in the other person's life and that she has a life worth living.<br/><br/><br/>In the case of a romantic relationship between two people who are not related by blood or marriage, there is no physical or emotional attraction between them. They do not share a common life experience or common interests. Rather, they have different life experiences and different interests, which may lead to a different level of physical and emotional intimacy between the two of them, as well as different levels of commitment and commitment to each other. As such, their love is based more on their different lives experiences than on any kind of emotional or romantic attraction that they may have for one another. Therefore, love between a couple who have not been married for a long period of time is called "non-marital love" or "open-ended love." This term is used to describe a type of relationship in which one person is in an open relationship with one or more other people, while the couple is living together as husband and wife. There are many different types of open relationships, each with its own set of rules and rules of conduct. Open relationships can be either monogamous or non-monogamous. Monogamy is when one partner is married to another and has sexual relations only with other members of the opposite sex. Non-polyamorous relationships are relationships where both partners are involved in sexual relationships with people of different genders and/or sexual orientations. Some of these relationships may be open to both men and women and some are open only to one gender or sexual orientation.</span></pre><figure class="kn ko kp kq fd jc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lk"><img src="../Images/12d73b93bd02b5a4af0b61cddb742ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N_-PZMAgBogYyVdkcvu_3A.jpeg"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">输出快照</figcaption></figure><blockquote class="jj jk jl"><p id="8fd4" class="jm jn jo jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj ip hb bi translated"><strong class="jp hj"> <em class="hi">保存生成的文本:</em> </strong></p></blockquote><figure class="kn ko kp kq fd jc er es paragraph-image"><div class="er es ll"><img src="../Images/c248300873355e801a3412f1fdf275be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*2QFV8MIFb1ZPdJQe8yZRbA.jpeg"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">将文本保存在txt文件中</figcaption></figure><blockquote class="jj jk jl"><p id="95cb" class="jm jn jo jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj ip hb bi translated"><strong class="jp hj">代码:</strong></p></blockquote><div class="lm ln ez fb lo lp"><a href="https://github.com/manthan89-py/AI-Blog-Writter" rel="noopener  ugc nofollow" target="_blank"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hj fi z dy lu ea eb lv ed ef hh bi translated">man than 89-py/AI-Blog-writer</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">这个项目是用来生成一个博客帖子使用自然语言处理，拥抱脸变形金刚和GPT-2…</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">github.com</p></div></div><div class="ly l"><div class="lz l ma mb mc ly md jd lp"/></div></div></a></div><blockquote class="jj jk jl"><p id="65fd" class="jm jn jo jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj ip hb bi translated"><strong class="jp hj">结论:</strong></p></blockquote><p id="1495" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated">在这篇博客中，我们学习了使用变形金刚和GPT-2预训练模型生成文本。我们还可以从这个模型中生成许多文章或段落。我希望你能从这个博客中学到一些东西。</p><blockquote class="jj jk jl"><p id="e459" class="jm jn jo jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj ip hb bi translated"><strong class="jp hj">领英:</strong></p></blockquote><div class="lm ln ez fb lo lp"><a href="https://www.linkedin.com/in/manthanbhikadiya/" rel="noopener  ugc nofollow" target="_blank"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hj fi z dy lu ea eb lv ed ef hh bi translated">印度古吉拉特邦苏拉特曼丹·比卡第亚-查罗特科技大学|…</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">查看Manthan Bhikadiya在世界上最大的职业社区LinkedIn上的个人资料。Manthan有一份工作列在…</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">www.linkedin.com</p></div></div></div></a></div><blockquote class="jj jk jl"><p id="02d9" class="jm jn jo jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj ip hb bi translated"><strong class="jp hj">更多博客和项目:</strong></p></blockquote><p id="0273" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated"><strong class="jp hj"> <em class="jo">项目:</em> </strong></p><div class="lm ln ez fb lo lp"><a href="https://github.com/manthan89-py" rel="noopener  ugc nofollow" target="_blank"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hj fi z dy lu ea eb lv ed ef hh bi translated">manthan89-py -概述</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">对AI、深度学习、机器学习、计算机视觉、区块链、Flutter感兴趣😇。做一些竞争性的…</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">github.com</p></div></div><div class="ly l"><div class="me l ma mb mc ly md jd lp"/></div></div></a></div><p id="ab95" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated"><strong class="jp hj"> <em class="jo">博客:</em> </strong></p><div class="lm ln ez fb lo lp"><a href="https://manthan-bhikadiya.medium.com/" rel="noopener follow" target="_blank"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hj fi z dy lu ea eb lv ed ef hh bi translated">曼丹·比卡第亚·🖋-中等</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">机器学习、深度学习、人工智能是未来。我们几乎在每个领域都使用这些技术…</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">manthan-bhikadiya.medium.com</p></div></div><div class="ly l"><div class="mf l ma mb mc ly md jd lp"/></div></div></a></div><blockquote class="jj jk jl"><p id="d764" class="jm jn jo jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj ip hb bi translated"><strong class="jp hj">最终注释:</strong></p></blockquote><p id="6f6f" class="pw-post-body-paragraph jm jn hi jp b jq jr js jt ju jv jw jx kk jz ka kb kl kd ke kf km kh ki kj ip hb bi translated"><strong class="jp hj">感谢阅读！如果你喜欢这篇文章，请点击</strong>👏<strong class="jp hj">尽可能多次按下按钮。这将意味着很多，并鼓励我继续分享我的知识。如果你喜欢我的内容，请在medium上关注我，我会尽可能多地发布博客。</strong></p></div></div>    
</body>
</html>