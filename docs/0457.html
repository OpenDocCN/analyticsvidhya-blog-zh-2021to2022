<html>
<head>
<title>Regularization: A Method to Solve Overfitting in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正则化:解决机器学习中过拟合的一种方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/regularization-a-method-to-solve-overfitting-in-machine-learning-ed5f13647b91?source=collection_archive---------14-----------------------#2021-01-17">https://medium.com/analytics-vidhya/regularization-a-method-to-solve-overfitting-in-machine-learning-ed5f13647b91?source=collection_archive---------14-----------------------#2021-01-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a4ed" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">了解正则化，它是什么，它的类型以及它如何减少方差和解决过度拟合。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/488d95f38876a33e0046451ea4d7602b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*NGQnXqG_XiDTkh1W30u2GA.png"/></div></figure><p id="2b18" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">先决条件</strong>:</p><ul class=""><li id="d0df" class="kb kc hi jh b ji jj jl jm jo kd js ke jw kf ka kg kh ki kj bi translated">在阅读本文之前，请确保您对机器学习环境中的偏差和方差有很好的了解，并且知道过度拟合。如果没有，请先查看<a class="ae kk" rel="noopener" href="/analytics-vidhya/bias-and-variance-613ff5c9ef5c">这篇</a>文章。</li><li id="b7b4" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">你还需要熟悉成本函数和梯度下降。如果没有，请在此阅读关于它们的<a class="ae kk" href="https://towardsdatascience.com/machine-leaning-cost-function-and-gradient-descend-75821535b2ef" rel="noopener" target="_blank">。</a></li></ul></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><p id="9d92" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">您还记得在我们之前的文章<a class="ae kk" rel="noopener" href="/analytics-vidhya/bias-and-variance-613ff5c9ef5c"> <strong class="jh hj"> <em class="kx">偏差和方差</em> </strong> </a>中，我们的一个模型具有低偏差和高方差。我们称之为过度拟合，因为回归线完美地拟合了训练数据，但它未能拟合甚至给出测试数据的良好预测。过拟合是机器学习中最常见的问题之一，它是由于大量输入/特征或者如果训练数据样本很小而导致的。</p><p id="3898" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">高方差模型非常关注数据中的噪声，并且该模型对数据中的任何微小波动都变得非常敏感。</p><p id="676e" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">我们的目标</strong>是让模型对那些波动不那么敏感，并且不要把噪音当作可以学习的东西。</p><p id="398a" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">解决过拟合问题最流行的方法之一是正则化。</p><h2 id="f7f5" class="ky kz hi bd la lb lc ld le lf lg lh li jo lj lk ll js lm ln lo jw lp lq lr ls bi translated">什么是正规化？</h2><p id="5114" class="pw-post-body-paragraph jf jg hi jh b ji lt ij jk jl lu im jn jo lv jq jr js lw ju jv jw lx jy jz ka hb bi translated">简单地说，正则化是某种平滑。</p><h2 id="9fde" class="ky kz hi bd la lb lc ld le lf lg lh li jo lj lk ll js lm ln lo jw lp lq lr ls bi translated">正规化是如何运作的？</h2><p id="e03b" class="pw-post-body-paragraph jf jg hi jh b ji lt ij jk jl lu im jn jo lv jq jr js lw ju jv jw lx jy jz ka hb bi translated">它试图调整目标函数中的参数，希望使模型对波动不那么敏感。它只是在用于评估模型的成本函数中增加了一个额外的项。这一项控制目标函数中的参数，并确保它们不取极值。我们准备看看实际效果如何，大家不要慌:)。</p><p id="2e86" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">一般的优化函数形式是:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/d4c582bbe712d46fcbd13c45a73b552a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*3liBCf9xQ8QySEb-8Z3Lbw.png"/></div></figure><p id="c81c" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">其中:</p><ul class=""><li id="9f83" class="kb kc hi jh b ji jj jl jm jo kd js ke jw kf ka kg kh ki kj bi translated"><strong class="jh hj">θ’</strong>s是正在调整的因子/权重。</li><li id="e65f" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated"><strong class="jh hj">‘λ’</strong>是正则化率，它控制应用于模型的正则化量。它是通过交叉验证选择的。</li><li id="d279" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated"><strong class="jh hj">‘R’</strong>是正则化函数，根据正则化类型不同而不同。</li><li id="0b17" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">该模型试图最小化<strong class="jh hj"> J(θ) </strong>。</li></ul><blockquote class="lz ma mb"><p id="aeff" class="jf jg kx jh b ji jj ij jk jl jm im jn mc jp jq jr md jt ju jv me jx jy jz ka hb bi translated"><strong class="jh hj">注:</strong>如果你不熟悉交叉验证，就把它想象成你只是在尝试不同的值，然后挑选出给你最好结果的值。</p></blockquote><p id="594b" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在实践中，正则化通常会导致稍高的偏差，但会显著降低方差。这就是我们所说的<strong class="jh hj">偏差-方差权衡</strong>。</p><p id="cae5" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">你可能注意到，正则化函数没有考虑模型方程中的常数项'<em class="kx">θo '</em>(x⁰<strong class="jh hj"><em class="kx">的系数</em> </strong>)。</p><blockquote class="lz ma mb"><p id="58e0" class="jf jg kx jh b ji jj ij jk jl jm im jn mc jp jq jr md jt ju jv me jx jy jz ka hb bi translated"><a class="ae kk" href="https://stackoverflow.com/questions/12578336/why-is-the-bias-term-not-regularized-in-ridge-regression" rel="noopener ugc nofollow" target="_blank"> <strong class="jh hj">回答</strong></a><strong class="jh hj">from stack overflow<br/></strong>假设这个简单的模型:<br/> <code class="du mf mg mh mi b">Y = aX + b</code></p><p id="bb4f" class="jf jg kx jh b ji jj ij jk jl jm im jn mc jp jq jr md jt ju jv me jx jy jz ka hb bi translated">正则化是基于这样一种想法，即<code class="du mf mg mh mi b">Y</code>上的过度拟合是由<code class="du mf mg mh mi b">a</code>的“过度特定”引起的。<code class="du mf mg mh mi b">b</code>仅仅抵消了这种关系，因此它的规模对这个问题来说远没有那么重要。此外，如果出于某种原因需要较大的偏移量，将其正则化将会妨碍找到正确的关系。</p><p id="a665" class="jf jg kx jh b ji jj ij jk jl jm im jn mc jp jq jr md jt ju jv me jx jy jz ka hb bi translated">所以答案就在于此:在<code class="du mf mg mh mi b">Y = aX + b</code>中，<code class="du mf mg mh mi b">a</code>乘以解释/自变量，<code class="du mf mg mh mi b">b</code>加在上面。</p><p id="0742" class="jf jg kx jh b ji jj ij jk jl jm im jn mc jp jq jr md jt ju jv me jx jy jz ka hb bi translated"><a class="ae kk" rel="noopener" href="/@shrutijadon10104776/why-we-dont-use-bias-in-regularization-5a86905dfcd6"><strong class="jh hj">Shruti Jadon</strong></a><strong class="jh hj"><br/></strong>虽然我们可以使用它，但在神经网络的情况下它不会有任何区别。但是我们可能会面临将‘θo<em class="hi">’</em>值降低太多的问题，这可能会混淆数据点。因此，在正则化中最好不要使用'θo <em class="hi"> ' </em>。</p></blockquote><h2 id="f7ad" class="ky kz hi bd la lb lc ld le lf lg lh li jo lj lk ll js lm ln lo jw lp lq lr ls bi translated"><strong class="ak">但是，降低θ如何能使模型更好，对波动更不敏感呢？</strong></h2><p id="5749" class="pw-post-body-paragraph jf jg hi jh b ji lt ij jk jl lu im jn jo lv jq jr js lw ju jv jw lx jy jz ka hb bi translated">考虑这个非常简单的例子:<strong class="jh hj"> <em class="kx"> y=ax+b. </em> </strong>其中<strong class="jh hj"><em class="kx">【a】</em></strong>是这条线的斜率，请看下图:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mj"><img src="../Images/907e2179f55545cb3bb7638ea383bade.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*AUMiTrKWJFJk4m8yZcunVg.png"/></div></figure><p id="7e69" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">1-如果斜率为1，那么对于'<em class="kx"> x' </em>中的每一个单位变化，<em class="kx"> y </em>中也会有一个单位变化。</p><p id="2628" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">2-如果斜率为2，那么对于'<em class="kx">' x '</em>，<em class="kx"> 'y' </em>中的半个单位变化，将会改变一个单位。因此，模型对输入数据的变化更敏感，在我们的例子中，输入数据是<em class="kx">‘x’的值。</em></p><p id="dd90" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">3-如果斜率为0.5，那么对于'<em class="kx"> x' </em>'中2个单位的变化，'<em class="kx"> y' </em>将仅变化1个单位。这导致了一个不太敏感的模型，这是我们的目标。</p></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><p id="d631" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">正规化有三种类型。我们将通过一个实例详细讨论它们。</p><h1 id="2619" class="mk kz hi bd la ml mm mn le mo mp mq li io mr ip ll ir ms is lo iu mt iv lr mu bi translated">(1) L1正则化</h1><p id="0cb8" class="pw-post-body-paragraph jf jg hi jh b ji lt ij jk jl lu im jn jo lv jq jr js lw ju jv jw lx jy jz ka hb bi translated">它也被称为<strong class="jh hj">、【L1-诺姆】、<strong class="jh hj">、【拉索回归】、</strong>。【Lasso代表“最小绝对收缩和选择算子”。</strong></p><p id="1690" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Lasso回归增加了模型系数绝对值之和的因子。套索回归试图最小化以下函数:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mv"><img src="../Images/ee8f1f41484752a044fbd98b540ae225.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*3pUyCpOCt4iXZdvnmvei4A.png"/></div><figcaption class="mw mx et er es my mz bd b be z dx translated">Lasso回归成本函数</figcaption></figure><p id="8b54" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">套索回归也可用于要素选择，因为不太重要的要素的coeﬃcients会减少到零。</p><p id="d2ce" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果<strong class="jh hj">‘λ’</strong>很大，那么正则项会很高，代价函数也会很高，所以梯度下降会尽量使所有<em class="kx"> θ的</em>的值都趋向于0以降低代价。</p><p id="fb19" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">不同的λ可以将不同的参数(θ)缩小到零，例如“λ1”可以惩罚θ2、θ3和θ4，而“λ2”可以只惩罚θ2和θ3。另一方面，λ3可能不会惩罚任何θ。</p><p id="491a" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">但是为什么‘λ2’只惩罚θ2和θ3而不惩罚θ4呢？？<br/> </strong>权重(θ)根据以下等式更新:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es na"><img src="../Images/e6cd17321dec2e0872c91fc889d18051.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*AUA6gXgh0xluXRrPfM10Qw.png"/></div></figure><p id="a4e0" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">θj的新值取决于其之前的值和J(θ)对该θj的偏导数。换句话说，我们可以认为θj的变化量取决于因该θj之前的值而发生的成本函数的变化。如果成本高，这意味着该特定θj需要降低。<strong class="jh hj"> λ </strong>的存在迫使<strong class="jh hj"> </strong> θj进一步减小，如果<strong class="jh hj"> λ </strong>非常大，θj可能会变为零，因此与θj相关的特征<strong class="jh hj"> ' <em class="kx"> x </em> ' </strong>将会消失。</p><blockquote class="lz ma mb"><p id="aff6" class="jf jg kx jh b ji jj ij jk jl jm im jn mc jp jq jr md jt ju jv me jx jy jz ka hb bi translated"><em class="hi">所以，Lasso算法本身并不能决定收缩哪些特征。但是，它是套索和交叉验证的组合，允许我们确定最佳回归率‘λ’。</em></p></blockquote><p id="f378" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们通过这个简单的例子来看看它的实际应用:</p><p id="478b" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">假设我们有一个遵循这个等式的模型:<strong class="jh hj"> <em class="kx"> y=x . </em> </strong>我们将使用<strong class="jh hj"> RSS </strong>函数<strong class="jh hj"> <em class="kx"> </em> </strong>(残差平方和)作为我们的损失函数。</p><p id="c0b5" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">假设我们不知道“真实”的关系，我们想为这个问题建立一个回归模型。因此，我们将从大量的功能开始，希望建立一个好的模型。我们的特色将是(<em class="kx"> x，x，x，…，x ⁰ </em>)。我们知道(<em class="kx"> x，…，x ⁰ </em>)的系数应该尽可能小。但让我们看看拉索会做出什么决定。</p><p id="6d1f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">当我们用不同的'<em class="kx"> λ'应用套索回归时，我们得到以下结果:</em></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nb"><img src="../Images/f7799ed2fb01a3335ded111f2cc6065b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*EcBSq3_ZN7ty5FyYEOzg-A.png"/></div></figure><p id="96cc" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">θ的值为:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nc"><img src="../Images/891892daa256705c278edb882c981b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2HOZnnkPC1C9_5Pa85sCyw.png"/></div><figcaption class="mw mx et er es my mz bd b be z dx translated">套索回归的系数。</figcaption></figure><h2 id="ca42" class="ky kz hi bd la lb lc ld le lf lg lh li jo lj lk ll js lm ln lo jw lp lq lr ls bi translated"><strong class="ak">观察:</strong></h2><ul class=""><li id="ff90" class="kb kc hi jh b ji lt jl lu jo nd js ne jw nf ka kg kh ki kj bi translated">当<em class="kx">‘λ’</em>很大(10，100，1000)时，套索消除所有特征。</li><li id="6854" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">当<em class="kx">‘λ’</em>为零时，不应用正则化，所有特征都存在，RSS最低。这就是过拟合的情况。</li><li id="6852" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">在<em class="kx">‘λ’</em>非常低(0.00001)时，Lass考虑了所有特征，但θ的值低于<em class="kx"> λ= </em> 0时的值。</li><li id="2a99" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">在<em class="kx"> λ = </em> (0.01，0.1) <em class="kx">，</em>处，尽管RSS更高，但是模型更好，因为它接近真实关系而没有过度拟合。</li></ul><h1 id="d4a5" class="mk kz hi bd la ml mm mn le mo mp mq li io mr ip ll ir ms is lo iu mt iv lr mu bi translated">(2) L2正规化</h1><p id="729b" class="pw-post-body-paragraph jf jg hi jh b ji lt ij jk jl lu im jn jo lv jq jr js lw ju jv jw lx jy jz ka hb bi translated">也被称为<strong class="jh hj">、【L2-诺姆】、</strong>或<strong class="jh hj">、【岭回归】、</strong></p><p id="8dd3" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">岭回归增加了模型系数平方值之和的因子。岭回归试图最小化以下函数:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ng"><img src="../Images/55288f8189ff989e77669859915d363f.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*dCexkIzgTNSuOFaENOgp2w.png"/></div><figcaption class="mw mx et er es my mz bd b be z dx translated">岭回归优化函数</figcaption></figure><p id="068d" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">L2正则化迫使权重变小，但不会使它们为零。</p><p id="5e56" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们将岭回归应用到前面的例子中:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nb"><img src="../Images/babb677e76ba85296b62cd2ef7f0032d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*AWPpoyae5W9aHJA1YwiDIg.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nc"><img src="../Images/e900a722bf03c2d66d59667241bd6e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CaP7FyREB54jETq1QT9ozQ.png"/></div><figcaption class="mw mx et er es my mz bd b be z dx translated">岭回归中的系数。</figcaption></figure><h2 id="53ca" class="ky kz hi bd la lb lc ld le lf lg lh li jo lj lk ll js lm ln lo jw lp lq lr ls bi translated">观察:</h2><ul class=""><li id="f340" class="kb kc hi jh b ji lt jl lu jo nd js ne jw nf ka kg kh ki kj bi translated">尽管θ的值非常小，但它们都不等于0。</li><li id="4213" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">当<em class="kx">‘λ’</em>为零时，不应用正则化，所有特征都存在，RSS最低。这就是过拟合的情况。</li></ul><p id="1bb6" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">L2对异常值并不稳健，因为平方项放大了异常值的误差差，所以正则化项试图通过惩罚权重来解决这个问题。</p><h1 id="23a4" class="mk kz hi bd la ml mm mn le mo mp mq li io mr ip ll ir ms is lo iu mt iv lr mu bi translated">(3)弹性网正则化</h1><p id="274b" class="pw-post-body-paragraph jf jg hi jh b ji lt ij jk jl lu im jn jo lv jq jr js lw ju jv jw lx jy jz ka hb bi translated">创建弹性网是为了改进Lasso回归，Lasso回归的变量选择过程可能过于依赖数据，因而不稳定。</p><p id="a30e" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> Lasso回归有一定的局限性:</strong></p><ul class=""><li id="649d" class="kb kc hi jh b ji jj jl jm jo kd js ke jw kf ka kg kh ki kj bi translated">如果变量/特征的数量(n)大于训练样本的数量，Lasso最多选择“n”个变量。因此，所选特征/变量的数量受到样本数量的限制。</li><li id="2a85" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">如果某些要素高度相关，Lasso倾向于从中选择一个要素，而忽略其他要素。这可能导致不准确的模型，因为那些特征可能是重要的。</li></ul><p id="892b" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">为了解决这个问题，弹性网结合了脊和套索的特性，并试图最小化以下损失函数:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nh"><img src="../Images/9a99038cba07ccbb19ef45944679ccfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*us3PKTSUK-LcER-Ow1pfbg.png"/></div><figcaption class="mw mx et er es my mz bd b be z dx translated">弹性净成本函数</figcaption></figure><p id="8abe" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">通过添加二次部分，即L2范数:</p><ul class=""><li id="4319" class="kb kc hi jh b ji jj jl jm jo kd js ke jw kf ka kg kh ki kj bi translated">这消除了对所选变量数量的限制。</li><li id="7d91" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">鼓励算法考虑更多变量，不要忽略高度相关的变量/特征。</li><li id="57db" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">让套索更稳定。</li></ul><p id="d55f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">弹性网方法同时执行变量选择和正则化，从而产生最佳输出。</p><p id="f810" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在上面的方程中，'𝞪'被称为'<strong class="jh hj">弹性网混合参数</strong>，它不同于梯度下降方程中的'𝞪'，后者被称为学习率。</p><p id="78e8" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Elastic Net还允许我们调整每个正则化的比例，(因此得名Elastic)。我们可以选择0到1之间的'𝞪'值来优化弹性网。将𝞪设定为0对应于山脊，𝞪 = 1对应于套索。</p><p id="bbcc" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们将这个应用到同一个问题中，选择𝞪 <strong class="jh hj"> =0.5 </strong>，这意味着有50%的套索和50%的山脊。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nb"><img src="../Images/83cc74db558fbfcb8120df9a14077f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*eRHc43A1CGxU4wE1V_m3ew.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nc"><img src="../Images/a188c62b52f3be412a205bea6b69c035.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*foEQWMZFB7ER9VyaozkgYA.png"/></div><figcaption class="mw mx et er es my mz bd b be z dx translated">弹性网的系数。</figcaption></figure><h2 id="32c9" class="ky kz hi bd la lb lc ld le lf lg lh li jo lj lk ll js lm ln lo jw lp lq lr ls bi translated">观察:</h2><ul class=""><li id="8ff5" class="kb kc hi jh b ji lt jl lu jo nd js ne jw nf ka kg kh ki kj bi translated">零的数量比套索的少得多。</li></ul></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><h1 id="8372" class="mk kz hi bd la ml ni mn le mo nj mq li io nk ip ll ir nl is lo iu nm iv lr mu bi translated">什么时候用哪个？</h1><ul class=""><li id="8a69" class="kb kc hi jh b ji lt jl lu jo nd js ne jw nf ka kg kh ki kj bi translated">如果希望保留所有特征并避免模型对训练数据中的噪声/波动过于敏感，则岭回归是合适的。</li><li id="a107" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">如果您认为只有少数特征是有用的，那么使用套索回归或弹性网会更好，因为它们会将不太重要的特征的权重设置为零。但请记住，Lasso提供了更高的稀疏度，即大多数系数都设置为零。</li><li id="49d6" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">通常，弹性网优于套索，因为当要素数量大于训练样本数量或许多要素高度相关时，套索可能会以不可预测的方式表现。</li></ul></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><h1 id="bc5b" class="mk kz hi bd la ml ni mn le mo nj mq li io nk ip ll ir nl is lo iu nm iv lr mu bi translated">附加说明:</h1><ul class=""><li id="08e9" class="kb kc hi jh b ji lt jl lu jo nd js ne jw nf ka kg kh ki kj bi translated">正则化项<strong class="jh hj">应仅在训练</strong>期间添加到成本函数中。一旦模型经过训练，您就可以使用未规范化的性能度量来评估模型的性能。</li><li id="dc35" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">在执行正则化之前缩放(归一化/标准化<strong class="jh hj"> ) </strong>数据很重要，因为它对输入要素的比例很敏感。正则化会影响系数的大小，因此所有预测变量(要素)必须在相同的比例上。</li><li id="ecac" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">当存在相关变量时，Lasso和Ridge的作用不同。Ridge以相同的方式处理相关变量，(即，它以类似的方式缩小它们的系数，并将其全部用于拟合)，而在Lasso中，其中一个相关变量/预测值具有较大的系数，而其余的几乎为零。(即Lasso从拟合中删除共线变量)。</li><li id="5a89" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">正则化减少了异常值对解的影响。如果离群值使变量/系数具有非常高的值，那么正则化项通过增加损失函数值来降低它。</li></ul></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><p id="e66e" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">感谢阅读，我希望你喜欢这篇文章，如果你有任何意见，请告诉我。</p><h2 id="57b5" class="ky kz hi bd la lb lc ld le lf lg lh li jo lj lk ll js lm ln lo jw lp lq lr ls bi translated">参考资料:</h2><ul class=""><li id="53c5" class="kb kc hi jh b ji lt jl lu jo nd js ne jw nf ka kg kh ki kj bi translated">Datacamp: <a class="ae kk" href="https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net" rel="noopener ugc nofollow" target="_blank">教程-山脊-套索-弹力网</a></li><li id="f40e" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">IQ.opengeus: <a class="ae kk" href="https://iq.opengenus.org/elastic-net-regularization/" rel="noopener ugc nofollow" target="_blank">弹性网络正则化</a></li><li id="cf59" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">Stanford . edu:<a class="ae kk" href="https://web.stanford.edu/~hastie/TALKS/enet_talk.pdf" rel="noopener ugc nofollow" target="_blank">enet _ talk . pdf</a></li><li id="a73a" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">Quora: <a class="ae kk" href="https://qr.ae/pNDDW9" rel="noopener ugc nofollow" target="_blank">为什么岭回归不强制系数正好为零</a></li><li id="27e5" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">复数视线:<a class="ae kk" href="https://www.pluralsight.com/guides/linear-lasso-ridge-regression-scikit-learn" rel="noopener ugc nofollow" target="_blank">线性-套索-岭回归</a></li><li id="e2d0" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">奥雷利图书馆:<a class="ae kk" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ch04.html" rel="noopener ugc nofollow" target="_blank">动手机器学习</a></li><li id="0cf8" class="kb kc hi jh b ji kl jl km jo kn js ko jw kp ka kg kh ki kj bi translated">安德烈·布尔科夫的百页机器学习书:<a class="ae kk" href="https://www.dropbox.com/s/nije38rerpfa18o/Chapter5.pdf?dl=0" rel="noopener ugc nofollow" target="_blank">第五章</a></li></ul></div></div>    
</body>
</html>