<html>
<head>
<title>What does it mean by Bidirectional LSTM?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">双向LSTM是什么意思？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-does-it-mean-by-bidirectional-lstm-63d6838e34d9?source=collection_archive---------8-----------------------#2021-02-09">https://medium.com/analytics-vidhya/what-does-it-mean-by-bidirectional-lstm-63d6838e34d9?source=collection_archive---------8-----------------------#2021-02-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4980" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这改变了旧的方法，从两个方向输入，这样它就能记住长序列。</p><p id="231d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我之前的文章中，我们讨论了RNN、LSTM和格鲁。现在，LSTM仍然存在一定的局限性，因为它不能在更长的时间内记住上下文。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/8fe65bb18f817575a6c6daaa7d903b31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QB_aL8gSN2Nn8k5I.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来自:<a class="ae jt" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>LSTM的顺序处理</figcaption></figure><p id="0f46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以在这个LSTM体系结构中看到，信息仍然需要通过更长的路径传递。引入LSTM和GRU来克服消失梯度和顺序数据存储的问题，但是两者的结构都具有多个顺序路径。因此，消失梯度问题仍然存在。此外，LSTM和GRU可以记住10和100的序列，但不能记住1000或更多。</p><p id="b1b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">双向网络</strong></p><p id="3c58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，当我们处理长序列数据时，模型也需要学习未来和过去单词之间的关系。我们需要以那种方式发送数据。为了解决这个问题，引入了双向网络。我们可以使用双向网络与LSTM和RNN，但由于的限制</p><p id="0c4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在双向LSTM中，我们从从右到左和从左到右两个方向给出输入。<strong class="ih hj">请注意，这不是反向传播，这只是来自两边的输入。</strong>因此，问题是如果我们有2个输入，数据如何组合成输出。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ju"><img src="../Images/a89e95102484665c287bf7bee1609982.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZsTT3zzTNGF-6OsR.jpg"/></div></div></figure><p id="5e9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常，在正常的LSTM网络中，我们直接获取输出，如图1所示，但是在双向LSTM网络中，每一级的前向和后向层的输出被提供给作为神经网络的激活层，并且考虑该激活层的输出。这个输出也包含过去和未来单词的信息或关系。</p><p id="1271" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们举个例子，假设我们有这样一个句子</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jv"><img src="../Images/91d26a479ca064e586cea82beeedf66e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/0*u1exzGPAtza4D50o.gif"/></div></figure><p id="1d44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们不能用普通的RNN网络预测下一个字，但是这可以在双向RNN网络中解决。此外，RNN网络可以是LSTM或格鲁</p><p id="ac8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">双向RNN在Tensorflow(Keras)上的实现</strong></p><p id="de21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Tensorflow实现</p></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><pre class="je jf jg jh fd kd ke kf kg aw kh bi"><span id="24f9" class="ki kj hi ke b fi kk kl l km kn"># Import Necessary Libraries<br/>import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import confusion_matrix,accuracy_score<br/>import tensorflow<br/>from tensorflow.keras.layers import Embedding,LSTM,Dense,Bidirectional<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences<br/>from tensorflow.keras.preprocessing.text import one_hot<br/>from tensorflow.keras.models import Sequential<br/>import nltk<br/>import re<br/>from nltk.corpus import stopwords<br/>from nltk.stem.porter import PorterStemmer</span><span id="59a2" class="ki kj hi ke b fi ko kl l km kn"># Load the dataset <br/>dataset can be found on kaggle Fake news classifier data <a class="ae jt" href="https://www.kaggle.com/c/fake-news/data#" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/fake-news/data#</a></span><span id="bbe2" class="ki kj hi ke b fi ko kl l km kn">data = pd.read_csv('train.csv')<br/># check how many values are none and we have to drop it.<br/>data.isnull().sum(axis=0)</span></pre><p id="a658" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果我们会得到低于null的值，所以我们需要删除它。</p><pre class="je jf jg jh fd kd ke kf kg aw kh bi"><span id="7a14" class="ki kj hi ke b fi kk kl l km kn">id           0<br/>title      558<br/>author    1957<br/>text        39<br/>label        0<br/>dtype: int64</span><span id="e6ae" class="ki kj hi ke b fi ko kl l km kn">df = data.dropna()</span></pre><p id="11f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经删除了所有的空值，这样就不会影响模型的准确性。现在我们将X和Y定义为自变量和因变量</p><pre class="je jf jg jh fd kd ke kf kg aw kh bi"><span id="94f3" class="ki kj hi ke b fi kk kl l km kn">x = df.drop(‘label’,axis=1)<br/>y = df[‘label’]</span></pre><p id="0507" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，NLP的关键部分是文本预处理，我们使用NLTK库对自变量进行预处理。我们将使用re库来删除标点符号，然后我们将从停用字词列表中传递数据，然后对数据进行词干处理。</p><pre class="je jf jg jh fd kd ke kf kg aw kh bi"><span id="5e09" class="ki kj hi ke b fi kk kl l km kn">sentences = x.copy()<br/>sentences.reset_index(inplace=True)<br/>nltk.download('stopwords')</span><span id="b09b" class="ki kj hi ke b fi ko kl l km kn">ps = PorterStemmer()<br/>corpus = []<br/>for i in range(0, len(sentences)):<br/>    review = re.sub('[^a-zA-Z]', ' ', sentences['title'][i])<br/>    review = review.lower()<br/>    review = review.split()<br/>    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]<br/>    review = ' '.join(review)<br/>    corpus.append(review)</span><span id="5f85" class="ki kj hi ke b fi ko kl l km kn">#here we can see that corpus contains the words after preprocessing done.<br/>corpus[:10]</span><span id="5616" class="ki kj hi ke b fi ko kl l km kn">['hous dem aid even see comey letter jason chaffetz tweet',<br/> 'flynn hillari clinton big woman campu breitbart',<br/> 'truth might get fire',<br/> 'civilian kill singl us airstrik identifi',<br/> 'iranian woman jail fiction unpublish stori woman stone death adulteri',<br/> 'jacki mason hollywood would love trump bomb north korea lack tran bathroom exclus video breitbart',<br/> 'beno hamon win french socialist parti presidenti nomin new york time',<br/> 'back channel plan ukrain russia courtesi trump associ new york time',<br/> 'obama organ action partner soro link indivis disrupt trump agenda',<br/> 'bbc comedi sketch real housew isi caus outrag']</span></pre><p id="af9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将对数据进行热编码，因为我们有单词列表，我们将得到索引w.r.t vocab_size</p><pre class="je jf jg jh fd kd ke kf kg aw kh bi"><span id="2f3e" class="ki kj hi ke b fi kk kl l km kn">vocab_size = 5000<br/>onehot = [one_hot(words,vocab_size) for words in corpus]<br/>onehot[:10]</span><span id="e62b" class="ki kj hi ke b fi ko kl l km kn">3090, 3921, 277, 3803, 561, 2494, 2349],<br/> [792, 2085, 3099, 206],<br/> [1083, 2836, 2939, 3433, 2700, 2344],<br/> [4308, 561, 750, 666, 2017, 2368, 561, 415, 869, 208],<br/> [4623,<br/>  3529,<br/>  4621,<br/>  2659,<br/>  3924,<br/>  4115,<br/>  2845,<br/>  2475,<br/>  4603,<br/>  4988,<br/>  1575,<br/>  959,<br/>  92,<br/>  1630,<br/>  2349],<br/> [4348, 1735, 189, 352, 3582, 757, 60, 4393, 373, 1561, 684],<br/> [1554, 61, 1640, 1548, 2048, 1673, 4115, 2910, 373, 1561, 684],<br/> [2484, 2659, 3720, 3690, 509, 4227, 4554, 310, 4115, 1168],<br/> [653, 1048, 646, 2146, 2026, 1062, 3558, 4097]]</span></pre><p id="94cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步是填充，因为我们的句子大小不同，所以我们必须做填充，使它们长度相等。我们可以使用前置或后置填充。</p><pre class="je jf jg jh fd kd ke kf kg aw kh bi"><span id="fcfd" class="ki kj hi ke b fi kk kl l km kn">length = 30<br/>embedding = pad_sequences(onehot,maxlen=length,padding=’pre’)<br/>embedding[:10]</span><span id="6238" class="ki kj hi ke b fi ko kl l km kn">array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>           0,    0,    0,    0,    0,    0,    0,    0,    0, 1076, 2004,<br/>        3855, 3964, 1251, 1177, 2432, 4548, 4821, 3157],<br/>       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>           0, 3090, 3921,  277, 3803,  561, 2494, 2349],<br/>       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>           0,    0,    0,    0,  792, 2085, 3099,  206],<br/>       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>           0,    0, 1083, 2836, 2939, 3433, 2700, 2344],<br/>       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>           0,    0,    0,    0,    0,    0,    0,    0,    0, 4308,  561,<br/>         750,  666, 2017, 2368,  561,  415,  869,  208],<br/>       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>           0,    0,    0,    0, 4623, 3529, 4621, 2659, 3924, 4115, 2845,<br/>        2475, 4603, 4988, 1575,  959,   92, 1630, 2349],<br/>       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>           0,    0,    0,    0,    0,    0,    0,    0, 4348, 1735,  189,<br/>         352, 3582,  757,   60, 4393,  373, 1561,  684],<br/>       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>           0,    0,    0,    0,    0,    0,    0,    0, 1554,   61, 1640,<br/>        1548, 2048, 1673, 4115, 2910,  373, 1561,  684],<br/>       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>           0,    0,    0,    0,    0,    0,    0,    0,    0, 2484, 2659,<br/>        3720, 3690,  509, 4227, 4554,  310, 4115, 1168],<br/>       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,<br/>         653, 1048,  646, 2146, 2026, 1062, 3558, 4097]])</span></pre><p id="5c51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从以上所有输出中，我们可以看到我们的句子是如何为LSTM输入进行预处理的。现在我们可以实现模型来训练我们的数据。</p><pre class="je jf jg jh fd kd ke kf kg aw kh bi"><span id="0698" class="ki kj hi ke b fi kk kl l km kn">embedding_vector_features = 40<br/>model = Sequential()<br/>model.add(Embedding(vocab_size,embedding_vector_features,input_length=length))<br/>model.add(Bidirectional(LSTM(100)))<br/>model.add(Dense(1,activation=’sigmoid’))<br/>model.compile(loss=’binary_crossentropy’,optimizer=’adam’,metrics=[‘accuracy’])<br/>print(model.summary())</span><span id="1c74" class="ki kj hi ke b fi ko kl l km kn"># Model Summary <br/>Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding (Embedding)        (None, 30, 40)            200000    <br/>_________________________________________________________________<br/>bidirectional (Bidirectional (None, 200)               112800    <br/>_________________________________________________________________<br/>dense (Dense)                (None, 1)                 201       <br/>=================================================================<br/>Total params: 313,001<br/>Trainable params: 313,001<br/>Non-trainable params: 0<br/>_________________________________________________________________<br/>None</span><span id="f5df" class="ki kj hi ke b fi ko kl l km kn">#Split the data into training and testing dataset</span><span id="3806" class="ki kj hi ke b fi ko kl l km kn">X = np.array(embedding)<br/>Y = np.array(y)<br/>X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.33,random_state=42)<br/>model.fit(X_train,y_train,batch_size=64,epochs=20,validation_data=(X_test,y_test))</span></pre><p id="88cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在20个纪元后训练，我得到了下面的结果。</p><pre class="je jf jg jh fd kd ke kf kg aw kh bi"><span id="0291" class="ki kj hi ke b fi kk kl l km kn">Epoch 1/20<br/>192/192 [==============================] - 9s 45ms/step - loss: 0.3227 - accuracy: 0.8448 - val_loss: 0.2034 - val_accuracy: 0.9152<br/>Epoch 2/20<br/>192/192 [==============================] - 8s 39ms/step - loss: 0.1433 - accuracy: 0.9411 - val_loss: 0.1821 - val_accuracy: 0.9238<br/>Epoch 3/20<br/>192/192 [==============================] - 8s 44ms/step - loss: 0.0897 - accuracy: 0.9668 - val_loss: 0.2037 - val_accuracy: 0.9218<br/>Epoch 4/20<br/>192/192 [==============================] - 10s 54ms/step - loss: 0.0573 - accuracy: 0.9803 - val_loss: 0.2556 - val_accuracy: 0.9193<br/>Epoch 5/20<br/>192/192 [==============================] - 10s 52ms/step - loss: 0.0307 - accuracy: 0.9903 - val_loss: 0.3273 - val_accuracy: 0.9158<br/>Epoch 6/20<br/>192/192 [==============================] - 9s 49ms/step - loss: 0.0170 - accuracy: 0.9956 - val_loss: 0.3483 - val_accuracy: 0.9168<br/>Epoch 7/20<br/>192/192 [==============================] - 9s 47ms/step - loss: 0.0138 - accuracy: 0.9950 - val_loss: 0.4654 - val_accuracy: 0.9099<br/>Epoch 8/20<br/>192/192 [==============================] - 10s 55ms/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.5041 - val_accuracy: 0.9117<br/>Epoch 9/20<br/>192/192 [==============================] - 11s 58ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.5213 - val_accuracy: 0.9100<br/>Epoch 10/20<br/>192/192 [==============================] - 14s 72ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.5411 - val_accuracy: 0.9079<br/>Epoch 11/20<br/>192/192 [==============================] - 14s 73ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.5063 - val_accuracy: 0.9122<br/>Epoch 12/20<br/>192/192 [==============================] - 13s 68ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 0.5529 - val_accuracy: 0.9087<br/>Epoch 13/20<br/>192/192 [==============================] - 13s 65ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.5330 - val_accuracy: 0.9095<br/>Epoch 14/20<br/>192/192 [==============================] - 13s 69ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.5599 - val_accuracy: 0.9118<br/>Epoch 15/20<br/>192/192 [==============================] - 11s 55ms/step - loss: 2.7169e-04 - accuracy: 1.0000 - val_loss: 0.6287 - val_accuracy: 0.9099<br/>Epoch 16/20<br/>192/192 [==============================] - 10s 51ms/step - loss: 1.1285e-04 - accuracy: 1.0000 - val_loss: 0.6487 - val_accuracy: 0.9097<br/>Epoch 17/20<br/>192/192 [==============================] - 11s 58ms/step - loss: 8.3286e-05 - accuracy: 1.0000 - val_loss: 0.6669 - val_accuracy: 0.9097<br/>Epoch 18/20<br/>192/192 [==============================] - 9s 48ms/step - loss: 6.5950e-05 - accuracy: 1.0000 - val_loss: 0.6818 - val_accuracy: 0.9092<br/>Epoch 19/20<br/>192/192 [==============================] - 9s 49ms/step - loss: 5.4030e-05 - accuracy: 1.0000 - val_loss: 0.6961 - val_accuracy: 0.9090<br/>Epoch 20/20<br/>192/192 [==============================] - 11s 59ms/step - loss: 4.4982e-05 - accuracy: 1.0000 - val_loss: 0.7104 - val_accuracy: 0.9094</span></pre><p id="7c31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们最终可以在测试数据上测试我们的模型，并且可以检查混淆矩阵。</p><pre class="je jf jg jh fd kd ke kf kg aw kh bi"><span id="de56" class="ki kj hi ke b fi kk kl l km kn">y_pred=model.predict_classes(X_test)<br/>CM = confusion_matrix(y_test,y_pred)<br/>score = accuracy_score(y_test,y_pred)<br/>print(CM)<br/>print(score)</span><span id="0dea" class="ki kj hi ke b fi ko kl l km kn">[[3122  297]<br/> [ 250 2366]]<br/>0.9093620546810274</span></pre><p id="5386" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有90%的准确率。然而，我们可以通过处理不同的参数来提高这种准确性，如vocab_size、句子长度、LSTM层大小、时期数。</p><p id="0216" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考</strong></p><h1 id="dc50" class="kp kj hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">参考</h1><p id="40ec" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">[1] S. Hochreiter，J. Schmidhuber，<a class="ae jt" href="https://www.researchgate.net/publication/13853244_Long_Short-term_Memory" rel="noopener ugc nofollow" target="_blank">长短期记忆</a> (1997)，神经计算</p><p id="d3c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]http://colah.github.io/posts/2015-08-Understanding-LSTMs/<a class="ae jt" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"/></p><p id="2989" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]<a class="ae jt" href="https://www.youtube.com/watch?v=MXPh_lMRwAI&amp;list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm&amp;index=22" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=MXPh_lMRwAI&amp;list = plzotaelrmxvmdj 5 sqbck 2 lim 0 hhqvwnzm&amp;index = 22</a></p><p id="74d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jt" rel="noopener" href="/@raghavaggarwal0089/bi-lstm-bc3d68da8bd0">https://medium.com/@raghavaggarwal0089/bi-lstm-bc3d68da8bd0</a></p></div></div>    
</body>
</html>