# 微调在 MLM 上训练的用于文本生成的 RoBERTa 编码器-解码器模型

> 原文：<https://medium.com/analytics-vidhya/fine-tune-a-roberta-encoder-decoder-model-trained-on-mlm-for-text-generation-23da5f3c1858?source=collection_archive---------2----------------------->

从产品描述生成名称的模型的第 2 部分

![](img/620aaafffc0632c1d657c1c3f93ffde3.png)

泽维尔·冯·埃拉赫在 [Unsplash](https://unsplash.com/s/photos/text-creation?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片

# 我们的问题描述

正如我在以前的帖子中提到的，几个星期以来，我一直在研究 Huggingface 中的不同模型和替代方案，以训练一个文本生成模型。我们有一个产品及其描述的候选名单，我们的目标是获得产品的名称。最后，为了加深 Huggingface transformers 的使用，我决定用一种不同的方法来解决这个问题，一种编码器-解码器模型。也许这不是最好的选择，但我想学习拥抱变形金刚的新知识。

首先，我必须承认，使用像 BERT 或 RoBERTa 这样的编码器模型，文本生成问题通常不会用这种解决方案来解决。但是在这个问题中，我们不打算生成“自由文本”，所以我们可以简化我们的任务。我们正在从产品描述中寻找组成产品名称的词的子集，我们的完整词汇存在于输入数据中。从这个角度来看，我们可以将产品描述编码成向量表示，解码成文本名称。因此，使用编码器-解码器是评估的一个选项。

我们的问题可以表示为序列到序列的问题，其中我们需要找到输入序列(产品描述)到输出序列(产品名称)的映射。在 Huggingface 的博客文章“为编码器-解码器模型利用预先训练的语言模型检查点”中，您可以找到使用 BERT 或 GTP2 transformers 模型构建许多编码器-解码器模型的深入解释和实验。强烈推荐你去读。

在之前的一篇文章中，我们创建了一个自定义的标记器并训练了一个 RoBERTa 模型，“[从头开始创建一个标记器并训练一个 Huggingface RoBERTa 模型](/analytics-vidhya/create-a-tokenizer-and-train-a-huggingface-roberta-model-from-scratch-f3ed1138180c)”。现在，我们将使用该训练模型来构建编码器-解码器模型，并将在我们的数据集上微调这个新模型。我们将只描述更有趣的代码部分，你可以在我的 Github [repo](https://github.com/edumunozsala/RoBERTa_Encoder_Decoder_Product_Names) 上找到完整的代码。

# 数据集

正如我们在我的帖子中提到的，我们的数据集包含大约 31，000 件商品，关于一家重要零售商的衣服，包括一个长的产品描述和一个短的产品名称，这是我们的目标变量。首先，我们执行一个探索性的数据分析，我们可以观察到具有异常值的行的数量很少。这是一个私有数据集，所以我们不能公开它。

# 编码器-解码器架构

在许多机器学习解决方案中，我们可以找到这样的架构，其中 DNN 的第一部分产生输入的向量表示，第二部分采用该向量和输入来产生预期的结果。这基本上是一个编码器-解码器模型，通过 RNN，它们成为解决 NLP 任务的一个非常强大的工具。

> 类似于基于 RNN 的编码器-解码器模型，基于变换器的编码器-解码器模型包括编码器和解码器，它们都是*剩余注意力块*的堆栈。基于变换器的编码器-解码器模型的关键创新在于，这种剩余注意力块可以处理可变长度的输入序列，而不呈现循环结构。不依赖于递归结构，允许基于变压器的编码器-解码器高度并行化，…" [2] " [基于变压器的编码器-解码器模型](https://huggingface.co/blog/encoder-decoder)"

该架构的目的是找到输入序列和其目标输出序列之间的映射函数。在变换器的情况下，编码器将输入序列编码成隐藏状态序列，然后解码器采用目标序列和编码的隐藏状态来模拟或学习映射函数。但是编码器仅在第一步中工作，在接下来的步骤中，解码器接收下一个目标令牌，并重用来自步骤 1 中产生的编码器的隐藏状态。关于上面提到的[2]“[基于变形金刚的编解码模型](https://huggingface.co/blog/encoder-decoder)，可以找到更深层次的解释。

考虑到这一点，我们可以将编码器-解码器模型视为仅编码器模型(如 BERT)和仅解码器模型(如 GPT-2 ),两者结合产生目标序列。目前，我们在 Huggingface 的模型中心有许多预训练模型可用，因此评估的第一个选项是使用这些预训练模型来构建我们的编码器-解码器，并根据我们的特定任务进行微调。

在文章“[利用编码器-解码器模型的预训练语言模型检查点](https://huggingface.co/blog/warm-starting-encoder-decoder)”，[1]中，解释并评估了许多组合，但总而言之，您需要考虑如何初始化模型:

*   仅从预训练的编码器初始化编码器和解码器
*   从仅编码器检查点初始化编码器，从仅解码器初始化解码器
*   从预训练的编码器或仅解码器模型中仅初始化编码器或仅解码器
*   是否与解码器共享编码器权重
*   编码器和解码器使用哪种变压器型号，可能是 BERT、GPT-2 或 RoBERTa 型号。

在我们的实验中，我们将尝试一种 *RoBERTaShared* 策略，其中编码器和解码器都基于 RoBERTa，它们共享它们的权重。这种结合已经在许多任务中显示出巨大的效果。

![](img/2e0f3ed9eed135782baecbbaf2640fd1.png)

照片由 [Danist Soh](https://unsplash.com/@danist07?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 在 [Unsplash](https://unsplash.com/s/photos/build?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄

# 从预训练的 RoBERTa 模型创建编码器-解码器模型

## 在我们特定的语言上加载经过训练的标记器

正如我们之前提到的，我们已经使用屏蔽语言造型手法从头开始训练了一个标记器和一个 RoBERTa 模型，试图将我们的模型集中在我们的特定任务上。现在，我们可以使用这个预训练模型来配置我们的编码器-解码器。

让我向您展示我们将为此项目导入哪些库。

第一步是加载我们需要应用的记号赋予器，以生成我们的输入和目标记号，并将它们转换成文本数据的向量表示。

## 准备并创建数据集

在下一步中，我们需要为模型训练生成数据集。使用加载的标记器，我们标记文本数据，应用填充技术，并截断输入和输出序列。记住，我们可以为输入数据定义一个最大长度，为输出数据定义一个不同的长度。最后，我们定义一个批量大小，将数据集分成几批进行训练和评估。

RoBERTa 是 BERT 模型的一个变种，所以期望输入是相似的:T0 和 T1。但是罗伯塔没有像伯特一样的`token_type_ids`参数，这没有意义，因为它不是为下一句预测而训练的，所以你不需要指出哪个标记属于哪个片段。只需用分隔符号`**tokenizer.sep_token**`(或`**</s>**`)分隔您的段。

## 创建罗伯塔编码器-解码器模型

我们正在基于本系列第 1 部分中构建的预训练模型构建我们的模型，由于 Hugginface 的库和包装器，创建模型非常简单。我们只需要从 Hugginface transformers 库的`EncoderDecoderModel`类中调用方法`from_encoder_decoder_pretrained`。我们设置编码器和解码器预训练模型，传递我们保存 RoBERTa 训练模型的文件夹，并指示我们希望“绑定”两个模型的权重，以便它们共享它们的权重。

一旦模型建立起来，我们需要指定一系列的参数，比如特殊的记号，来开始一个句子或 bos，结束一个句子或 eos。

需要声明的非常重要的参数是那些与**解码策略**相关的参数。任何文本生成器都需要*定义如何从所有可用的可能性中选择下一个输出或标记*。这种体系结构试图在考虑先前令牌序列的情况下对下一个令牌的概率进行建模。在这一点上，我们生成了下一个令牌的概率，我们需要一个方法来选择它们中的一个。

## 解码策略

我们不打算分析所有的可能性，但我们想提一下 Huggingface 库提供的一些选择。

我们的第一个也是最直观的近似方法是 **Greddy 搜索**，在每一步中，我们都选择词汇中概率最高的单词。不幸的是，这种策略倾向于产生模式，并在循环依赖中重复自己。这可能是一个非常确定的方法，而这不是文本生成器应该有的。并且这种策略产生了一个问题，即一个高概率标记“隐藏”在一个低概率标记之后，该低概率标记在句子顺序中位于它之前。我们会陷入一个次优的解决方案。

另一种试图最小化这个问题的方法是**波束搜索**，它保持多个可能的路径或序列来近似解。从这束结果中，应用贪婪搜索来允许我们确定性地近似最可能的单词序列。这不是一个非常有创造性的过程，但它很适合翻译任务，在语言之间翻译句子不需要很大的创造力。如果我们想创作一个故事或一段对话，我们需要“探索”替代方案，减少决定论。

**采样技术**是一种在我们的文本生成过程中引入随机选择的方法。但是使用随机抽样，我们无法控制产出是如何产生的，而“引导”抽样是一种更好的方法。 ***“一个简单，但是非常强大的抽样方案，叫做 Top-K 抽样。在 Top-K 采样中，K 个最可能的下一个单词被过滤，并且概率质量仅在这 K 个下一个单词*** ”，【3】中重新分配。*这具有约束生成过程以选择模型本身认为在上下文中更合理的单词的效果*。

在我们的训练阶段，我们使用 10 个波束应用波束搜索策略，并惩罚重复的 n 元语法和冗长的输出。

![](img/cdf147e51f0c32c1d20d0e86bef7770b.png)

图片设计图片设计[图片设计](https://pixabay.com/es/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2158737)

# 训练编码器-解码器

Huggingface 库的`Trainer`组件将以一种非常简单的方式训练我们的新模型，只需要几行代码。`Trainer` API 提供了我们训练几乎所有 transformer 模型所需的所有功能，我们根本不需要编写训练循环。

当训练一个 ML 模型时，我们需要一个评估度量，我们选择了 **Rouge 评分度量**，它用于评估翻译和摘要任务。我们把我们的问题作为一个总结，从产品描述到名称。这个度量标准似乎是正确的选择，但是可能有其他有趣的替代方法。

datasets 库为我们提供了度量，所以我们加载它并定义一个`compute_metrics`方法，在这里我们计算目标文本、产品描述和预测文本的度量。

现在是时候设置训练参数了:batch_size、训练时期、保存模型等。然后我们可以**实例化一个** `**Seq2SeqTrainer**`，它是我们提到的`Trainer`对象的子类，选择要训练的模型、训练参数、度量计算、训练和评估数据集。

我们准备只对几个时期训练我们的编码器-解码器模型，最后，我们保存微调的编码器-解码器模型。

# 在测试数据集上评估模型

一旦我们对模型进行了训练，我们就可以用它来为我们的产品命名，并检查我们在目标任务上的微调过程的结果。

我们加载一个没有标签的测试数据集，并删除包含空值的行。

然后，我们从一个保存的版本中加载标记化器和微调模型。

为了改善结果，我们将定义两种方法来生成文本，使用波束搜索解码策略和随机采样，我们将应用这两种方法并比较结果。

现在，我们可以使用波束搜索策略和 Top-k 采样技术对测试数据集进行预测。

我们准备使用波束搜索或采样技术作为解码策略来可视化结果。

*产品描述*:圆领长袖宽松连衣裙，褶饰细节，背部开衩，身高 69.6 寸

*用 BS* 命名:飘逸的带褶连衣裙用

*Top-K 打样*:带褶印花连衣裙

# **结论**

您已经看到了如何基于变压器构建编码器-解码器，并对其进行微调以解决特定的任务。这是一个简单而强大的工具，你可以用它来定制你的 ML 解决方案。这只是一个例子，可能还可以大大改进。我希望你喜欢它，并享受做一些研究的乐趣。

代码可以在我的 GitHub repo 中找到，这里是[笔记本](https://github.com/edumunozsala/RoBERTa_Encoder_Decoder_Product_Names/blob/main/RoBERTa%20Encoder%20Decoder%20MLM%20FineTuned%20for%20Text%20generation.ipynb)的链接。

# 参考

[1] Patrick von Platen，“[利用编码器-解码器模型的预训练语言模型检查点](https://huggingface.co/blog/warm-starting-encoder-decoder)”，2020 年 11 月 2 日，Huggingface post。

[2] Patrick von Platen，“基于[变压器的编码器-解码器模型](https://huggingface.co/blog/encoder-decoder)”，2020 年 10 月 2 日，Huggingface post。

[3] Patrick von Platen，“[如何生成文本:用变形金刚](https://huggingface.co/blog/how-to-generate)使用不同的解码方法进行语言生成”，Mar 2.020，Huggingface post。

[文本生成的解码策略](https://kirubarajan.com/blog/decoding)，Dic 2.020