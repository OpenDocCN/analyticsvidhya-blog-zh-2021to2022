<html>
<head>
<title>Text Classification — From Bag-of-Words to BERT — Part 4 ( Convolutional Neural Network)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本分类—从词袋到BERT —第4部分(卷积神经网络)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-4-convolutional-neural-network-53aa63941ade?source=collection_archive---------7-----------------------#2021-01-03">https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-4-convolutional-neural-network-53aa63941ade?source=collection_archive---------7-----------------------#2021-01-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/370340608fa46dfb3b4b82edd97a4901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pljQPTEKYANaeFLR"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的<a class="ae it" href="https://unsplash.com/@thisisengineering?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> ThisisEngineering RAEng </a></figcaption></figure></div><div class="ab cl iu iv go iw" role="separator"><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz"/></div><div class="ha hb hc hd he"><p id="3898" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">这个故事是一系列文本分类的一部分——从词袋到BERT在名为“<a class="ae it" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge" rel="noopener ugc nofollow" target="_blank"> <em class="jz">有毒评论分类挑战”</em> </a> <strong class="jd hi"> <em class="jz">的Kaggle比赛上实施多种方法。</em> </strong>在这场比赛中，我们面临的挑战是建立一个多头模型，能够检测不同类型的毒性，如<em class="jz">威胁、淫秽、侮辱和基于身份的仇恨。如果你还没看过之前的报道，那就去看看吧</em></p><p id="5bae" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9" rel="noopener">第一部分(BagOfWords) </a></p><p id="7ff0" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-2-word2vec-35c8c3b34ee3" rel="noopener">第二部分(Word2Vec) </a></p><p id="be47" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-3-fasttext-8313e7a14fce" rel="noopener">第三部分(快速文本)</a></p><p id="f8ef" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">在之前的故事中(<a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-3-fasttext-8313e7a14fce" rel="noopener">第三部分(fastText) </a>)，我们使用了fastText库来生成句子的嵌入以及输出变量的多标签文本分类。</p><p id="865b" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">在这个项目中，我们将使用Keras库(TensorFlow上的一个包装器)为多标签文本分类创建一维卷积神经网络(CNN)。我们还将看看模型的一些背景工作。</p><h1 id="0de5" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak"> <em class="ky">直觉</em> </strong></h1><p id="5af7" class="pw-post-body-paragraph jb jc hh jd b je kz jg jh ji la jk jl jm lb jo jp jq lc js jt ju ld jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz">怎么开始的？</em></strong>CNN最早是由Yann LeCun在20世纪80年代提出的，用来识别手写数字。但ConvNets仍然处于观望状态，因为他们面临着一个严重的问题，即需要大量的数据和计算资源来有效地处理大型图像。</p><p id="1988" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">2012年，AlexNet表明，也许是时候重新审视深度学习了，因为它已经赢得了各种比赛。大型数据集和大量计算资源的可用性使研究人员能够创建复杂的CNN，可以执行以前不可能完成的计算机视觉任务。</p><p id="9807" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz">什么是CNN？</em> </strong></p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es le"><img src="../Images/b08de85915ea06039a2a2ff0367b0349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cxsUf5Djz8VuKQQulhhfKg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="http://www.jetir.org/papers/JETIR2004207.pdf" rel="noopener ugc nofollow" target="_blank">http://www.jetir.org/papers/JETIR2004207.pdf</a></figcaption></figure><p id="601b" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">在上图所示的ConvNet中有四种主要操作:</p><p id="9cfb" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz"> 1。卷积</em> </strong></p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lj"><img src="../Images/4f8aba34507c05537424961c7763b984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/1*AbSQYpvN45OXNBrttHvX2A.gif"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://miro.medium.com/max/1920/1*D6iRfzDkz-sEzyjYoVZ73w.gif" rel="noopener">https://miro . medium . com/max/1920/1 * D6iRfzDkz-sezyjyovz 73w . gif</a></figcaption></figure><p id="d874" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">这一层是CNN的心脏。CNN使用特征映射/核来学习输入的特征。例如，图像中的核[[1，0，-1]，[1，0，-1]，[1，0，-1]]检测图像中的垂直线。神奇的是我们不需要指定内核。我们只需要提到内核的数量，模型就会自己学习内核，就像普通人工神经网络中的权重一样。总体思路是，随着Conv和池图层数量的不断增加，模型能够检测的复杂要素也越多。第一层识别简单的东西，如线条/颜色，随后的层识别更复杂的图案。</p><p id="17c0" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz"> 2。非线性</em> </strong></p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lk"><img src="../Images/0d04568397a5229f0a9150155feb63f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vBfna7lXrdMHa-1DWIyp-Q.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://machinelearningknowledge.ai/activation-functions-neural-network/#Why_we_need_Activation_Functions_in_Neural_Network" rel="noopener ugc nofollow" target="_blank">https://machine learning knowledge . ai/Activation-Functions-Neural-Network/# Why _ we _ needle _ Activation _ Functions _ in _ Neural _ Network</a></figcaption></figure><p id="f8c2" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">没有激活函数的ANN层将只产生所有输入及其权重之间的点积之和。通过使用合适的非线性激活函数，我们可以帮助神经网络理解这种非线性关系。这是一篇关于<a class="ae it" href="https://machinelearningknowledge.ai/activation-functions-neural-network/#Why_we_need_Activation_Functions_in_Neural_Network" rel="noopener ugc nofollow" target="_blank">创新功能</a>的深度博客。</p><p id="2bfa" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">在二进制分类问题的情况下，在输出神经元中使用<em class="jz"> sigmoid </em>函数，将输入信号转换为0到1的范围，以便将其解释为概率。</p><p id="e922" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">在隐藏层中，ReLU通常比其他函数更受欢迎，因为它训练神经网络的速度比其他函数快几倍，而且不会显著降低泛化精度。ReLU或整流线性单元，应用非饱和激活函数f(x)=max(0，x)。它也不会遭受消失的类似梯度的Sigmoid和Tanh激活函数的现象</p><p id="e5f8" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz"> 3。</em> </strong>混合抽样或次级抽样</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es ll"><img src="../Images/41091c6f5393840d7fd3ff0b8141b29c.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/1*GYHLNYztjguE9jF3GfcXow.gif"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://miro.medium.com/max/700/0*5xJdbktSufBcH7n5.gif" rel="noopener">https://miro.medium.com/max/700/0*5xJdbktSufBcH7n5.gif</a></figcaption></figure><p id="86d6" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">池图层用于减少要素地图的维度。因此，它减少了要学习的参数数量和网络中执行的计算量。汇集图层汇总了由卷积图层生成的要素地图区域中的要素。</p><p id="9170" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">这里有一个关于层类型的深度博客<a class="ae it" href="https://www.machinecurve.com/index.php/2020/01/30/what-are-max-pooling-average-pooling-global-max-pooling-and-global-average-pooling/" rel="noopener ugc nofollow" target="_blank">池</a>层</p><p id="2a0a" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi">4<em class="jz">。</em>分类(全连接层)</strong></p><p id="e051" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">最后，在几个卷积和最大池层之后，NN中的高级推理通过完全连接的层来完成。FC层根据前几层提取的特征进行分类。通常，这一层是传统的人工神经网络，它将权重与输入和传递激活相乘，以给出输出</p><p id="90b4" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">对完整代码感兴趣的人，可以在这里找到<a class="ae it" href="https://www.kaggle.com/anirbansen3027/jtcc-cnn" rel="noopener ugc nofollow" target="_blank">。那么让我们深入研究代码👨‍💻</a></p><h1 id="3e64" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">实施</strong></h1><h2 id="3614" class="lm kb hh bd kc ln lo lp kg lq lr ls kk jm lt lu ko jq lv lw ks ju lx ly kw lz bi translated"><strong class="ak"> <em class="ky"> 1。读取数据集</em> </strong></h2><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ma"><img src="../Images/0543de08e345f6b77adbe68a2a0922ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1oEsT7kJsaTm2saQmCu36g.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">提醒一下，这是训练数据的样子</figcaption></figure><h2 id="7490" class="lm kb hh bd kc ln lo lp kg lq lr ls kk jm lt lu ko jq lv lw ks ju lx ly kw lz bi translated"><strong class="ak">②<em class="ky">。文本预处理</em> </strong></h2><p id="2c63" class="pw-post-body-paragraph jb jc hh jd b je kz jg jh ji la jk jl jm lb jo jp jq lc js jt ju ld jw jx jy ha bi translated">文本数据必须编码为数字，才能用作ML/DL模型的输入或输出。Keras库提供了一些基本工具来帮助我们准备文本数据。我们将使用Tokenizer类，这是一个实用程序类，允许对文本语料库进行矢量化，方法是将每个文本转换为整数序列或向量，其中每个标记的系数可以是二进制的，基于字数，基于tf-idf。这将是一个3步流程:</p><p id="b413" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> 1。初始化记号赋予器类</strong></p><ul class=""><li id="341e" class="mb mc hh jd b je jf ji jj jm md jq me ju mf jy mg mh mi mj bi translated">默认情况下，所有标点符号都被删除，将文本转换为空格分隔的单词序列(单词可能包含'字符)。然后，这些序列被分割成记号列表。然后它们将被索引或矢量化。0是用于填充的保留索引。</li><li id="dc34" class="mb mc hh jd b je mk ji ml jm mm jq mn ju mo jy mg mh mi mj bi translated">我们将num_words设置为MAX_NUM_WORDS (20000 ),这是基于词频要保留的最大字数。只会保留最常见的num_words-1个单词。</li></ul><p id="57af" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> 2。调用fit_on_texts函数</strong> —根据文本列表更新内部词汇</p><p id="43b8" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">该方法基于词频创建词汇索引。所以如果你给它这样的话，“猫坐在垫子上。”它将创建一个字典s . t . word _ index[" the "]= 1；word_index["cat"] = 2它是word -&gt; index字典，所以每个单词都有一个唯一的整数值。所以较小的整数意味着更频繁的单词(通常前几个是停用词)。</p><p id="10b5" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> 3。调用texts_to_sequences函数</strong> —将文本中的每个文本转换为一个整数序列</p><p id="abd5" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">所以它基本上是将文本中的每个单词替换为word_index字典中相应的整数值。</p><p id="9930" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><em class="jz">注意:</em> <em class="jz">在fit_on_texts之后，实质上是为词汇表创建一个word_index矩阵，我们可以做两件事情中的任何一件</em></p><ul class=""><li id="8d4b" class="mb mc hh jd b je jf ji jj jm md jq me ju mf jy mg mh mi mj bi translated"><em class="jz">使用嵌入层时使用的文本序列</em></li><li id="7121" class="mb mc hh jd b je mk ji ml jm mm jq mn ju mo jy mg mh mi mj bi translated"><em class="jz"> text_to_matrix将文本转换成一个单词包</em></li></ul><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="96d2" class="lm kb hh mq b fi mu mv l mw mx">#Initializing the class<br/>tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)<br/>#Updates internal vocabulary based on a list of texts.<br/>tokenizer.fit_on_texts(train_texts)<br/>#Transforms each text in texts to a sequence of integers.<br/>train_sequences = tokenizer.texts_to_sequences(train_texts)<br/>test_sequences = tokenizer.texts_to_sequences(test_texts)<br/>word_index = tokenizer.word_index<br/>print(“Length of word Index:”, len(word_index))<br/>print(“First 5 elements in the word_index dictionary:”, dict(list(word_index.items())[0: 5]) )<br/>print(“First comment text in training set:\n”, train_sequences[0])</span></pre><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es my"><img src="../Images/ef546c6029f2e5f75e03cbf20763c148.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*8scoVJnbmLXsXzhEfWnKPw.jpeg"/></div></figure><p id="2012" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">既然我们已经标记了注释文本，我们需要填充句子以使所有的句子长度相等。</p><p id="90e6" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi">为什么会这样？</strong>深度学习库假设你的数据是矢量化的表示。在可变长度序列预测问题的情况下，这要求您的数据被转换为每个序列具有相同的长度。这种矢量化允许代码针对您选择的深度学习算法高效地批量执行矩阵运算。</p><p id="ff36" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">这也是在计算机视觉中完成的，在计算机视觉中，我们通常倾向于将所有图像的大小调整到固定的大小，这将是神经网络的输入大小</p><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="88e1" class="lm kb hh mq b fi mu mv l mw mx">#Pad tokenized sequences<br/>trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)<br/>test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)<br/>print(“Shape of padded sequence list:\n”, trainvalid_data.shape)<br/>print(“First comment text in training set — 0 for padding — only last 50 sequences as the rest are paddings:\n”, trainvalid_data[0][-50:])</span></pre><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mz"><img src="../Images/1fa37c32677814963625e8c07696613d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*tzS-6OyOUf1ZZZX9I_p-0A.png"/></div></div></figure><h2 id="37c3" class="lm kb hh bd kc ln lo lp kg lq lr ls kk jm lt lu ko jq lv lw ks ju lx ly kw lz bi translated"><em class="ky"> 3。定义1D CNN模型</em></h2><p id="93fa" class="pw-post-body-paragraph jb jc hh jd b je kz jg jh ji la jk jl jm lb jo jp jq lc js jt ju ld jw jx jy ha bi translated">在Keras中，定义模型最简单的方法是启动一个顺序模型类，并不断添加所需的层。序列模型是一个简单的层堆栈，其中每一层都有一个输入张量和一个输出张量。</p><p id="1bb5" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">文档分类的标准模型是使用嵌入层作为输入，接着是1D卷积神经网络、池层，然后是预测输出层。我们使用了1个嵌入层、3组卷积和池化层以及2组密集层。我们可以使用预先训练的嵌入(如Word2Vec)来生成大小为词汇*嵌入维度的嵌入矩阵，或者训练新的嵌入，将其作为输入层与其他权重一起使用。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es na"><img src="../Images/b7265e4a6457c9f4b5f751eb9f7b3f61.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*4MZJO8iKpkdXRzPZ66admQ.png"/></div></figure><p id="2803" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi">Conv1D</strong>:CNN是为图像分类开发的，其中模型接受表示图像像素和颜色通道的二维输入。同样的过程可以应用于1D数据序列。该模型从序列数据中提取特征，并映射序列的内部特征。CNN考虑单词的接近度来创建可训练的模式。卷积层中的内核大小/高度定义了当卷积通过输入文本文档时要考虑的单词数，提供了分组参数。在我们的例子中，它将一次考虑5个单词，而在图像中，它将一次考虑2个单词</p><p id="e6e3" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">最大池层将合并卷积层的输出。</p><p id="61a4" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">我们在输出层使用sigmoid激活。sigmoid函数为我们提供了每个输出节点的0到1之间的概率得分。如果我们使用softmax，它会给出跨输出节点的概率分布，加1。</p><p id="186b" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">总的来说，</p><ul class=""><li id="8712" class="mb mc hh jd b je jf ji jj jm md jq me ju mf jy mg mh mi mj bi translated">对于二进制分类，我们可以有1个输出单元，在输出层使用sigmoid激活，并使用二进制交叉熵损失</li><li id="0c34" class="mb mc hh jd b je mk ji ml jm mm jq mn ju mo jy mg mh mi mj bi translated">对于多类分类，我们可以有N个输出单元，在输出层使用softmax激活，并使用分类交叉熵损失</li><li id="ccd6" class="mb mc hh jd b je mk ji ml jm mm jq mn ju mo jy mg mh mi mj bi translated">对于多标签分类，我们可以有N个输出单元，在输出层使用sigmoid激活，并使用二进制交叉熵损失</li></ul><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es nb"><img src="../Images/cacd2fb340f2b6aa5cf1c987fff4f7ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*MzgMtDR-xGgexkECzV763g.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">cnn _ model.summar的结果</figcaption></figure><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="2376" class="lm kb hh mq b fi mu mv l mw mx">cnn_model = Sequential()<br/>cnn_model.add(Embedding(MAX_NUM_WORDS, 128))<br/>cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = “relu”))<br/>cnn_model.add(MaxPooling1D(pool_size = 5))<br/>cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = “relu”))<br/>cnn_model.add(MaxPooling1D(pool_size = 5))<br/>cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = “relu”))<br/>cnn_model.add(GlobalMaxPool1D())<br/>cnn_model.add(Dense(units = 128, activation = ‘relu’))<br/>cnn_model.add(Dense(units = 6, activation = ‘sigmoid’))<br/>print(cnn_model.summary())</span></pre></div><div class="ab cl iu iv go iw" role="separator"><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz"/></div><div class="ha hb hc hd he"><h2 id="852a" class="lm kb hh bd kc ln lo lp kg lq lr ls kk jm lt lu ko jq lv lw ks ju lx ly kw lz bi translated">4.编译并拟合CNN模型</h2><p id="320f" class="pw-post-body-paragraph jb jc hh jd b je kz jg jh ji la jk jl jm lb jo jp jq lc js jt ju ld jw jx jy ha bi translated">在开始训练模型之前，我们需要对其进行配置。我们需要提到<em class="jz">损失函数</em>，它将用于计算每次迭代的误差，<em class="jz">优化器</em>，它将指定如何更新权重，以及<em class="jz">指标</em>，它将由模型在训练和测试期间进行评估</p><p id="fec5" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">在拟合/训练模型时，除了训练集，我们还传递以下参数:</p><ul class=""><li id="b6bb" class="mb mc hh jd b je jf ji jj jm md jq me ju mf jy mg mh mi mj bi translated">batch_size =一次通过网络的样本数量。</li><li id="ff5d" class="mb mc hh jd b je mk ji ml jm mm jq mn ju mo jy mg mh mi mj bi translated">epochs =整个训练样本集通过网络的次数</li><li id="5514" class="mb mc hh jd b je mk ji ml jm mm jq mn ju mo jy mg mh mi mj bi translated">validation_data =将用于在每个时期结束时评估损失和任何模型指标的数据集。这套不会用于训练。</li></ul><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="63d1" class="lm kb hh mq b fi mu mv l mw mx">#Configures the model for training<br/>cnn_model.compile(loss <strong class="mq hi">=</strong> "binary_crossentropy", optimizer <strong class="mq hi">=</strong> "adam", metrics <strong class="mq hi">=</strong> ["AUC"])<br/>#Split the dataset into train and validation set for training and evaludating the model<br/>X_train, X_val, y_train, y_val <strong class="mq hi">=</strong> train_test_split(trainvalid_data, train_labels, shuffle <strong class="mq hi">=</strong> <strong class="mq hi">True</strong>, random_state <strong class="mq hi">=</strong> 123)<br/>print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)<br/>#Trains the model for a fixed number of epochs (iterations on a dataset)<br/>history <strong class="mq hi">=</strong> cnn_model.fit(X_train, y_train, batch_size <strong class="mq hi">=</strong> 128, epochs <strong class="mq hi">=</strong> 1, validation_data <strong class="mq hi">=</strong> (X_val, y_val)</span></pre><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es nc"><img src="../Images/2f4ddce307761f467ec188920179ece9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*f43iuS4M2nE-6M1kBt0NnA.png"/></div></figure><h2 id="461f" class="lm kb hh bd kc ln lo lp kg lq lr ls kk jm lt lu ko jq lv lw ks ju lx ly kw lz bi translated">5.<strong class="ak">改进的结果和范围</strong></h2><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es nd"><img src="../Images/b1742d6860f8e3b86b7f386a481827ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Z9w8C4SK9B5ShMAPQoZJJA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Kaggle排行榜分数(相当惊人的一个时代吧？)</figcaption></figure><ul class=""><li id="d60f" class="mb mc hh jd b je jf ji jj jm md jq me ju mf jy mg mh mi mj bi translated">使用一个多频道CNN，它可以同时查看不同长度的句子(例如，内核大小为3、5和7)</li><li id="cdf6" class="mb mc hh jd b je mk ji ml jm mm jq mn ju mo jy mg mh mi mj bi translated">调整模型层和超参数以提高性能</li></ul><p id="38d4" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">这是关于CNN的。下一个将是关于<strong class="jd hi">长</strong> - <strong class="jd hi">短时记忆</strong> ( <strong class="jd hi"> LSTM </strong>)，这是对自然语言处理中自然使用的rnn的改进。在那之前保持安全。同样，整个代码出现在<a class="ae it" href="https://www.kaggle.com/anirbansen3027/jtcc-cnn" rel="noopener ugc nofollow" target="_blank">(这里)</a>。请以回答和鼓掌的形式提供您的反馈:)</p></div></div>    
</body>
</html>