<html>
<head>
<title>Activation Functions — All You Need To Know!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">激活功能—您需要知道的一切！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-all-you-need-to-know-355a850d025e?source=collection_archive---------0-----------------------#2021-02-13">https://medium.com/analytics-vidhya/activation-functions-all-you-need-to-know-355a850d025e?source=collection_archive---------0-----------------------#2021-02-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/3e68f60f515fc96d073c8f89eb8c1143.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Lh0znG1oiJW15_J3xewA1Q.gif"/></div></div></figure><h1 id="ca84" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">那么，什么是激活函数呢？</h1><p id="0264" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">一个<strong class="jq hj">激活函数</strong>是一个<strong class="jq hj">函数</strong>，它被添加到一个<strong class="jq hj">人工神经网络</strong>中，以便帮助网络学习<strong class="jq hj">数据</strong>中的复杂模式。当与我们大脑中基于神经元的模型进行比较时，<strong class="jq hj">激活功能</strong>最终决定向下一个神经元发射什么。</p><blockquote class="km kn ko"><p id="be43" class="jo jp kp jq b jr kq jt ju jv kr jx jy ks kt kb kc ku kv kf kg kw kx kj kk kl hb bi translated">在<a class="ae ky" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj">人工神经网络</strong> </a>中，给定一个输入或一组输入，节点的<strong class="jq hj">激活函数</strong>定义该节点的输出。一个标准的<a class="ae ky" href="https://en.wikipedia.org/wiki/Integrated_circuit" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj">集成电路</strong> </a>可以被看作是一个<a class="ae ky" href="https://en.wikipedia.org/wiki/Digital_electronics" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj">数字网络</strong> </a>的激活功能，根据输入可以是“开”(1)或“关”(0)。— <strong class="jq hj">维基百科</strong></p></blockquote><p id="8f82" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">所以，总结一下，<strong class="jq hj">激活函数是决定神经网络输出的数学方程。</strong></p><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/a96ac39a219467a7942608073c96043c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*kyNdmhtc5oo4Q0-wtk7NTQ.png"/></div></figure><blockquote class="le"><p id="cd97" class="lf lg hi bd lh li lj lk ll lm ln kl dx translated">在这个博客中，我们将了解广泛使用的激活函数，其工作背后的后端数学，并讨论如何为您特定的深度学习问题陈述选择最佳的方法。</p></blockquote><p id="df0e" class="pw-post-body-paragraph jo jp hi jq b jr lo jt ju jv lp jx jy jz lq kb kc kd lr kf kg kh ls kj kk kl hb bi translated">在深入讨论不同类型的激活功能之前，让我们快速了解一下人工神经元是如何工作的</p><figure class="la lb lc ld fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/52dee64cc14859bad401f93cd8c87bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k9pQ6T6claVVn39ZMcqlKg.png"/></div></div></figure><p id="4edc" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">上述过程的数学可视化可以表示为</p><figure class="la lb lc ld fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/344266c27f51a84c5b04634813123303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0wDAsK-gy8NMxMyimPjw_g.png"/></div></div></figure><h2 id="1b95" class="lv ir hi bd is lw lx ly iw lz ma mb ja jz mc md je kd me mf ji kh mg mh jm mi bi translated">到目前为止，您一定非常熟悉人工神经网络的工作过程以及激活函数在该过程中的作用！</h2><h1 id="56ac" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">所以，拿起你的咖啡🥤，我们开始吧！</h1><h1 id="5f16" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">1.乙状结肠激活功能-</h1><figure class="la lb lc ld fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/e1a1733c618b28be641ee9dba64a2a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-NCZJnLMt-sX-zCJ4DRPYQ.png"/></div></div></figure><p id="77bc" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated"><strong class="jq hj">S形函数</strong>看起来像一条<strong class="jq hj"> S形</strong>曲线。</p><p id="79a7" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">公式:<strong class="jq hj"> f(z) = 1/(1+ e^-z) </strong></p><p id="ae0e" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">我们为什么以及何时使用Sigmoid激活函数？</p><ol class=""><li id="9fad" class="mk ml hi jq b jr kq jv kr jz mm kd mn kh mo kl mp mq mr ms bi translated">sigmoid函数<strong class="jq hj">的输出范围在0和1 </strong>之间。由于输出值介于0和1之间，因此<strong class="jq hj">对每个神经元的输出进行标准化</strong>。</li><li id="ffb4" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl mp mq mr ms bi translated">专门用于我们必须<strong class="jq hj">预测概率</strong>作为输出的模型。由于任何事情的概率只存在于<strong class="jq hj"> 0和1之间，</strong> sigmoid是<strong class="jq hj">完美的</strong>选择。</li><li id="bc7f" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl mp mq mr ms bi translated"><strong class="jq hj">平滑梯度</strong>，防止输出值“跳跃”。</li><li id="5597" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl mp mq mr ms bi translated">函数是<strong class="jq hj">可微的</strong>。这意味着，我们可以在任意两点找到s形曲线的斜率。</li><li id="cfb6" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl mp mq mr ms bi translated"><strong class="jq hj">清晰的预测</strong>，即非常接近1或0。</li></ol><p id="6b03" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">乙状结肠激活功能的<strong class="jq hj">缺点</strong>有哪些？</p><ol class=""><li id="79ec" class="mk ml hi jq b jr kq jv kr jz mm kd mn kh mo kl mp mq mr ms bi translated">容易出现梯度消失(当<strong class="jq hj"> sigmoid </strong>函数值过高或过低时，导数变得很小即&lt; &lt; 1。这导致<strong class="jq hj">消失梯度</strong>和深度网络的较差学习。)</li><li id="c415" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl mp mq mr ms bi translated">函数输出<strong class="jq hj">不是以0为中心，</strong>会降低权重更新的效率。</li><li id="a78a" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl mp mq mr ms bi translated">sigmoid函数执行指数运算，这对于计算机来说比较慢。</li></ol><h1 id="6b38" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">2.双曲正切激活函数</h1><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es my"><img src="../Images/4ac0f56527610991ee0d5af2ccbb44ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*WeuJzmlt3iNVWsUsvf24Eg.png"/></div></figure><p id="a256" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">双曲正切激活函数也是<strong class="jq hj">S形的。</strong></p><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/4611b35114572ca370da3e803285d0d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*SBcMLQ2rP77M9uuXYtX0hA.png"/></div><figcaption class="na nb et er es nc nd bd b be z dx translated">双曲正切激活函数公式</figcaption></figure><p id="dd30" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated"><strong class="jq hj"> Tanh是双曲正切函数</strong>。双曲正切函数和sigmoid函数的曲线比较相似。但是它比sigmoid函数有一些优势。让我们看看它是什么。</p><p id="0d28" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">为什么tanh <strong class="jq hj">比</strong>更适合乙状结肠激活功能？</p><figure class="la lb lc ld fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ne"><img src="../Images/390c879418d9717f466a477468c4f58a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p5Vbj8YE3l4JdgKOy6YqVg.jpeg"/></div></div></figure><ol class=""><li id="ef98" class="mk ml hi jq b jr kq jv kr jz mm kd mn kh mo kl mp mq mr ms bi translated">首先，输入大或小时，输出<strong class="jq hj">几乎平滑，梯度小</strong>，不利于权重更新。区别在于输出间隔。tanh的输出区间为1，整个函数以<strong class="jq hj"> 0为中心，比sigmoid好。</strong></li><li id="47db" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl mp mq mr ms bi translated"><strong class="jq hj">的主要优点</strong>是<strong class="jq hj">负输入</strong>将被强映射<strong class="jq hj">负输入</strong>并且<strong class="jq hj">零输入</strong>将被映射<strong class="jq hj">接近双曲正切图中的零</strong>。</li></ol><blockquote class="km kn ko"><p id="55b0" class="jo jp kp jq b jr kq jt ju jv kr jx jy ks kt kb kc ku kv kf kg kw kx kj kk kl hb bi translated">N <strong class="jq hj">注:</strong>在一般的<strong class="jq hj">二元分类问题</strong>中，tanh函数用于<strong class="jq hj">隐藏层</strong>，sigmoid函数用于<strong class="jq hj">输出层</strong>。不过这些都是<strong class="jq hj">不是静态的</strong>，具体要用的激活函数要根据具体问题具体分析，不然就要看调试了。</p></blockquote><h1 id="89e9" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">3.ReLU(整流线性单位)激活功能-</h1><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/0dd29165296d387ebeda0495ea85f6dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*jOU3PnNiB0YIH1Y_t-iXng.png"/></div></figure><p id="23f2" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">ReLU被半整流(从底部)。当z小于零时f(z)为零，当z大于或等于零时f(z)等于z。</p><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/a9b8b77e686cb9530d4a5b6d07ce6ea5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*Eav-4gyK6dKvV4MGf-Gq6w.png"/></div></figure><p id="8094" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated"><strong class="jq hj">范围:</strong>【0到无穷大】</p><p id="0980" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated"><strong class="jq hj"> ReLU(整流线性单元)</strong>函数是目前<strong class="jq hj">比深度学习中其他激活函数更流行的</strong>激活函数。</p><p id="8212" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">与sigmoid函数和tanh函数相比，它具有以下<strong class="jq hj">优势</strong>:</p><ol class=""><li id="ba1d" class="mk ml hi jq b jr kq jv kr jz mm kd mn kh mo kl mp mq mr ms bi translated">当输入为正时，不存在<strong class="jq hj">梯度饱和问题。</strong></li><li id="b1b3" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl mp mq mr ms bi translated">计算速度要比<strong class="jq hj">快得多</strong>。ReLU函数只有线性关系。无论是向前还是向后，都比乙状结肠和tanh快很多。(Sigmoid和tanh需要计算指数，会比较慢。)</li></ol><p id="3d2d" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">当然还有<strong class="jq hj">的缺点:</strong></p><p id="3bf6" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">1) <strong class="jq hj">死ReLU问题</strong> -当输入为负时，ReLU完全<strong class="jq hj">不活动</strong>，这意味着一旦输入负数，<strong class="jq hj"> ReLU就会死</strong>。这样，在正向传播过程中，就不是问题了。有些区域敏感，有些区域不敏感。但是在<strong class="jq hj">反向传播</strong>过程中，如果输入一个负数，<strong class="jq hj">梯度将完全为零，</strong>与sigmoid函数和tanh函数有<strong class="jq hj">相同的</strong>问题。</p><p id="254d" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">2)我们发现ReLU函数的输出不是0就是正数，也就是说ReLU函数<strong class="jq hj">不是以0为中心的函数。</strong></p><h1 id="3220" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">4.泄漏ReLU激活功能-</h1><p id="4ab2" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">一个激活功能<strong class="jq hj">专门设计用来补偿</strong>的将死的ReLU问题。</p><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/784e1555e49bebf5bd273f92451ee76b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZGazDurOnSW8sBsSNVEVHA.jpeg"/></div><figcaption class="na nb et er es nc nd bd b be z dx translated">ReLU与泄漏ReLU</figcaption></figure><p id="2aad" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">为什么漏的ReLU比ReLU好<strong class="jq hj"/>？</p><figure class="la lb lc ld fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ni"><img src="../Images/e0d84786b022fa708f427cb6be8a8aaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EDdrPlFPyxNPF377lrDzMw.png"/></div></div></figure><ol class=""><li id="02c0" class="mk ml hi jq b jr kq jv kr jz mm kd mn kh mo kl mp mq mr ms bi translated">泄漏ReLU <strong class="jq hj">通过给负输入<strong class="jq hj"> (0.01x) </strong>一个非常小的x </strong>线性分量来调整负值的零梯度问题。</li><li id="32ab" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl mp mq mr ms bi translated">泄漏有助于增加ReLU功能的范围。通常情况下，<strong class="jq hj"> a </strong>的值为<strong class="jq hj"> 0.01 </strong>左右。</li><li id="5ec8" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl mp mq mr ms bi translated">泄漏ReLU的范围是<strong class="jq hj">(-无穷大到无穷大)。</strong></li></ol><blockquote class="km kn ko"><p id="0b01" class="jo jp kp jq b jr kq jt ju jv kr jx jy ks kt kb kc ku kv kf kg kw kx kj kk kl hb bi translated"><strong class="jq hj">注:</strong>理论上，漏ReLU具有ReLU的所有优点，加上死ReLU不会有任何问题，但在实际操作中，并没有完全证明漏ReLU总是比ReLU好。</p></blockquote><h1 id="13f3" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">5.ELU(指数线性单位)函数</h1><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/ed6db18debae15715e452ed34c0ec3ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*T1kXV0Jys4-yMTIiWg88Xg.png"/></div><figcaption class="na nb et er es nc nd bd b be z dx translated">ELU vs泄漏的雷鲁vs雷鲁</figcaption></figure><p id="3d2a" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated"><strong class="jq hj"> ELU </strong>也是<strong class="jq hj">提出来解决ReLU</strong>的问题。与ReLUs相反，elu具有<strong class="jq hj">负值</strong>，这使得激活<strong class="jq hj">的平均值</strong>更接近于<strong class="jq hj">零。</strong>意味着更接近零的激活<strong class="jq hj">能够更快地学习</strong>，因为它们使<strong class="jq hj">梯度更接近自然梯度。</strong></p><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es nk"><img src="../Images/a0d93525bd380c475de7c871e88a4545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*SPRharcieVjGvA2yWqENPA.png"/></div></figure><p id="a31e" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">显然，<strong class="jq hj"> ELU拥有ReLU </strong>的所有优点，并且:</p><ul class=""><li id="5bcb" class="mk ml hi jq b jr kq jv kr jz mm kd mn kh mo kl nl mq mr ms bi translated"><strong class="jq hj">没有死ReLU </strong>问题，输出的平均值接近0，<strong class="jq hj">零点居中</strong>。</li><li id="45ca" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl nl mq mr ms bi translated">在<strong class="jq hj">中，对比</strong>和ReLUs，elu具有负值，这允许它们像<strong class="jq hj">批量标准化</strong>一样将平均单位激活推至更接近零，但是具有<strong class="jq hj">更低的计算复杂度</strong>。由于<strong class="jq hj">减少的偏移效应，均值向零偏移通过使正常梯度更接近单位自然梯度来加速学习。</strong></li><li id="556c" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl nl mq mr ms bi translated">ELUs <strong class="jq hj">在较小的输入下饱和为负值</strong>，从而减少了向前传播的变化和信息。</li></ul><p id="38ac" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">一个<strong class="jq hj">小问题</strong>是它的计算强度稍微大一些<strong class="jq hj">。</strong>类似于Leaky ReLU，虽然理论上优于ReLU，但目前实践中并没有很好的证据证明eLU总是优于ReLU。</p><h1 id="7087" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">6.PRelu(参数Relu)</h1><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es nm"><img src="../Images/c9ab41bc400c2a0bfd32ce27f1412217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*BmVRlcbyW-ri03BRm9HV6g.png"/></div></figure><p id="6648" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated"><strong class="jq hj"> PReLU </strong>也是<strong class="jq hj">ReLU的</strong>改进型。</p><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es nn"><img src="../Images/0ea002f2ebebea6a26060cb848490444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*8zNvM6g04IikhkT0JMzn6w.png"/></div></figure><p id="b383" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">我们看了PReLU的公式。参数<strong class="jq hj"> α </strong>一般是0到1之间的数，一般比较小，比如几个零。</p><ul class=""><li id="33c3" class="mk ml hi jq b jr kq jv kr jz mm kd mn kh mo kl nl mq mr ms bi translated">如果<strong class="jq hj"> aᵢ=0 </strong>，f变成ReLU</li><li id="e3f7" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl nl mq mr ms bi translated">如果<strong class="jq hj"> aᵢ &gt; 0 </strong>，f变成漏电继电器</li><li id="c56b" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl nl mq mr ms bi translated">如果<strong class="jq hj"> aᵢ是一个可学习的参数</strong>，f变成PReLU</li></ul><p id="4774" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">到了PReLU的优势</p><ol class=""><li id="4d34" class="mk ml hi jq b jr kq jv kr jz mm kd mn kh mo kl mp mq mr ms bi translated">在负区域，PReLU有一个<strong class="jq hj">小斜坡</strong>，也可以<strong class="jq hj">避免</strong>ReLU死的问题。</li><li id="01b8" class="mk ml hi jq b jr mt jv mu jz mv kd mw kh mx kl mp mq mr ms bi translated">与ELU相比，PReLU是负区域中的<strong class="jq hj">线性操作</strong>。虽然斜率小，但是它<strong class="jq hj">不趋向于0 </strong>，这是一定的优势。</li></ol><h1 id="f7bd" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">7.Softmax</h1><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es no"><img src="../Images/f15718cad3cd6f4a9d30d322b3c7ae9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*AHitfux7ddZiiGiC6jrlLA.jpeg"/></div></figure><p id="dd8d" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated"><strong class="jq hj"> Softmax </strong>用作多类分类问题的<strong class="jq hj">激活函数</strong>，其中需要两个以上类标签上的类成员。对于任意长度为K的实向量，Softmax可以<strong class="jq hj">将其压缩成长度为K的实向量</strong>，其值在<strong class="jq hj">范围(0，1) </strong>内，向量中元素之和为1。</p><figure class="la lb lc ld fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es np"><img src="../Images/bf42614e39f76f2f3f67131bade6897e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t2FbP10uDFZrufDCYELWFA.jpeg"/></div></div></figure><blockquote class="km kn ko"><p id="5dc5" class="jo jp kp jq b jr kq jt ju jv kr jx jy ks kt kb kc ku kv kf kg kw kx kj kk kl hb bi translated">Softmax的S <!-- -->与普通的max函数不同:max函数只输出最大值，soft max保证较小的值有较小的概率，不会被直接丢弃。是一个<strong class="jq hj">【max】</strong>就是<strong class="jq hj">【软】</strong>；可以认为它是argmax函数的<strong class="jq hj">概率版或<em class="hi">软</em>版。</strong></p></blockquote><p id="9dc3" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">Softmax函数的分母结合了原始输出值的所有因子，这意味着Softmax函数获得的不同概率是相互关联的。</p><p id="751c" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">softmax激活功能的主要缺点是-</p><ol class=""><li id="4705" class="mk ml hi jq b jr kq jv kr jz mm kd mn kh mo kl mp mq mr ms bi translated">在零点不可微，ReLU无界。</li></ol><p id="d643" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">2.负输入的梯度为零，这意味着对于该区域中的激活，权重在反向传播期间不更新。这会产生永远不会被激活的死亡神经元。</p><h1 id="1ca7" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">8.Swish(自门控)功能</h1><figure class="la lb lc ld fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nq"><img src="../Images/f8ce03090109bdd405b3c8ad358023ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*avgXN89BZtU8Amfh7fex7w.png"/></div></div></figure><p id="f893" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">公式为:<strong class="jq hj"> y = x * sigmoid (x) </strong></p><p id="db65" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">Swish的设计灵感来自LSTMs和高速公路网络中使用sigmoid函数作为门控。我们使用相同的值进行选通，以简化选通机制，称为<strong class="jq hj">自选通</strong>。</p><blockquote class="km kn ko"><p id="25fa" class="jo jp kp jq b jr kq jt ju jv kr jx jy ks kt kb kc ku kv kf kg kw kx kj kk kl hb bi translated">自门控的<strong class="jq hj">优势在于它只需要一个<strong class="jq hj">简单标量输入，</strong>而正常门控需要多个标量输入。这一特性使得自门控激活函数(如Swish)能够轻松地<strong class="jq hj">替换以单个标量作为输入的激活函数(如ReLU) </strong>，而不改变隐藏容量或参数数量。</strong></p></blockquote><p id="7679" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated"><strong class="jq hj">注意:</strong>只有当你的<strong class="jq hj">神经网络≥ 40层时，才能实现Swish激活功能。</strong></p><p id="c2d7" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">Swish激活功能的<strong class="jq hj">主要优点</strong>如下:</p><p id="4dd1" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">1.<strong class="jq hj">无界度</strong>有助于防止缓慢训练时梯度逐渐接近0，造成饱和。</p><p id="ad41" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">(同时有界也有好处，因为有界的活动函数可以有很强的正则化，更大的负输入会被解析。)</p><p id="539a" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">2.导数<strong class="jq hj">始终为&gt; 0。</strong></p><p id="29b8" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">3.<strong class="jq hj">平滑度</strong>在<strong class="jq hj">优化和泛化中也起着重要的作用。</strong></p><h1 id="1293" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">9.最大输出</h1><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/8eec1721b2ecf18a97f2fefcef2d5f75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*Ae28NCJH6ZYooc64jIPYQw.jpeg"/></div></figure><p id="f5dd" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">一个<strong class="jq hj">最大输出</strong>层仅仅是一个激活函数<strong class="jq hj">是输入的最大值的层。</strong>如<strong class="jq hj">论文</strong>中所述，即使是具有<strong class="jq hj"> 2个最大输出单元</strong>的MLP也可以<strong class="jq hj">近似任何函数。</strong></p><p id="1bdb" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">单个Maxout单元可以被解释为对一个<strong class="jq hj">实值函数</strong>进行<strong class="jq hj">分段线性逼近(PWL) </strong>，其中该函数的图形上任意两点之间的线段位于该图形上(<strong class="jq hj">凸函数</strong>)。</p><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/069caef7563053bb4bd062c21de51125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*erIcz2XPYd8r4L86icbA9Q.png"/></div></figure><p id="c66f" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">Maxout也可以为一个<strong class="jq hj"> d维向量(V)实现。</strong></p><figure class="la lb lc ld fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nr"><img src="../Images/37405b8ef5fb93050fbe946bf4725d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t8cNgQcRBYPtezzl0T1WVw.jpeg"/></div></div></figure><p id="9e3e" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">考虑两个凸函数<strong class="jq hj"> h1(x)和h2(x) </strong>，由两个Maxout单元近似。由上述命题可知，函数<strong class="jq hj"> g(x)是一个连续的PWL函数。</strong></p><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es ns"><img src="../Images/dfde0fcff1221483f0764fe4bddebb52.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*rKsfpy6_xsBBxiuE_b6ZRA.png"/></div></figure><p id="36e4" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">因此，发现由两个Maxout单元组成的<strong class="jq hj"> Maxout层可以很好地逼近任意连续函数。</strong></p><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/30ef1f74a91bc3d689beb63e29addc02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ILlj6PNNLiX96ggk6WNEeA.png"/></div></figure><h1 id="f05a" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">10.软加-</h1><figure class="la lb lc ld fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nt"><img src="../Images/7e31e4a96796a8666e091f44eec8f45d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*enmbqoxIDqtM6kODX7Sngg.png"/></div></div></figure><p id="b03a" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated"><strong class="jq hj">软加函数</strong> : f(x) = <strong class="jq hj"> ln(1+exp x) </strong></p><p id="36ab" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">softplus的<strong class="jq hj">导数</strong>是——</p><p id="944f" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">f′(x)= exp(x)/(1+exp⁡x)</p><p id="86a3" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">=<strong class="jq hj">1/(1+exp(x))</strong></p><p id="0985" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">，也称为<strong class="jq hj">逻辑/sigmoid函数。</strong></p><p id="2c2b" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">softplus函数和ReLU函数类似，但是<strong class="jq hj">比较平滑。</strong>和ReLU一样是单边打压。</p><p id="e95b" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated">接受范围广<strong class="jq hj"> (0，+ inf) </strong>。</p><blockquote class="km kn ko"><p id="1a60" class="jo jp kp jq b jr kq jt ju jv kr jx jy ks kt kb kc ku kv kf kg kw kx kj kk kl hb bi translated">一般来说，这些激活功能各有利弊。所有的好与坏，都必须通过用各种问题语句对它们进行实验来获得。</p></blockquote><h1 id="ac88" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">至此，我们结束了这篇博客。希望这能给你一个关于深度学习中最常用的激活函数的不错的知识。</h1><p id="aaaa" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi">😎</p><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es nu"><img src="../Images/1ec2265f1e34839500d247a03da764dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*HFP0YK-mUZ90Is2eg4kCDQ.gif"/></div></figure><p id="2419" class="pw-post-body-paragraph jo jp hi jq b jr kq jt ju jv kr jx jy jz kt kb kc kd kv kf kg kh kx kj kk kl hb bi translated"><strong class="jq hj"> <em class="kp">如果你是数据科学和机器学习的初学者，并对数据科学/ML-AI、向数据科学的职业过渡指导、面试/简历准备有一些具体的疑问，甚至想在你的D-Day之前获得模拟面试，请随时拨打</em> </strong> <a class="ae ky" href="https://topmate.io/sukannya" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj"> <em class="kp">这里</em> </strong> </a> <strong class="jq hj"> <em class="kp">预约1:1。我很乐意帮忙！</em>T29】</strong></p><h2 id="2ff8" class="lv ir hi bd is lw lx ly iw lz ma mb ja jz mc md je kd me mf ji kh mg mh jm mi bi translated"><strong class="ak">下次见！🤓</strong></h2><blockquote class="km kn ko"><p id="0dc6" class="jo jp kp jq b jr kq jt ju jv kr jx jy ks kt kb kc ku kv kf kg kw kx kj kk kl hb bi translated"><a class="ae ky" href="https://www.linkedin.com/in/sukannya/" rel="noopener ugc nofollow" target="_blank">领英</a></p></blockquote></div></div>    
</body>
</html>