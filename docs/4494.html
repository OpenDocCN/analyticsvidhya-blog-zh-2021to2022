<html>
<head>
<title>Let’s take a peek into PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们来看看PySpark</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/lets-take-a-peek-into-pyspark-29693769d150?source=collection_archive---------2-----------------------#2021-10-29">https://medium.com/analytics-vidhya/lets-take-a-peek-into-pyspark-29693769d150?source=collection_archive---------2-----------------------#2021-10-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/c869ee057a3fc3682695b8f584908d1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7MqXu8myolB1l6zZ"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">沃伦·王在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="e40c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">欢迎来到我的Pyspark博客，在这里我们可以试着了解pyspark的要点及其用例。我们还可以尝试使用一个CSV文件，使用Pyspark作为我们的第一手操作。</p><h1 id="8041" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak"> Spark —建立模型</strong></h1><blockquote class="kq kr ks"><p id="ab03" class="iu iv kt iw b ix iy iz ja jb jc jd je ku jg jh ji kv jk jl jm kw jo jp jq jr ha bi translated">Spark是一个集群计算平台。Spark允许你将数据和计算分散到有多个节点的集群上(把每个节点想象成一台独立的计算机)。拆分数据可以更容易地处理非常大的数据集，因为每个节点只处理少量的数据。</p></blockquote><h1 id="a7d6" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak"> <em class="kx">平行度</em> </strong></h1><p id="8da3" class="pw-post-body-paragraph iu iv hh iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated"><em class="kt">一般来说，Spark使用并行概念，使文件的读取能够同时进行，其中每个节点承担读取某组文件行的角色，这使得该过程更快，因为它是并行进行的。</em></p><h1 id="d541" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">火花用在哪里？</strong></h1><p id="eb2e" class="pw-post-body-paragraph iu iv hh iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">现在，这需要一些实践和经验，来决定spark是否是所提出问题的最佳商业解决方案。以我个人的经验，我在文件通常比较大的地方用过Spark。在pandas等其他库的帮助下，行和列方面的大文件需要时间来处理，这就是Spark派上用场的地方，与pandas等库相比，即使是大数据文件也可以处理得更快。但同样，像Spark这样的好框架也有更大的复杂性。</p><h1 id="9df9" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak"> PySpark </strong></h1><p id="23e7" class="pw-post-body-paragraph iu iv hh iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">由于spark是一个开源引擎，它提供了不同的编程API接口，以便进入它的Spark设置和python中曾经允许的语言。其他语言包括scala、java和r。</p><h1 id="8741" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">内部工作</h1><p id="4594" class="pw-post-body-paragraph iu iv hh iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">Spark通常有以下设置，其中包括一个主节点和多个工作节点。</p><figure class="le lf lg lh fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ld"><img src="../Images/150e2f002d74f50bea2b10cd9f45c238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xGMpCZJ6vVKhg773UeHwQQ.png"/></div></div></figure><ul class=""><li id="3bd9" class="li lj hh iw b ix iy jb jc jf lk jj ll jn lm jr ln lo lp lq bi translated">主节点负责在工作节点之间拆分数据，还负责管理计算。</li><li id="e68e" class="li lj hh iw b ix lr jb ls jf lt jj lu jn lv jr ln lo lp lq bi translated">Master发送要处理的数据和要运行的计算，worker在处理结束后将结果发送回master。</li><li id="9306" class="li lj hh iw b ix lr jb ls jf lt jj lu jn lv jr ln lo lp lq bi translated">Spark在集群的概念下工作，其中集群(由主节点和工作节点组成)托管在连接到其他节点的远程机器上。</li><li id="d4c2" class="li lj hh iw b ix lr jb ls jf lt jj lu jn lv jr ln lo lp lq bi translated">当开始使用Spark时，最好在本地运行集群。</li><li id="a545" class="li lj hh iw b ix lr jb ls jf lt jj lu jn lv jr ln lo lp lq bi translated">现在要进入Spark集群设置，我们只需创建一个SparkContext，作为Spark的入口点。</li><li id="8499" class="li lj hh iw b ix lr jb ls jf lt jj lu jn lv jr ln lo lp lq bi translated">SparkContext采用几个可选参数来配置集群设置。</li><li id="45a9" class="li lj hh iw b ix lr jb ls jf lt jj lu jn lv jr ln lo lp lq bi translated">要安装pyspark，请使用以下命令:pip install pyspark</li></ul><p id="b918" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们进入一些编码</p><figure class="le lf lg lh fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lw"><img src="../Images/6e7f7522f0a178a8b08623b7a4a230db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HtzX7JpZEZJfV8JqbEzX8A.png"/></div></div></figure><p id="6cba" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在上面的代码中，我们创建了一个spark上下文，其中master将在本地进行配置，并且给定了spark上下文的应用程序名称。这里sc是spark上下文。</p><p id="95c8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">sc中提供的一些其他定制包括:</p><figure class="le lf lg lh fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lx"><img src="../Images/1d14a7a6871160bfad39b4353260eefa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kAaiuqVcosy7vY-UDyL6wA.png"/></div></div></figure><p id="17d3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">spark的核心数据结构是<strong class="iw hi"> RDD(弹性分布式数据集)。</strong>这是让spark在多个节点上分裂的对象。但是这有点难操作，所以正如我们所讨论的，我们可以使用数据帧在pyspark中读取CSV文件。</p><p id="fe4d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> Dataframe类似于SQL表</strong>，由行和列组成。与RDD相比，这也是一个更好的数据处理选择。</p><p id="4589" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">要处理数据框，我们必须从SparkContext中创建一个<strong class="iw hi"> Spark session对象。</strong>看看下面的代码。</p><figure class="le lf lg lh fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lw"><img src="../Images/abb029f4b6db37b52707eba797d01dd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oYkTkFnMYq0uGsAWPLPAAg.png"/></div></div></figure><p id="6ccf" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在让我们使用这个创建的my_spark会话来读取CSV文件。</p><figure class="le lf lg lh fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ly"><img src="../Images/cbbcc5b15d91784cefc22b0bfdaa3a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iEofnw7zs2vqHbJEYPMWTw.png"/></div></div></figure><p id="8d05" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在上图中，我们正在读取一个名为5_a.csv的文件。在配置中，我们提到读取格式为csv，并推断schema为True，这有助于正确识别列的数据类型，但是，我们必须记住，spark中的数据类型识别并不准确。</p><p id="1af0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> df。show()将显示列和数据</strong>。您可能已经注意到，虽然y和proba是列的名称，但是它被标识为另一行，列名被标识为_c0和_c1。为了避免这种情况，我们需要给出一个额外的配置如下。</p><figure class="le lf lg lh fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lz"><img src="../Images/e55cd361fc9a531f52278ce03c39cf47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uy64vwtRziSh6-89sOXo8Q.png"/></div></div></figure><p id="f074" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="kt">将额外配置作为标题设为真，将第一行标识为CSV文件中的列名。</em>T11】</strong></p><p id="310d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">是的，我们已经到了这个博客的结尾。欢迎在评论区添加你的想法，我很乐意回答。</p><p id="80bb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">编码快乐！！！</p></div></div>    
</body>
</html>