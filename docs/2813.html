<html>
<head>
<title>Logistic Regression (Mathematics and Intuition behind Logistic Regression)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归(逻辑回归背后的数学和直觉)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-mathematics-and-intuition-behind-logistic-regression-8ca864286bff?source=collection_archive---------12-----------------------#2021-05-18">https://medium.com/analytics-vidhya/logistic-regression-mathematics-and-intuition-behind-logistic-regression-8ca864286bff?source=collection_archive---------12-----------------------#2021-05-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="9c38" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">了解关于<strong class="ig hi">逻辑回归</strong>的一切。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/78af0d3a44a6eba4b124975923b9cd43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AEdm_w-SeGFUEdx4"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae js" href="https://unsplash.com/@altumcode?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> AltumCode </a>的照片</figcaption></figure><h2 id="bb4a" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated"><strong class="ak">目录:</strong></h2><p id="d538" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated"><strong class="ig hi"> ∘ </strong> <a class="ae js" href="#39a2" rel="noopener ugc nofollow"> <strong class="ig hi">简介:</strong></a><strong class="ig hi"><br/>∘</strong><a class="ae js" href="#a0fb" rel="noopener ugc nofollow"><strong class="ig hi">线性回归</strong></a><strong class="ig hi"><br/>∘</strong><a class="ae js" href="#273c" rel="noopener ugc nofollow"><strong class="ig hi">逻辑回归</strong></a><strong class="ig hi"><br/>∘</strong><a class="ae js" href="#6e45" rel="noopener ugc nofollow"><strong class="ig hi">代价函数:</strong></a><strong class="ig hi"><br/>∘</strong><a class="ae js" href="#22df" rel="noopener ugc nofollow"><strong class="ig hi">梯度下降算法:【T44</strong></a></p><h2 id="39a2" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">简介:</h2><ul class=""><li id="04d0" class="kt ku hh ig b ih ko il kp ip kv it kw ix kx jb ky kz la lb bi translated"><strong class="ig hi">逻辑回归</strong>是用于<strong class="ig hi">二元分类</strong>的监督学习算法。</li><li id="e40b" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">例如(真或假，是或否，1或0)。</li><li id="9e23" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">也可用于<strong class="ig hi">多类分类</strong>。</li><li id="c416" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">但是对于<strong class="ig hi">多类分类</strong>我们必须做些别的事情，有一些概念叫做<strong class="ig hi">一个对一个，一个对所有的</strong>。我们将在接下来的文章中看到这些。</li></ul><p id="7e87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们用例子来理解<strong class="ig hi">逻辑回归</strong></p><p id="88f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">看一下下面的python代码… <br/>我们采用随机数据并绘制图表来理解这个概念。</p><pre class="jd je jf jg fd lh li lj lk aw ll bi"><span id="6120" class="jt ju hh li b fi lm ln l lo lp"># creating random data points <br/>x =  np.linspace(-10, 10, 10)<br/>y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])</span><span id="c0a5" class="jt ju hh li b fi lq ln l lo lp">plt.figure(figsize=(7, 4), dpi=100)<br/>plt.title('X vs class(1 or 0)')<br/>plt.xlabel('X values')<br/>plt.ylabel('class (0 or 1)')<br/>plt.scatter(x, y)<br/>plt.savefig('logistic_regression.jpg')<br/>plt.show()</span><span id="07ae" class="jt ju hh li b fi lq ln l lo lp">output:</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/8ccbbe1bf88694d59ce7d9755c811755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2rT-pr9AIlsbkuvFfVwpvg.jpeg"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图1.1</figcaption></figure><ul class=""><li id="7c26" class="kt ku hh ig b ih ii il im ip ls it lt ix lu jb ky kz la lb bi translated">我们看到，如果X值大于0，class为<strong class="ig hi"> 1 </strong>，如果X值小于0，class为<strong class="ig hi"> 0。</strong></li><li id="c48c" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">想到的第一个问题是，我们能用<strong class="ig hi">线性回归、</strong>解决这个问题吗，答案是<strong class="ig hi">是的</strong>，我们能用<strong class="ig hi">线性回归</strong>解决这个问题。</li></ul><p id="c476" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae js" rel="noopener" href="/analytics-vidhya/linear-regression-38f0cd6856f5"> <strong class="ig hi"> <em class="lv">点击这里</em> </strong>查看我关于<strong class="ig hi">线性回归</strong> </a> <strong class="ig hi">的文章。</strong></p><p id="8bb2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，我们看看如何用线性回归来解决这个问题，然后我们将使用逻辑回归来解决它。</p><p id="c4ec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们开始吧…</p><h2 id="a0fb" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated"><a class="ae js" rel="noopener" href="/analytics-vidhya/linear-regression-38f0cd6856f5">线性回归</a></h2><p id="da01" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">让我们用线性回归来理解这个问题。</p><p id="6bf2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下代码用于使用<strong class="ig hi">线性回归</strong>预测值并绘制图表。</p><pre class="jd je jf jg fd lh li lj lk aw ll bi"><span id="5dcb" class="jt ju hh li b fi lm ln l lo lp">from sklearn.linear_model import LinearRegression</span><span id="f0ca" class="jt ju hh li b fi lq ln l lo lp">lr = LinearRegression()<br/>lr.fit(x.reshape(-1,1), y)<br/>pred = lr.predict(x.reshape(-1,1))</span><span id="f3b2" class="jt ju hh li b fi lq ln l lo lp">print(y, pred, sep='\n')</span><span id="9900" class="jt ju hh li b fi lq ln l lo lp">plt.figure(figsize=(7, 4), dpi=100)<br/>plt.title('X vs class(1 or 0)')<br/>plt.xlabel('X values')<br/>plt.ylabel('class (0 or 1)')</span><span id="f0c5" class="jt ju hh li b fi lq ln l lo lp">plt.scatter(x, y, label="Actual")<br/>plt.plot(x, pred, label="Predicted")</span><span id="5fd3" class="jt ju hh li b fi lq ln l lo lp">plt.legend(loc='upper left')<br/>plt.savefig('logistic_regression_1.jpg')<br/>plt.show()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/fc2ba4a493ca0b1949f7744b5214bc22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1_XtZQdljeu_aFnFk5deLg.jpeg"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图1.2</figcaption></figure><p id="1627" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，如果预测值大于0.5，那么我们认为类是1，如果小于0.5，那么类是0。</p><p id="a641" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">线性回归优于逻辑回归或分类问题:</strong></p><ol class=""><li id="ce70" class="kt ku hh ig b ih ii il im ip ls it lt ix lu jb lw kz la lb bi translated">错误率非常高。</li><li id="5b9a" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb lw kz la lb bi translated">对异常值非常敏感。</li><li id="aabe" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb lw kz la lb bi translated">大多数情况下，我们会得到大于1小于0的预测值。</li></ol><p id="262e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图显示了最佳拟合线如何受到异常值的影响…</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/09ca15e5da1c9b69a6b7d8a4ba4930f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YuLW1SHwZlnwRdWJnVQdJw.jpeg"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图1.3</figcaption></figure><p id="afa2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在是时候了解一下<strong class="ig hi">逻辑回归</strong>了。</p><h2 id="273c" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated"><strong class="ak">逻辑回归</strong></h2><ul class=""><li id="83ff" class="kt ku hh ig b ih ko il kp ip kv it kw ix kx jb ky kz la lb bi translated"><strong class="ig hi">逻辑回归</strong>使用<strong class="ig hi"> sigmoid函数</strong>，该函数创建一条类似于<strong class="ig hi"> S形的最佳拟合线。</strong></li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lx"><img src="../Images/890e5e6ab4f244fa09dedc86fb6197b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*JgDenQiPa3n_RLhIjb6jsA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图2.1 sigmoid函数</figcaption></figure><ul class=""><li id="513f" class="kt ku hh ig b ih ii il im ip ls it lt ix lu jb ky kz la lb bi translated">现在想到的问题是，这个函数从何而来。所以让我们先了解… <br/>我们知道线性回归的函数<strong class="ig hi"> y=mx+c. </strong>我们只是把这个函数转换成假设形式。</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ly"><img src="../Images/edb02d01703373150ce8a6cf3167c5ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*W7oJZkQ254OVdchArXcUmg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图2.2线性方程</figcaption></figure><ul class=""><li id="7d48" class="kt ku hh ig b ih ii il im ip ls it lt ix lu jb ky kz la lb bi translated">但是正如我们看到的，当我们使用线性回归来解决二元分类问题时，一些预测值大于1且小于0。但是我们需要范围在0和1之间的输出值<strong class="ig hi">。我们如何做到这一点？？</strong></li><li id="76b5" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">我们需要将上面的等式修改如下...</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lz"><img src="../Images/9eed8100139c1be80bbff7ac6bbc1c9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*Vwka55QgHbl9aHZRtmo1jw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图2.3 sigmoid函数</figcaption></figure><ul class=""><li id="cfe2" class="kt ku hh ig b ih ii il im ip ls it lt ix lu jb ky kz la lb bi translated">上面的函数是我们最后的<strong class="ig hi"> Sigmod函数。</strong></li></ul><p id="eed8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">让我们看看如何使用这个sigmoid函数预测值… </strong></p><p id="5f18" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当θ =1时，我们将使用sigmoid函数预测X值。</p><pre class="jd je jf jg fd lh li lj lk aw ll bi"><span id="9083" class="jt ju hh li b fi lm ln l lo lp">def sigmoid(x, theta=1):<br/>    # Activation function used to map any real value between 0 and 1<br/>    return 1 / (1 + np.exp(-np.dot(x, theta)))</span><span id="27f5" class="jt ju hh li b fi lq ln l lo lp">y_pred = sigmoid(x, 1)<br/>y_pred</span><span id="2073" class="jt ju hh li b fi lq ln l lo lp">output:</span><span id="e373" class="jt ju hh li b fi lq ln l lo lp">array([4.53978687e-05, 4.18766684e-04, 3.85103236e-03, 3.44451957e-02,2.47663801e-01, 7.52336199e-01, 9.65554804e-01, 9.96148968e-01,9.99581233e-01, 9.99954602e-01])</span></pre><p id="5d09" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们将通过绘制简单的图表来比较实际值和预测值。</p><pre class="jd je jf jg fd lh li lj lk aw ll bi"><span id="5c5b" class="jt ju hh li b fi lm ln l lo lp">plt.figure(figsize=(7, 4), dpi=100)</span><span id="5541" class="jt ju hh li b fi lq ln l lo lp">plt.ylabel('class (0 or 1)')</span><span id="5394" class="jt ju hh li b fi lq ln l lo lp">plt.scatter(x, y, label="Actual")<br/>plt.scatter(x, y_pred, label="Predicted")<br/>plt.plot(x, y_pred, linestyle='-.')</span><span id="7b12" class="jt ju hh li b fi lq ln l lo lp">plt.legend()<br/>plt.savefig('logistic_regression_12.jpg')<br/>plt.show()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/820d5f17a6ff8c92020248f4d140219a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rYuUegFpgy8W1Jn79mfrDw.jpeg"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图2.4实际与预测</figcaption></figure><h2 id="6e45" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">成本函数:</h2><ul class=""><li id="ff51" class="kt ku hh ig b ih ko il kp ip kv it kw ix kx jb ky kz la lb bi translated">成本函数用于检查实际值和预测值之间的误差。</li><li id="7fca" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">但是在逻辑回归中我们不使用MSE函数。</li><li id="5ed8" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">在逻辑回归中，<strong class="ig hi"> y </strong>是一个<strong class="ig hi">非线性函数</strong>，如果我们将这个成本函数放入<strong class="ig hi"> MSE </strong>方程中，它将给出一条<strong class="ig hi">非凸曲线</strong>，如下图2.5所示。</li><li id="3fc9" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">当我们试图使用梯度下降来优化值时，它会使寻找全局最小值变得复杂。</li><li id="6b0b" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">在逻辑回归中，<strong class="ig hi">对数损失</strong>函数被用作<strong class="ig hi">成本函数。</strong></li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ma"><img src="../Images/46d306129d4d53118b60dbec42c7bead.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*JlXv9scLQSh3NeP9Vg8KMA.jpeg"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图2.5非凸曲线</figcaption></figure><p id="2d87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们来理解一下<strong class="ig hi">成本函数</strong> …</p><p id="a1a6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是逻辑回归的成本函数。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mb"><img src="../Images/89b073190ecfec5f6084acef8af70083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*kd7QTRZjCjfuwORfbLpxcA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图2.6成本函数</figcaption></figure><p id="3089" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">简化成本函数……</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mc"><img src="../Images/fb8c50091d17e980221d0449e007d6b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*paComO3McQeR0z-kROEglQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图2.7简化的成本函数</figcaption></figure><p id="5ff1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">如果y = 1那么:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es md"><img src="../Images/4bf78971c523f5cc53667bf573c56a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*-BD2iwEmvXvxAWYngVfL0Q.png"/></div></figure><p id="6016" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这与图2.6中y=1时的情况相同</p><p id="dc57" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">如果y = 0，则:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es me"><img src="../Images/9562ebce4f7a1527e02faf01fe49690f.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*fg4TuQPtu7P9-n00wt7SWQ.png"/></div></figure><p id="10ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这与图2.6中y=0时的情况相同</p><p id="1d96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">毕竟，我们已经得到了下面最终的<strong class="ig hi">成本函数</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mf"><img src="../Images/0e6ccd3f57e8b573a7c43aaef7a357f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PYlDQbpal9zn_568Hyk62A.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图2.8最终成本函数</figcaption></figure><p id="d113" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是成本函数的python代码:</p><pre class="jd je jf jg fd lh li lj lk aw ll bi"><span id="9eb5" class="jt ju hh li b fi lm ln l lo lp">def cost_function(x, y, t): # t= theta value<br/>    m = len(x)<br/>    total_cost = -(1 / m) * np.sum(<br/>        y * np.log(sigmoid(x, t)) + <br/>        (1 - y) * np.log(1 - sigmoid(x, t)))<br/>    return total_cost</span><span id="6742" class="jt ju hh li b fi lq ln l lo lp">cost_function(x, y, 1)</span><span id="b765" class="jt ju hh li b fi lq ln l lo lp">output:<br/>0.06478942360607087</span></pre><p id="712d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们将绘制不同θ值的图表</p><pre class="jd je jf jg fd lh li lj lk aw ll bi"><span id="ff0d" class="jt ju hh li b fi lm ln l lo lp"># ploting graph for diffrent values of m vs cost function</span><span id="ce9f" class="jt ju hh li b fi lq ln l lo lp">plt.figure(figsize=(10,5))</span><span id="b2ea" class="jt ju hh li b fi lq ln l lo lp">T = np.linspace(-1, 1,10)<br/>error = []<br/>i= 0</span><span id="249b" class="jt ju hh li b fi lq ln l lo lp">for t in T:<br/>    <br/>    error.append(cost_function(x,y, t)) <br/>    print(f'for t = {t} error is {error[i]}')<br/>    i+=1</span><span id="cde3" class="jt ju hh li b fi lq ln l lo lp">plt.plot(T, error)plt.scatter(T, error)<br/>plt.ylabel("cost function")<br/>plt.xlabel("t")<br/>plt.title("Error vs t")<br/>plt.savefig('costfunc.jpg')<br/>plt.show()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mg"><img src="../Images/ab6986dfc0a522bfccee0852a01d4f8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z6gS1VfZaUnk5PwNvbuyRA.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图2.9误差与温度的关系</figcaption></figure><h2 id="22df" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">梯度下降算法:</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mh"><img src="../Images/daa6de7cb3be6bffb9c3c2ba85e0cb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*G3evFxIAlDchOx5Wl7bV5g.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图3.1梯度下降</figcaption></figure><ul class=""><li id="d7d2" class="kt ku hh ig b ih ii il im ip ls it lt ix lu jb ky kz la lb bi translated"><strong class="ig hi">梯度下降</strong>算法与我们在<a class="ae js" rel="noopener" href="/analytics-vidhya/linear-regression-38f0cd6856f5#8d01">线性回归文章</a>中看到的算法相同。</li><li id="21c4" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">这里唯一的区别是图2.3所示的<strong class="ig hi">假设函数</strong></li><li id="58da" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><a class="ae js" rel="noopener" href="/analytics-vidhya/linear-regression-38f0cd6856f5#8d01">点击此处</a>了解梯度下降。</li></ul><p id="4510" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">现在是实施线性回归的时候了。</strong></p><h2 id="2e16" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">实施:</h2><pre class="jd je jf jg fd lh li lj lk aw ll bi"><span id="ad63" class="jt ju hh li b fi lm ln l lo lp">from sklearn.linear_model import LogisticRegression</span><span id="e203" class="jt ju hh li b fi lq ln l lo lp">lr = LogisticRegression()<br/>lr.fit(x.reshape(-1,1), y)<br/>pred = lr.predict(x.reshape(-1,1))<br/>prob = lr.predict_proba(x.reshape(-1,1))<br/>print(y, np.round(np.array(pred), 2), sep='\n')</span><span id="b653" class="jt ju hh li b fi lq ln l lo lp">output:<br/>[0 0 0 0 0 1 1 1 1 1]<br/>[0 0 0 0 0 1 1 1 1 1]</span></pre><ul class=""><li id="7f32" class="kt ku hh ig b ih ii il im ip ls it lt ix lu jb ky kz la lb bi translated"><strong class="ig hi">θ</strong>的值:</li></ul><pre class="jd je jf jg fd lh li lj lk aw ll bi"><span id="f9f2" class="jt ju hh li b fi lm ln l lo lp">lr.coef_</span><span id="3c59" class="jt ju hh li b fi lq ln l lo lp">output:<br/>array([[0.93587902]])</span></pre><ul class=""><li id="c316" class="kt ku hh ig b ih ii il im ip ls it lt ix lu jb ky kz la lb bi translated"><strong class="ig hi">截距</strong>的值:</li></ul><pre class="jd je jf jg fd lh li lj lk aw ll bi"><span id="9a5b" class="jt ju hh li b fi lm ln l lo lp">lr.intercept_</span><span id="20a9" class="jt ju hh li b fi lq ln l lo lp">output:<br/>array([-6.3343145e-17])</span></pre><ul class=""><li id="f3d3" class="kt ku hh ig b ih ii il im ip ls it lt ix lu jb ky kz la lb bi translated">最终<strong class="ig hi">分数:</strong></li></ul><pre class="jd je jf jg fd lh li lj lk aw ll bi"><span id="4b01" class="jt ju hh li b fi lm ln l lo lp">lr.score(x.reshape(-1, 1), y)</span><span id="e46a" class="jt ju hh li b fi lq ln l lo lp">output:<br/>1.0</span></pre><ul class=""><li id="a628" class="kt ku hh ig b ih ii il im ip ls it lt ix lu jb ky kz la lb bi translated">绘制实际与预测的图表</li></ul><pre class="jd je jf jg fd lh li lj lk aw ll bi"><span id="fb1e" class="jt ju hh li b fi lm ln l lo lp">plt.figure(figsize=(7, 4), dpi=100)<br/>plt.title('X vs class(1 or 0)')<br/>plt.xlabel('X values')<br/>plt.ylabel('class (0 or 1)')</span><span id="0e7b" class="jt ju hh li b fi lq ln l lo lp">plt.scatter(x, y, label="actual")<br/>plt.plot(x, pred, label="predicted", color='red')</span><span id="d559" class="jt ju hh li b fi lq ln l lo lp">plt.legend(loc='upper left')<br/>plt.savefig('logistic_regression.jpg')<br/>plt.show()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/aa3db29463d190d06c10bc36c5825db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3a4zKJeLhs354yCn7e47QA.jpeg"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图4.1</figcaption></figure><p id="6e25" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae js" href="https://github.com/sbswapnil/Data-Science/blob/main/Logistic%20Regression.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">点击这里</strong> </a> <strong class="ig hi">获取我关于逻辑回归的完整Jupyter笔记本。</strong></p><h2 id="ec1d" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">总结:</h2><ul class=""><li id="b0f0" class="kt ku hh ig b ih ko il kp ip kv it kw ix kx jb ky kz la lb bi translated">我们讨论<strong class="ig hi">逻辑回归、成本函数、</strong>和<strong class="ig hi">梯度下降算法。</strong></li><li id="baf0" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">我们已经理解了<strong class="ig hi">逻辑回归</strong>背后的<strong class="ig hi">直觉</strong>。</li></ul></div></div>    
</body>
</html>