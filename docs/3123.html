<html>
<head>
<title>Handling Imbalanced Dataset With UndersamplingTechnique. Its Pros and Cons!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用欠采样技术处理不平衡数据集。它的利弊！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/handling-imbalanced-dataset-with-undersamplingtechnique-its-pros-and-cons-2c634e763a9e?source=collection_archive---------2-----------------------#2021-06-09">https://medium.com/analytics-vidhya/handling-imbalanced-dataset-with-undersamplingtechnique-its-pros-and-cons-2c634e763a9e?source=collection_archive---------2-----------------------#2021-06-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="48b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">欠采样是指一组旨在平衡具有偏斜类分布的分类数据集的类分布的技术。</p><p id="0eb1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不均衡的类分布将有一个或多个实例很少的类(少数类)和一个或多个实例很多的类(多数类)。最好在二元(两类)分类问题的上下文中理解，其中类0是多数类，类1是少数类。了解更多关于<a class="ae jc" href="https://akalbir.medium.com/what-is-an-imbalanced-data-how-to-handle-imbalanced-data-in-python-e6067792950f" rel="noopener">不平衡数据</a>的信息。</p><p id="b0d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">欠采样技术从训练数据集中移除属于多数类的样本，以更好地平衡类分布，例如将偏斜从1:100减少到1:10、1:2或者甚至1:1的类分布。这不同于过采样，过采样涉及向少数类添加示例以减少类分布中的偏斜。在本文中，我们将讨论不同的欠采样技术。</p><ol class=""><li id="a5e6" class="jd je hh ig b ih ii il im ip jf it jg ix jh jb ji jj jk jl bi translated">选择要保留的示例的方法</li></ol><ul class=""><li id="3ebc" class="jd je hh ig b ih ii il im ip jf it jg ix jh jb jm jj jk jl bi translated">接近欠采样</li></ul><p id="dcaf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.选择要删除的示例的方法</p><ul class=""><li id="0835" class="jd je hh ig b ih ii il im ip jf it jg ix jh jb jm jj jk jl bi translated">欠采样的Tomek链接</li></ul><p id="615c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.保留和删除方法的组合</p><ul class=""><li id="d083" class="jd je hh ig b ih ii il im ip jf it jg ix jh jb jm jj jk jl bi translated">欠采样的单侧选择</li></ul></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es ju"><img src="../Images/9213b5504f5a2d7e13ed380ebc84abb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fOrt1t8yuRGgtJOOQkwTaw.png"/></div></div></figure><h1 id="9608" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">选择要保留的示例的方法</h1><p id="d792" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">在这一节中，我们将仔细研究两种方法，这两种方法从多数类中选择要保留的示例，即接近失败的方法族。</p><h2 id="0d88" class="lj kh hh bd ki lk ll lm km ln lo lp kq ip lq lr ku it ls lt ky ix lu lv lc lw bi translated">接近欠采样</h2><p id="1fbb" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">接近失误指的是基于多数类样本到少数类样本的距离来选择样本的欠采样方法的集合。</p><p id="c3a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些方法是由张建平和因德吉特·马尼在他们2003年发表的题为“<a class="ae jc" href="https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf" rel="noopener ugc nofollow" target="_blank"> KNN不平衡数据分布方法:涉及信息提取的案例研究</a>”的论文中提出的</p><p id="e89d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该技术有三个版本，分别命名为NearMiss-1、NearMiss-2和NearMiss-3。这里，在特征空间中使用欧几里德距离或类似距离来确定距离。</p><ul class=""><li id="b121" class="jd je hh ig b ih ii il im ip jf it jg ix jh jb jm jj jk jl bi translated"><strong class="ig hi"> NearMiss-1 </strong>:与三个最近的少数类样本平均距离最小的多数类样本。</li><li id="21c4" class="jd je hh ig b ih lx il ly ip lz it ma ix mb jb jm jj jk jl bi translated"><strong class="ig hi"> NearMiss-2 </strong>:与三个最远的少数类样本具有最小平均距离的多数类样本。</li><li id="16d0" class="jd je hh ig b ih lx il ly ip lz it ma ix mb jb jm jj jk jl bi translated"><strong class="ig hi"> NearMiss-3 </strong>:与每个少数类样本具有最小距离的多数类样本。</li></ul><p id="3e40" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面提到了用imblearn实现NearMiss-1的代码，供大家参考。</p><figure class="jv jw jx jy fd jz"><div class="bz dy l di"><div class="mc md l"/></div></figure><p id="842b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">运行该示例会对多数类进行欠采样，并创建转换数据集的散点图。</p><p id="a9e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以看到，正如预期的那样，只有那些与重叠区域中的少数类实例最接近的多数类实例被保留下来。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es me"><img src="../Images/c7c72f91a334c0665971feac309b6837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YfNuTxGB1NODdOjVhsaVgQ.jpeg"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">使用NearMiss-1欠采样的不平衡数据集的散点图</figcaption></figure><p id="ba43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，类似地，我们可以通过更改版本来执行未遂事件2和3。</p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><h1 id="d5d2" class="kg kh hh bd ki kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld bi translated">选择要删除的示例的方法</h1><p id="10dc" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">这一节将详细介绍从多数类中选择要删除的示例的方法，即。流行的托梅克链接方法。</p><h2 id="314a" class="lj kh hh bd ki lk ll lm km ln lo lp kq ip lq lr ku it ls lt ky ix lu lv lc lw bi translated">欠采样的Tomek链接</h2><p id="b8fe" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">Tomek链接是由Tomek (1976)开发的压缩最近邻(CNN，不要与卷积神经网络混淆)欠采样技术的修改之一。与CNN方法不同，其仅从想要移除的多数类中随机选择具有k个最近邻的样本，Tomek Links方法使用规则来选择满足这些属性的观测值对(例如，<strong class="ig hi"> a </strong>和<strong class="ig hi"> b </strong>):</p><ol class=""><li id="66fa" class="jd je hh ig b ih ii il im ip jf it jg ix jh jb ji jj jk jl bi translated">观测值<strong class="ig hi"> a </strong>的最近邻居是<strong class="ig hi"> b </strong>。</li><li id="567b" class="jd je hh ig b ih lx il ly ip lz it ma ix mb jb ji jj jk jl bi translated">观测值<strong class="ig hi"> b </strong>的最近邻居是<strong class="ig hi"> a </strong>。</li><li id="0806" class="jd je hh ig b ih lx il ly ip lz it ma ix mb jb ji jj jk jl bi translated">观察值<strong class="ig hi"> a </strong>和<strong class="ig hi"> b </strong>属于不同的类别。即<strong class="ig hi"> a </strong>和<strong class="ig hi"> b </strong>分别属于少数派和多数派阶级(或者<em class="mo">反之</em>)。</li></ol><p id="bce5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面提到了Tomek与imblearn链接的代码。</p><figure class="jv jw jx jy fd jz"><div class="bz dy l di"><div class="mc md l"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">Tomek链接欠采样</figcaption></figure><p id="d5ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">计数器({0: 9900，1: 100}) <br/>计数器({0: 9874，1: 100})</p><p id="b1ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">变换数据集的散点图不会使对多数类的次要编辑变得明显。</p><p id="a3c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这突出表明，尽管在类边界上找到不明确的例子是有用的，但它本身并不是一种很好的欠采样技术。在实践中，托梅克链接程序经常与其他方法结合使用，如压缩最近邻规则。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es me"><img src="../Images/d311ab7854da2b2945deb2b281494dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GflkE1kj--Q4rwZH0EIysA.jpeg"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">用托梅克链接法欠采样的不平衡数据集散点图</figcaption></figure></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><h1 id="b347" class="kg kh hh bd ki kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld bi translated">保留和删除方法的组合</h1><p id="4200" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">这一节将更仔细地研究一些技术，这些技术结合了我们已经研究过的从多数类中保留和删除示例的技术，例如单侧选择和邻域清理规则。</p><h2 id="af3f" class="lj kh hh bd ki lk ll lm km ln lo lp kq ip lq lr ku it ls lt ky ix lu lv lc lw bi translated">欠采样的单侧选择</h2><p id="bf81" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">单边选择，简称OSS，是一种欠采样技术，结合了Tomek链接和压缩最近邻(CNN)规则。</p><p id="cb97" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">具体来说，Tomek链接是类边界上的模糊点，并且在多数类中被识别和移除。然后使用CNN方法从远离决策边界的多数类中移除冗余样本。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es mp"><img src="../Images/e76244b972d1bfb69288b6b74ed7ad88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*fpTYCdcoNoJcGZSjWM2_Ew.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">欠采样程序的单侧选择概述<br/>摘自解决不平衡训练集的诅咒:单侧选择。</figcaption></figure><p id="4083" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以通过<a class="ae jc" href="https://imbalanced-learn.org/stable/generated/imblearn.under_sampling.OneSidedSelection.html" rel="noopener ugc nofollow" target="_blank">单边选择不平衡学习类</a>实现OSS欠采样策略。</p><p id="a48c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">种子示例的数量可以用<em class="mo"> n_seeds_S </em>设置，默认为1，KNN的<em class="mo"> k </em>可以通过<em class="mo"> n_neighbors </em>参数设置，默认为1。</p><p id="be69" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设CNN过程发生在一个块中，那么具有更大的多数类种子样本以有效地移除冗余的例子会更有用。在这种情况下，我们将使用值200。</p><figure class="jv jw jx jy fd jz"><div class="bz dy l di"><div class="mc md l"/></div></figure><p id="20f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该数据集的比率现在约为1:10。，低于1:100。</p><p id="aad5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">计数器({0: 9900，1: 100}) <br/>计数器({0: 940，1: 100})</p><p id="f66a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">创建了变换数据集的散点图，该散点图显示大部分剩余的多数类样本位于类边界周围，而重叠的样本来自少数类。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es me"><img src="../Images/837cd4a5e6f2985b583b0d7908b506fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3sK7cQQYuUvjsYh7_AmFKw.jpeg"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">单边选择欠采样的不平衡数据集散点图</figcaption></figure></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><h1 id="1936" class="kg kh hh bd ki kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld bi translated">赞成的意见</h1><ul class=""><li id="e0c9" class="jd je hh ig b ih le il lf ip mq it mr ix ms jb jm jj jk jl bi translated">当训练数据集很大时，它可以通过减少训练数据样本的数量来帮助改善运行时间和存储问题。</li></ul><h1 id="e9bd" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">面向连接的网络服务(Connection Oriented Network Service)</h1><ul class=""><li id="fe0f" class="jd je hh ig b ih le il lf ip mq it mr ix ms jb jm jj jk jl bi translated">它可能会丢弃可能对构建规则分类器很重要的潜在有用信息。</li><li id="8396" class="jd je hh ig b ih lx il ly ip lz it ma ix mb jb jm jj jk jl bi translated">随机欠采样选择的样本可能有偏差。这也不是人口的准确代表。从而导致实际测试数据集的结果不准确。</li></ul></div></div>    
</body>
</html>