<html>
<head>
<title>Principal Component Analysis (PCA)— Part 1 — Fundamentals and Applications</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析(PCA)第一部分基础与应用</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/principal-component-analysis-pca-part-1-fundamentals-and-applications-8a9fd9de7596?source=collection_archive---------5-----------------------#2021-02-17">https://medium.com/analytics-vidhya/principal-component-analysis-pca-part-1-fundamentals-and-applications-8a9fd9de7596?source=collection_archive---------5-----------------------#2021-02-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="4223" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">主成分分析是最流行、最快和最容易解释的降维技术之一，它利用了变量之间的线性相关性。它的一些应用是:</p><ul class=""><li id="e0de" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">去相关变量；使特征<strong class="ig hi">线性独立</strong></li><li id="918b" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">异常值/噪声消除</li><li id="961c" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">数据可视化</li><li id="532a" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">降维</li></ul><p id="e7f3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下面的文章中，我们将讨论PCA的应用和工作原理。</p><p id="0bf7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">为什么使用主成分分析进行降维有效？</strong></p><p id="4e70" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jq">由于数据中存在共线性(或特征间的线性相关性)，使用PCA进行降维是可行的。</em> </strong>让我们看看是什么意思。想象以下两种情况:</p><ol class=""><li id="f3b7" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jr ji jj jk bi translated"><strong class="ig hi">情况A: </strong>变量x1和x2高度共线(彼此线性相关)</li><li id="8f00" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jr ji jj jk bi translated"><strong class="ig hi">情况B: </strong>变量x1和x2线性无关</li></ol><p id="f35a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们绘制两种情况下x1对x2的散点图:</p><h2 id="0f43" class="js jt hh bd ju jv jw jx jy jz ka kb kc ip kd ke kf it kg kh ki ix kj kk kl km bi translated">案例A</h2><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kn"><img src="../Images/8aac7216f11a18a802fab6939a9e05a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5WQTnM9ppdzcydC-r4cuOQ.jpeg"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">情况A: x1和x2是线性相关的</figcaption></figure><p id="870e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我在图的边界画了一个框，表示数据所在的边界框。</p><p id="4fc5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，x1和x2具有高线性相关性。在这种情况下，这意味着它们具有很大的相关性(2个变量具有高相关性时，它们具有高线性相关性。当3个或更多变量具有高度线性相关性时，相关性并不总是衡量线性相关性的可靠指标，因为相关性一次只计算2个变量之间的线性相关性。我们观察到以下情况:</p><ol class=""><li id="1b6d" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jr ji jj jk bi translated">我们可以看到，数据分布非常接近一条直线(红色)，因此数据沿着直线的分布最大，垂直于直线的分布最小。因此，通过记住数据沿对角线的分布，我们保留了大部分信息。实际上，这就是我们使用PCA进行降维时发生的情况——我们使用数据变化最大的方向来表示数据。</li><li id="5da6" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jr ji jj jk bi translated"><strong class="ig hi"> <em class="jq">边界框的大部分是空的——数据占据了该框的很小一部分(靠近对角线)。</em> </strong>方框的剩余部分不包含数据(下图中黄色部分)。</li></ol><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ld"><img src="../Images/fee2de835973ee47fd0690e00293328c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cz7kHYqEbMvrvUirbxolBA.png"/></div></div></figure><h2 id="4fbc" class="js jt hh bd ju jv jw jx jy jz ka kb kc ip kd ke kf it kg kh ki ix kj kk kl km bi translated">案例B</h2><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kn"><img src="../Images/c1da27f3bab2f83b3a733deae0fbd21c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2eep23W1Uvm5QfFFrT0Zag.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">情况B:x1和x2之间没有线性相关性</figcaption></figure><p id="de97" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">x1和x2没有线性相关性，在这种情况下，这意味着它们没有相关性。我们观察到以下情况:</p><ol class=""><li id="8660" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jr ji jj jk bi translated"><strong class="ig hi">数据不在一条线上</strong>-不像情况A中数据靠近一条线-因此，不存在数据变化“更大”的特殊方向-所有方向都可能包含数据。因此，它没有显示出降维的前景。</li><li id="6583" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jr ji jj jk bi translated"><strong class="ig hi">盒子中的大部分区域包含数据</strong>(不像情况A，我们有没有数据的大块空白区域)。</li></ol><p id="1042" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是一些重要的细节:</p><ol class=""><li id="b64b" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jr ji jj jk bi translated"><strong class="ig hi">变量之间的线性相关性导致数据位于较低维度的子空间——或超平面，如情况a。这导致一个没有数据的大区域。事实上，线性关系越强，未被占用的空间就越大。</strong></li><li id="f4f2" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jr ji jj jk bi translated"><strong class="ig hi">因变量(包括变量之间的关系是非线性的)通常不能占据一个边界框中所有可用的空间，因为它们沿着某些方向一起<em class="jq">变化</em>(因此产生了依赖性)。变量依赖限制了数据可能存在的区域。我们说因变量主要存在于低维流形上。对于PCA，我们将只对线性流形感兴趣。</strong></li></ol></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><h1 id="346b" class="ll jt hh bd ju lm ln lo jy lp lq lr kc ls lt lu kf lv lw lx ki ly lz ma kl mb bi translated">什么是PCA？</h1><p id="4262" class="pw-post-body-paragraph ie if hh ig b ih mc ij ik il md in io ip me ir is it mf iv iw ix mg iz ja jb ha bi translated">PCA将样本和特征的矩阵作为输入，并返回一个新矩阵，其特征是原始矩阵中特征的线性组合。</p><ol class=""><li id="4f29" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jr ji jj jk bi translated">由PCA生成的这些新特征彼此正交(成直角)。</li><li id="c765" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jr ji jj jk bi translated">新要素按方差递减的顺序排序。第一个PC(主成分)解释了最大的方差。最后一个PC解释了最小方差。在情况A中，第一个PC将沿对角线放置，第二个PC将垂直于对角线放置(如第1点所述)</li></ol><h1 id="eaf1" class="ll jt hh bd ju lm mh lo jy lp mi lr kc ls mj lu kf lv mk lx ki ly ml ma kl mb bi translated"><strong class="ak">主成分分析的数据准备和解释:</strong></h1><p id="82c1" class="pw-post-body-paragraph ie if hh ig b ih mc ij ik il md in io ip me ir is it mf iv iw ix mg iz ja jb ha bi translated"><strong class="ig hi">在使用PCA之前，标准化数据非常重要。</strong> PCA测量数据沿正交方向的方差。如果特征A假定值在0–10000的范围内，标准偏差为200，而另一个特征B假定值在0–100的范围内，标准偏差为20，那么自然地，特征A在决定最大方差的方向方面会起更大的作用——仅仅是因为它的方差很大。例如，100个单位的变化导致特征A仅1%(特征A范围的1%)的变化，而它导致特征b 100%(范围的100%)的变化。</p><p id="c51a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们简单看一下<strong class="ig hi">如何解读</strong>PCA产生的成分。我们将PCA拟合到从波士顿住房数据集中选择的3个特征:LSTAT、RM、AGE</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es mm"><img src="../Images/37ad1b1a2ae1961bdea476e8e1fc3a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*-IhN31bnF81H3hrBW98hMA.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx translated">PCs的装载。图A</figcaption></figure><p id="7d84" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于原始数据有3列(或坐标)，我们转换后的数据(在主成分空间中)也将有3列(或坐标)。</p><p id="aba1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上表向我们展示了每个特征在形成变换数据的每个坐标中的重要性/贡献。这意味着在计算转换数据的第<strong class="ig hi">个坐标</strong>时，LSTAT的权重为. 6564，RM的权重为-.5365，age的权重为. 5304。第一坐标对应于第一主分量。</p><p id="dc61" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jq">这可以作为特征选择的基础</em> </strong>如<a class="ae mn" href="https://www.amazon.in/Applied-Predictive-Analytics-Principles-Professional/dp/8126550384/ref=sr_1_6?dchild=1&amp;keywords=predictive+analytics&amp;qid=1613553070&amp;sr=8-6" rel="noopener ugc nofollow" target="_blank">应用预测分析</a>中所述——我们可以保留每个主成分中负载最高的特征。例如，在载荷表中，LSTAT、AGE、LSTAT在第一、第二和第三主成分中分别具有最高载荷. 6564、. 7150、-.7544。因此，我们可以选择LSTAT和AGE作为建模的特征。<strong class="ig hi">这里的优势是，我们不需要将数据转换到主成分空间来减少特征的数量，因此我们保留了可解释性(这很好，因为执行PCA降低了ML管道的可解释性)。</strong></p></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><h1 id="b173" class="ll jt hh bd ju lm ln lo jy lp lq lr kc ls lt lu kf lv lw lx ki ly lz ma kl mb bi translated">主成分分析的应用</h1><h2 id="6bc7" class="js jt hh bd ju jv jw jx jy jz ka kb kc ip kd ke kf it kg kh ki ix kj kk kl km bi translated">1.消除数据中的共线性和相关性</h2><p id="1779" class="pw-post-body-paragraph ie if hh ig b ih mc ij ik il md in io ip me ir is it mf iv iw ix mg iz ja jb ha bi translated">使用PCA转换数据可以消除变量之间的相关性。换句话说，它强制变换后的要素之间没有相关性。让我们使用波士顿住房数据来检查使用PCA前后的相关矩阵。</p><p id="b9c8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">PCA之前:</strong></p><pre class="ko kp kq kr fd mo mp mq mr aw ms bi"><span id="6731" class="js jt hh mp b fi mt mu l mv mw">x = pd.DataFrame(datasets.load_boston().data, columns = datasets.load_boston().feature_names)</span><span id="42df" class="js jt hh mp b fi mx mu l mv mw"># Generate heatmap of the correlation matrix<br/>plt.figure(figsize = (12.5, 7.5))<br/>sns.heatmap(x.corr().round(3), vmax = 1, vmin = -1, fmt = '.2f', annot = True, linecolor = 'white', <br/>            linewidth = .1, annot_kws = {'fontsize': 12, 'weight': 'bold'}, cmap = 'coolwarm')<br/>plt.xticks(fontsize = 12)<br/>plt.yticks(fontsize = 12);<br/>plt.title('Correlation of Variables BEFORE transforming them using PCA', fontsize = 14, weight = 'bold')</span></pre><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es my"><img src="../Images/68c0213efe1a41eb804d905ec27b7a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u3TOFD-f4Zp-j6H2OZ5DkA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">变量之间的相关性，然后使用PCA进行转换</figcaption></figure><p id="d453" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以看到多个相关的特征。例如，RAD和TAX具有很高的相关性。让我们使用PCA转换这些数据，并绘制相关矩阵。</p><p id="94c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">PCA后:</strong></p><pre class="ko kp kq kr fd mo mp mq mr aw ms bi"><span id="5556" class="js jt hh mp b fi mt mu l mv mw">## Load Boston Housing Dataset<br/>from sklearn import datasets<br/>x = pd.DataFrame(datasets.load_boston().data, columns = datasets.load_boston().feature_names)</span><span id="b98b" class="js jt hh mp b fi mx mu l mv mw"># Scale Data to have zero mean and unit variance<br/>scaler = preprocessing.StandardScaler().fit(x)<br/>x = pd.DataFrame(scaler.transform(x), columns = x.columns)</span><span id="7647" class="js jt hh mp b fi mx mu l mv mw"># Fit PCA<br/>pca = decomposition.PCA().fit(x)</span><span id="d03c" class="js jt hh mp b fi mx mu l mv mw"># Get transformed data<br/>x_transformed  =  pd.DataFrame(pca.transform(x), columns = np.arange(1, x.shape[1]+1))</span><span id="379a" class="js jt hh mp b fi mx mu l mv mw"># Generate heatmap of the correlation matrix<br/>plt.figure(figsize = (12.5, 7.5))<br/>sns.heatmap(x_transformed.corr().round(3), vmax = 1, vmin = -1, fmt = '.2f', annot = True, linecolor = 'white', <br/>            linewidth = .1, annot_kws = {'fontsize': 12, 'weight': 'bold'}, cmap = 'coolwarm')<br/>plt.xticks(fontsize = 12)<br/>plt.yticks(fontsize = 12);<br/>plt.ylabel('Principal Component', fontsize = 12, weight = 'bold')<br/>plt.xlabel('Principal Component', fontsize = 12, weight = 'bold')</span><span id="f250" class="js jt hh mp b fi mx mu l mv mw">plt.title('Correlation of Variables AFTER transforming them using PCA', fontsize = 14, weight = 'bold')<br/></span></pre><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mz"><img src="../Images/81f4b97abfc0b4dedd60d67079cefa18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V1ywBsDoLXjba3W6wHNJJg.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">PCA后的相关矩阵—所有互相关为零</figcaption></figure><p id="c899" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用PCA — <strong class="ig hi"> <em class="jq">转换数据移除了数据</em> </strong> <em class="jq">中先前存在的共线性。</em>然而，由于<strong class="ig hi"> PCA仅去除变量之间的线性相关性</strong>；即使在使用PCA变换变量之后，它们可能仍然是相关的——以非线性的方式。</p><h2 id="8c03" class="js jt hh bd ju jv jw jx jy jz ka kb kc ip kd ke kf it kg kh ki ix kj kk kl km bi translated">2.噪声去除和异常值检测</h2><p id="300e" class="pw-post-body-paragraph ie if hh ig b ih mc ij ik il md in io ip me ir is it mf iv iw ix mg iz ja jb ha bi translated">这是PCA的一个重要应用，也是异常检测方法家族的一个特例。<strong class="ig hi">这是一种多元离群点检测方法。</strong> <em class="jq"> </em>这与单变量离群点剔除方法的不同之处在于，单变量方法(如z-score法、tukey法)在检测离群点时独立考虑每个变量，而PCA则相反，<strong class="ig hi">通过同时考虑所有变量的值来检测离群点。</strong></p><p id="2e3b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我们的数据有10列。这个想法是要做到以下几点:</p><ol class=""><li id="7cc7" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jr ji jj jk bi translated">将数据转换到主成分空间。</li><li id="9327" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jr ji jj jk bi translated">保留解释最大方差的主要成分(比如方差的99%)。例如，假设我们保留9列(即9个主成分)。</li><li id="bd84" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jr ji jj jk bi translated">现在，仅使用保留的主成分中包含的信息将数据转换回原始空间。由于我们在步骤2中消除了一个组件，我们不能指望原始数据的完美重建。</li><li id="a9ac" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jr ji jj jk bi translated">测量每次观察的重建误差。大部分数据必须精确重构——因为我们保留了大部分方差。重建不佳的观测值是潜在的异常值。</li></ol><p id="fa8a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">以下是上述算法的两个超参数:</strong></p><ol class=""><li id="7b01" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jr ji jj jk bi translated">步骤2中保留的差异百分比</li><li id="97c6" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jr ji jj jk bi translated">将观察值标记为异常/异常值的阈值重构误差</li></ol><p id="2413" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">和往常一样，从领域的角度分析使用上述方法检测到的异常值通常更好。</p><h2 id="b666" class="js jt hh bd ju jv jw jx jy jz ka kb kc ip kd ke kf it kg kh ki ix kj kk kl km bi translated">3.数据可视化</h2><p id="4ac7" class="pw-post-body-paragraph ie if hh ig b ih mc ij ik il md in io ip me ir is it mf iv iw ix mg iz ja jb ha bi translated">由于前两个主成分解释了大多数方差，我们可以通过前两个主成分的散点图来可视化数据。</p><p id="214c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">前两个主成分提供了数据的最佳二维近似，前提是我们只允许使用线性变换来创建成分，并且我们的损失是MSE。</strong></p><pre class="ko kp kq kr fd mo mp mq mr aw ms bi"><span id="1836" class="js jt hh mp b fi mt mu l mv mw">## Load Boston Housing Dataset<br/>from sklearn import datasets<br/>x = pd.DataFrame(datasets.load_boston().data, columns = datasets.load_boston().feature_names)</span><span id="7812" class="js jt hh mp b fi mx mu l mv mw"># Scale Data to have zero mean and unit variance<br/>scaler = preprocessing.StandardScaler().fit(x)<br/>x = pd.DataFrame(scaler.transform(x), columns = x.columns)</span><span id="ac88" class="js jt hh mp b fi mx mu l mv mw"># Fit PCA - Retain only 2 components<br/>pca = decomposition.PCA(n_components = 2).fit(x)</span><span id="392b" class="js jt hh mp b fi mx mu l mv mw"># Get transformed data<br/>x_transformed  =  pd.DataFrame(pca.transform(x), columns = ['Principal Component 1', 'Principal Component 2'])</span><span id="a51c" class="js jt hh mp b fi mx mu l mv mw"># Generate Scatter plot of the first 2 principal components<br/>plt.figure(figsize = (12.5, 7.5))<br/>#plt.scatter(x_transformed.iloc[:, 0], )<br/>x_transformed.plot.scatter(x = 'Principal Component 1', y = 'Principal Component 2', grid = True, figsize = (10, 6),<br/>                          fontsize = 14)<br/>plt.xlabel('Principal Component 1', fontsize = 14)<br/>plt.ylabel('Principal Component 2', fontsize = 14)</span></pre><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es na"><img src="../Images/bf801c371c997ca0b60c0dcda89f56f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-cwNZQ3hw1ftL4CA_CD2vA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">包含13个要素的波士顿住房数据的可视化-使用前两个主成分</figcaption></figure><p id="9130" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种可视化可用于可视化集群。通常，第三个变量被用来增加颜色。在聚类分析中，颜色变量可以是聚类ID。也可以制作包括前3个主要成分的3d图，尽管有时很难解释。</p></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><h1 id="173d" class="ll jt hh bd ju lm ln lo jy lp lq lr kc ls lt lu kf lv lw lx ki ly lz ma kl mb bi translated">摘要</h1><p id="bb98" class="pw-post-body-paragraph ie if hh ig b ih mc ij ik il md in io ip me ir is it mf iv iw ix mg iz ja jb ha bi translated">总之，主成分可用于去除噪声/检测异常值、去相关变量、在2维中可视化高维数据。它们提供了数据的最佳p(p </p><p id="e7dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PCA有很多种变体，最适合在不同的环境中使用。我将在下一篇文章中发表关于PCA变种的文章。</p></div></div>    
</body>
</html>