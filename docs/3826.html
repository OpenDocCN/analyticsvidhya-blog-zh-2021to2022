<html>
<head>
<title>Text Summarization on Wikipedia pages using NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于自然语言处理的维基百科网页文本摘要</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-summarization-on-wikipedia-38b80d0060b3?source=collection_archive---------3-----------------------#2021-07-30">https://medium.com/analytics-vidhya/text-summarization-on-wikipedia-38b80d0060b3?source=collection_archive---------3-----------------------#2021-07-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="edae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文涵盖了使用python在Wikipedia页面上实现提取和抽象文本摘要的实践。</p><h2 id="4499" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">为什么是文本摘要？</h2><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es jx"><img src="../Images/5f2cb503494461543b699605de79c713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1SmNYd1MtrW5TtRno5ruQg.jpeg"/></div></div><figcaption class="kj kk et er es kl km bd b be z dx translated">格伦·凯莉在Unsplash上拍摄的照片</figcaption></figure><p id="1f7f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随着数字内容的不断增长，在今天的自动化世界中，如果我们不用阅读大量的新闻文章、报纸或电子书形式的文本数据，而只需几个有意义的句子就可以获取所有内容，那该有多酷？</p><p id="1f58" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是文本摘要发挥作用的地方！让我们在下面深入了解一下。</p><h2 id="d1c1" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">文本摘要:</h2><blockquote class="kn ko kp"><p id="e89b" class="ie if kq ig b ih ii ij ik il im in io kr iq ir is ks iu iv iw kt iy iz ja jb ha bi translated">文本摘要是NLP中使用的一种技术，用于从文本资源(如文章、书籍、研究论文甚至网页)中创建简短的有意义的文本集合，称为摘要。</p></blockquote><h2 id="ab33" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">文本摘要技术的类型:</h2><p id="1e74" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">基于其创建的文本摘要的方式可以分为两种类型，</p><ol class=""><li id="b5e7" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb le lf lg lh bi translated"><strong class="ig hi">抽取摘要:</strong>在抽取摘要中，从整个文本数据中选择最重要的句子，并作为摘要一起列出。</li><li id="90f7" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated"><strong class="ig hi">抽象概括:</strong>抽象概括方法比抽取概括更复杂。这里，摘要者首先理解文档的主要概念，然后生成原始文档中没有的新句子。</li></ol><h2 id="1a3d" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">摘录摘要:</h2><p id="6328" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">让我们来看看一些常用的摘要技术。</p><h2 id="126f" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">文本排名算法:</h2><p id="e7e2" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">通过首先将文本数据分成句子并创建相似性矩阵来执行文本排名算法。相似度矩阵包含句子之间的相似度，然后转换成以句子为顶点、相似度得分为边的图，用于句子排名计算。并将排名靠前的句子列为摘要。</p><p id="db5e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们看看如何使用Python执行文本排名算法。</p><pre class="jy jz ka kb fd ln lo lp lq aw lr bi"><span id="f0a3" class="jc jd hh lo b fi ls lt l lu lv"># Importing genism package and summarizer<br/>import gensim<br/>from gensim.summarization.summarizer import summarize<br/>from gensim.summarization import keywords</span><span id="b7e0" class="jc jd hh lo b fi lw lt l lu lv">import wikipedia</span></pre><p id="82e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们获取与“微软”相关的维基百科内容</p><pre class="jy jz ka kb fd ln lo lp lq aw lr bi"><span id="cde1" class="jc jd hh lo b fi ls lt l lu lv"># Get wiki content.<br/>wikisearch = wikipedia.page("Microsoft")<br/>wikicontent = wikisearch.content<br/>print(wikicontent)</span></pre><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lx"><img src="../Images/383592116882b14c9b2feedec2137482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZBzqdomiRQSaAbKGca50iw.png"/></div></div></figure><p id="a935" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们使用TextRank算法通过创建一个为其原始内容的0.1%的摘要来进行总结。</p><pre class="jy jz ka kb fd ln lo lp lq aw lr bi"><span id="95c8" class="jc jd hh lo b fi ls lt l lu lv"># Summary by 0.1% of the original content<br/>summary_ratio = summarize(wikicontent, ratio = 0.01)<br/>print("summary by ratio")<br/>print(summary_ratio)</span></pre><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ly"><img src="../Images/f7b555f57ba0a6551935e47274674452.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oJXOoxy6OCw7ggOoZmglcA.png"/></div></div></figure><p id="79d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们按字数总结一下。这里我们使用了200个单词来创建摘要。</p><pre class="jy jz ka kb fd ln lo lp lq aw lr bi"><span id="6a02" class="jc jd hh lo b fi ls lt l lu lv">#summary by count of words<br/>summary_wordcount = summarize(wikicontent, word_count = 200)<br/>print("summary by word count")<br/>print(summary_wordcount)<br/></span></pre><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lz"><img src="../Images/7648ef50fc77e4247213cc6535dfa8af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ku-EFaREp7cUTa6aiXDBaw.png"/></div></div></figure><p id="b11b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们尝试另一个叫做LSA(潜在语义分析)的抽象概括器</p><h2 id="db58" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">LSA(潜在语义分析):</h2><p id="35ed" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">LSA方法通过诸如一起使用的单词以及在不同的句子中看到哪些常见单词的信息来提取语义重要的句子。句子中大量的常用词表明句子在语义上是相关的。这些语义上有意义的句子然后被列在一起作为摘要。</p><p id="7499" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们在上面的代码中实现LSA。</p><pre class="jy jz ka kb fd ln lo lp lq aw lr bi"><span id="6c09" class="jc jd hh lo b fi ls lt l lu lv"># importing the summarizer<br/>from sumy.summarizers.lsa import LsaSummarizer</span><span id="153b" class="jc jd hh lo b fi lw lt l lu lv"># creating the LSA summarizer with summary of 5 sentences<br/>lsa_summarizer=LsaSummarizer()<br/>lsa_summary= lsa_summarizer(my_parser.document,5)<br/>for sentence in lsa_summary:<br/>  print(sentence)</span></pre><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ma"><img src="../Images/8c8d64cfb0f1314986e9f4984ddd863d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*phBogqsOu0oOc9oRko_GnQ.png"/></div></div></figure><p id="c17b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然上述抽象的摘要更容易实现，并且有助于突出长文本文档中的关键句子，但是如果没有足够的上下文，这些摘要中的一些可能更难理解。</p><p id="107d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种情况下，在生成有意义的和简明的摘要时要考虑文档的上下文，我们可以采用抽象的摘要方法。</p><h2 id="9d26" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">抽象概括:</h2><p id="1f22" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">在摘要中，最能描述整个文档的新句子被列为摘要。</p><h2 id="2911" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">在本文中，我们将讨论用cnn新闻数据预先训练的<strong class="ak">巴特变形金刚</strong>。\</h2><h2 id="70aa" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated"><strong class="ak">巴特变形金刚:</strong></h2><blockquote class="kn ko kp"><p id="54bb" class="ie if kq ig b ih ii ij ik il im in io kr iq ir is ks iu iv iw kt iy iz ja jb ha bi translated">BART( <strong class="ig hi">双向和自回归变换器</strong>)使用标准的seq2seq机器翻译架构，带有双向编码器(类似于BERT)和从左到右解码器(类似于GPT)。预训练任务包括随机改变原始句子的顺序和一个新颖的填充方案，其中文本的跨度用单个掩码标记代替。</p></blockquote><p id="3819" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在BART中，输入文本首先通过<em class="kq">双向编码器</em>，即类似BERT的编码器。然后从左到右和从右到左查看文本，后续输出用于<em class="kq">自回归解码器</em>，该解码器基于编码器输入<em class="kq">和</em>到目前为止预测的输出令牌来预测输出。</p><p id="48c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">更多关于BART的细节可以在<a class="ae mb" href="https://huggingface.co/transformers/model_doc/bart.html#transformers.BartForConditionalGeneration" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="16d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了实现上面数据中的BART，我们先安装<a class="ae mb" href="https://huggingface.co/transformers/pretrained_models.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">拥抱脸变形金刚</strong> </a>。这个库运行在Pytorch和TensorFlow之上，可用于各种语言任务，如摘要、问题回答等。</p><pre class="jy jz ka kb fd ln lo lp lq aw lr bi"><span id="cc3d" class="jc jd hh lo b fi ls lt l lu lv"># importing BART<br/>from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig</span></pre><p id="ae48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们加载BART模型和记号赋予器，</p><pre class="jy jz ka kb fd ln lo lp lq aw lr bi"><span id="6b85" class="jc jd hh lo b fi ls lt l lu lv"># Loading the model and tokenizer for bart-large-cnn<br/>tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')<br/>model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')</span></pre><p id="4021" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们将输入文本数据提供给tokenizer.batch_encode_plus()方法，并生成摘要id。</p><pre class="jy jz ka kb fd ln lo lp lq aw lr bi"><span id="2622" class="jc jd hh lo b fi ls lt l lu lv">inputs = tokenizer.batch_encode_plus([wikicontent],return_tensors='pt',truncation=True)<br/>summary_ids = model.generate(inputs['input_ids'], early_stopping=True)</span></pre><p id="0f83" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们解码摘要id并打印摘要输出。</p><pre class="jy jz ka kb fd ln lo lp lq aw lr bi"><span id="b6a0" class="jc jd hh lo b fi ls lt l lu lv"># Decoding and printing the summary<br/>bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)<br/>print(bart_summary)</span></pre><p id="6293" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">摘要输出如下所示，</p><pre class="jy jz ka kb fd ln lo lp lq aw lr bi"><span id="7773" class="jc jd hh lo b fi ls lt l lu lv">Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. It rose to dominate the personal computer operating system market with MS-DOS in the mid-1980s, followed by Microsoft Windows. The company's 1986 initial public offering (IPO) created three billionaires and an estimated 12,000 millionaires among Microsoft employees. In 2018 Microsoft reclaimed its position as the most valuable publicly traded company in the world. As of 2020, Microsoft has the third-highest global brand valuation.</span></pre><p id="c4b9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">简短、精确，看起来比以前的总结好得多，对吗？请让我知道你的评论和反馈。</p><p id="5e87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">注:</strong>你可以在这个<a class="ae mb" href="https://github.com/sangee1301/Text-Summarization/blob/main/Text_Summarization.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub Repo </a>里找到完整的代码。</p><h2 id="8b67" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">参考资料:</h2><ol class=""><li id="a00b" class="kz la hh ig b ih ku il kv ip mc it md ix me jb le lf lg lh bi translated">与BART. <a class="ae mb" href="http://cs230.stanford.edu/projects_spring_2020/reports/38866168.pdf" rel="noopener ugc nofollow" target="_blank">相关的新冠肺炎出版物摘要</a></li><li id="c647" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">BERT:用于语言理解的深度双向转换器的预训练。<a class="ae mb" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1810.04805.pdf</a></li><li id="a6e3" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated"><a class="ae mb" href="https://huggingface.co/transformers/model_doc/bart.html#transformers.BartForConditionalGeneration" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/transformers/model _ doc/Bart . html # transformers。bartforconditional generation</a></li></ol></div></div>    
</body>
</html>