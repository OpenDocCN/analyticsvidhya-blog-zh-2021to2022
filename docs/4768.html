<html>
<head>
<title>Sequence to Sequence models (with implementation — make a translator) — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">序列到序列模型(包括实现—制作翻译器)—第2部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sequence-to-sequence-models-with-implementation-make-a-translator-part-2-49dbccd3d7d6?source=collection_archive---------1-----------------------#2022-02-08">https://medium.com/analytics-vidhya/sequence-to-sequence-models-with-implementation-make-a-translator-part-2-49dbccd3d7d6?source=collection_archive---------1-----------------------#2022-02-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="6c27" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们扩展了我们在第一部分  <strong class="ig hi"> </strong>中构建的seq2seq模型，并为其添加了“注意”。我之前在第一部分  <strong class="ig hi">中已经提到了序列对序列模型的需求，使用RNNs对其建模。</strong></p><p id="bdd4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，本文有一些代码片段。<strong class="ig hi">完整代码库</strong> <a class="ae jc" href="https://colab.research.google.com/drive/1Fy5nYokwJR3TDek-yS8QC75WDpV_fDxa" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">点击此处</strong> </a> <strong class="ig hi">。</strong></p><p id="3acc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我坚持看完第一部分，以便轻松进入第二部分</p></div><div class="ab cl je jf go jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="ha hb hc hd he"><h1 id="66a3" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">为什么关注？</h1><p id="3b8d" class="pw-post-body-paragraph ie if hh ig b ih kj ij ik il kk in io ip kl ir is it km iv iw ix kn iz ja jb ha bi translated">我们希望编码器记住输入句子中的重要单词，这些单词会影响解码器解码编码的方式。为此我们使用<strong class="ig hi">注意机制。</strong></p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es ko"><img src="../Images/023c7860b6e9ec3d81150a64b26bf329.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*RgF1q7ZpgvrS_JAj.jpg"/></div></figure><h2 id="a237" class="kw jm hh bd jn kx ky kz jr la lb lc jv ip ld le jz it lf lg kd ix lh li kh lj bi translated">建立直觉</h2><p id="b312" class="pw-post-body-paragraph ie if hh ig b ih kj ij ik il kk in io ip kl ir is it km iv iw ix kn iz ja jb ha bi translated">让我们考虑下面的翻译:</p><pre class="kp kq kr ks fd lk ll lm ln aw lo bi"><span id="4ce4" class="kw jm hh ll b fi lp lq l lr ls">ENGLISH<br/>I have a blue cat who ate a can of dog food two days ago .</span><span id="fd01" class="kw jm hh ll b fi lt lq l lr ls">SPANISH<br/>Tengo un gato azul que se comió una lata de comida para perros hace dos días.</span></pre><p id="a701" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于预测解码器中每个西班牙语单词的模型，它需要知道哪些英语单词对解码器将预测的下一个单词影响最大。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es lu"><img src="../Images/0522c1c3b9228553d9db5caf244facac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vgM3tKQd_u9CmUySW1SZ7g.png"/></div></div></figure><pre class="kp kq kr ks fd lk ll lm ln aw lo bi"><span id="071e" class="kw jm hh ll b fi lp lq l lr ls">Since we need to predict 'gato' (cat in spanish), the english word 'cat' has the highest weight and hence importance during prediction.</span><span id="a5fc" class="kw jm hh ll b fi lt lq l lr ls">This otherwise would have been lost due to <strong class="ll hi">vanishing gradients</strong> since the word 'cat' is at the beginning of the input sentence.</span></pre><h2 id="cd41" class="kw jm hh bd jn kx ky kz jr la lb lc jv ip ld le jz it lf lg kd ix lh li kh lj bi translated">进入细节</h2><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es lz"><img src="../Images/89ae97a7a3b1ca0be12e8aa44abe8154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*epj4ErwHZcxYA1nXa2kbCg.png"/></div></div></figure><p id="7ed3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">考虑上面的模型</p><pre class="kp kq kr ks fd lk ll lm ln aw lo bi"><span id="aba0" class="kw jm hh ll b fi lp lq l lr ls"><strong class="ll hi">α1 α2</strong> ... <strong class="ll hi">α7</strong>  -&gt;  the weights for attention<br/><strong class="ll hi">h1 h2 ... h7  </strong>-&gt; output of each RNN layer in the encoder<br/><strong class="ll hi">S0</strong> -&gt; Hidden state of the last RNN layer </span><span id="bb4d" class="kw jm hh ll b fi lt lq l lr ls">[h1, h2, ... h7] =&gt; H<br/>[S0]  -&gt; S</span></pre><h2 id="ae61" class="kw jm hh bd jn kx ky kz jr la lb lc jv ip ld le jz it lf lg kd ix lh li kh lj bi translated">计算注意力权重(alphas)</h2><p id="487e" class="pw-post-body-paragraph ie if hh ig b ih kj ij ik il kk in io ip kl ir is it km iv iw ix kn iz ja jb ha bi translated">我们引入了可训练的参数:WQ(查询权重)和WK(关键权重)。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es ma"><img src="../Images/4bd3302e7e884b0463dd6ca565ecc69e.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/1*d2qTGQgNBgiVdT66-gVbig.gif"/></div><figcaption class="mb mc et er es md me bd b be z dx translated">计算密钥(可以看作是一个密集层)</figcaption></figure><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es mf"><img src="../Images/a67ccf24b23cd8626b03547806082fc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/1*NNZppiSltMDKDLK-9l2lqQ.gif"/></div><figcaption class="mb mc et er es md me bd b be z dx translated">计算查询(可以看作是一个密集层)</figcaption></figure><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es mg"><img src="../Images/e8cca7c1a54b8868cf19d800fa843b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/1*oEOprQ1LE5XMbOynM8M3kg.gif"/></div><figcaption class="mb mc et er es md me bd b be z dx translated">计算注意力权重</figcaption></figure><p id="8024" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">讨论形状</p><pre class="kp kq kr ks fd lk ll lm ln aw lo bi"><span id="6f3e" class="kw jm hh ll b fi lp lq l lr ls">H -&gt; (64,16,1024)<br/>WK -&gt; (1024,1024)<br/>K -&gt; (64, 16, 1024)</span><span id="19a1" class="kw jm hh ll b fi lt lq l lr ls">S -&gt; (64,1,1024)<br/>WQ -&gt; (1024,1024)<br/>Q -&gt; (64,1,1024)</span><span id="174e" class="kw jm hh ll b fi lt lq l lr ls">alpha -&gt; (64,16,1)</span></pre><h2 id="aa37" class="kw jm hh bd jn kx ky kz jr la lb lc jv ip ld le jz it lf lg kd ix lh li kh lj bi translated">计算上下文向量</h2><p id="92e4" class="pw-post-body-paragraph ie if hh ig b ih kj ij ik il kk in io ip kl ir is it km iv iw ix kn iz ja jb ha bi translated">上下文向量乘以RNN输出(H)。它可以被直观地视为将每个单词输出乘以一些“重要性”权重。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es mh"><img src="../Images/5838e5494c2660ebb502506a65b3d8a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/1*hbeBg-oWY7t9qwwvAUEcjg.gif"/></div><figcaption class="mb mc et er es md me bd b be z dx translated">计算上下文向量</figcaption></figure><h2 id="9f7b" class="kw jm hh bd jn kx ky kz jr la lb lc jv ip ld le jz it lf lg kd ix lh li kh lj bi translated">继续看解码器</h2><p id="067e" class="pw-post-body-paragraph ie if hh ig b ih kj ij ik il kk in io ip kl ir is it km iv iw ix kn iz ja jb ha bi translated">解码器(E)中当前单词的单词嵌入与上下文向量(C)连接。这个联合向量被用作RNN中的隐藏状态。</p><p id="ee83" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意，在解码器预测每个新单词之前，计算新的上下文向量。</p><h2 id="f1c5" class="kw jm hh bd jn kx ky kz jr la lb lc jv ip ld le jz it lf lg kd ix lh li kh lj bi translated">关注点积编码</h2><figure class="kp kq kr ks fd kt"><div class="bz dy l di"><div class="mi mj l"/></div></figure></div><div class="ab cl je jf go jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="ha hb hc hd he"><h1 id="b0b5" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">用心构建解码器</h1><h2 id="dca7" class="kw jm hh bd jn kx ky kz jr la lb lc jv ip ld le jz it lf lg kd ix lh li kh lj bi translated">密码</h2><figure class="kp kq kr ks fd kt"><div class="bz dy l di"><div class="mi mj l"/></div></figure></div><div class="ab cl je jf go jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="ha hb hc hd he"><h1 id="4cb5" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">培养</h1><h2 id="e7ce" class="kw jm hh bd jn kx ky kz jr la lb lc jv ip ld le jz it lf lg kd ix lh li kh lj bi translated">密码</h2><figure class="kp kq kr ks fd kt"><div class="bz dy l di"><div class="mi mj l"/></div></figure></div><div class="ab cl je jf go jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="ha hb hc hd he"><h1 id="35c3" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">结果</h1><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es mk"><img src="../Images/fedc71d6b3324f7da419890cb8775c30.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*VtLLVyz5Cwr0JPJZYWPGaA.png"/></div></figure><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es ml"><img src="../Images/c07a7b7a8a0f685805cb2a651c8bb74c.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*6bmwaZKlRicWxOLVuHhYdg.png"/></div></figure></div><div class="ab cl je jf go jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="ha hb hc hd he"><h1 id="70b2" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">翻译</h1><figure class="kp kq kr ks fd kt"><div class="bz dy l di"><div class="mi mj l"/></div></figure><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es mm"><img src="../Images/717c9d0cfce854ced492ba1ecec4cb40.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*QKwikJnJ9saseOaD9GDXPA.png"/></div></figure></div><div class="ab cl je jf go jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="ha hb hc hd he"><h1 id="10cd" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">我们真的需要RNNs吗？</h1><h2 id="5ca2" class="kw jm hh bd jn kx ky kz jr la lb lc jv ip ld le jz it lf lg kd ix lh li kh lj bi translated">注意力提供了什么？</h2><ol class=""><li id="0b5d" class="mn mo hh ig b ih kj il kk ip mp it mq ix mr jb ms mt mu mv bi translated">考虑所有输入单词(双向、顺序rnn)</li><li id="51f8" class="mn mo hh ig b ih mw il mx ip my it mz ix na jb ms mt mu mv bi translated">计算试图求解消失梯度(GRUs/lstm)的上下文向量</li><li id="4b8c" class="mn mo hh ig b ih mw il mx ip my it mz ix na jb ms mt mu mv bi translated">允许在前向和后向传播期间并行计算权重，因为仅涉及密集层。</li></ol><pre class="kp kq kr ks fd lk ll lm ln aw lo bi"><span id="da10" class="kw jm hh ll b fi lp lq l lr ls">We see that attention can do a lot of things RNNs can do.<br/>Hence it is worth to see an experimental setting where we remove RNNs entirely and use only attention.</span><span id="f8ec" class="kw jm hh ll b fi lt lq l lr ls">It is in fact proven that Attention performs faster and much better than RNNs. </span><span id="4459" class="kw jm hh ll b fi lt lq l lr ls">This research work was done in a paper called Attention is all you need.</span><span id="5648" class="kw jm hh ll b fi lt lq l lr ls">We'll naively implement pure attention in <strong class="ll hi">Part 3 (COMING SOON!)</strong></span></pre><h1 id="b402" class="jl jm hh bd jn jo nb jq jr js nc ju jv jw nd jy jz ka ne kc kd ke nf kg kh ki bi translated">对于整个代码库访问</h1><div class="ng nh ez fb ni nj"><a href="https://colab.research.google.com/drive/1Fy5nYokwJR3TDek-yS8QC75WDpV_fDxa" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab dw"><div class="nl ab nm cl cj nn"><h2 class="bd hi fi z dy no ea eb np ed ef hg bi translated">Seq2Seq实现</h2></div><div class="nq l"><div class="nr l ns nt nu nq nv ku nj"/></div></div></a></div><h1 id="6054" class="jl jm hh bd jn jo nb jq jr js nc ju jv jw nd jy jz ka ne kc kd ke nf kg kh ki bi translated">参考</h1><ol class=""><li id="2b9c" class="mn mo hh ig b ih kj il kk ip mp it mq ix mr jb ms mt mu mv bi translated">https://www.youtube.com/watch?v=pLpzU-xGi2E&amp;t = 3028s</li><li id="da28" class="mn mo hh ig b ih mw il mx ip my it mz ix na jb ms mt mu mv bi translated"><a class="ae jc" href="https://github.com/YanXuHappygela/NLP-study/blob/master/seq2seq_with_attention.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/yanxuhapygela/NLP-study/blob/master/seq 2 seq _ with _ attention . ipynb</a></li><li id="9471" class="mn mo hh ig b ih mw il mx ip my it mz ix na jb ms mt mu mv bi translated">【https://www.youtube.com/watch?v=B3uws4cLcFw】T2&amp;list = plgtf 4d 9 zhho 8 p _ zdkstvqvtkv 80 jhhxoe</li></ol></div></div>    
</body>
</html>