# 降低非线性子空间中的数据维数:t-SNE

> 原文：<https://medium.com/analytics-vidhya/reducing-data-dimensions-in-a-non-linear-subspace-t-sne-f6925c34281b?source=collection_archive---------16----------------------->

![](img/6ef1aeddca37d361149d466bc4b8f5e9.png)

卢卡斯·布拉塞克在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

**T-分布式随机邻居嵌入(t-SNE)** 是一项获奖的非线性降维技术，特别适合于高维数据集的可视化。但我们先讨论一些背景。

**维度的诅咒:**看看下面一些随机的数据点，绘制在一维(线)上，然后投射到二维平面上，再进一步投射到三维空间。

你意识到上面的数字发生了什么吗？随着维数的增加，空间的体积增加，同样的数据随着维数的增加变得越来越稀疏。*随着维度数量的增加，开发有意义的机器学习模型所需的数据量呈指数增长*。这种现象被称为维数灾难。收集&过程中更多的数据是昂贵的，&有时根本不可能得到。

*但高维数据并不总是坏的，*更多的维度或特征也意味着更多的信息，更多的方式来看待数据&挑选你想要的。事实上，最好的方法是获得尽可能多的特性，然后根据用例选择合适的特性。

*有两种减少维数的常用方法，也称为特征:*

**方法 1:特性选择:**最直接的方法是删除一些特性。可以基于 a)用例&领域知识或 b)基于这些度量中的任何一个来选择要删除的特性:*缺失值比率、低方差过滤器、高相关性过滤器、随机森林模型、向前&向后逐步回归。*

**方法 2:降维:**不是丢弃任何特征，而是通过组合现有特征来创建新特征。然后，原始数据点被投影到这个新的坐标系上&，包含有意义信息的尺寸被保留。根据我们是希望将数据投影到线性还是非线性子空间的决定，降维技术进一步分为线性还是非线性。

**线性降维:**用原始维度的线性组合来创建新维度。*把线性变换想象成平移&拉伸尺度。* PCA 是应用最广泛的线性技术。它利用特征值分解或奇异值分解来正交旋转原始轴，因此新的坐标系将自己对准最大方差的方向。要详细了解 PCA 和 SVD，请参考我的另一篇文章: [**A bare it all 采访 PCA**](https://www.linkedin.com/pulse/bare-all-interview-pca-arvind-shukla/?trackingId=7libxUqRwziP0F2PLC4uyg%3D%3D) 。

**非线性降维:**当原始维度的非线性组合更有意义时使用。非线性转换会对数据产生更大的变化。

例如，从[维基百科](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)看下图。它显示了将二维数据从一个区域减少到一条线的效果。蓝线是创建的线性尺寸，而红线是从数据点创建的非线性尺寸。正如我们所看到的，将所有灰色数据点减少到蓝线上将是垂直轴上信息的巨大损失。在这种情况下，创建非线性维度(如红色线)比创建线性蓝色线能捕捉更多的数据差异。

**t-SNE:** 是一种非线性投影技术。它支持分布式随机邻居嵌入。

**t-SNE 的直觉:**正如我们在本文开头所看到的，点在低维中趋向于变得密集&这导致了过度拥挤。t-SNE 背后的想法很简单，*我们希望保留数据的原始结构&避免过度拥挤。这意味着，我们希望确保相似的对象保持相似&不相似的对象在将数据从高维映射到低维平面时保持不相似。*为此，我们首先定义一个函数，该函数表示高维数据点之间的相似性。然后，我们定义另一个函数来表示低维数据点之间的相似性。然后，我们迭代&在更低的维度上重新排列我们的数据点，这样这两个函数之间的差异就可以最小化。这将在两个维度上给我们相似的数据点。

让我们把它分成 3 个步骤&看看每一步:

**第一步:** *为高维数据创建相似度函数:*计算高维平面上所有数据点之间的欧氏距离。假设该距离遵循高斯分布，计算概率分布函数。这是数据点之间相似性(距离)的条件概率分布函数。根据这个条件分布函数，计算每一对数据点的联合概率分布函数。

**第二步:** *为低维数据创建相似度函数:*在低维空间中随机取 n (n =高维数据点的原始个数)个数据点。按照步骤 1 为这个数据集创建联合概率分布，但是这次不使用高斯分布，而是使用 t 分布。这是因为 t 分布是一种重尾分布&在低维空间使用它可以缓解拥挤问题。

**第三步:** *最小化两个分布的差异:*使用 [Kullback-Leiber 散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (KL 散度)计算两个联合概率分布的差异。计算两个分布之间的 KL 散度，然后以进一步减少 KL 散度的方式重新组织低维数据集。使用梯度下降迭代优化这些嵌入&成本函数是两个分布之间的 KL 散度。然而，成本函数是非凸的，这意味着存在陷入局部最小值的风险。

一旦 KL 散度最小，这意味着两个分布彼此相似，换句话说，这意味着较低维度中的数据点与较高维度中的数据点相似。

**结论:**简而言之，t-SNE 就是为高维数据点定义一个基于高斯的联合概率分布函数，为低维数据点定义一个基于 t-分布的联合概率分布函数&，然后重新排列低维数据点以减少两个分布之间的差异(就 KL 散度而言)。从高斯分布到 t 分布的映射用于利用 t 分布的重尾特性&，因此可以避免过度拥挤问题。

这是文章的结尾。我希望你喜欢阅读它&现在对 t-SNE 算法有了更好的直觉。

*最初发表于*[*【https://www.linkedin.com】*](https://www.linkedin.com/pulse/reducing-data-dimensions-non-linear-subspace-t-sne-arvind-shukla/)*。*