<html>
<head>
<title>Best NLP Algorithms to get Document Similarity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">获取文档相似度的最佳自然语言处理算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/best-nlp-algorithms-to-get-document-similarity-a5559244b23b?source=collection_archive---------0-----------------------#2021-05-27">https://medium.com/analytics-vidhya/best-nlp-algorithms-to-get-document-similarity-a5559244b23b?source=collection_archive---------0-----------------------#2021-05-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/5e11178c38959abf1f88b840043d77bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TZ9VDQI0vwcYO-41"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">在<a class="ae hu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae hu" href="https://unsplash.com/@jaredd_craig?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jaredd Craig </a>拍照</figcaption></figure><div class=""/><p id="2c2b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你有没有读过一本书，发现这本书和你以前读过的另一本书很相似？我已经做了。实际上，我读过的所有自助书籍都与拿破仑·希尔的书相似。</p><p id="c520" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所以我想知道自然语言处理(<a class="ae hu" href="https://machinelearningmastery.com/natural-language-processing/" rel="noopener ugc nofollow" target="_blank"> NLP </a>)是否可以模仿人类的这种能力，找到文档之间的相似之处。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="e2ce" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">相似性问题</h1><p id="0042" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">要找到文本之间的相似性，首先需要定义两个方面:</p><ol class=""><li id="fbd0" class="lc ld hx iw b ix iy jb jc jf le jj lf jn lg jr lh li lj lk bi translated">将用于计算嵌入之间相似性的相似性方法。</li><li id="85c1" class="lc ld hx iw b ix ll jb lm jf ln jj lo jn lp jr lh li lj lk bi translated">将用于将文本转换为嵌入的算法，嵌入是在向量空间中表示文本的一种形式。</li></ol></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="2d3c" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">相似方法</h1><h2 id="0190" class="lq ka hx bd kb lr ls lt kf lu lv lw kj jf lx ly kn jj lz ma kr jn mb mc kv md bi translated">余弦相似性</h2><p id="b2e3" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated"><strong class="iw hy">余弦相似度</strong>测量两个嵌入之间角度的余弦值。当嵌入指向相同方向时，它们之间的角度为零，因此它们的余弦相似度为1。当嵌入正交时，它们之间的角度为90度，余弦相似度为0。最后，当它们之间的角度为180度时，余弦相似度为-1。</p><p id="cb65" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">-1到1是余弦相似性可以变化的值的范围，其中嵌入越接近1越相似。</p><figure class="mf mg mh mi fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es me"><img src="../Images/e8d96c51145501cfcbb73dc547c02f18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S9LdEn9NyZ7bbABUJtZvFw.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">图片来自<a class="ae hu" href="https://datascience-enthusiast.com/DL/Operations_on_word_vectors.html" rel="noopener ugc nofollow" target="_blank">https://data science-发烧级. com/DL/Operations _ on _ word _ vectors . html</a>显示了余弦相似度为1的情况，因为法国和意大利相关，而其他情况下相似度为0，因为ball与鳄鱼不相似</figcaption></figure><p id="b936" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">从数学上讲，您可以计算余弦相似性，方法是取嵌入之间的点积，然后除以嵌入范数的乘积，如下图所示。</p><figure class="mf mg mh mi fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mj"><img src="../Images/dabb0ca6a1bbcf81c7cc35aee293535c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JJXxsZ1SdnaLZPXvVyxDbQ.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">余弦相似公式</figcaption></figure><p id="f9fc" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在python中，可以使用sklearn包中的<a class="ae hu" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html" rel="noopener ugc nofollow" target="_blank">余弦_相似度</a>函数来计算相似度。</p><h1 id="807a" class="jz ka hx bd kb kc mk ke kf kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw bi translated">欧几里得距离</h1><p id="bae7" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated"><strong class="iw hy">欧几里德距离</strong>可能是应用勾股定理计算两点间距离的最著名公式之一。要得到它，你只需要从向量中减去点，将它们提升到平方，将它们相加，然后取它们的平方根。看起来复杂吗？不要担心，在下图中会更容易理解。</p><figure class="mf mg mh mi fd hj er es paragraph-image"><div class="er es mp"><img src="../Images/bec33d0e94a4c6dbc13a07eb372943a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*F507EfvD_1GAA42iLcCANw.png"/></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">欧几里德距离公式</figcaption></figure><p id="0d74" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在python中，你可以使用sklearn包中的<a class="ae hu" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html" rel="noopener ugc nofollow" target="_blank">欧几里德距离</a>函数来计算它。</p><p id="f11a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你也可以使用其他度量标准，比如<em class="mq"> Jaccard、Manhattan和</em> Minkowski <em class="mq"> distance，但在本文中不会讨论。</em></p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="dcfa" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">预处理</h1><p id="8a75" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">在NLP问题中，通常让文本首先进入预处理管道。因此，对于所有用于将文本转换成嵌入的技术，首先使用以下步骤对文本进行预处理:</p><ol class=""><li id="cd96" class="lc ld hx iw b ix iy jb jc jf le jj lf jn lg jr lh li lj lk bi translated">规范化:将文本转换为小写，并删除所有特殊字符和标点符号。</li><li id="4e51" class="lc ld hx iw b ix ll jb lm jf ln jj lo jn lp jr lh li lj lk bi translated">标记化:获取规范化的文本，并将其拆分成一个标记列表。</li><li id="8389" class="lc ld hx iw b ix ll jb lm jf ln jj lo jn lp jr lh li lj lk bi translated">删除停用词:停用词是一种语言中最常用的词，不会给文本增加太多意义。一些例子是单词‘the’，‘a’，‘will’，…</li><li id="95ca" class="lc ld hx iw b ix ll jb lm jf ln jj lo jn lp jr lh li lj lk bi translated">词干化:这是获取单词词根的过程，有时这个词根不等于单词的词根，但词干化的目标是使相关单词映射到同一个词干。例子:分支和分支成为分支。</li><li id="6d56" class="lc ld hx iw b ix ll jb lm jf ln jj lo jn lp jr lh li lj lk bi translated">词汇化:这是从一组词形变化的词中获取同一个词的过程，最简单的方法是用字典。例子:是，曾经是，正在成为。</li></ol><p id="59e9" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个管道的输出是一个带有格式化令牌的列表。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="87c4" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">将文本转换成嵌入内容的算法</h1><h2 id="8885" class="lq ka hx bd kb lr ls lt kf lu lv lw kj jf lx ly kn jj lz ma kr jn mb mc kv md bi translated">术语频率-逆文档频率(TF-IDF)</h2><p id="914a" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">在谈论TF-IDF之前，我将谈论将单词转换成嵌入的最简单的形式，即<strong class="iw hy">文档-术语矩阵</strong>。在这种技术中，你只需要建立一个矩阵，其中每一行是一个短语，每一列是一个标记，单元的值是一个单词在短语中出现的次数。</p><figure class="mf mg mh mi fd hj er es paragraph-image"><div class="er es mr"><img src="../Images/336fb0ddf44feef4b26ad48195981b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*vOFvnrPM3fSG9-1mTeEDqQ.jpeg"/></div><figcaption class="hq hr et er es hs ht bd b be z dx translated"><strong class="bd kb">文档-术语矩阵示例</strong></figcaption></figure><p id="4b1a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">之后，要获得两个短语之间的相似性，您只需要选择相似性方法，并将其应用于短语行。这种方法的主要问题是所有的单词在短语中被认为具有相同的重要性。</p><p id="7228" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了解决这个问题<strong class="iw hy"> TF-IDF </strong>出现了一种数字统计，旨在反映一个单词对文档的重要性。</p><p id="1cd1" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hy"> TF-IDF </strong>通过获得术语的频率(TF)并将其乘以术语逆文档频率(IDF)来获得该重要性分数。<strong class="iw hy"> TF-IDF </strong>得分越高，该术语在文档中就越罕见，其重要性就越高。</p><h2 id="478b" class="lq ka hx bd kb lr ls lt kf lu lv lw kj jf lx ly kn jj lz ma kr jn mb mc kv md bi translated">如何计算TF-IDF？</h2><p id="1085" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">术语频率是指一个术语在文档中出现的次数，例如:如果单词“stock”在一个2000单词的文档中出现20次，那么stock的TF就是20/2000 = 0.01。</p><p id="40e8" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">IDF是文档总数除以包含该术语的文档总数的对数，例如:如果有50.000个文档，并且单词“stock”出现在500个文档中，则IDF是log(50000/500) = 4.6。所以‘股票’的TF-IDF是4.6 * 0.01 = 0.046。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="c0c9" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">Word2Vec</h1><p id="edcd" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">在<strong class="iw hy"> Word2Vec </strong>中，我们使用神经网络来获得语料库(文档集)中单词的嵌入表示。<strong class="iw hy"> Word2Vec </strong>很可能很好地捕捉到单词的上下文含义。</p><p id="28c4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">要获得单词嵌入，可以使用两种方法:连续单词包(CBOW)或Skip Gram。这两种方法都将单词的一键编码表示作为输入，要获得这种表示，您只需构建一个向量，其大小为语料库中唯一单词的数量，然后每个单词将在向量的特定位置表示为1，在所有其他位置表示为0。</p><p id="ee7a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如，假设我们的语料库只有3个词:飞机、蜜蜂和猫。所以我们可以把它们表示为:</p><ul class=""><li id="fa78" class="lc ld hx iw b ix iy jb jc jf le jj lf jn lg jr ms li lj lk bi translated">飞机:[1，0，0]</li><li id="289d" class="lc ld hx iw b ix ll jb lm jf ln jj lo jn lp jr ms li lj lk bi translated">蜜蜂:[0，1，0]</li><li id="b3d3" class="lc ld hx iw b ix ll jb lm jf ln jj lo jn lp jr ms li lj lk bi translated">猫:[0，0，1]</li></ul><p id="a506" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在<strong class="iw hy"> Word2Vec </strong>中我们对模型的输出不感兴趣，但是对隐藏层的权重感兴趣。这些权重将是单词的嵌入。</p><h2 id="e616" class="lq ka hx bd kb lr ls lt kf lu lv lw kj jf lx ly kn jj lz ma kr jn mb mc kv md bi translated">CBOW</h2><p id="c2ac" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">CBOW模型的目标是接收一组热编码单词，预测一个单词学习的上下文。因此，假设我们有短语“早上我喝咖啡”，CBOW模型的输入和输出将是:</p><figure class="mf mg mh mi fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mt"><img src="../Images/5072a6a15b47527ba963c14ae97435c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g9BcJAzjBKbDBIrLkbh3bA.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">CBOW输入和预期输出示例</figcaption></figure><p id="3cf4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，这是一个监督学习模型，神经网络使用一个称为反向传播的过程来学习隐藏层的权重。</p><h2 id="ae51" class="lq ka hx bd kb lr ls lt kf lu lv lw kj jf lx ly kn jj lz ma kr jn mb mc kv md bi translated">跳跃图</h2><p id="2dc8" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">Skip-Gram类似于CBOW的反义词，这里一个目标单词作为输入被传递，模型试图预测相邻的单词。</p><figure class="mf mg mh mi fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mu"><img src="../Images/0620c0664bd61ac55850390d4a28f673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0rcIYtvk8873umH79ybzbA.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">跳格输入和预期输出的示例</figcaption></figure><figure class="mf mg mh mi fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mv"><img src="../Images/d4039aa0a332e8c933b259fbe9e9067d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bPoBFhWO7qKvlYZWdGByQQ.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">左边是CBOW架构，右边是Skip-Gram架构</figcaption></figure></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="8f0c" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">Doc2Vect</h1><p id="d278" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">在<strong class="iw hy"> Word2Vec </strong>中，我们对每个单词进行一次嵌入。<strong class="iw hy"> Word2Vect </strong>做单词之间的比较很有用，但是如果要比较文档呢？</p><p id="88bd" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">您可以使用Word2Vec对文档中的单词进行一些向量平均，以获得文档的向量表示，或者您可以使用为文档构建的技术，如<strong class="iw hy"> Doc2Vect </strong>。</p><p id="f785" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hy">Gensim中的<strong class="iw hy"> Doc2Vect </strong>是le和Mikolov的文章</strong><strong class="iw hy">中句子和文档的分布式表示的实现。在这篇<a class="ae hu" href="https://arxiv.org/pdf/1405.4053.pdf" rel="noopener ugc nofollow" target="_blank">文章</a>中，作者提出了两种算法来获得文档的嵌入。段落向量的<strong class="iw hy">分布式记忆模型</strong> (PV-DM)和段落向量的</strong>分布式单词包版本<strong class="iw hy">(PV-DBOW)</strong></p><h2 id="a833" class="lq ka hx bd kb lr ls lt kf lu lv lw kj jf lx ly kn jj lz ma kr jn mb mc kv md bi translated">段落向量的分布式记忆模型</h2><p id="80a0" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">这个模型看起来像<strong class="iw hy"> CBOW </strong>，但是现在作者为这个模型创建了一个新的输入，叫做段落id。</p><blockquote class="mw mx my"><p id="efc9" class="iu iv mq iw b ix iy iz ja jb jc jd je mz jg jh ji na jk jl jm nb jo jp jq jr ha bi translated">“段落标记可以被认为是另一个单词。它起到了记忆的作用，记住了当前上下文或者段落主题中缺少的内容。”</p></blockquote><figure class="mf mg mh mi fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es nc"><img src="../Images/8baa72379c641b83f6e9ba37dd3659de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lijgWiYs5yO44uVs"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">PV-DM架构</figcaption></figure><h2 id="42a2" class="lq ka hx bd kb lr ls lt kf lu lv lw kj jf lx ly kn jj lz ma kr jn mb mc kv md bi translated">段落向量分布式单词包版本(PV-DBOW)</h2><p id="b698" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated"><strong class="iw hy"> PV-DBOW </strong>方法类似于<strong class="iw hy"> Skip-Gram </strong>，这里的输入是一个段落id，模型试图预测从文档中随机抽样的单词。</p><figure class="mf mg mh mi fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es nd"><img src="../Images/dcba2788b977e844706c9ab78d5565cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4m3H2LucrPQXXt5F"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">PV-DBOW架构</figcaption></figure><p id="fa28" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了获得更健壮的文档表示，作者将由<strong class="iw hy"> PV-DM </strong>生成的嵌入与由<strong class="iw hy"> PV-DBOW </strong>生成的嵌入结合起来。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="b469" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">变形金刚(电影名)</h1><p id="0db4" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated"><strong class="iw hy">变形金刚</strong>技术革新了<strong class="iw hy"> NLP </strong>问题，并为<strong class="iw hy"> BERT </strong>和<strong class="iw hy"> RoBERTa </strong>型号设定了最先进的性能。因此在<a class="ae hu" href="https://arxiv.org/pdf/1908.10084.pdf" rel="noopener ugc nofollow" target="_blank">Sentence-BERT:Sentence embedding using Siamese BERT-Networks</a>中，Reimers和Gurevych修改了<strong class="iw hy"> BERT </strong>和<strong class="iw hy"> RoBERTa </strong>的模型架构，使它们输出固定大小的句子嵌入。</p><p id="3d0d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了实现这一点，他们向转换器的输出添加了一个池操作，尝试了一些策略，如计算所有输出向量的平均值和计算输出向量的最大时间。</p><figure class="mf mg mh mi fd hj er es paragraph-image"><div class="er es ne"><img src="../Images/6acf6fee1807d8a0eb0ad52522741734.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*PDkwmfuh03IeJAnwUa6e9Q.png"/></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">句子嵌入BERT得到余弦相似度</figcaption></figure><p id="6b85" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在python中使用预先训练好的转换器很容易，你只需要使用来自<a class="ae hu" href="https://www.sbert.net/" rel="noopener ugc nofollow" target="_blank"> SBERT </a>的sentece _ transformes包。在SBERT中，还可以使用在不同数据中训练的多种架构。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="fe3b" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">比较技术</h1><p id="f35e" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">使用的机器是MacBook Pro，配有2.6 GHz双核英特尔酷睿i5和8 GB 1600 MHz DDR3内存。所用的数据来自沃伦·巴菲特每年写给伯克希尔·哈撒韦公司股东的信，巴菲特是这家公司的首席执行官。目标是得到与2008年字母相近的字母。</p><figure class="mf mg mh mi fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es nf"><img src="../Images/dd4784b0cc5d14c81edbb94f1df34d87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K1DpjBmfSgUxQGNCbkTKhA.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">包含方法、相似性度量、目标年份、执行时间(秒)和最相似年份(按最相似到最不相似排序)的表</figcaption></figure><p id="c737" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">TF-IDF是最慢的方法，需要295秒来运行，因为它的计算复杂度是O(nL log nL)，其中n是语料库中句子的数量，L是数据集中句子的平均长度。</p><p id="904c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一个奇怪的方面是，所有的技术在最相似的年份给出了不同的结果。因为数据是未标记的，我们不能肯定什么是最好的方法。在接下来的分析中，我将使用一个带标签的数据集来获得答案，敬请关注。</p><h1 id="9731" class="jz ka hx bd kb kc mk ke kf kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw bi translated">结束</h1><p id="e6c3" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">我实现了上面所有的技术，你可以在这个<a class="ae hu" href="https://github.com/jairNeto/warren_buffet_letters" rel="noopener ugc nofollow" target="_blank"> GitHub </a>仓库中找到代码。在那里，您可以选择将文档转换为嵌入的算法，并且可以在余弦相似度和欧几里德距离之间进行选择。</p><p id="2536" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我使用的一套文本是沃伦·巴菲特每年从伯克希尔·哈撒韦公司写给股东的信，他是这家公司的首席执行官。</p><p id="6402" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在本文中，我们探讨了NLP文档相似性任务。展示了将文本转换为嵌入内容的4种算法:TF-IDF、Word2Vec、Doc2Vect和Transformers，以及获取相似度的两种方法:余弦相似度和欧氏距离。</p><p id="0780" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你对我的<a class="ae hu" href="https://www.linkedin.com/in/jair-guedes-ferreira-neto/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>账户有任何意见，请随时联系我，感谢你阅读这篇文章。</p><p id="6b3d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你喜欢你所读的东西，一定要👏下面，分享给你的朋友，关注我，不要错过这一系列的帖子。</p></div></div>    
</body>
</html>