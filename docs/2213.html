<html>
<head>
<title>Decision Tree 101!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树101！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-tree-101-e94a5d131fa0?source=collection_archive---------13-----------------------#2021-04-12">https://medium.com/analytics-vidhya/decision-tree-101-e94a5d131fa0?source=collection_archive---------13-----------------------#2021-04-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/8d283cc82a833b960a4bb755f87a7a13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*kHz4cOpckFUTCeT4LL62Xg.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx translated">决策图表</figcaption></figure><p id="8246" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">决策树是机器学习领域中基于树的算法之一。它非常直观，易于理解，这使得它在解决一些经典的机器学习问题时非常有用。</p><p id="601f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，我们不会浪费太多时间，将深入探讨这种算法的本质细节。</p><p id="6ec6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">什么是决策树？</strong></p><ul class=""><li id="cb68" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">决策树可以用来处理分类数据和数字数据。</li></ul><figure class="jx jy jz ka fd ii er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jw"><img src="../Images/4901616a6c84e39605eba9c85649060d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J4WiwZIFlvfgnlaJDxJPTA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">图片来源:youtube上的Statquest频道</figcaption></figure><ul class=""><li id="d332" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">现在让我们看看上面的数据集，我们有一些特征，即胸痛，良好的血液循环，阻塞的动脉，以及一个标签，即心脏病。</li><li id="0d44" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">现在要创建一个决策树，我们首先要创建一个根节点，为了创建一个根节点，我们需要选择一个以最佳方式划分数据集的特征。</li><li id="1652" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">但是我们如何选择这个特性呢？有许多指标可以计算这一点，如熵、信息增益和基尼系数。</li><li id="a53e" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">就本文而言，我们将主要关注基尼系数。</li><li id="3ad9" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">基尼不纯度是用于计算节点不纯度的度量，由以下公式给出</li></ul><figure class="jx jy jz ka fd ii er es paragraph-image"><div class="er es kk"><img src="../Images/f313d3a596ff715a1d3098a86db8d0d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*Tul3Ohz24bBE2O2lwH1Tww.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">基尼杂质公式</figcaption></figure><figure class="jx jy jz ka fd ii er es paragraph-image"><div class="er es kl"><img src="../Images/cb5fd434afe6ef702c1352245c6adc46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*-Tl_XrAJhYP-fboUd8NHuQ.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">计算基尼系数</figcaption></figure><ul class=""><li id="10db" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">假设我们以胸痛为节点，通过查看上面的图表，我们可以看到，如果一个人有胸痛，那么其中105人会有心脏病，39人不会有心脏病。类似地，如果患者没有心脏病，那么这些患者中有34人会有心脏病，125人不会有心脏病。</li><li id="98ea" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">现在让我们计算病人胸痛时某个节点的基尼系数。</li></ul><figure class="jx jy jz ka fd ii er es paragraph-image"><div class="er es km"><img src="../Images/d3e045e98ed549d01334240411407432.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/1*aYHOlOl0qXqFKI2nJVVREA.gif"/></div><figcaption class="il im et er es in io bd b be z dx translated">节点1的Gini杂质</figcaption></figure><ul class=""><li id="c5e6" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">上述等式给出的值为0.394965。</li><li id="272f" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">现在我们计算病人没有胸痛时节点2的基尼系数。</li></ul><figure class="jx jy jz ka fd ii er es paragraph-image"><div class="er es km"><img src="../Images/826e4c7f8889e52380029910959a645e.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/1*-jDluntsKe5ZD0MEkFIo5A.gif"/></div><figcaption class="il im et er es in io bd b be z dx translated">节点2的Gini杂质</figcaption></figure><ul class=""><li id="4c61" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">上述等式给出的输出为0.33622</li><li id="a753" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">现在，为了计算胸痛的基尼指数，我们取两个节点的加权和，如下式所示。</li></ul><figure class="jx jy jz ka fd ii er es paragraph-image"><div class="er es kn"><img src="../Images/4872dd7ea3a6cacc39bf677f6224b2ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/1*Zw3jblHrJZKHVK1A_STr2w.gif"/></div><figcaption class="il im et er es in io bd b be z dx translated">基尼系数的加权和</figcaption></figure><ul class=""><li id="467e" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">现在，将这些值代入上述公式，得到以下等式</li></ul><figure class="jx jy jz ka fd ii er es paragraph-image"><div class="er es ko"><img src="../Images/26637c60b48ec63e066e05aed998af36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/1*D24-hj97rFCcagaSfEkKRw.gif"/></div></figure><ul class=""><li id="1c2f" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">上面的等式给出了下面的0.36413</li><li id="ee9a" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">同样，我们将计算所有要素的基尼系数，基尼系数最小的要素将成为我们的根节点。</li><li id="40aa" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">现在，我们已经将数据集分为两部分。我们将对每个节点应用相同的过程，直到我们得到纯叶节点或者父节点的Gini杂质小于子节点。</li></ul><p id="95c6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">我们的圣诞树终于准备好了！！</strong></p><ul class=""><li id="3834" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">现在，由于我们的树已经完全构建好了，我们可以通过测试数据来检查我们的预测。</li></ul><p id="25e1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">如何计算不同类型数据的基尼系数？</strong></p><ul class=""><li id="5076" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">我们已经在上面看到了如何计算是/否类型数据的基尼系数。</li></ul><h2 id="b7aa" class="kp kq hh bd kr ks kt ku kv kw kx ky kz ja la lb lc je ld le lf ji lg lh li lj bi translated">处理数字数据</h2><ul class=""><li id="6745" class="jn jo hh ir b is lk iw ll ja lm je ln ji lo jm js jt ju jv bi translated">对于连续数据，假设喜欢用身高来预测体重。我们需要问一个问题，比如将使用什么权重值来将数据分成节点。</li><li id="ad3a" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">首先，我们将重量数据按升序排序。然后我们取对应点对的平均值，然后用这个平均值计算每个平均值的基尼系数。</li><li id="99de" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">具有最小基尼系数杂质的平均权重将被用于该特征的基尼系数。</li></ul><p id="bdd8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">处理排名数据</strong></p><ul class=""><li id="3939" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">假设我们有一个包含等级或级别的特性，那么我们能做的如下。</li><li id="d916" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">将数据分成两部分，即一个节点具有秩≤ p的数据，另一个节点具有秩&gt; p的数据(p是任何有效的秩值)。</li><li id="e261" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">现在我们计算每个等级的基尼系数，并计算基尼系数最小的等级。</li></ul><p id="4d0a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">加贺</strong></p><ul class=""><li id="982e" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">决策树对非线性数据表现良好。</li><li id="36f3" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">它们很容易过拟合，即它们在训练数据上表现良好，但在测试数据上表现较差。</li><li id="efdc" class="jn jo hh ir b is kf iw kg ja kh je ki ji kj jm js jt ju jv bi translated">决策树是其他基于树的算法的基础，如随机森林等。</li></ul><p id="78fb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">直到那时快乐学习！！</strong></p></div></div>    
</body>
</html>