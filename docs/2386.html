<html>
<head>
<title>Solve Machine Learning Problems: Dimensionality Reduction (part-5)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解决机器学习问题:降维(第五部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/solve-machine-learning-problem-dimensionality-reduction-part-5-cf7676b20164?source=collection_archive---------27-----------------------#2021-04-20">https://medium.com/analytics-vidhya/solve-machine-learning-problem-dimensionality-reduction-part-5-cf7676b20164?source=collection_archive---------27-----------------------#2021-04-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/70b680b1325f6329738ba2cb6b1e8bda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1dEP_hUGEfVvo6i_r2tZHA.png"/></div></div></figure><h1 id="6eb4" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">介绍</h1><p id="17ef" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">在有大量变量的情况下，很难对数据集进行可视化和推断。因此，这些技术试图从该数据集中提取一个子集，它可以捕获由原始变量集显示的正常数量的信息。因此，如果我们有一个X维的数据集，我们可以将其转换为Y维的子集。这就是所谓的降维。</p><p id="56d4" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">降维可以是线性的，也可以是非线性的，这取决于所用的方法。素线性方法，称为主成分分析</p><h2 id="96db" class="kq iq hh bd ir kr ks kt iv ku kv kw iz jy kx ky jd kc kz la jh kg lb lc jl ld bi translated"><strong class="ak">主成分分析</strong></h2><p id="30f9" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">它的工作条件是，当高维空间中的数据被映射到低维空间中的数据时，低维空间中数据的方差应该是最大的。构建数据的协方差矩阵。计算这个矩阵的特征向量。第一个主成分考虑了原始数据的可能变化。之后每个随后的分量具有最高的可能方差。</p><p id="fe3a" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">第二主分量必须与第一主分量正交。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es le"><img src="../Images/cca59dda90d611d6ba10163a425bd2b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*WL75wGvRw7MmQvhIVUcdFg.png"/></div></figure><p id="2844" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在上面的例子中，我们使用PCA并选择3个主成分，并给出方差。</p><h2 id="af9b" class="kq iq hh bd ir kr ks kt iv ku kv kw iz jy kx ky jd kc kz la jh kg lb lc jl ld bi translated">t分布随机邻居嵌入</h2><p id="1004" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">t-SNE用于将高维数据分解成2或3维数据，并且还可视化高维数据。算法计算高维空间和低维空间中的实例对之间的相似性度量。它试图保持从高维空间到低维空间的相似性。</p><p id="4811" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">相似性度量在高维空间通过高斯分布计算，在低维空间通过单自由度的t分布计算。<br/> Kullback-Liebler散度(KL)是一种计算两个概率分布之间距离的度量。我们可以使用梯度下降来最小化KL成本函数。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lj"><img src="../Images/2662943b6cb91b3f778a664846ea0a83.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*JYGH8mtMgVtn1EZljXgulg.png"/></div></figure><p id="057d" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">上图我们可以看到t-sne后X的形状。</p><h2 id="13df" class="kq iq hh bd ir kr ks kt iv ku kv kw iz jy kx ky jd kc kz la jh kg lb lc jl ld bi translated"><strong class="ak">独立成分分析</strong></h2><p id="95b3" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">独立分量分析(ICA)基于信息论，也是应用最广泛的降维技术之一。PCA和ICA的主要区别在于PCA寻找不相关的因素，而ICA寻找独立的因素。如果两个变量不相关，这意味着它们之间没有线性关系。如果它们是独立的，就意味着它们不依赖于其他变量。</p><p id="49c6" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">该算法假设给定变量是一些未知潜变量的线性混合物。它还假设这些潜在变量是相互独立的，即它们不依赖于其他变量，因此它们被称为观察数据的独立成分。</p><ol class=""><li id="2294" class="lk ll hh jp b jq kl ju km jy lm kc ln kg lo kk lp lq lr ls bi translated">互信息最小化</li><li id="57f1" class="lk ll hh jp b jq lt ju lu jy lv kc lw kg lx kk lp lq lr ls bi translated">非高斯性最大化</li></ol><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es ly"><img src="../Images/3747febafba70f04d6d7e2e75edf67bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*xxjvPoHeuks1EaXYFHqCvw.png"/></div></figure><p id="ca2b" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">运行ICA后的输出显示在上面的图片中。</p><h2 id="e89d" class="kq iq hh bd ir kr ks kt iv ku kv kw iz jy kx ky jd kc kz la jh kg lb lc jl ld bi translated">线性判别分析</h2><p id="7ee9" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">LDA背后的一般概念非常类似于PCA，而PCA试图找到数据集中方差最大的正交分量轴，LDA的目标是找到优化类别可分性的特征子空间，并且为了服务于这个目的，它需要类别标签。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lz"><img src="../Images/751b12438028cfb58b97766c505e0997.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*gL2tRfHDvUYGO2Rw0VnooA.png"/></div></figure><p id="cf74" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">运行LDA后的输出显示在上面的图片中。</p><h2 id="c6bc" class="kq iq hh bd ir kr ks kt iv ku kv kw iz jy kx ky jd kc kz la jh kg lb lc jl ld bi translated">结论</h2><p id="bc1e" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">我们已经讨论了降维技术。大量的输入特征会导致机器学习算法的性能不佳。降维是与减少输入特征数量有关的一般研究领域。降维方法包括特征选择、线性代数方法。我希望这篇博客对你有所帮助。谢谢你的时间。</p></div></div>    
</body>
</html>