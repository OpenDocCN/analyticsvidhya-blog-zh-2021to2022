# 迁移学习是你需要学习的最重要的工具

> 原文：<https://medium.com/analytics-vidhya/transfer-learning-is-the-most-important-tool-you-need-to-learn-6b9f35b5bb8?source=collection_archive---------11----------------------->

*原载于 2021 年 6 月 25 日*[*【https://dramsch.net】*](https://dramsch.net/posts/transfer-learning-is-the-most-important-tool-you-need-to-learn/)*。*

当你学习 fast.ai 课程时，你几乎马上就能了解到迁移学习。这没有错！迁移学习让你快速开始你的深度学习之旅，在最短的时间内建立你的第一个基于神经网络的分类器。

![](img/880b73946ffcc1dd69d1d32ecdc2b8cd.png)

但是迁移学习是如此之多，它对科学中的机器学习有着惊人的重要性。

# 什么是迁移学习？

从技术角度来看，迁移学习意味着采用一个经过训练的机器学习模型，并将其用于一个新的、不同的、但相关的任务。比方说，你拿一个在 ImageNet 上训练过的分类器，比如 ResNet。现在在 ImageNet 上下载训练过的重量的 ResNet 很容易！通常，这种训练需要几天到几周的时间，而且要花费大量的研究经费。

然而，我们的数据不是 ImageNet。假设您想从一个猫/狗分类器开始。然后，迁移学习使您能够利用复杂 ImageNet 数据上的现有权重，这些数据在网络权重中包含更多类别和更多复杂性。通常，机器学习实践者会以较低的学习速率重新训练网络，称为微调，以保持所述网络的复杂性，但对他们的猫/狗分类器的权重进行非常轻微的调整。

# 为什么是为了科学？

科学并不真正关心猫/狗分类器。我们要扫描大脑，寻找黑洞，探索地核！

![](img/93ab3917f0ef15059456702142ffb66b.png)

*鸣谢:视界望远镜合作组织等*

这就是迁移学习的大兄弟的用武之地:**领域适应**。领域自适应假设，如果 **A** 和 **B** 足够相似，那么在任务 **A** 上受过训练的代理可以用最少的重新训练来完成任务 **B** 。听起来熟悉吗？没错，就是迁移学习的一般版本。潜在的想法是代理学习任务 **A** 的分布，就像神经网络学习的表示，即 ImageNet。然后，代理或我们的网络可以适应工作在一个任务上，其中的分布是足够相似的，学习到的分布可以“转移”到我们的目标任务。

那是什么意思？

这对我的科学有什么帮助？

# 直接使用预先训练好的网络

ImageNet 权重很容易获得，并在分类任务中捕捉到许多复杂性。当然，还有更大的数据集，但是这里的可用性至关重要。如果你对其他任务更感兴趣，比如对象识别，PASCAL-VOC 或 COCO 是不错的选择。

然后你可以使用这些预先训练好的网络，看看你是否能根据你的数据对它们进行微调。事实上，在 Kaggle 比赛中使用了预先训练的 ImageNet ResNets 来分割地震数据中的盐！是的，这是我 2018 年的[会议论文](https://dramsch.net/files/SEG_expanded_abstract_2018___Deep_learning_seismic_facies_on_state_of_the_art_CNN_architectures.pdf)，是[这部作品](https://www.kaggle.com/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics)的直接前身。

请注意，有些网络比其他网络概括得更好。VGG16 以其通用卷积滤波器而闻名，而 ResNet 可能会很棘手，这取决于您有多少数据可用。

请记住，在更大的架构中，您总是可以使用预先训练好的网络！在上面的比赛中，人们将预训练的 ResNet 包含在 U-Net 中，以执行语义分割，而不是经典的单一图像分类！

# 训练一次，微调一次

在科学领域，尤其是在新兴应用领域，科学家将发表一篇概念证明论文。让一个网络在一个特定的用例上工作是通向一个通用方法的一个有价值的入口。

伟大的科学家将公布他们训练过的模型和代码，以重现这些结果。通常，你可以使用这个训练好的网络，并在一个类似问题集的新站点上对网络进行微调。我今天参加的[自然灾害管理人工智能焦点小组会议](https://www.itu.int/en/ITU-T/focusgroups/ai4ndm/Pages/default.aspx)的一个例子是在一个容易发生雪崩的地点训练一个网络，在那里有大量数据可用，然后将模型转移到第一个地点捕获的新的复杂性。

这不是大多数机器学习科学家努力追求的概括。然而，对于特定领域的应用科学家来说，它可能是一个有价值的用例。这也是一个很好的概念验证，可以获得更多数据收集的授权。

# 训练大的那个

圣杯。统治他们所有人的网络。GPT 3 号来到我的科学小角落。

当你幸运地拥有大量不同的数据集时，你可以在所有数据集上训练一个模型。这个模型理想地捕捉了数据的一般抽象。然后，您可以根据需要对特定区域进行微调。

例如，这是在主动地震中完成的。许多数据经纪人和服务公司创建大规模模型，然后可以根据客户手头的特定细分任务进行微调。

在消融和可解释性研究中检查这些模型特别令人兴奋，以便更深入地了解特定领域以及神经网络如何解释它。

# 不要相信我的话！

你可以从 fast.ai 课程开始，学习迁移学习，或者阅读一些关于代码为的[神奇论文的研究论文。](https://paperswithcode.com/task/domain-adaptation/latest)

或者你可能想看 [CVPR 2021 研讨会](http://cvpr2021.thecvf.com/)，这在一定程度上激发了我写这篇关于迁移学习应用的短文:[不完美世界中的数据和标签高效学习](https://vita-group.github.io/cvpr_2021_data_efficient_tutorial.html)。