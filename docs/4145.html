<html>
<head>
<title>Akira’s Machine Learning news — #26</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Akira的机器学习新闻— #26</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/akiras-machine-learning-news-26-2d0c888572f3?source=collection_archive---------10-----------------------#2021-08-31">https://medium.com/analytics-vidhya/akiras-machine-learning-news-26-2d0c888572f3?source=collection_archive---------10-----------------------#2021-08-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><h2 id="41df" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">本周特稿/新闻。</h2><ul class=""><li id="ee71" class="jj jk hh jl b jm jn jo jp iw jq ja jr je js jt ju jv jw jx bi translated"><a class="ae jy" href="https://arxiv.org/abs/2106.02636" rel="noopener ugc nofollow" target="_blank">提出了对大量视频数据的无监督学习的研究</a>，使用Transformer在时间和空间方向上训练设计良好的任务。在处理600万条数据的同时，随着数据的增长精度的提高还没有达到上限，未来还有进一步提高的潜力。</li><li id="1fa0" class="jj jk hh jl b jm jz jo ka iw kb ja kc je kd jt ju jv jw jx bi translated">知识提炼传统上被描述为“通过学习教师模型的输出分布来使学生模型更聪明”，但是<a class="ae jy" href="https://arxiv.org/abs/2106.05945" rel="noopener ugc nofollow" target="_blank">的一项研究</a>表明，教师模型的分布和学生模型的分布之间的一致程度与学生模型的成绩准确性之间的相关性很差。有一项研究表明，即使教师模型很差，学习也是可能的，正如那里所声称的，提炼可能只是正则项的函数。</li></ul><p id="ff76" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi">— — — — — — — — — — — — — — — — — — –</p><p id="928c" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated">在下面的章节中，我将介绍各种文章和论文，不仅仅是关于上述内容，还包括以下五个主题。</p><ol class=""><li id="d540" class="jj jk hh jl b jm kg jo kj iw kv ja kw je kx jt ky jv jw jx bi translated">本周特稿/新闻</li><li id="a550" class="jj jk hh jl b jm jz jo ka iw kb ja kc je kd jt ky jv jw jx bi translated">机器学习用例</li><li id="8228" class="jj jk hh jl b jm jz jo ka iw kb ja kc je kd jt ky jv jw jx bi translated">报纸</li><li id="2aa5" class="jj jk hh jl b jm jz jo ka iw kb ja kc je kd jt ky jv jw jx bi translated">机器学习技术相关文章</li></ol><p id="edea" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="1851" class="kz im hh bd in la lb lc ir ld le lf iv lg lh li iz lj lk ll jd lm ln lo jh lp bi translated">1.本周特稿/新闻</h1><p id="ebbf" class="pw-post-body-paragraph ke kf hh jl b jm jn kh ki jo jp kk kl iw lq kn ko ja lr kq kr je ls kt ku jt ha bi translated"><a class="ae jy" href="https://arxiv.org/abs/2106.02636?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hi">用大量视频和语言进行自我监督学习</strong></a><strong class="jl hi">——</strong><a class="ae jy" href="https://arxiv.org/abs/2106.02636" rel="noopener ugc nofollow" target="_blank"><strong class="jl hi">arxiv.org</strong></a></p><figure class="lt lu lv lw fd lx er es paragraph-image"><div class="ab fe cl ly"><img src="../Images/ac6afa0159184b2913fe8b9189c1a6c8.png" data-original-src="https://miro.medium.com/v2/0*sDWm3z-X_Nzpevi0"/></div></figure><p id="82bf" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated">[2106.02636] MERLOT:多模态神经脚本知识模型<br/>使用多达600万个视频数据和伴随的字幕，MERIOT被提议对时间和空间任务执行自我监督学习。它不使用任何标签信息，但可以实现SotA性能。此外，即使有600万个数据，预训练的准确性也在继续增加，这被认为是未来有前途的研究方向。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="56ca" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated"><a class="ae jy" href="https://arxiv.org/abs/2106.05945?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hi">知识升华不符合分布</strong></a><strong class="jl hi">——</strong><a class="ae jy" href="https://arxiv.org/abs/2106.05945" rel="noopener ugc nofollow" target="_blank"><strong class="jl hi">arxiv.org</strong></a></p><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mb"><img src="../Images/eec7d4fb1260362dfb2a7d9943f8d52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dUZZdZya_WB2ZjXu7CnjrA.png"/></div></div></figure><p id="0a35" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated">知识蒸馏真的有用吗？<br/>他们声称知识的升华使学生模型学习到了教师模型的分布，但是这种一致性越高，并不意味着学生模型就越高精确。他们的结论是“知识提炼在提高学生模型的准确性方面起作用，但在匹配学生和教师模型的分布方面不起作用。”</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="b822" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="e6da" class="kz im hh bd in la lb lc ir ld le lf iv lg lh li iz lj lk ll jd lm ln lo jh lp bi translated">2.机器学习用例</h1><p id="41c2" class="pw-post-body-paragraph ke kf hh jl b jm jn kh ki jo jp kk kl iw lq kn ko ja lr kq kr je ls kt ku jt ha bi translated"><a class="ae jy" href="https://thegradient.pub/justitia-ex-machina/?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hi">机器学习中的公平与透明</strong></a><strong class="jl hi">—</strong><a class="ae jy" href="https://thegradient.pub/justitia-ex-machina/" rel="noopener ugc nofollow" target="_blank"><strong class="jl hi">the gradient . pub</strong></a></p><div class="mg mh ez fb mi mj"><a href="https://thegradient.pub/justitia-ex-machina/" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">贾斯汀·玛奇纳:道德自动化的案例</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">机器学习是一种强大的技术，可以从最近成为驱动因素的数据中自动学习模型…</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">thegradient.pub</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx lz mj"/></div></div></a></div><p id="6643" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated">这篇文章认为，机器学习也可以学习种族和犯罪率等歧视性因素(当相关时，包括历史上的)，并且还有透明度挑战来防止这种情况发生。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="bffc" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated"><a class="ae jy" href="https://blogs.nvidia.com/blog/2021/08/06/plainsight-cattle-management-ai/?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hi">牛的识别与疾病检测</strong></a><strong class="jl hi">——</strong><a class="ae jy" href="https://blogs.nvidia.com/blog/2021/08/06/plainsight-cattle-management-ai/" rel="noopener ugc nofollow" target="_blank"><strong class="jl hi">blogs.nvidia.com</strong></a></p><div class="mg mh ez fb mi mj"><a href="https://blogs.nvidia.com/blog/2021/08/06/plainsight-cattle-management-ai/" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">Plainsight通过人工智能增强牛群管理NVIDIA官方博客</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">计算机视觉和边缘人工智能正在超越牧场。总部位于旧金山的初创公司Plainsight和NVIDIA…</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">blogs.nvidia.com</p></div></div><div class="ms l"><div class="my l mu mv mw ms mx lz mj"/></div></div></a></div><p id="1b58" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated">牛的识别非常困难，并且由于错误识别造成的经济损失很高。Plainsight不仅可以高精度地识别牛，还可以根据牛的异常行为创建检测疾病的模型。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="f7ce" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="789f" class="kz im hh bd in la lb lc ir ld le lf iv lg lh li iz lj lk ll jd lm ln lo jh lp bi translated">3.报纸</h1><p id="1cfa" class="pw-post-body-paragraph ke kf hh jl b jm jn kh ki jo jp kk kl iw lq kn ko ja lr kq kr je ls kt ku jt ha bi translated">【arxiv.org】<strong class="jl hi">处理自然语言查询中的可变性</strong><strong class="jl hi">——</strong><a class="ae jy" href="https://arxiv.org/abs/2103.16848" rel="noopener ugc nofollow" target="_blank"><strong class="jl hi"/></a></p><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mz"><img src="../Images/454e4c2cee8aeb7168c8e54e1a3cc300.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UO_TGlGgHQBFQeC_BupJpw.png"/></div></div></figure><p id="6940" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated">[2103.16848]拥抱不确定性:去耦和去偏置实现稳健的时间基础<br/>他们提出了DeNet(去耦和去偏置)来处理时间基础任务中的查询可变性和注释可变性，该任务使用自然语言查询从视频中提取动作。他们证实了DeNet对于chariales-STA和ActivityNet字幕的有效性和稳健性。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="362a" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated"><a class="ae jy" href="https://arxiv.org/abs/2103.14211?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hi">突破对抗性攻击，防止深度假冒</strong></a><strong class="jl hi">—</strong><a class="ae jy" href="https://arxiv.org/abs/2103.14211" rel="noopener ugc nofollow" target="_blank"><strong class="jl hi"/></a></p><figure class="lt lu lv lw fd lx er es paragraph-image"><div class="ab fe cl ly"><img src="../Images/23953336396bd59560afd7fefbac8922.png" data-original-src="https://miro.medium.com/v2/0*G6J1EmevSz9hAgpY"/></div></figure><p id="2848" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated">【2103.14211】MagDR:Mask-guided Detection and re construction for defense Deep fakes<br/>作为一种针对深度假像的对策，有一种方法是通过在图像中添加对抗性噪声来防止深度假像的产生。相比之下，他们使用一个遮罩来确定图像中是否有噪声，并重建图像以使深度假工作。他们已经证明他们的方法可以处理黑盒和白盒攻击。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="b823" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated"><a class="ae jy" href="https://arxiv.org/abs/2106.04560?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hi">比例尺为ViT</strong></a><strong class="jl hi">——</strong><a class="ae jy" href="https://arxiv.org/abs/2106.04560" rel="noopener ugc nofollow" target="_blank"><strong class="jl hi">arxiv.org</strong></a></p><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es na"><img src="../Images/925ac63082b9ff67d1ee7e5da3854a1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8txECoWxL0HI1Djgphonkg.png"/></div></div></figure><p id="748a" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated">[2106.04560]缩放视觉转换器<br/>使用不同数量的数据和模型尺寸对ViT进行研究，以检查缩放定律。数据量越多，模型规模越大，微调后精度越好，数据量越少，精度越低。他们还改进了训练方法等。，创造了一个巨型ViT，在ImageNet上取得了90.45% (top-1)。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="4578" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated"><a class="ae jy" href="https://arxiv.org/abs/2106.10270?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"><strong class="jl hi">ViT中的数据扩充和正则化</strong></a><strong class="jl hi">——</strong><a class="ae jy" href="https://arxiv.org/abs/2106.10270" rel="noopener ugc nofollow" target="_blank"><strong class="jl hi">arxiv.org</strong></a></p><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es nb"><img src="../Images/b04c753d808c63d6ac4200b7191297cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iEbV0HX25_w69vNEJ7vkjA.png"/></div></div></figure><p id="c496" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated">【2106.10270】如何训练自己的ViT？视觉转换器中的数据、扩充和规则化<br/>一项调查数据扩充和规则化对视觉转换的影响的研究。当模型大小和计算资源较大时，强正则化和数据扩充更有效，并且数据扩充和正则化与将数据增加10倍一样好或更好。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="378f" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated"><a class="ae jy" href="https://arxiv.org/abs/2105.04906?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hi">用简单的正则化项</strong></a><strong class="jl hi">——</strong><a class="ae jy" href="https://arxiv.org/abs/2105.04906" rel="noopener ugc nofollow" target="_blank"><strong class="jl hi">arxiv.org</strong></a>防止自监督学习的崩溃</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es nc"><img src="../Images/84a4b75f9c66998fe992fa2a4660e985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J-psx0odVe-RQrp8c2dmMg.png"/></div></div></figure><p id="68de" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated">[2105.04906] VICReg:用于自监督学习的方差-不变性-协方差正则化<br/>提出了VICReg，其在自监督学习中使用正则化来使具有不同变换的相同图像的表示更加接近，保持批次之间的数据表示的方差，并且不跨维度学习相同的表示。它不需要像对比学习那样的大批量，并且防止崩溃。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="22b8" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="0721" class="kz im hh bd in la lb lc ir ld le lf iv lg lh li iz lj lk ll jd lm ln lo jh lp bi translated">4.机器学习技术相关文章</h1><p id="e314" class="pw-post-body-paragraph ke kf hh jl b jm jn kh ki jo jp kk kl iw lq kn ko ja lr kq kr je ls kt ku jt ha bi translated"><a class="ae jy" href="https://thegradientpub.substack.com/p/gradient-update-5-ai-generated-art?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"><strong class="jl hi">【VQ-甘+夹】简述。alpha fold 2</strong></a><strong class="jl hi">——</strong><a class="ae jy" href="https://thegradientpub.substack.com/p/gradient-update-5-ai-generated-art" rel="noopener ugc nofollow" target="_blank"><strong class="jl hi">thegradientpub.substack.com</strong></a></p><div class="mg mh ez fb mi mj"><a href="https://thegradientpub.substack.com/p/gradient-update-5-ai-generated-art" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">渐变更新#5:人工智能生成的艺术和AlphaFold</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">本版的新闻故事是人工智能生成的艺术场景爆炸，因为黑客创造了突破性的新工具摘要。一个…</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">thegradientpub.substack.com</p></div></div><div class="ms l"><div class="nd l mu mv mw ms mx lz mj"/></div></div></a></div><p id="253d" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated">一篇简要解释它们为什么重要的文章，包括专家的意见。还有很多参考链接。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="d177" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated"><a class="ae jy" href="https://www.kdnuggets.com/2021/07/high-performance-deep-learning-part3.html?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hi">高效深度学习模型的技术</strong></a><strong class="jl hi">——</strong><a class="ae jy" href="https://www.kdnuggets.com/2021/07/high-performance-deep-learning-part3.html" rel="noopener ugc nofollow" target="_blank"><strong class="jl hi">www.kdnuggets.com</strong></a></p><div class="mg mh ez fb mi mj"><a href="https://www.kdnuggets.com/2021/07/high-performance-deep-learning-part3.html" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">高性能深度学习:如何训练更小、更快、更好的模型-第3部分…</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">现在，您已经准备好使用正确的软件和硬件工具高效地构建高级深度学习模型…</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">www.kdnuggets.com</p></div></div><div class="ms l"><div class="ne l mu mv mw ms mx lz mj"/></div></div></a></div><p id="da3f" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated">一篇讨论快速学习好模型的技巧的文章。介绍了量化、剪枝、自监督学习及其技术。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="49a9" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="87b8" class="kz im hh bd in la lb lc ir ld le lf iv lg lh li iz lj lk ll jd lm ln lo jh lp bi translated">🌟我每周发布时事通讯！请订阅！🌟</h1><div class="mg mh ez fb mi mj"><a href="https://www.getrevue.co/profile/akiratosei" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">阿基拉的机器学习新闻- Revue</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">由Akira的机器学习新闻-由Akihiro FUJII:制造工程师/机器学习工程师/硕士…</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">www.getrevue.co</p></div></div><div class="ms l"><div class="nf l mu mv mw ms mx lz mj"/></div></div></a></div><p id="9d31" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="a5fa" class="kz im hh bd in la lb lc ir ld le lf iv lg lh li iz lj lk ll jd lm ln lo jh lp bi translated">关于我</h1><p id="859f" class="pw-post-body-paragraph ke kf hh jl b jm jn kh ki jo jp kk kl iw lq kn ko ja lr kq kr je ls kt ku jt ha bi translated">制造工程师/机器学习工程师/数据科学家/物理学硕士/<a class="ae jy" href="https://t.co/hjHHbG24Ph?amp=1" rel="noopener ugc nofollow" target="_blank">http://github.com/AkiraTOSEI/</a></p><p id="b251" class="pw-post-body-paragraph ke kf hh jl b jm kg kh ki jo kj kk kl iw km kn ko ja kp kq kr je ks kt ku jt ha bi translated">推特，我贴一句纸评论。</p></div></div>    
</body>
</html>