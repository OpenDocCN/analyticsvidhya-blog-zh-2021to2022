<html>
<head>
<title>Understand TF-IDF by building it from scratch.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过从头开始构建来了解TF-IDF。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understand-tf-idf-by-building-it-from-scratch-adc11eba7142?source=collection_archive---------8-----------------------#2021-03-06">https://medium.com/analytics-vidhya/understand-tf-idf-by-building-it-from-scratch-adc11eba7142?source=collection_archive---------8-----------------------#2021-03-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/75212153b5e65478de0148f85efa4a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xFDsYkhnIU_s423w"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">布雷特·乔丹在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="8c0b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">人们在机器学习领域学习的一个基本和第一个算法是TF-IDF矢量化。TF-IDF代表术语频率和逆文档频率。</p><p id="02e3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这里，我们将尝试了解TF-IDF到底是如何工作的，并将我们的结果与sklearn库进行比较。与其他算法相比，我们不会详细讨论它的用处。</p><h1 id="e4e0" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">什么是TF-IDF？</h1><p id="5d30" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">TF-IDF是一个统计工具，它测量一个单词与一堆文档中的一个文档的相关程度。</p><p id="b61b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">它由两部分组成:<br/> 1 .<strong class="iw hi">词频(TF) </strong>:一个词在文档中出现的次数<br/> 2。<strong class="iw hi">逆文档频率(IDF) </strong>:单词在一组文档中的逆文档频率。</p><p id="6ccf" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们在这里举一个例子，我们将在整个博客。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="3d72" class="le jt hh la b fi lf lg l lh li">import pandas as pd<br/>corpus = [('Document 1', 'Alot of people like to play football'),<br/>          ('Document 2', 'many like to eat'),<br/>          ('Document 3', 'According to data, many like to sing')]</span><span id="0ca6" class="le jt hh la b fi lj lg l lh li">data = pd.DataFrame(corpus,columns=['Document Number','text of Documents'])</span></pre><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">TF-IDF的示例数据</figcaption></figure><h2 id="df07" class="le jt hh bd ju lm ln lo jy lp lq lr kc jf ls lt kg jj lu lv kk jn lw lx ko ly bi translated"><strong class="ak">什么是词频(TF)？</strong></h2><p id="ec44" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">为了计算词频，让我们看看是否可以在每个文档中找到唯一的单词及其数量。<br/>计算词频有多种方法:</p><ul class=""><li id="a967" class="lz ma hh iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">原始计数:tf( <em class="mi"> t </em>，<em class="mi"> d </em> ) = <em class="mi"> f(t </em>，<em class="mi"> d) </em></li><li id="7235" class="lz ma hh iw b ix mj jb mk jf ml jj mm jn mn jr me mf mg mh bi translated">布尔频率:如果<em class="mi"> t </em>出现在<em class="mi"> d </em>中，tf( <em class="mi"> t </em>，<em class="mi"> d </em> ) = 1，否则为0。</li><li id="5434" class="lz ma hh iw b ix mj jb mk jf ml jj mm jn mn jr me mf mg mh bi translated">根据文档长度调整的词频:tf( <em class="mi"> t </em>，<em class="mi"> d </em> ) = <em class="mi"> f(t </em>，<em class="mi">d)</em>(d中的字数)</li><li id="ca63" class="lz ma hh iw b ix mj jb mk jf ml jj mm jn mn jr me mf mg mh bi translated">对数标度频率:tf( <em class="mi"> t </em>，<em class="mi"> d </em> ) = log (1 + <em class="mi"> f(t </em>，<em class="mi"> d) </em>)</li></ul><p id="3d11" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通常，原始计数或计数向量用于计算术语频率</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="4e5d" class="le jt hh la b fi lf lg l lh li">from sklearn.feature_extraction.text import CountVectorizer<br/>vectorizer = CountVectorizer()<br/>X = vectorizer.fit_transform(data['text of Documents'])<br/>cols = vectorizer.get_feature_names()<br/>count = pd.DataFrame(X.toarray(), columns=cols)</span></pre><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">文档中每个单词的计数。</figcaption></figure><h2 id="e70d" class="le jt hh bd ju lm ln lo jy lp lq lr kc jf ls lt kg jj lu lv kk jn lw lx ko ly bi translated"><strong class="ak">什么是逆文档频率？</strong></h2><p id="0723" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">由于术语频率倾向于给常用词更多的权重，我们需要一些东西来减少这种影响，因为像<em class="mi">“the”、“is”、“am”</em>这样的常用词对于确定文档的唯一性来说不是很有用。如果有两个文档，一个关于体育，一个关于医学领域，像“<em class="mi">足球”、“高血压”</em>这样的词将很少出现，并且将有助于更好地确定文档。</p><blockquote class="mo mp mq"><p id="3a9a" class="iu iv mi iw b ix iy iz ja jb jc jd je mr jg jh ji ms jk jl jm mt jo jp jq jr ha bi translated">Zipf定律指出，在一组或一组文档中，任何单词的频率都与其在频率表中的排名成反比。因此，最频繁出现的单词的出现频率大约是第二频繁出现的单词的两倍，第三频繁出现的单词的三倍，等等。</p></blockquote><p id="ee98" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">IDF的引入减少了文档集中出现频率很高的术语的权重，增加了出现频率很低的术语的权重。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="7bcb" class="le jt hh la b fi lf lg l lh li"># df is the document frequency (how many dcouments word has occured)<br/>import numpy as np<br/>df = np.array(count.astype(bool).sum())<br/># df = np.array([1,1,1,1,1,3,2,1,1,1,1,3])</span><span id="4cda" class="le jt hh la b fi lj lg l lh li"># No of documents <br/>n_samples = len(data)<br/># n_samples = 3</span><span id="e886" class="le jt hh la b fi lj lg l lh li">smooth_idf = True<br/># smooth_idf is used to avoid divide by zero<br/>df += int(smooth_idf)<br/>n_samples += int(smooth_idf)<br/>idf = np.log(n_samples / df) + 1</span></pre><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">每个单词的IDF</figcaption></figure><p id="22a1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为什么我们在这里使用log，添加log是为了降低高频词的重要性。某个单词的IDF(log之前)越多，该单词的重要性就越大。更多详情请参考此处的。</p><h2 id="21fc" class="le jt hh bd ju lm ln lo jy lp lq lr kc jf ls lt kg jj lu lv kk jn lw lx ko ly bi translated"><strong class="ak">TF *正常化前的IDF</strong></h2><p id="589d" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">这是通过将TF和IDF ((3*12 ) X (12，)相乘计算出来的</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="857f" class="le jt hh la b fi lf lg l lh li">df_before_normalization = count*idf</span></pre><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">标准化前的TF-IDF</figcaption></figure><p id="44c8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> TF*IDF归一化后<br/> </strong>既然sklearn库也对结果进行归一化，我们这里也是这么做的。L2标准化是在每个文档中完成的</p><figure class="kv kw kx ky fd ii er es paragraph-image"><div class="er es mu"><img src="../Images/7ecdea8bc6941d28dda0f9c5e4bd5c3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*0gRGuFvHeIlqPpgTaVXRHQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">L2范数公式</figcaption></figure><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="3619" class="le jt hh la b fi lf lg l lh li">from math import sqrt<br/># For document 1<br/>sqrt(pow(0.0,2)+pow(1.69315,2)+pow(0.0,2)+pow(0.0,2)+pow(1.69315,2)+pow(1.0,2)+pow(0.0,2)+pow(1.69315,2)+pow(1.69315,2)+pow(1.69315,2)+pow(0.0,2)+pow(1.0,2))=4.041507</span><span id="6c98" class="le jt hh la b fi lj lg l lh li">document 1 = 0.0/4.041507, 1.69315/4.041507, 0.0/4.041507, 0.0/4.041507, 1.69315/4.041507, 1.0/4.041507, 0.0/4.041507, 1.69315/4.041507, 1.69315/4.041507, 1.69315/4.041507, 0.0/4.041507, 1.0/4.041507 = <br/>0.0, 0.419, 0.0, 0.0, 0.419, 0.247, 0.0, 0.419, 0.419, 0.418, 0.0, 0.247</span><span id="efca" class="le jt hh la b fi lj lg l lh li"># same way do the same of Document 2 and Document 3</span><span id="27ac" class="le jt hh la b fi lj lg l lh li"># if you want to do this using python<br/># method 1<br/>from sklearn.preprocessing import normalize<br/>tf_idf = normalize(df_before_normalization, norm='l2', axis=1)</span><span id="3117" class="le jt hh la b fi lj lg l lh li"># method 2<br/>from numpy.linalg import norm<br/>for idx, row in df_before_normalization.iterrows():<br/>    print(row/norm(row))</span></pre><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">标准化后的TF-IDF</figcaption></figure><h1 id="2cd7" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak"> Sklearn库:</strong></h1><p id="5c66" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">现在，让我们与图书馆版本进行比较。</p><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">TF-IDF的代码段</figcaption></figure><p id="f5a4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">库的输出:</strong></p><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="lk ll l"/></div></figure><p id="586e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这里，我们可以看到输出与我们手动计算的相同，因为我们遵循了库中使用的相同步骤。我希望阅读博客可能会给你一些关于TD-IDF工作的新信息，如果是这样，请欣赏它。</p><h1 id="9957" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">TF-IDF的使用:</h1><div class="mv mw ez fb mx my"><a href="https://github.com/gskdhiman/zomato-recommendation" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hi fi z dy nd ea eb ne ed ef hg bi translated">gskdhiman/zom ATO-建议</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">github.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm in my"/></div></div></a></div><h2 id="2ad4" class="le jt hh bd ju lm ln lo jy lp lq lr kc jf ls lt kg jj lu lv kk jn lw lx ko ly bi translated"><strong class="ak">参考文献:</strong></h2><div class="mv mw ez fb mx my"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hi fi z dy nd ea eb ne ed ef hg bi translated">sk learn . feature _ extraction . text . tfidf vectorizer-sci kit-learn 0 . 24 . 1文档</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">class sk learn . feature _ extraction . text . tfidf vectorizer(*，input='content '，encoding='utf-8 '，decode_error='strict'…</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">scikit-learn.org</p></div></div><div class="nh l"><div class="nn l nj nk nl nh nm in my"/></div></div></a></div><div class="mv mw ez fb mx my"><a href="https://www.journaldev.com/45324/norm-of-vector-python" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hi fi z dy nd ea eb ne ed ef hg bi translated">Python中向量的范数-计算步骤- JournalDev</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">向量的范数是指向量的长度或大小。有不同的方法来计算…</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">www.journaldev.com</p></div></div><div class="nh l"><div class="no l nj nk nl nh nm in my"/></div></div></a></div><div class="mv mw ez fb mx my"><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hi fi z dy nd ea eb ne ed ef hg bi translated">tf-idf</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">在信息检索中，tf-idf，TF*IDF，或TFIDF，是词频-逆文献频率的缩写，是一个数字…</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">en.wikipedia.org</p></div></div></div></a></div></div></div>    
</body>
</html>