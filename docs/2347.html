<html>
<head>
<title>Word Similarity with Co-occurrence matrix and Word2Vec(Skip-gram)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于共现矩阵和词2Vec的词相似度</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/word-similarity-with-co-occurrence-matrix-and-word2vec-skip-gram-c7052d6993a8?source=collection_archive---------13-----------------------#2021-04-19">https://medium.com/analytics-vidhya/word-similarity-with-co-occurrence-matrix-and-word2vec-skip-gram-c7052d6993a8?source=collection_archive---------13-----------------------#2021-04-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/3585cb1dff127c917705bbcc27c2f2b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Kg8S7s4AGl9opdLv.png"/></div></div></figure><p id="9704" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">自然语言处理中最重要的领域之一是语义分析。识别两个词之间的相似性可以在这方面帮助我们。但首先，我们需要将每个单词转换成机器可以计算的向量。我们可以通过共生矩阵或神经网络来获得向量。在本实验中，我们使用了共现矩阵和跳跃图来显示结果，然后对它们进行比较。</p><h1 id="caa0" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">介绍</h1><p id="4130" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">单词嵌入是一种表示，意思相同的单词有相似的表示。这种表示单词的类型将是自然语言处理中的要点之一。单词嵌入是一种将单个单词表示为数字向量的技术。每个单词将被映射到一个向量，向量值在一些模型中计算，如神经网络或共生矩阵。计算单词嵌入向量有两种常用方法:共生矩阵和神经网络。</p><h1 id="1380" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">共生矩阵</h1><p id="516d" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">共现矩阵展示了单词之间的关系，并描述了单词如何一起出现。转换单词的最简单方法是统计每个单词在每个语料库中的出现次数。</p><p id="c6e2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">除了简单的计数，还有一些其他的方法来计算共生矩阵。在这个实验中，我们使用了<em class="jn"> PMI </em>(逐点互信息)和<em class="jn"> SPPMI </em>(移正PMI)。</p><p id="4b6c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jn"> PMI </em>的想法是，我们想要量化两个单词共现的可能性，考虑到它可能是由单个单词的频率引起的。在现实中，根据语料库中的单词计数来计算<em class="jn"> PMI </em>是有问题的，因此我们需要根据一个单词在任何目标单词的窗口中出现的次数来估计<em class="jn"> PMI </em>。其思路是，语料库中的任何一个词，大部分都是通过邻近词来实现的，而不是全部的语料库。当语料库不太长时，估计量的方差变高，因此我们使用移位正方法而不是<em class="jn"> PMI </em>:</p><figure class="ks kt ku kv fd ii er es paragraph-image"><div class="er es kr"><img src="../Images/5b63160564100fb44a728040307edf9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/0*6a88FKC0EVu_71aQ"/></div></figure><figure class="ks kt ku kv fd ii er es paragraph-image"><div class="er es kw"><img src="../Images/271667ade7d163bf202efbc1a35180c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/0*OlD1FLr0PKzeNr9y"/></div></figure><h1 id="06c3" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">神经网络</h1><p id="1c98" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">使用神经网络创建嵌入是有帮助的，因为它们在大型语料库上工作得更好，并且还降低了变量的维度，并在转换的空间中表示变量。神经网络嵌入可以克服一键编码的局限性。</p></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="a7c2" class="jo jp hh bd jq jr le jt ju jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl bi translated">资料组</h1><p id="b937" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">为了训练模型，丰富的文本是很重要的。数据集应该几乎覆盖其他著名测试集评估的每个领域，这些测试集主要由人工评分。在几个数据集当中，我们选择了一个定制的Wiki-Dump(build 2021–04–01 ),其中包含用于构建模型的页面和文章。有不同类型的维基转储文件(。xml。txt。sql和…)包含了大量的文本，似乎是这方面最好的数据集之一。Wiki-Dump的优点是它包括正式和非正式文本，因此它可以更好地显示两个单词之间的相似性。</p><h1 id="f772" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结果</h1><figure class="ks kt ku kv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lj"><img src="../Images/b7b23a2b23c7278e416392d6c956338b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1tvOceKzAHrgDDSwdgODaw.png"/></div></div></figure></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="b387" class="jo jp hh bd jq jr le jt ju jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl bi translated"><em class="lk">参考文献</em></h1><p id="d3da" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">奥马尔·利维和约夫·戈德堡。神经单词嵌入作为隐式矩阵分解。神经信息处理系统进展，27:2177–2185，2014。</p></div></div>    
</body>
</html>