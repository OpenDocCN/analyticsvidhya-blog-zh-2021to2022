<html>
<head>
<title>Out-and-Out in Artificial Neural Networks with Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带有Keras的人工神经网络中的完全</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/out-and-out-in-artificial-neural-networks-with-keras-3e4f1aa08cc7?source=collection_archive---------3-----------------------#2021-06-16">https://medium.com/analytics-vidhya/out-and-out-in-artificial-neural-networks-with-keras-3e4f1aa08cc7?source=collection_archive---------3-----------------------#2021-06-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="bc04" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">神经网络的基本直觉</p><h2 id="c8f2" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">动机</h2><p id="6108" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">当我开始阅读关于神经网络的文章时，我面临着许多斗争，以理解神经网络背后的基础知识以及它们是如何工作的。开始在网上看越来越多的文章，抓住那些重点，一起整理成私信给我。而且，我想把它们发表出来，让其他人更好地理解。</p><p id="295d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">了解任何领域的基础知识都会很有趣。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/964aae154fb691faa6c73895a79bede5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*H4qK8Bufpj9xMalZ"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">艾莉娜·格鲁布尼亚克在<a class="ae ks" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="a80a" class="kt jd hh bd je ku kv kw ji kx ky kz jm la lb lc jp ld le lf js lg lh li jv lj bi translated">感知器</h1><p id="4f6b" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">感知器是最简单的人工神经网络架构之一，由Frank Rosenblatt于1957年发明。它是一种略有不同的人工神经元，称为TLU(阈值逻辑单元)有时是线性阈值单元。每个输入连接都与一个权重相关联。</p><p id="86ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">单个TLU可用于简单的线性二元分类。它计算输入的线性组合，如果结果超过阈值，则输出正类，否则输出负类。</p><h1 id="6b87" class="kt jd hh bd je ku kv kw ji kx ky kz jm la lb lc jp ld le lf js lg lh li jv lj bi translated">感知器是如何训练的？</h1><p id="acab" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">使用该规则的变体来训练它们，该变体考虑了网络在进行预测时产生的误差；然后感知器学习规则加强连接，帮助减少错误。他们一次接受一个训练实例，对于每个实例，它都做出预测。对于每一个产生错误预测的输出神经元，它都加强了来自输入的连接权重，而这些输入本来会有助于正确的预测。</p><p id="8554" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Scikit-Learn的感知器类相当于使用SGD分类器。</p><blockquote class="lk ll lm"><p id="584e" class="ie if ln ig b ih ii ij ik il im in io lo iq ir is lp iu iv iw lq iy iz ja jb ha bi translated">与逻辑回归分类器相反，感知器不输出类别概率；相反，他们基于一个硬门槛做出预测。这是比感知器更喜欢逻辑回归的一个主要原因。</p></blockquote><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lr"><img src="../Images/e4d37593305cd8b6ef823b6089769b63.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*5AgVMmDvjUUVk7UYzg152A.png"/></div></figure><h1 id="ab38" class="kt jd hh bd je ku kv kw ji kx ky kz jm la lb lc jp ld le lf js lg lh li jv lj bi translated">反向传播</h1><p id="5eb1" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">这个过程非常重要，对于每个训练实例，反向传播算法首先进行预测(正向传递)并测量误差，然后反向遍历每个层以测量每个连接的误差贡献(反向传递)，最后调整连接权重以减少误差(梯度下降步骤)。</p><p id="5f5c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机初始化所有隐层连接权重是很重要的，否则训练将会失败。例如，如果将所有权重和偏差初始化为零，那么给定层中的所有神经元将完全相同，因此反向传播将以相同的方式影响它们，因此它们将保持相同。换句话说，尽管每层有数百个神经元，你的模型将表现得好像每层有一个神经元；不会太聪明。相反，如果你随机初始化权重，就会破坏对称性，允许反向传播来训练一个多样化的神经元团队。</p><h1 id="61e5" class="kt jd hh bd je ku kv kw ji kx ky kz jm la lb lc jp ld le lf js lg lh li jv lj bi translated">回归多层感知器</h1><p id="d2a0" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">在为回归建立MLP方程时，我们不需要输出神经元的激活函数，因此它们可以自由输出任何范围的值。但是如果你想要正的输出，那么使用ReLU激活函数，当值为负时，它是平滑的并且接近于0，当值为正时，它接近于值。否则，如果您只需要一个范围的值，则使用逻辑或正切，对于逻辑函数，将标签从0缩放到1，对于双曲正切，将标签从-1缩放到1。</p><p id="410b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Huber loss:是MAE和MSE的结合。当误差小于阈值时，它是二次的，而当误差大于阈值时，它是线性的。</p><h1 id="9664" class="kt jd hh bd je ku kv kw ji kx ky kz jm la lb lc jp ld le lf js lg lh li jv lj bi translated">MLP氏分类</h1><p id="0331" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">在MLP的中，有像二元分类这样的分类任务。我们可以说这种分类在数字0和1之间有一个输出神经元。MLP可以处理多标签分类任务。例如垃圾邮件或紧急检测。更一般地说，您可以为每个正类指定一个输出神经元。如果每个实例可以属于一个单独的类，那么我们可以对输出层使用softmax激活函数。</p><p id="268a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">熟悉神经网络:<a class="ae ks" href="http://playground.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">神经网络的游乐场</a></p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ls"><img src="../Images/69a17c5fccede4ae9941fcbafdf70a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VQGTFKDH9yr3J0VO"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">照片由<a class="ae ks" href="https://unsplash.com/@benhershey?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">本·赫尔希</a>在<a class="ae ks" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="fce9" class="kt jd hh bd je ku kv kw ji kx ky kz jm la lb lc jp ld le lf js lg lh li jv lj bi translated">用函数式API构建复杂问题</h1><p id="2a00" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">这种非顺序神经网络具有广度和深度的神经网络。</p><p id="38d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们建立这样一个网络:</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="7603" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是如果我们需要通过宽路径的特征子集和不同的子集(可能重叠)，我们将通过深路径。</p><p id="f571" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们建立这样一个网络:</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="dcb1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用功能子集的许多用例:</p><ol class=""><li id="e718" class="lv lw hh ig b ih ii il im ip lx it ly ix lz jb ma mb mc md bi translated">定位并分类图片中的主要对象。</li><li id="118d" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ma mb mc md bi translated">使用一个输出对面部表情进行分类，另一个输出对是否戴眼镜进行分类，对面部图片进行多任务分类。</li><li id="fb6c" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ma mb mc md bi translated">减少过拟合和提高模型能力的正则化技术。例如，在神经网络中使用辅助输出，就像它自己学习一样，而不依赖于其他网络。</li></ol><p id="5128" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">添加额外的输出非常简单，只需将它们连接到适当的层，并将它们添加到模型的输出列表中。参见下面的代码。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="lt lu l"/></div></figure><blockquote class="lk ll lm"><p id="b536" class="ie if ln ig b ih ii ij ik il im in io lo iq ir is lp iu iv iw lq iy iz ja jb ha bi translated">每个输出函数都需要其损失函数来编译模型，因此，为此，我们需要传递一个损失列表(如果我们传递一个损失，那么Keras将假设该损失必须用于所有输出，在这种情况下，默认情况下，Keras将计算所有这些损失，只需将它们相加，即可获得用于训练的最终损失)。我们更关心主输出而不是辅助输出(因为它只是用于正则化)。所以我们必须给主输出分配比辅助输出更多的权重。</p></blockquote><pre class="kd ke kf kg fd mj mk ml mm aw mn bi"><span id="85f3" class="jc jd hh mk b fi mo mp l mq mr">model.compile(loss=['mse', 'mse'], loss_weights=[0.9,0.1], optimizer='sgd')</span></pre><h1 id="7467" class="kt jd hh bd je ku kv kw ji kx ky kz jm la lb lc jp ld le lf js lg lh li jv lj bi translated">子类化API以构建动态模型</h1><p id="3931" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">它比顺序函数式API有更多的优势。更命令式的编程风格。</p><ul class=""><li id="b6a5" class="lv lw hh ig b ih ii il im ip lx it ly ix lz jb ms mb mc md bi translated">它可以很容易地保存、克隆和共享。</li><li id="a3e2" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ms mb mc md bi translated">可以显示和分析该结构。</li><li id="942c" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ms mb mc md bi translated">可以很容易地检测和调试错误。</li><li id="a2c6" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ms mb mc md bi translated">整个模型有一个静态的图层图。</li><li id="4b94" class="lv lw hh ig b ih me il mf ip mg it mh ix mi jb ms mb mc md bi translated">循环、条件分支和各种形状</li></ul><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="lt lu l"/></div></figure><h1 id="8c00" class="kt jd hh bd je ku kv kw ji kx ky kz jm la lb lc jp ld le lf js lg lh li jv lj bi translated">微调神经网络超参数</h1><p id="34c2" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">为了提高模型精度或使神经网络更灵活，我们必须微调超参数。有许多超参数需要调整，因此，我们必须聪明地选择需要调整的超参数。最初，尽可能多地调优超参数有助于理解微调。</p><h1 id="95f7" class="kt jd hh bd je ku kv kw ji kx ky kz jm la lb lc jp ld le lf js lg lh li jv lj bi translated">隐藏层数&amp;每个隐藏层要使用的神经元数量</h1><p id="1c13" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">在网络中使用隐藏层是神经网络中的一项任务。因此，隐藏层的使用完全取决于数据集。确保使用输入数据来分配每个隐藏层和输出层的神经元。不要为每个隐藏层分配相同的神经元，有时也有更好的，只有一个超参数要调整，而不是每层一个。</p><p id="7e78" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于像MNIST数据集这样的一些问题，使用一个或两个包含100个神经元的隐藏层就可以了。但是，对于更复杂的问题，我们可以增加隐藏层的数量，直到模型过度拟合。总之，在使用隐藏层时，我们可以使用尽可能多的层，直到它在训练中溢出。</p><p id="2b19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可以分配比我们需要的更多的层和神经元，然后使用早期停止和一些其他正则化技术来防止过度拟合。在某些情况下，如果一个层的神经元比它需要的少，那么它将由于缺少神经元而在训练时丢失信息。无论网络的其余部分有多么庞大和强大，这些信息都将永远无法恢复。</p><h1 id="9f5a" class="kt jd hh bd je ku kv kw ji kx ky kz jm la lb lc jp ld le lf js lg lh li jv lj bi translated">学习率</h1><p id="420b" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">在构建神经网络时，学习速率更为重要，因为这对训练模型很有意义。为了检查最佳学习率，然后开始使用低水平学习率(10^-5)并增加到(10)，然后开始乘以500次迭代的对数，并绘制它们以检查损失。最优学习率会比亏损开始攀升的点低太多。</p><blockquote class="lk ll lm"><p id="9efc" class="ie if ln ig b ih ii ij ik il im in io lo iq ir is lp iu iv iw lq iy iz ja jb ha bi translated">此外，最佳学习率取决于超参数，尤其是批量大小，因此如果您修改任何超参数，请确保也更新学习率</p></blockquote><h1 id="c44e" class="kt jd hh bd je ku kv kw ji kx ky kz jm la lb lc jp ld le lf js lg lh li jv lj bi translated">保存和恢复模型</h1><p id="c85c" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">当使用顺序和功能API时，我们可以保存模型。</p><pre class="kd ke kf kg fd mj mk ml mm aw mn bi"><span id="9cfc" class="jc jd hh mk b fi mo mp l mq mr">model.save("my_model.h5")</span></pre><p id="be47" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">保存和恢复只能通过顺序和功能API来完成，而不能通过模型子类化来完成。我们可以用<code class="du mt mu mv mk b">save_weights</code>和<code class="du mt mu mv mk b">load_weights</code>保存和恢复模型权重</p><p id="b4ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章将完全直觉化为神经网络。</p><p id="1124" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢阅读！！</p><p id="c54c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">希望你喜欢。点击拍手图标。</p></div></div>    
</body>
</html>