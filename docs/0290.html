<html>
<head>
<title>K-Nearest Neighbors (KNN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-最近邻(KNN)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-nearest-neighbors-knn-8f027ae1228f?source=collection_archive---------9-----------------------#2021-01-11">https://medium.com/analytics-vidhya/k-nearest-neighbors-knn-8f027ae1228f?source=collection_archive---------9-----------------------#2021-01-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="1a7b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇博客中，我将写一个非常著名的监督学习算法，即k近邻算法，简称KNN。</p><p id="d3e3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这里，我将解释什么是KNN算法，KNN算法的工业用途，KNN算法如何工作，如何选择K值，优点/缺点，最后我将提供链接到我的KNN算法实现的简要说明Jupyter笔记本。</p><p id="64b3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，我将提供链接到我的<strong class="ig hi">数字分类(在mnsit数据集)</strong>使用KNN算法实现。因此，没有任何进一步的到期让我们开始。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/db3d5416357b55436e176f376c8cb14a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*pP0zfIM915q4qudY5DkHKg.png"/></div></figure><h1 id="92d6" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">什么是KNN算法？</strong></h1><p id="9187" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">KNN是一种根据最相似的点对数据点进行分类的模型。它使用测试数据对未分类点应分类为什么做出“有根据的猜测”。</p><p id="9df3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNN是一种被认为是非参数的算法，也是懒惰学习的一个例子。这两个术语到底是什么意思？</p><ul class=""><li id="a500" class="kn ko hh ig b ih ii il im ip kp it kq ix kr jb ks kt ku kv bi translated">非参数意味着它不做任何假设。该模型完全是由提供给它的数据组成的，而不是假设它的结构是正常的。</li><li id="d344" class="kn ko hh ig b ih kw il kx ip ky it kz ix la jb ks kt ku kv bi translated">懒惰学习意味着算法不进行归纳。这意味着使用这种方法时几乎不需要培训。因此，当使用KNN时，所有的训练数据也用于测试。</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lb"><img src="../Images/2f619780c163f0d45ed9dadc2e633adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*0Pqqx6wGDfFm_7GLebg2Hw.png"/></div></figure><p id="015f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">KNN的K是什么？</strong></p><p id="66d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">k =最近邻的数量</p><p id="6c1a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果k=1，则测试示例被赋予与训练集中最接近的示例相同的标签。如果k=3，则检查三个最接近的类的标签，并分配最常见的(即，至少出现两次)标签，对于较大的k，依此类推。</p><p id="872f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">构建k-最近邻分类器时，您需要选择k的值。您可能已经有了一个特定的k值，或者您可以划分您的数据并使用类似交叉验证的方法来测试k的几个值，以便确定哪个值最适合您的数据。对于n=1000的情况，我敢打赌最佳的k值在1到19之间，但是你真的要试一试才能确定。</p><h1 id="dc4e" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">KNN算法的工业应用</strong></h1><p id="d92c" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">k-最近算法被应用于日常生活的各个领域。它很容易使用，所以数据科学家和机器学习的初学者使用这个算法来完成一个简单的任务。k最近邻算法的一些用途包括:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lc"><img src="../Images/15fc23d8eb0bab8e639c32c455a69477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*u2efvCGyRgmwvqjreG6NRw.jpeg"/></div></figure><h2 id="6758" class="ld jl hh bd jm le lf lg jq lh li lj ju ip lk ll jy it lm ln kc ix lo lp kg lq bi translated">发现糖尿病比率</h2><p id="4853" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">糖尿病基于年龄、健康状况、家庭传统和饮食习惯。但是在一个特定的位置，我们可以基于K最近邻算法来判断糖尿病的比率。如果你计算出年龄、怀孕、血糖、血压、皮肤厚度、胰岛素、体重指数和其他所需的数据，我们可以很容易地绘制出某一年龄患糖尿病的概率。</p><h2 id="9d28" class="ld jl hh bd jm le lf lg jq lh li lj ju ip lk ll jy it lm ln kc ix lo lp kg lq bi translated">推荐系统</h2><p id="2702" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">如果我们在任何在线商店搜索任何产品，它都会显示该产品。决定特定产品时，它会推荐一些其他产品。当你知道亚马逊35%的收入来自于推荐系统后，你会惊讶不已。决定在线商店，YouTube，网飞，和所有的搜索引擎使用的算法k-最近邻。</p><h2 id="cff7" class="ld jl hh bd jm le lf lg jq lh li lj ju ip lk ll jy it lm ln kc ix lo lp kg lq bi translated">概念搜索</h2><p id="e84b" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">概念搜索是K近邻算法的工业应用。这意味着同时搜索相似的文档。互联网上的数据每秒都在增加。主要问题是从大量数据库中提取概念。k-最近邻有助于从简单的方法中找到概念。</p><h2 id="71d2" class="ld jl hh bd jm le lf lg jq lh li lj ju ip lk ll jy it lm ln kc ix lo lp kg lq bi translated">发现乳腺癌的比率</h2><p id="aefc" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">在医学领域，KNN算法被广泛使用。它被用来预测乳腺癌。这里使用KNN算法作为分类器。K最近邻是这里最容易应用的算法。根据当地的历史、年龄和其他条件，KNN适合标记数据。</p><h1 id="67f3" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">KNN算法是如何工作的</h1><p id="d89e" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">考虑具有两个变量体重和身高的数据集。每个点被分为超重、正常和体重不足。不，我给出一组数据:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es lr"><img src="../Images/74a5ad71e3ce613886a94d873c5f9024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uUzg0yucIdyLTxoEElmDXQ.png"/></div></div></figure><p id="2178" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在如果我给出身高157 cm的值。之前没有给出数据。根据最接近的值，它预测体重为157厘米。它使用的是k近邻模型。</p><p id="ab90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以使用欧几里德距离公式来实现k近邻算法。它通过确定两个坐标的距离来工作。在图中，如果我们绘制(x，y)和(a，b)的值，那么我们将暗示公式为:</p><p id="3954" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">距离(d)=(x-a)+(y-b)</p><h1 id="894b" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">选择正确的K值</h1><p id="464f" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">为了选择最适合您的数据的K，我们使用不同的K值运行KNN算法几次，并选择减少我们遇到的错误数量的K，同时保持算法在给定以前从未见过的数据时准确做出预测的能力。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es lw"><img src="../Images/d05eef83567ee8732a4a50e44038da95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XGVL2aYmBHjNyD0_.png"/></div></div></figure><p id="9bab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是一些需要记住的事情:</p><ol class=""><li id="90c1" class="kn ko hh ig b ih ii il im ip kp it kq ix kr jb lx kt ku kv bi translated">当我们把K值减小到1时，我们的预测变得不稳定。只需思考一分钟，想象K=1，我们有一个由几个红色和一个绿色包围的查询点(我在想上面彩色图的左上角)，但绿色是单个最近邻。合理地说，我们会认为查询点最有可能是红色的，但是因为K=1，KNN错误地预测查询点是绿色的。</li><li id="ad36" class="kn ko hh ig b ih kw il kx ip ky it kz ix la jb lx kt ku kv bi translated">相反，当我们增加K的值时，由于多数投票/平均，我们的预测变得更加稳定，因此，更有可能做出更准确的预测(直到某一点)。最终，我们开始看到越来越多的错误。在这一点上，我们知道我们把K值推得太远了。</li><li id="f244" class="kn ko hh ig b ih kw il kx ip ky it kz ix la jb lx kt ku kv bi translated">如果我们在标签中采取多数投票(例如，在分类问题中选择模式)，我们通常将K设为奇数，以进行平局决胜。</li></ol><h1 id="13be" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">KNN算法的优势</strong></h1><ol class=""><li id="1493" class="kn ko hh ig b ih ki il kj ip ly it lz ix ma jb lx kt ku kv bi translated">该算法简单，易于实现。</li><li id="08a3" class="kn ko hh ig b ih kw il kx ip ky it kz ix la jb lx kt ku kv bi translated">没有必要建立一个模型，调整几个参数，或作出额外的假设。</li><li id="3706" class="kn ko hh ig b ih kw il kx ip ky it kz ix la jb lx kt ku kv bi translated">算法是通用的。它可以用于分类、回归和搜索(我们将在下一节中看到)。</li></ol><h1 id="9f61" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">KNN算法的缺点</h1><ol class=""><li id="a305" class="kn ko hh ig b ih ki il kj ip ly it lz ix ma jb lx kt ku kv bi translated">随着示例和/或预测器/独立变量的数量增加，算法变得明显更慢。</li></ol><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mb"><img src="../Images/30ffe02544d05915b5117b8ee31e28c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*Y6O7ze4lQehdWvva.png"/></div></figure><p id="c4f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是我的Jupyter笔记本，上面简要介绍了KNN的实现(从头开始)。</p><div class="mc md ez fb me mf"><a href="https://github.com/Shag10/Machine-Learning/blob/master/Internity_Internship/Day-8/KNN.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hi fi z dy mk ea eb ml ed ef hg bi translated">shag 10/机器学习</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">这个库包含机器学习算法的基础。基于监督学习的算法…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">github.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ji mf"/></div></div></a></div><p id="36a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">这是我的Jupyter笔记本，简要说明了使用KNN(使用内置模型)对mnsit数据进行数字分类的实现。</strong></p><div class="mc md ez fb me mf"><a href="https://github.com/Shag10/Machine-Learning/blob/master/Internity_Internship/Day-8/Digit_Classification__KNN.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hi fi z dy mk ea eb ml ed ef hg bi translated">shag 10/机器学习</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">这个库包含机器学习算法的基础。基于监督学习的算法…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">github.com</p></div></div><div class="mo l"><div class="mu l mq mr ms mo mt ji mf"/></div></div></a></div><p id="8dc0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">我试图提供所有关于开始使用线性回归及其实现的重要信息。希望你能在这里找到有用的东西。谢谢你一直读到最后。</strong></p></div><div class="ab cl mv mw go mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ha hb hc hd he"><h1 id="1501" class="jk jl hh bd jm jn nc jp jq jr nd jt ju jv ne jx jy jz nf kb kc kd ng kf kg kh bi translated"><strong class="ak">参考文献</strong></h1><div class="mc md ez fb me mf"><a href="https://www.fossguru.com/the-application-of-k-nearest-neighbor-algorithm-in-real-life/" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hi fi z dy mk ea eb ml ed ef hg bi translated">K近邻算法在现实生活中的应用</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">k最近邻算法是一种非常简单易行的机器学习算法，用于模式识别…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">www.fossguru.com</p></div></div><div class="mo l"><div class="nh l mq mr ms mo mt ji mf"/></div></div></a></div><div class="mc md ez fb me mf"><a href="https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761" rel="noopener follow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hi fi z dy mk ea eb ml ed ef hg bi translated">基于K-最近邻算法的机器学习基础</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">k-最近邻(KNN)算法是一个简单，易于实现的监督机器学习算法，可以…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="ni l mq mr ms mo mt ji mf"/></div></div></a></div></div></div>    
</body>
</html>