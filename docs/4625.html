<html>
<head>
<title>Squeeze and Excitation Implementation in TensorFlow and PyTorch — Idiot Developer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow和PyTorch中压缩和激发的实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/squeeze-and-excitation-implementation-in-tensorflow-and-pytorch-idiot-developer-7d169ea2917c?source=collection_archive---------2-----------------------#2021-12-01">https://medium.com/analytics-vidhya/squeeze-and-excitation-implementation-in-tensorflow-and-pytorch-idiot-developer-7d169ea2917c?source=collection_archive---------2-----------------------#2021-12-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/8589fe452e9631e63d8128b107a5375d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f6wd1-LLc8n0zLratTMasA.png"/></div></div></figure><p id="8270" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">挤压和激励网络是一种基于通道的注意力机制，用于提高网络的整体性能。在今天的文章中，我们将在TensorFlow和PyTorch中实现挤压和激发模块。</p><h1 id="0113" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">什么是挤压和激励网络？</h1><p id="10fe" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">挤压和激发注意机制是由胡等人在2018年2018年的论文“<a class="ae kq" href="https://arxiv.org/abs/1709.01507" rel="noopener ugc nofollow" target="_blank">挤压和激发网络</a>”中引入的，该论文在TPAMI中有期刊版本。这是注意力机制领域最具优势的论文之一，被引用超过8000次。</p><p id="02dd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">挤压和激励网络基本上为CNN(卷积神经网络)引入了一种新颖的通道式注意机制，以改善它们的通道相关性。网络添加一个参数，该参数相应地重新加权每个通道，使得它对重要特征变得更加敏感，同时忽略不相关的特征。</p><p id="ed2e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">挤压和激励网络是一种基于通道的注意力机制，它通过增强重要特征来相应地重新校准每个通道，从而创建更强大的表示。</p><p id="2c8a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">阅读更多:</strong> <a class="ae kq" href="https://idiotdeveloper.com/squeeze-and-excitation-networks/" rel="noopener ugc nofollow" target="_blank">挤压和激励网络</a></p><figure class="ks kt ku kv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kr"><img src="../Images/c7af2a14e8334ca87fcadff5e0459510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mXdyacUZpZ8iX42N.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">挤压和激励网络的框图。</figcaption></figure><h1 id="73ed" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">TensorFlow中压缩和激发的实现</h1><pre class="ks kt ku kv fd la lb lc ld aw le bi"><span id="601a" class="lf jo hh lb b fi lg lh l li lj">from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, Input </span><span id="db90" class="lf jo hh lb b fi lk lh l li lj">def SqueezeAndExcitation(inputs, ratio=8): <br/>    b, _, _, c = inputs.shape <br/>    x = GlobalAveragePooling2D()(inputs) <br/>    x = Dense(c//ratio, activation="relu", use_bias=False)(x) <br/>    x = Dense(c, activation="sigmoid", use_bias=False)(x) <br/>    x = inputs * x <br/>    return x </span><span id="6ef3" class="lf jo hh lb b fi lk lh l li lj">if __name__ == "__main__": <br/>    inputs = Input(shape=(128, 128, 32)) <br/>    y = SqueezeAndExcitation(inputs) <br/>    print(y.shape)</span></pre><h1 id="2daa" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">PyTorch中压缩和激发的实现</h1><pre class="ks kt ku kv fd la lb lc ld aw le bi"><span id="1873" class="lf jo hh lb b fi lg lh l li lj">import torch <br/>import torch.nn as nn </span><span id="0659" class="lf jo hh lb b fi lk lh l li lj">class SqueezeAndExcitation(nn.Module): <br/>    def __init__(self, channel, ratio=8): <br/>        super().__init__() </span><span id="33d3" class="lf jo hh lb b fi lk lh l li lj">        self.avg_pool = nn.AdaptiveAvgPool2d(1) <br/>        self.network = nn.Sequential(<br/>            nn.Linear(channel, channel//ratio, bias=False),<br/>            nn.ReLU(inplace=True), <br/>            nn.Linear(channel//ratio, channel, bias=False),<br/>            nn.Sigmoid()<br/>        ) </span><span id="811f" class="lf jo hh lb b fi lk lh l li lj">    def forward(self, inputs): <br/>        b, c, _, _ = inputs.shape <br/>        x = self.avg_pool(inputs) <br/>        x = x.view(b, c) <br/>        x = self.network(x) <br/>        x = x.view(b, c, 1, 1) <br/>        x = inputs * x <br/>        return x </span><span id="b9cf" class="lf jo hh lb b fi lk lh l li lj">if __name__ == "__main__": <br/>    inputs = torch.randn((8, 32, 128, 128)) <br/>    se = SqueezeAndExcitation(32, ratio=8) <br/>    y = se(inputs) <br/>    print(y.shape)</span></pre><h1 id="0ea6" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结论</h1><p id="1d20" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">在本编码教程中，您已经了解了最广泛使用的通道式注意力机制之一，称为“挤压和激励网络”。</p><p id="0c1e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">仍然，有一些问题或疑问？就在下面评论吧。更多更新。跟我来。</p><ul class=""><li id="e6be" class="ll lm hh ir b is it iw ix ja ln je lo ji lp jm lq lr ls lt bi translated"><a class="ae kq" href="https://www.youtube.com/idiotdeveloper" rel="noopener ugc nofollow" target="_blank"> YouTube </a></li><li id="ae3d" class="ll lm hh ir b is lu iw lv ja lw je lx ji ly jm lq lr ls lt bi translated"><a class="ae kq" href="https://facebook.com/idiotdeveloper" rel="noopener ugc nofollow" target="_blank">脸书</a></li><li id="6776" class="ll lm hh ir b is lu iw lv ja lw je lx ji ly jm lq lr ls lt bi translated"><a class="ae kq" href="https://twitter.com/nikhilroxtomar" rel="noopener ugc nofollow" target="_blank">推特</a></li><li id="b712" class="ll lm hh ir b is lu iw lv ja lw je lx ji ly jm lq lr ls lt bi translated"><a class="ae kq" href="https://www.instagram.com/nikhilroxtomar" rel="noopener ugc nofollow" target="_blank"> Instagram </a></li></ul></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="f71d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="mg">原载于2021年12月1日https://idiotdeveloper.com</em><a class="ae kq" href="http://idiotdeveloper.com/squeeze-and-excitation-implementation-in-tensorflow-and-pytorch/" rel="noopener ugc nofollow" target="_blank"><em class="mg"/></a><em class="mg">。</em></p></div></div>    
</body>
</html>