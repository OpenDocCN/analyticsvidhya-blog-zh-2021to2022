<html>
<head>
<title>Clustering, and its Methods in Unsupervised Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">聚类及其在无监督学习中的方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/clustering-and-its-methods-in-unsupervised-learning-c1a59e14f867?source=collection_archive---------23-----------------------#2021-01-04">https://medium.com/analytics-vidhya/clustering-and-its-methods-in-unsupervised-learning-c1a59e14f867?source=collection_archive---------23-----------------------#2021-01-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/d1c89b7b11ceea3015f8ec843e19edb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zttbcU5i_UASPRqx"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">埃文·丹尼斯在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="1022" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在不知道标签的情况下在数据集中检测模式的机器学习类型被称为无监督学习。通过分段从数据中提取信息，可以使用它对数据集执行更复杂的操作。无监督学习正在被使用的一些重要领域包括<strong class="iw hi">异常检测、客户细分、社交媒体分析、遗传学、天文数据分析、推荐系统、特征选择。</strong></p><p id="627b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">聚类、神经网络和异常检测是一些流行的无监督机器学习算法。在本文中，我们主要关注集群、其目标、类型和方法。</p><p id="1f17" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">聚类… </strong></p><p id="82ee" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">根据word的定义，它根据一些潜在的隐藏模式和相似性对数据进行聚类。因此，将具有共同结构的对象分组到同一个簇中或者将具有不同特征的对象分组到不同的簇中称为聚类。然后，这种聚类可以进一步用于业务预测分析。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es js"><img src="../Images/e8756429a11e58169a51361f5bfab484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*eAQJW6wP_oPtQWTBrG0mnA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">照片来自发表的论文<a class="ae it" href="https://link.springer.com/article/10.1186/s40537-016-0060-5/figures/1" rel="noopener ugc nofollow" target="_blank">“大图数据聚类的有限随机行走算法”</a></figcaption></figure><p id="4e77" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在该图中，“A”表示基于颜色特征的数据点的聚类,“B”表示未聚类的数据点。</p><p id="299d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们更深入地了解一下聚类的类型和方法！</p><h1 id="68de" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated"><strong class="ak">聚类类型</strong></h1><h2 id="2bcc" class="kv jy hh bd jz kw kx ky kd kz la lb kh jf lc ld kl jj le lf kp jn lg lh kt li bi translated">单形和多形聚类</h2><p id="419d" class="pw-post-body-paragraph iu iv hh iw b ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr ha bi translated">可以根据各种标准进行聚类。使用特征逐个聚类的对象称为<strong class="iw hi"> <em class="lo">单一聚类。</em> </strong>这样的星团有一些共同的性质。例子包括成群的冷血和温血哺乳动物等。相反，当所有的特征同时被使用并且聚类的对象没有相似的属性时，则称之为<strong class="iw hi"> <em class="lo">多面体聚类。</em> </strong>比如基于距离的聚类。</p><h2 id="1277" class="kv jy hh bd jz kw kx ky kd kz la lb kh jf lc ld kl jj le lf kp jn lg lh kt li bi translated">重叠聚类</h2><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/b2caa7f7f0bb1043366ca5f4007b9f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*CsxL3_PXxQMjNOKmujPg6w.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片</figcaption></figure><p id="3944" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一种群集类型，其中对象可以属于一个或多个组。如果它们属于且仅属于一个簇，则称之为<strong class="iw hi"> <em class="lo">硬簇</em> </strong>。而在<strong class="iw hi"> <em class="lo">软聚类</em> </strong>中，对象基于关联强度与多个聚类链接。</p><h2 id="c52a" class="kv jy hh bd jz kw kx ky kd kz la lb kh jf lc ld kl jj le lf kp jn lg lh kt li bi translated">平面聚类与分层聚类</h2><p id="197c" class="pw-post-body-paragraph iu iv hh iw b ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr ha bi translated">在平面聚类中，我们有聚类集或聚类组，而在层次聚类中，我们有不同级别的聚类组。</p><h1 id="81fd" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">聚类方法</h1><p id="51b7" class="pw-post-body-paragraph iu iv hh iw b ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr ha bi translated">根据应用的特征，在机器学习问题中有许多广泛使用的聚类方法。下面列出了一些:</p><ol class=""><li id="e725" class="lq lr hh iw b ix iy jb jc jf ls jj lt jn lu jr lv lw lx ly bi translated">k均值</li><li id="2262" class="lq lr hh iw b ix lz jb ma jf mb jj mc jn md jr lv lw lx ly bi translated">高斯混合模型</li><li id="fed0" class="lq lr hh iw b ix lz jb ma jf mb jj mc jn md jr lv lw lx ly bi translated">凝聚聚类</li><li id="d380" class="lq lr hh iw b ix lz jb ma jf mb jj mc jn md jr lv lw lx ly bi translated">分裂聚类</li><li id="48e1" class="lq lr hh iw b ix lz jb ma jf mb jj mc jn md jr lv lw lx ly bi translated">K-D树</li></ol><p id="950c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们详细研究一下最流行和最广泛使用的聚类方法。</p><h1 id="73c6" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated"><strong class="ak">K-均值聚类(Lloyd/ Forgy法)</strong></h1><p id="5283" class="pw-post-body-paragraph iu iv hh iw b ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr ha bi translated">k均值聚类是一种<strong class="iw hi"> <em class="lo">平坦、坚硬、多面聚类</em> </strong>技术。这种方法可以用于以无监督的方式发现类别，例如手写数字的聚类图像。<strong class="iw hi"> <em class="lo">降维</em> </strong>也是K均值聚类最大的用例之一。它旨在将最相似(<em class="lo">彼此最接近</em>)的数据点分组到同一组中。聚类的目标是最小化聚类内的距离(<em class="lo">聚类内距离</em>)。每个聚类由一个质心表示。与任何其他聚类的质心相比，同一聚类的数据点与其质心的距离最短。该算法分为以下步骤:</p><ol class=""><li id="dc7b" class="lq lr hh iw b ix iy jb jc jf ls jj lt jn lu jr lv lw lx ly bi translated">选择K个数据点作为质心，这样你将有K个聚类开始。</li></ol><p id="6a73" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2.用每个聚类的质心计算每个数据点的距离。</p><p id="fd08" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">3.将数据点分配给与其质心距离最短的聚类。</p><p id="7880" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">4.重新计算每个聚类的质心。</p><p id="251a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">5.重复从2开始的步骤，直到质心停止变化。</p><p id="ca55" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">完成这些步骤后，我们将得到与此类似的集群形状:</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es me"><img src="../Images/c6627aa645807da062b20530d4c938f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*ZqD4toWCoFpUicZ-F_BhEw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片</figcaption></figure><p id="5a45" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了清楚起见，请看一下<a class="ae it" href="http://shabal.in/visuals/kmeans/1.html" rel="noopener ugc nofollow" target="_blank">可视化</a>解释算法。</p><p id="6904" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">k-均值聚类并不保证总能找到最佳聚类。由于质心的初始位置在聚类形成中非常重要，所以存在附近的点不在同一个聚类中的可能性。因此，K-means有机会出现局部极小值。另一种叫做<a class="ae it" href="#2ff1" rel="noopener ugc nofollow">凝聚聚类</a>的聚类类型解决了这个问题。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/1ecda9c95bd6de452e2f8e7e304514dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*hDl99vMuLa4FifnoCCzOrQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片</figcaption></figure><p id="c43b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在该图中,‘A’表示好的聚类，而‘B’表示由于质心的初始错误定位而导致的坏的聚类。</p><p id="bf30" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">如何为K找到好的值？</strong></p><p id="ef5a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里出现的一个很基本的问题是，我们应该如何选择K的值？这对算法性能和评估有非常深刻的影响。选择K值的适当算法方式是<strong class="iw hi"> <em class="lo">肘法。</em> </strong>使用这种方法，我们尝试K的多个值，对每个值运行K-Means，并计算总距离。</p><blockquote class="mg mh mi"><p id="d1ef" class="iu iv lo iw b ix iy iz ja jb jc jd je mj jg jh ji mk jk jl jm ml jo jp jq jr ha bi translated">聚合距离是所有集群内距离的总和。换句话说，它表示所有分类的总方差。</p></blockquote><p id="f51c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">将在总距离最小的地方选择k值。下图将显示k = 3的最佳值。k=3后，方差有小的变化。随着k值的增加，也有可能过度拟合。所以我们更喜欢肘形开始形成或图形开始变平时的k值。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es mm"><img src="../Images/2b19cfe0cb01bbd4552cde8dfd5bcee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*LhGxFcau9duGdQbGafYLmw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图自肘法<a class="ae it" href="https://code-ai.mk/kmeans-elbow-method-tutorial/" rel="noopener ugc nofollow" target="_blank">教程</a></figcaption></figure><h1 id="6d23" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">分层聚类</h1><ol class=""><li id="2ff1" class="lq lr hh iw b ix lj jb lk jf mn jj mo jn mp jr lv lw lx ly bi translated"><strong class="iw hi">集聚集群</strong></li></ol><p id="4211" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">也称为<strong class="iw hi"> <em class="lo"> AGNES(凝聚嵌套)</em> </strong>是一种常见的聚类类型，其中对象根据相似性分组在一起。首先，每个对象被认为是一个单独的集群。然后最接近的一对聚类合并在一起，直到所有对象合并成一个大的聚类。这种自底向上的方法总共需要<strong class="iw hi"> <em class="lo"> n个步骤</em> </strong>并产生一个<strong class="iw hi"> <em class="lo">树状图(聚类树)</em> </strong>。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mq"><img src="../Images/18971e9e29b29435f301e064718870bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oHFVAudrHbdiTMKSgFOl0g.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片</figcaption></figure><p id="d98a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 2。分裂聚类</strong></p><p id="d528" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在分裂聚类中，有一个由所有对象组成的大聚类。对于所有这些点，我们运行K-means算法。对于每个结果聚类，我们再次运行K-means聚类，并一直这样做，直到我们得到最大数量的可能聚类。这种自上而下的方法是<strong class="iw hi"> <em class="lo">快速和</em> </strong>。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es mr"><img src="../Images/428b148ebda83124525d72ef90d9e11b.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*sAj3W4x7Ku2VpwU5y5k0BA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图片来自研究论文<a class="ae it" href="https://www.researchgate.net/publication/283349410_A_New_Method_For_Clustering_In_Credit_Scoring_Problems/figures?lo=1&amp;utm_source=google&amp;utm_medium=organic" rel="noopener ugc nofollow" target="_blank">“信用评分问题中聚类的新方法”</a></figcaption></figure></div><div class="ab cl ms mt go mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ha hb hc hd he"><p id="c19f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">从现在开始，我们研究了处理数字数据的聚类技术。如果我们必须对具有分类(文本)特征的数据进行分类会怎样？我们如何找到类别之间的距离度量？解决这种问题的一种简单技术是K模式聚类。</p><h1 id="e0bc" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">分类数据—K-模式聚类</h1><ol class=""><li id="0e32" class="lq lr hh iw b ix lj jb lk jf mn jj mo jn mp jr lv lw lx ly bi translated">首先，随机选择任何实例作为我们的质心。在下面给出的例子中，x1，x2，..，x10是给定的实例。选择x3、x6和x9作为我们的初始质心/群集。</li></ol><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es mz"><img src="../Images/dca74e44c3fd529a24eef5b975778a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*HXFu5g-d6wdDvbUiluskNw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片</figcaption></figure><p id="539d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2.将每个实例与每个集群进行比较(逐个功能)。匹配的计数+1，不匹配的计数为0，然后将分数相加。</p><p id="1c8b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">3.将每个实例分配到其最近的群集(分数最低的群集将被视为最近的群集)。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es na"><img src="../Images/8399cbf8c290e30da3bf79ef13d0a3e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MzV6vjYUBcf7fmJ5h0uelw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片</figcaption></figure><p id="c2f5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">4.对于重新计算质心，找到该聚类的指定实例，然后采用其特征值的模式。例如，为了重新计算c1，找到分配给集群1的实例，即x3，x5，x8，x10。采用特征值模式计算新的质心特征。当没有模式存在时，为该特性随机选择任意值，就像在特性2中我们随机选择b一样。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nb"><img src="../Images/a7dda7b29176adcb96872fb9b4fe0f18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-2WAJ64LjkjOBXyhqPFrA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片</figcaption></figure><p id="89f7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">5.重复同样的过程，直到收敛。</p><h1 id="f9f5" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">距离测量技术</h1><p id="a843" class="pw-post-body-paragraph iu iv hh iw b ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr ha bi translated">为了找出聚类之间的距离，使用了多种方法。一些重要的例子如下:</p><ul class=""><li id="1aa6" class="lq lr hh iw b ix iy jb jc jf ls jj lt jn lu jr nc lw lx ly bi translated"><strong class="iw hi">单链路</strong></li></ul><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es nd"><img src="../Images/4c5d6676ad9dc3534730f40c0a2cac03.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*W_1Qvyvn8JQxlgMB-7Lv6g.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图片摘自<a class="ae it" href="https://www.saedsayad.com/clustering_hierarchical.htm" rel="noopener ugc nofollow" target="_blank">文章</a></figcaption></figure><p id="c1f7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="lo">是星团中</em> <strong class="iw hi"> <em class="lo">两个最近点</em> </strong> <em class="lo">之间的距离。</em></p><ul class=""><li id="2f88" class="lq lr hh iw b ix iy jb jc jf ls jj lt jn lu jr nc lw lx ly bi translated"><strong class="iw hi">完成链接</strong></li></ul><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es ne"><img src="../Images/2ea2e0225692fefaa60a488e84a2aa90.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*S7MBsfMjokmK6Bn6EBWRfw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图片摘自<a class="ae it" href="https://www.saedsayad.com/clustering_hierarchical.htm" rel="noopener ugc nofollow" target="_blank">文章</a></figcaption></figure><p id="1630" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="lo">它是</em> <strong class="iw hi"> <em class="lo">星团中两个最远点</em> </strong> <em class="lo">之间的距离。</em></p><ul class=""><li id="6924" class="lq lr hh iw b ix iy jb jc jf ls jj lt jn lu jr nc lw lx ly bi translated"><strong class="iw hi">平均链接</strong></li></ul><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/0eca9dea516796f894449eb99029a6f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*T88hBiAxBDdqk5ac5WAw-g.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图片摘自<a class="ae it" href="https://www.saedsayad.com/clustering_hierarchical.htm" rel="noopener ugc nofollow" target="_blank">文章</a></figcaption></figure><p id="7066" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="lo">是由</em> <strong class="iw hi"> <em class="lo">平均所有两两距离</em> </strong> <em class="lo">计算出的距离。</em></p><ul class=""><li id="e0de" class="lq lr hh iw b ix iy jb jc jf ls jj lt jn lu jr nc lw lx ly bi translated"><strong class="iw hi">质心法</strong></li></ul><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es ng"><img src="../Images/56a92d2b77c97053ba952b1b14e053e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*hw8eTzUyzGFBhqwQFqu9Fw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者图片</figcaption></figure><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es nh"><img src="../Images/bb692d5e13c2b41975eac161bf9dea1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*3OhtaTInX127-0IjSbxbKw.png"/></div></figure><p id="3e93" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="lo">是两个星团的</em> <strong class="iw hi"> <em class="lo">质心之间的距离。</em> </strong></p><ul class=""><li id="e2c6" class="lq lr hh iw b ix iy jb jc jf ls jj lt jn lu jr nc lw lx ly bi translated"><strong class="iw hi">沃德氏法</strong></li></ul><p id="4fcd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">两个集群<strong class="iw hi">联合</strong>成一个集群，每个点与<strong class="iw hi">质心</strong>的<strong class="iw hi">总距离</strong>代表我们想要的距离。</p></div><div class="ab cl ms mt go mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ha hb hc hd he"><h1 id="c103" class="jx jy hh bd jz ka ni kc kd ke nj kg kh ki nk kk kl km nl ko kp kq nm ks kt ku bi translated">结论</h1><p id="2c5a" class="pw-post-body-paragraph iu iv hh iw b ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr ha bi translated">到目前为止，我们已经看到了集群，它的一些类型和方法。方法的选择取决于问题的性质和类型。还有许多其他的聚类方法，但是我们在本文中已经讨论了流行的方法。感谢阅读！:)</p></div></div>    
</body>
</html>