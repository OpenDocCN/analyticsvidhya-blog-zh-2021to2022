<html>
<head>
<title>A Kickstart to NLP competitions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP竞赛的开端</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-kickstart-to-nlp-competitions-626557929127?source=collection_archive---------7-----------------------#2021-02-25">https://medium.com/analytics-vidhya/a-kickstart-to-nlp-competitions-626557929127?source=collection_archive---------7-----------------------#2021-02-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/5d112af581022f57218a1ce92d55aeca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*rHIsmislHL9eYX7j2Jo_4Q.png"/></div></figure><p id="c486" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">决定你是想专攻自然语言处理还是计算机视觉的最好方法之一就是两个都试试。从我的个人经验来看，我建议初学者先尝试一下计算机视觉，因为与NLP相比，它的概念更简单，也更直观。在我的NLP学习旅程中，我经常发现资源不完整，所以我在这里收集了一些基础知识。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es jj"><img src="../Images/cd2df9f9f66b2b523036f4ef348b7a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*D5L_yJtD7_bCwkOBbL5hwA.png"/></div></figure><p id="2394" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在学习基础知识的过程中，我们将并行构建模型，以应对Kaggle上的一场竞赛，“<a class="ae jo" href="https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification" rel="noopener ugc nofollow" target="_blank">拼图多语言有毒评论分类</a>”。如果您不熟悉规则或数据集，请在我们继续之前查看一下。我选择这个是因为，它每年都会发生。</p><p id="af6a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">基础知识</strong></p><p id="73c9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">根据牛津词典，英语中大约有171，146个单词在使用。每个都有自己的意思，一组同义词和反义词。我们的模型需要了解每个单词都是唯一的，或者换一种方式说，每个单词都生活在一个单独的维度中。所以，有171，146个维度。这些维度中有些是相关或相近的，有些则相距甚远。比如说，“跑”和“爬”离“瓶子”和“树”更近更远。前两个词是动词，其余的是名词。英语中的每个单词都属于八种词类之一，即名词、代词、动词、形容词、副词、介词、连词和感叹词。你甚至可以添加，数词，文章，限定词到这个列表中，甚至更多的组。对于171，146个单词，我们至少可以得出10，000个类别来唯一地识别每个单词。利用这些信息，我们可以找到单词之间的关系。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es jp"><img src="../Images/48dba949bf7eeafc99605220082edb56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/0*Brq6qopf0tbUQW6i.png"/></div></figure><p id="142c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">每次尝试构建NLP应用程序时，训练模型从头开始学习这些关系是不现实的。这就是我们使用迁移学习的地方，其他人已经做了这项工作，训练模型来学习权重，使用它可以找到关系。有很多模型，像NLTK，spaCy，Glove，Word2Vec，Genism，AllenNLP，FuzzyWuzzy，Fast.ai，TorchText，Flair，Huggingface，OpenNMT，ParlAI，DeepPavlov，openNLP，Amazon understand等等。，这样的例子不胜枚举。人们可以从他们各自的网站上下载这些预先训练好的模型，并开始使用它们来构建架构。</p><p id="c8b0" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">余弦相似度vs欧几里德距离</strong></p><p id="2970" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">计算两个词之间的接近度的一般概念是通过使用余弦相似度而不是欧几里德距离。下图说明了为什么它更好。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es jq"><img src="../Images/2ba3b9d10d4fcc98487539379dce35ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*_QxZ7ZwLPPVGH_MCcDH83A.png"/></div></figure><p id="e005" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">分词</strong></p><p id="6d9d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">它只是把一个句子串分割成一个单词数组。数组中的每个单词或元素称为一个令牌。单词标记化的用途之一是统计单词在注释或数据集中的出现频率。我们还会删除停用词。停用词是最常见的词，如“in”、“the”、“I”、“me”、“you”、“it”、“was”、“was”、“but”、“because”等。你明白了。也看看像词汇化和词干化这样的概念。</p><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jr js l"/></div></figure><p id="e853" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">将代币编码成数字</strong></p><p id="27f7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在将数据分成训练集、验证集和测试集之后，我们对标记进行编码，并将每个唯一的单词映射到一个数字。因此，我们创建了自己的数字索引词典。数据集中的每个注释都有不同的长度。将单个评论作为输入输入到神经网络需要它们都有固定的长度。因此，我们用零填充，并将每个评论的最大长度固定为1500个单词。</p><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jr js l"/></div></figure><p id="ea81" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">创建嵌入向量</strong></p><p id="4817" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">看看<a class="ae jo" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>。他们有不同的预训练模型，可以下载并用于创建我们的嵌入向量。早些时候，我们讨论了如何提出10，000种不同的类别，利用这些类别我们可以在单词之间建立关系。50d、100d、200d、&amp; 300d规范是我们想要用多少不同的类别/组来定义每个令牌。使用下面的代码，可以从预先训练的权重中提取单词向量。它将加载400，000个向量。</p><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jr js l"/></div></figure><p id="700f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">每个单词的向量表示是一个长度等于数据集中唯一单词总数的向量。其中当前令牌索引保存值1，其余全为零。</p><p id="c9be" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi">[……..0,0,0,0,0,0,0,1,0,0,0,0,0…………]</p><p id="6ba5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">嵌入矩阵的形状为(100，总字数，唯一字数)。这里的行数是100，因为我们加载了glove.6B.100d.txt，如果我们加载glove.6B.300d.txt，它将是300。人们可以根据方便试验这些模型。下面是为我们的训练目的收集相关单词的手套表示的代码。因为它被训练来保持4，000，000个单词之间的关系，所以我们能够使用甚至首先不在我们的数据集中的单词的信息。这是迁移学习的一个例子。</p><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jr js l"/></div></figure><p id="0c56" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">递归神经网络</strong></p><p id="39bb" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在一个句子中，下一个单词都依赖于前一个单词。下一句话大部分时间也是依赖于上一句话的。这是给定当前单词的下一个单词比从我们的语料库(来自我们的数据集的字典)中随机选择更有可能是正确的概率。这是RNN的专长，它将处理过的以前的输出也作为输入来产生新的输出。</p><p id="6dd5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这是第一个模型，我们将对其进行试验，以便为性能设定一个基准。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jt"><img src="../Images/4ae8d4869c71940d51ddd87e629acac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Uh1YpVRiez0A2zmu.png"/></div></div></figure><p id="80dd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们称处理下一个令牌为时间步长。X0是令牌1，Xx1是令牌2，X3是令牌3，依此类推。有不同类型的rnn。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jt"><img src="../Images/326fc308b6d0524d1788bdbd01fe1135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_nksLEOU8LCwuxQM.png"/></div></div></figure><p id="4510" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们的场景要求RNN对评论本质上是有毒的还是良性的进行分类。这是一个多对一的RNN。使用Keras实现RNN非常容易。训练是在TPU的帮助下完成的，所以你会发现TPU的相关代码包含了模型定义。在这种情况下，使用镜像策略来优化TPU上的培训过程。</p><p id="4dca" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在TensorFlow的页面上查看如何调整一个<a class="ae jo" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN" rel="noopener ugc nofollow" target="_blank"> SimpleRNN </a>的超参数。</p><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jr js l"/></div></figure><p id="b067" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">“单位”参数是隐藏状态的维度或长度或激活向量的长度。简单的线条，任何人都可以做到。但是后台发生了什么呢？我们来探索一下。下面是RNN和一个节点的图示。</p><div class="jk jl jm jn fd ab cb"><figure class="jy ii jz ka kb kc kd paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><img src="../Images/58ba09c41fa20c97c88e997a20f842f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*h6EL_Q0uWuT1a-JrYL_IfQ.png"/></div></figure><figure class="jy ii ke ka kb kc kd paragraph-image"><img src="../Images/18250d95f5efb2af050f3c20e6edc930.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/0*UUGaIiQlbxUE6YCH.png"/><figcaption class="kf kg et er es kh ki bd b be z dx kj di kk kl translated">有4个单元的简单网络(右图)</figcaption></figure></div><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es km"><img src="../Images/a661d78397be1342b2e9473274d01ff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*-qPLpI7pCAFQ-x4CR-TNyw.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">g1 = tanh()</figcaption></figure><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es kn"><img src="../Images/86e3b58548bf1aeb89387eebc9241495.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*wp0JEHyqPCFsjyLfoQR4MQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">g2 = softmax()</figcaption></figure><p id="0421" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在这里，<t>是前一个时间步的信息如何传递到下一个时间步。y <t>是处理后的当前时间步长的输出。这是RNN的一般概念，但是在我们的例子中，我们不需要计算Wya，因为手头的问题是多对一的问题，情感分类。Wya在最后计算。</t></t></p><blockquote class="ko kp kq"><p id="899a" class="il im kr in b io ip iq ir is it iu iv ks ix iy iz kt jb jc jd ku jf jg jh ji ha bi translated">【learn学到了什么？</p><p id="e375" class="il im kr in b io ip iq ir is it iu iv ks ix iy iz kt jb jc jd ku jf jg jh ji ha bi translated">它学习魏如萱、瓦克斯、威亚、巴&amp; by</p></blockquote><p id="6abe" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">让我们来讨论形状… </strong></p><p id="4235" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">激活次数a <t>将等于Tx，最大长度是我们之前决定的1500个时间步长。让“nx”表示批次总数，让“m”表示每个批次的大小(TPU镜像策略，根据可用的内核，可以仔细计算该值以实现并行处理)。我们有nx * m个训练样本，gf(手套单词特征)值为100，是我们使用glove.6B.100d.txt以来的手套特征数，x_train是shape (num_of_batches，batch_size，comment_length，each_word_glove_features)。</t></p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/97b79bd1c3a048fad98b0727d0ece520.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*OdCAC5YkU3T9f4IhFY62Dw.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">这是训练数据的形状</figcaption></figure><p id="c0c2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">第一批形状是[m，Tx，gf]。对于第I个训练示例和时间步长‘t’，我们将表示x(i) <t>，其等于大小为‘gf’的向量。Tx的值是1500。</t></p><p id="5326" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">单元数量n_a = 128</p><p id="0808" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">激活向量a = [n_a]</p><p id="fe86" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">x的形状为[1500，100]，因为我们有1500个单词用于每个包含填充的注释，每个单词/时间步长的长度为[100]，所以Xt为[100]。</p><p id="2818" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">魏如萱= [128]*[128]。t =[128128]</p><p id="3646" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Wax = a * x = [ n_a ]*[gf] = [128，100]</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es km"><img src="../Images/a661d78397be1342b2e9473274d01ff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*-qPLpI7pCAFQ-x4CR-TNyw.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">g1 = tanh()</figcaption></figure><p id="5129" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">a <t> = g1( [128，128] * [128]+[128，100]*[100]+[128])=[128，128]</t></p><blockquote class="ko kp kq"><p id="002e" class="il im kr in b io ip iq ir is it iu iv ks ix iy iz kt jb jc jd ku jf jg jh ji ha bi translated">注意魏如萱和a <t>有着相同的形状！</t></p></blockquote><p id="0a89" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">简单RNN的输入格式应该是[batch，timesteps，feature]，这只是nx批处理中的一个。指定的单位数就是输出的数量。由于这是一个情感分类任务，所以输出只是一个。</p><p id="0ecf" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">随时间反向传播</strong></p><p id="6f11" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果你不想理解这背后的数学原理，就跳过这一部分。</p><p id="8af8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">下面是RNN的损失函数。在反向传播期间要更新的候选是魏如萱、瓦克斯和巴。从损失函数中，我们可以计算出最终时间步中的da <t>。我们需要计算魏如萱、瓦克斯和巴的损失，即dL/dWaa、dL/dWax和dL/dba。为什么？这些是RNN学习的参数。现在循环反向运行。</t></p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es kw"><img src="../Images/2401414ceff72a6fbf9ffbc4188fea9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*dtE1qjpKfeOzoWzcLUNnqQ.png"/></div></figure><blockquote class="ko kp kq"><p id="a80c" class="il im kr in b io ip iq ir is it iu iv ks ix iy iz kt jb jc jd ku jf jg jh ji ha bi translated">注:tanh⁡(u的衍生物)是(1坦(𝑢))𝑑𝑢.</p></blockquote><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es kx"><img src="../Images/a806490a88571e301c490b540d552def.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*WSKckjiPG_sjDAo1FV68Yg.png"/></div></figure><p id="f83b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">该循环向后迭代“t”。从头开始实现RNN时，必须注意矩阵的形状。上面的等式可能看起来很吓人。但对任何学过微积分的人来说都是非常基础的。这里有一个速成班。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es ky"><img src="../Images/4a96e00d33191781cd70d0c4c6d386cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*o221zcYpQI_nluLt.png"/></div></div></figure><p id="561d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">表现:</strong></p><p id="b341" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这就是simpleRNN在训练过程中的学习方式，以及它在验证集的预测中的表现(不用于训练)。</p><div class="jk jl jm jn fd ab cb"><figure class="jy ii kz ka kb kc kd paragraph-image"><img src="../Images/ec12d09d1313ab39af89867f65e57390.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*qb10_jUWy3H3yZ3tkV9kjw.png"/></figure><figure class="jy ii la ka kb kc kd paragraph-image"><img src="../Images/f250ad70f5d35eafb29516864b7b8e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*zfHtrm2OjRH-mBRmCC1HWw.png"/></figure></div><blockquote class="ko kp kq"><p id="da6b" class="il im kr in b io ip iq ir is it iu iv ks ix iy iz kt jb jc jd ku jf jg jh ji ha bi translated">SimpleRNN没做好。</p></blockquote><h1 id="9f79" class="lb lc hh bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">长短期记忆(LSTM) </strong></h1><p id="0fa7" class="pw-post-body-paragraph il im hh in b io lz iq ir is ma iu iv iw mb iy iz ja mc jc jd je md jg jh ji ha bi translated">一个简单的RNN或香草RNN的问题之一是消失梯度问题。这意味着，在一个长句子中，在句子开始时学习的信息在处理句子结尾的时间步长时可能没有多大用处。这就是记忆进入画面的地方，盖茨决定什么时候记忆，什么时候忘记。在一个简单的RNN中，当前时间步长只接收先前的激活输出，但是在LSTM中，我们也接收来自存储单元的激活输出。</p><div class="jk jl jm jn fd ab cb"><figure class="jy ii me ka kb kc kd paragraph-image"><img src="../Images/c007d03a56c9802cd8e0a4a0252a5655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*q_TKWSfY1n_vNcvJ"/></figure><figure class="jy ii mf ka kb kc kd paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><img src="../Images/38be438ce73f5b6e7b6211d29db45f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/0*6U66NAI-RuTDMboZ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx mg di mh kl translated">(右)一系列垂直的蓝框是隐藏层，每个都是一个LSTM，单位的数量是在定义模型时决定的。这条垂直线重复多次，以显示时间步长的计算。</figcaption></figure></div><p id="50ee" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">从上面的比较中可以看出sigmoid和tanh在LSTM中扮演的角色。Sigmoid压缩0和1之间的值，即大多数值接近1或0。因此，它的行为就像一个开关，关闭和开启活动。我们有三个sigmoids控制的活动，即，忘记，更新和退出。</p><p id="7f31" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">使用双曲正切函数的优点是梯度计算成本较低，并且允许非正损耗/梯度的误差传播，这是sigmoid或Relu所不能做到的。这个性质使它能够更快地收敛，即找到损失函数的全局最小值。负值的可能性是它用于确定更新状态的候选值的原因之一。</p><p id="62ea" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">每个LSTM有三个输入和两个输出。为什么他们不平等？内部只传输两种输出，隐藏状态和存储单元状态，每个LSTM接收一个时间步长。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mi"><img src="../Images/7983c1329dfc73f739102741f67610a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D1G7i_NXwadIFRQCa_5mTA.png"/></div></div></figure><blockquote class="ko kp kq"><p id="d7bd" class="il im kr in b io ip iq ir is it iu iv ks ix iy iz kt jb jc jd ku jf jg jh ji ha bi translated">LSTM学到了什么？</p><p id="640b" class="il im kr in b io ip iq ir is it iu iv ks ix iy iz kt jb jc jd ku jf jg jh ji ha bi translated">它学习Wf，Wi，Wc，Wo，bf，bi，bc &amp; bo</p><p id="60ca" class="il im kr in b io ip iq ir is it iu iv ks ix iy iz kt jb jc jd ku jf jg jh ji ha bi translated">即它学习忘记多少和记住多少的权重矩阵，缩放多少的权重矩阵并在每个时间步更新内存，如何缩放输出的权重矩阵。</p></blockquote><p id="6eff" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在我们深入LSTM背后的数学之前。让我们看看它在预测中是如何工作、学习和执行的。这是代码。</p><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jr js l"/></div></figure><div class="jk jl jm jn fd ab cb"><figure class="jy ii mj ka kb kc kd paragraph-image"><img src="../Images/316232c58ab33ba7cd7c662986f366d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*fx0blnZ7NBNiSlROa4uzKA.png"/></figure><figure class="jy ii mk ka kb kc kd paragraph-image"><img src="../Images/8972510c42ed9ad3a9a73addace031e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*DrLk2go0LqajQuUr2olesA.png"/></figure></div><blockquote class="ko kp kq"><p id="128f" class="il im kr in b io ip iq ir is it iu iv ks ix iy iz kt jb jc jd ku jf jg jh ji ha bi translated">LSTM做得相当不错，一点也不差！</p></blockquote><p id="52af" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">让我们在处理一条评论的同时讨论形状…..</strong></p><p id="9538" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">X = [1500，100]，Xt = [100]</p><p id="9777" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">h <t-1> = [128，128]，Wxi = [100，128]，Whi = [128，128]，bi = [128]</t-1></p><p id="8453" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">it，ft，ot，gt=[100，128]。T *[100]+[128，128]*[128，128]+[128]=[128，128]</p><p id="14c9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">ct = [128，128]*[128，128] + [128，128]*[128，128] = [128，128]</p><p id="a2d5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">h <t> = [128，128]*[128，128]</t></p><p id="ffcf" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">以上做1500遍。一条评论1500个时间步。</p><p id="5b2f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">穿越时间的反向传播</strong></p><p id="c062" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果你不想理解背后的数学原理，跳过这一部分。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es ml"><img src="../Images/b4dc6d99d411e53a98481f45a6f3a74e.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/0*OYhVOJD9YZYhiOgW.jpg"/></div></figure><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mm"><img src="../Images/a42bc0c3d773af657cb3460e45051c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JY00WE9J4k2U6WfHD5Tydg.png"/></div></div></figure><p id="205b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">没错。我们对每个评论的时间步长反向执行上述计算，以更新权重。</p><h1 id="81cc" class="lb lc hh bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">门控循环单元(GRU)</h1><p id="2e54" class="pw-post-body-paragraph il im hh in b io lz iq ir is ma iu iv iw mb iy iz ja mc jc jd je md jg jh ji ha bi translated">GRU是LSTM的变体。这里有一个GRU和LSTM的简单比较。正如你所看到的，遗忘门被包含在更新门本身中。计算出的新的细胞状态直接转移到下一个GRU的时间步骤，而不执行输出转换，在LSTM进一步过滤信息。更多信息可以在<a class="ae jo" href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><div class="jk jl jm jn fd ab cb"><figure class="jy ii mn ka kb kc kd paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><img src="../Images/9f449b1cfd107dc5a826bea644841786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*vF11H9ErAw_zLcxXA4z98g.png"/></div></figure><figure class="jy ii mo ka kb kc kd paragraph-image"><img src="../Images/d8284c204393c574429071d9bf76d55d.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*LdrY5OhVL2SVPn801mjENg.png"/><figcaption class="kf kg et er es kh ki bd b be z dx mp di mq kl translated">斯坦福小抄递归神经网络</figcaption></figure></div><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jr js l"/></div></figure><div class="jk jl jm jn fd ab cb"><figure class="jy ii mr ka kb kc kd paragraph-image"><img src="../Images/be48bd5885ee5b90c8617d71e185f424.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*sTRB8Hsw6kgmkxG5MGkmMw.png"/></figure><figure class="jy ii ms ka kb kc kd paragraph-image"><img src="../Images/8349039e5a33fc3a458348fe6c22be4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*vmoyk0MbDExBK8Mm4Bd_5g.png"/></figure></div><p id="151e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">以上是GRU在预测中的学习和表现。相比之下，它比LSTM表现得更好，时代更少。随着时间的推移，形状和反向传播几乎与LSTM相同。其实简单多了。所以，跳过它。</p><h1 id="6f09" class="lb lc hh bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">双向RNN</h1><p id="3140" class="pw-post-body-paragraph il im hh in b io lz iq ir is ma iu iv iw mb iy iz ja mc jc jd je md jg jh ji ha bi translated">基本单位仍然是RNN、LSTM、GRU等。但是学习是双向的。也就是说，一个词的概率和影响力不仅受到评论中它前面的词的影响，还受到它后面的词的影响。因此，在处理评论中的每个单词时，我们有了更多的信息，包括前面和后面。因为我们用时间步长来说话，它可以被描述为使用来自过去和未来的信息来评估现在。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mt"><img src="../Images/1fb7a090bcc5dde84ec0fef61a2ebf01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ypMXOQkHaFXUqPdu.jpg"/></div></div></figure><p id="973b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">双向LSTM还用于更复杂的任务，如填充缺失的单词、实体识别(名称、动词等)、机器翻译。</p><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jr js l"/></div></figure><div class="jk jl jm jn fd ab cb"><figure class="jy ii mu ka kb kc kd paragraph-image"><img src="../Images/1667e0ae72af607cb3922a6843fd56ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*QCqmJxfxzsOphV4XGJxQ2w.png"/></figure><figure class="jy ii mv ka kb kc kd paragraph-image"><img src="../Images/6ed0982eceb78db23e337e17b681d935.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*8kObiuwvP_TEfuWRdsotYw.png"/></figure></div><p id="b5b3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">仅用5个纪元就获得了0.94的高准确度分数！</p><p id="a010" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">最简单的方法之一就是看下面的图片。因为向前和向后RNN时间步长都是一次一个阶段。请注意，后向层向下一个前向层提供输出。可以推导出双向RNN的梯度方程，除了它多接收了一个额外的输入之外，这与LSTM的情况非常相似。</p><div class="jk jl jm jn fd ab cb"><figure class="jy ii mw ka kb kc kd paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><img src="../Images/e530ee81f16edaa49e992d3479a61ce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*Qkzpk9TD9UdQVMSEuUHjPg.png"/></div></figure><figure class="jy ii mx ka kb kc kd paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><img src="../Images/35a4141d062bff6d24894bd5340ea23b.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*sNRCjiFwSR9As0cxfIGSlQ.png"/></div></figure></div><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es my"><img src="../Images/67c629bc4350961589b70b374433e210.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*BS2K97T_kp6mW_99EydD8g.png"/></div></figure><h1 id="ac2b" class="lb lc hh bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak"> BERT </strong>(变压器的双向编码器表示)</h1><p id="4c73" class="pw-post-body-paragraph il im hh in b io lz iq ir is ma iu iv iw mb iy iz ja mc jc jd je md jg jh ji ha bi translated">既然我们探索了双向RNN学习。在进入BERT之前，我们仍然需要学习编码器-解码器网络和变压器。</p><blockquote class="ko kp kq"><p id="211c" class="il im kr in b io ip iq ir is it iu iv ks ix iy iz kt jb jc jd ku jf jg jh ji ha bi translated">要了解变形金刚，请查看杰伊·阿拉玛的博客。他还写了关于<a class="ae jo" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">伯特</a>的文章。</p></blockquote><p id="afa3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">BERT的“编码器”部分涉及seq2seq模型、注意力模型和神经机器翻译(NMT)的概念。这类似于计算机视觉任务中的自动编码器和GANs。NMT的编码器将句子从“英语”语言转换成一系列数字，然后解码器网络试图预测另一种语言的句子，比如“泰米尔语”。尽管泰米尔语和英语之间的语法差异很大，但NMT能够在它们之间架起一座复杂的桥梁，这显然是一种监督学习技术。</p><p id="4292" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">注意力模型是seq2seq模型的扩展，当要翻译的句子很长时，它的性能更好。关键的改进是注意模型中的编码器不仅向解码器提供最后一个隐藏层的输出序列，而且还提供来自所有隐藏层的输出。注意力模型的解码器接收相对于翻译中的目标单词的计算的上下文向量。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mz"><img src="../Images/9deae2666b760c648e3e69b8d9ce3bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*S4MTMw36tmvJBJx4.png"/></div></div></figure><p id="c9a1" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这就是查询和键的概念出现的地方。一个查询只是一个从我们当前试图翻译的时间步长中获得的向量。Keys是从所有输入时间步长获得的向量。以下是不同上下文向量的图示。通过查询和关键字之间的点积获得上下文向量，然后使用softmax进行缩放。这个向量包含关于其他单词的相关性的信息。我们不计算每个单词与所有其他单词的相关性向量来找到翻译，这在计算上是昂贵的。因此，我们使用带参数“B”的波束搜索，这是一个超参数，可以调整。因为我们只为上下文向量选择“B”个标记，所以我们随机进行“n”次，并对结果进行平均以获得上下文向量。这就是多头注意力机制。</p><p id="1730" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">查询、键和值是如何生成的？利用自我关注机制。没什么特别的，和注意力一样，但是目标是以这样一种方式产生最佳权重，即查询和关键字的点积提供相似性/相关性。这里价值观的作用有点像一般机器学习中‘偏见’的作用。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es na"><img src="../Images/cb06948415d115494bf9e552698fff28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Q0jFTFrat3zJdl-X.JPG"/></div></div></figure><p id="3082" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">人们可以更深入地了解变形金刚和伯特是如何被训练的，并理解为什么他们在各种资源下表现良好。如果你要训练你自己的“伯特”，你需要明白。但是，我们只是将它用于我们的情感分类任务，看看它的表现如何。</p><p id="afef" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在我们的例子中，我们使用变形金刚库中的DistilBertTokenizer来代替手套的嵌入矩阵。我们使用来自DistilBertTokenizer的预训练砝码。作为输入的每个注释的标记化数组必须以“[CLS]”开始，以“[SEP]”结束。</p><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jr js l"/></div></figure><p id="4e3b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">下面是构建BERT模型并训练它的代码。</p><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jr js l"/></div></figure><p id="ea19" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这是伯特如何通过50个时代学习的。</p><div class="jk jl jm jn fd ab cb"><figure class="jy ii nb ka kb kc kd paragraph-image"><img src="../Images/287a1519fc0f71c56537a73366f1dbea.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*U-sJpep_ZrDh1Emak5c6Dw.png"/></figure><figure class="jy ii nc ka kb kc kd paragraph-image"><img src="../Images/6dc8a391bcb8647329067532e2942841.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*4raBxSLCPgQi1feNEPbxdA.png"/></figure></div><p id="1326" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在，您需要做的就是model.predict(test_dataset)并创建您的提交文件，然后看看您在竞争中的排名。今后，应用不同的技术从数据中创建NLP特征将有助于提高预测性能。格兰特·英格索尔、<a class="ae jo" href="https://www.linkedin.com/in/tom-morton-5248552/" rel="noopener ugc nofollow" target="_blank">托马斯·莫顿</a>和<a class="ae jo" href="https://www.linkedin.com/in/drewfarris/" rel="noopener ugc nofollow" target="_blank">德鲁·法里斯</a>的《驯服文本》、本杰明·本福特、<a class="ae jo" href="https://www.amazon.com/s/ref=dp_byline_sr_book_2?ie=UTF8&amp;text=Rebecca+Bilbro&amp;search-alias=books&amp;field-author=Rebecca+Bilbro&amp;sort=relevancerank" rel="noopener ugc nofollow" target="_blank">丽贝卡·比尔布罗</a>、<a class="ae jo" href="https://www.amazon.com/s/ref=dp_byline_sr_book_3?ie=UTF8&amp;text=Tony+Ojeda&amp;search-alias=books&amp;field-author=Tony+Ojeda&amp;sort=relevancerank" rel="noopener ugc nofollow" target="_blank">托尼·奥赫达</a>的《用Python进行应用文本分析》以及许多其他书籍将有助于提高你在这篇博客中学习上述内容所获得的基础知识。</p><p id="adc3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">给你一些掌声来表达你的感激，会鼓励我写更多的博客！！！用以下链接在Linkedin上与我联系。</p><div class="nd ne ez fb nf ng"><a href="https://www.linkedin.com/in/krispective/" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab dw"><div class="ni ab nj cl cj nk"><h2 class="bd hi fi z dy nl ea eb nm ed ef hg bi translated">Krishna Kumar S -印度卡纳塔克邦班加卢鲁高等经济学院| LinkedIn</h2><div class="nn l"><h3 class="bd b fi z dy nl ea eb nm ed ef dx translated">我是一名数据科学家，我利用我的技能帮助组织解决棘手的问题并做出明智的决策…</h3></div><div class="no l"><p class="bd b fp z dy nl ea eb nm ed ef dx translated">www.linkedin.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu ij ng"/></div></div></a></div><p id="21ce" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">你可以在Kaggle上找到完整的代码。</p><div class="nd ne ez fb nf ng"><a href="https://www.kaggle.com/krishnakumarkk/kickstart-to-nlp-competitions" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab dw"><div class="ni ab nj cl cj nk"><h2 class="bd hi fi z dy nl ea eb nm ed ef hg bi translated">NLP竞赛的Kickstart</h2><div class="nn l"><h3 class="bd b fi z dy nl ea eb nm ed ef dx translated">使用Kaggle笔记本探索和运行机器学习代码|使用来自多个数据源的数据</h3></div><div class="no l"><p class="bd b fp z dy nl ea eb nm ed ef dx translated">www.kaggle.com</p></div></div><div class="np l"><div class="nv l nr ns nt np nu ij ng"/></div></div></a></div><p id="5c3b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="kr">我其他的一些博客！</em></p><div class="nd ne ez fb nf ng"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/multi-class-image-classification-using-transfer-learning-with-deep-convolutional-neural-networks-eab051cde3fb"><div class="nh ab dw"><div class="ni ab nj cl cj nk"><h2 class="bd hi fi z dy nl ea eb nm ed ef hg bi translated">基于深度卷积神经网络迁移学习的多类图像分类</h2><div class="nn l"><h3 class="bd b fi z dy nl ea eb nm ed ef dx translated">图像分类是一个有监督的机器学习问题，它试图将整个图像作为一个整体来理解。它使用了…</h3></div><div class="no l"><p class="bd b fp z dy nl ea eb nm ed ef dx translated">medium.com</p></div></div><div class="np l"><div class="nw l nr ns nt np nu ij ng"/></div></div></a></div></div></div>    
</body>
</html>