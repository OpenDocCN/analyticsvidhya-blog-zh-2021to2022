<html>
<head>
<title>Finding similar sentences</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">寻找相似的句子</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/similar-tweets-3bbb8494dd3f?source=collection_archive---------9-----------------------#2021-01-10">https://medium.com/analytics-vidhya/similar-tweets-3bbb8494dd3f?source=collection_archive---------9-----------------------#2021-01-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="885e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本笔记本中，我们尝试使用余弦相似度和欧几里德距离对美国航空公司情绪数据集的最相似推文进行分组。</p><h2 id="63c4" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">必要的进口</h2><p id="fb1a" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">首先，我们导入必要的库。我们在文本处理中使用了nltk，re中的停用词word_tokenize。</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="b7b4" class="jc jd hh kh b fi kl km l kn ko">import numpy as np<br/>import pandas as pd<br/>import re<br/>import nltk<br/>from nltk.corpus import stopwords<br/>from nltk.tokenize import word_tokenize<br/>stopwords = set(stopwords.words('english'))</span></pre><h2 id="967b" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">加载数据集</h2><p id="b261" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">我们使用了ka ggle-Twitter-美国-航空公司-情绪数据集。在现有的各种列中，我们只对包含美国航空公司推文的文本列感兴趣。</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="b1eb" class="jc jd hh kh b fi kl km l kn ko">data_source_url = "https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv"<br/>airline_tweets = pd.read_csv(data_source_url)</span><span id="1ced" class="jc jd hh kh b fi kp km l kn ko">print(airline_tweets.columns)</span><span id="23e2" class="jc jd hh kh b fi kp km l kn ko">Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',<br/>       'negativereason', 'negativereason_confidence', 'airline',<br/>       'airline_sentiment_gold', 'name', 'negativereason_gold',<br/>       'retweet_count', 'text', 'tweet_coord', 'tweet_created',<br/>       'tweet_location', 'user_timezone'],<br/>      dtype='object')</span><span id="1c3f" class="jc jd hh kh b fi kp km l kn ko">airline_tweets.head(2)</span></pre><figure class="kc kd ke kf fd kq er es paragraph-image"><div class="ab fe cl kr"><img src="../Images/fac6c9b64bac658bead2a7c3ac41d1a5.png" data-original-src="https://miro.medium.com/v2/format:webp/1*oti_V-ceTFkCz_CDJXlalw.png"/></div></figure><h2 id="03c6" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">文本预处理</h2><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="4290" class="jc jd hh kh b fi kl km l kn ko">features=airline_tweets.text</span></pre><h2 id="b34b" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">给句子做记号</h2><p id="800c" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">我们使用word_tokenize将句子转换成标记</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="1750" class="jc jd hh kh b fi kl km l kn ko">word_token_sent=[]<br/>for i in range(0,len(features)):<br/>    word_token_sent.append(word_tokenize(features[i]))</span><span id="6661" class="jc jd hh kh b fi kp km l kn ko">word_token_sent[1]</span><span id="27b0" class="jc jd hh kh b fi kp km l kn ko">['@',<br/> 'VirginAmerica',<br/> 'plus',<br/> 'you',<br/> "'ve",<br/> 'added',<br/> 'commercials',<br/> 'to',<br/> 'the',<br/> 'experience',<br/> '...',<br/> 'tacky',<br/> '.']</span></pre><h2 id="956b" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">数据预处理</h2><p id="e2c4" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">我们过滤掉停用词、特殊字符、多空格、单个字符，并将其转换为小写</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="78b3" class="jc jd hh kh b fi kl km l kn ko">filtered_sentence = []<br/>for w in word_token_sent[1]:<br/>    if w not in stopwords:<br/>        filtered_sentence.append(w)</span><span id="e8bd" class="jc jd hh kh b fi kp km l kn ko">processed_features = []<br/>for sentence in range(0, len(features)):<br/>    processed_feature = re.sub(r'\@.*?\s','', str(features[sentence])) #removing mentions<br/>    processed_feature = re.sub(r'\W', ' ', str(processed_feature)) #special character removal<br/>    processed_feature= re.sub(r'\s+[a-zA-Z]\s+', ' ', processed_feature) #single charracter<br/>    processed_feature = re.sub(r'\s+', ' ', processed_feature, flags=re.I) #multi space issues handled<br/>    processed_feature = re.sub(r'^b\s+', '', processed_feature) #removing prefix b<br/><br/>    # Converting to Lowercase<br/>    processed_feature = processed_feature.lower()<br/><br/>    processed_features.append(processed_feature)</span><span id="8f65" class="jc jd hh kh b fi kp km l kn ko">documents = set(processed_features) #removign dublicates</span><span id="5c83" class="jc jd hh kh b fi kp km l kn ko">list(documents)[:3]</span><span id="e44e" class="jc jd hh kh b fi kp km l kn ko">['all hear is blah blah blah you shoulda used that 400 to buy ticket instead amp ill be driving there not flying',<br/> 'fly 2301 delayed do to ice at jfk can switch to late flightr flight for free',<br/> 'where are my bags they weren in lax like your promised 9 out of 10 things today were mess today because of you ']</span></pre><h2 id="3e75" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">…向量化…</h2><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="9c82" class="jc jd hh kh b fi kl km l kn ko">The process of converting words into numbers are called Vectorization.</span><span id="fd59" class="jc jd hh kh b fi kp km l kn ko">from sklearn.feature_extraction.text import TfidfVectorizer<br/>tfidf_vectorizer = TfidfVectorizer()<br/>tfidf_matrix = tfidf_vectorizer.fit_transform(documents)<br/>print(tfidf_matrix.shape)</span><span id="161b" class="jc jd hh kh b fi kp km l kn ko">(14286, 14334)</span></pre><h2 id="f5ce" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">计算推文之间的相似度</h2><p id="8017" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">这里我们给出了两种计算推文相似度的方法。第一种方法是余弦相似性，第二种方法是欧几里得相似性。<strong class="ig hi">欧几里德距离</strong>是通过统计文档之间的常用词的数量来计算的。当普通单词的数量增加但文档谈论不同的主题时，可以看到这种方法的缺点。<strong class="ig hi">余弦相似度</strong>的方法被用来克服这个缺陷。</p><h2 id="ea0c" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">余弦相似性</h2><p id="ca33" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">为了找到相似性，我们使用sklearn库中的cosine_similarity。在这里，我们从我们的数据集中选取第6条推文，找出前5条类似的推文。</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="bb18" class="jc jd hh kh b fi kl km l kn ko">from sklearn.metrics.pairwise import cosine_similarity<br/>similarity_index=cosine_similarity(tfidf_matrix[6:7],tfidf_matrix)</span><span id="9c64" class="jc jd hh kh b fi kp km l kn ko">print(processed_features[6]) # input tweet</span><span id="aa7e" class="jc jd hh kh b fi kp km l kn ko">yes nearly every time fly vx this ear worm won go away</span><span id="2232" class="jc jd hh kh b fi kp km l kn ko">similarity_index=similarity_index.reshape(-1)</span></pre><h2 id="6539" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">结果:使用余弦相似度与输入推文相似的推文</h2><p id="0a8c" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">我们现在使用前面步骤中计算的相似性指数显示前5个最相似的句子。</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="6085" class="jc jd hh kh b fi kl km l kn ko">n=6<br/>print (similarity_index[np.argsort(similarity_index)[-n:]])</span><span id="ded4" class="jc jd hh kh b fi kp km l kn ko">[0.336306   0.35882412 0.36344862 0.36512195 0.41766352 1.        ]</span><span id="47c7" class="jc jd hh kh b fi kp km l kn ko">res = sorted(range(len(similarity_index)), key = lambda sub: similarity_index[sub])[-n:] <br/>  <br/># printing result <br/>print("Indices list of max N elements is : " + str(res))</span><span id="92ad" class="jc jd hh kh b fi kp km l kn ko">Indices list of max N elements is : [1196, 12737, 12480, 5289, 10579, 6]</span><span id="23cf" class="jc jd hh kh b fi kp km l kn ko">for i in res:<br/>    print(processed_features[i])<br/>    print('\n')</span><span id="40e7" class="jc jd hh kh b fi kp km l kn ko">The most similar tweets givne by this approach are:</span><span id="5db9" class="jc jd hh kh b fi kp km l kn ko">you suck 9 hour delay <br/><br/><br/>so bad service in miami airport <br/><br/><br/>flight 1679 n76200 prepares for flight at before departing for http co xbkvcraokn<br/><br/><br/>you just cancelled flightled my flight home so you better get me private jet or something need to get home now<br/><br/><br/> thanks for the 5 hour flight from pit to phx with zero entertainment guess why have quarter million miles on delta<br/><br/># input tweet<br/>yes nearly every time fly vx this ear worm won go away</span></pre><h2 id="7197" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">欧几里德距离</h2><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="e949" class="jc jd hh kh b fi kl km l kn ko">from sklearn.metrics.pairwise import euclidean_distances<br/><br/>similarity_index=euclidean_distances(tfidf_matrix[6:7],tfidf_matrix)</span></pre><h2 id="7bd5" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">结果:使用欧几里德距离，与输入推文相似的推文</h2><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="35e4" class="jc jd hh kh b fi kl km l kn ko">similarity_index=similarity_index.reshape(-1)<br/>n=6<br/>print (similarity_index[np.argsort(similarity_index)[-n:]])<br/>res = sorted(range(len(similarity_index)), key = lambda sub: similarity_index[sub])[-n:]</span><span id="e56c" class="jc jd hh kh b fi kp km l kn ko">[1.41421356 1.41421356 1.41421356 1.41421356 1.41421356 1.41421356]</span><span id="660b" class="jc jd hh kh b fi kp km l kn ko">Using euclidean distance we can see the result have almost similar words. Hence this approach is also called common word approach.</span><span id="d0f6" class="jc jd hh kh b fi kp km l kn ko">print("Indices list of max N elements is : " + str(res)) <br/>for i in res:<br/>    print(processed_features[i])<br/>    print('\n')</span><span id="5c74" class="jc jd hh kh b fi kp km l kn ko">Indices list of max N elements is : [14220, 14225, 14234, 14235, 14257, 14268]</span><span id="db7f" class="jc jd hh kh b fi kp km l kn ko"><br/>The most similar tweets are:</span><span id="035d" class="jc jd hh kh b fi kp km l kn ko">not to mention its three hour wait to get an agent on the phone <br/><br/><br/>aa agent said repeated myself when was explaining told him understand english his reply our conversation is over <br/><br/><br/>i ve got ba flight as your airport couldn accommodate us we were left without any info no vouchers for food nothing <br/><br/><br/>flight 1019 need 2 on the first flight out frm mia 2 tpa m extremely irritated ve been at this airport for 5 5 hrs <br/><br/><br/>i want the flight have on hold don want new flight <br/><br/><br/>what name and department does it come under thanks</span></pre><h2 id="68e5" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">参考</h2><ul class=""><li id="ea3a" class="ku kv hh ig b ih jx il jy ip kw it kx ix ky jb kz la lb lc bi translated"><a class="ae ld" href="https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/?fbclid=IwAR0032F5LjW_ZvFDV-p5G5Lxi4dz975_6ubOkEfaRg187Tu9GRPT670Xxso" rel="noopener ugc nofollow" target="_blank">https://stack abuse . com/python-for-NLP-opinion-analysis-with-scikit-learn/？FB clid = iwar 0032 f 5ljw _ ZvFDV-p5g 5 lxi 4 dz 975 _ 6 ubokefarg 187 tu 9 grpt 670 xxso</a></li><li id="90ce" class="ku kv hh ig b ih le il lf ip lg it lh ix li jb kz la lb lc bi translated"><a class="ae ld" href="https://towardsdatascience.com/understanding-nlp-word-embeddings-text-vectorization-1a23744f7223" rel="noopener" target="_blank">https://towards data science . com/understanding-NLP-word-embeddings-text-vectorization-1a 23744 f 7223</a></li></ul></div></div>    
</body>
</html>