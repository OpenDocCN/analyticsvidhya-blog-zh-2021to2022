<html>
<head>
<title>Which Machine Learning Model Is the Best For Predicting Which Stocks Are Worth Buying?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">哪种机器学习模型最适合预测哪些股票值得买？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/showdown-which-machine-learning-model-is-the-best-for-predicting-which-stocks-are-worth-buying-2516de68d34f?source=collection_archive---------5-----------------------#2021-05-08">https://medium.com/analytics-vidhya/showdown-which-machine-learning-model-is-the-best-for-predicting-which-stocks-are-worth-buying-2516de68d34f?source=collection_archive---------5-----------------------#2021-05-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/eb23c86dbf9f6ca11d965f2ec284759e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*raJCtjXLEVNomYOVZ_S47g.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">资料来源:wccftech</figcaption></figure><p id="04c4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">指导问题:</strong> <em class="js">哪种机器学习模型最适合预测哪些股票值得买入？</em></p><h2 id="47b3" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">数据集描述</h2><p id="3c09" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated">对于这个分析，我将使用来自Kaggle 的<a class="ae kt" href="https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018" rel="noopener ugc nofollow" target="_blank">“美国股票200+金融指标(2018)”数据集。</a></p><p id="a779" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">该数据集包含2018年美国股市的数据。它包含200多个财务指标，这些指标通常出现在每个上市公司每年发布的10-K文件中，涉及超过4k只股票。最后一列“类别”列出了每只股票的二进制分类。如果class = 1，那么股票的价格变化百分比在那一年之后上升；如果class = 0，那么股票的价格变化百分比在那一年之后下降。这个“类别”栏可以用来衡量股票是否值得购买。</p><h1 id="2524" class="ku ju hi bd jv kv kw kx jz ky kz la kd lb lc ld kg le lf lg kj lh li lj km lk bi translated">🕵️‍♀️数据勘探公司</h1><p id="46a4" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated">为了给我的模型选择提供信息，我将首先研究我的数据。</p><h2 id="fd23" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated"><strong class="ak">数据集概述</strong></h2><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="47e9" class="jt ju hi lq b fi lu lv l lw lx">df.info()</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/40e4cdbb2a200180a16eca48e16edd47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mTi9YTBjlye6ts93hYX2UA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jv">图1 </strong>:数据集信息</figcaption></figure><h2 id="f066" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated"><strong class="ak">关联热图</strong></h2><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="e147" class="jt ju hi lq b fi lu lv l lw lx">fig, ax = plt.subplots(figsize=(20,15))<br/>sns.heatmap(df.corr(), annot=False, cmap='YlGnBu', vmin=-1, vmax=1, center=0, ax=ax)<br/>plt.show()</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/827a21e915d7059e6f72e42fa7651703.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zoNs2MFNyQgCUZDJIqR6bw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jv">图2: </strong>显示数据集中要素之间相关性的热图</figcaption></figure><p id="f147" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">相关表</strong></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="6a51" class="jt ju hi lq b fi lu lv l lw lx">corr = df.corr().abs()<br/>s = corr.unstack()<br/>so = s.sort_values(kind="quicksort")<br/>print(so[-4470:-4460])</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/da3e25de971a941d838473cc59e8361d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AYk971PMJKk3Rf-tTHdNuQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jv">表1: </strong>显示具有最高相关性的特征和它们各自的相关系数的表</figcaption></figure><h2 id="10d5" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated"><strong class="ak">探索“类”功能</strong></h2><p id="8de4" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated"><strong class="iw hj">班级计数栏剧情</strong></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="4be7" class="jt ju hi lq b fi lu lv l lw lx">classes = df['Class'].value_counts()<br/>class_len = len(classes)<br/>sns.set_style("darkgrid")<br/>sns.barplot(np.arange(class_len), classes)<br/>plt.title('Class Count', fontsize=18)<br/>plt.show()</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/bf1d58e77a2b99d2f7fb2ec83570a6b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*ypj723MupWM2uValx6iIcw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jv">图3: </strong>显示数据集“类”特征中的值和这些值的计数的条形图</figcaption></figure><p id="2edf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">计算“类”中空值的数量</strong></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="0acb" class="jt ju hi lq b fi lu lv l lw lx">print("Number of null values in 'Class':", df['Class'].isnull().sum())</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/2525c5a8054dd41b569f89caccb526fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uEpS_Pqfrh-6DEGxLI58mQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jv">图4: </strong>显示了“类”特征中空值的数量</figcaption></figure><p id="db78" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这一探索揭示了两个主要发现:</p><ol class=""><li id="3f16" class="md me hi iw b ix iy jb jc jf mf jj mg jn mh jr mi mj mk ml bi translated">如<em class="js">图1 </em>所示，该数据集具有大量的特征(223)和条目(4392)，因此我将选择能够支持该数据集大小的模型</li><li id="5981" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mi mj mk ml bi translated">“类”特征是二进制的，它不是0就是1 ( <em class="js">图3 </em>)。对于任何行，它也不会丢失，这意味着每只股票都被赋予0或1值(<em class="js">图4 </em>)。这证实了我可以将“Class”作为目标变量来使用这个数据集的分类模型。</li></ol><p id="3159" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">基于这些发现，我将选择使用能够处理大型数据集的分类模型来分析这个数据集。</p><h1 id="2a0d" class="ku ju hi bd jv kv kw kx jz ky kz la kd lb lc ld kg le lf lg kj lh li lj km lk bi translated">数据清理和准备🛁</h1><h2 id="71f8" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">将“扇区”列转换为虚拟值</h2><p id="1f3c" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated">“部门”特征描述公司是什么部门的一部分；例如:医疗保健、科技、能源、房地产等。为了能够在我的模型中集成“Sector”特性，我将把这个列转换成虚拟值。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="1690" class="jt ju hi lq b fi lu lv l lw lx">dummy = pd.get_dummies(df['Sector'])<br/>dummy.head()</span></pre><p id="960a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然后，我可以从数据框中删除原始的“扇区”要素，并用新的虚拟值列替换它。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="77a3" class="jt ju hi lq b fi lu lv l lw lx">df = df.drop('Sector', axis=1)<br/>df1 = pd.concat([dummy, df], axis=1)</span></pre><h2 id="201e" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated"><strong class="ak">处理缺失值</strong></h2><p id="5dc4" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated">这个数据集有多少个缺失值？</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="de78" class="jt ju hi lq b fi lu lv l lw lx">print("Total number of null values:", df1.isnull().sum(axis = 1).sum())</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/31051fbcef53a61edce2cf3967d4a2b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*3hiJZORvPNxpAqIJm1AEug.png"/></div></div></figure><p id="db5e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">显然，这个数据集有需要处理的缺失值。但是，在我这样做之前，我想使用热图来了解一下这个缺失值的分布情况。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="5c03" class="jt ju hi lq b fi lu lv l lw lx">plt.figure(figsize = (40,10))<br/>sns.heatmap(df1.isnull(), yticklabels=False)</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/6cff1802a1c9874457758df5914eb83d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yxDbdKqsCF5o4KgmEeeSKA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jv">图5: </strong>显示数据集中所有要素的空值分布的热图</figcaption></figure><p id="a1e8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从<em class="js">图5 </em>中，我已经可以看出有一些特征的缺失值百分比非常高。为了查看丢失值最多的要素以及丢失值的百分比是否高到值得移除该要素，我将对每个要素的空值求和，将这些数据放入数据框中，按降序对数据框进行排序，并打印前20个值。我还将找到这些特征的空值的百分比。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="a320" class="jt ju hi lq b fi lu lv l lw lx">col_len = df1.shape[0]</span><span id="a81f" class="jt ju hi lq b fi mt lv l lw lx">null_df = df1.isnull().sum().sort_values(ascending=False).head(20)<br/>null_df = null_df.to_frame()<br/>null_df.rename(columns={0:"Null Count"}, inplace=True)<br/>null_df['Percentage Null'] = ((null_df["Null Count"] / col_len)*100)</span><span id="c702" class="jt ju hi lq b fi mt lv l lw lx">print(null_df)</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mu"><img src="../Images/d920d11a794b14af3f5b0bd34f1bfcac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*izcE1xnwDGL02rkdAN04_g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jv">表2: </strong>数据帧中具有最高空值计数的二十个特征及其各自缺失值的百分比</figcaption></figure><p id="1895" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">功能“cashConversionCycle”和“operatingCycle”的空值百分比非常高。建议移除缺失值超过70–75%的任何要素；由于这两列有超过99%的空值，它们将被删除。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="f0cd" class="jt ju hi lq b fi lu lv l lw lx">df1 = df1.drop('cashConversionCycle', axis=1)<br/>df1 = df1.drop('operatingCycle', axis=1)</span></pre><p id="54f7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">还剩下大量的空值；但是，我已经移除了缺失值百分比大到值得移除的要素。因为删除更多的特性不是最好的选择，所以我将使用Scikit-learn KNNImputer，它使用k-Nearest Neighbors方法来填充丢失的值。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="db8c" class="jt ju hi lq b fi lu lv l lw lx">imputer = KNNImputer(n_neighbors=2)<br/>df_filled = imputer.fit_transform(df1)</span></pre><p id="65b5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，所有缺失的值都已填充，我可以将数据放入模型中。</p><h1 id="5d39" class="ku ju hi bd jv kv kw kx jz ky kz la kd lb lc ld kg le lf lg kj lh li lj km lk bi translated">模型📈</h1><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="98b6" class="jt ju hi lq b fi lu lv l lw lx">X = df1.drop('Class',axis=1)<br/>y = df1.loc[:,'Class'].values</span><span id="a8b4" class="jt ju hi lq b fi mt lv l lw lx">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</span></pre><p id="8a59" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在确定我的目标变量并将我的数据分成训练和测试之后，我可以开始构建我的模型。</p><h2 id="59c2" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">决策图表</h2><p id="1bbb" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated"><strong class="iw hj">使用决策树的理由:</strong>我选择决策树作为我的模型之一，因为它能够在不同的分类阶段使用不同的特征子集和决策规则。虽然决策树通常容易过度拟合，但我希望通过修剪来减轻这种情况。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="471e" class="jt ju hi lq b fi lu lv l lw lx">dt = DecisionTreeClassifier(criterion='entropy', random_state=0, max_depth=10)<br/>dt.fit(X_train, y_train)<br/>y_pred = dt.predict(X_test)<br/>acc_testing = accuracy_score(y_pred, y_test)</span><span id="fd32" class="jt ju hi lq b fi mt lv l lw lx">print("Accuracy = ",acc_testing)</span></pre><p id="9985" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">结果:<strong class="iw hj">精度= 0.6641379310344827 </strong></p><h2 id="d685" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">随机森林模型</h2><p id="a3e1" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated"><strong class="iw hj">使用随机森林模型的理由:</strong>众所周知，随机森林模型可以很好地处理大量数据，这有利于此分析，因为数据集非常大。它们也不容易在分类中过度拟合，因为它们只考虑特征的子集，并且模型的最终结果依赖于所有的树。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="ff5a" class="jt ju hi lq b fi lu lv l lw lx">from sklearn.metrics import recall_score<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="1900" class="jt ju hi lq b fi mt lv l lw lx">model_rf = RandomForestClassifier(n_estimators=100, random_state=42)</span><span id="c381" class="jt ju hi lq b fi mt lv l lw lx">model_rf.fit(X_train, y_train)</span><span id="dead" class="jt ju hi lq b fi mt lv l lw lx">predict_rf = model_rf.predict(X_test)</span><span id="9e83" class="jt ju hi lq b fi mt lv l lw lx">recall_rf = recall_score(y_test, predict_rf, pos_label=1.0)</span><span id="f22f" class="jt ju hi lq b fi mt lv l lw lx">precision_rf = precision_score(y_test, predict_rf, pos_label=1.0)</span><span id="93fc" class="jt ju hi lq b fi mt lv l lw lx">print(' Accuracy = ', precision_rf)</span></pre><p id="dbba" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">结果:<strong class="iw hj">精度= 0.7627551020408163 </strong></p><h2 id="acbb" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">逻辑回归</h2><p id="51e4" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated"><strong class="iw hj">使用逻辑回归的理由:</strong>我选择了逻辑回归作为我的模型之一，因为它在分类方面非常有效，而且实现起来非常简单。不幸的是，这个数据框架比通常用于逻辑回归的要大，但是我可以通过增加“max_iter”来使我的模型适应这个情况。我也将使用L2正规化，以帮助防止过度拟合。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="6a48" class="jt ju hi lq b fi lu lv l lw lx">model = LogisticRegression(max_iter=500000000, penalty='l2', solver='liblinear')</span><span id="70b7" class="jt ju hi lq b fi mt lv l lw lx">model.fit(X_train, y_train)</span><span id="9de3" class="jt ju hi lq b fi mt lv l lw lx">y_pred = model.predict(X_test)</span><span id="f3df" class="jt ju hi lq b fi mt lv l lw lx">log_acc = accuracy_score(y_test, y_pred)</span><span id="5165" class="jt ju hi lq b fi mt lv l lw lx">print('Accuracy =', log_acc)</span></pre><p id="5145" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">结果:<strong class="iw hj">精度= 0.6793103448275862 </strong></p><h2 id="923f" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">XGBoost</h2><p id="74be" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated"><strong class="iw hj">使用XGBoost的理由:</strong>出于各种原因，我选择了一个XGBoost分类器作为我的模型之一:</p><ul class=""><li id="c276" class="md me hi iw b ix iy jb jc jf mf jj mg jn mh jr mv mj mk ml bi translated">众所周知，它具有出色的速度和性能</li><li id="7695" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mv mj mk ml bi translated">它有各种各样的调谐参数</li><li id="c3c9" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mv mj mk ml bi translated">其核心算法是可并行的</li><li id="4ca1" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mv mj mk ml bi translated">在多种机器学习基准数据集上，该方法表现出比其他ML方法更好的性能</li></ul><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="1b90" class="jt ju hi lq b fi lu lv l lw lx">xgb_model = XGBClassifier(objective="binary:logistic", random_state=2020, learning_rate=0.1)</span><span id="f539" class="jt ju hi lq b fi mt lv l lw lx">xgb_model.fit(X_train, y_train)</span><span id="df8f" class="jt ju hi lq b fi mt lv l lw lx">y_pred = xgb_model.predict(X_test)</span><span id="4cb4" class="jt ju hi lq b fi mt lv l lw lx">xgb_acc = accuracy_score(y_test, y_pred)</span><span id="9e84" class="jt ju hi lq b fi mt lv l lw lx">print("Accuracy = ", xgb_acc )</span></pre><p id="7fa1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">结果:<strong class="iw hj">精度= 0.7282758620689656 </strong></p><h2 id="b996" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">装袋分级机</h2><p id="7507" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated"><strong class="iw hj">使用Bagging分类器的理由:</strong>我的许多模型都使用决策树。不幸的是，决策树容易过度拟合。此外，决策树通常不稳定，这意味着即使训练数据中非常小的变化也会导致非常不同的决策树模型。</p><p id="1c7f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然而，根据我的<em class="js"> ITP 449:机器学习的应用</em>课堂笔记:“基于树的Bagging集成通过将每棵树拟合到训练数据的不同引导样本上来利用这些缺点。由于决策树的不稳定性质，使每棵树过度适应其特定的自举样本将导致具有不同树集合的集合。虽然每棵单独的树都过度拟合了它用来训练的数据，但由于这些不同的树，整体的方差减少了。”</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="180d" class="jt ju hi lq b fi lu lv l lw lx">model_bagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 10, random_state = 42)</span><span id="252e" class="jt ju hi lq b fi mt lv l lw lx">model_bagging.fit(X_train, y_train)</span><span id="9a5a" class="jt ju hi lq b fi mt lv l lw lx">pred_bagging = model_bagging.predict(X_test)</span><span id="8fed" class="jt ju hi lq b fi mt lv l lw lx">bagging_acc = accuracy_score(y_test, pred_bagging)</span><span id="e963" class="jt ju hi lq b fi mt lv l lw lx">print(' Accuracy = ', bagging_acc)</span></pre><p id="1aee" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">结果:<strong class="iw hj">精度= 0.6841379310344827 </strong></p><h2 id="13c5" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">投票组合</h2><p id="9dae" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated"><strong class="iw hj">使用投票集成的理由:</strong>我选择投票集成作为我的模型之一，因为该模型结合了多个模型的预测，从而有助于提高预测的准确性。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="ba60" class="jt ju hi lq b fi lu lv l lw lx">rfClf = RandomForestClassifier(max_depth = 10, n_estimators=200, random_state=0)<br/>dtcClf = DecisionTreeClassifier(max_depth = 10)<br/>logClf = LogisticRegression(max_iter=500000000, penalty='l2', solver='liblinear')<br/>xgbClf = XGBClassifier()</span><span id="9cdb" class="jt ju hi lq b fi mt lv l lw lx">clf2 = VotingClassifier(estimators = [('rf',rfClf), ('dt',dtcClf), ('xgb',xgbClf), ('log', logClf)], voting='soft')<br/>clf2.fit(X_train, y_train)<br/>clf2_pred = clf2.predict(X_test)</span><span id="51aa" class="jt ju hi lq b fi mt lv l lw lx">recall_voting = recall_score(y_test, clf2_pred, average="binary", pos_label=1.0)</span><span id="fd42" class="jt ju hi lq b fi mt lv l lw lx">precision_voting = precision_score(y_test, clf2_pred, pos_label=1.0)</span><span id="1a28" class="jt ju hi lq b fi mt lv l lw lx">vote_acc = accuracy_score(y_test, clf2_pred)</span><span id="8734" class="jt ju hi lq b fi mt lv l lw lx">print('Accuracy score', vote_acc)</span></pre><p id="8549" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">结果:<strong class="iw hj">精度= 0.7151724137931035 </strong></p><h1 id="76fc" class="ku ju hi bd jv kv kw kx jz ky kz la kd lb lc ld kg le lf lg kj lh li lj km lk bi translated">型号选择📊</h1><p id="68b2" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated">为了比较每个分类器的准确性并推断出最准确的一个，我将在一个带标签的条形图上绘制每个分类器的准确性。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="cb4d" class="jt ju hi lq b fi lu lv l lw lx">plt.style.use('ggplot')</span><span id="64c8" class="jt ju hi lq b fi mt lv l lw lx">data = {'Classifier':  ["DecisionTreeClassifier", "RandomForestClassifier", "Logistic Regression", "XGBoost", "Bagging Classifier", "Voting Ensemble"],'Acc': [acc_testing, precision_rf, log_acc, xgb_acc, bagging_acc, vote_acc ],}</span><span id="83c3" class="jt ju hi lq b fi mt lv l lw lx">for count, el in enumerate(data['Acc']):<br/>   data['Acc'][count] = el * 100</span><span id="2cb2" class="jt ju hi lq b fi mt lv l lw lx">acc_data = pd.DataFrame (data, columns = ['Classifier','Acc'])</span><span id="fce8" class="jt ju hi lq b fi mt lv l lw lx">g = sns.barplot(x='Classifier', y='Acc', data=acc_data)<br/>plt.xticks(rotation=45)<br/>plt.ylabel('Accuracy %')<br/>plt.title('Classifier Accuracy')</span><span id="c441" class="jt ju hi lq b fi mt lv l lw lx">for p in g.patches:<br/>    g.annotate(format(p.get_height(), '.1f'),(p.get_x() +          p.get_width() / 2., p.get_height()),ha = 'center', va = 'center', xytext = (0, 9), textcoords = 'offset points')</span><span id="a5e2" class="jt ju hi lq b fi mt lv l lw lx">plt.show()</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/015f0b9d1bbf73b429786645fc1d2de2.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*EqxxfnPrfZSUZiWm5HYSew.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jv">图5: </strong>每个分类器的准确度百分比</figcaption></figure><p id="2958" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">随机森林分类器具有最高的准确度% (76.3)，其次是XGBosst (72.8)和投票集成(71.5)。为了确保随机森林分类器实际上是最有希望的模型，而不是简单地过度拟合，我将对每个模型进行交叉验证。</p><p id="dfd0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">随机森林分类器的交叉验证</strong></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="ff71" class="jt ju hi lq b fi lu lv l lw lx">rfc_cv = RandomForestClassifier(n_estimators=100, random_state=42)</span><span id="91e9" class="jt ju hi lq b fi mt lv l lw lx">scores = cross_val_score(rfc_cv, X_train, y_train, cv=10, scoring = "accuracy")</span><span id="89c2" class="jt ju hi lq b fi mt lv l lw lx">print("Scores:", scores)<br/>print("Mean:", scores.mean())<br/>print("Standard Deviation:", scores.std())</span><span id="7f61" class="jt ju hi lq b fi mt lv l lw lx">rfc_cv_mean = scores.mean()</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/14a7fbef250bee23659d016cad13d160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dR2RBxT3amrxobQLqso-uQ.png"/></div></div></figure><p id="55c6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">XGBoost分类器的交叉验证</strong></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="d9b7" class="jt ju hi lq b fi lu lv l lw lx">xgb_cv = XGBClassifier(objective="binary:logistic", random_state=2020, learning_rate=0.1)</span><span id="1363" class="jt ju hi lq b fi mt lv l lw lx">scores = cross_val_score(xgb_cv, X_train, y_train, cv=10, scoring = "accuracy")</span><span id="0c81" class="jt ju hi lq b fi mt lv l lw lx">print("Scores:", scores)<br/>print("Mean:", scores.mean())<br/>print("Standard Deviation:", scores.std())</span><span id="85f4" class="jt ju hi lq b fi mt lv l lw lx">xgb_cv_mean = scores.mean()</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/7dcc5a3d4ae9defd5ba013ed40729f32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Qelj-Hx6DxTz2ZU3iLDew.png"/></div></div></figure><p id="df75" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">投票组合的交叉验证</strong></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="7703" class="jt ju hi lq b fi lu lv l lw lx">rfClf = RandomForestClassifier(max_depth = 10, n_estimators=200, random_state=0)<br/>dtcClf = DecisionTreeClassifier(max_depth = 10)<br/>logClf = LogisticRegression(max_iter=500000000, penalty='l2', solver='liblinear')<br/>xgbClf = XGBClassifier()</span><span id="23a5" class="jt ju hi lq b fi mt lv l lw lx">clf2 = VotingClassifier(estimators = [('rf',rfClf), ('dt',dtcClf), ('xgb',xgbClf), ('log', logClf)], voting='soft')</span><span id="9ccf" class="jt ju hi lq b fi mt lv l lw lx">vote_cv = XGBClassifier(objective="binary:logistic", random_state=2020, learning_rate=0.1)<br/>scores = cross_val_score(vote_cv, X_train, y_train, cv=10, scoring = "accuracy")</span><span id="da80" class="jt ju hi lq b fi mt lv l lw lx">print("Scores:", scores)<br/>print("Mean:", scores.mean())<br/>print("Standard Deviation:", scores.std())</span><span id="88f7" class="jt ju hi lq b fi mt lv l lw lx">vote_cv_mean = scores.mean()</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/eb4a11e773d67ffd0c4331e66662519d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dTc0IdbHjTEYUde_fWF6BQ.png"/></div></div></figure><p id="a9f1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">各模型间交叉验证分数的比较</strong></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="c2a0" class="jt ju hi lq b fi lu lv l lw lx">data1 = {'Classifier':  ["RandomForestClassifier","XGBoost", "Voting Ensemble"],'Cross Validation Score': [rfc_cv_mean, xgb_cv_mean, vote_cv_mean],}</span><span id="4480" class="jt ju hi lq b fi mt lv l lw lx">cv_data = pd.DataFrame (data1, columns = ['Classifier','Cross Validation Score'])</span><span id="ce92" class="jt ju hi lq b fi mt lv l lw lx">cv_data.sort_values(by='Cross Validation Score', ascending=False)</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/18c2fdce2fc3e7e1c234a5141c184ef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nTCH7JOBJMgWMKUq5I9DIw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jv">表3: </strong>以降序显示每个分类器的交叉验证分数</figcaption></figure><p id="1af1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">根据<em class="js">表3 </em>，随机森林分类器具有最高的交叉验证分数以及最高的准确性；因此，我将选择它作为我的最终模型。</p><h2 id="587b" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">模型微调</h2><p id="401e" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated">为了微调我的模型，我将使用Scikit-learn的GridSearchCV来优化我的模型的超参数。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="3f8a" class="jt ju hi lq b fi lu lv l lw lx">from sklearn.model_selection import GridSearchCV</span><span id="b0a6" class="jt ju hi lq b fi mt lv l lw lx">param_grid = {'max_depth': [10, None],'n_estimators': [10, 50, 200],'random_state':[0, 42, None]}</span><span id="d06b" class="jt ju hi lq b fi mt lv l lw lx">grid = GridSearchCV(RandomForestClassifier(), param_grid, refit = True, verbose = 2,n_jobs=-1)</span><span id="346b" class="jt ju hi lq b fi mt lv l lw lx">grid.fit(X_train, y_train)</span><span id="cae1" class="jt ju hi lq b fi mt lv l lw lx">grid_predictions = grid.predict(X_test)</span><span id="56dd" class="jt ju hi lq b fi mt lv l lw lx">print(classification_report(y_test, grid_predictions))</span><span id="59dc" class="jt ju hi lq b fi mt lv l lw lx">print(grid.best_params_)</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/a8c81e8cf801a81c541ded578596dc5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yiJGRej3ap1Z2kS_PnvDlg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图6</figcaption></figure><p id="5120" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Scikit-learn的GridSearchCV推荐以下超参数:</p><ul class=""><li id="80ef" class="md me hi iw b ix iy jb jc jf mf jj mg jn mh jr mv mj mk ml bi translated">max_depth = 10</li><li id="89b5" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mv mj mk ml bi translated">n _估计值= 200</li><li id="410f" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mv mj mk ml bi translated">随机状态= 0</li></ul><p id="11ca" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，我将用这些超参数创建一个新的随机森林分类器。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="9e34" class="jt ju hi lq b fi lu lv l lw lx">after_cv_rf = RandomForestClassifier(max_depth = 10, n_estimators = 200, random_state = 0)<br/>after_cv_rf.fit(X_train, y_train)<br/>after_cv_predict = model_rf.predict(X_test)<br/>recall_cv = recall_score(y_test, after_cv_predict, pos_label=1.0)<br/>aftercv_precision_rf = precision_score(y_test, after_cv_predict, pos_label=1.0)</span><span id="09ac" class="jt ju hi lq b fi mt lv l lw lx">print('Accuracy = ', aftercv_precision_rf)</span><span id="32e0" class="jt ju hi lq b fi mt lv l lw lx">aftercv_rfc_cv = RandomForestClassifier(max_depth = 10, n_estimators = 200, random_state = 0)<br/>scores = cross_val_score(aftercv_rfc_cv, X_train, y_train, cv=10, scoring = "accuracy")</span><span id="6e91" class="jt ju hi lq b fi mt lv l lw lx">aftercv_cv_mean = scores.mean()</span><span id="6ee5" class="jt ju hi lq b fi mt lv l lw lx">print("Mean cross-validation score = ", scores.mean())</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nc"><img src="../Images/8a3b54d424a34ca6c4424662f2af2804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E2jjO0sJNFSWt2IU-1n_CQ.png"/></div></div></figure><p id="3487" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，我们可以将它们整合在一起:</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="a5c3" class="jt ju hi lq b fi lu lv l lw lx">data2 = {"Before Cross-validation":  [precision_rf,rfc_cv_mean ], "After Cross-validation": [aftercv_precision_rf, aftercv_cv_mean], "Type": ["Accuracy Score", "Mean Cross-validation Score"]}</span><span id="57ba" class="jt ju hi lq b fi mt lv l lw lx">final_df = pd.DataFrame (data2, columns = ["Type", "Before Cross-validation","After Cross-validation"])</span><span id="8189" class="jt ju hi lq b fi mt lv l lw lx">final_df.set_index("Type")<br/>final_df.head()</span></pre><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nd"><img src="../Images/3d56619989bf5c217ce844fc87f9d24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oUe-KuwRIgxF7At0K9zLOA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jv">表4: </strong>对比交叉验证前后的准确度得分和平均交叉验证得分</figcaption></figure><p id="9ee2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">虽然模型的准确性得分没有增加，但其平均交叉验证得分增加了，这意味着模型由于超参数优化而得到了改善(<em class="js">表4 </em>)。</p><h2 id="30fd" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">🧙‍♀️最终判决:</h2><p id="887e" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated"><strong class="iw hj">随机森林模型具有76.2%的准确率和73.1%的交叉验证分数，是预测哪些股票值得购买的最佳机器学习模型</strong>🎉<em class="js">(至少对于这个数据集)</em></p><h1 id="eddc" class="ku ju hi bd jv kv kw kx jz ky kz la kd lb lc ld kg le lf lg kj lh li lj km lk bi translated">🎁额外收获:一些见解</h1><h2 id="7e60" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">特征重要性</h2><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/4b57c0129fc942cac51bdcd89ad1b89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*K4zsjfoaRf6h-mwjnu-IXg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jv">图7: </strong>根据我们的随机森林模型，预测股票“类别”的十个最重要的特征</figcaption></figure><p id="e9cd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">根据<em class="js">图7 </em>，在该模型中，以下特征在预测股票是否值得卖出时最为重要:</p><ol class=""><li id="8228" class="md me hi iw b ix iy jb jc jf mf jj mg jn mh jr mi mj mk ml bi translated"><strong class="iw hj">有形资产价值:</strong>“是公司经营中使用的有形的、可计量的资产。财产、厂房和设备等资产是有形资产。它们为公司提供生产商品和服务的手段，从而构成了公司业务的支柱。然而，由于有形资产是有形资产，自然发生的事故会损坏有形资产。”(通过Investopedia)</li><li id="1dd2" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mi mj mk ml bi translated"><strong class="iw hj">收益收益率:</strong>“指最近12个月的每股收益除以当前每股市价。收益收益率(市盈率的倒数)显示的是公司每股收益的百分比。”(通过Investopedia)</li><li id="d87a" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mi mj mk ml bi translated"><strong class="iw hj">总资产:</strong>“个人、公司或国家拥有或控制的具有经济价值的资源的总和，预期它将提供未来的利益。”(通过Investopedia)</li></ol><h2 id="63ec" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">行业洞察</h2><div class="ll lm ln lo fd ab cb"><figure class="nf ij ng nh ni nj nk paragraph-image"><img src="../Images/ce4a2bb6d8ea0706d4aa66a5a5ed0c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*PjoKamcsr0tg3wBtDLsaBg.png"/></figure><figure class="nf ij nl nh ni nj nk paragraph-image"><img src="../Images/893841053f8afe23e068f63cef5a7d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*1FsbTzW-H4zoES1UdnfzsQ.png"/><figcaption class="iq ir et er es is it bd b be z dx nm di nn no translated"><strong class="bd jv">图8 </strong>:统计数据集中每个板块的股票数量。<strong class="bd jv">图9: </strong>每个扇区中“0”和“1”计数的堆积条形图。</figcaption></figure></div><p id="73b2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从<em class="js">图8和图9 </em>中，我们可以看到数据集中最受欢迎的行业如下:</p><ol class=""><li id="8a13" class="md me hi iw b ix iy jb jc jf mf jj mg jn mh jr mi mj mk ml bi translated">金融服务</li><li id="bc9f" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mi mj mk ml bi translated">卫生保健</li><li id="a924" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mi mj mk ml bi translated">技术。</li></ol><p id="12e1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">而数据集中最不受欢迎的部门如下:</p><ol class=""><li id="cc64" class="md me hi iw b ix iy jb jc jf mf jj mg jn mh jr mi mj mk ml bi translated">通信服务</li><li id="2bc9" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mi mj mk ml bi translated">公用事业</li><li id="a918" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mi mj mk ml bi translated">消费者防御</li></ol><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es np"><img src="../Images/a358b8a21e014b5223d55b7946cfb040.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NQZNmGAi7L2kuPaYTbCMKw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jv">图10: </strong>每个扇区的“0”和“1”计数的分类图。</figcaption></figure><p id="dbf6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从<em class="js">图10 </em>中，我们可以看到每股值得购买股票比例最高的行业如下:</p><ol class=""><li id="43c5" class="md me hi iw b ix iy jb jc jf mf jj mg jn mh jr mi mj mk ml bi translated">金融服务</li><li id="edcb" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mi mj mk ml bi translated">房地产</li><li id="4754" class="md me hi iw b ix mm jb mn jf mo jj mp jn mq jr mi mj mk ml bi translated">工业</li></ol></div></div>    
</body>
</html>