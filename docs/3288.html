<html>
<head>
<title>Activation Function in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的激活函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-function-in-neural-networks-1680b8dc8de2?source=collection_archive---------8-----------------------#2021-06-23">https://medium.com/analytics-vidhya/activation-function-in-neural-networks-1680b8dc8de2?source=collection_archive---------8-----------------------#2021-06-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="b3ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你不熟悉神经网络和深度学习，请参考<a class="ae jd" href="https://towardsdatascience.com/a-gentle-introduction-to-neural-networks-series-part-1-2b90b87795bc" rel="noopener" target="_blank">这篇文章</a>以更好地理解我们在本文中讨论的激活函数。</p><p id="5e33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们知道，在神经网络中，每个神经元代表一个激活函数，那么，为什么要使用激活函数呢？常用的激活功能有哪些不同？</p><p id="f07e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们来详细讨论一下这个:</p><h1 id="70af" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">为什么要激活函数？</h1><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/9bf7a44efc63e6637111479acb33e680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-vWHD_W2vjfs7_EsNbXlkw.jpeg"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">神经元激活功能概述</figcaption></figure><p id="01d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在神经网络中，我们从输入层(x0)发送到神经元的数据集将是线性数据，因此我们需要隐藏层中的一个函数，该函数将过滤、归一化、限制和非线性化作为输入数据发送到其他层的数据集，因此，这就是激活函数的存在。</p><p id="c943" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">激活函数还有助于将每个神经元的输出归一化到1和0之间或-1和1之间的范围。</p><p id="9a81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">激活函数是帮助确定神经网络输出的函数。这些类型的函数附加到网络中的每个神经元，并根据每个神经元的输入是否与模型的预测相关来确定是否应该激活它。</p><p id="b2bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出层中的激活函数称为输出函数，如果它在隐藏层中，则称为激活函数。</p><p id="dce3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在神经网络中，输入被输入到输入层的神经元中。每个神经元都有一个权重，输入数乘以权重就得到神经元的输出，输出传递到下一层。激活函数是馈送当前神经元的输入和去往下一层的输出之间的数学“门”。它可以像阶跃函数一样简单，根据规则或阈值打开或关闭神经元输出。</p><p id="6165" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">神经网络使用非线性激活函数，可以帮助网络学习复杂的数据，计算和学习几乎任何代表问题的函数，并提供准确的预测。</p><p id="e101" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">激活功能包括:</p><ol class=""><li id="34fa" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc kx ky kz la bi translated"><strong class="ih hj">线性激活函数</strong></li></ol><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lb"><img src="../Images/240a3ef10bb174d1ab8ac140c2398c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Puh-rf9ZD32qh52H.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图片来源:<a class="ae jd" rel="noopener" href="/analytics-vidhya/comprehensive-synthesis-of-the-main-activation-functions-pros-and-cons-dab105fe4b3b">中</a></figcaption></figure><ul class=""><li id="5842" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc lc ky kz la bi translated">线性函数(直线形式→ y=mx+c ),其中f(x)值的范围在-无穷大到+无穷大之间，这导致输出(y)值的范围也在-无穷大到+无穷大之间。</li><li id="6037" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc lc ky kz la bi translated">线性函数的导数总是常数</li></ul><p id="22d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为什么我们不用线性函数作为激活函数？</strong></p><ul class=""><li id="0d2d" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc lc ky kz la bi translated">线性函数在处理非常复杂的数据时存在一些限制(复杂性差)我所说的复杂数据是指从不同的图像中识别人脸，我们需要在像素范围内计算更多的特征，线性函数不能有效地做到这一点，但非线性函数在复杂数据集方面表现更好。</li></ul><p id="c6c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。二元阶跃函数</strong></p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es li"><img src="../Images/5b161a2bcf8a902902d5939a6f9db4e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xOMoYa2k-AuWIv0l.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图片来源:<a class="ae jd" rel="noopener" href="/analytics-vidhya/comprehensive-synthesis-of-the-main-activation-functions-pros-and-cons-dab105fe4b3b">中</a></figcaption></figure><ul class=""><li id="abcc" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc lc ky kz la bi translated">与线性函数相比，二进制阶跃函数的计算量非常低，因为它只返回值0，1。</li></ul><p id="79b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为什么我们不用二元阶跃函数作为激活函数？</strong></p><ul class=""><li id="5c73" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc lc ky kz la bi translated">因为它只返回值0，1，二进制阶跃函数的导数总是零。我们知道，在反向传播过程中，我们计算方程的微分，如果激活函数给出0值，那么权重没有更新或变化，这是一个大问题。</li></ul><p id="24f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。非线性函数</strong></p><p id="b926" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在神经网络中，我们通常使用非线性函数作为激活函数，因为非线性函数是可微的，并且是单调函数。</p><p id="728d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以找出函数的微分，这有助于在有助于学习的反向传播过程中更新或改变权重。</p><h1 id="b44d" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">常用的非线性激活函数</h1><h2 id="5c13" class="lj jf hi bd jg lk ll lm jk ln lo lp jo iq lq lr js iu ls lt jw iy lu lv ka lw bi translated">1.Sigmod或逻辑激活函数</h2><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lx"><img src="../Images/79a4c2d50e6eeefb28c630e8109c8287.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*GEKktW6O4UKXB9lcavJGjQ.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图片来源:谷歌搜索</figcaption></figure><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ly"><img src="../Images/3f9f4ca60289ed66f809f8712aa1d9f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2c8FYwF0nIp-fltwTpHUbw.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图片来源:谷歌搜索</figcaption></figure><p id="bcdd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Sigmoid函数是深度学习开始时最常用的激活函数。</p><p id="3e0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在sigmoid函数中，输出在开区间(0，1)内。导数是f'(x) = f(x)(1-f(x))</p><p id="474c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">乙状结肠功能的优点:- </strong></p><ul class=""><li id="5786" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc lc ky kz la bi translated">平滑渐变，防止输出值“跳跃”。</li><li id="733a" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc lc ky kz la bi translated">Sigmoid是一个非线性函数和单调性函数，但导数不是。</li><li id="78e7" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc lc ky kz la bi translated">输出值介于0和1之间，使每个神经元的输出正常化。</li><li id="2e63" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc lc ky kz la bi translated">明确的预测，即非常接近1或0。</li></ul><p id="e9f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">乙状结肠功能的缺点:</strong></p><ul class=""><li id="70da" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc lc ky kz la bi translated"><strong class="ih hj">容易出现梯度消失:</strong>当输入稍微远离坐标原点时，函数的梯度变得很小，几乎为零。在神经网络反向传播的过程中，我们都是利用微分的链式法则来计算每个权值w的微分，当反向传播经过sigmoid函数时，这条链上的微分是很小的。而且可能会经过很多sigmoid函数，最终导致权重w对损失函数影响不大，不利于权重的优化。这个问题叫做梯度饱和或梯度分散。</li><li id="ae8a" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc lc ky kz la bi translated"><strong class="ih hj">函数输出不以零为中心:</strong>函数输出不以0为中心，会降低权重更新的效率。</li><li id="57aa" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc lc ky kz la bi translated"><strong class="ih hj">幂运算相对耗时:</strong>sigmoid函数执行指数运算，计算机计算速度较慢。</li></ul><h1 id="ac43" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">2.Tanh函数</h1><p id="5599" class="pw-post-body-paragraph if ig hi ih b ii lz ik il im ma io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">双曲正切函数公式和曲线如下</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es me"><img src="../Images/506130719c4221e08d54b87b779e5cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*fGHYXAb3BNbaRIxqYvMeXg.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图片来源:谷歌搜索</figcaption></figure><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ly"><img src="../Images/10c7962d6483ddf547d23ed8daf9fce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h9CBwL26E6Kj_GUfrsnIaQ.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图片来源:谷歌搜索</figcaption></figure><p id="249f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">双曲正切函数。双曲正切函数和sigmoid函数的曲线相对相似。</p><p id="19af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">tanh的输出区间为(-1，1)且整个函数以0为中心，优于sigmoid。导数是f'(x) = 1 — f(x)2</p><p id="edc0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在二进制分类问题中，tanh函数用于隐藏层，sigmoid函数用于输出层。但这些都不是一成不变的，具体要用的激活函数，要根据具体问题具体分析，还是要看调试。</p><p id="1eb6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">双曲正切函数的优点:- </strong></p><ul class=""><li id="4ecc" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc lc ky kz la bi translated">功能输出以零为中心</li></ul><p id="c791" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">双曲正切函数的缺点:- </strong></p><ul class=""><li id="5233" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc lc ky kz la bi translated">倾向于梯度消失</li><li id="e273" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc lc ky kz la bi translated">电源操作相对耗时</li></ul><h1 id="e7c1" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">3.ReLU函数</h1><p id="4f14" class="pw-post-body-paragraph if ig hi ih b ii lz ik il im ma io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">ReLU函数公式和曲线如下</p><p id="abd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">f(x) = max(0，x)，其中x→输入到神经元</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ly"><img src="../Images/70b5155081e58f8454251b50b2760d69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjlYjKkzgnJHmHG2AjvnNQ.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图片来源:谷歌搜索</figcaption></figure><p id="05a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ReLU(直线单位)函数取最大值。</p><p id="8479" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ReLu的输出区间为(0，+无穷大)，导数为f '(x)= { 0 for x&lt;0 , 1 for x≥0</p><p id="1fb6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">The ReLU (Rectified Linear Unit) function is an activation function that is currently more popular. Compared with the sigmoid function and the tanh function, it has the following advantages:</p><p id="e538" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">ReLu函数的优点:- </strong></p><ol class=""><li id="d9bd" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc kx ky kz la bi translated">当输入为正时，不存在梯度饱和问题。</li><li id="4498" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kx ky kz la bi translated">它不会一次激活所有的神经元。</li><li id="f118" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kx ky kz la bi translated">计算速度快了很多。ReLU函数只有线性关系。无论是向前还是向后，都比sigmoid和tanh快很多，因为Sigmoid和tanh需要计算指数，会慢一些。</li></ol><p id="1808" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">Relu功能的缺点:- </strong></p><p id="9234" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1)当输入为负数时，ReLU完全不活动，也就是说一旦输入负数，ReLU就会死掉。这样，在正向传播过程中，就不是问题了。有些区域敏感，有些区域不敏感。但是在反向传播过程中，如果你输入一个负数，梯度将完全为零，这与sigmoid函数和tanh函数有相同的问题。</p><p id="fa38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2)我们发现ReLU函数的输出不是0就是正数，说明ReLU函数不是以0为中心的函数。</p><h1 id="758f" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">4.泄漏ReLU函数</h1><p id="6d78" class="pw-post-body-paragraph if ig hi ih b ii lz ik il im ma io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">漏ReLU函数公式和曲线如下:</p><p id="cb45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">f(x)= max(alpha x，x)，其中alpha是反向传播期间的可学习参数。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ly"><img src="../Images/c02cb9328e04181cd02ab58f1eb93f2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K-_-Mkmpqj7v6fcO7FEWlw.png"/></div></div></figure><p id="634d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为什么会漏Relu？</strong></p><p id="505d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解决死的ReLU问题，人们提出将ReLU的前半部分设置为0.01x，而不是0。另一个直观的想法是基于参数的方法。</p><p id="ae93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">漏Relu如何解决死Relu问题？</p><p id="f7d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果Leaky Relu接收到正输入，它将返回x，并返回一个非常小的值，它是x的0.01倍。因此，leaky Relu也会返回负值的输出。通过这个负值的梯度，结果是非零的，因此不存在死区问题</p><h1 id="0486" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">5.ELU(指数线性单位)函数</h1><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es mf"><img src="../Images/b5b35298717d0fee441a6fe7de852083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UZaYz8tPwT0n-HOfwxlwrw.jpeg"/></div></div></figure><p id="16b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ELU还提议解决ReLU的问题。显然，ELU拥有ReLU的所有优势，而且:</p><ul class=""><li id="f222" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc lc ky kz la bi translated">没有死ReLU问题</li><li id="05c8" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc lc ky kz la bi translated">输出的平均值接近0，这意味着以零为中心</li></ul><p id="23df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个小问题是它的计算量稍微大一点。类似于Leaky ReLU，虽然理论上比ReLU好，但是目前在实践中没有很好的证据证明eLU总是比ReLU好。</p><h1 id="902d" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">6.Softmax</h1><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mg"><img src="../Images/f97638dcba0bb98a192e6e7e29983a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*v-_G9uWP537XakOprv9eYA.png"/></div></figure><p id="2d81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于任意一个长度为K的实向量，Softmax可以将其压缩成一个长度为K的实向量，取值范围为(0，1)，向量中元素的和为1。</p><p id="8f12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它在多类分类和神经网络中也有许多应用。Softmax与普通的max函数不同:max函数只输出最大值，Softmax保证较小的值有较小的概率，不会被直接丢弃。是一个“软”的“max”。</p><p id="f1f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Softmax函数的分母结合了原始输出值的所有因子，这意味着Softmax函数获得的不同概率是相互关联的。在二元分类的情况下，对于乙状结肠，有:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mh"><img src="../Images/d3ce3b71b9418115c38470a31d172ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*Oni9GPiUoI82CRub5_DyEA.png"/></div></figure><p id="d31b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于K = 2的Softmax，有:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mi"><img src="../Images/430d1f42656e5944db6c083619845c66.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*SslDPqwhqWiO1P5hLJWO5g.png"/></div></figure><p id="6fbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中可以看出，在二元分类的情况下，Softmax退化为Sigmoid。</p><h1 id="3f55" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">7.参数Relu</h1><p id="5eff" class="pw-post-body-paragraph if ig hi ih b ii lz ik il im ma io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">PReLU也是ReLU的改进版本。在负区，PReLU斜率小，也可以避免ReLU死的问题。与ELU相比，PReLU是负区域中的线性运算。虽然斜率小，但不趋向于0，这是一定的优势。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mi"><img src="../Images/3ac15c3ebc3b6a0e9d97b797dd4c06aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*oOgiLrRbZi8LL6CBNX7YQg.png"/></div></figure><p id="8bdd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们看了PReLU的公式。参数α一般是0到1之间的数，一般比较小，比如几个零。当α = 0.01时，我们称之为PReLU Leaky Relu，它是PReLU it的一个特例。</p><p id="ccae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面，yᵢ是第I个通道上的任何输入，aᵢ是负斜率，一个可学习的参数。</p><ul class=""><li id="f31c" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc lc ky kz la bi translated">如果aᵢ=0，f变成了ReLU</li><li id="485d" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc lc ky kz la bi translated">如果aᵢ&gt;0，f变成泄漏的ReLU</li><li id="5862" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc lc ky kz la bi translated">如果aᵢ是一个可学习的参数，f变成PReLU</li></ul><h1 id="d51b" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">8.Swish(自门控)功能</h1><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es mj"><img src="../Images/e209844af9f8bf60fddf37b75fec023d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lRbwLTydFY1I5A8rtl0OMg.png"/></div></div></figure><p id="cf61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">公式为:<strong class="ih hj"> y = x * sigmoid (x) </strong></p><p id="66ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Swish的设计灵感来自于LSTMs和高速公路网络中用于门控的sigmoid函数。我们使用相同的值进行选通，以简化选通机制，称为<strong class="ih hj">自选通</strong>。</p><p id="e6cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自门控的优点是它只需要一个简单的标量输入，而正常门控需要多个标量输入。这一特性使得自门控激活函数(如Swish)能够轻松替换以单个标量作为输入的激活函数(如ReLU)，而不改变隐藏容量或参数数量。</p><p id="4f37" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1)无界性(Unboundedness)有助于防止梯度在慢速训练时逐渐趋近于0，造成饱和。同时，有界也有好处，因为有界的活动函数可以有很强的正则化，更大的负输入会被解析。</p><p id="ed0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2)同时，光滑度在优化和泛化中也起着重要的作用。</p><h1 id="d7aa" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">9.最大输出</h1><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mk"><img src="../Images/c3dd6cd9bdfff15fb33c7c7c671bdcee.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*DYiHwSQQ3QOXxNnMXLj9JQ.jpeg"/></div></figure><p id="8fbf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个相对流行的选择是Maxout神经元(最近由Goodfellow等人引入)，它概括了ReLU及其泄漏版本。注意，ReLU和Leaky ReLU都是这种形式的特例(例如，对于ReLU，我们有w1，b1 =0)。因此，最大输出神经元享有ReLU单元的所有优点(线性操作方式，无饱和)并且没有其缺点</p><p id="ce14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Maxout激活是ReLU和leaky ReLU函数的推广。这是一个可学习的激活函数。</p><p id="82c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Maxout可以看做是在深度学习网络中增加了一层激活函数，其中包含了一个参数k .与ReLU，sigmoid等相比，，这一层的特殊之处在于增加了k个神经元，然后输出最大的激活值。价值。</p><h1 id="281f" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">10.Softplus</h1><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ml"><img src="../Images/7e31e4a96796a8666e091f44eec8f45d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*enmbqoxIDqtM6kODX7Sngg.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图片来源:<a class="ae jd" href="https://himanshuxd.medium.com/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e" rel="noopener">https://himanshuxd . medium . com/activation-functions-sigmoid-relu-leaky-relu-and-soft max-basics-for-neural-networks-and-deep-8d 9 c 70 eed 91 e</a></figcaption></figure><p id="51b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">软毛绒功能和ReLU功能差不多，但是比较流畅。是像ReLU一样的单方面打压。接受范围广(0，+ inf)。</p><p id="0fd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Softplus函数:<strong class="ih hj"> f(x) = ln(1+exp x) </strong></p><blockquote class="mm mn mo"><p id="8fbd" class="if ig mp ih b ii ij ik il im in io ip mq ir is it mr iv iw ix ms iz ja jb jc hb bi translated">Tensorflow Keras提供了不同类型的激活功能，实现在<a class="ae jd" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations?version=nightly" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> Tensorflow官方文档</strong> </a>中。我强烈建议你尝试实现所有的激活函数，以获得我上面讨论的关于激活函数的实用知识。</p></blockquote><p id="2e40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们学习了大部分非线性激活函数，但问题是在处理真实场景时使用哪个激活函数？那是乙状结肠，tanh，Relu，还是其他…</p><p id="c030" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">答案是，选择激活函数是一件主观的事情，这意味着取决于我们正在处理的问题陈述，以及我们想要的输出。在大多数情况下，人们选择在隐藏层选择ReLu作为激活函数，在输出层选择softmax激活函数。</p><p id="fff1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好地了解在特定场景中使用哪种激活功能，请参考本文。</p><p id="6db7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考:</p><ol class=""><li id="f43c" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc kx ky kz la bi translated"><a class="ae jd" rel="noopener" href="/analytics-vidhya/comprehensive-synthesis-of-the-main-activation-functions-pros-and-cons-dab105fe4b3b">https://medium . com/analytics-vid hya/comprehensive-synthesis-of-the-main-activation-functions-pros-cons-dab 105 Fe 4 b 3 b</a></li><li id="0c45" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kx ky kz la bi translated">【https://www.youtube.com/watch?v=y_baQlA8WwA T4】</li><li id="3fb9" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kx ky kz la bi translated"><a class="ae jd" href="https://www.appliedaicourse.com/course/11/Applied-Machine-learning-course" rel="noopener ugc nofollow" target="_blank">https://www . Applied ai course . com/course/11/Applied-Machine-learning-course</a></li><li id="8cf0" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kx ky kz la bi translated"><a class="ae jd" href="https://www.linkedin.com/pulse/activation-functions-deep-learning-sunil-kumar-cheruku/?trk=read_related_article-card_title" rel="noopener ugc nofollow" target="_blank">https://www . LinkedIn . com/pulse/activation-functions-deep-learning-sunil-Kumar-cheru ku/？trk = read _ related _ article-card _ title</a></li></ol></div></div>    
</body>
</html>