<html>
<head>
<title>How to Interpret the Logistic Regression model — with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用Python解释逻辑回归模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-interpret-the-logistic-regression-model-with-python-2bacfb50e223?source=collection_archive---------0-----------------------#2021-06-09">https://medium.com/analytics-vidhya/how-to-interpret-the-logistic-regression-model-with-python-2bacfb50e223?source=collection_archive---------0-----------------------#2021-06-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7dbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归模型是数据科学中一种有效而普遍的分类方法。</p><p id="a0de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated">任何商业问题都需要自动化决策。例如，给定客户的流失可能性有多大？给定客户点击广告的可能性有多大？如此多的相似之处。这些被归类为<em class="jm">分类</em>问题，它本身是一个叫做<em class="jm">监督学习</em>的更大主题的一部分。大多数分类问题的结果只有两个不同的值。这类分类问题被称为二元分类。二元结果的一些例子是网络钓鱼/非网络钓鱼、点击/不点击、流失/不流失。即使在两个以上的结果的情况下，问题也可以通过条件概率转化为一系列的二元问题。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es jn"><img src="../Images/b68a795b20ba09b5dabf5cf26c6f0cf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nEgq1LGhyrhgtFOUa0yFJQ.jpeg"/></div></div></figure><h1 id="a29e" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">逻辑回归</strong></h1><p id="9a57" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">逻辑回归有点用词不当。因为它的名字包含了<em class="jm">回归</em>，它实际上并不处理回归问题。逻辑回归是最有效的分类方法之一。由于与线性回归的高度相似性，它很容易解释，因此是数据探索(剖析)和预测的最佳候选之一。虽然线性回归和逻辑回归之间有一些相似之处，但也有一些不同之处。</p><h2 id="e3fb" class="lc ka hi bd kb ld le lf kf lg lh li kj iq lj lk kn iu ll lm kr iy ln lo kv lp bi translated">线性回归和逻辑回归的相似性</h2><p id="14f0" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">逻辑回归和线性回归之间的最大相似之处在于，两者都试图线性逼近特定的函数。在线性回归中，我们估计反应/目标结果的真实值，而在逻辑回归中，我们通过预测因子的线性函数来近似比值比。优势比是成功和失败的概率之比。在分类中，在二元分类问题中，大多数成功被标记为“1”(感兴趣的情况)，而失败被标记为“0”。</p><p id="dd68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归和线性回归之间最明显的区别是目标函数和数据背后的假设。在线性回归中，得出权重的目标函数是误差的平方和，并且假设目标响应的条件分布是正态分布。相反，在逻辑回归中，偏差函数用于权重推导。它还假设基础条件分布是<em class="jm">二项式</em>。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es lq"><img src="../Images/29d5db30d20b4dbf749cdd6471d7af2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G_AQ-zrajQ3V_OjBBk1nkg.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">线性回归与逻辑回归</figcaption></figure><p id="bc61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，两种模型的残差的性质和分析是不同的。逻辑回归中的部分残差虽然没有回归中的部分残差有价值，但仍然有助于确认非线性行为和识别高度有影响的记录。</p><h2 id="9997" class="lc ka hi bd kb ld le lf kf lg lh li kj iq lj lk kn iu ll lm kr iy ln lo kv lp bi translated">逻辑回归背后的数学</h2><p id="d4d0" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">对于0-1响应，我们需要建模</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es lv"><img src="../Images/9ae3bae36ec081854fb5f3ea110028d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UWConghISj7A_FMEFSuSug.png"/></div></div></figure><p id="7dc0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">换句话说，响应变量被建模为伯努利变量，其参数取决于协变量X_i。一个天真的想法是通过线性函数来近似上述概率。然而，有一个警告。模型函数不保证概率在0和1之间。为了解决这个问题，我们需要通过线性回归来转换概率和近似结果。为此，采用的变换是<em class="jm"> logit </em>变换。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es lw"><img src="../Images/88cf32daa9779bc77161382f054ac821.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*h95tqe-ewaylQ5hhZR_i9A.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">Logit函数</figcaption></figure><blockquote class="lx"><p id="4a41" class="ly lz hi bd ma mb mc md me mf mg jc dx translated">采用logit变换的基本原理是，它将大范围的值映射到有界的0和1。</p></blockquote><p id="2caf" class="pw-post-body-paragraph if ig hi ih b ii mh ik il im mi io ip iq mj is it iu mk iw ix iy ml ja jb jc hb bi translated">logit被解释为响应Y=1的“对数概率”。logit函数如下图所示。对于0.2和0.8范围内的概率，拟合值接近线性回归的值。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es mm"><img src="../Images/6e5172f69e347adc6e851fb69e8319c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QgxVMZvhiUjfAf9U2zwQxg.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">Logit变换和线性回归</figcaption></figure><p id="63a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图中的黑点反映了映射到1和0的真实响应值。</p><p id="af3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归建模为</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es mn"><img src="../Images/cea818537f76e018861379ef5dc42944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CJa_grF-l1Hy_a42w2lNvQ.png"/></div></div></figure><p id="5bd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以使用任何形式的广义线性模型(GLM)来逼近罗吉斯比。逻辑回归是<em class="jm"> GLM </em>的一个特例，被开发用于将线性回归扩展到其他设置。</p><p id="e896" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">拟合模型的优化方法是基于前面提到的偏差，与线性方法相反，它没有封闭形式。许多迭代算法可以用来推导逻辑回归参数的最大似然解。不幸的是，深入研究这些算法超出了本文的范围。</p><h1 id="46e5" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">Python实现</h1><p id="c57d" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">为了展示逻辑回归的实用性，我们的目标是使用<em class="jm"> Sci-kit Learn实现逻辑回归。</em>我们采用Titanic数据集进行逻辑回归。此外，还增加了4栏，从姓名栏重新设计为标题<em class="jm"> 1到标题</em> 4，表示男性&amp;女性，取决于他们是否结婚。(先生、夫人、主人、小姐)。另外一项分析是看已婚人士或者说有社会责任感的人是否有更多的生存本能&amp;这种趋势对男女来说是相似的。完整的数据集可以从<a class="ae mo" href="https://www.kaggle.com/azeembootwala/titanic" rel="noopener ugc nofollow" target="_blank">这里</a>下载。该数据集由15个预测因子组成，如性别、票价、p_class、家庭规模、…。目标反应是<em class="jm">幸存。</em>请注意，取值有限的因子变量已经通过一键编码进行了转换。为了避免因子变量的一键编码器的多重共线性效应，我们为每组因子变量省略了因子变量的一个级别。来自<code class="du mp mq mr ms b">sklearn.linaer_model</code>的<code class="du mp mq mr ms b">LogisticRegression</code>的预测值将提供逻辑回归核心实现。实现逻辑回归(<a class="ae mo" href="https://www.kaggle.com/vahidnaghshin/logisticregtitanic" rel="noopener ugc nofollow" target="_blank">全码</a>)的代码如下:</p><pre class="jo jp jq jr fd mt ms mu mv aw mw bi"><span id="51a9" class="lc ka hi ms b fi mx my l mz na">from sklearn.linear_model import LogisticRegression</span><span id="e307" class="lc ka hi ms b fi nb my l mz na">predictors = ['Sex', 'Age', 'Fare', 'Pclass_1',<br/> 'Pclass_2', 'Family_size', 'Title_1', 'Title_2', 'Title_3', 'Emb_1', 'Emb_2']</span><span id="7846" class="lc ka hi ms b fi nb my l mz na">outcome = 'Survived'<br/>X = train_df[predictors]<br/>y = train_df[outcome]</span><span id="7561" class="lc ka hi ms b fi nb my l mz na">logit_reg = LogisticRegression(penalty='l2', C=1e42, solver='liblinear')<br/>logit_reg.fit(X, y)</span><span id="8453" class="lc ka hi ms b fi nb my l mz na">print('intercept ', logit_reg.intercept_[0])<br/>print('classes', logit_reg.classes_)<br/>pd.DataFrame({'coeff': logit_reg.coef_[0]}, <br/>             index=X.columns)</span></pre><p id="ff55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出将是:</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es nc"><img src="../Images/106306d36a1869f31d19a959e0abad2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*uk1hI8TCZd-ZCGKF0mwJbA.png"/></div></figure><h2 id="acfd" class="lc ka hi bd kb ld le lf kf lg lh li kj iq lj lk kn iu ll lm kr iy ln lo kv lp bi translated">解释模型</h2><p id="9a77" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">预测值的截距和系数如上表所示。请注意，在解释系数时，应考虑参考水平。也请记住，线性方程是关于接近比值比的对数，而不是概率。如果需要基于概率的解释，应进行适当的转换。</p><p id="9d85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">举个例子，我们可以看到<em class="jm">性别</em>的系数是-3.55。因为男性的性别是1，女性的性别是0。身为男性会将性别为女性的情况下的存活优势比降低至约3%(经验值(-3.55)=0.028)！如果按概率来换算，概率差不多是溺水概率的0.03。</p><p id="e3b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于另一个因素变量，应考虑参考水平。例如，由于预测值中省略了<code class="du mp mq mr ms b">Title_4</code>，因此应该相应地解释<code class="du mp mq mr ms b">Title_1</code>系数。对于我们的情况，这个值是0.48，这意味着作为<code class="du mp mq mr ms b">Title_1 = Mr</code>(请参考Kaggle页的数据解释)，幸存的几率比在<code class="du mp mq mr ms b">Title_4=Miss.</code>可能是已婚男性处于高优先拯救的情况下增加了大约60%(exp(0.48)= 1.6)；)</p><p id="9963" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用来自<code class="du mp mq mr ms b">sklearn.metrics</code>的<code class="du mp mq mr ms b">confusion_matrix</code>和<code class="du mp mq mr ms b">precision_recall_fscore_support </code>，我们可以获得混淆矩阵。</p><pre class="jo jp jq jr fd mt ms mu mv aw mw bi"><span id="99f5" class="lc ka hi ms b fi mx my l mz na">from sklearn.metrics import confusion_matrix, precision_recall_fscore_support<br/>pred = logit_reg.predict(X)<br/>conf_mat = confusion_matrix(y, logit_reg.predict(X))<br/>print('Precision', conf_mat[0, 0] / sum(conf_mat[:, 0]))<br/>print('Recall', conf_mat[0, 0] / sum(conf_mat[0, :]))<br/>print('Specificity', conf_mat[1, 1] / sum(conf_mat[1, :]))<br/>precision_recall_fscore_support(list(y.values), list(logit_reg.predict(X)))</span></pre><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es nd"><img src="../Images/c150f04086dc6c67827b61257d9e1dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*GwH0ysrD3FCSFTiV82E6-w.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">混淆矩阵</figcaption></figure><p id="2651" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以看到在召回率和特异性之间有一个权衡。捕获更多的1通常意味着将更多的0误归类为1。理想的分类器会很好地对1进行分类，而不会将更多的0误分类为1。捕捉这种权衡的指标是“接收器工作特性”曲线，通常称为<em class="jm"> ROC曲线</em>。ROC曲线在y轴上绘制了召回率(灵敏度),在x轴上绘制了特异性。4 ROC曲线显示了当您改变临界值以确定如何对记录进行分类时召回率和特异性之间的权衡[1]。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es ne"><img src="../Images/834991ca304b9fabfc4e63ac2b9c1bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3eT06wgNaxN3Xgab918WA.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">皇家对空观察队</figcaption></figure><h1 id="6f06" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">总结</h1><p id="4700" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">在本文中，我们简要介绍了逻辑回归分类器，并分享了逻辑回归和线性回归之间的相似性和差异。我们还使用Python语言演示了分类器。我们还解释了基于系数的模型，并得出模型评估。</p><h1 id="4279" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">参考</h1><p id="4a0c" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">[1]布鲁斯、彼得、安德鲁·布鲁斯和彼得·格德克。<em class="jm">数据科学家实用统计学:使用R和Python的50多个基本概念</em>。奥莱利媒体，2020。</p></div></div>    
</body>
</html>