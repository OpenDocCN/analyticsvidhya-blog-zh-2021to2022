<html>
<head>
<title>Mining Fanfics on AO3 — Part 3: English &amp; Chinese Text Analysis with Decision Tree &amp; Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AO3粉丝挖掘第三部分:基于决策树和聚类的英汉文本分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mining-fanfics-on-ao3-part-3-english-chinese-text-analysis-with-decision-tree-clustering-6dcaa0e8a7a6?source=collection_archive---------17-----------------------#2021-03-08">https://medium.com/analytics-vidhya/mining-fanfics-on-ao3-part-3-english-chinese-text-analysis-with-decision-tree-clustering-6dcaa0e8a7a6?source=collection_archive---------17-----------------------#2021-03-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="7d65" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，让我们回顾一下迄今为止从AO3收集的数据:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="3344" class="jl jm hh jh b fi jn jo l jp jq">Title                     object<br/>Author                    object<br/>ID                         int64<br/>Date_updated      datetime64[ns]<br/>Rating                    object<br/>Pairing                   object<br/>Warning                   object<br/>Complete                  object<br/>Language                  object<br/>Word_count                 int64<br/>Num_chapters               int64<br/>Num_comments               int64<br/>Num_kudos                  int64<br/>Num_bookmarks              int64<br/>Num_hits                   int64<br/>Tags                      object<br/>Summary                   object<br/>Date_published    datetime64[ns]<br/>Content                   object<br/>Comments                  object</span></pre><p id="a1e0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">(数据收集过程见<a class="ae jr" href="https://moinmoin150.medium.com/mining-fanfics-on-ao3-part-1-data-collection-eac8b5d7a7fa" rel="noopener">第1部分</a>)</p><p id="0d6d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在第2部分中，我们将重点放在了更结构化的数据上，忽略了标签、摘要、内容(实际上是虚构的文本)和注释。这些将是这篇文章的重点。</p><p id="237d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">除了简单的词频统计，我首先探索了将决策树(DT)应用于标签的可能性，这样我们就可以将小说分为高流行和低流行两类。然后，我尝试了带摘要的k-means聚类。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="8fa4" class="jz jm hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">决策图表</h1><p id="1f1c" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">这本由<a class="lb lc ge" href="https://medium.com/u/d7e4530589?source=post_page-----6dcaa0e8a7a6--------------------------------" rel="noopener" target="_blank"> Gustavo Hideo </a>撰写的<a class="ae jr" href="https://towardsdatascience.com/decision-tree-build-prune-and-visualize-it-using-python-12ceee9af752" rel="noopener" target="_blank"> DT实施指南</a>给了我很大的帮助。理解sk learn DT分类器中的各种可用参数很有帮助。有一次，我不得不用Python从头开始构建DT模型，但它的灵活性和细微差别远远不及sklearn的“罐装”模型所能提供的所有修剪选项。简直是天赐良机。</p><p id="77d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是我们在这一部分需要的东西:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="0f59" class="jl jm hh jh b fi jn jo l jp jq">from sklearn import tree<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import confusion_matrix<br/>from sklearn.tree import export_graphviz<br/>import pydotplus<br/>import matplotlib.pyplot as plt<br/>import matplotlib.image as pltimg<br/>from sklearn.metrics import accuracy_score<br/>import pandas as pd<br/>import numpy as np</span></pre><h2 id="ca08" class="jl jm hh bd ka ld le lf ke lg lh li ki ip lj lk km it ll lm kq ix ln lo ku lp bi translated">数据操作</h2><p id="37e6" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">为了应用DT，我们需要为手头的文本数据创建虚拟变量。此外，标签以逗号分隔的字符串格式存储，例如</p><blockquote class="lq lr ls"><p id="b102" class="ie if lt ig b ih ii ij ik il im in io lu iq ir is lv iu iv iw lw iy iz ja jb ha bi translated">G <!-- -->对暴力、主要人物死亡、另一个宇宙、POV第一人称、交叉的图形描述……</p></blockquote><p id="38b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，在应用pandas的get_dummies()函数之前，它们需要一些额外的处理来将它们转换成列表:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="e240" class="jl jm hh jh b fi jn jo l jp jq">df.Tags = df.Tags.apply(lambda x: x.split(','))</span><span id="4313" class="jl jm hh jh b fi lx jo l jp jq"># first option<br/>t = pd.get_dummies(pd.DataFrame(df.Tags.values.tolist()), prefix_sep='', prefix='')</span></pre><p id="53d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第一个选项没有像预期的那样工作，主要是因为它用重复值扩大了列。对于有组织的、有序的或单值的列，它应该工作得很好，但是对于不同长度的无序列表，考虑每一列(垂直对齐)，然后依次“虚拟化”它们会导致意想不到的结果。</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="bfca" class="jl jm hh jh b fi jn jo l jp jq"># second option<br/>t2 = df.Tags.astype(str).str.strip('[]').str.get_dummies(', ')</span></pre><p id="6cb7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这第二个选项使用<a class="ae jr" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.get_dummies.html" rel="noopener ugc nofollow" target="_blank"> pd。Series.str.get_dummies() </a>采用了一种更复杂的方法，但在这种情况下可以按预期工作。</p><p id="e363" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可能想数一下最受欢迎的标签。虽然有许多其他方法可以做到这一点，但由于我们手头已经有了这个虚拟表，我们可以像这样构建一个字典:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="2bce" class="jl jm hh jh b fi jn jo l jp jq">dic = {}<br/>for val, i in zip(t2.sum(axis=0), t2.sum(axis=0).index):<br/>    dic[i] = val<br/>dic</span></pre><p id="9a50" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的树的特征列表可以被预先修剪。我认为一开始去掉极其罕见的标签是有意义的，但这是否有必要，是否是最佳实践还有待讨论。</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="6ad7" class="jl jm hh jh b fi jn jo l jp jq"># I examined the number of rare tags and found the cutoff point I'm most happy with<br/>len([key for key, val in dic.items() if val &lt; 30])</span><span id="41e8" class="jl jm hh jh b fi lx jo l jp jq">t2 = t2.drop([key for key, val in dic.items() if val &lt; 30], axis=1)</span><span id="8c3c" class="jl jm hh jh b fi lx jo l jp jq"># You may also want to exclude other non-informative tags, such as ['Creator Chose Not To Use Archive Warnings','No Archive Warnings Apply', 'Don't copy to another site'], which turned out to skew the result a lot</span></pre><p id="e138" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我的数据没有任何预定义的分类标签。出于我自己的探索目的，我任意创建了一个由高于中值的工藤数定义的“成功”变量:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="992d" class="jl jm hh jh b fi jn jo l jp jq">mark = np.median(df.Num_kudos)<br/>df['Success'] = df.Num_kudos.apply(lambda x: int(x&gt;=mark))</span></pre><p id="9c7c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我将这个变量以及其他感兴趣的变量添加到我的虚拟表中:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="026e" class="jl jm hh jh b fi jn jo l jp jq">for_tree = pd.concat([df[['Success','Word_count']],t2], axis=1)</span></pre><h2 id="b449" class="jl jm hh bd ka ld le lf ke lg lh li ki ip lj lk km it ll lm kq ix ln lo ku lp bi translated">训练和评估模型</h2><p id="1f63" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">我们需要定义预测值(x)和预测值(y ),然后将它们分成训练集和测试集:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="683b" class="jl jm hh jh b fi jn jo l jp jq">X = for_tree.iloc[:,1:]<br/>y = for_tree[['Success']]<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1) #default 75% train</span></pre><p id="47ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以训练我们的模型—</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="cbcf" class="jl jm hh jh b fi jn jo l jp jq">dt = DecisionTreeClassifier(criterion="entropy", max_depth=5, min_impurity_decrease=0.003)<br/>dt.fit(X_train, y_train)</span></pre><p id="76da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们可以看到哪些特征在分类中被认为是最重要的，即在分割后最有效地使组更纯:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="55e5" class="jl jm hh jh b fi jn jo l jp jq">for i in dt.feature_importances_.argsort()[:-20:-1]: # Top 20<br/>    print(X.columns[i])</span></pre><p id="d8f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们还可以通过以下方式检查深度</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="5c59" class="jl jm hh jh b fi jn jo l jp jq">dt.get_depth() # when max_depth is set low (to improve interpretability in my case), it usually reaches the maximum, but in some cases it might not.</span></pre><p id="d3a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了评估我们的模型在分类方面的表现，混淆矩阵就派上了用场:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="8979" class="jl jm hh jh b fi jn jo l jp jq">y_pred = dt.predict(X_test)<br/>a = confusion_matrix(y_test, y_pred)<br/>np.diag(a).sum()/a.sum() # percent of accurately classified fictions</span></pre><h2 id="d362" class="jl jm hh bd ka ld le lf ke lg lh li ki ip lj lk km it ll lm kq ix ln lo ku lp bi translated">解释结果</h2><p id="1882" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">由于DT在视觉表现上有优势，我们可能希望在树形图中看到我们的最终结果，并试图解释它。</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="4029" class="jl jm hh jh b fi jn jo l jp jq">data = tree.export_graphviz(dt, out_file=None, feature_names=list(X.columns))<br/>graph = pydotplus.graph_from_dot_data(data)<br/>graph.write_png('mydecisiontree.png')</span><span id="ba2c" class="jl jm hh jh b fi lx jo l jp jq"># I encounter "ValueError: Program dot not found in path" when running this. Using "<!-- -->brew install" instead of "pip" somehow solved the issue for me.</span></pre><figure class="jc jd je jf fd lz er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es ly"><img src="../Images/7ff8699856e92e5c24baa0ba3d00b31a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K9sE8cn_phG18rh9m1AL0g.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">结果树图的一部分</figcaption></figure><p id="84c3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">按照这个图，我们只需要记住，对于标签伪变量，不包含它(0)导致左树，包含它(1)导致右树。在“value = [a，b]”中，a表示“假”(0)的数量，而b给出还有多少“真”(1)。(经验法则:左边总是更小)</p><p id="1bb0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上面的图表中我们可以看到，字数似乎比任何标签都重要，并且该树将小说分为14k字以下、14k到41k之间和41k以上。对于不同长度的小说，包含某些标签会导致完全不同的结果。例如，对于较短的小说，包括“佳能同性恋关系”似乎是一个好主意。但是这决不是规定性的，因为我们不能在这里建立因果关系。但我们确实可以从数据中观察到一般读者的偏好。</p><p id="eebf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可能想知道我是如何为DT分类器选择参数值的。为什么是熵而不是基尼？事实上，这些选择可能会严重影响模型性能。我们可以使用一个循环来收集不同值的影响。例如，min _ infinity _ decrease作为是否会发生分裂的阈值是非常有影响的。我们可以检查并想象它的效果，以选择最佳方案:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="5f8f" class="jl jm hh jh b fi jn jo l jp jq">min_decrease = []<br/><br/>for i in range(1,10):<br/>    dtree = DecisionTreeClassifier(criterion='entropy', max_depth=20, min_impurity_decrease=i/1000)<br/>    dtree.fit(X_train, y_train)<br/>    pred = dtree.predict(X_test)<br/>    acc_entropy.append(accuracy_score(y_test, pred))<br/>    <br/>    min_decrease.append(i)</span><span id="d2db" class="jl jm hh jh b fi lx jo l jp jq">d = pd.DataFrame({'acc_entropy':pd.Series(acc_entropy), 'min_decrease':pd.Series([i/1000 for i in min_decrease])})<br/>plt.plot('min_decrease','acc_entropy', data=d, label='entropy')<br/>plt.xlabel('min_decrease')<br/>plt.ylabel('accuracy')<br/>plt.legend()</span></pre><figure class="jc jd je jf fd lz er es paragraph-image"><div class="er es mk"><img src="../Images/c2decda8f5ce51b7c88933726f90ee53.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*QGmk9vwLhYsedYKPf5bzww.png"/></div></figure><p id="510e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以构建多线图表来比较基尼系数和熵与不同深度或分割阈值的关系。前面提到的文章更详细地说明了这个过程。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="85ac" class="jz jm hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">k均值聚类</h1><p id="1dc2" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">我还对小说作者在这个爱好者圈子里写的主题感兴趣，而对小说文本进行聚类将是一项要求过高的工作，可能会烧坏我可怜的笔记本电脑——<em class="lt">对我来说，对像小说这样的长文本进行聚类是否是一件标准/明智/可行的事情仍然不清楚。</em></p><p id="009a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了保持这个项目的胃口，我决定先把摘要聚集起来，希望它们能为真实的小说提供一个很好的预览。</p><p id="f92f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是所需的附加库:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="4113" class="jl jm hh jh b fi jn jo l jp jq">import nltk<br/>import re<br/>from sklearn import feature_extraction<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.metrics.pairwise import cosine_similarity<br/>from sklearn.cluster import KMeans<br/>from nltk.stem.snowball import SnowballStemmer<br/># from gensim import corpora, models, similarities &lt;-- use if LDA</span></pre><h2 id="cd6e" class="jl jm hh bd ka ld le lf ke lg lh li ki ip lj lk km it ll lm kq ix ln lo ku lp bi translated">数据操作</h2><p id="5292" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">自然语言处理和信息检索的理论我就不深究了。一个简化的过程包括标记化、词干化、删除停用词和矢量化。</p><p id="0565" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是英语的流程:</p><figure class="jc jd je jf fd lz"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="09b2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们构建了自定义的停用词表、标记器和词干分析器，以输入到TfidfVectorizer中，然后使用它依次对每个摘要进行编码。结果将是形状的稀疏矩阵—(文档数、特征数)。</p><p id="2730" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以深入研究TfidfVectorizer的<a class="ae jr" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank">文档</a>来寻找更多的杠杆。例如，Max_df=0.8表示我不会查看在超过80%的文档中出现的术语(这可能不会提供太多有用的信息)。min_df=5表示该术语需要出现在5个以上的文档中才能被认为是有价值的。</p><p id="1502" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是获取这些功能的方法:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="faea" class="jl jm hh jh b fi jn jo l jp jq">terms = tfidf_vectorizer.get_feature_names()</span></pre><p id="501b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用TF-IDF(术语频率乘以逆文档频率)而不是简单的计数，因为它考虑了术语在特定文档中的重要性及其相对于整个语料库的重要性。这个想法是，如果一个在整个语料库中罕见的术语确实出现在某个文档中，它一定对该文档有特殊的意义。因此，我们降低了出现在大多数文档中的标记的权重，同时突出了更有意义的标记。</p><p id="2c69" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于我们的tfidf_matrix中的值表示它们的相对重要性，我们甚至可以通过以下方式找到某个文档的关键字——</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="c575" class="jl jm hh jh b fi jn jo l jp jq">for i in tfidf_matrix[your document's index].toarray()[0].argsort()[::-1][:30]:<br/>    print(terms[i])</span></pre><p id="e0a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们还可以使用余弦相似度来查找最接近的文档，但我不会在这里详细介绍:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="1d82" class="jl jm hh jh b fi jn jo l jp jq">cosine_similarity(tfidf_matrix)[your document's index].argsort()[:-5:-1] # top 4 neighbors since the first would always be itself</span></pre><p id="b521" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，这里有一个关于<a class="ae jr" href="https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words" rel="noopener ugc nofollow" target="_blank">使用停用词</a>的有用说明，因为我一路上遇到了很多这样的警告:</p><blockquote class="lq lr ls"><p id="5d58" class="ie if lt ig b ih ii ij ik il im in io lu iq ir is lv iu iv iw lw iy iz ja jb ha bi translated">Y <!-- -->您还应该确保停用词表已经应用了与矢量器中使用的相同的预处理和标记化。单词<em class="hh">we have</em>被CountVectorizer的默认标记器拆分为<em class="hh"> we </em>和<em class="hh"> ve </em>，因此如果<em class="hh">we have</em>在<code class="du mn mo mp jh b">stop_words</code>中，而<em class="hh"> ve </em>不在，那么在转换后的文本中，将从<em class="hh">we have</em>中保留<em class="hh"> ve </em>。我们的矢量器将试图识别和警告某些类型的不一致。</p></blockquote></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="145a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">Processing Chinese summaries is a totally different story. While English has clearly space-delimited words as tokens, Chinese token definition can be a little complicated. More than one characters can stick together to constitute one token, such as ‘学校’ for ‘school’ — either of these characters alone wouldn’t carry the intended meaning.</p><p id="142c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它还需要一组不同的停用词、句尾和正则表达式模式匹配规则。幸运的是，我们有很多可用的好资源:</p><blockquote class="lq lr ls"><p id="0226" class="ie if lt ig b ih ii ij ik il im in io lu iq ir is lv iu iv iw lw iy iz ja jb ha bi translated">停字:<a class="ae jr" href="https://github.com/stopwords-iso/stopwords-zh" rel="noopener ugc nofollow" target="_blank">https://github.com/stopwords-iso/stopwords-zh</a></p><p id="60a0" class="ie if lt ig b ih ii ij ik il im in io lu iq ir is lv iu iv iw lw iy iz ja jb ha bi translated">unicode中的汉字:【https://github.com/tsroten/zhon/blob/develop/zhon/hanzi.py T2】</p><p id="5416" class="ie if lt ig b ih ii ij ik il im in io lu iq ir is lv iu iv iw lw iy iz ja jb ha bi translated">标记化:【https://github.com/fxsjy/jieba】T4</p><p id="f4ac" class="ie if lt ig b ih ii ij ik il im in io lu iq ir is lv iu iv iw lw iy iz ja jb ha bi translated">深度学习:<a class="ae jr" href="https://github.com/PaddlePaddle/Paddle" rel="noopener ugc nofollow" target="_blank">https://github.com/PaddlePaddle/Paddle</a></p></blockquote><p id="1eb3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要加载的其他工具包可能需要先安装:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="36e3" class="jl jm hh jh b fi jn jo l jp jq">import jieba<br/>from __future__ import unicode_literals<br/>import paddle</span></pre><p id="e90b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">中国人的流程是这样的:</p><figure class="jc jd je jf fd lz"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="b2e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于这两种语言，下面的过程是相同的。</p><h2 id="1815" class="jl jm hh bd ka ld le lf ke lg lh li ki ip lj lk km it ll lm kq ix ln lo ku lp bi translated">构建模型</h2><p id="89ca" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">为了检查最佳的集群数量，我们可以通过以下方式建立一个肘形图</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="ac65" class="jl jm hh jh b fi jn jo l jp jq">km = [KMeans(n_clusters=i) for i in range(1,11)]<br/>score = [k.fit(tfidf_matrix).inertia_ for k in km]<br/>plt.plot(range(1,11),score)</span></pre><figure class="jc jd je jf fd lz er es paragraph-image"><div class="er es mq"><img src="../Images/752044f1751d6703824d848e2b1f3b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*EhxAVoCwkv-NE5mgXhsuhA.png"/></div></figure><p id="1935" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然看起来只有3个集群是最佳的，但经过几次尝试后，我发现5个集群的解决方案是最合理的。</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="a3a5" class="jl jm hh jh b fi jn jo l jp jq">km = KMeans(n_clusters=5)<br/>km.fit(tfidf_matrix)</span><span id="c803" class="jl jm hh jh b fi lx jo l jp jq"># append cluster labels back to the dataframe<br/>clusters = km.labels_.tolist()<br/>eng_sum['Clusters'] = clusters</span><span id="928e" class="jl jm hh jh b fi lx jo l jp jq"># compare each cluster's centrality measures<br/>grouped = eng_sum.groupby('Clusters') # or chi_sum<br/>grouped.mean()</span></pre><figure class="jc jd je jf fd lz er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es mr"><img src="../Images/611f78d104919a4d356e4e0514817d8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E2luPWcpgt5XC7VgEoqjgQ.png"/></div></div></figure><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="dc28" class="jl jm hh jh b fi jn jo l jp jq"># to know how many members each cluster has<br/>grouped.size()</span></pre><h2 id="60fd" class="jl jm hh bd ka ld le lf ke lg lh li ki ip lj lk km it ll lm kq ix ln lo ku lp bi translated">解释结果</h2><p id="b95c" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">为了理解这些聚类，我们可以按照TF-IDF值以及几个有代表性的假设来查看每个聚类的关键字:</p><pre class="jc jd je jf fd jg jh ji jj aw jk bi"><span id="3173" class="jl jm hh jh b fi jn jo l jp jq">importance_by_cluster = km.cluster_centers_.argsort()[:, ::-1] </span><span id="707b" class="jl jm hh jh b fi lx jo l jp jq">for i in range(5):<br/>    print("Cluster {} words:".format(i))<br/>    <br/>    for ind in importance_by_cluster[i, :20]:<br/>        print(terms[ind]+', ', end=' ') # top 20 words<br/>        # terms are the list of features<br/>    <br/>    subset = eng_sum[eng_sum.Clusters==i].copy() # or chi_sum<br/>    subset = subset.sort_values(by='Num_hits', ascending=False)<br/>    print(subset.iloc[:10,[0,2]]) # print out most popular fictions belong to the cluster as representatives</span></pre><p id="84b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，在我探索的粉丝圈中，我发现了一个关注哈利·波特和伏地魔之间互动的集群，关键词是“哈利、世界、波特、巫师、汤姆、黑暗、时间、生命、死亡、霍格沃茨、伏地魔、魔法、riddl、活着、力量，”另一个集群显然从tumblr的写作提示中获得了灵感——“提示、提示吻、吻、符号、Tumblr、drabbl、exchang、challeng，”还有另一个情感丰富的作者群触摸着厄里斯之镜——“镜子、镜子厄里斯、厄里斯、看见、袜子、看镜子</p><p id="9b3c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">While Chinese and English topics are vastly different, interestingly, they both had one cluster dedicated to the mirror of erised. The Chinese version has somewhat similar keywords — “厄里斯 (erised), 魔镜 (mirror), 自我, 站, 恨, 邓校, 欺骗, 厌弃, 年少轻狂, 少不更事, 德姆斯特朗, 看到 (see), 撒谎 (lie), 面前 (front).” (overlapping ones translated)</p><p id="78a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以将我们从查看这些关键字中获得的对聚类的理解与来自其他数值变量的中心性度量结合起来，以得出更有见地的结论，例如什么主题会引发最多的反应，以及哪些主题会是更好的点击诱饵。</p><p id="17bc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然我另外尝试了主题建模，但它基本上证实了来自集群的见解，所以我不会在这里介绍它。</p><p id="a738" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当然还有更多的可能性可以玩。我会继续探索新的选择😊</p></div></div>    
</body>
</html>