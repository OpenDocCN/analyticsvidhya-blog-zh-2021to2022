# æœºå™¨å­¦ä¹ é¡¹ç›®çš„ä¸»è¦æ­¥éª¤

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/principal-steps-of-a-machine-learning-project-7d7dcc6fe68b?source=collection_archive---------13----------------------->

![](img/2bddb873046041fe2ce96646ac65bab5.png)

[å›¾ç‰‡æ¥æº:[https://farm 2 . static Flickr . com/1816/30212411048 _ 2a1d 7200 E2 _ b . jpg](https://farm2.staticflickr.com/1816/30212411048_2a1d7200e2_b.jpg)]

æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆä¸ä»…éœ€è¦æœ‰è‰¯å¥½çš„ç¼–ç¨‹æŠ€èƒ½ï¼Œä»–ä»¬è¿˜éœ€è¦æœ‰ä¸€äº›æ•°æ®ç§‘å­¦å®¶çš„æŠ€èƒ½ï¼Œå¦‚æ”¶é›†å’Œç®¡ç†æ•°æ®ï¼Œç»Ÿè®¡å­¦å®¶åˆ†ææ•°æ®çš„æŠ€èƒ½ï¼Œä»¥åŠæ•°å­¦å®¶çš„æŠ€èƒ½ã€‚è¿™æ˜¯å› ä¸ºä¸€ä¸ªæœºå™¨å­¦ä¹ é¡¹ç›®éœ€è¦å¾ˆå¤šæ­¥éª¤ï¼Œä»ç®¡ç†æ•°æ®ï¼Œå»ºç«‹å’Œè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œåˆ°åº”ç”¨è¿™ä¸ªæ¨¡å‹é¢„æµ‹æµ‹è¯•é›†ä¸­çš„æ–°æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢æ‰€æœ‰è¿™äº›æ­¥éª¤ï¼ŒåŒ…æ‹¬:

1.  æ”¶é›†æ•°æ®
2.  æ¢ç´¢æ€§æ•°æ®åˆ†æ
3.  é¢„å¤„ç†æ•°æ®
4.  å»ºæ¨¡å’Œè¯„ä¼°
5.  è¶…å‚æ•°è°ƒè°
6.  é¢„æµ‹ã€‚

æ‰€æœ‰è¿™äº›å·¥ä½œéƒ½å°†é€šè¿‡ä¸€ä¸ªå¯¹ King Country çš„æˆ¿ä»·è¿›è¡Œé¢„æµ‹çš„é¡¹ç›®æ¥è¯¦ç»†è¯´æ˜ã€‚è¿™ä¸ª[æ•°æ®](https://www.kaggle.com/harlfoxem/housesalesprediction)æ˜¯ä» Kaggle æ”¶é›†çš„ã€‚

# ä¸€.æ”¶é›†æ•°æ®

æ­£å¦‚æˆ‘åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://lekhuyen.medium.com/introduction-to-machine-learning-78e8c5d708e6)ä¸­ä»‹ç»çš„é‚£æ ·ï¼Œæ•°æ®å¯¹äºæ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹éå¸¸é‡è¦ã€‚å› ä¸ºæ¨¡å‹å°†ä»æ‚¨æä¾›ç»™å®ƒçš„æ•°æ®é›†ä¸­å­¦ä¹ ã€‚å› æ­¤ï¼Œæ”¶é›†å’Œç®¡ç†æ•°æ®åœ¨æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚

æ”¶é›†æ•°æ®çš„æ¥æºæœ‰å¾ˆå¤šï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¼€æ”¾çš„æ¥æºï¼Œæ‚¨å¯ä»¥ä¸‹è½½å„ç§æ•°æ®é›†ä»¥è¾“å…¥åˆ°æ‚¨çš„æ¨¡å‹ä¸­:

*   Kaggle æ˜¯ä¸€ä¸ªæ•°æ®ç§‘å­¦å®¶çš„åœ¨çº¿ç¤¾åŒºï¼Œåœ¨è¿™é‡Œä½ å¯ä»¥æ‰¾åˆ°è®¸å¤šæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ é¡¹ç›®ä»¥åŠå¼€æ”¾æ•°æ®æºã€‚é€šè¿‡åšç°å®ç”Ÿæ´»ä¸­çš„é¡¹ç›®æˆ–è€…å‚åŠ æ¯”èµ›æ¥å­¦ä¹ æœºå™¨å­¦ä¹ æ˜¯å¾ˆæœ‰ç”¨çš„ä¸€é¡µã€‚
*   [UCI æœºå™¨å­¦ä¹ çŸ¥è¯†åº“](https://archive.ics.uci.edu/ml/index.php)ä¸ºæœºå™¨å­¦ä¹ ç¤¾åŒºæä¾›äº†ä¸€ç³»åˆ—æ•°æ®åº“ã€‚å®ƒå®é™…ä¸Šç»´æŠ¤ç€ 560 ä¸ªæ•°æ®é›†ã€‚
*   [AWS ä¸Šçš„å¼€æ”¾æ•°æ®](https://aws.amazon.com/fr/opendata/?wwps-cards.sort-by=item.additionalFields.sortDate&wwps-cards.sort-order=desc)æ˜¯ä¸€ä¸ªç”¨æˆ·å¯ä»¥å…±äº«é€šè¿‡ AWS èµ„æºè·å¾—çš„æ•°æ®é›†çš„åœ°æ–¹ã€‚
*   [OpenDataSoft](https://www.opendatasoft.com/blog/2015/11/02/how-we-put-together-a-list-of-1600-open-data-portals-around-the-world-to-help-open-data-community) æ˜¯ä¸€ä¸ªåŒ…å«å…¨çƒ 2600 å¤šä¸ªå¼€æ”¾æ•°æ®é—¨æˆ·çš„æ•°æ®æºã€‚
*   â€¦

æˆ‘ä»¬çš„æ•°æ®é›†ä¿å­˜åœ¨ä¸€ä¸ªè¡¨ä¸­ã€‚csv)æ ¼å¼ã€‚å¯ä»¥é€šè¿‡**ç†ŠçŒ«**åŒ…ä¸­çš„ **read_csv** å‡½æ•°è¯»å–:

```
import pandas as pddf = pd.read_csv('kc_house_data.csv')
```

æ£€æŸ¥æ•°æ®:

```
df.head()
```

![](img/93b6bab362fcbb027f25a0d304458b4f.png)

# äºŒã€‚æ¢ç´¢æ€§æ•°æ®åˆ†æ

è¿™ä¸€æ­¥çš„ç›®æ ‡æ˜¯å°½å¯èƒ½å¤šåœ°ç†è§£æ•°æ®é›†ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿä¸ºæ¨¡å‹åŒ–æ­¥éª¤åˆ¶å®šå¿«é€Ÿç­–ç•¥ã€‚

å› ä¸ºæœºå™¨å­¦ä¹ åœ¨ç»“æ„åŒ–æ•°æ®ä¸Šå·¥ä½œå¾ˆå¤šï¼Œè¿™äº›æ•°æ®ä¿å­˜åœ¨ã€‚csv æˆ–ã€‚xlsx æ ¼å¼ï¼Œæ‰€ä»¥åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹åˆ†æè¿™ç§ç±»å‹çš„æ•°æ®ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸€äº›åŸºæœ¬åˆ†æå¼€å§‹ï¼Œå¦‚å‘ç°ç›®æ ‡å˜é‡ã€è¡Œæ•°ã€åˆ—æ•°ã€æ¯åˆ—çš„æ•°æ®ç±»å‹ï¼Œæ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦å­˜åœ¨ä»»ä½• NaN å€¼:

å¯¹äºæˆ‘ä»¬çš„æ•°æ®é›† **kc_house_data.csv:**

*   åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œç›®æ ‡ç‰¹æ€§æ˜¯ä»·æ ¼åˆ—ã€‚
*   è¯¥æ•°æ®é›†åŒ…æ‹¬ 21613 è¡Œå’Œ 21 åˆ—ã€‚

```
print(df.shape)>>> (21613, 21)
```

*   å‘ç°æ¯åˆ—çš„ç±»å‹:

```
df.dtypes
```

![](img/bbcaf0d1df47c9e0d73426b318bb7831.png)

è¿™ä¸ªç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬å‡ ä¹æ‰€æœ‰çš„ç‰¹å¾éƒ½æœ‰ä¸€ä¸ªæ•°å­—ç±»å‹(æ•´æ•°æˆ–æµ®ç‚¹)ã€‚åªæœ‰åˆ—â€œæ—¥æœŸâ€å…·æœ‰å¯¹è±¡ç±»å‹ã€‚

*   é€šè¿‡é¥¼å›¾å¯è§†åŒ–ç±»å‹æ¯”ç‡:

```
df.dtypes.value_counts().plot.pie()
```

![](img/8c4be5a7cd5c09d896df283a8347005e.png)

*   éªŒè¯æ•°æ®é›†æ˜¯å¦åŒ…å«ä»»ä½• NaN å€¼:

```
df.isna().sum()
```

![](img/0be3c2afae82e374cba5f2e2dfbf70e1.png)

è¯¥ç»“æœæ˜¾ç¤ºæ‰€æœ‰åˆ—éƒ½ä¸åŒ…å«ä»»ä½•ç¼ºå¤±å€¼ã€‚

*   ä½¿ç”¨å‡½æ•° **df.describe()** æè¿°æ•°æ®é›†çš„ç»Ÿè®¡å€¼ã€‚è¯¥å‡½æ•°å…è®¸æˆ‘ä»¬è®¡ç®—æ¯ä¸ªæ•°å­—åˆ—çš„ä¸€äº›åŸºæœ¬ç»Ÿè®¡å€¼ï¼Œä¾‹å¦‚æ•°æ®ç‚¹çš„æ•°é‡ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€å¹³å‡å€¼ã€æ ‡å‡†åå·®å€¼(std)å’Œåˆ†ä½æ•°ã€‚

```
df.describe().transpose()
```

![](img/99dff8fb04748c6dbd8bfd6e1d66695e.png)

*   ä¸ºäº†æ›´å¥½åœ°ç†è§£æ•°æ®é›†ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ç›´æ–¹å›¾æˆ–æ¡å½¢å›¾æ¥å‘ç°æ¯ä¸ªç‰¹å¾çš„åˆ†å¸ƒã€‚

ä¸‹å›¾æ˜¾ç¤ºäº†ä»·æ ¼åˆ—çš„åˆ†å¸ƒæƒ…å†µ:

```
plt.figure(figsize = (10,8))sns.distplot(df['price'],hist = True, label = 'Price')plt.show()
```

![](img/0a756eddb8dfc817825d89c915607205.png)

æ ¹æ®ä¸Šå›¾ï¼Œæˆ‘ä»¬çœ‹åˆ°

*   å‡ ä¹æˆ¿ä»·éƒ½æ˜¯ 0 åˆ° 100 ä¸‡ç¾å…ƒåˆ†å¸ƒã€‚
*   50 ä¸‡ç¾å…ƒå·¦å³çš„ä»·æ ¼å‡ºç°å¾—æœ€å¤šã€‚
*   å­˜åœ¨ä¸€äº›å¼‚å¸¸å€¼ï¼Œæˆ‘ä»¬å¯ä»¥è·³è¿‡å®ƒä»¬æ¥è¯´æ˜å®ƒä»¬å¯¹æˆ‘ä»¬çš„ ML æ¨¡å‹çš„å½±å“ã€‚

è¯¥å›¾ä¸ä»…æœ‰åŠ©äºæˆ‘ä»¬æ‰¾å‡ºæ•°æ®çš„æœ€é›†ä¸­å€¼ï¼Œè€Œä¸”å¯¹äºç¡®å®šå¼‚å¸¸å€¼ä¹Ÿå¾ˆæœ‰ç”¨

æˆ¿å­çš„ä»·æ ¼æ˜¾ç„¶å–å†³äºæ¥¼å±‚ã€å§å®¤ã€å«ç”Ÿé—´çš„æ•°é‡ã€‚å› æ­¤ï¼Œå°†è¿™äº›ç‰¹å¾å½¢è±¡åŒ–ä¹Ÿå¾ˆæœ‰è¶£:

```
import matplotlib.pyplot as plt
import seaborn as sns
```

**æ¥¼å±‚:**

```
plt.figure()sns.countplot(df['floors'])plt.show() 
```

![](img/e6419557a5ac06b245d667a2db07876e.png)

**æµ´å®¤:**

```
plt.figure(figsize = (12,5))sns.countplot(df['bathrooms'])plt.show()
```

![](img/3936829ad807cbd6ab0101cd46bfe2c7.png)

**å§å®¤:**

```
plt.figure(figsize = (7,5))sns.countplot(df['bedrooms'])plt.show()
```

![](img/13ec1cc01c1d67a35163e8c483fba754.png)

*   å†è€…ï¼Œå‘ç°å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§ä¹Ÿå¾ˆé‡è¦ã€‚ç”±äºè¿™ç§åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸æˆ‘ä»¬çš„ç›®æ ‡ç‰¹å¾æœ€ç›¸å…³çš„å˜é‡ï¼Œå¹¶å¿½ç•¥å¼±ç›¸å…³çš„å˜é‡ã€‚

```
plt.figure(figsize = (14,8))sns.heatmap(df.corr(), linewidths = 0.5, annot = True)
```

![](img/ff2325bf54e0c9b132d7cb3b313ac6a7.png)

æ‰¾å‡ºä»·æ ¼ä¸å…¶ä»–ç‰¹å¾çš„ç›¸å…³æ€§:

```
df.corr()["price"].sort_values(ascending = False)
```

![](img/9c3ec0dd4e316792802b5092530b1b8a.png)

ä»¥ä¸Šç»“æœè¡¨æ˜ï¼Œä»·æ ¼ä¸ä¸€äº›å˜é‡é«˜åº¦ç›¸å…³ï¼Œå¦‚ **sqft_living** ã€**å“ä½**ã€ **sqft_above** ã€ **sqft_living15** ã€**æµ´å®¤**ã€‚ **Id** å’Œ**é‚®æ”¿ç¼–ç **ä¸ä»·æ ¼çš„ç›¸å…³æ€§å¾ˆå¼±ã€‚

# ä¸‰ã€‚é¢„å¤„ç†æ•°æ®

å¦‚ä½ æ‰€çŸ¥ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•ä»ä½ æä¾›ç»™å®ƒçš„æ•°æ®ä¸­å­¦ä¹ ã€‚å¦‚æœæ•°æ®é›†ä¸å¥½(å­˜åœ¨ç¼ºå¤±å€¼ã€ç¦»ç¾¤å€¼æˆ–ç‰¹å¾æ²¡æœ‰ä»¥æ­£ç¡®çš„æ ¼å¼å‘ˆç°)ï¼Œé‚£ä¹ˆä»è¯¥æ•°æ®æ„å»ºçš„æœºå™¨å­¦ä¹ æ¨¡å‹å°†éå¸¸ç³Ÿç³•ã€‚å› æ­¤ï¼Œåœ¨å°†æ•°æ®è¾“å…¥æ¨¡å‹ä¹‹å‰å‡†å¤‡æ•°æ®éå¸¸é‡è¦ï¼Œè¿™éœ€è¦æ•°æ®ç§‘å­¦å®¶å¤§çº¦ 80%çš„å·¥ä½œæ—¶é—´ã€‚

åœ¨é¢„å¤„ç†æŠ€æœ¯ä¸­ï¼Œé€šå¸¸ä½¿ç”¨çš„æ–¹æ³•æœ‰ç¼–ç ã€å½’ä¸€åŒ–ã€æ’è¡¥ã€å¼‚å¸¸å€¼å’Œå¼‚å¸¸å€¼å‰”é™¤ã€å˜é‡é€‰æ‹©ã€å˜é‡æå–ã€‚åœ¨åº”ç”¨è¿™äº›æŠ€æœ¯å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹å®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

## 1.ç¼–ç 

**ç¼–ç **æ˜¯ä¸€ç§åœ¨ä½¿ç”¨åˆ†ç±»æ•°æ®æ‹Ÿåˆæ¨¡å‹ä¹‹å‰å°†åˆ†ç±»æ•°æ®ç¼–ç æˆæ•°å­—çš„æ–¹æ³•ã€‚æœ€æµè¡Œçš„ä¸¤ç§æŠ€æœ¯æ˜¯ ***é¡ºåºç¼–ç *** å’Œ ***ä¸€é”®ç¼–ç *** ã€‚

åœ¨**åºæ•°ç¼–ç **ä¸­ï¼Œæ¯ä¸ªåˆ†ç±»å€¼ç”±ä¸€ä¸ªæ•´æ•°å€¼ç¼–ç ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªåˆ†ç±»å€¼[â€œç‹—â€ã€â€œçŒ«â€ã€â€œé¸Ÿ]]ï¼Œåºæ•°ç¼–ç ä¼šå°†è¿™ä¸¤ä¸ªå€¼è½¬æ¢ä¸ºä¸‰ä¸ªæ•´æ•° 0ã€1 å’Œ 2ï¼Œæ ¹æ®å®ƒä»¬åœ¨å­—å…¸ä¸­å‡ºç°çš„é¡ºåºã€‚å› æ­¤ï¼Œâ€œé¸Ÿâ€è¢«æŒ‡å®šä¸º 0ï¼Œâ€œçŒ«â€è¢«æŒ‡å®šä¸º 1ï¼Œâ€œç‹—â€è¢«æŒ‡å®šä¸º 2ã€‚åœ¨ Python ä¸­ï¼Œè¿™æ˜¯ç”±æ¨¡å— **sklearn ä¸­çš„ **OrdinalEncoder** å®Œæˆçš„**

```
from sklearn.preprocessing import OrdinalEncoderfrom numpy import asarrayexample = asarray([['dog'], ['cat'],['bird']])encoder = OrdinalEncoder()encode_example = encoder.fit_transform(example)print(encode_example)>>> [[2.]  
     [1.]  
     [0.]]
```

æˆ‘ä»¬è¿˜å¯ä»¥åå‘è½¬æ¢ä»¥æ‰¾åˆ°ç¼–ç å€¼çš„åŸå§‹åˆ†ç±»å€¼:

```
import numpy as npencoder.inverse_transform(np.array([[0],[2],[2]]))>>> array([['bird'], ['dog'], ['dog']], dtype='<U4')
```

åœ¨å˜é‡ä¹‹é—´ä¸å­˜åœ¨é¡ºåºå…³ç³»çš„æƒ…å†µä¸‹ï¼Œæ•´æ•°ç¼–ç å¯èƒ½ä¸åˆé€‚ã€‚å®ƒå¯ä»¥è¢«ç‹¬çƒ­ç¼–ç æŠ€æœ¯æ‰€å–ä»£ã€‚è¯¥æŠ€æœ¯æ—¨åœ¨å°†æ¯ä¸ªç›®æ ‡è½¬æ¢ä¸ºé•¿åº¦ç­‰äºç±»åˆ«æ•°é‡çš„å‘é‡ã€‚å¦‚æœä¸€ä¸ªæ•°æ®ç‚¹å±äº iáµ—Ê°-categoryï¼Œé‚£ä¹ˆè¿™ä¸ªå‘é‡ä¸­çš„ iáµ—Ê°åˆ†é‡è¢«èµ‹å€¼ä¸º 1ï¼Œå…¶ä»–çš„è¢«èµ‹å€¼ä¸º 0ã€‚

```
from sklearn.preprocessing import OneHotEncoderfrom numpy import asarrayexample = asarray([['dog'], ['cat'],['bird']])encoder = OneHotEncoder(sparse = False)encode_example = encoder.fit_transform(example)print(encode_example)>>> [[0\. 0\. 1.]  
     [0\. 1\. 0.]  
     [1\. 0\. 0.]]
```

åœ¨æœ¬ä¾‹ä¸­ï¼Œâ€œç‹—â€è¢«åˆ†é…ç»™ vector [0ï¼Œ0ï¼Œ1]ï¼Œâ€œçŒ«â€è¢«åˆ†é…ç»™[0ï¼Œ1ï¼Œ0]ï¼Œâ€œé¸Ÿâ€è¢«åˆ†é…ç»™[1ï¼Œ0ï¼Œ0]ã€‚

**2ã€‚æ­£å¸¸åŒ–**

æœ‰æ—¶ï¼ŒåŸå§‹æ•°æ®ä¸­æ•°å­—ç‰¹å¾çš„èŒƒå›´å¯èƒ½å˜åŒ–å¾ˆå¤§ã€‚å› æ­¤ï¼Œåœ¨å°†æ•°æ®æ‹Ÿåˆåˆ°æˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹å‰ï¼Œæœ‰å¿…è¦å¯¹å®ƒä»¬è¿›è¡Œå½’ä¸€åŒ–ã€‚è§„èŒƒåŒ–çš„ç›®æ ‡æ˜¯å°†æ•°å€¼åˆ—çš„å€¼æ›´æ”¹ä¸ºä¸€ä¸ªé€šç”¨çš„èŒƒå›´ï¼Œè€Œä¸ä¼šä¸¢å¤±å€¼èŒƒå›´çš„å·®å¼‚ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹ **sklearn** ä¸­æœ€æµè¡Œçš„æ ‡å‡†åŒ–åŠŸèƒ½çš„æŠ€æœ¯:

*   **æœ€å°-æœ€å¤§å½’ä¸€åŒ–:**

æ­¤æ–¹æ³•å°†è¦ç´ çš„èŒƒå›´é‡æ–°è°ƒæ•´ä¸º[0ï¼Œ1]ä¸­çš„æ–°èŒƒå›´ã€‚é€šå¼ç”±ä¸‹å¼ç»™å‡º:

![](img/690b25a27af9c8a025c83151dd8a86a0.png)

å…¶ä¸­ X æ˜¯åŸå§‹å€¼ï¼ŒX 'æ˜¯æ ‡å‡†åŒ–å€¼ã€‚

åœ¨ Python ä¸­ï¼Œè¿™ä¸ªå…¬å¼æ˜¯ç”±**çš„æœ€å°æœ€å¤§ç¼©æ”¾å™¨**ä»æ¨¡å— **sklearn ä¸­è®¡ç®—å‡ºæ¥çš„**

**ç¤ºä¾‹**:å‘é‡(1ï¼Œ2ï¼Œ3)è¢«é‡æ–°ç¼©æ”¾ä¸º(0ï¼Œ0.5ï¼Œ1)

```
from sklearn.preprocessing import MinMaxScalerimport numpy as npX = np.array([[1],[2],[3]])scaler = MinMaxScaler()scaler.fit_transform(X)
>>> array([[0\. ], [0.5], [1\. ]])
```

*   **æ ‡å‡†åŒ–(Z åˆ†æ•°æ ‡å‡†åŒ–):**

è¯¥æ–¹æ³•æ—¨åœ¨ä»¥é›¶å‡å€¼å’Œæ ‡å‡†å·®ä¸º 1 åœ¨æ–°èŒƒå›´å†…é‡æ–°ç¼©æ”¾è¦ç´ ã€‚è¯¥æŠ€æœ¯çš„å…¬å¼ç”±ä¸‹å¼ç»™å‡º:

![](img/a53984975b2467a9b3343c83e2846584.png)

å…¶ä¸­ mean(X)å’ŒÏƒåˆ†åˆ«è¡¨ç¤º X çš„å¹³å‡å€¼å’Œæ ‡å‡†åå·®ã€‚

åŠŸèƒ½**æ ‡å‡†ç¼©æ”¾å™¨**å±äº **sklearn .é¢„å¤„ç†**æ¨¡å—**ã€‚**

**ä¾‹:**ç”¨è¿™ç§æ–¹æ³•å¾—åˆ°çš„å‘é‡(1ï¼Œ2ï¼Œ3)çš„æ–°å°æ•°ä½æ•°æ˜¯(-1.22474487ï¼Œ0ï¼Œ1.22474487)

```
from sklearn.preprocessing import StandardScalerimport numpy as npX = np.array([[1],[2],[3]])scaler = StandardScaler()scaler.fit_transform(X)>>> array([[-1.22474487],
          [ 0\.        ],
          [ 1.22474487]])
```

*   **é²æ£’å®šæ ‡å™¨:**

å½“æ•°æ®é›†åŒ…å«å¼‚å¸¸å€¼æ—¶ï¼Œä¸Šè¿°ä¸¤ç§æ–¹æ³•å¹¶ä¸çœŸæ­£é€‚ç”¨ã€‚è¿™ä¸ªç¼ºç‚¹å¯ä»¥é€šè¿‡ç¨³å¥çš„æ ‡åº¦æ³•æ¥å…‹æœï¼Œå…¶ä¸­è€ƒè™‘äº†ä¸­ä½æ•°å’Œå››åˆ†ä½é—´è·ã€‚è¯¥æŠ€æœ¯çš„å½’ä¸€åŒ–å…¬å¼ç”±ä¸‹å¼ç»™å‡º:

![](img/a41c106f0a4dd12e5792e8f96676e881.png)

å…¶ä¸­*ä¸­ä½æ•°* (X)å’Œ *IQR* è¡¨ç¤ºæ•°æ® X çš„ä¸­ä½æ•°å’Œå››åˆ†ä½æ•°èŒƒå›´

**3ã€‚å—çš„æ‹’ç»**

å½“æ•°æ®é›†åŒ…å«ç¼ºå¤±å€¼æ—¶ï¼Œéœ€è¦åœ¨å°†è¿™äº›æ•°æ®æ‹Ÿåˆåˆ°æ¨¡å‹ä¸­ä¹‹å‰æ‹’ç»æˆ–æ›¿æ¢å®ƒä»¬ã€‚Pandas æä¾›äº†ä¸€äº›æœ‰ç”¨çš„åŠŸèƒ½æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚

*   ç†ŠçŒ«ã€‚DataFrame.isna() ç¡®å®šæ•°æ®å¸§ä¸­æ˜¯å¦å­˜åœ¨ä»»ä½•ç¼ºå¤±å€¼
*   ç†ŠçŒ«ã€‚DataFrame.fillna(Î±) ç”¨ç»™å®šå€¼Î±æ›¿æ¢ DataFrame ä¸­ç¼ºå¤±çš„å€¼
*   ç†ŠçŒ«ã€‚DataFrame.dropna() åˆ é™¤ DataFrame ä¸­æ‰€æœ‰ç¼ºå¤±çš„å€¼

æ‚¨å¯ä»¥æŸ¥çœ‹[å‚è€ƒ](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)äº†è§£æ›´å¤šè¯¦æƒ…ã€‚

**4ã€‚æ’è¡¥**

æœ‰æ—¶ï¼Œåˆ é™¤ä¸¢å¤±çš„å€¼å¯èƒ½ä¼šä¸¢å¤±æœ‰ä»·å€¼çš„æ•°æ®ã€‚æ›´å¥½çš„ç­–ç•¥æ˜¯é€šè¿‡ä¸€äº›ç»Ÿè®¡å€¼æ¥ä¼°ç®—è¿™äº›ç¼ºå¤±å€¼ï¼Œå¦‚å¹³å‡å€¼ã€ä¸­å€¼ã€æœ€é¢‘ç¹å€¼æˆ–ä¸€äº›å¸¸æ•°â€¦â€¦è¿™å¯ä»¥é€šè¿‡æ¥è‡ª **sklearn.impute** æ¨¡å—çš„ [**SimpleImputer**](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) å‡½æ•°æ¥å¸®åŠ©å®ç°ã€‚

**5ã€‚å˜é‡é€‰æ‹©:**

é€‰æ‹©æœ€ç›¸å…³çš„å˜é‡å¯¹äºæ„å»ºæœºå™¨æ¨¡å‹éå¸¸é‡è¦ã€‚è¿™æ˜¯å‡ºäºæŸäº›åŸå› è€Œä½¿ç”¨çš„:

*   ä¸ºäº†ç®€åŒ–æ¨¡å‹ï¼Œä½¿å…¶æ›´å®¹æ˜“è§£é‡Š
*   ä¸ºäº†å‡å°‘è®­ç»ƒæ—¶é—´
*   ä¸ºäº†å‡å°‘è¿‡åº¦æ‹Ÿåˆ

æœ‰ä¸€äº›æµè¡Œçš„é€‰æ‹©å˜é‡çš„æŠ€æœ¯ï¼Œå¦‚å¡æ–¹æ£€éªŒï¼Œä¸ªäººç›¸å…³æ€§é€‰æ‹©ï¼Œå¥—ç´¢ï¼Œé€’å½’ç‰¹å¾æ¶ˆé™¤ï¼Œâ€¦

**6ã€‚å˜é‡æå–**

æœ‰æ—¶ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†ç”±æ–‡æœ¬å’Œå›¾åƒç­‰éç»“æ„åŒ–æ•°æ®ç»„æˆã€‚æœ‰å¿…è¦å°†è¿™äº›æ•°æ®ä¸­çš„ç‰¹å¾æå–ä¸ºæœºå™¨å­¦ä¹ ç®—æ³•æ”¯æŒçš„æ ¼å¼ã€‚[sk learn . feature _ extraction](https://scikit-learn.org/stable/modules/feature_extraction.html)æ˜¯å¤„ç†è¿™ä¸ªé—®é¢˜çš„æœ‰ç”¨æ¨¡å—ã€‚

**7ã€‚å°†æ•°æ®åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†**

å°†æ•°æ®é›†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ˜¯æ•°æ®é¢„å¤„ç†çš„é‡è¦éƒ¨åˆ†ã€‚è®­ç»ƒé›†ç”¨äºå¤„ç†æ¨¡å‹ï¼Œæµ‹è¯•é›†ç”¨äºæµ‹è¯•æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œè®­ç»ƒé›†åº”è¯¥è¶³å¤Ÿå¤§ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿæ­£ç¡®åœ°â€œå­¦ä¹ â€ã€‚äº‹å®ä¸Šï¼Œå¤§éƒ¨åˆ†æ•°æ®ç”¨äºè®­ç»ƒï¼Œå°éƒ¨åˆ†æ•°æ®ç”¨äºæµ‹è¯•ã€‚

è¯¥ä»»åŠ¡å¯ä»¥é€šè¿‡æ¨¡å—**sk learn . model _ selection**ä¸­çš„å‡½æ•°[**train _ test _ split**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)æ¥å¸®åŠ©å®Œæˆã€‚

ç°åœ¨ï¼Œæ˜¯æ—¶å€™å›åˆ°æˆ‘ä»¬çš„é¡¹ç›®äº†ï¼:-)

8ã€‚åº”ç”¨äºæˆ‘ä»¬çš„æˆ¿ä»·é¢„æµ‹é¡¹ç›®

ç”±äºæˆ‘ä»¬çš„æ•°æ®é›†æ—¢ä¸åŒ…æ‹¬åˆ†ç±»æ•°æ®ä¹Ÿä¸åŒ…æ‹¬ NaN å€¼ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦åšä¸€äº›å·¥ä½œï¼Œå¦‚æ‹’ç»ç¦»ç¾¤å€¼ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„å˜é‡å¹¶å°†æ•°æ®åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

**a .å¼‚å¸¸å€¼å‰”é™¤**

åŸºäºä»·æ ¼çš„åˆ†å¸ƒï¼Œåªæœ‰ä¸€äº›å€¼å¤§äº 250 ä¸‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†ğœ= 250 ä¸‡è§†ä¸ºè¿‡æ»¤å¼‚å¸¸å€¼çš„é˜ˆå€¼ï¼Œæ‰€æœ‰ä»·æ ¼é«˜äºğœçš„æˆ¿å±‹éƒ½å°†ä»æ•°æ®é›†ä¸­åˆ é™¤ã€‚

```
t = 2.5*10**6df_new = df[df['price']<= t]
```

æ–°æ•°æ®é›†ä¸­ä»·æ ¼åˆ—çš„åˆ†å¸ƒ:

```
plt.figure(figsize=(10,7))sns.distplot(df_new['price'])plt.xlabel('price', fontsize = 16)plt.ylabel('Density', fontsize = 16)plt.show()
```

![](img/eba4c7c70bf17890de84ec79191da2b6.png)

**b .å˜é‡é€‰æ‹©**

ä¸€äº›ç‰¹æ€§ï¼Œå¦‚â€œæ—¥æœŸâ€ã€â€œidâ€å’Œâ€œé‚®æ”¿ç¼–ç â€å¹¶æ²¡æœ‰çœŸæ­£æŒ‰ç…§æˆ‘ä»¬çš„ç›®æ ‡(ä»·æ ¼)è¿›è¡Œä¿®æ­£ï¼Œå› æ­¤ä¸ºäº†ç®€åŒ–æ¨¡å‹ï¼Œå®ƒä»¬å¯ä»¥è¢«æ‹’ç»ã€‚

```
df_new = df_new.drop(['id','date', 'zipcode'], axis = 1)df_new.head()
```

![](img/5e8e2f739fb4662f91e22006bdb7119b.png)

å°†å‰©ä½™å˜é‡çš„ç›¸å…³æ€§å¯è§†åŒ–:

```
plt.figure(figsize = (8,8))sns.clustermap(df_new.corr())
```

![](img/8ee4f0c0a016b12458e80a0eec30830d.png)

**c .å°†æ•°æ®åˆ†æˆè®­ç»ƒ/æµ‹è¯•é›†**

```
from sklearn.model_selection import train_test_splittrain_set, test_set = train_test_split(df_new, test_size = 0.2, random_state = 0)print('Train size: ', train_set.shape[0], 'Test size: ', test_set.shape[0])>>> Train size:  17212 Test size:  4304
```

å¯è§†åŒ–å…¨å›½çš„ä»·æ ¼åˆ†å¸ƒ:

```
plt.figure()df_new.plot(kind = 'scatter', x = 'long', y = 'lat', alpha = 0.8, c = 'price',cmap=plt.get_cmap('jet'), figsize = (12,8))plt.legend()plt.show()
```

![](img/ee42a0b52b152696cce9db12a7949574.png)

**d .æ­£å¸¸åŒ–**

```
X_train = train_set.drop('price', axis = 1)y_train = train_set['price']X_test = test_set.drop('price', axis = 1)y_test = test_set['price']from sklearn.preprocessing import StandardScalerscaler = StandardScaler()X_train = scaler.fit_transform(X_train)X_test = scaler.transform(X_test)
```

æ£€æŸ¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å¤§å°:

```
print(X_train.shape, X_test.shape)>>> (17212, 17) (4304, 17)
```

è¿™é‡Œï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒé›†ä¸­æœ‰ 17 212 ä¸ªè§‚å¯Ÿå€¼ï¼Œåœ¨æµ‹è¯•é›†ä¸­æœ‰ 4304 ä¸ªè§‚å¯Ÿå€¼ã€‚

# å››ã€‚å»ºæ¨¡å’Œè¯„ä¼°

æœ‰å„ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½ å¯ä»¥æ ¹æ®ç›®æ ‡é€‰æ‹©ï¼Œå¦‚çº¿æ€§å›å½’ï¼Œæ”¯æŒå‘é‡æœºï¼Œå†³ç­–æ ‘ï¼Œéšæœºæ£®æ—ï¼ŒK-æœ€è¿‘é‚»ï¼Œç¥ç»ç½‘ç»œï¼ŒK-å‡å€¼ï¼Œâ€¦

ç‰¹åˆ«æ˜¯ï¼Œè¿™äº›ç®—æ³•åœ¨ **sklearn** ä¸­çš„å®ç°æ˜¯ç›¸ä¼¼çš„ï¼Œå®ƒåŒ…å« 3 ä¸ªä¸»è¦æ­¥éª¤:

*   æ­¥éª¤ 1:åˆå§‹åŒ–æ¨¡å‹
*   æ­¥éª¤ 2:åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆæ¨¡å‹
*   æ­¥éª¤ 3:è¯„ä¼°æµ‹è¯•é›†ä¸­çš„æ¨¡å‹

**ä¸¾ä¾‹:**

*   çº¿æ€§å›å½’æ¨¡å‹:

```
from sklearn.linear_model import LinearRegression# initialize the modelmodel = LinearRegression()# fit the model on the training setmodel.fit(X_train, y_train)# evaluate the model on the test set:y_pred = model.predict(X_test)
```

*   é€»è¾‘å›å½’æ¨¡å‹:

```
from sklearn.linear_model import LogisticRegression# initialize the modelmodel = LogisticRegression()# fit the model on the training setmodel.fit(X_train, y_train)# evaluate the model on the test set:y_pred = model.predict(X_test)
```

*   æ”¯æŒå‘é‡æœºæ¨¡å‹ï¼›

```
from sklearn.svm import SVC# initialize the modelmodel = SVC()# fit the model on the training setmodel.fit(X_train, y_train)# evaluate the model on the test set:y_pred = model.predict(X_test)
```

*   éšæœºæ£®æ—

```
from sklearn.ensemble import RandomForestClassifier# initialize the modelmodel = RandomForestClassifier()# fit the model on the training setmodel.fit(X_train, y_train)# evaluate the model on the test set:y_pred = model.predict(X_test)
```

*   â€¦

é€‰æ‹©è¯„ä¼°æœºå™¨å­¦ä¹ ç®—æ³•çš„æŒ‡æ ‡ä¹Ÿéå¸¸é‡è¦ã€‚åœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œæ ¹æ®ä½ çš„ç›®æ ‡æ˜¯åˆ†ç±»è¿˜æ˜¯å›å½’ï¼Œä½ å¯ä»¥é€‰æ‹©ä¸åŒçš„åº¦é‡æ ‡å‡†:

*   **åˆ†ç±»åº¦é‡**:å‡†ç¡®ç‡ã€æŸå¤±ã€ROC æ›²çº¿ã€æ··æ·†çŸ©é˜µã€åˆ†ç±»æŠ¥å‘Šã€‚
*   **å›å½’åº¦é‡**:å¹³å‡ç»å¯¹è¯¯å·®ã€å‡æ–¹è¯¯å·®(MSE)ã€å‡æ–¹æ ¹è¯¯å·®(RMSE)ã€R åº¦é‡ã€‚

**æˆ‘ä»¬é¡¹ç›®çš„åº”ç”¨:**

å›åˆ°æˆ‘ä»¬çš„æˆ¿ä»·é¢„æµ‹é¡¹ç›®ä¸Šæ¥ã€‚æˆ‘ä»¬å°†å°è¯•ä¸åŒçš„æ¨¡å‹ï¼Œå¦‚çº¿æ€§å›å½’ã€å†³ç­–æ ‘å›å½’ã€éšæœºæ£®æ—å›å½’ã€‚ç”±äºæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å›å½’é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬é€‰æ‹©å‡æ–¹æ ¹è¯¯å·®(RMSE)æ¥è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹:

![](img/7ceabf2338d361125e24160449023b3f.png)

å…¶ä¸­ n æ˜¯æµ‹è¯•é›†çš„è§‚æµ‹å€¼ï¼Œy^{(i)}å’Œ f(x^{(i)})å¯¹åº”äº x^{(i)}.çš„çœŸå®ç›®æ ‡å’Œä¼°è®¡ç›®æ ‡è¯¥æŒ‡æ ‡å¯ä»¥ä» **sklearn.metrics** æ¨¡å—å¯¼å…¥ã€‚

```
from sklearn.metrics import mean_squared_error
```

1.  **çº¿æ€§å›å½’**

```
from sklearn.linear_model import LinearRegression# Initialize the model
lin_reg = LinearRegression()# Fit the model on the training set
lin_reg.fit(X_train,y_train)# Evaluate the model on the test set: 
y_pred_lin = lin_reg.predict(X_test)
mse_lin = mean_squared_error(y_test,y_pred_lin)
rmse_lin = np.sqrt(mse_lin)print('RMSE of Linear Regression is: ', round(rmse_lin,1))
RMSE of Linear Regression is:  167347.9
```

**2ã€‚å†³ç­–æ ‘**

```
from sklearn.tree import DecisionTreeRegressor# Initialize the model
tree_reg = DecisionTreeRegressor()# Fit the model on the training set
tree_reg.fit(X_train, y_train)# Evaluate the model on the test set
y_pred_tree = tree_reg.predict(X_test)
mse_tree = mean_squared_error(y_test, y_pred_tree)
rmse_tree = np.sqrt(mse_tree)print('RMSE of Decision Tree is: ', rmse_tree)
RMSE of Decision Tree is:  156869.0
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå†³ç­–æ ‘ç®—æ³•ç»™å‡ºçš„ç»“æœä¼˜äºçº¿æ€§å›å½’çš„ç»“æœã€‚æˆ‘ä»¬è¯•è¯•éšæœºæ£®æ—ç®—æ³•ï¼Œçœ‹çœ‹æ˜¯å¦æ¯”è¿™ä¸¤ä¸ªç®—æ³•å¥½ã€‚

**3ã€‚éšæœºæ£®æ—**

```
from sklearn.ensemble import RandomForestRegressor# Initialize the model 
forest_reg = RandomForestRegressor()# Fit the model on the training set
forest_reg.fit(X_train, y_train)# Evaluate the model on the test set
y_pred_forest = forest_reg.predict(X_test)mse_forest = mean_squared_error(y_test1, y_pred_forest)rmse_forest = np.sqrt(mse_forest)print('RMSE of Random Forest method is: ', round(rmse_forest,1))
RMSE of Random Forest method is:  114766.6
```

**ç»“è®º**:åœ¨ä»¥ä¸Šä¸‰ç§æ–¹æ³•ä¸­ï¼Œéšæœºæ–¹æ³•ç»™å‡ºçš„ç»“æœæœ€å¥½ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡å¯»æ‰¾æ›´å¥½çš„å‚æ•°æ¥æ”¹è¿›è¯¥ç®—æ³•ï¼Œä»¥ä¾¿æ¨¡å‹è·å¾—æ›´é«˜çš„æ€§èƒ½ã€‚

# åŠ¨è¯ ï¼ˆverb çš„ç¼©å†™ï¼‰è¶…å‚æ•°è°ƒè°

**sci kit-learn . model _ selection**æ¨¡å—åŒ…ä¸­çš„å‡½æ•° GridSearchCV å…è®¸æˆ‘ä»¬ç”¨ä¸åŒçš„è¶…å‚æ•°ç»„åˆæ¥è®­ç»ƒæ¨¡å‹ï¼Œå®ƒä¼šè‡ªåŠ¨ç¡®å®šä¸ºæˆ‘ä»¬æä¾›æœ€ä½³æ€§èƒ½çš„å‚æ•°ã€‚

å¯¼å…¥ **GridSearchCV** å‡½æ•°:

```
from sklearn.model_selection import GridSearchCV
```

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å‘ç°å®é™…éšæœºæ£®æ—æ¨¡å‹çš„å‚æ•°:

```
forest_reg **RandomForestRegressor**(bootstrap=True, ccp_alpha=0.0,criterion='mse',                       
max_depth=None, max_features='auto', max_leaf_nodes=None,                                           max_samples=None, min_impurity_decrease=0.0,                          min_impurity_split=None, min_samples_leaf=1,                       min_samples_split=2, min_weight_fraction_leaf=0.0,                       n_estimators=100, n_jobs=None, oob_score=False,                         random_state=None, verbose=0, warm_start=False)
```

æˆ‘ä»¬å¯ä»¥æ”¹å˜è®¸å¤šå‚æ•°ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°è¯•ä¸€äº›å‚æ•°ï¼Œå¦‚ bootstrapï¼Œmax_featuresï¼Œmin_samples_splitï¼Œn_estimorsã€‚è¿™äº›å‚æ•°çš„æµ‹è¯•å€¼ä¿å­˜åœ¨å­—å…¸ **params_grid** ä¸­:

```
params_grid = [{'bootstrap': [False, True],
                'min_samples_split': [2,4,5],
                'n_estimators': [100,150,200],
                'max_features': [8,10,12]}]
```

æˆ‘ä»¬æœ‰è¶…å‚æ•° bootstrapã€max_featuresã€min_samples_splitã€n_estimators çš„ 2Ã—3Ã—3 = 54 ä¸ªç»„åˆã€‚

```
# Initialize the model
forest_reg = RandomForestRegressor()# Apply **GridSearchCV** on our model with all parameters in **params_grid**
grid_search = GridSearchCV(forest_reg, params_grid, cv = 5,
                           scoring = 'neg_mean_squared_error',
                           return_train_score = True)# Fit all models in the training set
grid_search.fit(X_train, y_train)
```

æ¯ä¸ªæ¨¡å‹è®­ç»ƒ 5 æ¬¡ï¼Œå¯¹åº”äº¤å‰éªŒè¯å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ€»å…±æœ‰ 54 Ã— 5 = 270 è½®è®­ç»ƒã€‚

ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥ç¡®å®šå‚æ•°ï¼Œç»™æˆ‘ä»¬æœ€å¥½çš„ä¼°è®¡æ¨¡å‹ã€‚

æœ€ä½³å‚æ•°:

```
grid_search.best_params_
{'bootstrap': True,  
 'max_features': 10,  
 'min_samples_split': 2,  
 'n_estimators': 150}
```

å› æ­¤ï¼Œå¯¹äºç»™å®šçš„å‚æ•°ï¼Œå½“ bootstrap = Trueï¼Œmax_features = 10ï¼Œmin_samples_split = 2ï¼Œn_estimators = 150 æ—¶ï¼Œæ¨¡å‹æ•ˆæœæœ€ä½³ã€‚å¯¹åº”äºè¿™äº›å‚æ•°çš„æ¨¡å‹ç”±ä¸‹å¼ç»™å‡º:

```
final_model = grid_search.best_estimator_
final_model
**RandomForestRegressor**(bootstrap=True, ccp_alpha=0.0,criterion='mse',                       
max_depth=None, max_features=10, max_leaf_nodes=None,                       max_samples=None, min_impurity_decrease=0.0,                       min_impurity_split=None, min_samples_leaf=1,                       min_samples_split=2, min_weight_fraction_leaf=0.0,                       n_estimators=150, n_jobs=None, oob_score=False,                           random_state=None, verbose=0, warm_start=False)
```

# ä¸åŠç‰©åŠ¨è¯é¢„æµ‹ã€‚

ç¡®å®šæœ€ä½³æ¨¡å‹åï¼Œå®ƒå¯ç”¨äºé¢„æµ‹æµ‹è¯•é›†ä¸Šçš„æ–°æ ·æœ¬:

```
y_pred_final = final_model.predict(X_test)mse_final = mean_squared_error(y_test1, y_pred_final)rmse_final = np.sqrt(mse_final)print('RMSE of final model is: ', round(rmse_final,1))
RMSE of final model is:  110775.0
```

ä¸‹å›¾æ˜¾ç¤ºäº†æµ‹è¯•é›†ä¸­ 100 ä¸ªæ•°æ®ç‚¹çš„çœŸå®å€¼å’Œé¢„æµ‹å€¼ã€‚

![](img/9018f46cac44c8dad0f0d137fb5bcc41.png)

å¬èµ·æ¥æˆ‘ä»¬çš„é¢„æµ‹æ¥è¿‘çœŸå®å€¼ã€‚ä½†æ˜¯æ¨¡å‹ä»ç„¶å¯ä»¥é€šè¿‡å°è¯•æ›´å¤šçš„è¶…å‚æ•°æ¥æ”¹è¿›ã€‚(*è¿™ä¸ªä»»åŠ¡å¯¹ä½ æ¥è¯´æ˜¯ä½œä¸ºä¸€ä¸ªç»ƒä¹ å·¥ä½œæ¥åº¦è¿‡çš„ã€‚:-)* )

**ç»“è®º:**

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†æœºå™¨å­¦ä¹ é¡¹ç›®çš„ä¸€äº›å¿…è¦æ­¥éª¤ã€‚å®ƒä»¬åŒ…æ‹¬æ”¶é›†ã€åˆ†æã€é¢„å¤„ç†æ•°æ®ã€æ¨¡å‹åŒ–ã€è¯„ä¼°æ¨¡å‹ã€è¶…å‚æ•°è°ƒæ•´ä»¥åŠæœ€ç»ˆä½¿ç”¨æ¨¡å‹é¢„æµ‹æ–°æ•°æ®ç‚¹ã€‚æœ‰æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»å°è¯•ä¸åŒçš„æ¨¡å‹å’Œä¸åŒçš„å‚æ•°ï¼Œä»¥é€‰æ‹©ç²¾åº¦æœ€é«˜çš„æœ€ä½³æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹éƒ½æœ‰ sklearn åŒ…æ”¯æŒï¼Œsk learn åŒ…æ˜¯æœºå™¨å­¦ä¹ çš„æœ‰åŠ›å·¥å…·ã€‚ä½ å¯ä»¥çœ‹åˆ°[è¿™ä¸ªå‚è€ƒ](https://scikit-learn.org/stable/)è·å¾—è¿™ä¸ªåŒ…çš„æ›´å¤šç»†èŠ‚ã€‚

æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« æœ‰åŠ©äºè§„åˆ’æ‚¨çš„é¡¹ç›®ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ã€‚æ¬¢è¿æ‰€æœ‰çš„è´¡çŒ®ã€‚ã€‚^

æ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼

**Github ä»£ç :**[https://Github . com/khu yenle-maths/House-price-prediction/blob/main/House _ price _ prediction . ipynb](https://github.com/KhuyenLE-maths/House-price-prediction/blob/main/House_price_prediction.ipynb)

å…³äºæœºå™¨å­¦ä¹ é¡¹ç›®çš„æ•°æ®å‡†å¤‡çš„å…¶ä»–å‚è€ƒèµ„æ–™:

[](https://github.com/KhuyenLE-maths/Project_PublicHealth_France/blob/main/notebook.ipynb) [## Project _ public health _ France/notebook . ipynb at main khu yenle-maths/Project _ public health _ France

### å¤§å‹æ•°æ®é›†ä¸­çš„æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ŒPCAï¼Œtest ANOVA-Project _ public health _ France/notebook . ipynb at mainâ€¦

github.com](https://github.com/KhuyenLE-maths/Project_PublicHealth_France/blob/main/notebook.ipynb) [](https://github.com/KhuyenLE-maths/Project_EDA_Paris_trees/blob/main/Paris_SmartCity.ipynb) [## Project _ EDA _ Paris _ trees/Paris _ smart city . ipynb at main khu yenle-maths/Project _ EDA _ Paris _ trees

### å¤§æ•°æ®é›†çš„æ¢ç´¢æ€§æ•°æ®åˆ†æã€‚å¯¹ khu yenle-maths/Project _ EDA _ Paris _ trees å¼€å‘çš„è´¡çŒ®æ¥è‡ªâ€¦

github.com](https://github.com/KhuyenLE-maths/Project_EDA_Paris_trees/blob/main/Paris_SmartCity.ipynb) 

ç”¨å„ç§æœºå™¨å­¦ä¹ ç®—æ³•å‚è€ƒæ”¶å…¥åˆ†ç±»

[https://github . com/khu yenle-maths/Project _ Income _ Classification/blob/main/notebook . ipynb](https://github.com/KhuyenLE-maths/Project_Income_Classification/blob/main/notebook.ipynb)