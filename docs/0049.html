<html>
<head>
<title>Deep dive into Decision Tree Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入探讨决策树第一部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-dive-into-decision-tree-part-i-9576ac66c2f8?source=collection_archive---------19-----------------------#2021-01-02">https://medium.com/analytics-vidhya/deep-dive-into-decision-tree-part-i-9576ac66c2f8?source=collection_archive---------19-----------------------#2021-01-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="d370" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">用python构建决策树模型的数据预处理。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/448063b924447d5847d146d80ac55f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ivA9KcvgjdDvwJKWwD-j9A.jpeg"/></div></div></figure><p id="db62" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">大家好🙌。</p><p id="518c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">今天我将谈论决策树，但是它将被分成两部分。</p><blockquote class="jo jp jq"><p id="17c5" class="ie if jr ig b ih ii ij ik il im in io js iq ir is jt iu iv iw ju iy iz ja jb ha bi translated">第一部分:决策树概念，数据预处理，模型构建。</p><p id="6513" class="ie if jr ig b ih ii ij ik il im in io js iq ir is jt iu iv iw ju iy iz ja jb ha bi translated">第二部分:幕后的数学。</p></blockquote></div><div class="ab cl jv jw go jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="ha hb hc hd he"><p id="036f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">决策树由Leo Breimann等人在1984年开发，是一种机器学习算法，属于监督学习，目标是通过问一系列问题将数据分成类(分类)/数字箱(回归)。</p><p id="1fd8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可以把它想象成一棵颠倒的树，根(根节点)在顶部，叶(叶节点)在底部，中间的一切都只是被称为节点。因此，从根节点开始，我们提出问题，并根据答案分离数据。分离前的数据属于被分离成两个子节点的父节点。</p><p id="7d9a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将只讨论二元决策树，这意味着数据只有两种分离方式。尽管决策树可以有多于二进制的答案，但它的计算量非常大。</p></div><div class="ab cl jv jw go jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="ha hb hc hd he"><p id="40cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在继续之前，我将简要地谈谈利弊，这样你就可以决定这是不是适合你的问题的算法，然后继续读下去或者跳过它。</p><p id="2926" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">优点:</strong></p><ul class=""><li id="661b" class="kc kd hh ig b ih ii il im ip ke it kf ix kg jb kh ki kj kk bi translated">可解释性。</li><li id="95b9" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">能够同时处理数值和分类特征，无需数据预处理。</li><li id="53cc" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">自动特征选择。</li><li id="fa37" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">多重共线性不影响算法质量。</li><li id="8741" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">使用训练模型进行预测时计算效率高。</li></ul><p id="575c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">缺点:</strong></p><ul class=""><li id="ca06" class="kc kd hh ig b ih ii il im ip ke it kf ix kg jb kh ki kj kk bi translated">倾向于过度拟合</li><li id="fd29" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">数据的微小变化会导致决策树结构的巨大变化。</li><li id="3b66" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">如果数据相似，训练时计算量很大→需要多个问题来分离数据。</li></ul></div><div class="ab cl jv jw go jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="ha hb hc hd he"><p id="e40f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你认为这是你想要的算法，那就跟着做👇。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="kq kr l"/></div></figure><p id="6701" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们有了所有的依赖项，让我们加载数据，但是要做一些调整。</p><p id="2c96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">加载iris数据集后，用<code class="du ks kt ku kv b">NaN</code>替换<code class="du ks kt ku kv b">petal length </code>和<code class="du ks kt ku kv b">sepal length</code>列中的15个随机单元格，使我们的数据集变得凌乱。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="kq kr l"/></div></figure><p id="84d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们的数据集看起来像这样:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kw"><img src="../Images/cddd6ff96d494b7546c107e9c6825ba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*PgkAdeTf9wFHf2oSfO_Htw.png"/></div></figure><p id="f603" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我们刚刚得到这些数据，我们现在的目标是训练一个决策树模型来分类花的类型(目标)。</p><p id="dce8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">像往常一样，我们必须探索我们的数据，找出它是否有缺失的数据，并搜索特征之间的关系。了解你正在处理的数据的背景信息是很重要的，因为这将大大减少数据预处理的时间，在这种情况下，了解sepal，petal指的是什么将是有益的。</p><p id="7776" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，如果我们知道什么是花瓣，我们可以肯定地假设它的长度和宽度一定是相关的。让我们想象花瓣和萼片的长度和宽度之间的关系。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="kq kr l"/></div></figure><p id="011f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">输出</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kx"><img src="../Images/7ba0ca29804c327bf4d929688d1f1ff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*XbTGUFxcC3WzEre0I6aiyg.png"/></div></figure><p id="0e32" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以看到花瓣宽度和花瓣长度之间的高度正相关。因此，我们可以执行线性回归，通过将<code class="du ks kt ku kv b">petal width</code>值插入最佳拟合的<strong class="ig hi">线来填充<code class="du ks kt ku kv b">petal length</code>列中的<code class="du ks kt ku kv b">NaN</code>值，最佳拟合</strong>线将输出预测的<code class="du ks kt ku kv b">petal length</code>值<strong class="ig hi">。</strong></p><p id="5372" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于<code class="du ks kt ku kv b">sepal length</code>栏中缺失的数据，我们可以用相应花卉类型的平均值/中值/众数来代替。因此，例如，<code class="du ks kt ku kv b">sepal length</code>列中的第2行将由<strong class="ig hi"> setosa </strong>的<code class="du ks kt ku kv b">sepal length</code>的平均值/中值/众数值替换。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="kq kr l"/></div></figure><p id="6b44" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们的数据集准备好用决策树算法进行训练。我们将使用sklearn的决策树分类器，并使用“基尼系数”作为衡量杂质的标准(这将在第二部分深入讨论)。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="kq kr l"/></div></figure><p id="d114" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的决策树对花型分类很好。这基本上是对决策树的高级理解。在第二部分中，我们将回顾决策树的幕后，以了解它如何在每个节点选择特征、分割停止标准以及用于树修剪的其他参数。</p></div><div class="ab cl jv jw go jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="ha hb hc hd he"><p id="4b4a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢您的阅读！非常欢迎反馈，如果有任何错误信息，请评论。</p><p id="11d0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">参考资料:</p><ul class=""><li id="2289" class="kc kd hh ig b ih ii il im ip ke it kf ix kg jb kh ki kj kk bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Decision_tree#:~:text=Disadvantages%20of%20decision%20trees%3A,perform%20better%20with%20similar%20data." rel="noopener ugc nofollow" target="_blank">决策树的优缺点</a></li></ul></div></div>    
</body>
</html>