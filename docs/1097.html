<html>
<head>
<title>Ensemble Methods in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的集成方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ensemble-methods-in-machine-learning-31084c3740be?source=collection_archive---------7-----------------------#2021-02-14">https://medium.com/analytics-vidhya/ensemble-methods-in-machine-learning-31084c3740be?source=collection_archive---------7-----------------------#2021-02-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="fc8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将尝试熟悉不同的集成技术和一些常见的算法。</p><p id="1df6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">集成方法是对来自训练数据集的多个样本多次使用算法或模型。“集成方法的目标是将几个基本估计量的预测与给定的学习算法结合起来，以提高单个估计量的可推广性/稳健性。”</p><p id="1451" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，如果给你一个5人团队的任务“A ”,你将有多种选择来完成这个任务。选项1 :你试图独自完成这件事，你会找到一个更简单的方法，但最终不会有好的结果。<br/> <strong class="ih hj">选项2 </strong>:或者你可能会全力以赴完美地完成任务，但当你被分配到一个类似的新任务‘B’时，你将会一无所知，或者你将不得不为那个任务重新训练自己(过度适应)。选项3 :你把你的任务分配给你的团队成员，然后把他们结合起来，得到好的结果。现在，当类似的任务再次被分配时，同样的成员可以更有效地完成它。</p><p id="55c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从以上三个选项来看，你足够聪明，可以选择第三个。对吗？如果我们能同样训练模型。这些集合方法允许我们做同样的事情。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/23d9ad65e67e686ee355c69a563f2b82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bic1BVl02k2evh0MmebhIQ.png"/></div></div></figure><p id="f85e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">集成学习是解决偏差-方差权衡的一种方法。一个好的模型应该在这两种类型的错误之间保持平衡。这就是所谓的偏差-方差误差的权衡管理。</p><p id="c104" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">系综方法主要可以分为两类:<br/> </strong> 1。并行集成方法<strong class="ih hj"> <br/> </strong> 2。顺序集成方法</p><h1 id="67ba" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"> 1。并行集成方法:</strong></h1><p id="2f3f" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">在并行集成方法中，基本学习器或算法是并行生成的。几个估计或模型是独立建立的，然后平均他们的预测。平均而言，组合估计量通常比任何单基估计量都要好，因为它的<strong class="ih hj">方差减少了。<br/>组合预测的方差减少到1/n (n是模型或样本的数量)。</strong></p><p id="57fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">并行集成技术中最常见的方法之一是自举聚合(Bagging)。</p><h1 id="b10d" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">自举汇总(装袋):</strong></h1><p id="86ef" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">在这种方法中，使用相同的算法生成模型，使用bootstrap采样方法对数据集的随机子样本进行采样，以减少方差。在bootstrap抽样中，有些原始样本出现不止一次，有些原始样本在样本中不存在。</p><p id="8eb3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">装袋技术对回归和分类都有用。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ks"><img src="../Images/0d7d64ef6e21b27c38c0d16457d13bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G279X-1YwECFstqqY_nbIg.jpeg"/></div></div></figure><p id="ede6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在回归中，它取所有模型的平均值，在分类中，它考虑每个模型的投票。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kt"><img src="../Images/f7d6bc6a85c6b3fb25785a6169a786da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5foRi1c0bf3_H3gj6lj6MA.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">分类投票</figcaption></figure><p id="84cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林是流行的装袋算法之一。</p><p id="fe84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">随机森林(Bagging算法):<br/> </strong>在每个样本的随机森林中，使用共同形成森林的决策树，因此形成随机森林。Rest几乎类似于一个简单的决策树。</p><p id="c8e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用随机森林时要调整的主要超参数:<br/> n_estimators:树的数量<br/> max_features:采样时要考虑的要素的数量。</p><p id="9463" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其他的超参数有<br/> max_depth:树的最大深度(默认= 'None') <br/> min_samples_split:节点的分裂数(默认= 2)。<br/>标准:衡量分割质量的函数(默认值=' Gini ')</p><p id="5dd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，当适当的程序被忽略时，结果模型可能经历许多偏差。此外，它的计算量也很大。</p><h1 id="e10b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"> 2。顺序集成方法(Boosting算法):</strong></h1><p id="1477" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">在顺序集成方法中，基本学习器是顺序产生的。通过用更高的权重对先前错误标记的示例进行加权，可以提高整体性能。总的来说，它减少了组合估计量的偏差。<br/>简单来说，boosting指的是能够将弱学习者转化为强学习者的算法家族。boosting的主要原理是拟合一系列在不同版本的数据中表现更好的弱学习器/模型。前几轮被错误分类的例子会得到更多的重视。<br/>这里每个模型都依赖于前一个模型。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ky"><img src="../Images/d7aa0d58b2484c6b81cee437c54366c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DKDI6EZJHkw2YgjLVDohiQ.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">在Boosting中，ML模型建立在每个阶段的错误分类数据上。最后，聚合每个阶段的所有正确分类器，形成一个强学习器。</figcaption></figure><p id="7f95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">算法:</strong></p><h1 id="05f5" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"> 1。梯度推进决策树(GBDT) </strong></h1><p id="3987" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">在决策树上使用梯度下降算法来减少每一层的损失。它通过允许优化任意可微损失函数来概括模型。下一个模型建立在前一个模型的错误之上。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kz"><img src="../Images/70404310b71af8db82352b63a4ad8637.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sq5D1hOK3wAQGZcWniiTIg.jpeg"/></div></div></figure><h1 id="9630" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">②<em class="la">。XG升压</em>T19】</strong></h1><blockquote class="lb lc ld"><p id="67d7" class="if ig le ih b ii ij ik il im in io ip lf ir is it lg iv iw ix lh iz ja jb jc hb bi translated">XG Boost的工作方式类似于GBDT，但它有更多的功能(见下文),这使它更加高效。 <br/> i) <strong class="ih hj">正则化:</strong>标准GBM实现没有XGBoost那样的正则化，因此也有助于减少过拟合。事实上，XGBoost也被称为“正则化增强”技术。<br/> <strong class="ih hj"> ii)并行处理:</strong> XGBoost在根节点实现并行处理，与GBM相比速度更快。<br/> iii) <strong class="ih hj">高灵活性:</strong> XGB允许定制评估标准。<br/> iv) <strong class="ih hj">处理缺失值:</strong>可以自己处理缺失值。<br/> v) <strong class="ih hj">树修剪:</strong> XGB自己做树修剪，减少方差。<br/> vi) <strong class="ih hj">内置交叉验证:</strong> XGBoost允许用户在boosting过程的每次迭代中运行交叉验证。<br/> vii) <strong class="ih hj">继续现有模型:</strong>用户可以从上次运行的最后一次迭代开始训练XGBoost模型。</p></blockquote><h1 id="ee88" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"> 3。Ada Boost </strong></h1><p id="3a70" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">这里，通常，每个弱学习器被开发为决策树桩(树桩是只有一个裂口和两个终端节点的树)，用于对观察结果进行分类。<br/>训练完每个分类器后，根据其准确度计算分类器的权重。准确度越高，分类器的权重就越大，反之亦然。</p><h1 id="d629" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"> 4。轻型GBM </strong></h1><p id="ebf5" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">它以最佳拟合的方式按叶分割树，而其他boosting算法按深度或级别而不是按叶分割树，并试图尽快获得纯叶。这使得Light GBM比其他增强算法更快，因此有了“Light”这个词。</p><h1 id="3aee" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"> 5。卡特彼勒助力</strong></h1><p id="ca1d" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">“CatBoost”名字来源于两个词“Category”和“Boosting”。CatBoost使用分类特征组合以及分类和数字特征组合的各种统计数据将分类值转换为数字。因此，我们不需要像在其他模型中那样将分类变量转换成数字变量。它生成了一棵平衡树，可以像XGB一样处理丢失的值。</p><h1 id="1a1a" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"> 3。堆叠</strong></h1><p id="2acb" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">堆叠是一种集成学习技术，它通过元分类器或元回归器组合多个分类或回归模型。基于完整的训练集来训练基础级模型，然后在基础级模型的输出上训练元模型作为特征。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es li"><img src="../Images/6d3806961ebb01ee7fe02ecbc39d1b67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JlHSr4mJGaorqzQF6y_kug.jpeg"/></div></div></figure><p id="b935" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望这篇文章有助于对不同的集成方法和常用算法有一个基本的了解。如果您有任何疑问，请在下面留下。</p></div></div>    
</body>
</html>