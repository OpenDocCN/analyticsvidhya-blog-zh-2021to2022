<html>
<head>
<title>Linear Regression in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-in-machine-learning-783bb5e58944?source=collection_archive---------12-----------------------#2021-04-22">https://medium.com/analytics-vidhya/linear-regression-in-machine-learning-783bb5e58944?source=collection_archive---------12-----------------------#2021-04-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="2490" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">什么是回归？</h1><p id="5aeb" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">回归是用于根据独立特征预测目标列中的连续变量或因变量的方法。它属于监督技术。它是一种统计工具，用于找出结果变量、因变量和一个或多个变量(通常称为自变量)之间的关系。</p><h1 id="633e" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">什么是线性回归？</h1><p id="6096" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">线性回归用于寻找目标和一个或多个预测值之间的线性关系。</p><p id="3b28" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">简单线性回归找出因变量(Y)和自变量(X)之间的关系，并试图通过最小化误差来找到最佳拟合线。此拟合函数表示您的模型有多好，或者您可以定义一个成本函数来衡量它有多差。成本函数测量模型预测和训练数据之间的距离。但是<strong class="jf hj">我们线性回归的目标是最小化距离(or)误差(or)残差</strong>。范围从-inf到+inf。该算法采用<strong class="jf hj">普通最小二乘(OLS) </strong>。</p><p id="f400" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">最佳拟合线公式(或预测公式)</strong></p><p id="6a6e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 1)简单线性回归</strong> :- Y= β0+ β1X</p><p id="ac2f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 2)多元线性回归</strong> :- Y= β0+ β1X1+ β2X2+ β3X3+…。+ βnXn</p><p id="d7b0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">βo和β1是两个未知常数。β1是y的斜率，β0是y的截距。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kg"><img src="../Images/fdaa0bb7f08387372d9cade8a43bf0c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*a8t0Kosar_Nkfvr5Fp8nyA.png"/></div></figure><p id="28af" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">P1=给定X的原始Y数据点</p><p id="3c08" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">P2=给定X的估计y值</p><p id="810a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">Y条= Y平均值</p><p id="4c61" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">X条= X平均值</p><h1 id="b15e" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">现在让我们了解一下线性回归模型的性能。</h1><p id="8127" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">误差= Y实际值—预测线</strong></p><p id="2d00" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 1。SSE(残差(或)无法解释的误差):——</strong>误差是Y的观测值与Y的预测值之差</p><p id="256d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 2。SSR(回归误差(or)解释误差):——</strong>误差是Y的平均值与Y的预测值之差</p><p id="6bfa" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 3。SST(总误差平方和):——</strong>SST量化Y的观测值围绕Y的平均值的变化。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ko"><img src="../Images/5bb0cd9ea278a42ccfbfc11ced51e24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*CC-LbVTKSdhuigrOh0EKjg.png"/></div></figure><p id="d31a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 4。MAE(平均绝对误差):— </strong>实际值与预测值的平均绝对差值。</p><p id="d807" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 5。MAPE(平均绝对百分比误差):——</strong>实际&amp;预测值与实际值的绝对差值之比的平均值。</p><p id="2526" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">6。RMSE(均方根误差):— 计算实际值和预测值的平方差之和的平方根平均值。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kp"><img src="../Images/5f540478cb18d83ca69588cae5c1a321.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*nSxLSFmhopNkXebhxBOn2Q.png"/></div></figure><p id="fc5f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">7 .<strong class="jf hj">。R平方(R2): — </strong> R平方值描述了由模型中自变量解释的因变量变化的百分比，也称为决定系数。r平方的范围在0到1之间。如果r平方接近1，则模型可预测性高。如果我们在模型中加入更多的变量，不管这个变量是否对模型有贡献，R2的值都会增加。这就是使用R2的缺点。</p><p id="0e6c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 8。调整后的R方:——</strong>R方的缺点由调整后的R方值来修复。只有当添加的变量对模型有重大贡献时，调整后的R2值才会提高。调整后的R平方值在模型中增加了惩罚。调整后的R平方始终小于R平方。</p><p id="bd5e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">其中，R2是R平方值，n =观测值总数，k =模型中使用的变量总数。如果我们增加变量的数量，分母变小，总体比率就会很高。从1中减去将减少整个调整后的R2。因此，为了增加调整后的R2，附加特征对模型的贡献应该非常高。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ko"><img src="../Images/46240ced6fb9149da1a68e202e4069ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*JhhUkh5tRPhuaVJ6k3OLDA.png"/></div></figure><h1 id="4008" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">线性回归的假设</h1><h2 id="28bc" class="kq ig hi bd ih kr ks kt il ku kv kw ip jo kx ky it js kz la ix jw lb lc jb ld bi translated">1.多重共线性</h2><p id="9459" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">两个或两个以上的自变量之间不应该有很高的相关性。<strong class="jf hj">如果我们的模型中存在多重共线性，我们需要逐步删除这些特征</strong>。可以使用相关矩阵、容差和方差影响因子(VIF)来检查多重共线性。</p><p id="c07b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> VIF = 1/1- R平方</strong></p><h2 id="d2dc" class="kq ig hi bd ih kr ks kt il ku kv kw ip jo kx ky it js kz la ix jw lb lc jb ld bi translated">2.常态</h2><p id="63eb" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">当我们的数据以正态分布流动时，我们可以说正态存在。如果我们的数据不正常，我们需要对目标变量应用转换技术。可以使用夏皮罗检验(或)概率图(或)偏差来检查正态性。偏斜度应该用残差来检验。<strong class="jf hj">夏皮罗测试和偏斜</strong>的范围应为-0.5至+0.5。</p><h2 id="d16a" class="kq ig hi bd ih kr ks kt il ku kv kw ip jo kx ky it js kz la ix jw lb lc jb ld bi translated">3.线性</h2><p id="81a2" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因变量和自变量之间应该是线性关系。如果我们的数据不是线性的，<strong class="jf hj">我们需要对因变量进行变换，使其呈线性形式</strong>。可使用统计库<strong class="jf hj">中的线性彩虹检查线性度。概率值应大于0.5 </strong>。</p><h2 id="fe70" class="kq ig hi bd ih kr ks kt il ku kv kw ip jo kx ky it js kz la ix jw lb lc jb ld bi translated">4.自相关</h2><p id="5693" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">两个变量之间的关联称为序列相关。当存在重复数据时，我们可以说变量之间存在自相关。不应该有自相关。检查残差的自相关。</p><p id="cb69" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这可以通过做<strong class="jf hj"> OLS总结报告</strong>中的杜宾沃森测试来检查。<strong class="jf hj">如果Durbin Watson是2，那么没有自相关，如果它大约在1.5到2.5之间，那么优选的是，如果它大于2.5，那么模型中存在自相关</strong>。</p><h2 id="b511" class="kq ig hi bd ih kr ks kt il ku kv kw ip jo kx ky it js kz la ix jw lb lc jb ld bi translated">5.同方差性</h2><p id="9644" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我的模型应该是同方差。它可以被定义为均匀方差。这可以通过统计库的het_goldfeldquandt来检查。<strong class="jf hj">概率值应该大于0.5 </strong>。</p><blockquote class="le lf lg"><p id="5749" class="jd je lh jf b jg kb ji jj jk kc jm jn li kd jq jr lj ke ju jv lk kf jy jz ka hb bi translated"><strong class="jf hj">键</strong></p></blockquote><p id="a77a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">如果我们有分类数据，我们需要在建立模型之前将其转换为虚拟变量。</p><p id="e30c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">无多重共线性</p><p id="0a38" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">残差应该是正常的</p><p id="24c4" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">残差应该是线性的</p><p id="8b63" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">无自相关</p><p id="4df3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">同质性的存在。</p></div><div class="ab cl ll lm gp ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="hb hc hd he hf"><h1 id="01e3" class="if ig hi bd ih ii ls ik il im lt io ip iq lu is it iu lv iw ix iy lw ja jb jc bi translated">这是下一个调整模型的博客。</h1><h2 id="2e65" class="kq ig hi bd ih kr ks kt il ku kv kw ip jo kx ky it js kz la ix jw lb lc jb ld bi translated">线性回归链接中的超参数调整<a class="ae lx" href="https://sruthils22.medium.com/hyperparameter-tuning-in-linear-regression-e0e0f1f968a1" rel="noopener">点击此处</a></h2><h1 id="8a5b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">参考</h1><p id="337b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae lx" href="https://www.ajo.com/article/S0002-9394(10)00167-4/fulltext" rel="noopener ugc nofollow" target="_blank">https://www . ajo . com/article/s 0002-9394(10)00167-4/full text</a></p><p id="ba68" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><a class="ae lx" href="https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/R5_Correlation-Regression4.html" rel="noopener ugc nofollow" target="_blank">https://SPH web . bumc . bu . edu/otlt/MPH-Modules/BS/R/R5 _ Correlation-Regression/R5 _ Correlation-Regression 4 . html</a></p></div></div>    
</body>
</html>