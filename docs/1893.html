<html>
<head>
<title>Ridge And Lasso Regression Made Easy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">脊和套索回归变得容易</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ridge-and-lasso-regression-made-easy-343df45a90b9?source=collection_archive---------7-----------------------#2021-03-24">https://medium.com/analytics-vidhya/ridge-and-lasso-regression-made-easy-343df45a90b9?source=collection_archive---------7-----------------------#2021-03-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/8b156b4d93550ee1895890ef4974db02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6woabNEB02GDN4qo"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">由<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae it" href="https://unsplash.com/@grakozy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Greg Rakozy </a>拍摄的照片</figcaption></figure><p id="4be1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在本文中，让我们来理解正则化回归中的两种不同技术</p><ol class=""><li id="c156" class="js jt hh iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">里脊回归</li><li id="cf12" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">套索回归</li></ol><p id="2775" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们从理解什么是正规化开始学习。</p><p id="7e6a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">什么是正规化？</strong> <br/>在ml中，我们经常会遇到这样的情况:我们的模型在训练数据上表现很好，但在看不见的数据上却无法推广(过拟合条件)或(低偏差和高方差条件)。正则化是一种降低模型复杂性的方法，或者是一种创建最佳复杂模型的过程。由于高系数值和大量特征，模型的复杂性增加，正则化的目标是处理这两个问题。</p><ol class=""><li id="576c" class="js jt hh iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated"><strong class="iw hi">岭回归(L2) </strong></li></ol><p id="cdb6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">正则化成本函数有两个组成部分误差项，正则化项</p><figure class="kh ki kj kk fd ii er es paragraph-image"><div class="er es kg"><img src="../Images/a4efb5e855a59d30ee91c6ddd451805f.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*QRajfT2MZAm2XX1Doc42AQ.png"/></div></figure><p id="c58f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里的损耗只是一个误差项，后面跟着一个正则项，“lambda”是一个需要调整的超参数或平衡因子，“beta”是系数的平方和。因此，通过改变'<strong class="iw hi"> λ' </strong>，我们正在控制惩罚项，如果<strong class="iw hi"> λ=0 </strong>则意味着我们仅使用误差项而没有任何惩罚，如果'<strong class="iw hi"> λ' </strong>是一个高值，则意味着惩罚更大，因此系数的幅度减小，选择最佳'<strong class="iw hi"> λ' </strong>值很重要。</p><p id="b779" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">本质上，岭回归的目标是通过降低系数的大小来降低模型的复杂性。我们也可以说岭回归解决了多重共线性问题。</p><p id="ed21" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2.<strong class="iw hi">拉索回归(L1) </strong></p><p id="0a1f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">lasso的意思是“最小绝对收缩选择算子”,它类似于岭回归，唯一的区别是lasso使用系数的绝对值，它使冗余特征的系数为零，这意味着它有助于选择最佳特征。</p><figure class="kh ki kj kk fd ii er es paragraph-image"><div class="er es kg"><img src="../Images/ecbe11bb71f046292a786f6dbdd75362.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*eafPlv3B9wVO2PPLlom2tg.png"/></div></figure><p id="cf6f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">即使对于'<strong class="iw hi"> λ' </strong>的小值，特征的系数减小到零，我们需要选择最佳的'<strong class="iw hi"> λ </strong>值。</p><p id="aa2d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">注意:我们也可以使用<strong class="iw hi">弹性网</strong>，它是脊线和套索回归的组合。假设我们有一个500个变量的情况，如果我们使用ridge，它只会减少系数的大小，如果我们使用lasso，它会将冗余要素的系数减少到零(要素选择)，这将导致信息丢失，这里使用弹性网很好，它在大型数据集上表现良好。</p></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><p id="1916" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们关于正则化回归的文章到此结束。我希望这篇文章能帮助你理解正则化回归。</p></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><p id="4452" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是我的第一篇媒体文章，如果你喜欢这篇文章，请在下面为这篇文章鼓掌以示支持，这真的激励我写更多有趣的文章，如果你有任何问题，请在下面留下评论，我很乐意听到你的意见。</p></div><div class="ab cl kl km go kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ha hb hc hd he"><p id="9935" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我是一名数据科学爱好者，目前正在班加罗尔国际信息技术学院攻读机器学习和人工智能的pg文凭。</p><p id="b769" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你也可以在www.linkedin.com/in/manoj-gadde找到我</p></div></div>    
</body>
</html>