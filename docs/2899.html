<html>
<head>
<title>Road to Dimensionality Reduction expert</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维专家之路</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/road-to-dimensionality-reduction-expert-7f4aa425c140?source=collection_archive---------8-----------------------#2021-05-23">https://medium.com/analytics-vidhya/road-to-dimensionality-reduction-expert-7f4aa425c140?source=collection_archive---------8-----------------------#2021-05-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/af28384411a9aeddd41e4d0257fd1240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fh8ycOEd4OWwpptL9n6fCQ.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Haneul Kim摄</figcaption></figure><h1 id="c6f6" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">目录</h1><blockquote class="jr js jt"><p id="0c4d" class="ju jv jw jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ha bi translated"><a class="ae kt" href="#f9cb" rel="noopener ugc nofollow"> 1。简介</a></p><p id="a89f" class="ju jv jw jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ha bi translated"><a class="ae kt" href="#06c6" rel="noopener ugc nofollow"> 2。维度的诅咒</a></p><p id="e788" class="ju jv jw jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ha bi translated"><a class="ae kt" href="#7c3f" rel="noopener ugc nofollow"> 3。线性降维技术</a></p><p id="115b" class="ju jv jw jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ha bi translated"><a class="ae kt" href="#8e79" rel="noopener ugc nofollow"> 4。非线性降维技术</a></p><p id="8b04" class="ju jv jw jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ha bi translated"><a class="ae kt" href="#1f4b" rel="noopener ugc nofollow"> 5。结论</a></p></blockquote></div><div class="ab cl ku kv go kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="ha hb hc hd he"><h1 id="f9cb" class="it iu hh bd iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm lf jo jp jq bi translated"><strong class="ak">简介</strong></h1><p id="e7fb" class="pw-post-body-paragraph ju jv hh jx b jy lg ka kb kc lh ke kf li lj ki kj lk ll km kn lm ln kq kr ks ha bi translated">大数据是3Vs、<strong class="jx hi">量</strong>、<strong class="jx hi">种</strong>、<strong class="jx hi">速</strong>高的数据。大数据有很多好处，但它也带来了一些后果，比如增加了<strong class="jx hi">计算复杂性</strong>，遏制了<strong class="jx hi">噪声数据</strong>，并且<strong class="jx hi">难以可视化</strong>。</p></div><div class="ab cl ku kv go kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="ha hb hc hd he"><h1 id="06c6" class="it iu hh bd iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm lf jo jp jq bi translated">维度的诅咒</h1><p id="711d" class="pw-post-body-paragraph ju jv hh jx b jy lg ka kb kc lh ke kf li lj ki kj lk ll km kn lm ln kq kr ks ha bi translated">维数高会导致众所周知的“维数灾难”问题，即<strong class="jx hi">随着维数的增加，行数必须呈指数增长，以便具有相同的解释能力</strong>。例如，如果你有10000x100个数据集，如果你增加2个维度，你将需要10000 x102个数据集才能有相同的解释能力。</p><p id="a622" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">从左至右看下图:</p><ol class=""><li id="8e02" class="lo lp hh jx b jy jz kc kd li lq lk lr lm ls ks lt lu lv lw bi translated">dimension = 1，2数据(行数)需要获取表示数据间距离= 1的信息。</li><li id="cd4d" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks lt lu lv lw bi translated">维度= 2，2**2=4获取相同信息所需的数据(数据之间的距离= 1)。</li><li id="2bf2" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks lt lu lv lw bi translated">维度= 3，2**3=8获取相同信息所需的数据(数据之间的距离= 1)。</li><li id="863c" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks lt lu lv lw bi translated">维度= 4，2**4=16，以此类推…</li></ol><figure class="md me mf mg fd ii er es paragraph-image"><div class="er es mc"><img src="../Images/0c3cde764ff1726987103822cb245849.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*upSSxREqeJqVkEtLTjjvcQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae kt" href="https://www.youtube.com/watch?v=ytRmxBvyGG0&amp;t=650s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=ytRmxBvyGG0&amp;t = 650s</a></figcaption></figure><p id="38e1" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated"><strong class="jx hi">其他问题</strong>:</p><ul class=""><li id="afa2" class="lo lp hh jx b jy jz kc kd li lq lk lr lm ls ks mh lu lv lw bi translated">数据中有噪声的百分比更高</li><li id="3079" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">增加计算负担</li><li id="c1cb" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">需要更多数据来确保泛化能力(如上所述)</li></ul></div><div class="ab cl ku kv go kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="ha hb hc hd he"><p id="8366" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">因此，随着数据量的增加，需要使用降维技术来提取关键信息并减少计算时间。</p><p id="f1aa" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">它们分为两种主要类型:</p><ul class=""><li id="acfe" class="lo lp hh jx b jy jz kc kd li lq lk lr lm ls ks mh lu lv lw bi translated"><strong class="jx hi">特征选择</strong> : <strong class="jx hi">选择</strong>特征子集以减少尺寸。</li><li id="e4c3" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated"><strong class="jx hi">特征提取</strong> : <strong class="jx hi">从原始特征创建</strong>新的特征集。它涉及特征的转换，因此不是完全可逆的(可以逆转，但会丢失一些信息)。</li></ul><p id="05ae" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">特征提取有两种方式:</p><ul class=""><li id="bc3e" class="lo lp hh jx b jy jz kc kd li lq lk lr lm ls ks mh lu lv lw bi translated">线性:使用<strong class="jx hi">线性函数</strong>将高维转换为低维，当数据是线性可分时效果很好。这些被称为线性降维技术(LDRTs)。</li><li id="970b" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">非线性:不属于LDRTs的任何东西，当数据不是线性可分的时候使用= &gt;复杂的非线性结构。这些技术被称为NLDRTs。</li></ul><p id="9086" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">线性技术计算效率高并且能够找到全局最优，但是它不能理解数据的非线性结构。这就是更复杂的非线性降维的用武之地，尽管计算量更大，但它能够找出数据的非线性结构。</p></div><div class="ab cl ku kv go kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="ha hb hc hd he"><h1 id="7c3f" class="it iu hh bd iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm lf jo jp jq bi translated"><strong class="ak">线性DRTs </strong></h1><p id="5c69" class="pw-post-body-paragraph ju jv hh jx b jy lg ka kb kc lh ke kf li lj ki kj lk ll km kn lm ln kq kr ks ha bi translated"><strong class="jx hi">主成分分析</strong></p><ul class=""><li id="e7f9" class="lo lp hh jx b jy jz kc kd li lq lk lr lm ls ks mh lu lv lw bi translated">目标是通过执行解释大多数数据差异的线性变换来找到特征，这些特征被称为主成分(PCs)。</li><li id="a2c5" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">应用:图像、语音处理、可视化、EDA和机器人传感器数据。</li></ul><p id="08b7" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">给定n(特征数)，PCA将输出n个PC(第一个PC，第二个PC，…第n个PC)，其中第一个PC解释了原始数据中最大的变化量，因此我们选择n-k个PC来降低维数。通常，在实践中，我们保留95%方差的PC数量，例如，当n=100时，只需要前50个PC来解释我们数据中95%的方差，那么k将被设置为50，这样我们就剩下原始维度的一半大小。</p><p id="6ec3" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated"><strong class="jx hi">奇异值分解</strong></p><ul class=""><li id="9f1d" class="lo lp hh jx b jy jz kc kd li lq lk lr lm ls ks mh lu lv lw bi translated">计算量大且对异常值敏感。</li><li id="3e16" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">应用:数字图像处理、模式识别、自然语言处理、文本摘要、推荐系统等。…</li></ul><p id="3286" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">所有实矩阵a都可以分解成唯一的USV^T.，也就是说，有一种方法可以通过再次旋转(u)、缩放(s)和rotating(V^T将NxN单位矩阵变换成a。矩阵S是一个对角矩阵，奇异值按降序排列，因此使用的奇异值越多，矩阵看起来就越接近A。因此，就像PCA一样，我们只保留足够数量的奇异值，以构建足够接近原始数据(A)的矩阵。</p><p id="b338" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated"><strong class="jx hi">潜在语义分析(LSA) </strong></p><ul class=""><li id="4576" class="lo lp hh jx b jy jz kc kd li lq lk lr lm ls ks mh lu lv lw bi translated">用于从大型文本文档中提取相关文本。</li><li id="1b9d" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">尝试学习文本的语义表示和单词之间的联系。</li><li id="26b9" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">使用SVD来实现上述目标。</li><li id="4c18" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">应用:术语分类、摘要、单词搜索、查找同义词和反义词、分析开药时患者和医生之间的交流等</li></ul><p id="b66a" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated"><strong class="jx hi">局部保留投影(LPP) </strong></p><ul class=""><li id="9828" class="lo lp hh jx b jy jz kc kd li lq lk lr lm ls ks mh lu lv lw bi translated">目标是保留数据集的邻域结构，类似于LLE(NDRT)，但是LPP是线性的，因此在环境空间的任何地方都可以定义。</li><li id="98d1" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">应用:图像检索、图像和视频分类、人脸/语音/模式识别和计算机视觉。</li></ul><p id="11e1" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">我并不完全了解LPP，但只是想提一下，因为它有很多应用。如果您有兴趣深入了解LPP，参考资料部分将列出何晓飞的著名论文《局部保存投影》,请阅读！</p><p id="d915" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated"><strong class="jx hi">独立成分分析</strong></p><ul class=""><li id="1d25" class="lo lp hh jx b jy jz kc kd li lq lk lr lm ls ks mh lu lv lw bi translated">目标是找到独立的组件。</li><li id="30b6" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">用于分隔信息。而PCA压缩信息。</li><li id="ba5d" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">应用:信号处理，消除噪音，以及许多科学领域。</li></ul><p id="bcd5" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">与PCA的唯一区别是P被独立变量取代。它将原始特征映射到新的特征空间。新特征空间中的特征必须尽可能独立，同时保留原始特征空间中的许多信息。注意，映射特征空间(独立分量)中的数据被假设为<strong class="jx hi">非高斯</strong>。</p></div><div class="ab cl ku kv go kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="ha hb hc hd he"><h1 id="8e79" class="it iu hh bd iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm lf jo jp jq bi translated"><strong class="ak">非线性DRTs </strong></h1><p id="162f" class="pw-post-body-paragraph ju jv hh jx b jy lg ka kb kc lh ke kf li lj ki kj lk ll km kn lm ln kq kr ks ha bi translated"><strong class="jx hi">内核主成分分析(KPCA) </strong></p><ul class=""><li id="ba57" class="lo lp hh jx b jy jz kc kd li lq lk lr lm ls ks mh lu lv lw bi translated">PCA的扩展，用于处理不可线性分离的数据。</li><li id="4479" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">应用:健康、传感器和图像数据</li></ul><p id="3da0" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">使用核方法增加维数，使数据成为线性可分的，然后进行主成分分析。这是可能的原因，因为核方法允许创建线性判定边界，当它被投影回原始空间时，它变成非线性判定边界，因此我们可以使用非线性判定边界成功地分离非线性可分离数据。有多种核，如高斯核、多项式核和径向基核，根据您的数据进行选择。</p><p id="3097" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated"><strong class="jx hi"> t分布随机邻居嵌入(t-SNE) </strong></p><ul class=""><li id="a073" class="lo lp hh jx b jy jz kc kd li lq lk lr lm ls ks mh lu lv lw bi translated">在保留数据簇的同时降低维度。</li><li id="500f" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">基于分布之间的匹配距离。</li><li id="084f" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">非常适合可视化具有非线性结构的数据。</li><li id="9f21" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">应用:生物数据和各种科学领域。</li></ul><p id="0256" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">它的目标是在降维的同时保持相似性度量。相似性度量是我们想要优化的成本函数(Kullback-Liebler散度),但是由于非凸性，需要良好的初始化来达到全局最优。</p><p id="dee9" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated"><strong class="jx hi">等距映射(Isomap) </strong></p><ul class=""><li id="a069" class="lo lp hh jx b jy jz kc kd li lq lk lr lm ls ks mh lu lv lw bi translated">流行的NDRT在许多领域表现出高性能。</li><li id="b5f2" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">属于多种学习方法。</li><li id="086a" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">目标是在保持<strong class="jx hi">测地线距离</strong>的同时减少尺寸。</li><li id="a7f0" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">使用邻域图计算数据点之间的测地线距离。</li></ul><figure class="md me mf mg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mi"><img src="../Images/38554de4c8c41d1dbf9b372e9cfbf086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q1lYE5pJ6QyERx5tQrJBSg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae kt" href="https://www.youtube.com/watch?v=3FAAILDbDd8&amp;t=918s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=3FAAILDbDd8&amp;t = 918s</a></figcaption></figure><p id="0646" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">图A显示了原始数据结构，在给定的数据结构中，使用欧几里得距离(红线)计算A-B之间的距离是不正确的，A-B之间的距离应使用蓝线来测量。为了实现这一点，Isomap通过连接边和它的相邻点来创建一个图，然后使用最短路径算法从A-B(图C中的红线)获得。</p></div><div class="ab cl ku kv go kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="ha hb hc hd he"><h1 id="1f4b" class="it iu hh bd iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm lf jo jp jq bi translated">结论</h1><p id="8528" class="pw-post-body-paragraph ju jv hh jx b jy lg ka kb kc lh ke kf li lj ki kj lk ll km kn lm ln kq kr ks ha bi translated">这个博客的目标是获得不同类型的降维技术的广泛观点。这些技术是为处理特定数据而开发的，这意味着没有一个DRT能很好地处理所有数据。提到的所有维度技术都有多种变体来满足更具体的需求，请参考我在下面链接的研究论文(第一参考文献)了解更多信息。</p><p id="c2b4" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">由于计算速度更快(随着数据集变大，执行DRT需要很长时间)，从简单的技术开始总是一个好的做法，它会告诉你下一步需要走哪个方向。</p><p id="ba6b" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">对每种技术的深入解释以及用python例子对非线性和线性DRT的比较是下一篇博客的主题，敬请关注！</p><p id="79bc" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated">感谢您的阅读，如果您发现任何不正确的信息，请评论:)</p></div><div class="ab cl ku kv go kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="ha hb hc hd he"><p id="e4ce" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf li kh ki kj lk kl km kn lm kp kq kr ks ha bi translated"><strong class="jx hi">参考文献:</strong></p><ul class=""><li id="a937" class="lo lp hh jx b jy jz kc kd li lq lk lr lm ls ks mh lu lv lw bi translated"><a class="ae kt" href="https://www.sciencedirect.com/science/article/pii/S156625351930377X" rel="noopener ugc nofollow" target="_blank">高维数据降维技术综述及比较研究</a></li><li id="eaf3" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated"><a class="ae kt" href="https://www.youtube.com/watch?v=DG7YTlGnCEo&amp;ab_channel=ArtificialIntelligence-AllinOneArtificialIntelligence-AllinOne" rel="noopener ugc nofollow" target="_blank">奇异值分解和图像压缩</a></li><li id="5af0" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated"><a class="ae kt" href="https://www.youtube.com/watch?v=P5mlg91as1c&amp;t=3s&amp;ab_channel=ArtificialIntelligence-AllinOne" rel="noopener ugc nofollow" target="_blank">奇异值分解——斯坦福大学</a></li><li id="b18a" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated"><a class="ae kt" href="https://papers.nips.cc/paper/2003/file/d69116f8b0140cdeb1f99a4d5096ffe4-Paper.pdf" rel="noopener ugc nofollow" target="_blank">局部保留投影— NIPS </a></li><li id="82a2" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated"><a class="ae kt" href="https://www.youtube.com/watch?v=2WY7wCghSVI&amp;ab_channel=Udacity" rel="noopener ugc nofollow" target="_blank"> ICA </a></li><li id="10b1" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated"><a class="ae kt" href="https://www.youtube.com/watch?v=GgLaP4Des1Q&amp;ab_channel=ShawhinTalebiShawhinTalebi" rel="noopener ugc nofollow" target="_blank"> ICA与鸡尾酒会示例</a></li><li id="4fba" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated"><a class="ae kt" href="https://nirpyresearch.com/pca-kernel-pca-explained/" rel="noopener ugc nofollow" target="_blank"> PCA与内核PCA </a></li><li id="ba33" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated"><a class="ae kt" href="https://www.youtube.com/watch?v=HbDHohXPLnU&amp;t=485s&amp;ab_channel=caltechcaltech" rel="noopener ugc nofollow" target="_blank">非线性降维:KPCA —加州理工</a></li><li id="836b" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated">t-SNE，解释清楚— StatQuest </li><li id="2f46" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated"><a class="ae kt" href="https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1" rel="noopener" target="_blank">使用Python示例介绍t-SNE</a></li><li id="3fe6" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated"><a class="ae kt" href="https://scikit-learn.org/stable/modules/manifold.html" rel="noopener ugc nofollow" target="_blank">流形学习</a></li><li id="632f" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated"><a class="ae kt" href="https://www.youtube.com/watch?v=3FAAILDbDd8&amp;t=918s" rel="noopener ugc nofollow" target="_blank"> ISOMAP &amp; LLE —高丽大学(韩语)</a></li><li id="fdc2" class="lo lp hh jx b jy lx kc ly li lz lk ma lm mb ks mh lu lv lw bi translated"><a class="ae kt" href="https://www.youtube.com/watch?v=ytRmxBvyGG0&amp;t=650s" rel="noopener ugc nofollow" target="_blank">降维概述(维数灾难)——高丽大学(韩语)</a></li></ul></div></div>    
</body>
</html>