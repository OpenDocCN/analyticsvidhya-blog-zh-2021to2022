<html>
<head>
<title>Activation Functions-A General Overview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">激活功能-概述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-a-general-overview-c836ca759456?source=collection_archive---------15-----------------------#2021-01-29">https://medium.com/analytics-vidhya/activation-functions-a-general-overview-c836ca759456?source=collection_archive---------15-----------------------#2021-01-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/105857d9e08807fbe2952d62f6ecda70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*1PmWWl5LaprEbC2o"/></div></figure><p id="87bd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">神经网络由多层“神经元”组成，这些神经元使用各自的权重和偏好来学习关于输入的信息。激活网络是其中的一部分——它将非线性引入模型，帮助它模拟更复杂的任务。</p><p id="7b96" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">它还涉及反向传播，允许模型根据损失更新其网络权重。权重和激活函数的偏导数确定权重和偏差将如何以及在什么方向上通过梯度下降而改变。</p><p id="c9d3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们在神经网络中使用三种标准激活函数:</p><h1 id="29f2" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">Sigmoid函数(包括Tanh)</h1><p id="32dd" class="pw-post-body-paragraph il im hh in b io kh iq ir is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji ha bi translated">主要用于二元分类问题，有两种类型的Sigmoid函数- Sigmoid和Tanh，或双曲正切。</p><p id="4d77" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Sigmoid用于逻辑回归并将数据归一化到[0，1]范围内-与概率空间相同。在二元分类中，如果回归值大于或小于0.5，则结果为1或0。</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div class="er es km"><img src="../Images/bbffb26f2744babef3037af3c8366264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*mohvBRtCAc1ZN5ld.png"/></div></figure><p id="671f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Tanh是一个缩放的Sigmoid函数，其输出范围在[-1，1]之间。由于其数值上更重的导数、计算效率和零中心性，它被认为比未缩放的sigmoid函数更好。特别是，它的零中心使权重能够在正负两个方向学习，并使输出居中。这个属性也有助于模型更快地收敛。</p><p id="a22d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">由于它们的形状，这两个函数都存在梯度消失的问题——在任一方向上的极端输入都会导致导数收敛到零。在应用链式法则足够多次后，梯度下降就几乎不存在了。人们只需认识到0.01乘以自身收敛到0的速度有多快，就可以看到这一点。</p><h1 id="d493" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">ReLU(整流线性单元)和泄漏ReLU</h1><p id="22a7" class="pw-post-body-paragraph il im hh in b io kh iq ir is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji ha bi translated">相比之下，ReLU通过为大输入提供更响亮的信号来解决消失梯度问题，并为大于0的值分配常数导数。因此，对于小于零的数字，神经元根本不会被激活。</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div class="er es kr"><img src="../Images/f92b2da5974ee53c8941f8541738ed7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/0*WjsciXOjluC6c0Fx.png"/></div></figure><p id="8a6f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">失活的神经元也被称为“死”神经元。当学习速率太高时，太多的神经元死亡，这导致消失梯度问题的另一个实例。</p><p id="5908" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这种情况下的解决方案？修改你的超参数——或泄漏的ReLU</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div class="er es ks"><img src="../Images/4123d042a4e3b3d4fa7d2fe233873416.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/0*smgoEfsUZC89idOQ"/></div></figure><p id="e4c6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">当输入低于零时，泄漏ReLU是具有固定斜率的ReLU，因此我们仍然有一个梯度。(我们仍然有x的导数&lt;0)</p><p id="30d5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Other equations that mimic the function of the ReLU and ‘fix’ the dead neuron include the ‘swish’ (Sigmoid Linear Unit) by Google or the Exponential linear unit(Elu), which all have significantly higher computational costs than the ReLU but may provide a better learning rate.</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es kt"><img src="../Images/53e6fad22662d290e63c26fb7b260615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yMGdQxiI9gQx6Fr2.png"/></div></div></figure><p id="f37a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Note that the ReLU is <strong class="in hi">总是</strong>用于隐藏层，而<strong class="in hi">从不</strong>用于输出层。</p><h1 id="9a7c" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">Softmax函数</h1><p id="96d1" class="pw-post-body-paragraph il im hh in b io kh iq ir is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji ha bi translated">这个标准化的指数函数将逻辑函数推广到多个维度——非常适合多类回归模型。</p><p id="906c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">它的输出范围是概率空间——它对输出进行除法运算，使它们的和等于1，没有负值。</p><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es kt"><img src="../Images/e331973342151ea5c7a82faa77fb22a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6O-FRRePaHl1ou9_.jpeg"/></div></div></figure><p id="81a5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">将softmax activator用于其最后一层的模型使用逻辑损失-方便，因为softmax的输出是指数的。</p><p id="21a9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">对于负对数损失可能性，当模型对其预测不确定时，损失会很高，即使其预测是正确的。当模型预测准确且非常有信心时，损失是最低的——对softmax试图预测的东西是直观的。</p><p id="b7ca" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">交叉熵损失就是这么做的。</p><p id="61d3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><a class="ae ky" href="https://guandi1995.github.io/Softmax-Function-and-Cross-Entropy-Loss-Function/" rel="noopener ugc nofollow" target="_blank">https://Guandi 1995 . github . io/Softmax-Function-and-Cross-Entropy-Loss-Function/</a>更详细地介绍了实现soft max的代码，并分解了损耗等式。</p><p id="cb26" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">最后，仍然创建新的激活函数，并对其效率和优化进行测试。方程成为激活函数的基本要求是:</p><ol class=""><li id="f68f" class="kz la hh in b io ip is it iw lb ja lc je ld ji le lf lg lh bi translated">它必须是非线性的。</li><li id="9e90" class="kz la hh in b io li is lj iw lk ja ll je lm ji le lf lg lh bi translated">它必须是单调的。(它的斜率必须保持正或负，包括零)</li><li id="2ea3" class="kz la hh in b io li is lj iw lk ja ll je lm ji le lf lg lh bi translated">它必须是可微分的(对于反向传播，它通过损失更新权重)。计算导数的计算强度也会影响整个模型的效率。</li></ol><p id="9b64" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在<a class="ae ky" rel="noopener" href="/ai-in-plain-english/activation-functions-in-neural-networks-3d8211678fb2">https://medium . com/ai-in-plain-English/activation-functions-in-neural-networks-3d 8211678 fb2</a>有对这些激活函数的等式和性质的很好的总结</p></div></div>    
</body>
</html>