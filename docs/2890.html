<html>
<head>
<title>Boosting Trees and AdaBoost: An Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Boosting树和AdaBoost:简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/boosting-trees-and-adaboost-an-introduction-1f20e2bc83c3?source=collection_archive---------15-----------------------#2021-05-22">https://medium.com/analytics-vidhya/boosting-trees-and-adaboost-an-introduction-1f20e2bc83c3?source=collection_archive---------15-----------------------#2021-05-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="8d71" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Boosting是一种迭代组装机制，其中模型被一个接一个地训练。这些模型被称为“糟糕的学习者”，因为它们是基本的预测规则，仅比随机猜测好一点点(即略好于50%的准确度)。</p><p id="30fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">助推背后的核心前提是专注于“困难”情况，或模型无法准确预测的情况。通过倾斜发现的分布来强调这些例子，使它们更有可能出现在样本中。结果，下一个学习慢的人会更专注于纠正这些困难的例子。你总是能从连续的训练中获得一些知识，因为你的学习者总是比随机的学习者做得更好。当所有的基本预测规则合并到一个支配性模型中时，有效的预测器就出现了。</p><p id="04b2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">增强的集成学习器的一个优点是，只要使用简单的学习器，集成模型就非常抗过拟合。在训练模型时，如果您继续为训练数据的完全准确性进行优化，则有过度拟合的风险；这就是为什么交叉验证是必要的，因为它确保模型在新数据输入时仍然可以很好地工作。</p><p id="19c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当你开始过度拟合你的结果时，传统的有效性曲线(作为迭代训练的函数的训练和交叉验证分数的曲线)将会偏离。而助推模型则没有这种发散，非常有用。对此的解释是，boosting将其精力集中在“硬”例子上。考虑分类情况:“硬”例子是那些接近决策边界的例子，因此boosting算法可以集中精力分离这些数据点。结果，决策边界两边的结果之间的差距增大了。正如我们在帮助向量机中发现的那样，宽边距分隔符可以很好地概括，并且可以防止过度拟合。</p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><h1 id="8c72" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">adaboost算法</h1><p id="9eca" class="pw-post-body-paragraph ie if hh ig b ih kh ij ik il ki in io ip kj ir is it kk iv iw ix kl iz ja jb ha bi translated">Freund和Schapire在1996年提出了AdaBoost，即“自适应增强”的缩写，作为第一个实用的增强算法。它关注分类问题，目的是将一组弱分类器转换成一组强分类器。以下是一般步骤:</p><p id="5343" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第一步</strong>:先给每个样本分配权重，目标是均匀分布。给定m个观察值，我们将从平等对待所有数据开始。我们将调整这些权重，将更多的注意力放在模型中表现不佳的观察上。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es km"><img src="../Images/698ac78310935ea4d8e16d3fd11b6fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*A1SApZGtydsgtlUpyB4YXg.png"/></div></figure><p id="f923" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第二步</strong>:在一个随机样本上训练一个差学习器ht(x)，这个随机样本在开始时用均匀分布加权，w1 (i)。</p><p id="1be6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第三步</strong>:确定学习者的错误，ht (x)。我们将误差定义为模型将数据点错误分类的机会；这将隐含地考虑观察值的加权分布。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es ku"><img src="../Images/33ebf3823552b1de9a93c2d18ca6ed02.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*Nh84vzkAyZs2t97aJ2I8NA.png"/></div></figure><p id="f804" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第四步</strong>:根据模型的误差计算模型的分数。这个等式不是一个天真的断言；它是从微积分中推导出来的，目的是计算出一个最小化总误差的公式。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es kv"><img src="../Images/cde35840e63db1be772886f5eb364156.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*HHNbD2Y34L1S4vEH3aDsPA.png"/></div></figure><p id="690c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第五步</strong>:重新分配，重新计算每次观察的权重。错误分类的发现会得到更多的关注。归一化因子zt确保权重相加为1(从而形成分布)。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es kw"><img src="../Images/dbcacadc60f8623590c0f5b63cfd0078.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*PVQ6CEXTJ3zH9qTuozhwIA.png"/></div></figure><p id="d12e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">步骤6 :反复训练模型并重新计算分布，将更多的注意力放在没有正确建模的结果上。</p><p id="62d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从某种意义上来说，每个连续的假设都可以预测xi的等级，要么是-1，要么是1。我们的最终模型将结合这些预测，通过单个模型的性能(由t衡量)进行加权。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es kx"><img src="../Images/4dfb5781522065dc66d89a847345ede1.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*nHn6sOALIwFnDX7fFoottw.png"/></div></div></figure></div></div>    
</body>
</html>