<html>
<head>
<title>Preparing for Optimization Algorithms for data science interviews</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为数据科学面试准备优化算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/prepare-for-optimization-algorithm-for-data-science-interviews-67adb4ae4021?source=collection_archive---------12-----------------------#2021-02-10">https://medium.com/analytics-vidhya/prepare-for-optimization-algorithm-for-data-science-interviews-67adb4ae4021?source=collection_archive---------12-----------------------#2021-02-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/b447992c40e550ad9ac536efd3741415.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kRrOMAD8gZER6PqA31z1zQ.jpeg"/></div></div></figure><p id="500e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们被问到的一个最基本的数据科学面试问题是关于深度学习中不同的优化算法。这篇文章将是关于优化算法的所有信息的一站式草稿。</p><p id="09cb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将涵盖哪些内容？</p><p id="71c3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将讨论以下优化算法及其优缺点:</p><ol class=""><li id="60ce" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">基于权重更新规则的算法。</li></ol><ul class=""><li id="c442" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">梯度下降</li><li id="4dbc" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">基于动量的梯度下降</li><li id="93ba" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">内斯特罗夫加速梯度下降</li></ul><p id="56bc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">2.基于批量的优化算法。</p><ul class=""><li id="dec4" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">批量梯度下降</li><li id="c230" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">小批量梯度下降</li><li id="af8a" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">随机梯度下降</li></ul><p id="b22d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">3.基于自适应学习率的优化算法。</p><ul class=""><li id="69b8" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">阿达格拉德</li><li id="1cf5" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">RMSProp</li><li id="5d2f" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</li></ul><p id="ceaa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，不用浪费太多时间，我们从算法开始:</p><h1 id="5b37" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">基于更新规则的算法</strong></h1><h2 id="12a2" class="la kd hh bd ke lb lc ld ki le lf lg km ja lh li kq je lj lk ku ji ll lm ky ln bi translated">梯度下降</h2><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lo"><img src="../Images/636a2ce9cbfa4dd4fe6f5a2e70547708.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*WqTs8yP6e5vH5Y_jWbd9Ig.jpeg"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">梯度下降(图1.1)</figcaption></figure><p id="d617" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">梯度下降是机器学习领域中最重要的算法之一。梯度下降帮助我们找到权重的最佳值，使得损失函数值最小。</p><p id="3981" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">梯度下降的方程由下式给出</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/24ea5e5c33b05941b53e2b9dc84fe05c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*PGzygNav4Klv_WSxsXFLDA.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">梯度下降算法(图1.2)</figcaption></figure><p id="eaf3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们来理解这个等式</p><ol class=""><li id="dfd8" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">Qj代表我们模型中的任何参数。</li><li id="7168" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm js jt ju jv bi translated"><strong class="ir hi"> α </strong>代表学习率，即您希望更新参数的快慢。</li><li id="33b9" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm js jt ju jv bi translated">∂J(Q)/∂Qj导数是为了发现当我们稍微改变我们的参数时，成本函数是如何变化的(也称为该点切线的斜率)。</li></ol><p id="4fe6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在让我们看看为什么梯度下降有效</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lx"><img src="../Images/c3efa502304c73c68cfcde18f7c76010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AZzu43KoxDamVpWMVW0zfw.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">用于更新权重的梯度下降(图1.3)</figcaption></figure><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es ly"><img src="../Images/120f2c7d86ebb089f44749ac1ef86eb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:60/0*GUSxGQg7Yre3-Lad"/></div></figure><p id="7249" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们来理解上图。假设我们计算梯度，结果是正的。现在，在我们的梯度下降方程中(参见图5)，如果梯度为正，则w值将减小或移向一个较小的正值，反之亦然。用纸和笔检查一下，试着解决这个问题。</p><p id="bcd4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">但是梯度下降有个问题</strong></p><p id="f96b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设在优化我们的模型期间，我们到达一个点，在该点处的斜率或梯度是平坦的或δw→0，在这种情况下，我们对参数的更新将非常小或者没有更新。因为w = w-ξδw。</p><p id="20f4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这种情况下，我们的优化算法可能会卡在平台区域，这会减慢我们的学习过程。反之亦然，在陡峭的区域，梯度下降非常快，因为梯度很高。</p><p id="3626" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，由于图中不同点的优化速度不一致，权重初始化或梯度下降算法的起点可能是我们的模型将如何执行的决定点。</p><p id="8dd6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是不要担心，这个问题有一个解决方案，那就是基于动量的梯度下降。</p></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="a4b4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">基于动量的梯度下降。</strong></p><p id="350d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以我们在梯度下降中发现的一个问题是它在高原地区运行非常慢。基于动量的梯度下降以一种非常优雅的方式处理了这个问题。</p><p id="0dd3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">直觉:</strong>假设你进入一家酒店，想要到达一个特定的房间，你问服务员，他告诉你直走，然后你问接待员，她告诉你怎么走，在问了2-3个人之后，你对这条路变得更加自信，开始迈出更大的步伐，最终到达你的房间。</p><p id="dd58" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，MBGD所做的基本上是记录你之前采取的所有步骤，并使用这些知识来更新权重。</p><p id="bf3f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">基于动量的梯度下降方程由下式给出:</p><pre class="lp lq lr ls fd mg mh mi mj aw mk bi"><span id="9ffb" class="la kd hh mh b fi ml mm l mn mo">Vₜ = γ*Vₜ-₁ + η(Δw)- (1)<br/>wₜ₊₁  = wₜ - vₜ - (2)<br/>combining eq 1 and 2 we get<br/>wₜ₊₁ = wₜ - γ*Vₜ-₁ - η(Δw) -(3)<br/>0&lt;γ&lt;1<br/>if γ*Vₜ-₁ is zero, then we have our old gradient descent algo<br/>Let's see how this works<br/>v₀ = 0<br/>v₁ = γ*v₀ + η(Δw₁) = η(Δw₁) (since v₀ is 0)<br/>v₂ = γ*v₁ + η(Δw₂) = γ*η(Δw₁) + η(Δw₂)<br/>Similarly<br/>vₜ = γ^t-1*ηΔw₁ ..... + ηΔwₜ</span></pre><p id="254f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我知道上面的方程式有点复杂，所以我会给你一个要点。上面的方法被称为指数衰减平均值，因为我们离ηδwₜ越来越远，它的重要性就越来越小。</p><p id="7db6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，最新计算的梯度将具有较大的影响，而先前的梯度将对梯度更新具有较低的影响。</p><p id="ef12" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这种技术也被称为指数衰减加权和。</p><p id="c210" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">基于动量梯度下降的优势</strong></p><ul class=""><li id="8754" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">与梯度下降算法不同，它甚至可以在梯度下降图的平坦区域快速更新。</li></ul><p id="b744" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">基于动量梯度下降的缺点</strong></p><ul class=""><li id="ebce" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">由于获得了较高的动量，它可能会超调并需要时间来收敛</li><li id="6f91" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">它在极小山谷里来回摆动</li></ul><p id="0fba" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">尽管有这些缺点，它仍然比梯度下降法收敛得快。</p></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="5cf7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">内斯特罗夫加速梯度下降</strong></p><p id="dc2f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在基于动量的梯度中，我们看到尽管它收敛得很快，但仍然存在超调问题。这个问题由内斯特罗夫加速梯度下降法处理。</p><p id="e702" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们再次看看基于动量的梯度下降方程。</p><pre class="lp lq lr ls fd mg mh mi mj aw mk bi"><span id="23cd" class="la kd hh mh b fi ml mm l mn mo">wₜ₊₁ = wₜ - γ*Vₜ-₁ - η(Δw)</span></pre><p id="9160" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如你所见，wₜ减去了两项，这导致了wₜ₊₁的大幅更新，进而导致了超调。所以内斯特罗夫所做的就是把上面的等式分成两部分。</p><pre class="lp lq lr ls fd mg mh mi mj aw mk bi"><span id="d1b9" class="la kd hh mh b fi ml mm l mn mo">wₐ = wₜ - γ*Vₜ-₁<br/>wₜ₊₁ = wₐ - η(Δwₐ)<br/>vₜ = γ*Vₜ-₁ + η(Δwₐ)</span></pre><p id="4fea" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们首先进行临时权重更新，并检查我们是否接近我们的收敛点，如果不是，那么我们对权重进行第二次更新。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mp"><img src="../Images/b0f39603e901c5b2ab215c358ff0a7fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jo7YQU3piLJOa3DDShCMMA.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">基于动量法和nesterov梯度下降法的比较</figcaption></figure></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="1a37" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">基于批量的优化算法</strong></p><p id="adbe" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">它基本上意味着在更新权重参数之前，您的模型实际上看到了多少个示例。根据批量大小，它可以分为三类。</p><p id="d546" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">批量梯度下降</strong></p><p id="77b9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在批量梯度下降中，您的模型将累积所有训练示例的梯度，然后更新参数。下面的伪代码可以给出为</p><pre class="lp lq lr ls fd mg mh mi mj aw mk bi"><span id="edec" class="la kd hh mh b fi ml mm l mn mo">X = np.arange(10) # input features having 10 examples<br/>Y = np.arange(10) # output labels having 10 examples</span><span id="98aa" class="la kd hh mh b fi mq mm l mn mo">for a in range(epochs):<br/>    for x,y in zip(X,Y):<br/>       Δw+=calculate_grad(w,x,y)<br/>    w = w - η(Δw)</span></pre><p id="41cb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">批量梯度下降的优势</strong></p><ul class=""><li id="9b1e" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">批量梯度下降可以通过使用矢量化而受益</li><li id="1d20" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">由于权重是在看到所有示例后更新的，因此我们的梯度下降算法出现噪声和曲折移动的可能性较小。</li></ul><p id="cc7a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">批量梯度下降的缺点</strong></p><ul class=""><li id="8482" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">有时数据的大小可能太大而无法在内存中累积，您可能需要额外的内存来处理它。</li><li id="f66a" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">由于噪声较少，可能会出现可能卡在局部极小值的情况。</li></ul></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="bb0d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">随机梯度下降</strong></p><p id="acec" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在随机梯度下降中，我们在每个例子之后更新参数。其伪随机数代码如下所示</p><pre class="lp lq lr ls fd mg mh mi mj aw mk bi"><span id="9f39" class="la kd hh mh b fi ml mm l mn mo">X = np.arange(10) # input features having 10 examples<br/>Y = np.arange(10) # output labels having 10 examples</span><span id="4b55" class="la kd hh mh b fi mq mm l mn mo">for a in range(epochs):<br/>    for x,y in zip(X,Y):<br/>       Δw+=calculate_grad(w,x,y)<br/>       w = w - η(Δw)</span></pre><p id="2a02" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">随机梯度下降的优势</strong></p><ul class=""><li id="f3e5" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">单次通过或单个时期中的多次更新。</li><li id="d169" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">它更容易放入内存，处理时间也少得多。</li><li id="0fe9" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">由于频繁更新，引入了大量噪声，如果梯度下降陷入局部最小值，这可能会有所帮助。</li></ul><p id="d54f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">随机梯度下降的缺点</strong></p><ul class=""><li id="50b0" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">它失去了使用矢量化的优势，因为我们一次只处理一个示例。</li><li id="df77" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">由于大量的噪声，收敛到全局最小值可能需要时间。</li></ul></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="8ab5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">小批量梯度下降</strong></p><p id="6307" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">迷你批量谎言享受两全其美。在这种情况下，我们在看到示例子集后更新参数。</p><pre class="lp lq lr ls fd mg mh mi mj aw mk bi"><span id="056b" class="la kd hh mh b fi ml mm l mn mo">X = np.arange(10) # input features having 10 examples<br/>Y = np.arange(10) # output labels having 10 examples<br/>Batch_size=2</span><span id="ae16" class="la kd hh mh b fi mq mm l mn mo">for a in range(epochs):<br/>    count=0<br/>    for x,y in zip(X,Y):<br/>       count+=1<br/>       Δw+=calculate_grad(w,x,y)<br/>       if(count%Batch_size):<br/>           w = w - η(Δw)</span></pre><p id="ea3c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">小批量梯度下降的优势</strong></p><ul class=""><li id="065b" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">它利用了矢量化的优势，因此处理速度很快</li><li id="f8fd" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">噪音可以通过调整批量大小来控制</li></ul><p id="4d8b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">根据数据集的大小，首选的批量大小为32、64、128。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mr"><img src="../Images/7d84584ea8ca0be8172830be9c52da91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ta1p2_KLXrSOH1BexnGTuw.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">3个算法的GD移动</figcaption></figure><p id="a51a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上述3种算法可以与梯度下降、基于动量的梯度下降、内斯特罗夫梯度下降以及我们将要讨论的其他算法结合使用。</p></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><h1 id="a9f1" class="kc kd hh bd ke kf ms kh ki kj mt kl km kn mu kp kq kr mv kt ku kv mw kx ky kz bi translated"><strong class="ak">自适应学习率</strong></h1><h2 id="38f1" class="la kd hh bd ke lb lc ld ki le lf lg km ja lh li kq je lj lk ku ji ll lm ky ln bi translated">为什么我们需要自适应学习率？</h2><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mx"><img src="../Images/82c59add0e43ed046e4abad8e42e6bd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RwLOqQj__8I6C9yyYTXr6w.png"/></div></div></figure><p id="fed2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上图中，假设X1是稀疏要素，X2是密集要素。</p><p id="9b05" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，如果写出w1和w2的梯度方程，它将是</p><pre class="lp lq lr ls fd mg mh mi mj aw mk bi"><span id="a385" class="la kd hh mh b fi ml mm l mn mo">Δw₁ = f(x)*(1-f(x))*x1<br/>Δw₂ = f(x)*(1-f(x))*x2</span><span id="ee4c" class="la kd hh mh b fi mq mm l mn mo">Gradient Descent for w1 and w2<br/>w1 = w1 - η(Δw₁)<br/>w2 = w2 - η(Δw₂)</span></pre><p id="3b99" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，我们可以看到梯度直接依赖于特征。如果X1是稀疏的，那么其中的大部分元素都是零。在梯度下降算法中，我们可以看到，如果梯度为零，参数将不会更新。因此，对于稀疏特征，我们想要的是每当我们得到任何非零元素时，学习率应该是高的，使得它在w中进行大的更新(以从这些元素中获得最大信息)。类似地，在密集特征的情况下，参数将非常频繁地更新，为此，我们希望我们的学习率低，因为我们的δw₂将高。</p><ul class=""><li id="c67f" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">我们需要根据数据集中不同的要素是稀疏的还是密集的，对它们采用不同的学习率。即密集特征的低η和稀疏特征的高η。</li></ul><p id="8874" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">我们有三种算法来解决同一个问题</strong>。</p><ul class=""><li id="ce12" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">阿达格拉德</li></ul><p id="375f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Adagrad的工作原理是，我们按照参数更新历史的比例衰减参数的学习速率。</p><p id="3d6c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">阿达格拉德的方程式如下</p><pre class="lp lq lr ls fd mg mh mi mj aw mk bi"><span id="1ed9" class="la kd hh mh b fi ml mm l mn mo">Vₜ = Vₜ-₁ + (Δw)^2 - (Eq.1)<br/>wₜ₊₁  = wₜ - (η/<strong class="mh hi">√</strong>Vₜ + ε)*(Δw) - (Eq.2)</span></pre><p id="cac9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">理解上面的等式:</p><ul class=""><li id="cfa9" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">Vₜ是历史术语，现在从等式1我们可以看到，对于稀疏特征，Vₜ将非常小(因为对于大多数元素来说(δw)将为零)。</li><li id="ad61" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">现在，如果我们用η除以Vₜ的一个小值，它将对wₜ₊₁(think做一个大的更新！).</li><li id="262a" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">类似地，对于密集特征，Vₜ将是一个大值(因为对于大多数元素来说(δw)将是非零的)。</li><li id="5ff1" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">现在，如果我们将η除以Vₜ的一个大值，它将对wₜ₊₁(since做一个小的更新。由于除以一个大值，学习率降低到一个小值！).</li></ul><p id="14f5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">但是阿达格拉德有个问题</strong></p><ul class=""><li id="bf52" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">对于密集特征，值Vₜ变得如此之大，以至于它将学习速率降低到非常小的值，因为这使得该特征的学习过程非常慢。这将产生问题，而收敛到最低限度。</li></ul></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="25e9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> RMSProp </strong></p><p id="1c5f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">RMSProp通过控制Vₜ参数不爆炸来解决Adagrad造成的问题。</p><p id="45b8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">RMSProp的计算公式如下</p><pre class="lp lq lr ls fd mg mh mi mj aw mk bi"><span id="2616" class="la kd hh mh b fi ml mm l mn mo">Vₜ = <strong class="mh hi">β*</strong>Vₜ-₁ + (1-β)*(Δw)^2 - (Eq.1)<br/>wₜ₊₁  = wₜ - (η/<strong class="mh hi">√</strong>Vₜ + ε)*(Δw) - (Eq.2)</span></pre><p id="3880" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">引入的测试版控制了历史术语的更新量。拿出你的纸和笔，试着想想这将如何运作。</p></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="effa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">亚当</strong></p><p id="374e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Adam是业界使用最多的优化算法之一。Adam是基于动量的梯度下降和RMSProp的组合</p><p id="5a27" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">亚当的方程式由下式给出</p><pre class="lp lq lr ls fd mg mh mi mj aw mk bi"><span id="4c22" class="la kd hh mh b fi ml mm l mn mo">mₜ = <strong class="mh hi">β</strong>₁<strong class="mh hi">*</strong>Vₜ-₁ + (1-β₁)*(Δw)^2 - (Eq.1)<br/>vₜ = <strong class="mh hi">β</strong>₂<strong class="mh hi">*</strong>Vₜ-₁ + (1-β₂)*(Δw)^2 - Eq.2<br/>wₜ₊₁  = wₜ - (η/<strong class="mh hi">√</strong>Vₜ + ε)*(mₜ) - (Eq.3)</span></pre><ul class=""><li id="1161" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">mₜ负责保持算法的势头，防止它陷入平稳区域。</li><li id="556f" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">Vₜ负责处理自适应学习速率</li></ul></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="c292" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">给你的作业</strong></p><ul class=""><li id="510e" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm jw jt ju jv bi translated">Adam还有一个步骤，即偏差校正。在这方面做一些研究。</li><li id="d5f9" class="jn jo hh ir b is jx iw jy ja jz je ka ji kb jm jw jt ju jv bi translated">除了上述优化算法，还有几个可以帮助训练你的模型更快。探索这些。</li></ul><p id="c376" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你喜欢这篇文章，请点击下面的拍手图标👏👏</p></div></div>    
</body>
</html>