<html>
<head>
<title>Implementation Of Neural Machine Translation Using Attentions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用注意力实现神经机器翻译</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementation-of-neural-machine-translation-using-attentions-3b36337a5b23?source=collection_archive---------6-----------------------#2021-08-23">https://medium.com/analytics-vidhya/implementation-of-neural-machine-translation-using-attentions-3b36337a5b23?source=collection_archive---------6-----------------------#2021-08-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="ef7f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">什么是NMT？</h1><p id="e6ad" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">NMT代表神经机器翻译，它来源于机器翻译。在这里，使用神经网络完成了从一种语言到另一种语言的机器翻译。</p><p id="8d01" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">NMT帮助翻译使用语法，词类，词汇的语言，以找到正确的替代在其他语言。</p><p id="0781" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">示例:</p><p id="732d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">1.你今天好吗？</p><p id="cbcc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">翻译成德语</p><p id="2cf5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">你今天怎么样？</p><p id="2f3e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">以下是JFK在冷战期间制造混乱的演讲的历史性翻译。</p><p id="e831" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">“我是柏林人”，这也意味着“我是一个果冻甜甜圈”。</p><p id="399e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">可选阅读的链接</p><p id="a626" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">"<a class="ae kf" href="https://www.theatlantic.com/magazine/archive/2013/08/the-real-meaning-of-ich-bin-ein-berliner/309500/" rel="noopener ugc nofollow" target="_blank"> Https://Www。theatlantic . Com/Magazine/Archive/2013/08/The-Real-Meaning-Of-Ich-Bin-Ein-Berliner/309500/</a>"</p><p id="48a8" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">上面的例子让我们不仅需要找到正确的单词来替换，还需要匹配单词之间的关系，这样我们就不会最终做出错误的翻译。</p><h1 id="0342" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">神经机器翻译的体系结构</h1><p id="c4d4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">NMT由帮助翻译的编码器和解码器组成。编码器将一个序列作为一种语言的输入，而解码器对给定的输入进行解码，并试图在模型正在翻译的语言中找到合适的替换单词。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/1a1491cf8bee90f79546ef75e3c9842c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tKO8GXT8jIu8WvCj"/></div></div></figure><p id="eec6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">编码器和解码器只不过是两个LSTM-RNN模型，它们的功能是变化的。</p><p id="0d59" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">然而，编码器采用序列到矢量编码，而解码器采用矢量到序列解码。</p><h1 id="e9a1" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">NMT的数据准备</h1><p id="a91c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">数据集包含格式的语言翻译对。</p><p id="c1e5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">类似这样的话“我可以借这本书吗？“这是自由吗？”</p><p id="72a0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这里有一个获得预格式化数据集的链接“【Http://Www.Manythings.Org/Anki/】T2”。</p><p id="d45c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">因此，上面的链接有许多可用于NMT模型的Zip包。</p><p id="aeaf" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">从上面的链接下载数据集后，需要一些步骤来准备数据:</p><ol class=""><li id="7840" class="ks kt hh je b jf ka jj kb jn ku jr kv jv kw jz kx ky kz la bi translated">给每个句子添加一个开始和结束标记，看起来像<sos>和<eos>。</eos></sos></li><li id="1b0c" class="ks kt hh je b jf lb jj lc jn ld jr le jv lf jz kx ky kz la bi translated">通过删除特殊字符和不必要的字符来清理句子。</li><li id="3174" class="ks kt hh je b jf lb jj lc jn ld jr le jv lf jz kx ky kz la bi translated">创建单词索引和倒排单词索引(字典从单词→ Id和Id →单词映射)。</li><li id="00d1" class="ks kt hh je b jf lb jj lc jn ld jr le jv lf jz kx ky kz la bi translated">每个句子的填充必须符合最大长度的句子。</li></ol><h1 id="9661" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">利用注意力实现神经机器翻译</h1><p id="6942" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">注意前训练的NMT模型使用Seq2seq结构。这种结构有一个被称为固定编码器表示的问题，它导致了输出向量的瓶颈。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/dce7f269c9eb3cd43d87a40789b675f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*T1sX6maUEI6zQU7p"/></div></div></figure><p id="31bc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这里你可以看到最新的单词比之前的单词有更高的优先级，这使得NMT模型很难维持这些单词之间的关系。这就是注意力发挥作用的地方。</p><h1 id="5621" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">什么是注意力？</h1><p id="6696" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">注意力顾名思义，它允许模型“关注句子的重要部分”。</p><p id="93de" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">术语“注意力”最初是在论文<a class="ae kf" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">神经机器翻译中引入的，通过联合学习对齐和翻译</a>，其唯一目的是解决固定表示问题。</p><p id="f488" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">注意机制是神经网络的一部分，它使网络只关注重要的数据。在每个解码步骤中，它决定句子的哪个部分更重要。因此，编码器不必将句子中的所有标记都放入一个向量中。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/8970bac87a09e4b399ff81d26afc0603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kSKfmuv7Cc8N2IFv"/></div></div></figure><p id="31f7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如前所述，编码器和解码器部分是相同的，解码器的输出被发送到Softmax激活函数，以获得NMT模型的最终结果。</p><p id="12ae" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">基于句子的重要部分，有各种计算参与每个关注值的计算。</p><h1 id="1d28" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">培训NMT模型</h1><p id="2b16" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在这里，我们将使用编码器和解码器结构来创建NMT模型，并额外注意。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/a93a0751f1a8b06bb260e0a7d7f66e07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*T03TL-R4yDxLS4_1"/></div></div></figure><p id="9fd6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在这个例子中，我们将英语单词翻译成德语单词。输入用0表示，目标用1表示。输入令牌的一个副本被馈送到输入编码器，以被转换成键向量和值向量。目标令牌的另一个副本进入预注意解码器。这里需要注意的是，预注意解码器不是我们之前看到的产生解码输出的解码器。</p><p id="c3a2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">预注意力解码器将预测目标转换到称为查询向量的各种向量空间中。更具体地说，预注意解码器获取目标标记并将它们向右移动一个位置。这就是老师强迫发生的地方。</p><p id="9cd7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">然而，当我们预测时，我们可以只输入正确的目标词(即教师强迫)。</p><p id="c449" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">输入编码器为您提供键和值。一旦有了查询、键和值，就可以计算关注度了。在获得关注层的输出后，剩余块将关注前解码器中生成的查询添加到关注层的结果中。</p><p id="ead2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">然后激活进入第二阶段，使用先前创建的掩码。我们现在在图像的右上角。选择用于删除掩码。它从注意力层(0)和目标令牌的第二个副本(2)获取激活。这些是解码器需要与预测进行比较的真实目标。</p><p id="7dca" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">最后，你只需通过一个解码器LSTM/密集层或一个简单的线性层与你的目标Vocab大小的一切。这使您的输出大小合适。我们使用Log Softmax来计算概率。真正的目标令牌仍然挂在这里，我们将把它与对数概率一起传递，以便与预测相匹配。</p><p id="0291" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">关于NMT的完整代码的链接将在参考资料部分给出。</p><h1 id="f631" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论:</h1><p id="2cf0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在NMT模型上工作有点棘手，因为需要首先理解背后发生的计算，以提高模型性能。</p><p id="9d1d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">参考资料:</p><ol class=""><li id="82f5" class="ks kt hh je b jf ka jj kb jn ku jr kv jv kw jz kx ky kz la bi translated">NMT论文n <a class="ae kf" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">欧洲机器翻译通过联合学习对齐和翻译</a>。</li><li id="4c42" class="ks kt hh je b jf lb jj lc jn ld jr le jv lf jz kx ky kz la bi translated">这里有一个获取数据集的链接“h<a class="ae kf" href="http://www.manythings.org/anki/" rel="noopener ugc nofollow" target="_blank">TTP://www . many things . org/anki/</a>”。</li><li id="72cb" class="ks kt hh je b jf lb jj lc jn ld jr le jv lf jz kx ky kz la bi translated">code Link " h<a class="ae kf" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention#training" rel="noopener ugc nofollow" target="_blank">ttps://www . tensor flow . org/tutorials/text/NMT _ with _ attention # training</a></li><li id="3e73" class="ks kt hh je b jf lb jj lc jn ld jr le jv lf jz kx ky kz la bi translated">NMT车型详解" h<a class="ae kf" href="https://lena-voita.github.io/nlp_course/" rel="noopener ugc nofollow" target="_blank">ttps://Lena-voita . github . io/NLP _ course/seq 2 seq _ and _ attention . html # attention _ intro</a></li></ol></div></div>    
</body>
</html>