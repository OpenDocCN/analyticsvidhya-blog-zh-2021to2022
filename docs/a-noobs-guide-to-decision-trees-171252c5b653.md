# Noob 决策树指南

> 原文：<https://medium.com/analytics-vidhya/a-noobs-guide-to-decision-trees-171252c5b653?source=collection_archive---------20----------------------->

你好，我的伙伴们。今天我为决策树编辑了一个博客，这个概念乍看起来像是一团乱麻，但实际上要简单得多。在这里，我们将掌握决策树的思想和工作原理，以及它是如何让你的生活变得更简单的。

![](img/198f3cd241377f82168367b1dd98ba4a.png)

来源:better.sg

# 介绍

> 它是一种用于分类和回归问题的监督学习算法。它使用其树状表示来直观地显示给定问题时可以做出的所有决策。

用非常简单的术语来说，决策树是一棵代表一个问题所有可能解决方案的树。举个例子，

假设我们有一组鸟、鱼、猫和狗。我们想把所有这些动物归入各自的类别。那么我们可以问什么问题来回答这个问题呢？

我们可以先问，“动物有皮毛吗？”。如果这是真的，那么猫和狗就属于那一类。没有皮毛的动物将会有一个包含鸟类和鱼类的独立群体。

现在，让我们把注意力集中在有毛的动物身上。我们现在问，“那种动物会叫吗？”。如果是的话，它就被归类为狗。如果不是，那它就是一只猫。现在，这一组已经分类，我们进入下一组。

如果动物没有皮毛，我们可以问，“那种动物会飞吗？”如果是的话，那它就是一只鸟，否则它就是一条鱼。这个决策可以用下面给出的树的形式来描述:

![](img/739eee0aab90a43d0daf1647de909523.png)

来源:wiki.tum.de

## 决策树有两种类型:

*   **分类变量决策树:**目标值是分类值。例如，如果我们有一个用于诊断糖尿病的数据集，那么诊断的目标值是“是”或“否”。
*   **连续变量决策树:**目标值是一个像回归中的连续值。例如，根据地下室、一楼、二楼、浴室数量、卧室数量等的平方英尺预测房价。

# 应用程序

*   它用于多变量系统中的数据挖掘和分类。
*   用于开发目标变量的预测算法。
*   用于人口统计数据，以寻找潜在客户。
*   它帮助你规划出不同的选择，并根据不同的情况选择最好的一个。

# 基本术语

*   **根节点:**这是我们开始做决定的地方。它代表整个数据集，然后基于所选问题，我们对其进行拆分。
*   叶节点:这是我们进入死胡同的地方。也可以说，我们不能使用这样的节点获得任何新的信息，因此，不能被分割。
*   **修剪:**删除树中不需要的节点的过程。这是分裂的反义词。
*   **拆分:**是基于某种问题或条件对我们的数据集进行划分的过程。
*   **父/子节点:**被划分的节点是父节点，子节点是从父节点开始的子节点。
*   **分支或子树:**分割树/节点的结果。

# 它是如何工作的？

*我们将为使用熵作为度量之一的决策树实现* ***ID3 算法*** *。* ***我们将讨论涉及分类目标变量的决策树的度量。***

作为一个决策树，它需要问很多问题。但真正的任务是找到需要问的问题的顺序。

**如何选择一个问题，让我们更容易做出下一步的决定？**为此，我们必须引入某些 ***指标*** ，通过这些指标我们可以评估我们的问题并对它们进行排序。

但是要理解这些术语，我们需要理解一些更基本的术语:

![](img/dc4ac4cd224e6c7998b9f2bd1ae813b5.png)

*   **杂质:**它是节点标签同质性的度量。让我们以之前定义的同一个组为例。如果我们给这些动物中的任何一种随机贴上标签(比如猫)(比如鱼)**“错误分类的概率有多大？”。这个问题定义了杂质的概念。**
*   **熵:**它是杂质的量度，定义了数据集中的随机性。如果数据集的一部分属于一个类别，而另一部分属于另一个类别，那么我们可以说我们的数据集是随机的，因为我们无法据此做出任何决定。举个例子，

![](img/e521f7af58f0ae8e27ea2559b6aa5220.png)

来源:www。dataversity.net

在上图中，我们可以看到，与第二幅图像(纯净)相比，将第一幅图像(不纯净)分组的难度更大。它可以计算如下:

![](img/faf168621fec106132303a39a07ddb70.png)

p 是标签为 I 的项目的概率，从 i=1 到 c，其中 c 是组的数量

![](img/badd9c5ab7806ed6d9ff47856e59a83a.png)

如果你想让它更有诗意，那么熵意味着在蒸馏水中加盐，然后将它们分组。它被溶解了，所以它本身不会有什么不同。你得努力把他们分开！！！

既然同义词库和我的繁琐已经结束，我们将解释基于属性选择问题的度量标准，

*   **基尼指数**:衡量杂质的指标，其值介于 0 和 1 之间。其计算方法如下:

![](img/778b4283d5ffbfc4700e50dbf6a97476.png)

其中，pi 是对象被分类到组中的概率。基尼指数只对二元分割有效。基尼系数越高，同质性越高。此指标用于决策树的 CART(分类和回归树)实施。

*   信息增益(Information Gain):它给出了一个属性能给出多少关于类/组的信息的度量。它指定了使用属性可以减少多少熵。

![](img/fbcc88b149ab2ba8b66f08ab5f20fbb2.png)

当我们沿着决策树往下走时，我们希望从高熵(非常不确定)到低熵(分类变得简单)。我们还可以直观地看到，如果儿童熵的加权平均值较小，那么，信息增益一定很高，即我们已经获得了许多新的信息来容易地对项目进行分类。

也就是说，首先选择具有最高信息增益的属性，并且当我们沿着决策树向下时，重复相同的过程。

![](img/54097d698c1f39e7a4729e6c8b8973fc.png)

(来源:towardsdatascience.com)从高熵到低熵。

> 基尼系数比熵和信息增益更受欢迎，因为它不像熵和信息增益那样需要大量的计算。

## 现在我们将看到实施决策树的步骤:

![](img/62d46a0116a37469c24a3e0b06aad836.png)

我们这里的目标变量是“Return”。(来源:blog.quantinsti.com)

1.  对于数据集的每个属性，我们将从中发现信息增益。例如，对于属性“过去趋势”，这里有两种类型的值，即正值和负值。
2.  对于每一种类型，我们根据目标变量找出它们各自的分类。对于“过去趋势”中的正类型，我们有 4 个“上升”和 2 个“下降”。对于负类型，我们有“向下”的所有值。所以，负型是同质的。
3.  对于每种类型，我们将找到熵，然后找到它们的加权平均值。在这个例子中，对于正值，我们的熵将是:(-4/6)log₂(4/6)+(-2/6)log₂(2/6)=(-2/3)log₂(2/3)+(-1/3)log₂(1/3)=0.3899+0.5283=0.9182；同样，对于负值，熵=0
4.  现在，我们发现它的加权平均值是:(6/10)*(0.9182) + (4/10)*0 = 0.5509
5.  找到这个加权平均值后，我们从母体的熵中减去它。然而，对于根节点，我们发现数据集的熵。这是通过找到目标变量中每种类型值的概率，然后对这些概率中的每一个应用熵公式来找到的。
6.  这里，我们的目标变量“回报”有 4 个“上涨”和 6 个“下跌”。
7.  应用熵公式，(-4/10)*log₂(4/10)+(-6/10)*log₂(6/10)= 0.9709
8.  从数据集的熵中减去加权平均，我们得到 0.42，这是从“过去的趋势”中获得的信息。
9.  对所有其他属性执行步骤(1)到(4 ),然后我们选择具有最大信息增益的属性作为根。
10.  对于“未平仓利息”，(-4/6)log₂(4/6)+(-2/6)log₂(2/6)= 0.9182 熵对于“低”类型的值，对于“高”类型的值，我们有 1。加权平均值是(6/10)*0.9182 + (4/10)*1 = 0.9509。获得的信息将是 0.02。对于“交易价值”，我们得到的信息= 0.9709-0.6896=0.2813。
11.  因此，我们选择“过去的趋势”作为我们的根，并相应地分割我们的数据集。我们对“过去趋势”的子节点执行所有这些步骤，并构建一个树，直到我们不能从节点中获得任何信息，或者我们已经达到我们的最大指定深度。

## 优势:

*   该算法不需要对数据集进行预处理。离群值对其结果没有太大影响。
*   适用于大型数据集。
*   能够处理数字和分类数据。
*   很容易解释它的决策规则。

## 缺点:

*   它们倾向于过度拟合，因此不能很好地概括整个数据集。我们使用修剪来避免这种情况。
*   数据集中的一个小变化可能导致树中的大变化，从而导致最终预测的大变化。
*   它只能在训练数据中响应变量的最小和最大范围内进行预测。

所以，我们以此结束我们的课程。我希望你已经更接近理解决策树了。感谢您的阅读，祝您愉快。