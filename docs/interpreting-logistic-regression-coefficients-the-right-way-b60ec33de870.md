# 正确解释逻辑回归系数

> 原文：<https://medium.com/analytics-vidhya/interpreting-logistic-regression-coefficients-the-right-way-b60ec33de870?source=collection_archive---------9----------------------->

学会正确地*解释逻辑回归的系数，并在此过程中自然地导出其成本函数——对数损失！*

*![](img/9fb44bdbdd03b287f99b848b3a97ebb3.png)*

*来源: [Unsplash](https://unsplash.com/photos/UCjEln8rlIc)*

# *概观*

*当可解释性和可解释性对解决方案至关重要时，像逻辑回归这样的模型通常会战胜它们复杂的对应模型。尽管如此，不幸的是，逻辑回归系数并不像通常的线性回归系数那样容易解释。*

*想象一下，选择逻辑回归的唯一原因是可解释性，但却向业务涉众提供了错误的描述。哎哟，这绝对不是一个愉快的场景！*

*在这篇博客中，我描述了我们如何自然地推导出逻辑回归系数的解释，这样就没有必要去记住那些难看的术语了！*

# *解释模型系数*

1.  *让我们从已知的线性回归方程开始:*

```
*y = θ0 + θ1X1 + θ2X2 + θ3X3 +  ….. + θnXn                     (1)*
```

*然而，对于逻辑回归，我们现在的目标是预测一个类别概率值(而不是像线性回归那样预测一个真正连续的 *y* 值)。因此，我们需要一种方法来限制 *y* 的范围为[0，1](而不是原来的范围(-∞，+∞))。*

*2.一个非常好的函数是 Sigmoid 函数( 𝜎 **)，它可以将范围(-∞，+∞)内的任何值转换为[0，1]。让我们利用这一点。
在两侧使用乙状结肠:***

```
*𝜎(y) = 𝜎(θ*X)                                                 (2)
***Therefore, predicted probability* p = 𝜎(θ*X)***
```

*到目前为止，一切顺利。我们现在有了一个线性模型，可以预测给定一组特征 *X* 及其权重 *θ* 的分类概率。这就是逻辑回归所做的，但是还有一些变化。
想知道它们是什么？请继续阅读，了解更多信息。*

*3.对于企业来说，ML 模型的可解释性是至关重要的。回想一下线性回归中我们如何解释它的系数:*

> **假设所有其他预测因子保持不变，预测因子(独立)x 变化 1 个单位，输出(因变量)y 会变化多少？**

*我们想以类似的方式解释逻辑回归系数。不幸的是，我们的系数目前被*包裹*在 sigmoid 函数𝜎(θ*X 之内)使得我们很难构建我们的解释:*

> **假设所有其他预测变量保持不变，预测变量(自变量)x 变化 1 个单位****s 形*** *时，输出(因变量)y 变化多少？**

*▶️听起来很奇怪，对吧？！*1 单位乙状结肠变化究竟是什么？！**

**我们当然想简化这一点。这就是 **logit 函数**帮助我们的地方！**

> **Logit 和 sigmoid 互为倒数。**

**4.在等式两边应用 logit 函数。2:**

```
**logit(*p*) = logit(*𝜎(θ*X)*)                                      (3)**
```

**取消等式右侧的 logit 和 sigmoid (𝜎)。3:**

```
**logit(p) = *θ*X *                                               (4)**
```

**回想一下，根据定义，**logit(p)= log(odds)= log(p/1-p)****

```
**log(p/1-p) = *θ*X                                            (5)***
```

**答对了。我们所有的 *θ* 系数现在都不受 sigmoid 函数的影响。因此，我们可以将我们的系数解释为(以与线性回归相同的方式):**

> ***在所有其他预测因子保持不变的情况下，预测因子(独立)x 的 1 个单位变化，属于某个类别* *的* ***对数几率变化多少？*****

**5.执行上述活动还使我们得到了逻辑回归的漂亮的成本函数— **对数损失(或交叉熵损失)**。**

```
**Loss = -yi * log(*p*) - (1-yi) * log(1-*p*)where *p* = *𝜎(θ*X)***
```

**在其一般形式中，交叉熵被写为:**

```
***Cross Entropy = - Σ yi * log(pi)***
```

**感谢您花时间阅读这篇文章！**