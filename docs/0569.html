<html>
<head>
<title>Q1c. An analysis of the variational quantum classifier using data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Q1c。基于数据的变分量子分类器分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/q1c-an-analysis-of-the-variational-quantum-classifier-using-data-5cd29be55a58?source=collection_archive---------8-----------------------#2021-01-22">https://medium.com/analytics-vidhya/q1c-an-analysis-of-the-variational-quantum-classifier-using-data-5cd29be55a58?source=collection_archive---------8-----------------------#2021-01-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><blockquote class="il im in"><p id="7c92" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">我们之前的博客在:</strong> <a class="ae jn" href="https://rodneyosodo.medium.com/qa2-explaining-variational-quantum-classifiers-b584c3bd7849?source=friends_link&amp;sk=9ccabff95cb8c2ffa7c8c32bd5424a39" rel="noopener"> <strong class="ir hi">讲解变分量子量词</strong></a><strong class="ir hi"/></p></blockquote></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="761f" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">现在你应该知道变分量子分类器是如何工作的了。之前系列的代码在<a class="ae jn" href="https://github.com/0x6f736f646f/variational-quantum-classifier-on-heartattack" rel="noopener ugc nofollow" target="_blank"> Github repo </a></p><h1 id="37fe" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">介绍</h1><p id="806a" class="pw-post-body-paragraph io ip hh ir b is kp iu iv iw kq iy iz jo kr jc jd jp ks jg jh jq kt jk jl jm ha bi translated">在二进制分类中，比方说标记某人是否可能患有心脏病，我们将构建一个函数，该函数接收关于患者的信息，并给出与现实相符的结果。例如，</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ku"><img src="../Images/cf54a3d1b2180f4d619ca1daee77b5db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*eoXLYJdN3tBlRodKt2_jsw.png"/></div></figure><p id="6b5a" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">这种概率分类非常适合量子计算，我们希望建立一个量子态，当测量和后处理时，返回</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lc"><img src="../Images/c5e2d9cb50210a8b88f37da9b7a5fb07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7gItUIkHJNMFiJ3cLTeqxA.png"/></div></div></figure><p id="e4c5" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated"><code class="du lh li lj lk b">P(hear attack = YES)</code></p><p id="c435" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">通过优化电路，你可以找到基于训练数据的最接近现实概率的参数。</p><h1 id="a7ff" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">问题陈述</h1><p id="9019" class="pw-post-body-paragraph io ip hh ir b is kp iu iv iw kq iy iz jo kr jc jd jp ks jg jh jq kt jk jl jm ha bi translated">给定一个关于病人信息的数据集，我们能预测病人是否有可能心脏病发作吗？这是一个二进制分类问题，在<code class="du lh li lj lk b">{0, 1}</code>中有实输入向量<code class="du lh li lj lk b">{x}</code>和二进制输出<code class="du lh li lj lk b">{y} </code>。我们想建立一个量子电路，它的输出是量子态:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es ll"><img src="../Images/8b661965a5e5dcba5d715dfb4ab8d11c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Zuf0_DQI6lQRX-OznASHw.png"/></div></div></figure><h1 id="8617" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">履行</h1><ol class=""><li id="88c0" class="lm ln hh ir b is kp iw kq jo lo jp lp jq lq jm lr ls lt lu bi translated">我们在零状态下初始化我们的电路(所有量子位都在零状态)</li></ol><pre class="kv kw kx ky fd lv lk lw lx aw ly bi"><span id="8b62" class="lz js hh lk b fi ma mb l mc md">self.sv = Statevector.from_label('0' * self.no_qubit)</span></pre><p id="e14e" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">2.我们使用诸如<code class="du lh li lj lk b">ZZFeaturemap, ZFeaturemap</code>或<code class="du lh li lj lk b">PauliFeaturemap</code>的特征图，并根据数据的输入维度和我们想要的重复次数(即电路深度)来选择量子位的数量。我们用1，3，5。</p><p id="a51a" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">3.我们选择变分形式为<code class="du lh li lj lk b">RealAmplitudes</code>，并指定量子位的数量以及我们想要的重复次数。我们使用1、2、4来使模型具有越来越多的可训练参数。</p><p id="9948" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">4.然后，我们将我们的特征图结合到变分电路中。<code class="du lh li lj lk b">ZZfeaturemap and RealAmplitudes both with a depth of 1</code></p><pre class="kv kw kx ky fd lv lk lw lx aw ly bi"><span id="35ea" class="lz js hh lk b fi ma mb l mc md">def prepare_circuit(self):<br/>    """<br/>    Prepares the circuit. Combines an encoding circuit, feature map, to a variational circuit, RealAmplitudes<br/>    :return:<br/>    """<br/>    self.circuit = self.feature_map.combine(self.var_form)</span></pre><p id="5ffe" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">5.我们创建一个函数，将特征图的参数与数据相关联，并将变分电路的参数与传递的参数相关联。这是为了在Qiskit中确保电路中正确的变量与正确的量相关联。</p><pre class="kv kw kx ky fd lv lk lw lx aw ly bi"><span id="e29c" class="lz js hh lk b fi ma mb l mc md">def get_data_dict(self, params, x):<br/>    """<br/>    Assign the params to the variational circuit and the data to the featuremap<br/>    :param params: Parameter for training the variational circuit<br/>    :param x: The data<br/>    :return parameters:<br/>    """<br/>    parameters = {}<br/>    for i, p in enumerate(self.feature_map.ordered_parameters):<br/>        parameters[p] = x[i]<br/>    for i, p in enumerate(self.var_form.ordered_parameters):<br/>        parameters[p] = params[i]<br/>    return parameters</span></pre><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es me"><img src="../Images/a691e8c9bf5ec170c6f5d458a548184d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZhVdtLXyVygYNbDA1j0t4g.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">参数化电路</figcaption></figure><p id="fb36" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">6.我们创建另一个函数来检查传递的位串的奇偶性。如果奇偶校验是偶数，则返回“是”标签，如果奇偶校验是奇数，则返回“否”标签。我们选择这个是因为我们有两个类，奇偶校验对于给定的位串或者返回真或者返回假。还有其他的方法，例如，对于3个类，你可以将bistring转换成一个数字，并通过一个激活函数来传递。或者将电路的期望值解释为概率。需要注意的重要一点是，从量子电路的输出中分配标签有多种方式，你需要证明为什么或如何这样做。在我们的例子中，宇称的想法最初是在这篇非常好的论文(https://arxiv.org/abs/1804.11326)中被激发的，并且细节包含在其中。</p><p id="83b7" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">7.现在我们创建一个函数，返回模型类的概率分布。在多次测量量子电路(即，多次拍摄)后，我们分别合计与“是”和“否”相关的概率，以获得每个标签的概率。</p><pre class="kv kw kx ky fd lv lk lw lx aw ly bi"><span id="a170" class="lz js hh lk b fi ma mb l mc md">def return_probabilities(self, counts):<br/>    """<br/>    Calculates the probabilities of the class label after assigning the label from the bit string measured<br/>    as output<br/>    :type counts: dict<br/>    :param counts: The counts from the measurement of the quantum circuit<br/>    :return result: The probability of each class<br/>    """<br/>    shots = sum(counts.values())<br/>    result = {self.class_labels[0]: 0, self.class_labels[1]: 0}<br/>    for key, item in counts.items():<br/>        label = self.assign_label(key)<br/>        result[label] += counts[key] / shots<br/>    return result</span></pre><p id="6216" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">8.最后，我们创建一个对数据进行分类的函数。它接收数据和参数。对于数据集中的每个数据点，我们将参数分配给特征图，将参数分配给变分回路。然后我们进化我们的系统并储存量子电路。我们储存电路，以便在结束时立即运行它们。我们测量每个电路，并根据位串和类别标签返回概率。</p><pre class="kv kw kx ky fd lv lk lw lx aw ly bi"><span id="8948" class="lz js hh lk b fi ma mb l mc md">def classify(self, x_list, params):<br/>    """<br/>    Assigns the x and params to the quantum circuit the runs a measurement to return the probabilities<br/>    of each class<br/>    :type params: List<br/>    :type x_list: List<br/>    :param x_list: The x data<br/>    :param params: Parameters for optimizing the variational circuit<br/>    :return probs: The probabilities<br/>    """<br/>    qc_list = []<br/>    for x in x_list:<br/>        circ_ = self.circuit.assign_parameters(self.get_data_dict(params, x))<br/>        qc = self.sv.evolve(circ_)<br/>        qc_list += [qc]<br/>        probs = []<br/>    for qc in qc_list:<br/>        counts = qc.to_counts()<br/>        prob = self.return_probabilities(counts)<br/>        probs += [prob]<br/>    return probs</span></pre><h1 id="2a1f" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">结果</h1><p id="889e" class="pw-post-body-paragraph io ip hh ir b is kp iu iv iw kq iy iz jo kr jc jd jp ks jg jh jq kt jk jl jm ha bi translated">数据分类是通过使用IBM框架中的VQC实现版本来执行的，并在提供者模拟器上执行</p><pre class="kv kw kx ky fd lv lk lw lx aw ly bi"><span id="792e" class="lz js hh lk b fi ma mb l mc md">qiskit==0.23.1<br/>qiskit-aer==0.7.1<br/>qiskit-aqua==0.8.1<br/>qiskit-ibmq-provider==0.11.1<br/>qiskit-ignis==0.5.1<br/>qiskit-terra==0.16.1</span></pre><p id="356f" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">使用优化器的实现版本，用1024个镜头执行实验的每个组合。我们在Qiskit中用不同的特征图和深度、不同深度和不同优化器的实振幅变化形式进行了测试。在每种情况下，我们都在训练数据上比较了50次训练迭代后的损失值。我们最好的模型配置是</p><pre class="kv kw kx ky fd lv lk lw lx aw ly bi"><span id="35e5" class="lz js hh lk b fi ma mb l mc md">ZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 5 : Cost: 0.13492279429495616<br/>ZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 3 : Cost: 0.13842958846394343<br/>ZFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 3 : Cost: 0.14097642258192988<br/>ZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 1 : Cost: 0.14262128997684975<br/>ZFeatureMap(4, reps=1) COBYLA(maxiter=50) vdepth 1 : Cost: 0.1430145495411656<br/>ZZFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 5 : Cost: 0.14359757088670677<br/>ZFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 5 : Cost: 0.1460568741051525<br/>ZFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 3 : Cost: 0.14830080135566964<br/>ZFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 5 : Cost: 0.14946706294763648<br/>ZFeatureMap(4, reps=1) COBYLA(maxiter=50) vdepth 3 : Cost: 0.15447151389989414</span></pre><p id="d371" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">从结果来看，深度为2的ZFeatureMap、深度为5的实振幅变分形式和SPSA优化器实现了最低的成本。这些结果似乎表明，导致较低成本函数的特征映射通常是ZFeatureMap。但是这是否意味着ZFeaturemap通常表现得更好呢？</p><h1 id="bb2d" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">问题</h1><h2 id="8489" class="lz js hh bd jt mj mk ml jx mm mn mo kb jo mp mq kf jp mr ms kj jq mt mu kn mv bi translated">1.增加变分形式深度会增加收敛性吗？</h2><ul class=""><li id="fa77" class="lm ln hh ir b is kp iw kq jo lo jp lp jq lq jm mw ls lt lu bi translated">有趣的是，增加变分形式的深度似乎并没有实质性地增加这些模型的收敛性。注意，增加变分形式的深度意味着在模型中引入更多的可训练参数。有人会天真地认为，模型中更多的参数会让我们更好地建模，并捕捉数据中存在的更复杂的关系，但也许这些模型太小了，无法通过更高的参数化来利用这些优势。</li></ul><h2 id="6882" class="lz js hh bd jt mj mk ml jx mm mn mo kb jo mp mq kf jp mr ms kj jq mt mu kn mv bi translated">2.增加特征图深度会增加收敛吗？</h2><ul class=""><li id="8097" class="lm ln hh ir b is kp iw kq jo lo jp lp jq lq jm mw ls lt lu bi translated">当增加<code class="du lh li lj lk b">ZZFeatureMap ADAM (maxiter=50)</code>和<code class="du lh li lj lk b">PauliFeatureMap ADAM(maxiter=50)</code>上的特征图深度时，这确实增加了模型训练的收敛性。其他模型配置没有显著变化(在某些情况下，增加特征图深度实际上几乎线性地减少了收敛——为什么会发生这种情况可以成为一个有趣的研究项目！).</li></ul><h2 id="4837" class="lz js hh bd jt mj mk ml jx mm mn mo kb jo mp mq kf jp mr ms kj jq mt mu kn mv bi translated">3.模型如何在不同的数据集上进行归纳？</h2><ul class=""><li id="a00d" class="lm ln hh ir b is kp iw kq jo lo jp lp jq lq jm mw ls lt lu bi translated">作为最后的实验，我们在iris和wine数据集上对这些结果进行了基准测试。经典机器学习中使用的两个流行数据集，具有相同维度的心脏病发作数据，因此我们也可以使用4个量子位对其建模。这一次，最佳模型配置是:</li></ul><p id="abe3" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated"><strong class="ir hi">虹膜数据集</strong></p><pre class="kv kw kx ky fd lv lk lw lx aw ly bi"><span id="6f89" class="lz js hh lk b fi ma mb l mc md">PauliFeatureMap(4, reps=4) SPSA(max_trials=50) vdepth 3 : Cost: 0.18055905629600544<br/>ZZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 5 : Cost: 0.18949957468013437<br/>ZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 5 : Cost: 0.18975231416858743<br/>ZZFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 3 : Cost: 0.1916829328746686<br/>ZZFeatureMap(4, reps=4) SPSA(max_trials=50) vdepth 3 : Cost: 0.19264230430490895<br/>ZZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 3 : Cost: 0.19356269726482855<br/>ZFeatureMap(4, reps=4) COBYLA(maxiter=50) vdepth 1 : Cost: 0.19415440209151674<br/>ZZFeatureMap(4, reps=4) SPSA(max_trials=50) vdepth 5 : Cost: 0.19598553766368446<br/>ZFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 1 : Cost: 0.19703058320810934<br/>ZFeatureMap(4, reps=4) SPSA(max_trials=50) vdepth 3 : Cost: 0.19970277845347006</span></pre><p id="11a7" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated"><strong class="ir hi">葡萄酒数据集</strong></p><pre class="kv kw kx ky fd lv lk lw lx aw ly bi"><span id="a37a" class="lz js hh lk b fi ma mb l mc md">PauliFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 5 : Cost: 0.1958180042610037<br/>PauliFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 3 : Cost: 0.1962278498243972<br/>PauliFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 3 : Cost: 0.20178754496022344<br/>ZZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 1 : Cost: 0.20615090555639448<br/>PauliFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 1 : Cost: 0.20621624103441463<br/>ZZFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 1 : Cost: 0.20655139975269518<br/>PauliFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 1 : Cost: 0.20655139975269518<br/>ZZFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 1 : Cost: 0.20655139975269518<br/>PauliFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 1 : Cost: 0.20655139975269518<br/>ZFeatureMap(4, reps=4) SPSA(max_trials=50) vdepth 5 : Cost: 0.20674662980116945<br/>ZFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 5 : Cost: 0.2076046292803965<br/>ZZFeatureMap(4, reps=4) SPSA(max_trials=50) vdepth 5 : Cost: 0.20892451316076094</span></pre><h1 id="bb7d" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">讨论</h1><p id="8dc3" class="pw-post-body-paragraph io ip hh ir b is kp iu iv iw kq iy iz jo kr jc jd jp ks jg jh jq kt jk jl jm ha bi translated">这一次，我们最好的模型配置完全不同！令人着迷的是，所使用的数据集似乎需要特定的模型结构。这在直觉上是有道理的，对吗？因为这些量子机器学习模型的第一步是加载数据并将其编码成量子态。如果我们使用不同的数据，根据您拥有的数据类型，可能会有不同的(或更优的)数据编码策略。</p><p id="0390" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">另一件让我惊讶的事情，特别是来自一个经典的ML背景，是SPSA优化器的性能。我本以为更先进的东西，比如亚当，会是明显的赢家。事实完全不是这样。理解为什么SPSA似乎非常适合优化这些量子模型会很酷。</p><p id="7fd4" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">最后，我们只研究了训练数据的损失值。最终，我们还想看看这些量子模型中是否有人擅长一般化。如果一个模型能够在以前从未见过的新数据上表现良好，那么它就具有良好的泛化能力。这种情况的代表通常是我们在测试数据中得到的错误。通过选择这里的最佳配置并在测试集上检查它们的性能，我们可以衡量这些玩具模型的性能并进行归纳，即使在这些小例子中也会非常有趣！</p><p id="27dd" class="pw-post-body-paragraph io ip hh ir b is it iu iv iw ix iy iz jo jb jc jd jp jf jg jh jq jj jk jl jm ha bi translated">我们现在(可悲！)在终点线。我们已经走了这么远，但仍有许多未解决的问题有待发现。如果你对这些工作感兴趣，请随时联系我们，也许我们可以合作一些很酷的东西！希望你已经理解了使用真实世界数据训练量子机器学习算法的管道。感谢您阅读这些帖子，并感谢<a class="ae jn" href="https://scholar.google.com/citations?user=-v3wO_UAAAAJ" rel="noopener ugc nofollow" target="_blank"> Amira Abbas </a>在QOSF计划中指导我。直到下次👋</p></div></div>    
</body>
</html>