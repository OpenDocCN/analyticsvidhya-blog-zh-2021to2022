<html>
<head>
<title>Akira’s Machine Learning News — Issue #34</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Akira的机器学习新闻—第34期</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/akiras-machine-learning-news-issue-34-55111a45fef7?source=collection_archive---------3-----------------------#2021-11-13">https://medium.com/analytics-vidhya/akiras-machine-learning-news-issue-34-55111a45fef7?source=collection_archive---------3-----------------------#2021-11-13</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="6957" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本周特稿/新闻。</p><ul class=""><li id="e0b4" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">过去曾有<a class="ae jl" href="https://arxiv.org/abs/2105.07197" rel="noopener ugc nofollow" target="_blank">的一项研究</a>表明，ViT的分类行为比CNN更像人类，但现在<a class="ae jl" href="https://arxiv.org/abs/2110.07858" rel="noopener ugc nofollow" target="_blank">发表了一项新的研究</a>表明，即使在逐个补丁的基础上受到干扰，ViT也能正确分类。CNN靠的是纹理，ViT靠的是贴片，两者似乎都与人的感知不同。</li><li id="6060" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jh ji jj jk bi translated"><a class="ae jl" href="https://arxiv.org/abs/2106.10860" rel="noopener ugc nofollow" target="_blank">提出了一种将矩阵乘积加速100倍的方法</a>。由于矩阵乘积在深度学习的总计算量中占很大比例，这种技术可能更容易在社会上实现大规模模型。</li></ul><p id="a79f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — — — — — — — — — –</p><p id="c830" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下面的章节中，我将介绍各种文章和论文，不仅仅是关于上述内容，还包括以下五个主题。</p><ol class=""><li id="8831" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jr ji jj jk bi translated">本周特稿/新闻</li><li id="1988" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jr ji jj jk bi translated">机器学习用例</li><li id="6126" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jr ji jj jk bi translated">报纸</li><li id="5ae5" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jr ji jj jk bi translated">机器学习技术相关文章</li></ol><p id="d393" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="3f31" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">1.本周特稿/新闻</h1><p id="6c7a" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2110.07858?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">即使旋转或混洗补丁，ViT也不会失去精度</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://arxiv.org/abs/2110.07858" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">arxiv.org</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl la"><img src="../Images/1f0ebd166d8e8e8d56bbd4135e87789d.png" data-original-src="https://miro.medium.com/v2/0*pO1Zg0ZHFF593JoN"/></div></figure><p id="242c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ld">【2110.07858】通过基于补丁的负增强了解和提高视觉转换器的鲁棒性</em> <br/>由于ViT通过将图像划分为补丁来对图像进行分类，因此作者发现，即使图像在每个补丁中被错误地混洗或旋转，ViT的准确性也不会下降。为了防止这种情况，他们提出了一种数据扩充方法，这种方法通过混合的补丁数据来惩罚正确的分类。</p></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><p id="147c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">【arxiv.org】<strong class="ig hi"/><strong class="ig hi">——</strong><a class="ae jl" href="https://arxiv.org/abs/2106.10860" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"/></a>通过使用查找表的加法来加速矩阵乘积</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es ll"><img src="../Images/b314792d20660a95696017f8a34a7c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n_bbRRyr09l_e6jn1tsn0w.png"/></div></div></figure><p id="840f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ld">【2106.10860】乘矩阵不乘<br/> </em>研究加快矩阵乘法。将一个向量分成若干子部分，并有一个子部分的查找表。通过搜索目标向量的最近邻，将它们求和来计算矩阵乘积。这比通常的矩阵乘积快100倍，而且精度不打折扣。</p></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><p id="7821" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="1aea" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">2.机器学习用例</h1><p id="99ba" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">Photoshop中使用的<a class="ae jl" href="https://thenextweb.com/news/ai-driving-powerful-photoshop-features-and-shaping-adobe-product-strategy-syndication?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"/></a><strong class="ig hi">——</strong><a class="ae jl" href="https://thenextweb.com/news/ai-driving-powerful-photoshop-features-and-shaping-adobe-product-strategy-syndication" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">【thenextweb.com】</strong></a></p><div class="lq lr ez fb ls lt"><a href="https://thenextweb.com/news/ai-driving-powerful-photoshop-features-and-shaping-adobe-product-strategy-syndication" rel="noopener  ugc nofollow" target="_blank"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hi fi z dy ly ea eb lz ed ef hg bi translated">人工智能如何塑造Adobe的产品战略</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">像每年一样，Adobe的Max 2021活动展示了世界领先的产品展示和其他创新…</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">thenextweb.com</p></div></div><div class="mc l"><div class="md l me mf mg mc mh lb lt"/></div></div></a></div><p id="6a32" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一篇介绍Adobe的Photoshop中使用的机器学习功能的文章，其中使用了机器学习技术来自动遮罩复杂的形状，并使用样式转移来转换背景。虽然许多公司都在努力采用机器学习技术，但Adobe的例子可能是一个很好的案例研究。</p></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><p id="3f91" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="c2ef" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">3.机器学习论文</h1><p id="1abd" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2104.01778?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> AST模型，即使当讲话的长度变化很大时也能使用相同的模型</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://arxiv.org/abs/2104.01778" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">arxiv.org</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es mi"><img src="../Images/198d060d164efebe20d7b9ef74c8193b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W_GnpPi0VOWuY5rlGzUcWw.png"/></div></div></figure><p id="f216" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ld">【2104.01778】AST:音频声谱图变换器<br/> </em>提出了一种使用变换器的音频用AST(音频声谱图变换器)。声谱图作为面片输入到变换器。即使当音频长度相差很大(1~10s)时，也可以使用相同的模型，并且在三个数据集上实现了SotA性能。</p></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><p id="647f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2109.14545?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">激活功能调查</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://arxiv.org/abs/2109.14545" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">arxiv.org</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl la"><img src="../Images/63b6f12731d9f8331b01b52302fd5045.png" data-original-src="https://miro.medium.com/v2/0*bGMeiKizMZH79u7c"/></div></figure><p id="cc64" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ld">【2109.14545】深度学习中激活函数的综合调查与性能分析<br/> </em>这是对激活函数的综合调查:ReLU LReLU等。在残差网络中工作良好，并且参数化激活函数具有快速收敛。</p></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><p id="09ff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2108.06084?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">课程学习提高预学语言模型效率</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://arxiv.org/abs/2108.06084" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">arxiv.org</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es mj"><img src="../Images/707ba4a296384ebf892aafed410527a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fwpnt_2OIj9fxXgTZoAqvA.png"/></div></div></figure><p id="38ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ld">【2108.06084】课程学习:高效稳定的亿级GPT模型预训练的正则化方法<br/> </em>一篇声称课程学习在预训练GPT-2这样的大规模自回归语言模型中有效的论文。使用序列长度作为难度指标，用逐渐变长的序列训练模型。它能够实现稳定的学习并提高学习效率。</p></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><p id="b1d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2110.07641?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">在12层深度的ImageNet中达到top-1 80%的准确率</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://arxiv.org/abs/2110.07641" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">arxiv.org</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es mk"><img src="../Images/dea2455bb950b06460b1bc669eada3fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yq1tKCqWJv2SUpva1Hn1lg.png"/></div></div></figure><p id="e1e3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2110.07641]非深度网络<br/>通过增加子网而不是加深来提高准确性的研究。通过创建一个仅共享输入和输出的子网，作者可以在只有12层深度的ImageNet中实现最高80%的准确性。</p></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><p id="fe90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2110.14168?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">提供数学问题数据集</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://arxiv.org/abs/2110.14168" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">arxiv.org</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl la"><img src="../Images/7241a9c49dbe65af1f5ab2a98a729adf.png" data-original-src="https://miro.medium.com/v2/0*Dw0xNoFX7Zm_Eox8"/></div></figure><p id="8d2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ld">【2110.14168】训练验证者解决数学应用题<br/> </em>作者提供了一个8500个小学水平算术的数据集GSM8K，并开发了一个求解方法。由于简单的微调会导致过拟合，所以他们使用了一种叫做verifier的方法，这种方法使用一种模型，这种模型会沿途生成计算，直到得出最终答案，然后对其进行验证。</p></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><p id="5d2d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="c711" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">4.技术文章</h1><p id="6575" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated"><a class="ae jl" href="https://pytorch.org/blog/overview-of-pytorch-autograd-engine/?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">自动微分的解释</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://pytorch.org/blog/overview-of-pytorch-autograd-engine/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">pytorch.org</strong></a></p><div class="lq lr ez fb ls lt"><a href="https://pytorch.org/blog/overview-of-pytorch-autograd-engine/" rel="noopener  ugc nofollow" target="_blank"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hi fi z dy ly ea eb lz ed ef hg bi translated">PyTorch</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">由Preferred Networks，Inc .撰写。这篇博文基于PyTorch版本，尽管它应该适用于旧版本…</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">pytorch.org</p></div></div><div class="mc l"><div class="ml l me mf mg mc mh lb lt"/></div></div></a></div><p id="37f7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PFN解释自动微分的文章。易于理解的代码和图表解释。</p></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><p id="cf48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://www.youtube.com/playlist?list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&amp;utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">deep mind</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://www.youtube.com/playlist?list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">www.youtube.com</strong></a></p><figure class="kv kw kx ky fd kz"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="64cd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">DeepMind解释强化学习的视频列表。有13段视频，每段大约1到2小时。</p></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><p id="abf5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="0f91" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">5.其他主题</h1><p id="96b6" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated"><a class="ae jl" href="https://blogs.microsoft.com/ai/new-azure-openai-service/?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank">【blogs.microsoft.com】</a><a class="ae jl" href="https://blogs.microsoft.com/ai/new-azure-openai-service/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"/></a><strong class="ig hi">——<a class="ae jl" href="https://blogs.microsoft.com/ai/new-azure-openai-service/?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank">GPT-3现在可供更多的人使用</a></strong></p><div class="lq lr ez fb ls lt"><a href="https://blogs.microsoft.com/ai/new-azure-openai-service/" rel="noopener  ugc nofollow" target="_blank"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hi fi z dy ly ea eb lz ed ef hg bi translated">新的Azure OpenAI服务将对强大的GPT-3语言模型的访问与Azure的企业…</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">自从人工智能研究和部署公司OpenAI推出其开创性的GPT-3自然语言模型以来…</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">blogs.microsoft.com</p></div></div><div class="mc l"><div class="mo l me mf mg mc mh lb lt"/></div></div></a></div><p id="0373" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">微软已经宣布，它将把GPT-3作为AzureOpenAI服务发布，扩展OpenAI提供的有限访问权限，让更多人看到它。价格尚未确定。</p></div><div class="ab cl le lf go lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ha hb hc hd he"><h2 id="46c8" class="mp jt hh bd ju mq mr ms jy mt mu mv kc ip mw mx kg it my mz kk ix na nb ko nc bi">— — — — — — — — — — — — — — — — — — –</h2><h1 id="e981" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">其他博客</h1><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl la"><img src="../Images/320b3be2d0988ceedc629811496f20d7.png" data-original-src="https://miro.medium.com/v2/0*Si8TSky0f3JN0Koi"/></div></figure><p id="9a4e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://towardsdatascience.com/machine-learning-2020-summary-84-interesting-papers-articles-45bd45c0d35b?gi=c93267109140&amp;utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener" target="_blank"> <strong class="ig hi">机器学习2020总结:84篇有趣的论文/文章|作者藤井昭弘|走向数据科学</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://towardsdatascience.com/machine-learning-2020-summary-84-interesting-papers-articles-45bd45c0d35b?utm_campaign=Akira's%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter&amp;gi=c93267109140" rel="noopener" target="_blank"><strong class="ig hi">towardsdatascience.com</strong></a><br/>在这篇文章中，我一共呈现了84篇我发现特别有趣的发表于2020年的论文和文章。为了清楚起见，我将它们分成12个部分。我个人对…的总结</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl la"><img src="../Images/bad4b9a86ee20ab3dcc0818e77288819.png" data-original-src="https://miro.medium.com/v2/0*QID-uFibsDKqwsXP"/></div></figure><p id="2f53" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://towardsdatascience.com/recent-developments-and-views-on-computer-vision-x-transformer-ed32a2c72654?gi=5e36973da6db&amp;utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener" target="_blank"> <strong class="ig hi">关于计算机视觉x Transformer的最新发展和观点|作者Akihiro FUJII |走向数据科学</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://towardsdatascience.com/recent-developments-and-views-on-computer-vision-x-transformer-ed32a2c72654?utm_campaign=Akira's%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter&amp;gi=5e36973da6db" rel="noopener" target="_blank"><strong class="ig hi">towardsdatascience.com</strong></a><br/>本文讨论了自视觉变形金刚问世以来，Transformer x计算机视觉研究中的一些有趣的研究和见解。本文的四个主题如下…</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl la"><img src="../Images/05c6ba98dbb16522347cdad52d324763.png" data-original-src="https://miro.medium.com/v2/0*3Q_j-5cl_llW2KyO"/></div></figure><p id="1eac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" rel="noopener" href="/analytics-vidhya/reach-and-limits-of-the-supermassive-model-gpt-3-5012a6ddff00?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter"><strong class="ig hi">medium.com</strong></a><br/>超大质量模型GPT-3的到达和极限|藤井昭弘|分析vid hya | Medium<strong class="ig hi">——</strong><a class="ae jl" rel="noopener" href="/analytics-vidhya/reach-and-limits-of-the-supermassive-model-gpt-3-5012a6ddff00?utm_campaign=Akira%27s+Machine+Learning+News+++&amp;utm_medium=email&amp;utm_source=Revue+newsletter"><strong class="ig hi">超大质量模型GPT-3的到达和极限。在这篇博文中，我将从技术角度解释GPT 3号、GPT 3号取得的成就以及GPT 3号没能做到的事情..</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl la"><img src="../Images/02f36cd5271443f475b734a8a41d4bd8.png" data-original-src="https://miro.medium.com/v2/0*OcgVj20q_d9LN7id"/></div></figure><p id="6567" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://towardsdatascience.com/do-vision-transformers-see-like-convolutional-neural-networks-paper-explained-91b4bd5185c8?gi=c70696d9d536&amp;utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener" target="_blank"> <strong class="ig hi">视觉变形金刚看起来像卷积神经网络吗？(论文解释)| by Akihiro FUJII | 2021年10月|走向数据科学</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://towardsdatascience.com/do-vision-transformers-see-like-convolutional-neural-networks-paper-explained-91b4bd5185c8?gi=c70696d9d536" rel="noopener" target="_blank"><strong class="ig hi">towardsdatascience.com</strong></a><br/>视觉变压器(ViT)近年来势头越来越猛。本文将解释“视觉变形器看起来像卷积神经网络吗？”(Raghu等人，2021年)发表了…</p><h2 id="8905" class="mp jt hh bd ju mq mr ms jy mt mu mv kc ip mw mx kg it my mz kk ix na nb ko nc bi translated"><strong class="ak">—————</strong></h2><h1 id="e456" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">🌟我每周发布时事通讯！请订阅！🌟</h1><div class="lq lr ez fb ls lt"><a href="https://www.getrevue.co/profile/akiratosei" rel="noopener  ugc nofollow" target="_blank"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hi fi z dy ly ea eb lz ed ef hg bi translated">阿基拉的机器学习新闻- Revue</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">由Akira的机器学习新闻-由Akihiro FUJII:制造工程师/机器学习工程师/硕士…</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">www.getrevue.co</p></div></div><div class="mc l"><div class="nd l me mf mg mc mh lb lt"/></div></div></a></div><h2 id="e8e5" class="mp jt hh bd ju mq mr ms jy mt mu mv kc ip mw mx kg it my mz kk ix na nb ko nc bi">— — — — — — — — — — — — — — — — — — –</h2><h1 id="64c1" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">关于我</h1><p id="2a19" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">制造工程师/机器学习工程师/数据科学家/物理学硕士/<a class="ae jl" href="https://t.co/hjHHbG24Ph?amp=1&amp;utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank">http://github.com/AkiraTOSEI/</a></p><p id="ca73" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://www.linkedin.com/in/%E4%BA%AE%E5%AE%8F-%E8%97%A4%E4%BA%95-999868122/" rel="noopener ugc nofollow" target="_blank"> LinkedIn个人资料</a></p><p id="df03" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">推特，我贴一句纸评论。</p></div></div>    
</body>
</html>