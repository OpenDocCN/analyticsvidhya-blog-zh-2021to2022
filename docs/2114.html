<html>
<head>
<title>Gradient Decent in Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归中的梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-decent-in-linear-regression-ec2308439478?source=collection_archive---------11-----------------------#2021-04-05">https://medium.com/analytics-vidhya/gradient-decent-in-linear-regression-ec2308439478?source=collection_archive---------11-----------------------#2021-04-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/d6cf25a3493c875321b05970a4820d07.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*NOYiPo5-rbLuw5HtH45jAw.png"/></div></figure><p id="d4a4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi jj translated"><span class="l jk jl jm bm jn jo jp jq jr di"/>线性回归模型试图用直线来解释因变量(输出变量)和一个或多个自变量(预测变量)之间的关系。</p><p id="7255" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这条直线用下面的公式表示:</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es js"><img src="../Images/49344002f879c16752a4da89847cc298.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*ILAVUsbIpLINjnGZfPL19w.png"/></div></figure><p id="961d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在哪里，</p><p id="e167" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> y: </strong>因变量</p><p id="7766" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> x: </strong>自变量</p><p id="06e9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> m: </strong>直线的斜率(X量增加一个单位，Y增加m.1 = m个单位。)</p><p id="4265" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> c: </strong> y截距(X值为0时Y值为c)</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es jx"><img src="../Images/4c5168d5e8425f5b4da05da24d4539ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*c917pyfOnodoOO8zsVXj2g.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">直线方程</figcaption></figure><p id="a990" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">寻找线性回归方程的第一步是确定两个变量之间是否有关系。我们可以通过使用<strong class="in hi">相关系数</strong>和<strong class="in hi">散点图</strong>来做到这一点。当相关系数显示数据可能能够预测未来的结果，并且数据的散点图似乎形成一条直线时，我们可以使用简单的线性回归来找到预测函数。让我们考虑一个例子。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kc"><img src="../Images/d9556e8203a37bf6dadae37122fcadfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*U9ahhm3KNadQGiTBqPfjtQ.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">销售与营销支出散点图</figcaption></figure><p id="1d85" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">从散点图中我们可以看出，销售和营销支出之间存在线性关系。下一步是在销售和营销之间找到一条直线，解释它们之间的关系。但是可以有多条线通过这些点。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kd"><img src="../Images/76224dda730b6550222caf4ceba9ce3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*e56HILnzEYDeElHO5z6oZQ.png"/></div></figure><p id="8942" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">那么我们如何知道这些线中的哪一条是最佳拟合线呢？这就是我们将在本文中解决的问题。为此，我们首先来看看成本函数。</p><h1 id="0494" class="ke kf hh bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">价值函数</h1><p id="c16d" class="pw-post-body-paragraph il im hh in b io lc iq ir is ld iu iv iw le iy iz ja lf jc jd je lg jg jh ji ha bi translated">成本是我们预测值的误差。我们将使用均方误差函数来计算成本。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es lh"><img src="../Images/b62cf44a6482fa534b7450950418a6ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*SN8YgZ4MPxnTxZDUYKWVUA.png"/></div></figure><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es li"><img src="../Images/b0ab0a6fe08563c054186341669bb75a.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*Hls-F1jj-RSfR-eFs8-Ymw.png"/></div></figure><p id="73f9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们的目标是尽可能降低成本，以找到最佳拟合线。我们不打算尝试所有的<strong class="in hi"> m </strong>和<strong class="in hi"> c </strong>的排列组合(低效方式)来寻找最佳拟合线。为此，我们将使用梯度下降算法。</p><h1 id="ba2e" class="ke kf hh bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">梯度下降算法</h1><p id="be21" class="pw-post-body-paragraph il im hh in b io lc iq ir is ld iu iv iw le iy iz ja lf jc jd je lg jg jh ji ha bi translated">梯度下降是一种在较少的迭代次数中为给定的训练数据集找到最佳拟合线的算法。</p><p id="bb17" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果我们根据MSE绘制m和c，它将获得一个碗形(如下图所示)</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lj"><img src="../Images/7df062f6e772013b49b0f5048d457d44.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*4ov6t0bikVu2xTrJXlJZEw.png"/></div></div></figure><p id="8c7d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">对于m和c的某种组合，我们将得到最小的误差(MSE)。m和c的组合将给出我们的最佳拟合线。</p><p id="29b6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">该算法从m和c的某个值开始(通常从m=0，c=0开始)。我们计算m=0，c=0点的MSE(成本)。假设m=0，c=0时的MSE(成本)为100。然后我们将m和c的值减少一定量(学习步骤)。我们会注意到MSE(成本)的降低。我们将继续这样做，直到我们的损失函数是一个非常小的值或理想的0(这意味着0误差或100%的准确性)。</p><h1 id="449b" class="ke kf hh bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">逐步算法</h1><ol class=""><li id="4b51" class="lo lp hh in b io lc is ld iw lq ja lr je ls ji lt lu lv lw bi translated">设m = 0，c = 0。设L为我们的学习率。为了获得良好的精度，它可以是0.01这样的小值。</li></ol><blockquote class="lx ly lz"><p id="1d93" class="il im ma in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated">学习率给出了梯度下降过程中梯度移动的速度。设置太高会使你的路径不稳定，太低会使收敛缓慢。把它设置为零意味着你的模型没有从梯度中学到任何东西。</p></blockquote><p id="8c6b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">2.计算成本函数相对于m的偏导数。假设成本函数相对于m的偏导数为Dm(m变化很小，成本函数变化多少)</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es me"><img src="../Images/4a696f8657d203711a54351887355358.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*nEsjBQMJSFYHTclhUhgJXg.png"/></div></figure><p id="78fe" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">类似地，让我们找到相对于c的偏导数，让成本函数相对于c的偏导数为Dc(c变化很小，成本函数变化多少)。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/2bf801f0f04d62b034d0dc3028cc8bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*KmcMvLHTl4ZN8r4mTCJryQ.png"/></div></figure><p id="9149" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">3.现在，使用以下等式更新m和c的当前值:</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/e1f09d7c46dd3656825a1f5a07fd2647.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*QCSAUCGY8Rg7eyrx3g2olQ.png"/></div></figure><p id="e3f6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">4.我们将重复这个过程，直到我们的成本函数非常小(理想情况下为0)。</p><p id="d412" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">梯度下降算法</strong>给出m和c的最佳值。利用这些m和c值，我们将得到最佳拟合线的方程，并准备进行预测。</p></div></div>    
</body>
</html>