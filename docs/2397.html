<html>
<head>
<title>Unsupervised Multi-View Stereo — An Emerging Trend</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无监督多视图立体——一种新兴趋势</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/unsupervised-multi-view-stereo-an-emerging-trend-4d3034e23e9e?source=collection_archive---------9-----------------------#2021-04-21">https://medium.com/analytics-vidhya/unsupervised-multi-view-stereo-an-emerging-trend-4d3034e23e9e?source=collection_archive---------9-----------------------#2021-04-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e346" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">多视图立体(MVS)算法旨在利用给定的内在参数和已知的姿态，从一组校准的图像中计算场景的密集3D点云。虽然经典几何方法仍然是该领域中性能最好的方法(例如在<a class="ae jc" href="https://www.eth3d.net/high_res_multi_view" rel="noopener ugc nofollow" target="_blank">高分辨率ETH基准</a>上)，但近年来越来越多的深度神经网络开始出现，并取得了令人难以置信的结果。然而，这些工作中的大部分依赖于地面真实深度进行训练，这既难以获得，也不能扩展到真实世界的情况。基于学习的MVS研究的一个新兴趋势是在没有监督的情况下训练深度网络，借鉴自监督单目深度估计文献的思想。幸运的是，现有的无人监管的MVS框架非常少，以至于它们完全适合发表在一篇博客文章中。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h2 id="e5a8" class="jk jl hh bd jm jn jo jp jq jr js jt ju ip jv jw jx it jy jz ka ix kb kc kd ke bi translated">基于鲁棒光度一致性的无监督多视图立体视觉学习</h2><p id="1ff1" class="pw-post-body-paragraph ie if hh ig b ih kf ij ik il kg in io ip kh ir is it ki iv iw ix kj iz ja jb ha bi translated">在没有监督的情况下学习MVS的第一次尝试出现在CVPR 2019年的一个研讨会上[1]，它立即解决了一个相当明显的问题:对于单目情况，用无监督的损失函数代替有监督的损失函数是否足够？由于MVS数据的性质，答案是否定的。由于单目深度网络通常是在视频上训练的，因此基线较窄，照明大多相似，并且存在有限的遮挡或视点变化。另一方面，MVS算法的输入通常是一组图像，这些图像之间具有显著的视觉变化，应当被鲁棒地处理。</p><figure class="kl km kn ko fd kp er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es kk"><img src="../Images/9c1d2ca95097977389e40dfb8740c6f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*soAJCXWQkbFc3eibLgKXuw.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">典型MVS数据中的遮挡。图片来自[1]。</figcaption></figure><p id="94dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为此，提出并评估了稳健的光度一致性损失。具体来说，他们基于两个简单的观察:</p><ul class=""><li id="4f53" class="la lb hh ig b ih ii il im ip lc it ld ix le jb lf lg lh li bi translated">与亮度相比，图像梯度对于光照变化更加不变，因此原始梯度和扭曲梯度之间的差异应该被明确地惩罚。</li><li id="120a" class="la lb hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">给定像素的光度一致性应该在某些视图中实施，但不是所有视图。从实用的角度来看，通过计算所有的损失图并从它们的堆栈中选择最少的条目，只选择最好的相机。</li></ul><figure class="kl km kn ko fd kp er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es lo"><img src="../Images/57e003b73283847679671a836d7f840e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ur06GZthP-zfNm9p4G8bQw.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">[1]中的稳健光度损失。</figcaption></figure><p id="0758" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总损失函数补充了平滑项和来自单目文献的相同SSIM项，然后用于在DTU数据集上训练MVSNet [2]的相同架构。尽管提供的结果比受监督的同行更差，但这篇开创性的论文开创了一个新趋势。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h2 id="b05b" class="jk jl hh bd jm jn jo jp jq jr js jt ju ip jv jw jx it jy jz ka ix kb kc kd ke bi translated">MVS2:具有多视图对称性的深度无监督多视图立体</h2><p id="4b49" class="pw-post-body-paragraph ie if hh ig b ih kf ij ik il kg in io ip kh ir is it ki iv iw ix kj iz ja jb ha bi translated">另一个作品同时出现在3DV 2019上，号称是第一个无监督的MVS网络[3]。即使这可能不是真的，这种方法本身是非常聪明和新颖的。网络架构再次从MVSNet借用，但该框架建立在以下关键见解上:现有网络是不对称的，这意味着它们使用来自源图像的信息来估计参考视图的深度图，但源视图的深度图是独立估计的。由于这个原因，提出了一种对称框架，其中所有的深度图用共享的权重同时估计，并且明确地加强了跨视图一致性，因为所有的图像都在观察相同的3D几何形状。</p><figure class="kl km kn ko fd kp er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es lp"><img src="../Images/b432c1f4278ce58f0bec2c5a91a10a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Z6RAW6wpJhPv6JoYXTIUg.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">在[3]中提出的对称MVS框架。</figcaption></figure><p id="7b3f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文的主要创新点是提出了由三项组成的跨视图一致性损失:</p><ul class=""><li id="fc63" class="la lb hh ig b ih ii il im ip lc it ld ix le jb lf lg lh li bi translated">成对图像一致性是通过惩罚图像和二阶合成邻居之间的差异(即，原始视图扭曲到另一个视图中，然后再次扭曲回来)来制定的。</li><li id="95a9" class="la lb hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">成对深度一致性迫使特定视点的深度与扭曲到相邻视点的深度一致。</li><li id="b9e4" class="la lb hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">多视图亮度一致性将损失扩展到三个一组的图像。</li></ul><p id="e173" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，通过对图像的深度图和其二阶扭曲深度之间的差进行阈值化来计算多视图遮挡掩模，从而掩蔽损失函数。这些贡献导致了比以前的开创性方法更好的结果，大大缩小了与MVSNet监督版本的差距。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h2 id="ef23" class="jk jl hh bd jm jn jo jp jq jr js jt ju ip jv jw jx it jy jz ka ix kb kc kd ke bi translated">M3VSNET:无监督的多度量多视图立体网络</h2><p id="5b67" class="pw-post-body-paragraph ie if hh ig b ih kf ij ik il kg in io ip kh ir is it ki iv iw ix kj iz ja jb ha bi translated">一年后，这两项首次试验都被一种名为M3VSNet [4]的新方法消化和提炼，这种方法在无监督的MVS中创造了新的艺术水平，同时比M2VS更有效。网络架构再次模仿MVSNet，但有两个关键贡献:</p><ul class=""><li id="2b18" class="la lb hh ig b ih ii il im ip lc it ld ix le jb lf lg lh li bi translated">首先，损失函数也是通过扭曲特征而不仅仅是图像来计算的。这在单目深度估计文献中也被证明是成功的[5]。</li><li id="fcdd" class="la lb hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">其次，受[6]的启发，增加了新的正常深度一致性改进。</li></ul><p id="5d6c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个想法相当简单:将每个点的法线估计为每个像素的成对邻居提供的法线之间的平均值，如下所示。</p><figure class="kl km kn ko fd kp er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es lq"><img src="../Images/0bcc9cc7d1a3b90052f6fcdf9d3f2019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dlC0ZMNAluUtX_ueQjA0Jw.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">每个像素的正常估计程序。图片来自[4]。</figcaption></figure><p id="2aa7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了使深度法线对一致，法线应该正交于给定像素周围的局部表面。这可以通过计算法线本身与连接对应于像素及其每个邻居的3D点的向量之间的角度来容易地确保。每个邻居被充分加权以适应场景的局部几何形状(即，通过简单地惩罚具有大图像梯度的邻居),并且邻居的细化深度作为对先前正交性约束的解的加权平均来获得。这种正则化项，结合多尺度特征度量损失，允许在所有度量上胜过M2VS，甚至在DTU数据集上胜过监督MVSNet。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h2 id="a083" class="jk jl hh bd jm jn jo jp jq jr js jt ju ip jv jw jx it jy jz ka ix kb kc kd ke bi translated">通过有效的共同分割和数据增强的自我监督的多视图立体</h2><p id="ffc5" class="pw-post-body-paragraph ie if hh ig b ih kf ij ik il kg in io ip kh ir is it ki iv iw ix kj iz ja jb ha bi translated">AAAI 2021的另一项最新工作[7](你可以在本次会议上找到我的外卖<a class="ae jc" rel="noopener" href="/analytics-vidhya/3d-reconstruction-news-aaai-2021-5f435cba6718">这里</a>)通过建立在以下观察之上，设立了一个更高的标准:以前的无监督MVS网络强烈依赖于来自不同视图的点之间的颜色恒常性，这导致了模糊的监督(见下图)。具体地说，迄今为止，语义信息从未被考虑过，并且对不同照明条件的鲁棒性也没有被明确地实施。</p><figure class="kl km kn ko fd kp er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es lr"><img src="../Images/c17d21b25511b39f94e16aff3ee363b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SQFjNMliwwt1uu0gPXOq3A.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">现有无监督MVS网络中模糊监督的例子。图片来自[7]。</figcaption></figure><p id="6069" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了缓解这个问题，他们提出用两个先验来指导自我监督:</p><ul class=""><li id="570a" class="la lb hh ig b ih ii il im ip lc it ld ix le jb lf lg lh li bi translated">多视图中的对应点应该语义一致。他们使用基于非负矩阵分解的无监督共同分割方法来实现这一想法，但在更具体的场景中(如自动驾驶)，来自预训练网络的语义地图将做得很好。语义损失项仅仅是给定分割图和扭曲分割图之间的每像素交叉熵损失。</li><li id="4564" class="la lb hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">该网络应该足够健壮，以便从具有不同颜色、照明和模糊的数据中学习。为此，对输入图像应用随机变换并将其投影到源视图，然后计算原始深度图和受损深度图之间的差异。这是一个正则项。</li></ul><p id="7efc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这两项以及常见的光度损失和平滑度损失都有助于训练MVSNet。总体框架在该领域建立了一个新的艺术状态，并且比许多其他监督方法(不仅仅是MVSNet)执行得更好。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h2 id="52d7" class="jk jl hh bd jm jn jo jp jq jr js jt ju ip jv jw jx it jy jz ka ix kb kc kd ke bi translated">额外收获:作为模型改进的自我监督的多视图立体</h2><p id="f594" class="pw-post-body-paragraph ie if hh ig b ih kf ij ik il kg in io ip kh ir is it ki iv iw ix kj iz ja jb ha bi translated">虽然以前的方法侧重于从零开始训练无监督的MVS网络，但最近的一系列研究建议使用自我监督来完善现有模型，即要么在没有监督的情况下进行训练，要么在其他数据集上用地面真理进行训练。目前有两种现有的方法:</p><ul class=""><li id="7710" class="la lb hh ig b ih ii il im ip lc it ld ix le jb lf lg lh li bi translated">第一个在BMVC 2020上展示[8]，包括使用模型不可知的元学习(MAML) [9]来训练具有地面真理的网络，然后根据需要通过对新数据集的自我监督来完善网络。MVSNet也是这项工作的架构选择。</li><li id="08cf" class="la lb hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">第二篇论文将在2021年CVPR会议上发表[10]，它是完全无监督的，这意味着输入数据用于在无监督的情况下生成伪标签，然后这些粗略的深度估计通过自我监督进行细化。具体而言，在自监督框架的每次迭代中，生成深度图，用简单的融合方案将其投影到3D中，网格化，然后再次观察以建立自训练损失。这种迭代过程可以实现令人印象深刻的结果，并与监督网络的性能相当。</li></ul><figure class="kl km kn ko fd kp er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es ls"><img src="../Images/cd1137fd9d66b205ae91b24f0d916c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0XbCH8p49YZy_OvA7T05Q.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">在[10]中形成的迭代自我监督训练方案。</figcaption></figure></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h2 id="6c65" class="jk jl hh bd jm jn jo jp jq jr js jt ju ip jv jw jx it jy jz ka ix kb kc kd ke bi translated">参考</h2><p id="9273" class="pw-post-body-paragraph ie if hh ig b ih kf ij ik il kg in io ip kh ir is it ki iv iw ix kj iz ja jb ha bi translated">[1] Khot等人，<em class="lt">通过鲁棒光度一致性学习无监督多视图立体视觉</em>，CVPR研讨会2019</p><p id="4909" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2]姚等，<em class="lt"> MVSNet:面向非结构化多视点立体的深度推理</em>，2018</p><p id="a05b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3]戴等，<em class="lt"> MVS2:多视点对称的深度无监督多视点立体</em>，3DV 2019</p><p id="522b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4]黄等，<em class="lt"> M3VSNet:无监督多度量多视点立体网络</em>，ArXiV 2020</p><p id="317a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[5]舒等，<em class="lt">深度和自我运动自监督学习的特征度量损失</em>，2020</p><p id="798e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[6]杨等，<em class="lt">边缘感知深度法线一致性几何的无监督学习</em>，ArXiV 2017</p><p id="5842" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[7]徐等，<em class="lt">基于有效共分割和数据增强的自监督多视点立体视觉</em>，2021</p><p id="97ec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[8] Mallick等人，<em class="lt">通过自我监督学习适应多视点立体视觉</em>，BMVC 2020</p><p id="9361" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[9] Finn等人，<em class="lt">用于深度网络快速适应的模型不可知元学习</em>，ICML 2017</p><p id="6566" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[10]杨等.<em class="lt">多视点立体深度推理的自监督学习</em>，2021</p></div></div>    
</body>
</html>