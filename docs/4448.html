<html>
<head>
<title>Linear Regression with one variable</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一元线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-with-one-variable-b5f59f92ab22?source=collection_archive---------0-----------------------#2021-10-16">https://medium.com/analytics-vidhya/linear-regression-with-one-variable-b5f59f92ab22?source=collection_archive---------0-----------------------#2021-10-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="8a8c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在监督学习中，我们被给予一个具有特征和结果集的数据集。回归算法用于预测无法按定义的类别分离的连续值输出。房价预测就是回归问题的例子之一。</p><p id="b36e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">单变量线性回归是最简单的算法之一，当数据只存在一个特征时，它可以预测结果。</p><p id="8774" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们首先用符号来表示这个模型，</p><p id="e5bc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">x =输入数据/特征</p><p id="e928" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y =输出数据</p><p id="6995" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">m =训练数据集中的输入数量</p><p id="fabb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><code class="du jc jd je jf b">Training set → Learning Algorithm → h (hypothesis function)</code></p><p id="7c73" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个假设函数然后为输入x产生一个输出，</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/35af85b57414155866114efb5d384ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/0*tlZLzC01LeeVW6iY.png"/></div></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jo"><img src="../Images/2b0ce2ddb3097001dc9da2b4850b67a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/0*AkMTuEPRrI-5ZwkI.png"/></div></figure><p id="47b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它也可以写成这种形式，表示数学中的斜率函数，</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jp"><img src="../Images/2cefda76d444eb85181532e6d79c0e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/0*WlFOCp-W8zblYZPK.png"/></div></figure><p id="6d7d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中y是预测的输出</p><p id="2664" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">w和b是参数</p><p id="2d24" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">x是给定的输入数据</p><p id="7f87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参数决定模型</strong>的输出。当我们用训练数据集训练模型时，模型会学习这些参数。</p><p id="a31c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个模型不可能100%准确地预测结果，总会有误差。这些误差是正确输出和预测输出之间的差异。它们可以通过成本或损失函数来计算。</p><h1 id="f9f8" class="jq jr hh bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">成本或损失函数</h1><p id="d961" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">成本函数用于决定如何选择参数以获得最佳拟合。</p><p id="bb67" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，使用最小平方损失函数。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es kt"><img src="../Images/04f707b765d62fdb8c099f18ddbb4fa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/0*c5bstf7XwHX0nZJQ.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">成本/损失函数</figcaption></figure><p id="7f49" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了最小化损失函数的误差，我们必须找到引起最小误差的最小参数。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ky"><img src="../Images/6854ef97a5f7f92310c54895d0da06db.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/0*ULJFzQCeCn7NJlXT.png"/></div></figure><p id="659f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将损失或成本函数除以(平均)2m，以降低误差值。这样做不会改变他们之间的关系。为了便于计算梯度下降，平均值减半，因为平方函数的导数项将抵消1/2项。这也被称为平方误差函数或均方误差。它是线性回归问题常用的代价函数之一。</p><p id="d86e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了简化成本函数，我们将假设</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es kz"><img src="../Images/7deabd9769826a9f165c2eacd8a27608.png" data-original-src="https://miro.medium.com/v2/resize:fit:150/format:webp/0*TcWcR7rFMiCK5y7l.png"/></div></figure><p id="6ee7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">损失函数变成了，</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es la"><img src="../Images/ca3132207601aa98af942bb497e1a4dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/0*HfmVAK4zIukCEbee.png"/></div></figure><p id="0802" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们考虑参数= 1时，输入x等于输出y，因此成本函数为0。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lb"><img src="../Images/6434d3b3e3e6a4995f410527b388cbc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*wSBmbNqlyV3QaR6X_ITjvw.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">参数=1时的假设函数，预测输出=实际输出</figcaption></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es kt"><img src="../Images/49934aaa5083324b1d36396df03cd78b.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/0*yPKyEnsKVo-G0Eyq.png"/></div></figure><p id="ac14" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种特殊情况下，损失为零。</p><p id="c639" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们假设参数= 0.5，那么图表将是这样的，</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lc"><img src="../Images/3fe6b778b6816d1e512961a583f9bba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*DKyt03CX4uAXQdaz2fWQbQ.png"/></div></figure><p id="afcd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">损失函数将是，</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ld"><img src="../Images/67e84bb518e67778839953a20772dc05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/0*_PbEdUEVOMN6Qtjn.png"/></div></figure><p id="2afa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">同样的，</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es le"><img src="../Images/d6e72c414f543da9b78b34d8af83fa4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/0*aF4D8ms1Rl7Er-Mp.png"/></div></figure><p id="fb47" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们不断计算不同的参数值时，我们会得到不同的损失函数值。<strong class="ig hi">最后，我们选择产生最小损失函数的参数值作为参数的值。该参数被称为处于其全局最小值。</strong></p><p id="a7b1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，当考虑到这两个参数时，绘图就变成三维的了。损失函数具有最小值的点，即模型考虑的参数值。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lf"><img src="../Images/2beb4a42648e3f6585283ba5aed565e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*UIGSMPZUxbIJ17MaPpkY_w.png"/></div></figure><h1 id="8d8d" class="jq jr hh bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">梯度下降</h1><p id="ea43" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">梯度下降法是寻找使模型损失最小的参数最小值的方法之一。这个方法可以用来最小化任何函数。</p><p id="c8a3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们假设模型中有两个参数。让我们为参数分配不同的值，并找到相应的损失函数。利用获得的参数值和损耗值，我们绘制一个图(3D)。图表如下所示:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lg"><img src="../Images/8f1184133b0e53639b369a48d2266818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*uBaqM-yWZLUw9AZdhcudjg.png"/></div></figure><p id="eb36" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们可以选择图上的任何一点，并开始朝着将达到最低值的路径移动。采用最初选择的点。向其绘制一条切线，并计算直线的斜率。下一步移动固定在坡度值上。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lh"><img src="../Images/c3775608882d48ef78b00a7897bd6e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*RHzyFoAQft85Xzo85yJscA.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">这个图有两个最小值，它们被称为局部最小值。</figcaption></figure><p id="2c26" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数学上，下面的算法可以用来寻找最小点。</p><blockquote class="li lj lk"><p id="4c45" class="ie if ll ig b ih ii ij ik il im in io lm iq ir is ln iu iv iw lo iy iz ja jb ha bi translated">重复下面的计算直到收敛:</p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lp"><img src="../Images/cc4631d646ae42ee5299223fa996da38.png" data-original-src="https://miro.medium.com/v2/resize:fit:166/format:webp/0*uuyrlGFYWI8_Mcmd.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">趋同；聚集</figcaption></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lq"><img src="../Images/8e1b6432c322de44c837f55dcda95546.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/0*fLCwtxJKbvVF86IB.png"/></div></figure><p id="bf04" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在每次迭代中，参数应该同时更新。像这样，</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es lr"><img src="../Images/2d5e0ccac957b799e566ec205798c781.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/0*Xb-Ck8vnNLen1Y_l.png"/></div></div></figure><p id="ad17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了更容易理解这个过程，让我们只考虑这个函数的一个参数。</p><p id="1bc3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">等式是这样的，</p><blockquote class="li lj lk"><p id="d8fa" class="ie if ll ig b ih ii ij ik il im in io lm iq ir is ln iu iv iw lo iy iz ja jb ha bi translated">重复直到收敛:</p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lw"><img src="../Images/61aa30ecac74ae9d90f01b483dbf84bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/0*aN7LSx3Lv68ejiNI.png"/></div></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lx"><img src="../Images/465804ec060904fb3095f3eb505968ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*BYWk4CZaRedj4mXj-Ik0Rw.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">当我们只考虑一个参数时，函数将是凸的</figcaption></figure><p id="51a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于正斜率，我们可以通过下式找到全局最小值</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ly"><img src="../Images/2da0d46fe2b1a041f395b0438f8ac8f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/0*OnrWWPlKQYkXc7tD.png"/></div></figure><p id="5181" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于负斜率，我们可以通过下式找到全局最小值</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ly"><img src="../Images/f256ea3621a7386c8a3a99cd6e46a631.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/0*-oC7a2S17gxAc810.png"/></div></figure><p id="c5df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">谈到参数α，我们可以在每次迭代中调整它的值。如果alpha太小，那么梯度下降(从任何一点到最小值的过程)会很慢。如果α太大，梯度下降会超过最小值。它可能无法收敛甚至发散。</p><p id="0ba6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">即使学习速率α固定，梯度下降也能收敛到局部最小值。当我们接近局部最小值时，梯度下降将自动采取较小的步骤。这是因为损失函数(w.r.t参数)的偏导数随着其接近最小点而改变(大部分减少)。</p><p id="6db8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于一个线性回归损失函数(是二次函数)，我们得到的图总是凸的(碗朝上)。在凸图形中，只有一个极小点。因此，最小点既是局部最小值又是全局最小值。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lf"><img src="../Images/2beb4a42648e3f6585283ba5aed565e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*UIGSMPZUxbIJ17MaPpkY_w.png"/></div></figure><p id="b2c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这意味着无论我们从哪里选择一个点开始下降，我们总是到达相同的最小值点。</p></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="21a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ll">注:本文是#30DaysOfData的一部分，文章内容是我自己从</em> <a class="ae mg" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank"> <em class="ll"> Andrew NG机器学习课程</em> </a> <em class="ll">中的笔记。</em></p></div></div>    
</body>
</html>