<html>
<head>
<title>Apache Spark : Secondary Sorting in Spark in Java</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark:Java中Spark的二级排序</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/apache-spark-secondary-sorting-in-spark-in-java-b9fcba989fab?source=collection_archive---------7-----------------------#2021-02-18">https://medium.com/analytics-vidhya/apache-spark-secondary-sorting-in-spark-in-java-b9fcba989fab?source=collection_archive---------7-----------------------#2021-02-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="4dc6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可能都见过Mapreduce/Hadoop中的二级排序及其关键实现。关于这一点有足够的信息和博客。让我们来学习如何在Java中使用Spark实现二次排序。</p><p id="6a4b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们继续之前，您可以从这里引用数据集—<a class="ae jc" href="https://www.kaggle.com/bhoomika216/airline-dataset-for-analysis?select=FINALdata1.csv" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/bhoomika 216/airline-dataset-for-analysis？select=FINALdata1.csv </a></p><p id="d79e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据定义，<strong class="ig hi"> <em class="jd">二级排序问题涉及到reduce阶段中与键相关的值的排序。有时，它被称为值到键的转换。二级排序技术将使我们能够对传递给每个reducer的值进行排序(按升序或降序)。简单来说，你可以用sql </em> </strong>把它和ORDER BY联系起来。<strong class="ig hi"> <em class="jd">同样，它依赖于使用一个组合键，该组合键将包含我们想要用于排序的所有值</em> </strong>。现在，使用这个你可能已经从kaggle下载的数据集，我们将尝试实现二次排序。现在让我们来看一下，我们使用的数据是航空公司的准点表现数据，也有几个可用的数据点，但我们将重点关注哪些航空公司的航班最晚到达，以及这些航班最晚到达发生在哪个机场。从这个语句中，我们可以确定我们的排序顺序:AirlineId(或UNIQUE_CARRIER)、AirportId(或DEST _机场_ID)和延迟时间(或ARR_DELAY)。我的意思是说——订单由(<strong class="ig hi">唯一_承运人</strong> ) ASC，(<strong class="ig hi">DEST _机场_ID </strong> ) ASC，<strong class="ig hi"> ARR_DELAY </strong> DESC。在我们进入代码之前，我们需要首先理解这些方法，至少有两种可能的方法来对reducer值进行排序。这些解决方案可以应用于MapReduce/Hadoop和Spark框架。</p><h2 id="edbf" class="je jf hh bd jg jh ji jj jk jl jm jn jo ip jp jq jr it js jt ju ix jv jw jx jy bi translated">第一种方法—</h2><p id="3d51" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip kb ir is it kc iv iw ix kd iz ja jb ha bi translated"><strong class="ig hi"> <em class="jd">在内存或减速器排序中进行二次排序</em> </strong></p><p id="9638" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种方法中，我们在每个reducer的节点/执行器上收集分组数据，这些数据将在内存中进行缓冲和排序(使用Java的<code class="du ke kf kg kh b">Collections.sort())</code>)。由于内存排序，这种方法存在可伸缩性问题。一旦分组的数据超出了reducer的节点/执行器的内存，就会抛出内存不足异常，因此作业将会失败。它将适用于小规模数据集。</p><h2 id="ab0a" class="je jf hh bd jg jh ji jj jk jl jm jn jo ip jp jq jr it js jt ju ix jv jw jx jy bi translated">第二种方法—</h2><p id="f2eb" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip kb ir is it kc iv iw ix kd iz ja jb ha bi translated"><strong class="ig hi"> <em class="jd">使用Spark框架进行二次排序或混排排序</em> </strong></p><p id="0b18" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种方法中，我们将首先理解“组合键”。当来自有效负载的值被提升并被附加到键或自然键时，形成组合键。让我们对此进行更多的讨论——</p><h1 id="5b0b" class="ki jf hh bd jg kj kk kl jk km kn ko jo kp kq kr jr ks kt ku ju kv kw kx jx ky bi translated">输入格式</h1><p id="74f2" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip kb ir is it kc iv iw ix kd iz ja jb ha bi translated">输入数据为CSV格式，包含以下各列，粗体显示的是我们将应用二次排序的值:</p><p id="07a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">月日，星期日，航班日期，<strong class="ig hi">唯一承运人，</strong>承运人始发地机场ID，始发地城市市场ID，始发地州ABR，<strong class="ig hi"> DEST机场ID </strong>，DEST城市市场ID，DEST州ABR，CRS DEP时间，DEP时间，DEP延迟新时间，出租车外出，车轮关闭，车轮开启，出租车进入，CRS到达时间，到达时间，<strong class="ig hi">到达延迟【T5</strong></p><p id="8f03" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">二次分拣将按照类似的顺序进行，由(<strong class="ig hi"> UNIQUE_CARRIER </strong> ) ASC，(<strong class="ig hi">DEST _机场_ID </strong> ) ASC，<strong class="ig hi">ARR _ DELAY</strong>desc；</p><p id="6bf0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">组合键将类似于</strong> —</p><pre class="kz la lb lc fd ld kh le lf aw lg bi"><span id="4600" class="je jf hh kh b fi lh li l lj lk">UNIQUE_CARRIER,DEST_AIRPORT_ID,ARR_DELAY</span></pre><p id="8775" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里的“自然”关键字是“唯一承运人”<strong class="ig hi">、<strong class="ig hi"> DEST机场ID </strong>和<strong class="ig hi"> ARR_DELAY </strong>是我们将包含在自然关键字中的值。定义组合键(通过将DEST _机场_ID和ARR_DELAY添加到自然键，即UNIQUE_CARRIER)使我们能够使用Spark框架对reducer值进行排序，即系统洗牌将首先按键的第一部分排序，然后按第二部分排序。我们将看到我们如何在Spark中实现这一点，现在，只要记住我们从Spark 1.2开始就有了一个名为“repartitionandsortwithpartitions()”的东西。现在让我们看看代码，以了解到底做了什么。</strong></p></div><div class="ab cl ll lm go ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ha hb hc hd he"><pre class="kz la lb lc fd ld kh le lf aw lg bi"><span id="d1dc" class="je jf hh kh b fi lh li l lj lk">package com.spark.rdd.tutorial;<br/><br/>import org.apache.spark.Partitioner;<br/>import org.apache.spark.SparkConf;<br/>import org.apache.spark.api.java.JavaPairRDD;<br/>import org.apache.spark.api.java.JavaRDD;<br/>import org.apache.spark.api.java.JavaSparkContext;<br/>import org.apache.spark.api.java.function.Function;<br/>import org.apache.spark.api.java.function.PairFunction;<br/>import scala.Tuple2;<br/><br/>import java.io.Serializable;<br/>import java.util.ArrayList;<br/>import java.util.Comparator;<br/>import java.util.List;<br/><br/>public class SecondarySorting {<br/>    public static void main(String args[]) {<br/>        SparkConf conf = new SparkConf().setMaster("local[*]").setAppName("app");<br/>        JavaSparkContext jsc = new JavaSparkContext(conf);<br/><br/>        JavaRDD&lt;String&gt; javaRDD = jsc.textFile("D:\\data\\airlineDataSmall.csv");<br/><br/>        JavaRDD&lt;String[]&gt; map = javaRDD.map(new Function&lt;String, String[]&gt;() {<br/>            @Override<br/>            public String[] call(String s) throws Exception {<br/>                String[] split = s.split(",");<br/>                return split;<br/>            }<br/>        });<br/>        JavaPairRDD&lt;CustomKey, List&lt;String&gt;&gt; pairRDD = map.mapToPair(new PairFunction&lt;String[], CustomKey, List&lt;String&gt;&gt;() {<br/>            @Override<br/>            public Tuple2&lt;CustomKey, List&lt;String&gt;&gt; call(String[] s) throws Exception {<br/>                CustomKey key = new CustomKey();<br/>                key.setUniqueCarrier(s[3]);<br/>                key.setDestAirportId(s[8]);<br/>                key.setArrivalDelay(Integer.<em class="jd">valueOf</em>(s[20]));<br/><br/>                List&lt;String&gt; l = new ArrayList&lt;&gt;();<br/>                l.add(s[2]);<br/>                l.add(s[5]);<br/>                l.add(s[6]);<br/><br/>                return new Tuple2&lt;&gt;(key, l);<br/>            }<br/>        });<br/><br/>        JavaPairRDD&lt;CustomKey, List&lt;String&gt;&gt; customKeyListJavaPairRDD = pairRDD.repartitionAndSortWithinPartitions(new CustomPartitioner(), new CustomComparator());<br/><br/>        List&lt;Tuple2&lt;CustomKey, List&lt;String&gt;&gt;&gt; collect = customKeyListJavaPairRDD.collect();<br/>        for (Tuple2&lt;CustomKey, List&lt;String&gt;&gt; l : collect) {<br/>            System.<em class="jd">out</em>.println(l._1().getUniqueCarrier() + "::" + l._1().getDestAirportId() + "::" + l._1().getArrivalDelay());<br/>        }<br/>    }<br/><br/>    public static class CustomPartitioner extends Partitioner {<br/>        @Override<br/>        public int numPartitions() {<br/>            return 1;<br/>        }<br/><br/>        @Override<br/>        public int getPartition(Object key) {<br/>            if (key instanceof CustomKey) {<br/>                return((CustomKey) key).getUniqueCarrier().hashCode()%numPartitions();<br/>            }<br/>            return -1;<br/>        }<br/>    }<br/><br/>    public static class CustomComparator implements Comparator&lt;CustomKey&gt;, Serializable <br/>    {<br/>        @Override<br/>        public int compare(CustomKey o1, CustomKey o2) <br/>        {<br/>            int value1 = o1.getUniqueCarrier().compareTo(o2.getUniqueCarrier());<br/>            if (value1 != 0)<br/>            {<br/>                return value1;<br/>            } <br/>            else <br/>            {<br/>                int value2 = o1.getDestAirportId().compareTo(o2.getDestAirportId());<br/>                if (value2 != 0) <br/>                {<br/>                    return value2;<br/>                } <br/>                else <br/>                {<br/>                    return o2.getArrivalDelay().compareTo(o1.getArrivalDelay());<br/>                }<br/>            }<br/>        }<br/>    }<br/>}</span></pre><p id="2682" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">自定义密钥类-</p><pre class="kz la lb lc fd ld kh le lf aw lg bi"><span id="1c66" class="je jf hh kh b fi lh li l lj lk">package com.spark.rdd.tutorial;<br/><br/>import java.io.Serializable;<br/><br/>public class CustomKey implements Serializable<br/>{<br/>    private String uniqueCarrier;<br/>    private String destAirportId;<br/>    private Integer arrivalDelay;<br/><br/>    public String getUniqueCarrier() {<br/>        return uniqueCarrier;<br/>    }<br/><br/>    public void setUniqueCarrier(String uniqueCarrier) {<br/>        this.uniqueCarrier = uniqueCarrier;<br/>    }<br/><br/>    public String getDestAirportId() {<br/>        return destAirportId;<br/>    }<br/><br/>    public void setDestAirportId(String destAirportId) {<br/>        this.destAirportId = destAirportId;<br/>    }<br/><br/>    public Integer getArrivalDelay() {<br/>        return arrivalDelay;<br/>    }<br/><br/>    public void setArrivalDelay(Integer arrivalDelay) {<br/>        this.arrivalDelay = arrivalDelay;<br/>    }<br/>}</span></pre><h2 id="46ee" class="je jf hh bd jg jh ji jj jk jl jm jn jo ip jp jq jr it js jt ju ix jv jw jx jy bi translated">创建键值对</h2><p id="cf77" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip kb ir is it kc iv iw ix kd iz ja jb ha bi translated">数据为CSV格式，将被转换为键值格式。二级排序的重要部分是在键中包含哪些值以支持额外的排序。“自然”关键字是唯一的承运人，DEST机场ID和ARR_DELAY是我们将包含在关键字中的值。使用javaRDD.map()和map.mapToPair()在代码中完成了相同的操作，请参考上面的代码。</p><h2 id="c9c7" class="je jf hh bd jg jh ji jj jk jl jm jn jo ip jp jq jr it js jt ju ix jv jw jx jy bi translated">分割和排序代码</h2><p id="a937" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip kb ir is it kc iv iw ix kd iz ja jb ha bi translated">现在我们需要对数据进行分区和排序。有两点我们需要考虑。</p><ol class=""><li id="6ae6" class="ls lt hh ig b ih ii il im ip lu it lv ix lw jb lx ly lz ma bi translated">我们需要按UNIQUE_CARRIER对数据进行分组，以便在缩减阶段到达同一个分区。但是我们的键是一个有3个字段的组合键。仅仅按键分区是不行的。因此，我们将创建一个定制的分区器，它知道使用哪个值来确定数据将到达的分区。这已经在上面的代码中通过扩展<a class="ae jc" href="https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/Partitioner.html" rel="noopener ugc nofollow" target="_blank">划分器</a>类完成了。</li><li id="cf14" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb lx ly lz ma bi translated">我们还需要告诉Spark我们希望我们的数据如何排序，比如按(<strong class="ig hi">UNIQUE _ CARRIER</strong>ASC，(<strong class="ig hi">DEST _机场_ID </strong> ) ASC，<strong class="ig hi">ARR _ DELAY</strong>desc；意思是，我们希望<strong class="ig hi"> ARR_DELAY </strong>按降序排列，因此延迟最大的航班将首先列出。这是通过实现一个定制的java比较器来实现的。参考上面的代码。</li></ol><pre class="kz la lb lc fd ld kh le lf aw lg bi"><span id="44f9" class="je jf hh kh b fi lh li l lj lk">CustomComparator implements Comparator&lt;CustomKey&gt;, Serializable {}</span></pre><h2 id="a2b4" class="je jf hh bd jg jh ji jj jk jl jm jn jo ip jp jq jr it js jt ju ix jv jw jx jy bi translated">摘要</h2><p id="89be" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip kb ir is it kc iv iw ix kd iz ja jb ha bi translated">现在是时候将我们的分区和排序付诸行动了。这是通过使用<code class="du ke kf kg kh b">repartitionAndSortWithinPartitions </code>实现的，它不是一个分组操作。它只会将具有相同键的数据移动到相同的分区，并根据<a class="ae jc" href="https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html" rel="noopener ugc nofollow" target="_blank">ordereddfunctions</a>类上的比较器方法对其进行排序。来自spark java doc，它说—</p><blockquote class="mg mh mi"><p id="ef60" class="ie if jd ig b ih ii ij ik il im in io mj iq ir is mk iu iv iw ml iy iz ja jb ha bi translated">根据给定的分割器对RDD进行重新分区，并在每个分区中按键对记录进行排序。</p><p id="925a" class="ie if jd ig b ih ii ij ik il im in io mj iq ir is mk iu iv iw ml iy iz ja jb ha bi translated">这比调用repartition然后在每个分区内排序更有效，因为它可以将排序下推到shuffle机制中。</p></blockquote><p id="42fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">输出—</p><figure class="kz la lb lc fd mn er es paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="er es mm"><img src="../Images/109f9ded2b2fb866f6d8771c17afb6d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rD07fJB5lbFx8N-Ufl047g.png"/></div></div></figure><p id="4bd3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以看到按<strong class="ig hi"> UNIQUE_CARRIER </strong>、<strong class="ig hi">DEST _机场_ID </strong>和<strong class="ig hi"> ARR_DELAY排序的结果。</strong></p></div></div>    
</body>
</html>