<html>
<head>
<title>Gradient Descent Application in 30 mins (NO BS)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">30åˆ†é’Ÿå†…æ¢¯åº¦ä¸‹é™åº”ç”¨(æ— BS)</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/analytics-vidhya/gradient-descent-application-in-30-mins-no-bs-e0654e302775?source=collection_archive---------2-----------------------#2021-04-07">https://medium.com/analytics-vidhya/gradient-descent-application-in-30-mins-no-bs-e0654e302775?source=collection_archive---------2-----------------------#2021-04-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3100" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">è®©æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªç®€å•çš„å¤šé¡¹å¼æ¨¡å‹:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/37820ae6edd52b34e3b357358eed6e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*CnVaIgI172TnPMzRkrymdQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">æˆ‘ä»¬çš„æ¨¡å‹ç¤ºä¾‹</figcaption></figure><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="efc2" class="jy jz hi ju b fi ka kb l kc kd">def sin_model(x, theta):<br/>    """<br/>    Predict the estimate of y given x, theta_1, theta_2</span><span id="c26f" class="jy jz hi ju b fi ke kb l kc kd">    Keyword arguments:<br/>    x -- the vector of values x<br/>    theta -- a vector of length 2<br/>    """</span><span id="46cf" class="jy jz hi ju b fi ke kb l kc kd">    theta_1 = theta[0]<br/>    theta_2 = theta[1]<br/>    return theta_1 * x + np.sin(theta_2 * x)</span></pre><p id="30a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">æ¢¯åº¦ä¸‹é™èƒŒåçš„ç›´è§‰:</strong></p><ol class=""><li id="7d83" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc kk kl km kn bi translated">æœ€ä½³Î¸å€¼æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚</li><li id="7206" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">æˆ‘ä»¬æ‰¾åˆ°Î¸æœ€å°å€¼çš„æ–¹æ³•æ˜¯<strong class="ih hj">é€šè¿‡å¯¹æŸå¤±å‡½æ•°å¯¹Î¸</strong>æ±‚å¯¼</li></ol></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="f060" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ç¬¬ä¸€æ­¥æ˜¯ä¸ºæˆ‘ä»¬çš„æ¨¡å‹é€‰æ‹©ä¸€ä¸ªæŸå¤±å‡½æ•°ã€‚</p><p id="7b53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">å‡è®¾æˆ‘ä»¬ä½¿ç”¨å‡æ–¹æŸå¤±å‡½æ•°ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå› æ­¤:((y_hat â€” y_obs) ** 2) / n</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="abb9" class="jy jz hi ju b fi ka kb l kc kd">def sin_MSE(theta, x, y):<br/>    """<br/>    Compute the numerical value of the l2 loss of our sinusoidal model given theta</span><span id="055b" class="jy jz hi ju b fi ke kb l kc kd">Keyword arguments:<br/>    theta -- the vector of values theta<br/>    x     -- the vector of x values<br/>    y     -- the vector of y values<br/>    """</span><span id="a8d1" class="jy jz hi ju b fi ke kb l kc kd">    y_hat = sin_model(x, theta)<br/>    return np.mean((y - y_hat)** 2)</span></pre><p id="efa5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æˆ‘ä»¬å°†è¿™ä¸ªæŸå¤±å‡½æ•°å¯¹Î¸_ 1å’ŒÎ¸_ 2æ±‚å¯¼ï¼Œæˆ‘ä»¬å¾—åˆ°å¦‚ä¸‹ç»“æœ:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es la"><img src="../Images/db052b0e3b683adebbb6073007bcda3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*CsPyMQTkpHaL5XUiOGn3TA.png"/></div></figure><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="c32c" class="jy jz hi ju b fi ka kb l kc kd">def sin_MSE_dt1(theta, x, y):<br/>    """<br/>    Compute the numerical value of the partial of l2 loss with respect to theta_1</span><span id="d616" class="jy jz hi ju b fi ke kb l kc kd">Keyword arguments:<br/>    theta -- the vector of values theta<br/>    x     -- the vector of x values<br/>    y     -- the vector of y values<br/>    """<br/>    <br/>    theta_1 = theta[0]<br/>    theta_2 = theta[1]<br/>    <br/>    dt_1 = -2 * (y - theta_1 * x - np.sin(theta_2 * x)) * x<br/>    return np.mean(dt_1)<br/>    <br/>def sin_MSE_dt2(theta, x, y):<br/>    """<br/>    Compute the numerical value of the partial of l2 loss with respect to theta_2</span><span id="a017" class="jy jz hi ju b fi ke kb l kc kd">Keyword arguments:<br/>    theta -- the vector of values theta<br/>    x     -- the vector of x values<br/>    y     -- the vector of y values<br/>    """<br/>    theta_1 = theta[0]<br/>    theta_2 = theta[1]<br/>    <br/>    dt_2 = -2 * (y - theta_1 * x - np.sin(theta_2 * x)) * x * np.cos(theta_2 * x)<br/>    return np.mean(dt_2)</span></pre><p id="08c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æœ€åï¼Œæˆ‘ä»¬å°†å®ƒä»¬æ”¾åœ¨ä¸€ä¸ªæ•´ä½“æ¢¯åº¦æŸå¤±å‡½æ•°ä¸­</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="07d6" class="jy jz hi ju b fi ka kb l kc kd">def sin_MSE_gradient(theta, x, y):<br/>    """<br/>    Returns the gradient of l2 loss with respect to vector theta</span><span id="0f00" class="jy jz hi ju b fi ke kb l kc kd">Keyword arguments:<br/>    theta -- the vector of values theta<br/>    x     -- the vector of x values<br/>    y     -- the vector of y values<br/>    """<br/>    return np.array([sin_MSE_dt1(theta, x, y), sin_MSE_dt2(theta, x, y)])</span></pre></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="5e42" class="jy jz hi bd lb lc ld le lf lg lh li lj iq lk ll lm iu ln lo lp iy lq lr ls lt bi translated">å®ç°æ¢¯åº¦ä¸‹é™åŠŸèƒ½</h2><p id="25f7" class="pw-post-body-paragraph if ig hi ih b ii lu ik il im lv io ip iq lw is it iu lx iw ix iy ly ja jb jc hb bi translated">æœ‰å¾ˆå¤šæ–¹æ³•å¯ä»¥å†™å‡ºä½ çš„æ¢¯åº¦ä¸‹é™å‡½æ•°ï¼›è¯·è®°ä½ï¼Œå¯¹æ‚¨æ¥è¯´ï¼Œæœ€é‡è¦çš„äº‹æƒ…æ˜¯ç†è§£æ‰€æœ‰è¿™äº›å®ç°èƒŒåçš„é€»è¾‘ï¼Œå¹¶åœ¨æ‚¨è‡ªå·±çš„åˆ†æä¸­æ‰¾åˆ°å¯¹æ‚¨æ¥è¯´æœ€æœ‰æ„ä¹‰çš„å®ç°ã€‚</p><p id="775a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">éœ€è¦è®°ä½çš„ä¸€äº›ç›´è§‰:</strong></p><ul class=""><li id="1717" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc lz kl km kn bi translated">æˆ‘ä»¬è¯•å›¾æ‰¾åˆ°å¯¼è‡´æŸå¤±å‡½æ•°æ•´ä½“æœ€å°å€¼çš„Î¸_ 1å’ŒÎ¸_ 2çš„ç»„åˆ</li><li id="079c" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc lz kl km kn bi translated">æ¢¯åº¦ä¸‹é™åœ¨è´Ÿæ¢¯åº¦æ–¹å‘è½»æ¨Î¸ï¼Œç›´åˆ°Î¸æ”¶æ•›ã€‚</li></ul><p id="0b20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æˆ‘ä»¬å¼€å§‹å§ï¼</p><ol class=""><li id="f4d1" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc kk kl km kn bi translated"><strong class="ih hj">å®æ–½æ¢¯åº¦ä¸‹é™çš„é€æ­¥æŒ‡å—</strong></li></ol><ul class=""><li id="4f99" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc lz kl km kn bi translated">å…¶ä¸­<code class="du ma mb mc ju b">df</code>æ˜¯æˆ‘ä»¬è¦æœ€å°åŒ–çš„å‡½æ•°çš„æ¢¯åº¦(åˆåã€‚æŸå¤±å‡½æ•°)å’Œ<code class="du ma mb mc ju b">initial_guess</code>æ˜¯è¯¥å‡½æ•°çš„èµ·å§‹å‚æ•°(ä»»æ„çš„)ã€‚</li></ul><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="17ea" class="jy jz hi ju b fi ka kb l kc kd"><strong class="ju hj">def</strong> gradient_descent(df, initial_guess, alpha, n):<br/>    guesses = [initial_guess]<br/>    guess = initial_guess</span><span id="ce55" class="jy jz hi ju b fi ke kb l kc kd">    <strong class="ju hj">while</strong> len(guesses) &lt; n:<br/>        guess = guess - alpha * df(guess)<br/>        guesses.append(guess)<br/>    <strong class="ju hj">return</strong> np.array(guesses)</span></pre><p id="7076" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æ­£å¦‚æ‚¨å¯èƒ½æ³¨æ„åˆ°çš„ï¼Œæˆ‘ä»¬çš„sin_MSE_gradientå‡½æ•°æ¥å—3ä¸ªå‚æ•°(thetaï¼Œxï¼Œy)ï¼Œå¦‚æœæˆ‘ä»¬æƒ³åœ¨gradient_descentå‡½æ•°ä¸­ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›å¯¹å®ƒè¿›è¡Œå¦‚ä¸‹ä¼˜åŒ–ï¼Œä»¥ä¾¿å®ƒåªæ¥å—1ä¸ªå‚æ•°(theta):</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="cac4" class="jy jz hi ju b fi ka kb l kc kd"><strong class="ju hj">def</strong> mse_loss_derivative_single_arg(theta):<br/>    x = data['a', 'b']<br/>    y_obs = data['y']<br/><br/>    <strong class="ju hj">return</strong> sin_MSE_gradient(theta, x, y_obs)</span></pre><p id="188b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ç„¶åè®©æˆ‘ä»¬å°è¯•è¿è¡Œæ¢¯åº¦ä¸‹é™å‡½æ•°ï¼Œåˆå§‹Î¸å‘é‡[0ï¼Œ0]ï¼Œalpha = 0.001ï¼Œmax_iteration = 100:</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="0b3d" class="jy jz hi ju b fi ka kb l kc kd">gradient_descent(mse_loss_derivative_single_arg, np.array([0, 0]), 0.0001, 100)</span></pre></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="1f88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">æœ‰é™æ¬¡è¿­ä»£çš„æ¢¯åº¦ä¸‹é™</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es md"><img src="../Images/f0a647f53daa40e05026e2ca59d4d210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*y8NGL8AX-zpH38pLRhJJ8A.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">æ›´æ–°åŠŸèƒ½</figcaption></figure><p id="421e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œä½¿ç”¨æ¢¯åº¦å’Œ<code class="du ma mb mc ju b">alpha</code>æ¥æ›´æ–°ä½ å½“å‰çš„<code class="du ma mb mc ju b">theta</code>ã€‚æˆ‘ä»¬è¿˜å°†ç”µæµ<code class="du ma mb mc ju b">theta</code>ä¿å­˜åœ¨<code class="du ma mb mc ju b">theta_history</code>ä¸­ï¼ŒåŒæ—¶å°†å¹³å‡å¹³æ–¹æŸè€—(ç”¨ç”µæµ<code class="du ma mb mc ju b">theta</code>è®¡ç®—)ä¿å­˜åœ¨<code class="du ma mb mc ju b">loss_history</code>ä¸­</p><ul class=""><li id="b4f6" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc lz kl km kn bi translated">è®©æˆ‘ä»¬é¦–å…ˆå®šä¹‰åˆå§‹Î¸å€¼ï¼Œå®ƒå®é™…ä¸Šæ˜¯å›¾ä¸Šçš„ä»»æ„ç‚¹</li></ul><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="73ad" class="jy jz hi ju b fi ka kb l kc kd">def init_theta():<br/>    """Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent"""</span><span id="9c04" class="jy jz hi ju b fi ke kb l kc kd">    return np.zeros((2,))</span></pre><p id="b938" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æ¢¯åº¦ä¸‹é™å‡½æ•°:</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="4b55" class="jy jz hi ju b fi ka kb l kc kd">def grad_desc(loss_f, gradient_loss_f, theta, data, num_iter=20, alpha=0.1):<br/>    """<br/>    Run gradient descent update for a finite number of iterations and static learning rate</span><span id="e997" class="jy jz hi ju b fi ke kb l kc kd">Keyword arguments:<br/>    loss_f -- the loss function to be minimized (used for computing loss_history)<br/>    gradient_loss_f -- the gradient of the loss function to be minimized<br/>    theta -- the vector of values theta to use at first iteration<br/>    data -- the data used in the model <br/>    num_iter -- the max number of iterations<br/>    alpha -- the learning rate (also called the step size)<br/>    <br/>    Return:<br/>    theta -- the optimal value of theta after num_iter of gradient descent<br/>    theta_history -- the series of theta values over each iteration of gradient descent<br/>    loss_history -- the series of loss values over each iteration of gradient descent<br/>    """</span><span id="cee2" class="jy jz hi ju b fi ke kb l kc kd">    theta_history = []<br/>    loss_history = []<br/>    <br/>    #initial x and y values<br/>    x = part_1_data['x']<br/>    y = part_1_data['y']<br/> <br/>    <br/>    #append the original theta and loss     <br/>    theta_history.append(theta)<br/>    loss_history.append(loss_f(theta, x, y))<br/>    <br/>    #main update function</span><span id="effd" class="jy jz hi ju b fi ke kb l kc kd">    for iteration in range (num_iter):<br/>        <br/>        #update and append theta<br/>        theta_cur = theta - alpha * gradient_loss_f(theta, x, y)<br/>        theta_history.append(theta_cur)<br/>        <br/>        #calculate and append loss using cur_theta<br/>        cur_loss = loss_f(theta_cur, x, y)<br/>        loss_history.append(cur_loss)<br/>        <br/>        #reset theta<br/>        theta = theta_cur</span><span id="b110" class="jy jz hi ju b fi ke kb l kc kd">return theta, theta_history, loss_history</span><span id="4fea" class="jy jz hi ju b fi ke kb l kc kd">#running gradient descent <br/>theta_start = init_theta()<br/>theta_hat, thetas_used, losses_calculated = grad_desc(<br/>    sin_MSE, sin_MSE_gradient, theta_start, part_1_data, num_iter=20, alpha=0.1<br/>)<br/>for b, l in zip(thetas_used, losses_calculated):<br/>    print(f"theta: {b}, Loss: {l}")</span></pre></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="68f6" class="me jz hi bd lb mf mg mh lf mi mj mk lj ml mm mn lm mo mp mq lp mr ms mt ls mu bi translated"><span class="l mv mw mx bm my mz na nb nc di"> E </span>ä½“éªŒæ›´å¤šçš„æ¢¯åº¦ä¸‹é™</h1><h2 id="cb4d" class="jy jz hi bd lb lc ld le lf lg lh li lj iq lk ll lm iu ln lo lp iy lq lr ls lt bi translated"><strong class="ak">è¡°å‡é˜¿å°”æ³•(å­¦ä¹ ç‡)æ¢¯åº¦ä¸‹é™</strong></h2><p id="f7d3" class="pw-post-body-paragraph if ig hi ih b ii lu ik il im lv io ip iq lw is it iu lx iw ix iy ly ja jb jc hb bi translated"><strong class="ih hj">ç›´è§‰:</strong></p><p id="6feb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">é€šè¿‡è¡°å‡çš„å­¦ä¹ é€Ÿç‡ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ•°å­—ğ›¼ï¼Œå­¦ä¹ ç°åœ¨åº”è¯¥æ˜¯ğ›¼/(ğ‘¡+1)å…¶ä¸­ğ‘¡æ˜¯æ¢¯åº¦ä¸‹é™çš„å½“å‰è¿­ä»£çš„æ•°å­—ã€‚</p><p id="5c8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ä½ å‡ ä¹å¯ä»¥å›æ”¶ä¸Šé¢çš„å¤§éƒ¨åˆ†ä»£ç ï¼Œè€Œä¸æ˜¯æ¯æ¬¡è¿­ä»£åªæ›´æ–°ğ›¼åˆ°ğ›¼/(iteration+1ã€‚</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="a0e3" class="jy jz hi bd lb lc ld le lf lg lh li lj iq lk ll lm iu ln lo lp iy lq lr ls lt bi translated">éšæœºæ¢¯åº¦ä¸‹é™</h2><p id="c3d0" class="pw-post-body-paragraph if ig hi ih b ii lu ik il im lv io ip iq lw is it iu lx iw ix iy ly ja jb jc hb bi translated"><strong class="ih hj">ç›´è§‰:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nd"><img src="../Images/691995071a950b4841190d8f178de8b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tNzpI42xtLjCDFXUNSpC6Q.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">æ¯æ‰¹å¤§å°çš„Î¸æ›´æ–°å‡½æ•°</figcaption></figure><p id="a8ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">åœ¨ä¸Šé¢çš„æ›´æ–°è§„åˆ™ä¸­ï¼Œğ‘(æ‰¹é‡å¤§å°)æ¯”ğ‘›(æ•°æ®çš„æ€»å¤§å°)å°å¾—å¤šã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé™æ€å­¦ä¹ ç‡å’Œä¸€ä¸ªå‚æ•°<code class="du ma mb mc ju b">batch_size</code>æ¥è¡¨ç¤ºæ¯æ¬¡è¿­ä»£çš„å°æ‰¹é‡æ ·æœ¬çš„å¤§å°ã€‚</p><p id="a1d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ä¸Šé¢ä»£ç çš„ä¸»è¦å˜åŒ–æ˜¯æˆ‘ä»¬è¿­ä»£æ—¶çš„æ›´æ–°åŠŸèƒ½:</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="adc6" class="jy jz hi ju b fi ka kb l kc kd">for iteration in range (num_iter):<br/>        #sample batch size each iterations <br/>        sample = data.sample(batch_size)<br/>        out_arr_x = sample['x']<br/>        out_arr_y = sample['y']<br/>        <br/>        #update and append theta<br/>        theta_cur = theta - alpha * gradient_loss_f(theta, out_arr_x, out_arr_y)<br/>        theta_history.append(theta_cur)<br/>        <br/>        #calculate and append loss using cur_theta<br/>        cur_loss = loss_f(theta_cur, out_arr_x, out_arr_y)<br/>        loss_history.append(cur_loss)<br/>        <br/>        #reset<br/>        theta = theta_cur</span></pre><p id="f050" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æ³¨æ„:å¯¹äºcur_lossï¼Œæˆ‘ä»¬è®¡ç®—å¹¶ä¿å­˜æ¯ä¸ªæ‰¹é‡çš„loss_valï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ã€‚</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="720b" class="jy jz hi bd lb lc ld le lf lg lh li lj iq lk ll lm iu ln lo lp iy lq lr ls lt bi translated">å¯è§†åŒ–å’Œæ¯”è¾ƒæ€§èƒ½</h2><p id="77b3" class="pw-post-body-paragraph if ig hi ih b ii lu ik il im lv io ip iq lw is it iu lx iw ix iy ly ja jb jc hb bi translated">è®©æˆ‘ä»¬æƒ³è±¡ä¸€ä¸‹æˆ‘ä»¬çš„å‡½æ•°ï¼Œçœ‹çœ‹æ¯ä¸ªå‡½æ•°åœ¨æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼æ—¶çš„è¡¨ç°ã€‚</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="d9eb" class="jy jz hi ju b fi ka kb l kc kd">plt.plot(np.arange(len(loss)), loss, label='Static Alpha')<br/>plt.plot(np.arange(len(loss)), loss_decay, label='Decaying Alpha')<br/>plt.plot(np.arange(len(loss)), loss_stoch, label='SGD')<br/>plt.xlabel('Iteration #')<br/>plt.ylabel('Avg Loss')<br/>plt.title('Avg Loss vs Iteration # vs Learning Rate Type')<br/>plt.legend();</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ne"><img src="../Images/ec31e0f679b78138e9c55270ebd8bc7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*WdKcVIaLUG0YU-ckzz61QQ.png"/></div></figure><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="0b95" class="jy jz hi ju b fi ka kb l kc kd">#3D plot (gradient descent with static alpha)<br/>plot_3d(thetas[:, 0], thetas[:, 1], loss, average_squared_loss, sin_model, x, y)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nf"><img src="../Images/bb94415c6b84413a0f12498ce65f6515.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i5oYS9SBhKGjHxdK8AU0eg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">é™æ€é˜¿å°”æ³•æ¢¯åº¦ä¸‹é™</figcaption></figure><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="50b9" class="jy jz hi ju b fi ka kb l kc kd">#3D plot (gradient descent with decaying alpha)<br/>plot_3d(thetas_decay[:, 0], thetas_decay[:, 1], loss_decay, average_squared_loss, sin_model, x, y)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ng"><img src="../Images/bfd35f15ea46b1f88ec3313c174f7aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RH6q6qzMFx-Lc74RHiRcVQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">é˜¿å°”æ³•è¡°å‡çš„æ¢¯åº¦ä¸‹é™</figcaption></figure><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="3184" class="jy jz hi ju b fi ka kb l kc kd">#3D plot (stochastic gradient descent)<br/>plot_3d(thetas_stoch[:, 0], thetas_stoch[:, 1], loss_stoch, average_squared_loss, sin_model, x, y)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nh"><img src="../Images/0bcfe3c88902367833305364262d7264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMnU6bSSP-Fc37bdXOYKJg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">éšæœºæ¢¯åº¦ä¸‹é™</figcaption></figure></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="33b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi ni translated"><span class="l mv mw mx bm my mz na nb nc di"> G </span>æ¢¯åº¦ä¸‹é™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§é€šç”¨çš„æ–¹æ³•ï¼Œå½“æˆ‘ä»¬æ— æ³•è§£æåœ°æ±‚è§£Î¸çš„æœ€å°å€¼æ—¶ï¼Œå¯ä»¥ä½¿æŸå¤±å‡½æ•°æœ€å°åŒ–ã€‚éšç€æˆ‘ä»¬çš„æ¨¡å‹å’ŒæŸå¤±å‡½æ•°è¶Šæ¥è¶Šå¤æ‚ï¼Œæˆ‘ä»¬å°†è½¬å‘æ¢¯åº¦ä¸‹é™ä½œä¸ºæ‹Ÿåˆæ¨¡å‹çš„å·¥å…·ã€‚</p><p id="d1e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ç”±äºè®¡ç®—å¤§å‹æ•°æ®é›†å¯èƒ½æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶è¾ƒé•¿ï¼Œå› æ­¤éšæœºGDè®¡ç®—æ›´æ–°æ¯”æ‰¹é‡æ¢¯åº¦ä¸‹é™å¿«å¾—å¤šã€‚åˆ°æ‰¹é‡æ¢¯åº¦ä¸‹é™å®Œæˆä¸€æ¬¡æ›´æ–°æ—¶ï¼Œå®ƒå¯ä»¥æœç€æœ€ä¼˜Î¸å–å¾—æ˜¾è‘—è¿›å±•ã€‚</p><p id="eb5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">è€Œè¡°å‡çš„Î±GDè€ƒè™‘äº†æ›´æ–°å­¦ä¹ ç‡ï¼Œè¿™æœ‰åˆ©äºé˜²æ­¢è¿‡å†²ï¼Œå› æ­¤æ­¥é•¿æ›´ç²¾ç¡®ã€‚</p></div></div>    
</body>
</html>