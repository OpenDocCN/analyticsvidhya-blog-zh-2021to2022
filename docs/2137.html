<html>
<head>
<title>Gradient Descent Application in 30 mins (NO BS)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">30分钟内梯度下降应用(无BS)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-application-in-30-mins-no-bs-e0654e302775?source=collection_archive---------2-----------------------#2021-04-07">https://medium.com/analytics-vidhya/gradient-descent-application-in-30-mins-no-bs-e0654e302775?source=collection_archive---------2-----------------------#2021-04-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3100" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们首先定义一个简单的多项式模型:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/37820ae6edd52b34e3b357358eed6e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*CnVaIgI172TnPMzRkrymdQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">我们的模型示例</figcaption></figure><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="efc2" class="jy jz hi ju b fi ka kb l kc kd">def sin_model(x, theta):<br/>    """<br/>    Predict the estimate of y given x, theta_1, theta_2</span><span id="c26f" class="jy jz hi ju b fi ke kb l kc kd">    Keyword arguments:<br/>    x -- the vector of values x<br/>    theta -- a vector of length 2<br/>    """</span><span id="46cf" class="jy jz hi ju b fi ke kb l kc kd">    theta_1 = theta[0]<br/>    theta_2 = theta[1]<br/>    return theta_1 * x + np.sin(theta_2 * x)</span></pre><p id="30a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">梯度下降背后的直觉:</strong></p><ol class=""><li id="7d83" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc kk kl km kn bi translated">最佳θ值最小化损失函数。</li><li id="7206" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">我们找到θ最小值的方法是<strong class="ih hj">通过对损失函数对θ</strong>求导</li></ol></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="f060" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一步是为我们的模型选择一个损失函数。</p><p id="7b53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们使用均方损失函数作为损失函数，因此:((y_hat — y_obs) ** 2) / n</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="abb9" class="jy jz hi ju b fi ka kb l kc kd">def sin_MSE(theta, x, y):<br/>    """<br/>    Compute the numerical value of the l2 loss of our sinusoidal model given theta</span><span id="055b" class="jy jz hi ju b fi ke kb l kc kd">Keyword arguments:<br/>    theta -- the vector of values theta<br/>    x     -- the vector of x values<br/>    y     -- the vector of y values<br/>    """</span><span id="a8d1" class="jy jz hi ju b fi ke kb l kc kd">    y_hat = sin_model(x, theta)<br/>    return np.mean((y - y_hat)** 2)</span></pre><p id="efa5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将这个损失函数对θ_ 1和θ_ 2求导，我们得到如下结果:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es la"><img src="../Images/db052b0e3b683adebbb6073007bcda3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*CsPyMQTkpHaL5XUiOGn3TA.png"/></div></figure><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="c32c" class="jy jz hi ju b fi ka kb l kc kd">def sin_MSE_dt1(theta, x, y):<br/>    """<br/>    Compute the numerical value of the partial of l2 loss with respect to theta_1</span><span id="d616" class="jy jz hi ju b fi ke kb l kc kd">Keyword arguments:<br/>    theta -- the vector of values theta<br/>    x     -- the vector of x values<br/>    y     -- the vector of y values<br/>    """<br/>    <br/>    theta_1 = theta[0]<br/>    theta_2 = theta[1]<br/>    <br/>    dt_1 = -2 * (y - theta_1 * x - np.sin(theta_2 * x)) * x<br/>    return np.mean(dt_1)<br/>    <br/>def sin_MSE_dt2(theta, x, y):<br/>    """<br/>    Compute the numerical value of the partial of l2 loss with respect to theta_2</span><span id="a017" class="jy jz hi ju b fi ke kb l kc kd">Keyword arguments:<br/>    theta -- the vector of values theta<br/>    x     -- the vector of x values<br/>    y     -- the vector of y values<br/>    """<br/>    theta_1 = theta[0]<br/>    theta_2 = theta[1]<br/>    <br/>    dt_2 = -2 * (y - theta_1 * x - np.sin(theta_2 * x)) * x * np.cos(theta_2 * x)<br/>    return np.mean(dt_2)</span></pre><p id="08c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们将它们放在一个整体梯度损失函数中</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="07d6" class="jy jz hi ju b fi ka kb l kc kd">def sin_MSE_gradient(theta, x, y):<br/>    """<br/>    Returns the gradient of l2 loss with respect to vector theta</span><span id="0f00" class="jy jz hi ju b fi ke kb l kc kd">Keyword arguments:<br/>    theta -- the vector of values theta<br/>    x     -- the vector of x values<br/>    y     -- the vector of y values<br/>    """<br/>    return np.array([sin_MSE_dt1(theta, x, y), sin_MSE_dt2(theta, x, y)])</span></pre></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="5e42" class="jy jz hi bd lb lc ld le lf lg lh li lj iq lk ll lm iu ln lo lp iy lq lr ls lt bi translated">实现梯度下降功能</h2><p id="25f7" class="pw-post-body-paragraph if ig hi ih b ii lu ik il im lv io ip iq lw is it iu lx iw ix iy ly ja jb jc hb bi translated">有很多方法可以写出你的梯度下降函数；请记住，对您来说，最重要的事情是理解所有这些实现背后的逻辑，并在您自己的分析中找到对您来说最有意义的实现。</p><p id="775a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">需要记住的一些直觉:</strong></p><ul class=""><li id="1717" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc lz kl km kn bi translated">我们试图找到导致损失函数整体最小值的θ_ 1和θ_ 2的组合</li><li id="079c" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc lz kl km kn bi translated">梯度下降在负梯度方向轻推θ，直到θ收敛。</li></ul><p id="0b20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们开始吧！</p><ol class=""><li id="f4d1" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc kk kl km kn bi translated"><strong class="ih hj">实施梯度下降的逐步指南</strong></li></ol><ul class=""><li id="4f99" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc lz kl km kn bi translated">其中<code class="du ma mb mc ju b">df</code>是我们要最小化的函数的梯度(又名。损失函数)和<code class="du ma mb mc ju b">initial_guess</code>是该函数的起始参数(任意的)。</li></ul><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="17ea" class="jy jz hi ju b fi ka kb l kc kd"><strong class="ju hj">def</strong> gradient_descent(df, initial_guess, alpha, n):<br/>    guesses = [initial_guess]<br/>    guess = initial_guess</span><span id="ce55" class="jy jz hi ju b fi ke kb l kc kd">    <strong class="ju hj">while</strong> len(guesses) &lt; n:<br/>        guess = guess - alpha * df(guess)<br/>        guesses.append(guess)<br/>    <strong class="ju hj">return</strong> np.array(guesses)</span></pre><p id="7076" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如您可能注意到的，我们的sin_MSE_gradient函数接受3个参数(theta，x，y)，如果我们想在gradient_descent函数中使用它，我们可能希望对它进行如下优化，以便它只接受1个参数(theta):</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="cac4" class="jy jz hi ju b fi ka kb l kc kd"><strong class="ju hj">def</strong> mse_loss_derivative_single_arg(theta):<br/>    x = data['a', 'b']<br/>    y_obs = data['y']<br/><br/>    <strong class="ju hj">return</strong> sin_MSE_gradient(theta, x, y_obs)</span></pre><p id="188b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后让我们尝试运行梯度下降函数，初始θ向量[0，0]，alpha = 0.001，max_iteration = 100:</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="0b3d" class="jy jz hi ju b fi ka kb l kc kd">gradient_descent(mse_loss_derivative_single_arg, np.array([0, 0]), 0.0001, 100)</span></pre></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="1f88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">有限次迭代的梯度下降</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es md"><img src="../Images/f0a647f53daa40e05026e2ca59d4d210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*y8NGL8AX-zpH38pLRhJJ8A.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">更新功能</figcaption></figure><p id="421e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在每个时间步，使用梯度和<code class="du ma mb mc ju b">alpha</code>来更新你当前的<code class="du ma mb mc ju b">theta</code>。我们还将电流<code class="du ma mb mc ju b">theta</code>保存在<code class="du ma mb mc ju b">theta_history</code>中，同时将平均平方损耗(用电流<code class="du ma mb mc ju b">theta</code>计算)保存在<code class="du ma mb mc ju b">loss_history</code>中</p><ul class=""><li id="b4f6" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc lz kl km kn bi translated">让我们首先定义初始θ值，它实际上是图上的任意点</li></ul><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="73ad" class="jy jz hi ju b fi ka kb l kc kd">def init_theta():<br/>    """Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent"""</span><span id="9c04" class="jy jz hi ju b fi ke kb l kc kd">    return np.zeros((2,))</span></pre><p id="b938" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降函数:</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="4b55" class="jy jz hi ju b fi ka kb l kc kd">def grad_desc(loss_f, gradient_loss_f, theta, data, num_iter=20, alpha=0.1):<br/>    """<br/>    Run gradient descent update for a finite number of iterations and static learning rate</span><span id="e997" class="jy jz hi ju b fi ke kb l kc kd">Keyword arguments:<br/>    loss_f -- the loss function to be minimized (used for computing loss_history)<br/>    gradient_loss_f -- the gradient of the loss function to be minimized<br/>    theta -- the vector of values theta to use at first iteration<br/>    data -- the data used in the model <br/>    num_iter -- the max number of iterations<br/>    alpha -- the learning rate (also called the step size)<br/>    <br/>    Return:<br/>    theta -- the optimal value of theta after num_iter of gradient descent<br/>    theta_history -- the series of theta values over each iteration of gradient descent<br/>    loss_history -- the series of loss values over each iteration of gradient descent<br/>    """</span><span id="cee2" class="jy jz hi ju b fi ke kb l kc kd">    theta_history = []<br/>    loss_history = []<br/>    <br/>    #initial x and y values<br/>    x = part_1_data['x']<br/>    y = part_1_data['y']<br/> <br/>    <br/>    #append the original theta and loss     <br/>    theta_history.append(theta)<br/>    loss_history.append(loss_f(theta, x, y))<br/>    <br/>    #main update function</span><span id="effd" class="jy jz hi ju b fi ke kb l kc kd">    for iteration in range (num_iter):<br/>        <br/>        #update and append theta<br/>        theta_cur = theta - alpha * gradient_loss_f(theta, x, y)<br/>        theta_history.append(theta_cur)<br/>        <br/>        #calculate and append loss using cur_theta<br/>        cur_loss = loss_f(theta_cur, x, y)<br/>        loss_history.append(cur_loss)<br/>        <br/>        #reset theta<br/>        theta = theta_cur</span><span id="b110" class="jy jz hi ju b fi ke kb l kc kd">return theta, theta_history, loss_history</span><span id="4fea" class="jy jz hi ju b fi ke kb l kc kd">#running gradient descent <br/>theta_start = init_theta()<br/>theta_hat, thetas_used, losses_calculated = grad_desc(<br/>    sin_MSE, sin_MSE_gradient, theta_start, part_1_data, num_iter=20, alpha=0.1<br/>)<br/>for b, l in zip(thetas_used, losses_calculated):<br/>    print(f"theta: {b}, Loss: {l}")</span></pre></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="68f6" class="me jz hi bd lb mf mg mh lf mi mj mk lj ml mm mn lm mo mp mq lp mr ms mt ls mu bi translated"><span class="l mv mw mx bm my mz na nb nc di"> E </span>体验更多的梯度下降</h1><h2 id="cb4d" class="jy jz hi bd lb lc ld le lf lg lh li lj iq lk ll lm iu ln lo lp iy lq lr ls lt bi translated"><strong class="ak">衰减阿尔法(学习率)梯度下降</strong></h2><p id="f7d3" class="pw-post-body-paragraph if ig hi ih b ii lu ik il im lv io ip iq lw is it iu lx iw ix iy ly ja jb jc hb bi translated"><strong class="ih hj">直觉:</strong></p><p id="6feb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过衰减的学习速率，而不仅仅是一个数字𝛼，学习现在应该是𝛼/(𝑡+1)其中𝑡是梯度下降的当前迭代的数字。</p><p id="5c8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你几乎可以回收上面的大部分代码，而不是每次迭代只更新𝛼到𝛼/(iteration+1。</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="a0e3" class="jy jz hi bd lb lc ld le lf lg lh li lj iq lk ll lm iu ln lo lp iy lq lr ls lt bi translated">随机梯度下降</h2><p id="c3d0" class="pw-post-body-paragraph if ig hi ih b ii lu ik il im lv io ip iq lw is it iu lx iw ix iy ly ja jb jc hb bi translated"><strong class="ih hj">直觉:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nd"><img src="../Images/691995071a950b4841190d8f178de8b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tNzpI42xtLjCDFXUNSpC6Q.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">每批大小的θ更新函数</figcaption></figure><p id="a8ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的更新规则中，𝑏(批量大小)比𝑛(数据的总大小)小得多。我们将使用一个静态学习率和一个参数<code class="du ma mb mc ju b">batch_size</code>来表示每次迭代的小批量样本的大小。</p><p id="a1d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面代码的主要变化是我们迭代时的更新功能:</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="adc6" class="jy jz hi ju b fi ka kb l kc kd">for iteration in range (num_iter):<br/>        #sample batch size each iterations <br/>        sample = data.sample(batch_size)<br/>        out_arr_x = sample['x']<br/>        out_arr_y = sample['y']<br/>        <br/>        #update and append theta<br/>        theta_cur = theta - alpha * gradient_loss_f(theta, out_arr_x, out_arr_y)<br/>        theta_history.append(theta_cur)<br/>        <br/>        #calculate and append loss using cur_theta<br/>        cur_loss = loss_f(theta_cur, out_arr_x, out_arr_y)<br/>        loss_history.append(cur_loss)<br/>        <br/>        #reset<br/>        theta = theta_cur</span></pre><p id="f050" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:对于cur_loss，我们计算并保存每个批量的loss_val，而不是整个数据集。</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="720b" class="jy jz hi bd lb lc ld le lf lg lh li lj iq lk ll lm iu ln lo lp iy lq lr ls lt bi translated">可视化和比较性能</h2><p id="77b3" class="pw-post-body-paragraph if ig hi ih b ii lu ik il im lv io ip iq lw is it iu lx iw ix iy ly ja jb jc hb bi translated">让我们想象一下我们的函数，看看每个函数在收敛到全局最小值时的表现。</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="d9eb" class="jy jz hi ju b fi ka kb l kc kd">plt.plot(np.arange(len(loss)), loss, label='Static Alpha')<br/>plt.plot(np.arange(len(loss)), loss_decay, label='Decaying Alpha')<br/>plt.plot(np.arange(len(loss)), loss_stoch, label='SGD')<br/>plt.xlabel('Iteration #')<br/>plt.ylabel('Avg Loss')<br/>plt.title('Avg Loss vs Iteration # vs Learning Rate Type')<br/>plt.legend();</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ne"><img src="../Images/ec31e0f679b78138e9c55270ebd8bc7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*WdKcVIaLUG0YU-ckzz61QQ.png"/></div></figure><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="0b95" class="jy jz hi ju b fi ka kb l kc kd">#3D plot (gradient descent with static alpha)<br/>plot_3d(thetas[:, 0], thetas[:, 1], loss, average_squared_loss, sin_model, x, y)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nf"><img src="../Images/bb94415c6b84413a0f12498ce65f6515.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i5oYS9SBhKGjHxdK8AU0eg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">静态阿尔法梯度下降</figcaption></figure><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="50b9" class="jy jz hi ju b fi ka kb l kc kd">#3D plot (gradient descent with decaying alpha)<br/>plot_3d(thetas_decay[:, 0], thetas_decay[:, 1], loss_decay, average_squared_loss, sin_model, x, y)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ng"><img src="../Images/bfd35f15ea46b1f88ec3313c174f7aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RH6q6qzMFx-Lc74RHiRcVQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">阿尔法衰减的梯度下降</figcaption></figure><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="3184" class="jy jz hi ju b fi ka kb l kc kd">#3D plot (stochastic gradient descent)<br/>plot_3d(thetas_stoch[:, 0], thetas_stoch[:, 1], loss_stoch, average_squared_loss, sin_model, x, y)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nh"><img src="../Images/0bcfe3c88902367833305364262d7264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMnU6bSSP-Fc37bdXOYKJg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">随机梯度下降</figcaption></figure></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="33b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi ni translated"><span class="l mv mw mx bm my mz na nb nc di"> G </span>梯度下降为我们提供了一种通用的方法，当我们无法解析地求解θ的最小值时，可以使损失函数最小化。随着我们的模型和损失函数越来越复杂，我们将转向梯度下降作为拟合模型的工具。</p><p id="d1e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于计算大型数据集可能成本高昂且耗时较长，因此随机GD计算更新比批量梯度下降快得多。到批量梯度下降完成一次更新时，它可以朝着最优θ取得显著进展。</p><p id="eb5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">而衰减的αGD考虑了更新学习率，这有利于防止过冲，因此步长更精确。</p></div></div>    
</body>
</html>