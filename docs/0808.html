<html>
<head>
<title>What is Topic Modeling?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是主题建模？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-is-topic-modeling-161a76143cae?source=collection_archive---------9-----------------------#2021-02-01">https://medium.com/analytics-vidhya/what-is-topic-modeling-161a76143cae?source=collection_archive---------9-----------------------#2021-02-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/1fbe787e850e142c1c2a5ba41f8b18db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MrrnZRE5iwVuz-7uX90_vg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Joyce McCown 在<a class="ae iu" href="https://unsplash.com/s/photos/pages?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="c034" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">主题建模是一种统计建模工具，用于评估一组文档中讨论的所有抽象主题。主题建模通过其构造解决了以无监督的方式创建主题的问题。</p><p id="146f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一般来说，使用统计方法是因为考虑到每个文档谈论几个不同的主题，而每个主题通常由词的分布来表示。也就是说，假设文档有一个两步结构，例如文档=[主题1，主题2，主题3，…，主题N]，然后主题1 = [w 1，w 2，…，w N]。</p><p id="78cb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">很明显，主题建模一般是通过统计字数、其比例以及相关指标来进行的。我将基于我刚刚讨论的想法描述两个模型，LSA(潜在语义分析)和LDA(潜在狄利克雷分析)。</p><p id="bd00" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，主题建模是一种无监督的机器学习方法，用于从未标记的数据中建模主题，因此它可以在没有任何训练的情况下工作。由于显而易见的原因，主题建模因此是一个粗略的开始方法，而不是一个复杂的最终解决方案。</p><p id="2b0b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">主题分类建模<br/> </strong>虽然主题建模是一种无监督的建模，但我们需要针对我们的定制主题训练模型，以实现高端和更准确的系统使用。例如，如果您正在构建用于检测支持票证的分类模型，在这种情况下，您可能希望为票证分配特定的主题，而不是像我们在主题建模中那样从票证创建主题。因此，主题分类模型更像是一个简单的文本分类模型，我们将文本分类到多个主题中。</p><p id="9570" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">主题建模与主题分类建模的区别<br/> </strong>正如我在上面的段落中所描述的，很明显，主题分类是一个有监督的过程，而主题建模是一个无监督的建模工作。在一般的建模工作中，我们从主题建模开始，然后找出自然出现的主题。最后，我们创建人类策划的主题，用于主题分类建模。</p><p id="2c67" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">两种不同的主题建模技术:</p><p id="143c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将讨论主题建模的两种主要方法。这两种方法是:</p><ol class=""><li id="c7a8" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">LSA:潜在语义分析</li><li id="7ea9" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">LDA:潜在狄利克雷分析</li></ol><h1 id="203a" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak"> LSA:潜在语义分析</strong></h1><p id="209d" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">这是一种利用词包和术语-文档矩阵来检测主题的方法。LSA背后的假设是，不同的主题有不同的词汇分布，以及不同的主题有不同的主题分布。</p><p id="a336" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">o用数学方法解决这个问题，我们首先从语料库中创建一个单词包。然后，我们创建一个单词-文档频率矩阵，其中行中有单词，列中有不同的文档。因此，矩阵的单元(I，j)表示单词w_i在文档D_j中出现了多少次</p><p id="b3ac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们使用LSA的上述假设将这个矩阵分解为三个矩阵，它们是:</p><ol class=""><li id="1f6d" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">单词-主题频率矩阵:在这个矩阵中，单词在行，主题在列。每个单元格表示一个单词在相应列中的出现。[MxN矩阵]</li><li id="68aa" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">主题重要性矩阵:这是一个对角矩阵，矩阵的第I个对角元素表示主题的重要性</li><li id="edef" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">主题-文档矩阵:在这个矩阵中，主题在行，文档在列，每个单元格表示一个主题在相应文档中的权重。[NxM矩阵]</li></ol><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/29ba76daa174c8952ecfc013df9f0175.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*H_tDboyb665vzt1H.jpg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:https://goo.gl/images/Fsw2ak</figcaption></figure><p id="d5c7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于矩阵使用，我们使用奇异值分解矩阵分解。使用SVD，我们分解矩阵M如下:</p><p id="2604" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">M = UDV</p><p id="b0c7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中U是酉矩阵，D是对角矩阵，V也是酉矩阵。这确保了中间矩阵是对角线。现在你可能会问，为什么我们要用这个特殊的矩阵分解。</p><p id="5a6d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个问题的答案是，从线性代数的角度考虑，SVD将线性映射转换为不同的基表示，这样它就变成了d。例如，因为U和V是酉矩阵，所以我们可以写为，</p><p id="c26f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">M = UDV =&gt; UTMVT = D</p><p id="2269" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这可以解释为术语频率矩阵基本上是通过创建由主题(单词的线性组合)构成的基础，然后通过另一个从主题到单词的映射来表示从单词到矩阵的映射。</p><p id="fc50" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，由于原始svd将对角矩阵表示为改变的基表示的SVD(某种重要性值);这清楚地暗示了在这种情况下对角矩阵代表主题的权重(这是新的基础)。</p><h1 id="84c1" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak"> LDA:潜在狄利克雷分析</strong></h1><p id="0f10" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">LDA是LSA的一个重大改进，因为LSA认为在文档结构中没有概率确定。在LDA，这是主要的变化。</p><p id="4a07" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">潜在狄利克雷分配(LDA)是一种语料库的生成概率模型。其基本思想是将文档表示为潜在主题的随机混合，其中每个主题都以单词分布为特征。LDA为语料库D中的每个文档w假设以下生成过程:</p><ol class=""><li id="8a2a" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">选择N，它服从泊松分布，λ= e</li><li id="ccc0" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">选择θ，它遵循狄利克雷分布，α=α</li><li id="210d" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">对于N中N的每个单词w_n:选择一个服从θ的多项式分布的主题z_n，选择一个来自p(w_n|z_n，beta)的单词w_n，p是以主题z_n为条件的多项式概率</li></ol><p id="bb29" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由此，我们可以计算出理论上依赖于α，β的单字的概率分布。对于不同的单字，我们也可以从实际数据集得到分布。</p><p id="8fba" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了最终解决这个问题，我们需要优化α和β，以减小理论概率分布和观测概率分布之间的kullback leibler散度距离</p></div></div>    
</body>
</html>