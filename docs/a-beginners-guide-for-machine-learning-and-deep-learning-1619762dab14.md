# 机器学习和深度学习初学者指南。

> 原文：<https://medium.com/analytics-vidhya/a-beginners-guide-for-machine-learning-and-deep-learning-1619762dab14?source=collection_archive---------18----------------------->

![](img/d1007acad4ccd587a598ac5e17584bc5.png)

约翰·莫塞斯·鲍恩在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

> 理解数据表示

在继续之前，让我们查找一个名为 mnist 的样本数据集。这是许多机器学习和深度学习实践中最常用的数据集之一。你可以从 http://yann.lecun.com/exdb/mnist/的[下载](http://yann.lecun.com/exdb/mnist/)

可从该页面获得的 MNIST 手写数字数据库具有 60，000 个样本的训练集和 10，000 个样本的测试集。这是从 NIST 可获得的更大集合的子集。数字已经过大小标准化，并在固定大小的图像中居中。

对于那些想在真实世界的数据上尝试学习技术和模式识别方法，同时花费最少的精力进行预处理和格式化的人来说，这是一个很好的数据库。

不幸的是，我们不打算覆盖这个数据集太多，它只是用来参考理解进一步的主题提到。

**涵盖的主题**

*   关于张量及其类型的简单描述。
*   关键属性(轴数、数据类型、形状)
*   数据张量的真实世界示例。
*   张量运算(整形、点积、广播)
*   神经网络引擎(基于梯度的优化，导数)

> 张量

张量是存储在多维数组中的数据(数字)的容器。一般来说，所有当前的机器学习系统都使用张量作为它们的基本数据结构。2D 张量是矩阵到任意维数的推广。在张量的情况下，一个维度通常称为轴。

> 张量/张量表示的类型

*   ***Scaler/0D Tensors****—只包含一个 eg 的数字。*

*数组(32)*

*   ****向量/ 1D 张量****——一个数字数组称为向量或 1D 张量，例如**

*np.array([1，2，3，4，5])*

*上述向量有 5 个条目，并被表示为 5 维向量，请不要将 5D 向量视为 5D 张量。 ***一个 5D 向量只有一个轴和 5 个沿轴的维度，而一个 5D 张量有 5 个轴。****

*维数可以表示为沿特定轴的条目数或张量中的轴数。是的，同意它令人困惑。*

*   ****矩阵/ 2D 张量—*** *一个* n 数组的向量是一个矩阵，一个矩阵有 2 个轴常指行和列。你可以直观地看到。*

*np.array([[1，2，3，4，5]，[6，7，8，9，10]，[11，12，13，14，15]])*

*   ****3D 张量和高维张量—*** 如果你把这样的矩阵打包成一个新的数组，你会得到一个 3D 张量，通过打包一个 3D 张量，你会得到一个 4D 张量，等等。*

> *关键性控制程序*

*   ****轴的数量*** *—您可以使用名为 ndim 的简单 numpy 函数查看一个张量轴。**
*   ****Shape*** —您可以使用 numpy 中名为 Shape 的函数查看张量的形状，该函数返回一个整数元组，该元组描述了张量沿每个轴有多少维。*
*   ****数据类型或 dtype*** —您可以使用名为 dtype 的 numpy 函数查看您的张量数据类型，数据类型如“float32”、“float 64”、“unit8”等。*

> *数据张量的真实世界示例*

*让我们用几个与我们以后会遇到的例子相似的例子来使张量更具体。我们将要操作的数据几乎属于上述类别之一*

*   *矢量数据/ 2D 张量形状*
*   *时间序列/序列/ 3D 张量形状*
*   *图像/ 4D 张量形状*
*   *视频数据/ 5D 张量形状*

> *矢量数据/ 2D 张量数据*

*这是最常见的情况，其中每个数据点被编码为向量，因此一批数据将被编码为 2D 张量(向量阵列)，其中第一轴是样本轴，第二轴是特征轴。*

****为 eg* 。**文本文档的数据集，表示每个单词在其中出现的次数(假设一个字典有 10，000 个单词)。每个文档可以被编码为 10，000 个值的向量，因此 600 个单词的条目可以被存储到(600，10000)的张量中。*

> *时间序列/序列/ 3D 张量形状*

*在具有明确时间轴的 3D 张量中存储顺序或时间序列数据总是有意义的。每个样本可以被编码为一个矢量(2D 张量)，因此一批数据将被编码为一个三维张量。*

****为 eg* 。**一个推文数据集，其中我们将每条推文编码为 128 个独特字符的字母表中的 280 个字符的序列，因此每条推文可以编码为形状的 2D 张量(280，128)，10，000 条推文的数据集将得到(10000，280，128)。*

> *图像/ 4D 张量形状*

*图像基本上有三维高度，宽度和颜色深度。尽管灰度数据(特别是像 mnist 数据)具有单一的颜色通道，因此可以存储在 2D 张量中，但是按照惯例，图像张量对于灰度图像总是具有一维颜色通道的 3D。因此，一批大小为 256 × 256 的 128 幅灰度图像可以存储在形状张量(128，256，256，1)中，一批 128 幅彩色图像可以存储在形状张量(128，256，256，3)中。*

> *视频数据/ 5D 张量形状*

*视频数据是少数几种需要 5D 张量的真实世界数据之一。视频可以理解为一系列帧，每一帧都是一幅彩色图像。因为每个帧可以存储在 3D 张量(高度，宽度，颜色深度)中，所以帧序列可以存储在 4D 张量(帧，高度，宽度，颜色深度)中，因此一批不同的视频可以存储在形状的 5D 张量(样本，帧，高度，宽度，颜色深度)中。例如，以每秒 4 帧的速度采样的 60 秒、144 × 256 的 YouTube 视频剪辑将有 240 帧。一批四个这样的视频剪辑将被存储在形状张量(4，240，144，256，3)中。总共有 106，168，320 个值！如果张量的数据类型是 float32，那么每个值将存储在 32 位中，因此张量将表示 405 MB。沉重！你在现实生活中遇到的视频要轻得多，因为它们不是存储在 float32 中的，它们通常是以较大的系数压缩的(如 MPEG 格式)。*

> *神经网络或张量运算的齿轮*

*   ****元素式操作*** —考虑独立应用于张量中每个条目的操作。*
*   ****广播—*** 通过广播，您可以轻松应用 2 个张量元素的方式操作。*
*   ****张量点—*** 点运算也称为乘积运算，是最常见、最有用的张量运算。与逐元素运算相反，它合并输入张量中的元素。*
*   ****张量整形—*** 张量整形是指重新排列其行和列，以匹配目标形状。自然地，整形后的张量具有与初始
    张量相同的系数总数。*

> *神经网络的引擎*

*   ****基于梯度的优化—*** 神经网络层通常将其输入数据转换为 eg。*

*outputlayer=relu(dot(W，input)+b)*

*在该表达式中，W 和 b 是张量，它们是层的属性，被称为层的 ***【权重】*** 和 ***可训练参数*** 。这些权重包含网络从训练数据的暴露中学习到的信息。最初，这些权重矩阵用小的随机值填充(这一步称为随机初始化)。当然，当 W 和 b 是随机的时，没有理由期望 relu(dot(W，input) + b)会产生任何有用的表示。由此产生的表示毫无意义——但它们是一个起点。接下来是根据反馈信号逐渐调整这些权重。这种逐步调整，也叫
训练。*

*这发生在所谓的训练循环中，其工作方式如下。根据需要，循环重复这些步骤:*

*   *画出一批训练样本 x 和对应的目标 y。*
*   *在 x 上运行网络(称为正向传递的步骤)以获得预测 y_pred。*
*   *计算批次上的网络损失，这是 y_pred 和 y 之间不匹配
    的度量。*
*   *以略微减少该批次损失的方式更新网络的所有权重。*

*我们最终得到一个训练数据损失非常低的网络:预测 y_pred 和预期目标 y 之间的不匹配很低。网络已经“学会”将其输入映射到正确的目标。从远处看，它可能看起来像魔术，但当我们把它简化为基本步骤时，它就变得简单了。*

****张量运算的导数:梯度—*** 梯度是张量运算的导数。它是导数概念
对多维输入函数的推广:即以张量为输入的函数。单个系数的函数 f(x)的导数可以解释为 f 的曲线的斜率。同样，梯度(f)(W0)可以解释为描述 f(W)在 W0 周围的曲率的张量。*

*   ***应用于神经网络，这意味着 ***解析地找到产生最小可能损失函数*** 的权重值的组合。这可以通过求解等式 gradient(f)(W) = 0 来实现。***
*   ****链式导数:反向传播算法—*** 在前面的算法中，我们很随意地假设，因为一个函数是可微的，所以我们可以显式地计算它的导数。实际上，一个神经网络函数由许多连锁在一起的张量运算组成，其中每一个都有一个简单的已知导数。例如，这是由三个张量运算 a、b 和 c 组成的网络 f，具有权重矩阵 W1、W2 和 W3:*

*f(W1，W2，W3) = a(W1，b(W2，c(W3)))*

*微积分告诉我们，这样的函数链可以用下面的恒等式导出，叫做链法则:f(g(x)) = f'(g(x)) * g'(x)。将链规则应用于神经网络梯度值的计算产生了一种称为反向传播(有时也称为反向模式微分)的算法。反向传播从最终损失值开始，从顶层到底层反向工作，应用链规则来计算每个参数在损失值中的贡献。*

****谢谢。****