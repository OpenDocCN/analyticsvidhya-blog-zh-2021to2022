<html>
<head>
<title>Deep dive into Decision Tree Part II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入探讨决策树第二部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-dive-into-decision-tree-part-ii-e1b5e60ab734?source=collection_archive---------17-----------------------#2021-01-03">https://medium.com/analytics-vidhya/deep-dive-into-decision-tree-part-ii-e1b5e60ab734?source=collection_archive---------17-----------------------#2021-01-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/0ba53182bfe893cbbe29e26d577032d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pE-YlMO3bmKKFndfr8BJog.jpeg"/></div></div></figure><p id="e419" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">大家好🙌。</p><p id="0611" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在前面的第一部分中，我们建立了一个决策树模型来分类花的类型。</p><p id="ff70" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将回顾之前创建的模型，并尝试理解模型提出的问题以及它为什么会提出某些问题。</p><p id="8e64" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于那些没有读过《T2》第一部的人，推荐你读一读，因为这个博客是它的延续。</p><p id="9209" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">以下是将要涉及的一些关键词:</p><ul class=""><li id="fc78" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">信息增益</li><li id="bb3f" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">杂质测量:基尼杂质，熵，分类误差</li><li id="4bad" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">树木修剪</li><li id="53a5" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">过度拟合</li></ul></div><div class="ab cl kc kd go ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ha hb hc hd he"><p id="4735" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们想象一下我们的决策树。</p><figure class="kj kk kl km fd ii"><div class="bz dy l di"><div class="kn ko l"/></div></figure><p id="0083" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">sklearn中的Tree类有plot_tree方法，它允许我们通过matplotlib可视化决策树。</p><figure class="kj kk kl km fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kp"><img src="../Images/7dc5ad34a7580b187e9846aad7ee88d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*83bmPxxa1yq-pNdQSf7OnQ.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">iris _ data _决策树. png</figcaption></figure><p id="f3f2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上面是我们的决策树，你可以看到它问了什么问题来分离数据。从根节点开始，它通过询问“花瓣宽度(cm) ≤ 0.8”来分离数据，回答为真的数据将移到左边的子节点，否则移到右边的子节点。(注意X[3]→X数据帧中的第4列)。重复这个过程，直到叶子是纯的(所有数据都是同一类)，这通常导致过度拟合。</p><p id="bf8e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这就是树修剪的用武之地，为了避免过度拟合，我们必须设置停止分裂的标准，因此它不会无限分裂，直到所有的叶子都是纯净的。</p><p id="9c83" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">可以使用sk learn . tree . decision tree classifier中的参数来调整停止拆分标准，以下是一些常见的标准:</p><ul class=""><li id="fab4" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><strong class="ir hi"> min_samples_split </strong>:当节点包含少于指定限制n时停止</li><li id="da8b" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi"> max_depth </strong>:询问到树的深度为N(从根节点开始=深度1)后停止，否则继续，直到所有叶子都是纯的。</li><li id="f8cd" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">min _ infinity _ split:</strong>当节点杂质低于n%时停止。例如，如果n = 5，那么当超过95%的数据属于a类时，树停止分裂。</li><li id="a489" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">min _杂质_减少:</strong>如果没有导致更多的信息增益，停止分裂。</li></ul></div><div class="ab cl kc kd go ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ha hb hc hd he"><p id="8c09" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">看着我们的决策树，好奇的人可能会问以下问题</p><ol class=""><li id="aab7" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm ku ju jv jw bi translated">为什么它用≤ 0.8(花瓣宽度(cm))来分隔数据？</li><li id="fd17" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm ku ju jv jw bi translated">为什么它先用第4列(花瓣宽度(cm))呢？</li></ol><p id="3a0d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">决策树喜欢获取尽可能多的信息，所以它所做的每一个动作都取决于它获取了多少信息。</p><p id="75a5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于<code class="du kv kw kx ky b">petal width</code>列来说，选择导致最大信息增益分裂值，遵循以下步骤:</p><p id="157f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">I)根据<code class="du kv kw kx ky b">petal width</code>列对值进行升序排序。</p><p id="ba69" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">ii)计算所有行的相邻值之间的平均值。</p><figure class="kj kk kl km fd ii er es paragraph-image"><div class="er es kz"><img src="../Images/45aa27a6f342137ab983b01aaf191d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*jRLv9l1IfkErveg8_U0fxg.png"/></div></figure><p id="897b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">iii)对于每个平均值，计算信息增益。</p><p id="c273" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">iv)从步骤iii中选择输出最大值(信息增益)的平均值。</p><p id="8c77" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们的数据集只包含连续的列，但是其他类型(如分类和排序数据类型)更容易处理。</p><p id="2c42" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于分类列，只需问类似“类别= A？”、“类别= B”等...并且首先选择导致最多信息增益的一个来使用。</p><p id="63dc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于排名列，只需问类似“排名≤ 1？”，“秩≤ 2？”等等……首先选择能获得最多信息的问题。</p><p id="a6a2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是我们如何计算信息增益呢？别担心，公式如下:</p><figure class="kj kk kl km fd ii er es paragraph-image"><div class="er es la"><img src="../Images/526e1c5137d6f95d8f2fc134b5ecf699.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*PdYW0yZj-xvAl12wstL_WQ.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">信息增益公式</figcaption></figure><p id="7084" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">I(Dp) =父节点的杂质测量</p><p id="a1f2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">I(Di) =子节点I的杂质测量值</p><p id="1371" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Ni =子节点I中的数据数量</p><p id="a5d0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Np =父节点中的数据数量</p><p id="73db" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因为我们只考虑二叉树，所以我们的公式可以简化成这样</p><figure class="kj kk kl km fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lb"><img src="../Images/cba1bb31331c06eb1d78a26c3aa9e31c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TNjYH10O8Y2kpP0E6itz7g.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">信息增益公式，二叉决策树</figcaption></figure><p id="8a2c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">决策树中的另一个重要概念是杂质。这是对你的节点有多不纯的度量，意味着你的节点有多混杂(只有一个类的节点被称为纯节点，0不纯)。</p></div><div class="ab cl kc kd go ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ha hb hc hd he"><p id="fe7c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有三种常用杂质测量方法</p><p id="0301" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">I)基尼不纯度</p><p id="5787" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">二)熵</p><p id="c50b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">iii)分类错误</p><p id="4a7b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">基尼杂质</strong></p><figure class="kj kk kl km fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lc"><img src="../Images/07a80d8afebb7c4730f0cd88f38c7720.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*umuZULCe7WAWdlaxbDpcJA.png"/></div></div></figure><ul class=""><li id="b2ad" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">可以理解为最小化错误分类概率的标准。</li></ul><p id="d028" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">熵</strong></p><figure class="kj kk kl km fd ii er es paragraph-image"><div class="er es ld"><img src="../Images/5c14606f29469d5a2b1ad5faa5c0cea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*FgdYoiSKqRbt9AL2MsiFLg.png"/></div></figure><ul class=""><li id="d19e" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">熵的范围是从[0，1]。0表示最纯，1表示最不纯。</li><li id="d708" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">在实践中，熵通常会产生与基尼系数相似的结果，因此没有必要花费太多时间来评估使用哪种杂质指标。应该花更多的时间来试验不同的修剪切断。</li></ul><p id="37dd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">分类错误</strong></p><figure class="kj kk kl km fd ii er es paragraph-image"><div class="er es le"><img src="../Images/5de9840b294f0f0c8691468a7afc5564.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*ap9HCDf2zl_kSYuPoaoddw.png"/></div></figure><ul class=""><li id="caa4" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">树后修剪的有用标准(在树构建后进行修剪)，不推荐用于生长树，因为它对节点的类%的变化不敏感。</li></ul><p id="b5de" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">p(I | t)= t(节点)中第一类的概率</p></div><div class="ab cl kc kd go ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ha hb hc hd he"><p id="28a6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们知道了如何计算信息增益，让我们通过编写自己的代码来仔细检查，看看按花瓣宽度≤ 0.8(由adj_avg选择)分割数据是否确实会导致该列的最大信息增益。</p><figure class="kj kk kl km fd ii"><div class="bz dy l di"><div class="kn ko l"/></div></figure><p id="4ac6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们的最后一行代码按“ig”的升序排列数据帧。以下是输出:</p><figure class="kj kk kl km fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lf"><img src="../Images/8f5bdb76ac3db0b08fb447036ac3eab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X73HrnYuichlqAdJTpVTQQ.png"/></div></div></figure><p id="7cd7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你可以看到，当<code class="du kv kw kx ky b">petal width (cm)</code>列时，0.8cm花瓣宽度的分裂(从adj_avg列计算)导致最大(信息增益)。</p><p id="006b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们回答了为什么花瓣宽度列使用0.8的值来分割数据，现在是时候回答为什么它首先使用花瓣宽度列了？而不是萼片长度或任何其他列？</p><p id="8a0e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">同样，决策树首先使用导致最大信息增益列。就像我们对<code class="du kv kw kx ky b">petal width</code>列所做的一样，对所有其他列也这样做，比较所有列的最大信息增益值，然后首先使用包含最大信息增益的列。</p><p id="8753" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">每次分割后重复此过程(前一个子对象成为父对象)，直到满足停止分割标准或所有叶子都是纯的。你可以在我们的决策树中看到，在第一次分裂后，树的左边停止了，因为它是纯的，只剩下树的右边了。</p></div><div class="ab cl kc kd go ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ha hb hc hd he"><p id="4ea9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">总之，决策树是一个非常简单的模型，具有高度的可解释性，因此，当您需要与非技术人员交流您的发现时，通常会用到它。它也是随机森林的基础，随机森林是将多个决策树组合在一起，遵循多数表决原则来回答问题，通常可以减轻单个决策树的过拟合行为。</p></div><div class="ab cl kc kd go ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ha hb hc hd he"><p id="4938" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢您的阅读！反馈是非常受欢迎的，请评论，如果有任何错误信息，我会尽快纠正他们！</p></div><div class="ab cl kc kd go ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ha hb hc hd he"><p id="edb5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">参考资料:</p><ul class=""><li id="9fff" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">克莱恩决策树文档</li><li id="cca4" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><a class="ae jn" href="https://www.packtpub.com/product/python-machine-learning-third-edition/9781789955750" rel="noopener ugc nofollow" target="_blank"> Python机器学习第3版第3章</a></li><li id="6272" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><a class="ae jn" href="https://math2510.coltongrainger.com/books/2017-bruce-and-bruce-pratical-statistics-for-data-scientists.pdf" rel="noopener ugc nofollow" target="_blank">数据科学家实用统计学第1版第6章</a></li><li id="705d" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><a class="ae jn" href="https://www.youtube.com/watch?v=7VeUPuFGJHk&amp;t=33s&amp;ab_channel=StatQuestwithJoshStarmer" rel="noopener ugc nofollow" target="_blank">决策树教程视频第一部分</a></li><li id="62a1" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><a class="ae jn" href="https://www.youtube.com/watch?v=wpNl-JwwplA&amp;ab_channel=StatQuestwithJoshStarmer" rel="noopener ugc nofollow" target="_blank">决策树教程视频第二部分</a></li><li id="a99c" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><a class="ae jn" href="http://www.alanfielding.co.uk/multivar/crt/dt_example_04.htm" rel="noopener ugc nofollow" target="_blank">分割停止标准</a></li></ul></div></div>    
</body>
</html>