<html>
<head>
<title>K-nearest Neighbor: The maths behind it, how it works and an example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k近邻:背后的数学原理，工作原理和一个例子</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-nearest-neighbor-the-maths-behind-it-how-it-works-and-an-example-f1de1208546c?source=collection_archive---------3-----------------------#2021-05-17">https://medium.com/analytics-vidhya/k-nearest-neighbor-the-maths-behind-it-how-it-works-and-an-example-f1de1208546c?source=collection_archive---------3-----------------------#2021-05-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/04b211bcb111c036399c56ea7996451b.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/0*rc5_e6-6AHzqppcr"/></div></figure><p id="5a8b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">k-最近邻(KNN)是一种监督分类算法，它基于通过查找与基础数据的相似性来预测数据。KNN最广泛地用于分类问题，但也可用于解决回归问题。最初的假设是数据以簇的形式存在，或者以非常接近的方式存在。KNN是一种非参数算法，这意味着它不会试图对数据进行假设，例如，它不关心数据是否正态分布。</p><p id="0f22" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">那么KNN是如何实现的呢？现在让我们来复习一下。初始化K的值，K将是我们将尝试并关联新预测的接近数据点的总数。</p><p id="26ce" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们必须知道的一件事是，我们如何选择k的值。显然，这里没有适合所有人的大小，但正统和首选的值通常是5。1或2可能太低，无法捕捉未知或新数据点周围的大量信息，甚至可能指向异常值或过于具体的数据点，如果太多，您可能会有一个过于笼统的模型。</p><p id="9124" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">2.计算所有数据点和新数据点之间的相似性。这是使用欧几里德距离来完成的。你以为你逃脱了数学的部分，是吗？我们开始吧，好吗？</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es jj"><img src="../Images/ca353297484c196817e9871da72164e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/0*vlCcihtVCS9G-6ip"/></div></figure><p id="8444" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这个等式表示点<em class="jo"> p </em>和点<em class="jo"> q </em>之间的距离，也是从<em class="jo"> q </em>到<em class="jo"> p </em>的距离。要计算距离，我们将每个坐标的维度相减，求和，应用2的幂，然后平方根。让我们举一个例子:我们希望找到点(3，4)和点(11，7)之间的距离。注意:在大多数情况下，你会发现多维数据点之间的距离远远大于2。不过，让我们继续，等式应该是这样的:</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es jp"><img src="../Images/48861669abf536cdddb4579eca7bec33.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/0*DkQ6p1IjfTZ5UfRm"/></div></figure><p id="023f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">结果是8.544。</p><p id="7a24" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">3.使用上述欧几里德距离在训练数据中选择最近的K个观察值。</p><p id="c3a2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">4.使用最流行的响应作为新数据点的预测。</p><p id="e13e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在，为了知道哪个K对于数据集是正确的，我们使用不同的值运行KNN算法几次，例如在3和6之间的范围上，因此运行算法4次。一个好的做法是尝试为K选择一个奇数，以避免票数相等，并实现平局决胜。</p></div></div>    
</body>
</html>