<html>
<head>
<title>Support Vector Machines: SVM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机:SVM</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/support-vector-machines-svm-87841ab63b8?source=collection_archive---------12-----------------------#2021-01-20">https://medium.com/analytics-vidhya/support-vector-machines-svm-87841ab63b8?source=collection_archive---------12-----------------------#2021-01-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="1c1d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从头开始的完整指南</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/ee622ddcd3642ec8e9fd1b4abc666ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/0*5I9iNCmi5SHSgdmb.png"/></div></figure><p id="f85b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你先了解线性回归和逻辑回归会更好。如果没有，请去探索它们(<em class="jk">你可以参考下面的帮助链接</em>)。</p><div class="jl jm ez fb jn jo"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/linear-regression-c6625caf9e8e"><div class="jp ab dw"><div class="jq ab jr cl cj js"><h2 class="bd hi fi z dy jt ea eb ju ed ef hg bi translated">线性回归</h2><div class="jv l"><h3 class="bd b fi z dy jt ea eb ju ed ef dx translated">单个和多个因变量</h3></div><div class="jw l"><p class="bd b fp z dy jt ea eb ju ed ef dx translated">medium.com</p></div></div><div class="jx l"><div class="jy l jz ka kb jx kc ji jo"/></div></div></a></div><div class="jl jm ez fb jn jo"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/logistic-regression-all-you-wanna-know-e7938f402663"><div class="jp ab dw"><div class="jq ab jr cl cj js"><h2 class="bd hi fi z dy jt ea eb ju ed ef hg bi translated">逻辑回归:你想知道的一切</h2><div class="jv l"><h3 class="bd b fi z dy jt ea eb ju ed ef dx translated">从头开始完成实施</h3></div><div class="jw l"><p class="bd b fp z dy jt ea eb ju ed ef dx translated">medium.com</p></div></div><div class="jx l"><div class="kd l jz ka kb jx kc ji jo"/></div></div></a></div><p id="1d65" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">支持向量机是一个非常强大的分类器，可以处理线性和非线性可分数据。它可以用于回归以及分类问题，但主要用于分类问题。</p><blockquote class="ke"><p id="a184" class="kf kg hh bd kh ki kj kk kl km kn jb dx translated">支持向量机有许多优点:</p></blockquote><ul class=""><li id="c525" class="ko kp hh ig b ih kq il kr ip ks it kt ix ku jb kv kw kx ky bi translated">在高维空间有效。</li><li id="83d9" class="ko kp hh ig b ih kz il la ip lb it lc ix ld jb kv kw kx ky bi translated">在维数大于样本数的情况下仍然有效。</li><li id="2dc3" class="ko kp hh ig b ih kz il la ip lb it lc ix ld jb kv kw kx ky bi translated">在决策函数中使用训练点的子集(称为支持向量)，因此它也是内存高效的。</li><li id="287d" class="ko kp hh ig b ih kz il la ip lb it lc ix ld jb kv kw kx ky bi translated">它是通用的，因为可以为决策函数指定不同的核函数。提供了通用内核，但是也可以指定定制内核。</li></ul><blockquote class="le lf lg"><p id="a2a8" class="ie if jk ig b ih ii ij ik il im in io lh iq ir is li iu iv iw lj iy iz ja jb ha bi translated">SVM的主要目标是找到n个最佳超平面，它们最好地分隔我们的数据，使得空间中最近的点到它自身的距离(也称为余量)最大化。这些最近的点被称为<strong class="ig hi">支持向量</strong></p></blockquote><p id="3deb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi lk translated"><span class="l ll lm ln bm lo lp lq lr ls di"> W </span>什么是超平面？</p><p id="4a17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">超平面是在<em class="jk"> n </em>维特征空间中<em class="jk"> n-1 </em>维的平面，它将两个类分开。对于2-D特征空间，它将是一条线，而对于3-D特征空间，它将是平面，等等。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lt"><img src="../Images/597e5ac72459a806ae541e334c10806d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/0*kDNdf-y0YnaRnCW2.jpg"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">2D空间</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ly"><img src="../Images/302bfbf9089cf2de4ecc0fb87025d399.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/0*QDy2DTKEtPvoP_n_.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">三维空间</figcaption></figure><p id="5fb9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi lk translated"><span class="l ll lm ln bm lo lp lq lr ls di"> S </span>支持向量是更接近超平面并影响超平面的位置和方向的数据点。使用这些支持向量，我们最大化分类器的余量。</p><h1 id="5f82" class="lz ma hh bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">最大边缘超平面</h1><p id="bb5a" class="pw-post-body-paragraph ie if hh ig b ih mx ij ik il my in io ip mz ir is it na iv iw ix nb iz ja jb ha bi translated">最佳超平面最好地分离我们的数据，使得空间中最近点(称为支持向量)到其自身的距离/余量最大化。</p><p id="8a49" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">超平面能够分离类，如果对于所有点-</p><h2 id="8376" class="nc ma hh bd mb nd ne nf mf ng nh ni mj ip nj nk mn it nl nm mr ix nn no mv np bi translated">w x + b &gt; 0</h2><p id="654f" class="pw-post-body-paragraph ie if hh ig b ih mx ij ik il my in io ip mz ir is it na iv iw ix nb iz ja jb ha bi translated">(对于类别1中的数据点)</p><h2 id="95fd" class="nc ma hh bd mb nd ne nf mf ng nh ni mj ip nj nk mn it nl nm mr ix nn no mv np bi translated">w x + b &lt; 0</h2><p id="8c32" class="pw-post-body-paragraph ie if hh ig b ih mx ij ik il my in io ip mz ir is it na iv iw ix nb iz ja jb ha bi translated">(对于0类数据点)</p><p id="68bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设超平面的方程是w x + b = 0，其中w是向量，b是截距。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nq"><img src="../Images/eaa37533c6113d0429b82a08f04c7d65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*t2S4QB9dcAloA_v1.png"/></div></figure><p id="5fd8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们还希望我们的预测是可信的，这意味着点与平面之间的距离越大，它就越可信，因为值的微小变化不会改变可信点的类别。</p><h2 id="ad49" class="nc ma hh bd mb nd ne nf mf ng nh ni mj ip nj nk mn it nl nm mr ix nn no mv np bi translated">处理SVM的异常值</h2><p id="27e3" class="pw-post-body-paragraph ie if hh ig b ih mx ij ik il my in io ip mz ir is it na iv iw ix nb iz ja jb ha bi translated">我们将允许我们的算法在训练样本上犯一些错误。对于每一个错误，都会涉及到一些成本，这些成本将被添加到我们的函数中。</p><h1 id="b6dc" class="lz ma hh bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">SVM背后的数学</h1><blockquote class="le lf lg"><p id="f1c1" class="ie if jk ig b ih ii ij ik il im in io lh iq ir is li iu iv iw lj iy iz ja jb ha bi translated">关键思想是最大限度地扩大差距，即最大限度地扩大最小距离点的距离。</p></blockquote><p id="81a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">超平面方程</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nr"><img src="../Images/343a885f7b7f8f5b7c34c02c41a34cd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/0*dYlgmt132TMFgNo2.png"/></div></figure><p id="8a51" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们所看到的，两个边距之间的距离是2/| | w | |(w的L2范数),必须最大化。因此||w||/2必须最小化。为了便于计算，我们取范数的平方。</p><p id="3c81" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们前面讨论的，这涉及到成本(损失)，称为铰链损失。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="er es ns"><img src="../Images/0452965ba34267c30a47ddd74c686754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TgFMskYN61t8_1za.png"/></div></div></figure><p id="1fe3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">类似地，当yi * (w.xi + b) &lt;1时，偏置项→ b = b -C X xi，而当该值≥1时，b不会更新。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nx"><img src="../Images/2bc7a9dd12cebcc33b7956d0e0c5e4bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/0*9p7KsZQxtzI7Nvqh.png"/></div></figure><p id="533b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用这个损失，我们使用学习率和梯度下降得到我们的最终权重和偏差。</p><p id="6172" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">w = w-lr *损失</p><p id="64b1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">偏差=偏差-lr *损耗</p><p id="5e0c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您可以通过查看以下GitHub资源库中的代码来理解它。</p><div class="jl jm ez fb jn jo"><a href="https://github.com/ads-22/ML-Practice/blob/main/Task-12%20SVM/SVM.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="jp ab dw"><div class="jq ab jr cl cj js"><h2 class="bd hi fi z dy jt ea eb ju ed ef hg bi translated">ads-22/ML-实践</h2><div class="jv l"><h3 class="bd b fi z dy jt ea eb ju ed ef dx translated">在GitHub上创建一个帐户，为ads-22/ML-Practice开发做贡献。</h3></div><div class="jw l"><p class="bd b fp z dy jt ea eb ju ed ef dx translated">github.com</p></div></div><div class="jx l"><div class="ny l jz ka kb jx kc ji jo"/></div></div></a></div></div></div>    
</body>
</html>