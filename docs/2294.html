<html>
<head>
<title>How to Predict Coronary Heart Disease Risk using Logistic Regression?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用Logistic回归预测冠心病风险？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-predict-coronary-heart-disease-risk-using-logistic-regression-c069ab95cbec?source=collection_archive---------3-----------------------#2021-04-17">https://medium.com/analytics-vidhya/how-to-predict-coronary-heart-disease-risk-using-logistic-regression-c069ab95cbec?source=collection_archive---------3-----------------------#2021-04-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="50ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">心脏病</strong>是全球<strong class="ih hj">死亡的首要原因</strong>，占2019年死亡人数的三分之一。心脏病病例在此期间增加了近一倍，从1990年的2.71亿增加到2019年的<strong class="ih hj">5.23亿</strong>，而<strong class="ih hj">心脏病死亡人数</strong>从1210万上升到<strong class="ih hj">1860万。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/a8933b136a7760e85e440fa1d1e1fc3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pEVAvMDQ_tnizfRV"/></div></div></figure><p id="5fcf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">冠心病</strong>由于心脏动脉中斑块的堆积，导致流向心肌的血液减少。</p><p id="463d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最具挑战性的任务之一是确定这种疾病的原因并尽可能地预防它。医疗诊断推理正在成为<strong class="ih hj">机器学习</strong>的流行应用，其中专家系统和基于模型的方案提供了用于开发假设的机制，然后将使用建模和模拟技术或统计分析对其进行测试。</p><p id="b51a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">心脏病的预测被认为是健康领域中最重要的课题之一。利用机器学习算法和大量数据，可以推断出可以帮助医生做出更准确预测的信息。</p><p id="3056" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑到获得准确结果所需的专业技能和知识水平，预测<strong class="ih hj">冠心病</strong>是一项非常复杂的挑战。根据世卫组织的一项调查，医学专家可以正确预测心脏病，但准确率只有67%。</p><p id="6dfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，在训练阶段将使用多个自变量，如<strong class="ih hj">性别、年龄、吸烟日、总胆固醇、sysBP </strong>和<strong class="ih hj">葡萄糖</strong>以及因变量(<strong class="ih hj"> TenYearCHD </strong> class)来建立分类模型。<strong class="ih hj">分类目标</strong>是预测患者未来10年是否有患冠心病<em class="jp">或冠心病</em> (CHD)的风险。</p><p id="9549" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">好了，我们开始吧！</strong></p><h1 id="7a8c" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">步骤1。导入所需的库</strong></h1><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="faf4" class="kt jr hi kp b fi ku kv l kw kx">import pandas as pd<br/>import numpy as np<br/>import statsmodels.api as sm<br/>import scipy.stats as st<br/>import matplotlib.pyplot as plt<br/>import seaborn as sn<br/>from sklearn.metrics import confusion_matrix<br/>import matplotlib.mlab as mlab<br/>%matplotlib inline</span></pre><h1 id="8405" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">步骤二。数据准备</strong></h1><p id="c2fd" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">数据集来自Kaggle网站(<a class="ae ld" href="https://www.kaggle.com/amanajmera1/framingham-heart-study-dataset" rel="noopener ugc nofollow" target="_blank">弗雷明汉心脏研究数据集</a>)。数据集提供了患者的信息。它包括4000多条记录和15个属性，如下所述。</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="62a4" class="kt jr hi kp b fi ku kv l kw kx">df = pd.read_csv('framingham.csv')<br/>df.drop(['education'],axis=1,inplace=True)<br/>df.rename(columns={'male':'sex_male'},inplace=True)<br/>df.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es le"><img src="../Images/e1ab0310decf2e0a9f5f32ededc65b53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pckhZS5uHqOY2L8DUxVnyw.png"/></div></div></figure><p id="700d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输入变量:</strong></p><ol class=""><li id="09fc" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc lk ll lm ln bi translated"><strong class="ih hj">性别</strong>:男或女(名义)</li><li id="96f9" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">年龄</strong>:患者年龄(连续)</li><li id="60a8" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">当前吸烟者</strong>:患者是否是当前吸烟者(名义上)</li><li id="1f6c" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">烟/日</strong>:一个人平均一天吸的烟的数量(连续)</li><li id="d47b" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj"> BPMeds </strong>:患者是否在服用降压药(标称)</li><li id="5ce0" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">中风</strong>:患者之前是否有中风史(名义上)</li><li id="9301" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">患病率类型</strong>:患者是否为高血压(正常)</li><li id="23ab" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">糖尿病</strong>:患者是否患有糖尿病(标称)</li><li id="4c2d" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">总胆固醇</strong>:总胆固醇水平(连续)</li><li id="b69c" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj"> sysBP </strong>:收缩压(持续)</li><li id="3f24" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj"> diaBP </strong>:舒张压(持续)</li><li id="91ce" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">身体质量指数</strong>:体重指数(连续)</li><li id="5ae8" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">心率</strong>:心率(连续)</li><li id="9180" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">葡萄糖</strong>:葡萄糖水平(持续)</li><li id="70ec" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">十年冠心病</strong> : 10年冠心病风险(二进制:1(是)，0(否))</li></ol><h1 id="758e" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">第三步。处理缺失值</strong></h1><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="1cb2" class="kt jr hi kp b fi ku kv l kw kx">df.isnull().sum()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/8aba2819de500314d97f2f9bc5224b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*rlmHlo9msFDSAFXlguhp-A.png"/></div></figure><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="2dac" class="kt jr hi kp b fi ku kv l kw kx">count = 0<br/>for i in df.isnull().sum(axis=1):<br/>    if i &gt; 0:<br/>        count = count+1<br/>print('Total number of rows with missing values is', count)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/ed8d5f2d37dfb8a7485d56b9d1dabdbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*Za_WrssBNZsYi7zj6kxnEg.png"/></div></figure><p id="16dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的输出中，我们得到缺少值的行的总数是<strong class="ih hj"> 489。</strong>在这种情况下，因为它只占整个数据集的<strong class="ih hj"> 12 % </strong>，所以具有缺失值的行被排除。</p><p id="da6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们从排除过程中获得数据，如下所示:</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="b478" class="kt jr hi kp b fi ku kv l kw kx">df.dropna(axis=0, inplace=True)<br/>df.info()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/ea50947ff2b30b3aa90338f7ce1ee214.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*JUaTa1CFicytyHpWCsPmMw.png"/></div></figure><h1 id="b4fe" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">步骤四。探索性视觉分析</strong></h1><p id="f81b" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">这一步的目的是查看每个数据的分布。</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="ca66" class="kt jr hi kp b fi ku kv l kw kx">def draw_histograms(dataframe, features, rows, cols):<br/>    fig=plt.figure(figsize=(20,20))<br/>    for i, feature in enumerate(features):<br/>        ax=fig.add_subplot(rows,cols,i+1)<br/>        dataframe[feature].hist(bins=20,ax=ax,facecolor='red')<br/>        ax.set_title(feature+"Distribution", color='blue')<br/>    fig.tight_layout()<br/>    plt.show()<br/>draw_histograms(df, df.columns, 6, 3)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/6429f8b28c12482bc0509e6c01a97f90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wBArngdvOctRj1Ldm4HZag.png"/></div></div></figure><p id="d713" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的直方图中，我们可以更好地理解数据的分布。</p><p id="95aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步是看看有多少病人有患冠心病的风险。</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="261b" class="kt jr hi kp b fi ku kv l kw kx">sn.countplot(x='TenYearCHD', data=df)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/c5d343f8fc31cbdbd0e484761d08e537.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*X1-yBpJVbiR5xA_7F2mpKw.png"/></div></figure><p id="21d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上图中，我们可以得出结论，是否有<strong class="ih hj"> 3179 </strong>患者没有<strong class="ih hj">冠心病</strong>和<strong class="ih hj"> 572 </strong>患者有<strong class="ih hj">冠心病风险。</strong></p><h1 id="d7ac" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">第五步。咱们造型吧！</h1><blockquote class="ly lz ma"><p id="960d" class="if ig jp ih b ii ij ik il im in io ip mb ir is it mc iv iw ix md iz ja jb jc hb bi translated"><strong class="ih hj">逻辑回归</strong> n是一种机器学习<strong class="ih hj">分类算法</strong>，用于预测<strong class="ih hj">分类因变量</strong>的概率。这是分类问题的线性回归模型的扩展。与输出连续数值的线性回归不同，逻辑回归使用<strong class="ih hj">逻辑sigmoid </strong>函数转换其输出，以返回<strong class="ih hj">一个概率值</strong>，该概率值可以映射到两个或更多个<strong class="ih hj">离散类。</strong></p></blockquote><p id="29be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就建模过程而言，我们要做的第一步是向清理后的数据集添加一个常数值。</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="52e5" class="kt jr hi kp b fi ku kv l kw kx">from statsmodels.tools import add_constant as add_constant<br/>df_constant = add_constant(df)<br/>df_constant.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es me"><img src="../Images/0a369f4bfd8999a68422b66b2321f622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3pIzxDQhXMaRZrt2VpNxRQ.png"/></div></div></figure><p id="c774" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步，我们将尝试查看逻辑回归模型的参数和结果:</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="82db" class="kt jr hi kp b fi ku kv l kw kx">st.chisqprob = lambda chisq, df: st.chi2.sf(chisq, df)<br/>cols = df_constant.columns[:-1]<br/>model = sm.Logit(df.TenYearCHD, df_constant[cols])<br/>result = model.fit()<br/>result.summary()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mf"><img src="../Images/47ff521b0c1c7912b26452ab6c8f7166.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*gXOVnudoR732hLK4YSsN4A.png"/></div></figure><p id="3ccb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的结果显示，一些属性的P值高于首选的α值(5%)，这表明这些属性与冠心病(CHD)的概率有<strong class="ih hj">低的统计显著性关系。</strong></p><p id="18db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解决这个问题，<strong class="ih hj">我们使用了一种<em class="jp">反向剔除</em> </strong>方法，一次移除一个具有最高P值的属性，然后重复运行回归，直到所有属性的P值都小于0.05。</p><p id="fd5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">特征选择:反向消除(<em class="jp"> P值</em>逼近)</strong></p><p id="3e3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因变量和列名列表，重复运行回归<strong class="ih hj">删除P值高于alpha (5%)的要素</strong> <strong class="ih hj">一次一个</strong>并返回所有P值低于alpha (5%)的回归汇总。</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="482c" class="kt jr hi kp b fi ku kv l kw kx">def back_feature_elem (data_frame, dep_var, col_list):<br/>    while len(col_list)&gt;0 :<br/>        model = sm.Logit(dep_var,data_frame[col_list])<br/>        result = model.fit(disp=0)<br/>        largest_pvalue = round(result.pvalues,3).nlargest(1)<br/>        if largest_pvalue[0]&lt;(0.05):<br/>            return result<br/>            break<br/>        else:<br/>            col_list = col_list.drop(largest_pvalue.index)<br/>result = back_feature_elem(df_constant, df.TenYearCHD, cols)<br/>result.summary()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mg"><img src="../Images/449cd7ae18d2ef10001c6f72a0e093c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*AZ0rh7ukeCDc9ebugZrEBg.png"/></div></figure><p id="4b58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们得到了p值小于α(5%)的属性:</p><ul class=""><li id="24b0" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc mh ll lm ln bi translated"><strong class="ih hj">性</strong></li><li id="f9c7" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj">年龄</strong></li><li id="b082" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj">香烟日</strong></li><li id="f1f9" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated">托彻尔</li><li id="593c" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj"> sysBP </strong></li><li id="a8e1" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj">葡萄糖</strong></li></ul><p id="fd43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到这个逻辑回归方程:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mi"><img src="../Images/5ae4945b0baa2058ce45f35cd1620423.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*zroCfIXdGCBmpejW_GODSA.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mj"><img src="../Images/81852e2eeeaaf7b0a65d41dda3101767.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ybJ3vdKyHHztcxESPtZmdw.png"/></div></div></figure><h1 id="221d" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">第六步。解释:比值比、置信区间和P值</strong></h1><p id="b51b" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">下一步，我们将使用以下代码解释比值比、置信区间和P值:</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="f63e" class="kt jr hi kp b fi ku kv l kw kx">params = np.exp(result.params)<br/>conf = np.exp(result.conf_int())<br/>conf['OR'] = params<br/>pvalue = round(result.pvalues,3)<br/>conf['pvalue'] = pvalue<br/>conf.columns = ['CI 95%(2.5%)','CI 95%(97.5%)', 'Odds Ratio', 'pvalue']<br/>print((conf))</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mk"><img src="../Images/3327ac458a5420d6fd5d65efc3a6ed65.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*NotXdVYYRNvSiArWiUeCoA.png"/></div></figure><p id="ce3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">对上面输出的解释是:</strong></p><ul class=""><li id="f6a1" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc mh ll lm ln bi translated"><strong class="ih hj">男性<strong class="ih hj">【性别=1】比女性<strong class="ih hj">【性别=0】被诊断为</strong>冠心病<strong class="ih hj"/>的几率为exp(0.5815) = <strong class="ih hj"> 1.788687 </strong>。就百分比变化而言，我们可以说男性被诊断的几率比女性高<strong class="ih hj"> 78.8%。</strong></strong></strong></li><li id="2240" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj">年龄</strong>系数说，如果我们要看被诊断为冠心病的可能性，通过看年龄增加一岁的参数，大约是exp(0.0655) = <strong class="ih hj"> 1.067644 </strong>或者它在<strong class="ih hj"> 7%左右。</strong></li><li id="e788" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated">一个人每多吸一支烟，被诊断为CDH病的几率就增加<strong class="ih hj">2%</strong><strong class="ih hj"/>(exp(0.0197)=<strong class="ih hj">1.019895</strong>)。</li><li id="0583" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj">收缩压</strong>每增加一个单位，优势就增加<strong class="ih hj">1.7%</strong>(exp(0.0174)=<strong class="ih hj">1.017552</strong>)。</li><li id="7407" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj">葡萄糖水平每增加一个单位，患CDH病的几率增加<strong class="ih hj">0.7%</strong>(exp(0.0076)=<strong class="ih hj">1.007628)</strong>。</strong></li></ul><h1 id="04f2" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">第七步。拆分数据:训练和测试</h1><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="4a03" class="kt jr hi kp b fi ku kv l kw kx">import sklearn<br/>new_features =  df[['age','sex','cigsPerDay','totChol','sysBP','glucose','TenYearCHD']]<br/>x = new_features.iloc[:,:-1]<br/>y = new_features.iloc[:,-1]<br/>from sklearn.model_selection import train_test_split<br/>x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.20, random_state=5)</span><span id="5b49" class="kt jr hi kp b fi ml kv l kw kx">from sklearn.linear_model import LogisticRegression<br/>logreg = LogisticRegression()<br/>logreg.fit(x_train, y_train)<br/>y_pred = logreg.predict(x_test)</span></pre><h1 id="84ef" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">第八步。模型评估</h1><p id="b574" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">模型评估旨在评估模型对未来(未知/样本外)数据的泛化精度。</p><h2 id="e9cf" class="kt jr hi bd js mm mn mo jw mp mq mr ka iq ms mt ke iu mu mv ki iy mw mx km my bi translated">1.准确(性)</h2><p id="2ebb" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">精确度描述了一个模型正确分类的精确度。<strong class="ih hj">正确预测的数量占所有预测的比例</strong>。我们使用sklearn模块来计算分类任务的准确度。</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="5f1f" class="kt jr hi kp b fi ku kv l kw kx">sklearn.metrics.accuracy_score(y_test,y_pred)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mz"><img src="../Images/500eeeb8edc5d498f127542ce42b9203.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*Z9qP8IIwdlNvgAiwG5vMxQ.png"/></div></figure><p id="85ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从该输出，我们可以解释我们得到的模型的准确度在验证集上是否大约为<strong class="ih hj"> 87.5% </strong>。</p><h2 id="917f" class="kt jr hi bd js mm mn mo jw mp mq mr ka iq ms mt ke iu mu mv ki iy mw mx km my bi translated"><strong class="ak"> 2。混乱矩阵</strong></h2><p id="e372" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">混淆矩阵检测分类器预测中的<strong class="ih hj"> TP </strong>(真阳性)、<strong class="ih hj"> TN </strong>(真阴性)、<strong class="ih hj"> FP ( </strong>假阳性)、<strong class="ih hj"> FN </strong>(假阴性)的计数。</p><p id="89ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从混淆矩阵中，我们可以导出<strong class="ih hj">准确度</strong>，其由<strong class="ih hj">校正预测的总和</strong>除以<strong class="ih hj">预测总数</strong>给出:</p><ul class=""><li id="de0b" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc mh ll lm ln bi translated"><strong class="ih hj">准确度= TP+TN/TP+FP+FN+TN </strong></li></ul><p id="a092" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个标准的绩效衡量标准是:</p><ul class=""><li id="88c1" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc mh ll lm ln bi translated"><strong class="ih hj">灵敏度</strong>或<strong class="ih hj">召回= TP / TP + FN </strong>或<strong class="ih hj">真阳性率(TPR) </strong></li><li id="6f0c" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj">特异性= TN / TN + FP </strong>或<strong class="ih hj">真阴性率(TNR) </strong></li><li id="779b" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj">精度= TP / TP + FP </strong>或<strong class="ih hj">正向预测值</strong></li><li id="4198" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj">假阳性率(FPR) = FP / FP + TN </strong></li><li id="5784" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj">假阴性率(FNR) = FN / FN + TP </strong></li><li id="100f" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj"> F1得分= 2*(召回率*精确度)/(召回率+精确度)</strong></li></ul><p id="f214" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于好的分类器，<strong class="ih hj"> TPR和TNR </strong>都应该更接近<strong class="ih hj"> 100% </strong>。类似的情况还有<strong class="ih hj">精度</strong>和<strong class="ih hj">精度</strong>参数。相反，<strong class="ih hj"> FPR和FNR </strong>都应尽可能接近<strong class="ih hj"> 0% </strong>。</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="5153" class="kt jr hi kp b fi ku kv l kw kx">from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred)<br/>conf_matrix = pd.DataFrame(data=cm, columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])<br/>plt.figure(figsize = (8,5))<br/>sn.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlGnBu')</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es na"><img src="../Images/ef2091c22a915500825ae1c4386cb7d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*IE9prC1efIPdAPEXS747Jw.png"/></div></figure><p id="de11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的混淆矩阵显示了652+5 = 657个正确的预测和87+5= 92个错误的预测。</p><ul class=""><li id="d9a6" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc mh ll lm ln bi translated"><strong class="ih hj">真阳性(TP) : </strong> 5</li><li id="3434" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj">真底片(TN) : </strong> 652</li><li id="3a72" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj">误报</strong><strong class="ih hj">(FP):</strong>7(I型错误)</li><li id="b925" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated"><strong class="ih hj">假阴性</strong> <strong class="ih hj"> (FN) : </strong> 87(第二类错误)</li></ul><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="ee4f" class="kt jr hi kp b fi ku kv l kw kx">TN = cm[0,0]<br/>TP = cm[1,1]<br/>FN = cm[1,0]<br/>FP = cm[0,1]</span><span id="46fa" class="kt jr hi kp b fi ml kv l kw kx">sensitivity = TP/float(TP+FN)<br/>specificity = TN/float(TN+FP)</span><span id="59f3" class="kt jr hi kp b fi ml kv l kw kx">print('The acuuracy of the model = TP+TN/(TP+TN+FP+FN) = ',(TP+TN)/float(TP+TN+FP+FN),'\n',</span><span id="84f2" class="kt jr hi kp b fi ml kv l kw kx">'The Missclassification = 1-Accuracy = ',1-((TP+TN)/float(TP+TN+FP+FN)),'\n',</span><span id="7036" class="kt jr hi kp b fi ml kv l kw kx">'Sensitivity or True Positive Rate = TP/(TP+FN) = ',TP/float(TP+FN),'\n',</span><span id="2831" class="kt jr hi kp b fi ml kv l kw kx">'Specificity or True Negative Rate = TN/(TN+FP) = ',TN/float(TN+FP),'\n',</span><span id="dadb" class="kt jr hi kp b fi ml kv l kw kx">'Positive Predictive value = TP/(TP+FP) = ',TP/float(TP+FP),'\n',</span><span id="591d" class="kt jr hi kp b fi ml kv l kw kx">'Negative Predictive Value = TN/(TN+FN) = ',TN/float(TN+FN),'\n',</span><span id="99be" class="kt jr hi kp b fi ml kv l kw kx">'Positive Likelihood Ratio = Sensitivity/(1-Specificity) = ',sensitivity/(1-specificity),'\n',</span><span id="7cf1" class="kt jr hi kp b fi ml kv l kw kx">'Negative Likelihood Ratio = (1-Sensitivity)/Specificity = ',(1-sensitivity)/specificity)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nb"><img src="../Images/2adf28aa7eba2d3ea057cb917fb06e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*Ox-s3iYWdOx8XDiInJcSBg.png"/></div></figure><ol class=""><li id="0c47" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc lk ll lm ln bi translated">模型的准确率为87.48 %</li><li id="9f3e" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">误分类率 = 12.52 %</li><li id="a6a2" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">敏感度或真阳性率(TPR) = </strong> 54.35 %</li><li id="8018" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">特异性或真阴性率(TNR) = </strong> 98.94 %</li><li id="caa8" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">精度或阳性预测值= </strong> 41.67 %</li><li id="82b9" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">阴性预测值= </strong> 88.23 %</li><li id="645a" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">阳性似然比= 5.12 </strong></li><li id="d67f" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">负似然比= 0.96 </strong></li></ol><p id="848f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的统计数据可以清楚地看出，该模型是<strong class="ih hj">高度特异性的</strong>而非<strong class="ih hj">敏感性的。</strong> <strong class="ih hj">预测负值</strong><strong class="ih hj">比预测正值<strong class="ih hj">更准确</strong>。</strong></p><h1 id="1723" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">第九步。预测概率</h1><p id="b1a9" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">0(冠心病:否)和1(冠心病:是)用于测试数据，默认分类阈值为0.5</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="a9fe" class="kt jr hi kp b fi ku kv l kw kx">y_pred_prob = logreg.predict_proba(x_test)[:,:]<br/>y_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['Prob of no hearts disease(0)','Prob of Heart Disease (1)'])<br/>y_pred_prob_df.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nc"><img src="../Images/29807b6152db56acaaed201f46dbd80b.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*eMUVJj0KGnRMXF_R05VnTA.png"/></div></figure><h2 id="e333" class="kt jr hi bd js mm mn mo jw mp mq mr ka iq ms mt ke iu mu mv ki iy mw mx km my bi translated">下限阈值</h2><p id="4ebc" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">从混淆矩阵中，我们可以得出结论，如果<strong class="ih hj">假阴性(FN)</strong>(II型错误)的数量非常大，则可以将其归类为<strong class="ih hj">中度危险</strong>，因为这意味着在实际存在一个或<strong class="ih hj">真</strong>的情况下忽略疾病的概率。</p><p id="4bba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此为了<strong class="ih hj">增加灵敏度</strong>，可以降低<strong class="ih hj">阈值</strong>。</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="4c01" class="kt jr hi kp b fi ku kv l kw kx">from sklearn.preprocessing import binarize<br/>for i in range(1,5):<br/>    cm2=0<br/>    y_pred_prob_yes=logreg.predict_proba(x_test)<br/>    y_pred2=binarize(y_pred_prob_yes,i/10)[:,1]<br/>    cm2=confusion_matrix(y_test,y_pred2)<br/>    print ('With',i/10,'threshold the Confusion Matrix is ','\n',cm2,'\n',<br/>            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\n\n',<br/>          'Sensitivity: ',cm2[1,1]/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]/(float(cm2[0,0]+cm2[0,1])),'\n\n\n')</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nd"><img src="../Images/01465a7a8011fdc8cd344900f109bf5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*vLzKfsB4MDMLa8bafEfGLA.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nd"><img src="../Images/52e08f11af5198cbe7c81afc3659d3dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*pGMId1vDp_Uql7GHTOPIuw.png"/></div></figure><h2 id="ccc7" class="kt jr hi bd js mm mn mo jw mp mq mr ka iq ms mt ke iu mu mv ki iy mw mx km my bi translated"><strong class="ak"> 3。ROC曲线</strong></h2><p id="d8bf" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">ROC曲线下的面积是用于测量<em class="jp">二元分类器</em>区分正类和负类的能力的性能度量。<strong class="ih hj"> ROC </strong>是概率曲线，<strong class="ih hj"> AUC </strong>代表可分性的度量。它告诉我们模型在多大程度上能够区分不同的类。</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="f563" class="kt jr hi kp b fi ku kv l kw kx">from sklearn.metrics import roc_curve<br/>fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_yes[:,1])<br/>plt.plot(fpr,tpr)<br/>plt.xlim([0.0, 1.0])<br/>plt.ylim([0.0, 1.0])<br/>plt.title('ROC curve for Heart disease classifier')<br/>plt.xlabel('False positive rate (1-Specificity)')<br/>plt.ylabel('True positive rate (Sensitivity)')<br/>plt.grid(True)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ne"><img src="../Images/7f67066127843c88c7a0902c04677c3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*XMSR3ngdSYpycJqXsuhp9Q.png"/></div></figure><h2 id="d976" class="kt jr hi bd js mm mn mo jw mp mq mr ka iq ms mt ke iu mu mv ki iy mw mx km my bi translated"><strong class="ak">曲线下面积(AUC) </strong></h2><p id="38c2" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">ROC曲线下的面积量化了模型分类的准确性；<strong class="ih hj">面积越大，真阳性和假阳性之间的差异越大，</strong>并且模型在对训练数据集的成员进行分类时越强。0.5的面积对应于执行不比随机分类更好的模型，并且好的分类器尽可能远离随机分类。面积为1是理想的。AUC越接近1越好。</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="eb80" class="kt jr hi kp b fi ku kv l kw kx">sklearn.metrics.roc_auc_score(y_test,y_pred_prob_yes[:,1])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nf"><img src="../Images/2f5bf0832ce897e42af790f72f7a7ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*GkF_JQ5XD-DoVkOha_L2AQ.png"/></div></div></figure><h1 id="182e" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">结论:</h1><ul class=""><li id="c731" class="lf lg hi ih b ii ky im kz iq ng iu nh iy ni jc mh ll lm ln bi translated">在排除过程之后选择的所有属性显示出低于5%的P值，从而表明已经选择的属性在冠心病(CHD)预测中具有重要作用。</li><li id="c0ae" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated">男性似乎比女性更容易患心脏病。年龄、每天吸烟量和收缩压的增加也表明患心脏病的几率增加。</li><li id="4815" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated">模型预测的准确率为87.5%。该模型更具体而非敏感。</li><li id="060c" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated">ROC曲线下的面积是73.5，这多少令人满意。</li><li id="6a05" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc mh ll lm ln bi translated">总体模型可以用更多的数据来改进。</li></ul><h1 id="fd2f" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">参考文献:</strong></h1><div class="nj nk ez fb nl nm"><a href="https://www.tandfonline.com/doi/full/10.1080/23311916.2020.1723198" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab dw"><div class="no ab np cl cj nq"><h2 class="bd hj fi z dy nr ea eb ns ed ef hh bi translated">冠心病预测中的机器学习:结构方程建模方法</h2><div class="nt l"><h3 class="bd b fi z dy nr ea eb ns ed ef dx translated">摘要本研究是机器学习在医学领域的应用。这项研究的目的是…</h3></div><div class="nu l"><p class="bd b fp z dy nr ea eb ns ed ef dx translated">www.tandfonline.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa jn nm"/></div></div></a></div><div class="nj nk ez fb nl nm"><a href="https://dl.acm.org/doi/10.1145/3342999.3343015" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab dw"><div class="no ab np cl cj nq"><h2 class="bd hj fi z dy nr ea eb ns ed ef hh bi translated">利用机器学习预测冠心病</h2><div class="nt l"><h3 class="bd b fi z dy nr ea eb ns ed ef dx translated">医学分析领域通常被认为是丰富信息的宝贵来源。冠心病…</h3></div><div class="nu l"><p class="bd b fp z dy nr ea eb ns ed ef dx translated">dl.acm.org</p></div></div><div class="nv l"><div class="ob l nx ny nz nv oa jn nm"/></div></div></a></div><div class="nj nk ez fb nl nm"><a href="https://www.kaggle.com/neisha/heart-disease-prediction-using-logistic-regression" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab dw"><div class="no ab np cl cj nq"><h2 class="bd hj fi z dy nr ea eb ns ed ef hh bi translated">使用逻辑回归预测心脏病</h2><div class="nt l"><h3 class="bd b fi z dy nr ea eb ns ed ef dx translated">使用Kaggle笔记本探索和运行机器学习代码|使用来自弗雷明汉心脏研究数据集的数据</h3></div><div class="nu l"><p class="bd b fp z dy nr ea eb ns ed ef dx translated">www.kaggle.com</p></div></div><div class="nv l"><div class="oc l nx ny nz nv oa jn nm"/></div></div></a></div><div class="nj nk ez fb nl nm"><a href="https://heartbeat.fritz.ai/introduction-to-machine-learning-model-evaluation-fa859e1b2d7f" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab dw"><div class="no ab np cl cj nq"><h2 class="bd hj fi z dy nr ea eb ns ed ef hh bi translated">机器学习模型评估简介</h2><div class="nt l"><h3 class="bd b fi z dy nr ea eb ns ed ef dx translated">机器学习继续成为我们生活中不可或缺的一部分，无论我们是将这些技术应用于…</h3></div><div class="nu l"><p class="bd b fp z dy nr ea eb ns ed ef dx translated">heartbeat.fritz.ai</p></div></div><div class="nv l"><div class="od l nx ny nz nv oa jn nm"/></div></div></a></div><div class="nj nk ez fb nl nm"><a rel="noopener follow" target="_blank" href="/swlh/confusion-matrix-in-machine-learning-920eda3d2cb6"><div class="nn ab dw"><div class="no ab np cl cj nq"><h2 class="bd hj fi z dy nr ea eb ns ed ef hh bi translated">机器学习中的混淆矩阵</h2><div class="nt l"><h3 class="bd b fi z dy nr ea eb ns ed ef dx translated">在这篇博客中，我将试着解释混淆矩阵的所有基本术语以及如何计算它们，用…</h3></div><div class="nu l"><p class="bd b fp z dy nr ea eb ns ed ef dx translated">medium.com</p></div></div><div class="nv l"><div class="oe l nx ny nz nv oa jn nm"/></div></div></a></div><div class="nj nk ez fb nl nm"><a href="https://antoniocastiglione-9550.medium.com/machine-learning-algorithms-for-coronary-heart-disease-prediction-ec25f4d7ee42" rel="noopener follow" target="_blank"><div class="nn ab dw"><div class="no ab np cl cj nq"><h2 class="bd hj fi z dy nr ea eb ns ed ef hh bi translated">用于冠心病预测的机器学习算法</h2><div class="nt l"><h3 class="bd b fi z dy nr ea eb ns ed ef dx translated">简介:</h3></div><div class="nu l"><p class="bd b fp z dy nr ea eb ns ed ef dx translated">antoniocastiglione-9550.medium.com</p></div></div><div class="nv l"><div class="of l nx ny nz nv oa jn nm"/></div></div></a></div></div></div>    
</body>
</html>