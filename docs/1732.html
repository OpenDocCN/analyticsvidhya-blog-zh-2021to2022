<html>
<head>
<title>Super Resolution with SRResnet, SRGAN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用SRResnet、SRGAN实现超高分辨率</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/super-resolution-with-srresnet-srgan-2859b87c9c7f?source=collection_archive---------7-----------------------#2021-03-15">https://medium.com/analytics-vidhya/super-resolution-with-srresnet-srgan-2859b87c9c7f?source=collection_archive---------7-----------------------#2021-03-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="b6ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原文:<a class="ae jd" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank">使用生成式对抗网络的照片级单幅图像超分辨率</a></p><h1 id="8054" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">论文摘要</h1><ul class=""><li id="343a" class="kc kd hi ih b ii ke im kf iq kg iu kh iy ki jc kj kk kl km bi translated">本文提出了一个名为SRResNet的网络架构，它在PSNR基准测试中表现出优于其他先前方法的性能[2]。</li><li id="8b17" class="kc kd hi ih b ii kn im ko iq kp iu kq iy kr jc kj kk kl km bi translated">建议使用感知VGG损失函数来恢复精细纹理细节，而不是之前用于寻找平均纹理的MSE损失。</li><li id="100f" class="kc kd hi ih b ii kn im ko iq kp iu kq iy kr jc kj kk kl km bi translated">集成GAN(生成对抗网络)对抗损失，以生成更精细的纹理细节和更好的感知质量，通过平均意见得分进行评估。</li></ul><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ks"><img src="../Images/97bf326e6c2fd10553b61c32e6d7d2a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-IzCaZ61GFyDzRizIoeJWQ.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">利用本文提出的方法获得超分辨率图像。</figcaption></figure><h1 id="569a" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">SR的感知损失</h1><p id="74f9" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">虽然使用像素方式的MSE误差作为度量来测量模型的性能并因此导致最大化PSNR分数可能是令人信服的，但是这种损失定义对于生成感知上高质量的图像具有一些明显的缺陷。这是因为基于MSE的解决方案在输出所有可能解决方案的平均值时被优化，这可能不在HR图像流形上，并且有时可能模糊和不真实。下图显示了这种现象，蓝色块是基于MSE的最优解。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es ll"><img src="../Images/83b31547ae3ab0860513ca1b30f67679.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*Ppky63hcFL7z3S3LRWZNLQ.png"/></div></figure><p id="a186" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解决这个问题，作者首先提出了一种基于GAN的方法来捕获自然图像流形，并提出了一种综合上下文损失和敌对损失的混合损失。为了进一步提高性能，作者还提出了一种改进的上下文损失，通过查看预训练的VGG-19网络的中间激活来比较图像的更多高级特征。这个损失描述如下，φi，j表示VGG19网络内第I个maxpooling层之前第j个卷积之后(激活之后)的特征映射。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es lm"><img src="../Images/4a310b050805a7232572a45b2dd64ae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*6_ye0DlaCDK-vaD6De24ZQ.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">VGG损失</figcaption></figure><p id="6aec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如下所示，SRResnet恢复图像的非常模糊的补丁，而包含敌对损失和VGG上下文损失的损失实现了照片级的图像超分辨率。VGG22和VGG54的区别在于用于计算损耗的VGG网络的层数(22-&gt;4层，54-&gt;19层)。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ln"><img src="../Images/2fa9e80dc102ac5c1d66dfdc03ee45f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QnAmisSPqLf_jPULyXkOKA.png"/></div></div></figure><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es lo"><img src="../Images/df3b85a62582a0ce2792aa4c4fdb880f.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*ZGdB7nOsjElHv4GRrerTeg.png"/></div></figure><p id="9048" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实验中评估每种损失的PSNR、SSIM、MOS评分。PSNR和SSIM评分对于SRResNet-MSE模型来说是最好的，而使用更多的感知损失函数导致显著更高的MOS评分。尽管如此，常规SRResNet-MSE模型在PSNR/SSIM和MOS评分上也优于所有先前的方法，显示了所提出的模型架构的有效性。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es lp"><img src="../Images/5bd20751dc0473600cad50353033cbc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*wvNUC3CWRTfjjZL4ovi-9w.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">与以前方法的比较</figcaption></figure><h1 id="9dec" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">模型架构</h1><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es lq"><img src="../Images/6f111a219e963058cedeb8de33987095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JudvsYtBuEyyStlBVUqXeg.png"/></div></div></figure><p id="c4b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文提出了一个生成器网络和一个鉴别器网络，分别用于超分辨率图像，以及从高分辨率地面实况图像中鉴别超分辨率图像。生成器网络由5个在较低尺度上操纵图像的残差块组成，以及ESPCN[3]提出的一种无需手动填充中间像素值即可重建超分辨率图像的方法。下面的图片和链接详细介绍了这种方法。生成器的每个残差块具有两组常规(Conv-BN-ReLU)块，对于所有块，其具有恒定的通道数64和核大小3，以及之后的跳过连接。</p><p id="d552" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">鉴别器是输入图像的常规CNN，并且旨在分类它是真实图像还是虚假图像。想了解更多关于GANs的信息，请尝试Tensorflow DCGAN教程。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es lr"><img src="../Images/af48d14e21e722de4da2978ab4ef131f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ksiKvBXI9PtwiSZe.jpg"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated"><a class="ae jd" href="https://torch.vision/2020/01/14/Efficient_Sub_Pixel_Convolutional_Neural_Network.html" rel="noopener ugc nofollow" target="_blank">https://torch . vision/2020/01/14/Efficient _ Sub _ Pixel _ convolatile _ Neural _ network . html</a></figcaption></figure><p id="d9a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文的实现将很快发布。</p><h1 id="c961" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">参考</h1><p id="3885" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">[1] Ledig，Christian等，“使用生成式对抗网络的照片级单幅图像超分辨率”<em class="ls">IEEE计算机视觉和模式识别会议论文集</em>。2017.</p><p id="94bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]董，晁，等.利用深度卷积网络实现图像超分辨率."IEEE模式分析与机器智能汇刊38.2(2015):295–307。</p><p id="9eba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]石，，等.“利用高效亚像素卷积神经网络实现单幅图像和视频的实时超分辨率”<em class="ls">IEEE计算机视觉和模式识别会议论文集</em>。2016.</p></div></div>    
</body>
</html>