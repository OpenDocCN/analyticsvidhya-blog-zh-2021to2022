<html>
<head>
<title>Text Classification — From Bag-of-Words to BERT — Part 3(fastText)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本分类—从词袋到BERT —第3部分(快速文本)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-3-fasttext-8313e7a14fce?source=collection_archive---------3-----------------------#2021-01-02">https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-3-fasttext-8313e7a14fce?source=collection_archive---------3-----------------------#2021-01-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es im"><img src="../Images/78e35b6cfda2995bfc40717357daf631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mPqqyQNx4SI7J9dD"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">格伦·惠勒在<a class="ae jc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="2d78" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这个故事是一系列文本分类的一部分——从词袋到BERT在Kaggle竞赛上实施多种方法，由Jigsaw(Alphabet的子公司)<strong class="jf hj"> <em class="kb">命名为“<a class="ae jc" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge" rel="noopener ugc nofollow" target="_blank"> <em class="kb">【有毒评论分类挑战】</em> </a> <em class="kb"> </em>”。</em> </strong>在这场比赛中，我们面临的挑战是建立一个多头模型，能够检测不同类型的毒性，如<em class="kb">威胁、淫秽、侮辱和基于身份的仇恨。如果你还没有查看之前的故事，一定要查看一下，因为这将有助于理解未来的事情。</em></p><p id="3286" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae jc" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9" rel="noopener">第一部分(BagOfWords) </a></p><p id="6dda" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae jc" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-2-word2vec-35c8c3b34ee3" rel="noopener">第二部分(Word2Vec) </a></p><p id="4869" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在早期的故事中(<a class="ae jc" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-2-word2vec-35c8c3b34ee3" rel="noopener">第2部分(Word2Vec) </a>)，我们使用Gensim为句子中使用的单词获得预训练的Word2Vec模型/嵌入向量，将它们映射到输出变量toxic、severe_toxic、淫秽、威胁、侮辱、identity_hate，并使用sklearn的多输出逻辑回归分类器包装器为所有6个输出变量创建逻辑回归模型。</p><p id="2407" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这个例子中，我们将使用fastText库来生成句子的嵌入和文本分类。事实上，这给了我们一个一气呵成的选择。直觉部分将比其他方法简短，因为快速文本的文档/阅读材料有限。</p><p id="b588" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj"> <em class="kb">什么是fastText？</em>T25】</strong></p><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es kc"><img src="../Images/d599cfe55c78919d02e940a4886ae32e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRvz53WKr4koIKgmJZoRFQ.png"/></div></div></figure><p id="5694" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">2016年，脸书人工智能研究院(FAIR)开源了fastText，这是一个旨在帮助构建文本表示和分类的可扩展解决方案的库。fastText将Word2Vec中单词嵌入的思想向前推进了一步，学习了字符n元语法的表示，并将单词表示为n元语法向量的总和。以单词“where”和n = 3为例，用字符n-grams表示:<wh whe="" her="" ere="" re="">。除了子词嵌入形式的文本表示之外，它还提供了现成的分类模型，该模型被优化以与这些嵌入一起工作并给出快速结果。</wh></p><p id="52a6" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj"> <em class="kb">为什么需要fastText？</em> </strong></p><p id="dae5" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">与常规的word2vec嵌入相比，fastText有两个主要优点:</p><ol class=""><li id="3575" class="kd ke hi jf b jg jh jk jl jo kf js kg jw kh ka ki kj kk kl bi translated"><strong class="jf hj"> <em class="kb"> Word2Vec面临词汇量不足的问题(OOV) </em> </strong>假设我们从零开始训练一个Word2Vec模型，我们建立一个包含训练数据中所有单词的词汇表。现在，如果我们在测试数据中有一个可能需要嵌入的新单词，新的缺失单词将是OOV。在word2vec中我们完全忽略了这样的词。通过在fastText中使用子单词嵌入，我们试图为一个也是OOV的单词获得嵌入</li><li id="f696" class="kd ke hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><strong class="jf hj"> <em class="kb">通过对每个单词使用不同的向量表示，Word2Vec模型忽略了单词的内部结构:</em> </strong>在word2vec中，每个单词都是基于它出现的上下文唯一学习的。例如，boxer和boxing在不同的上下文中使用，我们无法捕捉潜在的相似性。将其分解为字符n-gram会有所帮助</li></ol><p id="fbca" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj"><em class="kb">fast text有什么额外的好处？</em> </strong></p><p id="f253" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">虽然fastText超越了单词级别，达到了字符-n-gram级别，但是它非常快(因此得名)。实验表明，fastText在准确性方面通常与深度学习分类器不相上下，并且在训练和评估方面快了许多数量级。我们可以使用标准的多核CPU在不到十分钟的时间内对超过10亿个单词进行fastText训练，并在不到一分钟的时间内对312K个类别中的50万个句子进行分类。</p><p id="f287" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">此外，fastText提供了在维基百科和Crawl上训练的157种语言的词向量(这太令人惊讶了)。</p><p id="7432" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj"><em class="kb">fast text是如何工作的？</em> </strong></p><ol class=""><li id="959e" class="kd ke hi jf b jg jh jk jl jo kf js kg jw kh ka ki kj kk kl bi translated"><strong class="jf hj"> <em class="kb">创建单词嵌入:</em> </strong>子单词模型基于来自Word2Vec的skip-gram模型，并且不使用单词的矢量表示，而是使用字符n元语法的矢量表示的平均值。其他一切都与跳格模型非常相似。</li><li id="5943" class="kd ke hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><strong class="jf hj"> <em class="kb">文本分类:</em> </strong>下图摘自实际论文《高效文本分类锦囊》，其中介绍了用于分类的fastText。fastText使用类似于Word2Vec网络的浅层神经网络。</li></ol><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es kr"><img src="../Images/1971e55e25940c171ebd0cfccfc79ce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*YsikSFz91DOfNUHWhS8afg.png"/></div></figure><p id="3862" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们使用softmax函数f来计算预定义类别的概率分布。事实上，它使用了一种基于霍夫曼编码树的称为分层softmax的东西(简而言之，最常见的单词/字母被赋予最小的代码)。因此，在层次softmax中，节点的概率总是低于其父节点的概率。当我们有大量的类时，以及在我们搜索最可能的类时，这都是有帮助的。</p><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es ks"><img src="../Images/9d6a135fbc9673bdaacd82c41274fd48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rib6YYOoTTkSh63IicdaPw.png"/></div></div></figure><p id="c470" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">例如，如果旅行、食物和印度美食是3个类别，如果旅行的概率更高，我们甚至不需要计算食物和印度美食的概率。这降低了复杂性。</p><p id="8031" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">此外，它还使用单词n元语法作为除嵌入之外的附加特征来捕获关于局部词序的一些部分信息，否则在正常的BagOfWords中捕获这些信息在计算上是非常昂贵的。</p><p id="c7e3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">对于对完整代码感兴趣的人，你可以在这里找到它<a class="ae jc" href="https://www.kaggle.com/anirbansen3027/jtcc-fasttext-supervised" rel="noopener ugc nofollow" target="_blank">。那么让我们深入研究代码👨‍💻</a></p><blockquote class="kt ku kv"><p id="161c" class="jd je kb jf b jg jh ji jj jk jl jm jn kw jp jq jr kx jt ju jv ky jx jy jz ka hb bi translated"><strong class="jf hj">实现</strong></p></blockquote><ol class=""><li id="cba6" class="kd ke hi jf b jg jh jk jl jo kf js kg jw kh ka ki kj kk kl bi translated"><strong class="jf hj"> <em class="kb">读取数据集</em> </strong></li></ol><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es kz"><img src="../Images/0543de08e345f6b77adbe68a2a0922ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1oEsT7kJsaTm2saQmCu36g.png"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">提醒一下，这是训练数据的样子</figcaption></figure><p id="342c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj"> <em class="kb"> 4。基本预处理</em> </strong></p><p id="a6fb" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">就预处理而言，我们正在执行以下步骤:</p><ol class=""><li id="5777" class="kd ke hi jf b jg jh jk jl jo kf js kg jw kh ka ki kj kk kl bi translated">我们为一些标点符号引入了空格，比如？, ., ), (, !并删除了一些只是为了使文字更清晰</li><li id="679e" class="kd ke hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">我们已经删除了“\n ”,因为我们已经可以在文本中看到很多</li><li id="96e5" class="kd ke hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">我们做了一些Unicode标准化来处理一些可能出现的Unicode问题</li><li id="6362" class="kd ke hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">这个是最重要的，将所有输出变量的二进制标签从0和1转换为_ _ class _ _ 0和__class__1，因为fastText分类器需要这样做。我们只需要对training_data这样做，因为分类器不会查看验证数据/测试数据的真实标签</li><li id="d8fa" class="kd ke hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">打乱数据集以引入一些随机性并消除有序性(如果存在的话)</li></ol><pre class="in io ip iq fd la lb lc ld aw le bi"><span id="4313" class="lf lg hi lb b fi lh li l lj lk"># Lets do some cleaning of this text<br/>def clean_it(text,normalize=True):<br/>    # Replacing possible issues with data. We can add or reduce the replacemtent in this chain<br/>    s = str(text).replace(',',' ').replace('"','').replace('\'',' \' ').replace('.',' . ').replace('(',' ( ').\<br/>            replace(')',' ) ').replace('!',' ! ').replace('?',' ? ').replace(':',' ').replace(';',' ').lower()<br/>    s = s.replace("\n"," ")<br/>    <br/>    # normalizing / encoding the text<br/>    if normalize:<br/>        s = s.normalize('NFKD').str.encode('ascii','ignore').str.decode('utf-8')<br/>    return s</span><span id="30ea" class="lf lg hi lb b fi ll li l lj lk"># Now lets define a small function where we can use above cleaning on datasets<br/>def clean_df(data, cleanit= False, shuffleit=False, encodeit=False, label_prefix='__class__'):<br/>    # Defining the new data<br/>    df = data[['comment_text']].copy(deep=True)<br/>    for col in y_cols:<br/>        df[col] = label_prefix + data[col].astype(str) + ' '<br/>    # cleaning it<br/>    if cleanit:<br/>        df['comment_text'] = df['comment_text'].apply(lambda x: clean_it(x,encodeit))<br/>    # shuffling it<br/>    if shuffleit:<br/>        df.sample(frac=1).reset_index(drop=True)<br/>    return df</span><span id="a863" class="lf lg hi lb b fi ll li l lj lk"># Transform the datasets using the above clean functions<br/>df_train_cleaned = clean_df(train, True, True)<br/>df_val_cleaned = clean_df(val, True, True, label_prefix='')</span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es lm"><img src="../Images/103f1a7393c85b6ef649845feae66628.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tFGLWxpYVh0Fec1-pOS3zw.png"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">因此，如前所述，我们已经更改了训练数据集的标签类，但没有更改验证集的标签类，因为fastText分类器不会查看它们</figcaption></figure><p id="cc47" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj"> <em class="kb"> 5。训练和验证快速文本分类器</em> </strong></p><ul class=""><li id="4d08" class="kd ke hi jf b jg jh jk jl jo kf js kg jw kh ka ln kj kk kl bi translated">由于fastText分类器输入一个带有文本数据和类标签的CSV文件，所以我们不能使用在早期笔记本中使用的多输出分类器包装器。因此，我们必须运行for循环来为每个输出变量训练单独的模型，并将每个输出变量的预测存储在验证集中。</li><li id="17db" class="kd ke hi jf b jg km jk kn jo ko js kp jw kq ka ln kj kk kl bi translated">train_supervised是用于快速文本分类的函数。我们可以调整学习参数来改进模型。</li><li id="73c9" class="kd ke hi jf b jg km jk kn jo ko js kp jw kq ka ln kj kk kl bi translated">到目前为止，还没有API可以接受一个验证集，并给出肯定情况的概率。我们一次只能得到一个句子的概率。因此，我们对每个验证语句运行一个for循环，并将概率存储在一个列表中。我们需要概率，因为性能指标是ROC-AUC</li></ul><pre class="in io ip iq fd la lb lc ld aw le bi"><span id="3cc1" class="lf lg hi lb b fi lh li l lj lk">#Will contain all the predictions for validation set for all the output variables<br/>all_preds = []<br/>#Iterating over all output variables to create separate models<br/>for col in tqdm(y_cols):<br/>    #Path for saving the training dataset<br/>    train_file = '/kaggle/working/final_train.csv'<br/>    #Saving the Output Variable and the text data to a csv<br/>    df_train_cleaned[[col, "comment_text"]].to_csv(train_file, header=None, index=False, columns=[col, "comment_text"]) <br/>    #Training the model<br/>    model = train_supervised(input=train_file, label="__class__", lr=1.0, epoch=2, loss='ova', wordNgrams=2, dim=200, thread=2, verbose=100)<br/>    #Predictions for validation sets for that ouput variable<br/>    col_preds = []<br/>    #Iterating over each sentence in the validation set<br/>    for text in df_val_cleaned["comment_text"].values:<br/>        #Get the prediction for class 1<br/>        pred = model.predict(text, k = 2)[1][1]<br/>        #Append the prediction to the list of predictions for that output variable<br/>        col_preds.append(pred)<br/>    #Append the list of predictions for a output variable to the overall set of predictions for all columns<br/>    all_preds.append(col_preds)</span></pre><p id="6dd4" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，我们已经从验证集的模型中获得了预测，让我们看看结果/性能。由于竞争使用平均ROC-AUC作为评估指标，我们将在笔记本中使用相同的指标。</p><pre class="in io ip iq fd la lb lc ld aw le bi"><span id="4092" class="lf lg hi lb b fi lh li l lj lk">#Function for calculating roc auc with given actual binary values across target variables and the probability score made by the model<br/>def accuracy(y_test, y_pred):<br/>    aucs = []<br/>    #Calculate the ROC-AUC for each of the target column<br/>    for col in range(y_test.shape[1]):<br/>        aucs.append(roc_auc_score(y_test[:,col],y_pred[:,col]))<br/>    return aucs</span><span id="98d1" class="lf lg hi lb b fi ll li l lj lk">#Actual Labels<br/>y_val_actuals = df_val_cleaned[y_cols].astype("int").to_numpy()<br/>#Prediction probability - minor ordering<br/>all_preds_array = np.transpose(np.array(all_preds))<br/>#Calculate the mean of the ROC-AUC for each of the ouput variable<br/>mean_auc = mean(accuracy(y_val_actuals,all_preds_array))</span></pre><p id="27e1" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">验证集上的ROC-AUC趋向于0.77左右，这比Word2Vec模型好得多。此外，这些只是2个时期和未调整参数的早期结果。我认为通过更好的调整，我们可以获得更好的结果。学习和预测也非常快。</p><p id="40bd" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">7 .<strong class="jf hj">。改进的结果和范围</strong></p><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es lo"><img src="../Images/69bce93a8b13f92bee1d104bbfa5fe10.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*8o9KFzyLA7nGfiZHMJ6qRw.png"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">Kaggle排行榜分数</figcaption></figure><p id="aba6" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><em class="kb">托多斯:</em></p><ol class=""><li id="b37a" class="kd ke hi jf b jg jh jk jl jo kf js kg jw kh ka ki kj kk kl bi translated">可以做更好的文本预处理、打字错误纠正等来进一步改进模型</li><li id="0961" class="kd ke hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">尝试调整超参数以获得更好的结果</li></ol><p id="f323" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这是关于快速文本的。下一个将是关于卷积神经网络(CNN)。细胞神经网络通常用于计算机视觉，然而，它们已经应用于各种NLP任务有一段时间了，结果是有希望的。在那之前保持安全。同样，整个代码呈现在<a class="ae jc" href="https://www.kaggle.com/anirbansen3027/jtcc-fasttext-supervised" rel="noopener ugc nofollow" target="_blank">(这里)</a>。请以回答和鼓掌的形式提供您的反馈:)</p></div></div>    
</body>
</html>