<html>
<head>
<title>Understanding Q,K,V In Transformer( Self Attention)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解变压器中的Q、K、V(自我关注)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-q-k-v-in-transformer-self-attention-9a5eddaa5960?source=collection_archive---------1-----------------------#2021-01-01">https://medium.com/analytics-vidhya/understanding-q-k-v-in-transformer-self-attention-9a5eddaa5960?source=collection_archive---------1-----------------------#2021-01-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="734a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我写这篇文章是对以前关于Transformer的文章的跟进。(<a class="ae jd" rel="noopener" href="/analytics-vidhya/nlp-transformer-unit-test-95459fefbea9">链接</a>)这里我假设你已经知道什么是Transformer，只是想了解更多关于架构中的Q、K、V向量。</p><p id="cad4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我从<a class="ae jd" href="#https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb" rel="noopener ugc nofollow">这里</a>得到基础代码。但是我变化太大了，所以你可能跟不上我的台词。代码在github，但使用nbviewer.com查看颜色。nbviewer中的<br/>代码<a class="ae jd" href="https://nbviewer.jupyter.org/github/mcelikkaya/medium_articles/blob/main/transformer_debugging.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>github中的<br/>代码<a class="ae jd" href="https://github.com/mcelikkaya/medium_articles/blob/main/transformer_debugging.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="25d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我把一切都简单化。什么是<strong class="ih hj"> n </strong>维的向量？<br/>对具有<strong class="ih hj"> n </strong>个特征的数据进行编码。所以当维度很高时，很难想象。</p><p id="7278" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以让我们做最简单的事情。<br/>假设我们下面有句子是:</p><p id="87e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">吃苹果<br/>吃面包<br/>喝水<br/>喝啤酒<br/>看报纸<br/>看书</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/8fdf287fbd1f1a93a273ae36493e3fa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*wbJ5y4dmT0_AoeLFpl8tDQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">维度2的向量表</figcaption></figure><p id="0b0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们为这些句子生成二维向量(不是单词的向量，而是句子的向量)，我们可以像上面那样创建。(维度越多，编码越好)通过这种过度(过度，过度)简化，如果我们将这些句子向量相乘，我们可以看到，当与食物相关的项目相乘时，食物相关的得分更高。</p><p id="3281" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">吃苹果x吃面包= 0.8 * 0.7 + 0.1 * 0.2 = 0.58 <br/>吃苹果x看书= 0.8 * 0.2 + 0.1 * 0.8 = 0.24</p><p id="2190" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是二维空间。现在想想，对于一个NLP问题，我们实际上需要多少个维度。回到我们的问题，我们的问题是学习英德翻译。我们的向量是64维的。我会尽量把它们调小2或3。</p><p id="348b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在transformer <strong class="ih hj">中，Q，K，V </strong>是我们用来对源词和目标词进行更好编码的向量。</p><p id="ffa6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">问:</strong>矢量(线性层输出)与我们编码的内容相关(输出，可以是编码器层的输出，也可以是解码器层的输出)<br/> <strong class="ih hj"> K: </strong>矢量(线性层输出)与我们用什么作为输入来输出相关。<br/> <strong class="ih hj"> V </strong>:作为计算结果的学习矢量(线性层输出)，与输入相关</p><p id="0415" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在《变形金刚》中，我们有三个地方可以使用自我关注，所以我们有Q，K，V向量。</p><p id="c8f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1- <strong class="ih hj">编码器自我关注<br/> </strong> Q = K = V =我们的源句子(英文)</p><p id="47b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2- <strong class="ih hj">解码器自我关注<br/> </strong> Q = K = V =我们的目标句子(德语)</p><p id="7f69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3- <strong class="ih hj">解码器-编码器注意<br/> </strong> Q =我们的目标句(德语)<br/> K = V =我们的源句(英语)</p><p id="8182" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有了这个Q和K向量，我们就可以计算注意力，然后乘以V。所以非常粗略地说</p><p id="fd9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">attention = soft max((<strong class="ih hj">Q * K</strong>)/Scale)<br/>enbed for attention layer = attention *<strong class="ih hj">V</strong></p><p id="5586" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这只是其他博客或教程上的热身概述。首先，这些不是什么特别的东西，它们只是线性层。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="jq jr l"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="ak">Q、K、V用法的简化代码</strong></figcaption></figure><p id="0c2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如你所见，我们正在做一些非常基础的事情。我们有线性层，我们正在对它们进行一些操作。更有趣的是线性层的输入和输出大小是相同的。所以我们不做降维。我们在做什么？只是学习！</p><p id="5852" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当你开始训练一个网络时，所有的权重都是随机的，然后通过展示你的训练数据，根据你的损失进行小的更新，希望训练后有更好的权重。</p><p id="4226" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以现在我将展示Q，K，V的各种可视化，以了解它们的作用。</p><p id="108c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在编码器<strong class="ih hj">处，Q、K、V </strong>来自编码器嵌入，因此它们的嵌入层是针对同一种语言的。(Src语言英语)那么它们是来自相同空间的向量。对于两个不同的句子，我正在收集时间点[1，15，30]的[Q，K，V，X1]向量，并转储到3d或2d。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="jq jr l"/></div></figure><pre class="jf jg jh ji fd js jt ju jv aw jw bi"><span id="8c63" class="jx jy hi jt b fi jz ka l kb kc">energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale<br/>...                            <br/>attention = torch.softmax(energy, dim = -1)<br/>...<br/>x1 = torch.matmul(self.dropout(attention), V)<br/>#x1 is the vector of attention multiplied by V, so it is what Q,K,V learns</span><span id="97f9" class="jx jy hi jt b fi kd ka l kb kc">sentence1 = "we want to eat apple"<br/>sentence2 = "we want to eat bread"</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ke"><img src="../Images/52b9b856471ad640ec814f6cb4dc9e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*--uufcl0GMkT8_1cNLX0gA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="bd kf">编码器的Q、K、V和x1向量行进解空间</strong></figcaption></figure><p id="175a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图显示了两个句子(不同颜色)的向量和它们的聚类。标签解释:</p><p id="f4c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">吃@k@30</p><p id="3a39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">吃</strong>:息字<br/> <strong class="ih hj"> K </strong>:向量<br/> <strong class="ih hj"> 30 </strong>:历元</p><p id="4e66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的图表显示了什么？我有两个句子，都有动词“吃”。在语言建模中，单词必须有符合其上下文的含义。所以一个好的模型必须有不同的表现<strong class="ih hj">【吃面包】</strong><strong class="ih hj">【吃苹果】</strong>。</p><p id="66c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如你所见，Q、K、V形成了它们的聚类，并且句子1的结果X1@30对于<strong class="ih hj">“eat”</strong>变得不同于句子2。这是编码器侧。</p><p id="4f26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以在解码器端做同样的事情。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kg"><img src="../Images/de0fb1c529d58945d61966c86eb4f723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*Q_sYiQA-JgUI8WB2c2DrIQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="bd kf">解码器的Q、K、V和x1向量行进解空间</strong></figcaption></figure><p id="b8a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如你所见，解码器端更分散。因为编码器只有1个输入类型，(源语言)，但是解码器端有他的自我关注+编码器关注+位置层。</p><p id="6aa9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你能理解与Q，K，V和一个单词的嵌入的关系吗？想我有一个只有<strong class="ih hj">输入- &gt; Q - &gt; K - &gt; V - &gt;输出</strong>的网络，我把我的话馈入这个网络。在每个时期，我更新这个网络的权重，根据损失(这里是多类分类)网络更新这些Q、K、V(线性层)权重。现在，在训练结束时，V输出将符合输入字，对不对？这就是神经网络学习的方式，这就是这里发生的事情。</p><p id="fe0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们总结一下，在一个大的网络架构中，我们有3个线性层，Q，K，V，通过这些层，我们为我们的单词创建了一个嵌入。不如我们只学1编码。</p><pre class="jf jg jh ji fd js jt ju jv aw jw bi"><span id="f035" class="jx jy hi jt b fi jz ka l kb kc">Think we have 3 values <br/>10,20,30      -&gt; Their multiplication is 6000<br/>If we decrease every value<br/>9 x 19 x 29   -&gt; 4959<br/>If we increase every value<br/>11 x 21 x 21  -&gt; 7161</span><span id="a7bf" class="jx jy hi jt b fi kd ka l kb kc">As you see with little changes(with little updates to parameters by backpropagation) we can make big differences.</span></pre><p id="5a6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们试试，<br/> <strong class="ih hj">理论</strong>:改变句子的顺序一定会得到相同的X4。<br/>为什么？因为Q，K，V向量不讲究位置，而且是通过，乘以所有其他的来计算的。认为这些是对一本书的总结，或者更好地表达一些特征。汽车是轮子、窗户、座位和刹车的组合。所以创建Q，K，V在某种程度上是特征提取。</p><p id="9a1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还要记住，Q，K，V的计算是排列不变的。本质上，我认为结果向量X4是相似的。(校验码)</p><p id="2471" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我声称句子<strong class="ih hj">“我们能吃苹果”</strong>和<strong class="ih hj">“苹果能吃我们”</strong>将有:<br/> <strong class="ih hj"> Q:相似的</strong>(最后它们将产生相同的句子，所以在每一步它们将有相似的Q)，对角线上相同的条目<br/> <strong class="ih hj"> K:不同的</strong>，不是所有的对角线都不同，但是不同位置的相同项目(苹果，吃…)将有相同的嵌入。(反对角线)<br/> <strong class="ih hj"> V:不同</strong>，不是所有的对角线都不一样，但是相同的物品(苹果，吃…)在不同的位置会有相同的嵌入。(反对角线)<br/> <strong class="ih hj"> X4:相同</strong>，因为最后3个矩阵相乘(几乎与scale和softmax相乘)的效果会产生相似的向量，因为源语句相同，我们一个一个生成输出。<br/> <strong class="ih hj">注意:</strong>必须是<strong class="ih hj">相同</strong>不同位置的相同项目。</p><p id="7a23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我生成了两个句子的Q，K，V向量，并得到所有组合的余弦相似度。检查对角线，以了解如何在不同位置为相同的项目创建相同的嵌入。列和索引是相同的，因为两个句子创建了相同的输出。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kh"><img src="../Images/fff565c819c1b31aafcaa3d0ea7f1ef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*XtCdhI9TcJOYh71WC40m-A.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="bd kf"> Q，K，V为2个句子的解码器。</strong></figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ki"><img src="../Images/e41e8409defaa8ea1e8dfb778d304ccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*rq-nuTrPDuLnxGJ_p0n5rA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="bd kf">解码器x4及注意事项</strong></figcaption></figure><p id="9495" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过检查上面的表格，你可以理解这些向量的关系。你也可以尝试不同的句子，不同的组合。</p><p id="0583" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们做另一个例子。对于5个句子，我将得到Q，K，V，X4向量，并检查它们在空间中的位置。第一句和第二句是恰当的，第三句和第四句是废话。最后一句都有“苹果”和“书”，所以它是有误导网络。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="jq jr l"/></div></figure><pre class="jf jg jh ji fd js jt ju jv aw jw bi"><span id="d3d2" class="jx jy hi jt b fi jz ka l kb kc">#The translations of 5 sentences <br/><strong class="jt hj">source = ['i', 'can', 'eat', 'apple']</strong><br/>predicted target = ['ich', 'konnen', 'apfel', 'essen', '&lt;eos&gt;']<br/><strong class="jt hj">source = ['i', 'can', 'eat', 'bread']</strong><br/>predicted target = ['ich', 'konnen', 'brot', 'essen', '&lt;eos&gt;']<br/><strong class="jt hj">source = ['i', 'can', 'eat', 'book']</strong><br/>predicted target = ['ich', 'konnen', 'apfel', 'essen', '&lt;eos&gt;']<br/><strong class="jt hj">source = ['i', 'can', 'eat', 'newspaper']</strong><br/>predicted target = ['ich', 'konnen', 'brot', 'essen', '&lt;eos&gt;']<br/><strong class="jt hj">source = ['i', 'can', 'eat', 'apple', 'book']</strong><br/>predicted target = ['ich', 'konnen', 'apfel', 'essen', '&lt;eos&gt;']</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kj"><img src="../Images/fa02a6613d1b677ed768dc52f8d6b6da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*kdp0zZDvTLaxtdEbdiKe-g.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">q矢量</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kk"><img src="../Images/2c3f9d07a4648836a54b4c0abcfd22f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BSJmOCwWMiEislu5_x2pbQ.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">k向量</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kp"><img src="../Images/67bbb9914759d9c7eb336159f60008ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*UBvYNBKoY-dmEVLVHEEXtQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">v向量</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kq"><img src="../Images/7fcddc375d2078403b8691ac082d34e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*v-cB3JBaE0ipgDNyEv9C8w.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kr"><img src="../Images/f6cdd8fbb4d4a8a47fc36aff9077920f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*13H8xIEYueHsg-pS3BUSHA.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ks"><img src="../Images/3ce27c5b8a888e99f15af81d970a0a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*jPiLBB9MM2hP2fln96y5Xg.png"/></div></figure><p id="a7b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如你所见，<strong class="ih hj"> Q，K，V </strong>向量看起来如此相似，但结果(<strong class="ih hj"> X1，X4，src_final </strong>)却是分散的。因为正如我说的，也许一个向量和其他向量在类别上没有太大的不同，但是它们的组合效果是不同的。我们也可以展示它的解码器部分。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kt"><img src="../Images/18cc1bc7d7d6ab398c4dfa41f8aaa457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*reph6xUxkvB_5xfrNkU9mw.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kt"><img src="../Images/18cc1bc7d7d6ab398c4dfa41f8aaa457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*reph6xUxkvB_5xfrNkU9mw.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ku"><img src="../Images/7ee94a1cacfe21564568ca637ced1cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*3hD8pG2sq2Qm2y2dxUafGQ.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kv"><img src="../Images/44a449ea15b7081ca4add88c5022d42c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*kwQR544XhS9RfOA1kfJbpw.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kw"><img src="../Images/36162181245410ce065106c6ba600b4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*Y568MD-n9zUIb29Xe7YFMg.png"/></div></figure><p id="a7c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你看到的，解码器有更多的分散向量，因为它有更多的数据来源，(包括计算中更多和不同的向量)</p><p id="6587" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过上面我展示的例子，我们用这些向量所做的是创建单词的良好表示。我们怎么知道“水”是“可以喝的”，而“苹果”是可以吃的？因为我们看到很多成对出现的句子。所以当你看到一个自我关注的逻辑时，你会明白，随着时间的推移，网络学习哪些是随机出现的，哪些不是。然后根据上下文创建好的输入向量。</p></div></div>    
</body>
</html>