<html>
<head>
<title>Longformer: A Transformer for Long Form Documents</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Longformer:长格式文档的转换器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/longformer-a-transformer-for-long-form-documents-e2145da6a58b?source=collection_archive---------6-----------------------#2021-01-05">https://medium.com/analytics-vidhya/longformer-a-transformer-for-long-form-documents-e2145da6a58b?source=collection_archive---------6-----------------------#2021-01-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><ul class=""><li id="99bf" class="im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd bi translated"><strong class="io hj"> <em class="je">与较长表单文档上的普通变压器相关的问题:</em> </strong></li></ul><ol class=""><li id="1acd" class="im in hi io b ip iq ir is it iu iv iw ix iy iz jf jb jc jd bi translated">对具有远距离连接的较长段落的上下文进行编码不够健壮。一种方法是将段落分成几个块，然后分别对每个块进行编码。但是这样做并不能确保来自一个组块的标记和来自其他组块的标记之间的任何联系，因为一个组块中的标记并不关注其他组块中的标记。因此，我们需要找到一种方法，这样我们就可以将整个文档传递给模型，这样即使在较长的距离上，信息也可以保持连接。</li><li id="3841" class="im in hi io b ip jg ir jh it ji iv jj ix jk iz jf jb jc jd bi translated">就存储器需求和复杂性而言，自我注意本质上是二次的，O(n)是因为每个令牌都参与前一层中的所有令牌。随着n值的增加，内存需求将呈二次方增加。</li></ol></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><ul class=""><li id="a5b1" class="im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd bi translated"><strong class="io hj"> <em class="je">如何解决这些问题？</em> </strong></li></ul><p id="27af" class="pw-post-body-paragraph jl jm hi io b ip iq jn jo ir is jp jq it jr js jt iv ju jv jw ix jx jy jz iz hb bi translated">我们必须开始考虑将内存需求降低到线性复杂度的可能性。从CNN(使用过滤器/内核，并在输入数据上滑动，以线性复杂度提取信息)汲取灵感，<a class="ae ka" href="https://arxiv.org/pdf/2004.05150.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的作者提出了用滑动窗口注意力模式(本文稍后解释)取代这种昂贵的注意力模式的想法。这也将有助于对整个长格式文档进行编码，从而保留整个文档中的上下文。Longformer中使用的注意模式还有两种:扩大的滑动窗口&amp;全局+滑动窗口。我们现在将了解这些注意力模式如何在该架构中发挥作用，但在此之前，我将带您了解这些不同类型的注意力模式及其优势:</p></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><blockquote class="kb kc kd"><p id="3f66" class="jl jm je io b ip iq jn jo ir is jp jq ke jr js jt kf ju jv jw kg jx jy jz iz hb bi translated">1.<strong class="io hj">二次关注:</strong></p></blockquote><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/802bba7f97fb64107cd44c217a7bc395.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*kIi5ZbGGWme8q2o2wKNTVA.png"/></div></figure><p id="2a14" class="pw-post-body-paragraph jl jm hi io b ip iq jn jo ir is jp jq it jr js jt iv ju jv jw ix jx jy jz iz hb bi translated">这是Vanilla Transformer所遵循的注意模式，其中每个令牌都关注其他令牌。如果我们认为这个注意域的长度和宽度等于n(即，令牌的数量)，那么每个单个令牌将每隔一个令牌出现，导致O(n)个存储器需求。</p><blockquote class="kb kc kd"><p id="0027" class="jl jm je io b ip iq jn jo ir is jp jq ke jr js jt kf ju jv jw kg jx jy jz iz hb bi translated">2.<strong class="io hj">滑动窗口注意:</strong></p></blockquote><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kp"><img src="../Images/d0b639c7cb283ce608c62fd4b16c3a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*oMxaCwsYd7pWwpN3GAIkOA.png"/></div></figure><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kq"><img src="../Images/7d091146d065d2d86a3fa80801bfd8a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*yt46aPUn6Q9tx7p0QdmIrw.jpeg"/></div></figure><p id="76a5" class="pw-post-body-paragraph jl jm hi io b ip iq jn jo ir is jp jq it jr js jt iv ju jv jw ix jx jy jz iz hb bi translated">这是Longformer架构中采用的注意力模式。这涉及在序列长度n上滑动的大小为(= w)的窗口，每个标记将关注其自身以及前一层中该窗口内的其他标记，并且将滑动直到序列的末尾，线性复杂度为O(w*n) = O(n)，上层中的标记能够掌握更长的上下文信息。窗口(w)的大小是最初在<a class="ae ka" href="https://arxiv.org/pdf/2004.05150.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中讨论的512。但是为了便于理解，我们在本文中假设了更小的窗口大小。</p><p id="de36" class="pw-post-body-paragraph jl jm hi io b ip iq jn jo ir is jp jq it jr js jt iv ju jv jw ix jx jy jz iz hb bi translated">在上图中可以看到，当窗口大小=3时，第一个隐藏层的令牌<strong class="io hj"> x41 </strong>正在参加上一层窗口大小=3内的令牌(<strong class="io hj"> x30，x40，x50 </strong>)。类似地，第二隐藏层中的记号<strong class="io hj"> x42 </strong>参与来自前一隐藏层(即，第一隐藏层)的窗口内的记号，前一隐藏层本身参与记号<strong class="io hj"> x20、x30、x40、x50、x60 </strong>，这表明上层记号能够从输入中学习更长的上下文，这就是hack！！因此，如果我们继续增加层的数量，那么变换器的最后一层中的令牌将参与到具有线性复杂度的存储器需求的非常长的上下文中。</p><p id="aacf" class="pw-post-body-paragraph jl jm hi io b ip iq jn jo ir is jp jq it jr js jt iv ju jv jw ix jx jy jz iz hb bi translated">这种注意模式对于在窗口内对来自输入层的本地信息进行编码是有用的。我们希望收集直接来自输入的每一个信息，因此在transformer的初始层使用滑动窗口注意模式来提取所有局部信息是明智的。</p><blockquote class="kb kc kd"><p id="be0a" class="jl jm je io b ip iq jn jo ir is jp jq ke jr js jt kf ju jv jw kg jx jy jz iz hb bi translated">3.<strong class="io hj">放大滑动窗口注意:</strong></p></blockquote><div class="ki kj kk kl fd ab cb"><figure class="kr km ks kt ku kv kw paragraph-image"><img src="../Images/4c4ebc635f7518491a1ab5a271f022b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*KIegc873H5_P-HWuGUAMkw.png"/></figure><figure class="kr km kx kt ku kv kw paragraph-image"><img src="../Images/f05199bee4e250e64c134dc86ef855e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*SUk94EMQvdW6dwiIqAvJ8g.jpeg"/></figure></div><p id="c1c3" class="pw-post-body-paragraph jl jm hi io b ip iq jn jo ir is jp jq it jr js jt iv ju jv jw ix jx jy jz iz hb bi translated">在扩大的滑动窗口注意模式中，transformer层中的每个标记都关注自身，并关注前一层中的更多标记，从而留下一个一致的间隙(在右图中，间隙=1)。第一个隐藏层的令牌<strong class="io hj"> x41 </strong>正在参加令牌:<strong class="io hj"> x40 </strong>、<strong class="io hj"> x20 </strong>、<strong class="io hj"> x60 </strong>，遗漏令牌<strong class="io hj"> x30 </strong>、<strong class="io hj"> x50 </strong>。这种注意模式用于在相对较长的上下文中掌握大范围的信息，因此，用于长格式转换程序的上层或下层。</p><blockquote class="kb kc kd"><p id="adf3" class="jl jm je io b ip iq jn jo ir is jp jq ke jr js jt kf ju jv jw kg jx jy jz iz hb bi translated">4.<strong class="io hj">全局+滑动窗口注意:</strong></p></blockquote><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ky"><img src="../Images/c55ea19a3d5ee4f2fb8cde3361389370.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*RIfwDi7l7tyZTNYqI974Hw.png"/></div></figure><p id="f3cc" class="pw-post-body-paragraph jl jm hi io b ip iq jn jo ir is jp jq it jr js jt iv ju jv jw ix jx jy jz iz hb bi translated">这种注意模式使用全局注意和滑动窗口注意的混合，全局注意是在一些特殊的标记上计算的，如通过序列长度<strong class="io hj"> n. </strong>参与全局信息的【CLS】标记</p></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><h2 id="0dd6" class="kz la hi bd lb lc ld le lf lg lh li lj it lk ll lm iv ln lo lp ix lq lr ls lt bi translated"><strong class="ak">可利用Longformer执行:</strong></h2><ul class=""><li id="c393" class="im in hi io b ip lu ir lv it lw iv lx ix ly iz ja jb jc jd bi translated"><strong class="io hj"> <em class="je">【自回归建模(从左到右上下文学习):</em> </strong></li></ul><p id="e270" class="pw-post-body-paragraph jl jm hi io b ip iq jn jo ir is jp jq it jr js jt iv ju jv jw ix jx jy jz iz hb bi translated">对于自回归语言建模，随着层数的增加，滑动注意窗口的大小也增加。如前所述，较低的层使用滑动窗口注意模式，而后面的层使用扩展窗口注意模式来学习远处的信息，而不损害本地上下文。以此为目标的培训分5个阶段进行。每一阶段，输入序列长度增加，学习速率减半。第一阶段从序列长度为2048开始，到最后阶段的序列长度为23，040。</p><p id="4020" class="pw-post-body-paragraph jl jm hi io b ip iq jn jo ir is jp jq it jr js jt iv ju jv jw ix jx jy jz iz hb bi translated">在评估模型时，数据集被分割成每个长度为32，256的序列，在该序列上以512的步长评估模型。</p><ul class=""><li id="0e31" class="im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd bi translated"><strong class="io hj"> <em class="je">使用MLM(掩蔽语言建模)的预训练目标</em> </strong> <em class="je"> : </em></li></ul><p id="208c" class="pw-post-body-paragraph jl jm hi io b ip iq jn jo ir is jp jq it jr js jt iv ju jv jw ix jx jy jz iz hb bi translated">预训练Longformer非常昂贵，因此论文<a class="ae ka" href="https://arxiv.org/pdf/2004.05150.pdf" rel="noopener ugc nofollow" target="_blank">的作者建议从预训练RoBERTa的检查点开始，然后是Longformer。这里使用的位置嵌入是预训练的RoBERTa的绝对位置嵌入，唯一的区别是我们必须通过多次复制来增加位置嵌入的长度，直到达到序列的长度。使用MLM(掩蔽语言模型)目标进行预训练，设置与RoBERTa中的设置相同(包括权重、层数等。).注意窗口的长度保持为512。</a></p><ul class=""><li id="eaec" class="im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd bi translated"><strong class="io hj"> <em class="je">微调:</em> </strong></li></ul><p id="67d5" class="pw-post-body-paragraph jl jm hi io b ip iq jn jo ir is jp jq it jr js jt iv ju jv jw ix jx jy jz iz hb bi translated">这些模型在几个任务上得到了很好的调整，比如文档分类、共指消解、问答任务。由于这篇文章越来越长，我建议参考原<a class="ae ka" href="https://arxiv.org/pdf/2004.05150.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><ul class=""><li id="9c44" class="im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd bi translated"><strong class="io hj"> <em class="je">【龙前编码器解码器(LED)】</em></strong></li></ul><p id="ea4f" class="pw-post-body-paragraph jl jm hi io b ip iq jn jo ir is jp jq it jr js jt iv ju jv jw ix jx jy jz iz hb bi translated">这需要在Longformer编码器的顶部添加一个解码器，它可以对生成任务(如摘要)进行预测。回想一下BART总结程序，它能够总结不太长的段落。为生成任务预先训练一个Longformer可能非常昂贵，所以我们用BART的检查点用类似的设置(包括层数和权重)初始化Longformer。唯一的区别是，我们必须将BART的位置嵌入大小从1K增加到16K。这个摘要器在长文档摘要任务上的表现优于所有其他模型，如BigBird模型。</p></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="b7f6" class="pw-post-body-paragraph jl jm hi io b ip iq jn jo ir is jp jq it jr js jt iv ju jv jw ix jx jy jz iz hb bi translated">我已经解释了该架构的机制，以及为什么这种想法对较长形式的文档有用，但是如果您对获得的结果中的确切数字以及用于训练和测试的数据集感兴趣，我会建议您参考这篇<a class="ae ka" href="https://arxiv.org/pdf/2004.05150.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。这将帮助你获得更多的见解，一旦你完成这篇文章，最初的<a class="ae ka" href="https://arxiv.org/pdf/2004.05150.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>将看起来更容易理解。我试图从我的角度清晰地解释它，希望它能帮助你，我感谢你的耐心。祝你旅途愉快，下次再见！</p></div></div>    
</body>
</html>