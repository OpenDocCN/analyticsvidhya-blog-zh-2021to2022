# 带有 Keras 的人工神经网络中的完全

> 原文：<https://medium.com/analytics-vidhya/out-and-out-in-artificial-neural-networks-with-keras-3e4f1aa08cc7?source=collection_archive---------3----------------------->

神经网络的基本直觉

## 动机

当我开始阅读关于神经网络的文章时，我面临着许多斗争，以理解神经网络背后的基础知识以及它们是如何工作的。开始在网上看越来越多的文章，抓住那些重点，一起整理成私信给我。而且，我想把它们发表出来，让其他人更好地理解。

了解任何领域的基础知识都会很有趣。

![](img/964aae154fb691faa6c73895a79bede5.png)

艾莉娜·格鲁布尼亚克在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

# 感知器

感知器是最简单的人工神经网络架构之一，由 Frank Rosenblatt 于 1957 年发明。它是一种略有不同的人工神经元，称为 TLU(阈值逻辑单元)有时是线性阈值单元。每个输入连接都与一个权重相关联。

单个 TLU 可用于简单的线性二元分类。它计算输入的线性组合，如果结果超过阈值，则输出正类，否则输出负类。

# 感知器是如何训练的？

使用该规则的变体来训练它们，该变体考虑了网络在进行预测时产生的误差；然后感知器学习规则加强连接，帮助减少错误。他们一次接受一个训练实例，对于每个实例，它都做出预测。对于每一个产生错误预测的输出神经元，它都加强了来自输入的连接权重，而这些输入本来会有助于正确的预测。

Scikit-Learn 的感知器类相当于使用 SGD 分类器。

> 与逻辑回归分类器相反，感知器不输出类别概率；相反，他们基于一个硬门槛做出预测。这是比感知器更喜欢逻辑回归的一个主要原因。

![](img/e4d37593305cd8b6ef823b6089769b63.png)

# 反向传播

这个过程非常重要，对于每个训练实例，反向传播算法首先进行预测(正向传递)并测量误差，然后反向遍历每个层以测量每个连接的误差贡献(反向传递)，最后调整连接权重以减少误差(梯度下降步骤)。

随机初始化所有隐层连接权重是很重要的，否则训练将会失败。例如，如果将所有权重和偏差初始化为零，那么给定层中的所有神经元将完全相同，因此反向传播将以相同的方式影响它们，因此它们将保持相同。换句话说，尽管每层有数百个神经元，你的模型将表现得好像每层有一个神经元；不会太聪明。相反，如果你随机初始化权重，就会破坏对称性，允许反向传播来训练一个多样化的神经元团队。

# 回归多层感知器

在为回归建立 MLP 方程时，我们不需要输出神经元的激活函数，因此它们可以自由输出任何范围的值。但是如果你想要正的输出，那么使用 ReLU 激活函数，当值为负时，它是平滑的并且接近于 0，当值为正时，它接近于值。否则，如果您只需要一个范围的值，则使用逻辑或正切，对于逻辑函数，将标签从 0 缩放到 1，对于双曲正切，将标签从-1 缩放到 1。

Huber loss:是 MAE 和 MSE 的结合。当误差小于阈值时，它是二次的，而当误差大于阈值时，它是线性的。

# MLP 氏分类

在 MLP 的中，有像二元分类这样的分类任务。我们可以说这种分类在数字 0 和 1 之间有一个输出神经元。MLP 可以处理多标签分类任务。例如垃圾邮件或紧急检测。更一般地说，您可以为每个正类指定一个输出神经元。如果每个实例可以属于一个单独的类，那么我们可以对输出层使用 softmax 激活函数。

熟悉神经网络:[神经网络的游乐场](http://playground.tensorflow.org/)

![](img/69a17c5fccede4ae9941fcbafdf70a2a.png)

照片由[本·赫尔希](https://unsplash.com/@benhershey?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

# 用函数式 API 构建复杂问题

这种非顺序神经网络具有广度和深度的神经网络。

让我们建立这样一个网络:

但是如果我们需要通过宽路径的特征子集和不同的子集(可能重叠)，我们将通过深路径。

让我们建立这样一个网络:

使用功能子集的许多用例:

1.  定位并分类图片中的主要对象。
2.  使用一个输出对面部表情进行分类，另一个输出对是否戴眼镜进行分类，对面部图片进行多任务分类。
3.  减少过拟合和提高模型能力的正则化技术。例如，在神经网络中使用辅助输出，就像它自己学习一样，而不依赖于其他网络。

添加额外的输出非常简单，只需将它们连接到适当的层，并将它们添加到模型的输出列表中。参见下面的代码。

> 每个输出函数都需要其损失函数来编译模型，因此，为此，我们需要传递一个损失列表(如果我们传递一个损失，那么 Keras 将假设该损失必须用于所有输出，在这种情况下，默认情况下，Keras 将计算所有这些损失，只需将它们相加，即可获得用于训练的最终损失)。我们更关心主输出而不是辅助输出(因为它只是用于正则化)。所以我们必须给主输出分配比辅助输出更多的权重。

```
model.compile(loss=['mse', 'mse'], loss_weights=[0.9,0.1], optimizer='sgd')
```

# 子类化 API 以构建动态模型

它比顺序函数式 API 有更多的优势。更命令式的编程风格。

*   它可以很容易地保存、克隆和共享。
*   可以显示和分析该结构。
*   可以很容易地检测和调试错误。
*   整个模型有一个静态的图层图。
*   循环、条件分支和各种形状

# 微调神经网络超参数

为了提高模型精度或使神经网络更灵活，我们必须微调超参数。有许多超参数需要调整，因此，我们必须聪明地选择需要调整的超参数。最初，尽可能多地调优超参数有助于理解微调。

# 隐藏层数&每个隐藏层要使用的神经元数量

在网络中使用隐藏层是神经网络中的一项任务。因此，隐藏层的使用完全取决于数据集。确保使用输入数据来分配每个隐藏层和输出层的神经元。不要为每个隐藏层分配相同的神经元，有时也有更好的，只有一个超参数要调整，而不是每层一个。

对于像 MNIST 数据集这样的一些问题，使用一个或两个包含 100 个神经元的隐藏层就可以了。但是，对于更复杂的问题，我们可以增加隐藏层的数量，直到模型过度拟合。总之，在使用隐藏层时，我们可以使用尽可能多的层，直到它在训练中溢出。

可以分配比我们需要的更多的层和神经元，然后使用早期停止和一些其他正则化技术来防止过度拟合。在某些情况下，如果一个层的神经元比它需要的少，那么它将由于缺少神经元而在训练时丢失信息。无论网络的其余部分有多么庞大和强大，这些信息都将永远无法恢复。

# 学习率

在构建神经网络时，学习速率更为重要，因为这对训练模型很有意义。为了检查最佳学习率，然后开始使用低水平学习率(10^-5)并增加到(10)，然后开始乘以 500 次迭代的对数，并绘制它们以检查损失。最优学习率会比亏损开始攀升的点低太多。

> 此外，最佳学习率取决于超参数，尤其是批量大小，因此如果您修改任何超参数，请确保也更新学习率

# 保存和恢复模型

当使用顺序和功能 API 时，我们可以保存模型。

```
model.save("my_model.h5")
```

保存和恢复只能通过顺序和功能 API 来完成，而不能通过模型子类化来完成。我们可以用`save_weights`和`load_weights`保存和恢复模型权重

这篇文章将完全直觉化为神经网络。

感谢阅读！！

希望你喜欢。点击拍手图标。