<html>
<head>
<title>Text Processing and Classification Intro (Part 2 — Text Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本处理和分类介绍(第二部分——文本分类)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-processing-and-classification-intro-part-2-text-classification-9dc58782acb6?source=collection_archive---------2-----------------------#2021-12-07">https://medium.com/analytics-vidhya/text-processing-and-classification-intro-part-2-text-classification-9dc58782acb6?source=collection_archive---------2-----------------------#2021-12-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/36412d9ba11733fe62b14430642f7dd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*P_2SDygl1gtFKrKU.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://cfml.se/blog/sentiment_classification/" rel="noopener ugc nofollow" target="_blank">https://cfml.se/blog/sentiment_classification/</a></figcaption></figure><blockquote class="iu"><p id="21d1" class="iv iw hh bd ix iy iz ja jb jc jd je dx translated">我们如何预测新评论的情绪？</p></blockquote><p id="62d8" class="pw-post-body-paragraph jf jg hh jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb je ha bi translated">在我写作的前一部分，我试图涵盖分析文本数据的步骤。对上一部分执行的分析是描述性的。这一部分将试图解释我们将如何预测新评论的情绪。对于这一部分，我从这篇<a class="ae it" rel="noopener" href="/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e">中的文章</a>中学习了很多。</p><blockquote class="kc kd ke"><p id="0501" class="jf jg kf jh b ji kg jk jl jm kh jo jp ki kj js jt kk kl jw jx km kn ka kb je ha bi translated">链接上一部分写作:<br/><a class="ae it" href="https://sea-remus.medium.com/text-processing-and-classification-intro-part-1-sentiment-analysis-7e22a83e1c4" rel="noopener">https://sea-Remus . medium . com/text-processing-and-class ification-intro-part-1-sensation-analysis-7e 22a 83 E1 C4</a></p></blockquote><h1 id="bfd7" class="ko kp hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">简要回顾第1部分</h1><p id="2881" class="pw-post-body-paragraph jf jg hh jh b ji lm jk jl jm ln jo jp jq lo js jt ju lp jw jx jy lq ka kb je ha bi translated">我们希望使用的数据包含以下各列:</p><figure class="lr ls lt lu fd ii er es paragraph-image"><div class="ab fe cl lv"><img src="../Images/95e9b81e753f545e761650780c09cbd6.png" data-original-src="https://miro.medium.com/v2/format:webp/1*TRc8apBd4obifoMIJKoF2A.png"/></div></figure><blockquote class="kc kd ke"><p id="644e" class="jf jg kf jh b ji kg jk jl jm kh jo jp ki kj js jt kk kl jw jx km kn ka kb je ha bi translated">我们知道评论的观点实际上是通过“分数”栏分类的(这使得建模实际上是多余的)，但是本文假设我们没有“分数”栏。</p></blockquote><p id="00c6" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">通过分析我们的文本数据，我们知道市场上存在两种情绪。我们将这些情绪分为积极情绪和消极情绪。我们将使用停用词来删除多余的词，这些词还包括“br”、“href”、“amazon”、“product”、“one”、“find”、“taste”、“flavor”、“good”、“buy”、“make”和“coffee”，因为通过分析我们发现这些词也是多余的。<br/>根据之前的分析和EDA，我们最终得出结论，我们不会使用“摘要”栏，因为一些用户没有给出他们评论的摘要。</p><h1 id="9e05" class="ko kp hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">数据预处理</h1><p id="2ecc" class="pw-post-body-paragraph jf jg hh jh b ji lm jk jl jm ln jo jp jq lo js jt ju lp jw jx jy lq ka kb je ha bi translated">我们将首先导入基本包，并使用以下代码获取数据:</p><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="6a0e" class="mb kp hh lx b fi mc md l me mf">#essentials<br/>import pandas as pd<br/>import numpy as np<br/>import sqlite3<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="cbc2" class="mb kp hh lx b fi mg md l me mf">#obtaining the data<br/>con = sqlite3.connect('database.sqlite')<br/>raw = pd.read_sql_query("select ProductId,ProfileName, HelpfulnessNumerator, HelpfulnessDenominator, Time, Text, case when Score &gt;= 4 then 1 else 0 end Sentiment from Reviews", con)<br/>con.close()</span></pre><p id="f1dc" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">在我们开始讨论数据之前，我们将数据分成3部分，即训练数据、测试数据和验证数据。目的实际上是区分用于超参数调整的测试数据(如果有)和用于模拟真实世界新数据的测试数据。</p><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="600f" class="mb kp hh lx b fi mc md l me mf">from sklearn.model_selection import train_test_split<br/>xtrain, xsplit, ytrain, ysplit = train_test_split(raw.drop('Sentiment',axis = 1),<br/>                                                  raw.Sentiment, <br/>                                                  test_size = 0.3, <br/>                                                  random_state = 42)<br/>xtest, xval, ytest, yval = train_test_split(xsplit,ysplit,<br/>                                            test_size = 0.5,<br/>                                            random_state = 42)</span></pre><p id="39b1" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">我们还将定义一个标记化函数。我们将在删除冗余单词(停用词)后对文本进行分词，并在分词后对文本进行分词。</p><blockquote class="kc kd ke"><p id="9621" class="jf jg kf jh b ji kg jk jl jm kh jo jp ki kj js jt kk kl jw jx km kn ka kb je ha bi translated">所以对于那些在阅读这篇文章时对文本处理一无所知的人来说:</p><p id="505f" class="jf jg kf jh b ji kg jk jl jm kh jo jp ki kj js jt kk kl jw jx km kn ka kb je ha bi translated">自然语言处理中的记号化，基本上就是<strong class="jh hi">把句子(单词序列)拆分成单词(或记号)</strong>。实际上，我们可以通过使用内置的Python字符串方法来实现这一点。split()或通过使用NLTK包中的word_tokenize()来实现。</p><p id="7227" class="jf jg kf jh b ji kg jk jl jm kh jo jp ki kj js jt kk kl jw jx km kn ka kb je ha bi translated">词汇化是一个获取词根的过程。词干化的一种替代方法是词干化，即只对单词进行词干化。两者的想法都是缩短单词，这样如果我们发现两个或更多的单词有相似的意思，我们会认为它们是同一个东西。例如，我们有一个单词列表，如['播放'，'播放'，'已播放']。当被词干化或被列化时，我们将获得['play '，' play '，' play']。在某些情况下，词干化比变元化更好，而在其他情况下，情况正好相反。可以把这看作是数字数据的规范化和标准化。</p></blockquote><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="9262" class="mb kp hh lx b fi mc md l me mf">def tokenize(txt):<br/>    import re<br/>    from wordcloud import STOPWORDS<br/>    import nltk<br/>    from nltk.tokenize import word_tokenize<br/>    from nltk.stem.wordnet import WordNetLemmatizer<br/>    stpwrd = set(STOPWORDS)<br/>    #we then add frequent irrelevant words discovered before<br/>    stpwrd.update(['br','href','amazon','product','one',<br/>                   'find','taste','flavor','good','buy',<br/>                   'make','coffee'])<br/>    removeapos = txt.lower().replace("'",'') #remove any apostrophe<br/>    text = re.sub(r"[^a-zA-Z0-9\s]"," ", removeapos) #sub weird char to space<br/>    words = word_tokenize(text)<br/>    lemma = [WordNetLemmatizer().lemmatize(w) for w in words if w not in<br/>             stpwrd]<br/>    return lemma</span></pre><p id="2085" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">在定义了记号化函数之后，我们定义一个函数来处理丢失的值和设计新的变量。</p><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="60d8" class="mb kp hh lx b fi mc md l me mf">def preprocess_engineer(data,train = True):<br/>    #this is to handle missing values before using the data<br/>    data['ProfileName'] = data['ProfileName'].apply(lambda x: 'Anonymous' if <br/>                                                (x == 'nan')|(x == 'NaN')|<br/>                                                (x == 'N/A')|(x == '0')|<br/>                                                (x == '')|(x == '-1')|<br/>                                                (x == 'null')|(x == 'Null')|<br/>                                                (x == 'NA')|(x == 'na')|<br/>                                                (x == 'none')|(x == 'unknown')<br/>                                                else x)<br/>    <br/>   #this is to simulate that after data train we would <br/>   #face a number of new  reviews</span><span id="e3cc" class="mb kp hh lx b fi mg md l me mf">    if train == True:<br/>        countprof = data.ProfileName.value_counts().reset_index()<br/>        countprof.columns = ['ProfileName','ProfReviewCount']<br/>        <br/>        countprod = data.ProductId.value_counts().reset_index()<br/>        countprod.columns = ['ProductId','ProdReviewCount']<br/>        <br/>        countspam = data.groupby(['ProductId',<br/>                                  'ProfileName']).size().reset_index()<br/>        countspam.columns = ['ProductId','ProfileName','SpamReviewCount']<br/>        <br/>        engineered = pd.merge(<br/>                        pd.merge(<br/>                         pd.merge(data,countprof,how='left',on='ProfileName'),<br/>                            countprod,how = 'left', on = 'ProductId'),<br/>                        countspam, how = 'left',on =['ProductId',<br/>                                                     'ProfileName'])<br/>        return engineered      <br/>    if train == False:<br/>        #this is from train<br/>        countproftrain = xtrain.ProfileName.value_counts().reset_index()<br/>        countproftrain.columns = ['ProfileName','ProfReviewCounttrain']<br/>        <br/>        countprodtrain = xtrain.ProductId.value_counts().reset_index()<br/>        countprodtrain.columns = ['ProductId','ProdReviewCounttrain']<br/>        <br/>        countspamtrain = xtrain.groupby(['ProductId',<br/>                                         'ProfileName']).size().reset_index()<br/>        countspamtrain.columns = ['ProductId',<br/>                                  'ProfileName',<br/>                                  'SpamReviewCounttrain']</span><span id="ce1b" class="mb kp hh lx b fi mg md l me mf">        #this is from the newly introduced data<br/>        countprof = data.ProfileName.value_counts().reset_index()<br/>        countprof.columns = ['ProfileName','ProfReviewCountadd']<br/>        <br/>        countprod = data.ProductId.value_counts().reset_index()<br/>        countprod.columns = ['ProductId','ProdReviewCountadd']<br/>        <br/>        countspam = data.groupby(['ProductId',<br/>                                  'ProfileName']).size().reset_index()<br/>        countspam.columns = ['ProductId','ProfileName','SpamReviewCountadd']<br/>        <br/>        <br/>        <br/>        #this is to add newly introduced data with train<br/>        #ProfileName<br/>        countproffinal = countprof.merge(countproftrain,<br/>                                         how = 'left',<br/>                                         on ='ProfileName')<br/>        countproffinal.fillna(0,inplace = True)<br/>        countproffinal['ProfReviewCount'] = countproffinal.ProfReviewCounttrain + countproffinal.ProfReviewCountadd<br/>        countproffinal.drop(['ProfReviewCounttrain',<br/>                             'ProfReviewCountadd'],<br/>                             axis = 1,inplace = True)<br/>    <br/>        #ProductId<br/>        countprodfinal = countprod.merge(countprodtrain,<br/>                                         how = 'left',<br/>                                         on = 'ProductId')<br/>        countprodfinal.fillna(0,inplace = True)<br/>        countprodfinal['ProdReviewCount'] = countprodfinal.ProdReviewCounttrain + countprodfinal.ProdReviewCountadd<br/>        countprodfinal.drop(['ProdReviewCounttrain',<br/>                             'ProdReviewCountadd'],<br/>                             axis = 1,inplace = True)<br/>        <br/>        #SpamReviewCount<br/>        countspamfinal = countspam.merge(countspamtrain,how = 'left',on = ['ProductId','ProfileName'])<br/>        countspamfinal.fillna(0,inplace = True)<br/>        countspamfinal['SpamReviewCount'] = countspamfinal.SpamReviewCounttrain + countspamfinal.SpamReviewCountadd<br/>        countspamfinal.drop(['SpamReviewCounttrain',<br/>                             'SpamReviewCountadd'],<br/>                             axis = 1,inplace = True)<br/>        <br/>        engineered = pd.merge(<br/>                        pd.merge(<br/>                            pd.merge(data,countproffinal,<br/>                                     how='left',on='ProfileName'),<br/>                                 countprodfinal,<br/>                                 how = 'left', <br/>                                 on = 'ProductId'),<br/>                             countspamfinal, <br/>                             how = 'left',<br/>                             on = ['ProductId','ProfileName'])<br/>        return engineered</span></pre><p id="0bcc" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">总结一下上面的函数，基本思想是填充ProfileName列中缺少的值，并创建新的变量，这些变量表示某个客户的评论数、某个产品收到的评论数以及同一客户对某个产品的多次评论数。</p><p id="06ee" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">创建完函数后，我们现在应用函数。</p><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="26f4" class="mb kp hh lx b fi mc md l me mf">xtrainpreprocessed = preprocess_engineer(xtrain,train = True)<br/>xtestpreprocessed = preprocess_engineer(xtest,train = False)<br/>xvalpreprocessed = preprocess_engineer(xval,train = False)</span></pre><blockquote class="kc kd ke"><p id="d726" class="jf jg kf jh b ji kg jk jl jm kh jo jp ki kj js jt kk kl jw jx km kn ka kb je ha bi translated">注意，我们没有使用tokenize。我稍后会谈到它。</p></blockquote><h2 id="d55a" class="mb kp hh bd kq mh mi mj ku mk ml mm ky jq mn mo lc ju mp mq lg jy mr ms lk mt bi translated">作为模型输入的文本数据</h2><p id="8050" class="pw-post-body-paragraph jf jg hh jh b ji lm jk jl jm ln jo jp jq lo js jt ju lp jw jx jy lq ka kb je ha bi translated">在开始建模之前，我们必须将文本数据转换成数字数据。到目前为止，我学到了两种基本的方法，第一种是<strong class="jh hi">单词包(BOW) </strong>，第二种是<strong class="jh hi">单词嵌入(Word2Vec，W2V) </strong>。我不打算深入研究这些理论和它是如何工作的，相反，我只想告诉你它是做什么的，因为这篇文章只是一个介绍。</p><ul class=""><li id="461b" class="mu mv hh jh b ji kg jm kh jq mw ju mx jy my je mz na nb nc bi translated">基本上，BOW试图将文本数据中的单词转换成数字，以表示该单词在数据中出现的频率。单词出现的频率可以由单词的计数(CountVectorizer)或单词的权重(TF-IDF)来表示。你可以试着谷歌一下，以便更好地理解它们。</li><li id="819f" class="mu mv hh jh b ji nd jm ne jq nf ju ng jy nh je mz na nb nc bi translated">Word2Vec是弓的替代品。Word2Vec，就像它的名字一样，试图将数据中的单词转换成向量。Word2Vec使用神经网络来产生代表我们原始单词的向量。有两种方法可以做到这一点，第一种是CBOW(连续单词包)和Skip-gram。</li></ul><figure class="lr ls lt lu fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ni"><img src="../Images/089817d66c8d38cc6e08aab991b3bd1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9VkLRpp-L_XQ5Gm_.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://www.researchgate.net/figure/CBOW-and-Skip-gram-models-architecture-6_fig1_332543231" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/figure/CBOW-and-Skip-gram-models-architecture-6 _ fig 1 _ 332543231</a></figcaption></figure><p id="e055" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">比方说我们有这样一句话:</p><blockquote class="iu"><p id="6e95" class="iv iw hh bd ix iy nj nk nl nm nn je dx translated">我要去购物</p></blockquote><p id="cb21" class="pw-post-body-paragraph jf jg hh jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb je ha bi translated">简单解释一下这个，<br/> CBOW试图通过查看周围的其他单词来预测一个单词(例如:output = " gonna "，input = ["I'm "，" go "，" shopping"])。<br/> Skip-gram试图预测某个单词周围的其他单词(例如:input = " gonna "，output = ["I'm "，" go "，" shopping"])</p><p id="be06" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">我们将使用Word2Vec将文本数据转换成数字数据，因为单词袋方法的一个缺点是它经常遇到维数问题。</p><figure class="lr ls lt lu fd ii er es paragraph-image"><div class="er es no"><img src="../Images/3f277e48867429e459af3bda378f4adb.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*t_y37ITHG-1UG2GXJcXw5w.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">虚拟数据框架(带有计数矢量器的单词包)</figcaption></figure><p id="0bbd" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">想象我们有一个如上的数据帧。数据中存在的字数成为数据框架中的列数。上面的数据框架假设您使用CountVectorizer作为您的单词包方法，因此您可以看到，对于数据的第一行(第一个文本数据)，有3个“get”单词、1个“this”单词、4个“out”单词和3个“can”单词。现在，你可以想象如果我们使用单词袋方法，输入会有多大。使用Word2Vec，我们可以通过计算文本数据中单词向量的平均值来处理这个问题。</p><figure class="lr ls lt lu fd ii er es paragraph-image"><div class="er es no"><img src="../Images/c00a2e3c47a7e0ec60d269a9a7bc3d91.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*ZmE7uxX0NQaCH-cfxmCLqg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">虚拟数据帧(用Word2Vec嵌入单词)</figcaption></figure><p id="3848" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">上面假想的数据帧显示了Word2Vec的输出。零向量意味着该单词没有出现在文本数据上(例如，第三个文本数据没有“this”和“can”)。通常对每个数据点的向量求平均值。平均后，列数不再等于字数。相反，列的数量将等于向量的大小。</p><h2 id="38cd" class="mb kp hh bd kq mh mi mj ku mk ml mm ky jq mn mo lc ju mp mq lg jy mr ms lk mt bi translated">将文本转换为数字</h2><p id="a076" class="pw-post-body-paragraph jf jg hh jh b ji lm jk jl jm ln jo jp jq lo js jt ju lp jw jx jy lq ka kb je ha bi translated">理解了这些简短的概念，我们现在将一次编写几个代码。</p><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="d6cd" class="mb kp hh lx b fi mc md l me mf">#function to average<br/>def AverageVectors(wordvectmod,oritokenizedtxt):<br/>    todict = dict(zip(wordvectmod.wv.index_to_key,<br/>                      wordvectmod.wv.vectors))<br/>    #if a text is empty we should return a vector of zeros<br/>    #with the same dimensionality as all the other vectors<br/>    dim = len(next(iter(todict.values())))<br/>    return np.array([np.mean([todict[w] for w in words if w in todict]<br/>                              or <br/>                             [np.zeros(dim)],axis = 0)<br/>                     for words in oritokenizedtxt<br/>                    ])</span><span id="f858" class="mb kp hh lx b fi mg md l me mf">w2v = Word2Vec(xtrainpreprocessed.Text.apply(tokenize),<br/>                 sg = 0,<br/>                 hs = 1,<br/>                 seed = 42,<br/>                 vector_size = 128)<br/>#convert to vect average<br/>xtrainvect = AverageVectors(w2v,xtrainpreprocessed.Text.apply(tokenize))<br/>xtestvect = AverageVectors(w2v,xtestpreprocessed.Text.apply(tokenize))<br/>xvalvect = AverageVectors(w2v,xvalpreprocessed.Text.apply(tokenize))</span></pre><p id="9ed1" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">我在上面的代码中所做的是创建一个函数，该函数将接受我们创建的Word2Vec模型，然后对每个数据点的单词向量进行平均。之后，我们将函数应用于我们拥有的文本数据。<br/>在对Word2Vec的向量进行平均后，我们将它们转换为dataframe，然后添加其他可能有助于检测客户情绪的特征。</p><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="745c" class="mb kp hh lx b fi mc md l me mf">#add the remaining features<br/>def vecttodata(aftervec,beforevec):<br/>    df = pd.DataFrame(aftervec)<br/>    df['HelpfulnessNumerator'] = beforevec.reset_index(drop=True)['HelpfulnessNumerator']<br/>    df['HelpfulnessDenominator'] = beforevec.reset_index(drop=True)['HelpfulnessNumerator']<br/>    df['Time'] = beforevec.reset_index(drop=True)['Time']<br/>    df['ProfReviewCount'] = beforevec.reset_index(drop=True)['ProfReviewCount']<br/>    df['ProdReviewCount'] = beforevec.reset_index(drop=True)['ProdReviewCount']<br/>    df['SpamReviewCount'] = beforevec.reset_index(drop=True)['SpamReviewCount'] <br/>    <br/>    return df<br/>    <br/>xtraindf = vecttodata(xtrainvect,xtrainpreprocessed)<br/>xtestdf = vecttodata(xtestvect,xtestpreprocessed)<br/>xvaldf = vecttodata(xvalvect,xvalpreprocessed)</span></pre><h1 id="ef31" class="ko kp hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">训练分类模型</h1><blockquote class="kc kd ke"><p id="0794" class="jf jg kf jh b ji kg jk jl jm kh jo jp ki kj js jt kk kl jw jx km kn ka kb je ha bi translated">借此机会，我想使用XGBoost分类器来构建分类模型。培训时间比较长，大概花了一个小时才把培训流程做完。存在同质性(每个属性/特征具有彼此相似的性质，如文本、图像、视频等。等等)和异构(每个属性/特征对于一个和另一个具有不同的性质，就像年龄与收入、混合数据集等。etc)数据。我尝试使用XGBoost，因为它适用于异构数据，但是如果读者想尝试仅使用文本数据作为输入来构建分类模型，我推荐使用神经网络，因为它可能快得多，并且更适合同构数据集。</p></blockquote><p id="fa4d" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">在前面的部分中，您还可以看到正面情绪和负面情绪的数量不平衡，因此为了处理这种不平衡，我们可以执行数据上采样或计算训练数据的样本权重。我选择后者。以下是我编写的用于构建和训练情感分类模型的代码。</p><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="b069" class="mb kp hh lx b fi mc md l me mf">rom xgboost import XGBClassifier<br/>from sklearn.utils.class_weight import compute_sample_weight</span><span id="dce3" class="mb kp hh lx b fi mg md l me mf">sample_weights = compute_sample_weight(<br/>    class_weight='balanced',<br/>    y=ytrain<br/>)</span><span id="6568" class="mb kp hh lx b fi mg md l me mf">xgb = XGBClassifier(booster = 'gbtree',<br/>                    objective = 'binary:logistic',<br/>                    eval_metric='auc',<br/>                    seed = 42, use_label_encoder = False,<br/>                    num_parallel_tree = 10,<br/>                    n_estimators = 50,<br/>                    verbosity = 2)<br/>xgb.fit(xtraindf,ytrain,sample_weight = sample_weights)<br/>pred = xgb.predict(xtestdf)<br/>from sklearn.metrics import confusion_matrix,classification_report<br/>print(classification_report(pred,ytest, target_names = ['Negative','Positive']))</span></pre><figure class="lr ls lt lu fd ii er es paragraph-image"><div class="er es np"><img src="../Images/eccd594d681e8162e8acf3ac50663354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*vxuW6T8DWGvt8DxiGOJw_A.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">测试数据分类报告</figcaption></figure><h1 id="9d82" class="ko kp hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">最终检验和结论</h1><p id="a459" class="pw-post-body-paragraph jf jg hh jh b ji lm jk jl jm ln jo jp jq lo js jt ju lp jw jx jy lq ka kb je ha bi translated">我们可以说，该模型在测试数据上表现良好，但我们可以通过执行超参数调整、设置概率截止值等来进一步改善它。我们对验证数据运行该模型，以再次测试它在未知数据上的表现。</p><figure class="lr ls lt lu fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nq"><img src="../Images/6e53fc545eddf16628b9e5a5fc9b83f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sBIpADEjKK9K4HSt0JWRWg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">验证数据分类报告</figcaption></figure><p id="4b44" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">您可以看到分类报告与测试数据的分类报告没有任何不同。</p><p id="d66f" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">这篇文章虽然远非完美，但它旨在给出一个粗略的分步过程，说明当我们想要操作文本数据时应该做什么。有很多文本操作方法，我自己还没有完全理解，也没有尝试去学习，但是我希望我可以与其他NLP初学者分享这些小知识。像往常一样，我希望你们能在评论区给我留下任何关于我的方法的建议。如果你们觉得这篇文章有用，请鼓掌。:)</p><p id="2b61" class="pw-post-body-paragraph jf jg hh jh b ji kg jk jl jm kh jo jp jq kj js jt ju kl jw jx jy kn ka kb je ha bi translated">下次见！</p></div></div>    
</body>
</html>