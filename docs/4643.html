<html>
<head>
<title>“All you need is Attention,” Part I But Why?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“你所需要的只是关注，”第一部分但是为什么？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/why-do-we-need-attention-42bb99fe938d?source=collection_archive---------1-----------------------#2021-12-09">https://medium.com/analytics-vidhya/why-do-we-need-attention-42bb99fe938d?source=collection_archive---------1-----------------------#2021-12-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/e1a489175ae897c5c74f55657c90630e.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*f12NPVbsg0EJG7A58nJWNg.png"/></div></figure><p id="9583" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">尽管这个标题看起来令人困惑，但是如果我们想要理解这个标题的意思，我们需要做一些挖掘工作。系好安全带，这是时光倒流。</p><p id="7413" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">当谷歌在2014年推出Seq2Seq模型时，就像为NLP找到了圣杯。这种基于深度学习的简单编码器-解码器模型，在最初引入时，主要用于机器翻译。简而言之，序列到序列学习是关于在机器翻译上下文中将来自一个领域(例如英语句子)的序列转换成另一个领域(例如翻译成法语的相同句子)的序列的训练模型。渐渐地，它成为所有以文本生成为主要目标的NLP任务中最受欢迎的架构设置。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es jj"><img src="../Images/f7de8a42c8965060d7ab3e87406e4966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SbPNtHAZeEyu7Vpu6je5xQ.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">编码器/解码器模型的典型描述及相关瓶颈</figcaption></figure><p id="164a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">递归神经网络(RNN)具有输入之间依赖的特性，无疑成为seq2seq模型采用的goto技术。但是通常的训练障碍仍然产生了固有的问题<a class="ae jw" href="https://www.superdatascience.com/blogs/recurrent-neural-networks-rnn-the-vanishing-gradient-problem" rel="noopener ugc nofollow" target="_blank">消失梯度</a>。一如既往的LSTM和GRU，RNN最成功的扩展原来是这里的弥赛亚。LSTMs具有保留句子中最相关信息的能力，证明了它在生成输出文本序列时提供了上下文信息考虑。因此，seq2seq凭借架构和算法的正确结合而大受欢迎。或者是？</p><p id="8180" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Seq2seq模型一次处理一步，它收集输入并通过编码器、隐藏状态和解码器向前移动。但是它收集的输入是固定长度的。因此，每当输入超过这个固定长度的内存，隐藏的状态就会在它们与数据一起到达解码器的途中遇到瓶颈。在这个问题上，模型开始给予序列中后面的单词更多的相关性，这导致输出文本片段的上下文生成。因此，对于普通seq2seq来说，较长的序列变得难以处理。因此，为了解决这个困难，引入了“每个单词一个向量”而不是一个向量句子的概念。这个想法有助于克服固定长度输入的缺点。但是隐藏状态仍然会因为处理所有这些向量的额外责任而被抑制，从而导致内存耗尽。</p><p id="dfef" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在我们有两个主要的挑战要克服，记忆问题和较长序列的上下文信息掌握。语境对预测提出了新的要求，模型应该考虑句子的对齐。现在，我们有了所有这些代表序列中每个单词的向量，所以我们必须以某种方式找到上下文，避免处理整个向量集。这正是注意力的作用。在注意力的帮助下，我们能够研究在新产出预测中起作用的特定单词。最后，选择的单词有助于理解上下文，记忆问题只需要处理这些单词向量就可以解决。</p><p id="f297" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在我们知道了为什么我们需要注意力，是时候解释一下注意力是如何工作的了。请继续关注第二部分。</p></div></div>    
</body>
</html>