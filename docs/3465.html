<html>
<head>
<title>“ARTGAN” — A Simple Generative Adversarial Networks Based On Art Images Using DeepLearning &amp; Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“Art gan”——使用深度学习和Pytorch的基于艺术图像的简单生成对抗网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/artgan-a-b77ecb1bc25a?source=collection_archive---------1-----------------------#2021-07-02">https://medium.com/analytics-vidhya/artgan-a-b77ecb1bc25a?source=collection_archive---------1-----------------------#2021-07-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/b66ffb24262bec0381e0f4a4bdbaf7f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*zeFgL4qLzvrdl9BE8PP8cA.png"/></div></figure><div class=""/><blockquote class="im in io"><p id="1421" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">生成对抗网络，简称GANs，是一种使用深度学习方法(如卷积神经网络)进行生成建模的方法。</p></blockquote><h1 id="5b35" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">简介:</h1><p id="3eb8" class="pw-post-body-paragraph ip iq hp is b it km iv iw ix kn iz ja ko kp jd je kq kr jh ji ks kt jl jm jn hb bi translated">这个关于“ARTGAN”的项目是一个简单的生成性对抗网络——基于艺术图像使用<a class="ae ku" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度学习</a> &amp; <a class="ae ku" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>。这里我们使用<a class="ae ku" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"> matplotlib </a>，<a class="ae ku" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>来实现我们的项目。</p><p id="5c79" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">生成对抗网络，简称GANs，是一种使用深度学习方法(如卷积神经网络)进行生成建模的方法。这种技术学习生成具有与给定训练集的训练集相同的统计数据的新数据。</p><p id="8159" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">生成建模是机器学习中的一项无监督学习任务，它涉及自动发现和学习输入数据中的规律或模式，使得模型可以用于生成或输出新的示例，这些示例很可能是从原始数据集中提取的。</p><p id="57aa" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">如果有人让你想象一个人，我们的想象力会把我们引向什么？我们在头脑中创造了一个人吗？嗯，不。人类的大脑总是倾向于想到他们已经认识的人，所以我们会想到我们亲近的人或我们最后一次遇见的人，甚至可能是我们遇见的某个随机的人，但我们无法想到我们的眼睛从未捕捉到图像的人。这个任务官方可以用这个技术来完成，GAN。为了更好的理解，让我们举个例子。</p><p id="0fb8" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">假设我们有一个人的随机照片，现在这张照片可以很容易地使用GAN来创建逼真的假人类照片。这些照片看起来是真实的，而且是完全不同的人，但只是对另一张照片的处理。</p><p id="3490" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">GANs是一种训练生成模型的聪明方法，它通过将问题框架化为具有两个子模型的监督学习问题，第一个是生成器模型，通过它我们可以训练生成新的示例，第二个是鉴别器模型，通过它我们可以将示例分类为真实的(来自领域)或虚假的(生成的)。这两个模型在一个零和游戏中一起训练，对抗，直到鉴别器模型被愚弄了大约一半的时间，这意味着生成器模型正在生成似是而非的例子。</p><p id="607d" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">GANs是一个令人兴奋且快速变化的领域，它实现了生成模型的承诺，能够在一系列问题领域生成逼真的示例，最显著的是在图像到图像的翻译任务中，例如将夏天的照片翻译成冬天的照片或白天到夜晚的照片，创建不同场景的幻觉，以及生成甚至人类都无法辨别的物体、场景和人物的真实感照片。</p><p id="33c1" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">在这里，我的整个项目是操纵图纸。</p><h1 id="ea32" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">数据集:</h1><p id="ed9f" class="pw-post-body-paragraph ip iq hp is b it km iv iw ix kn iz ja ko kp jd je kq kr jh ji ks kt jl jm jn hb bi translated">我们可以使用opendatasets库从Kaggle下载数据集。opendatasets使用Kaggle官方API从Kaggle下载数据集。按照下面提到的步骤查找您的API凭证:</p><p id="67c3" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">1.登录<a class="ae ku" href="https://kaggle.com/," rel="noopener ugc nofollow" target="_blank">https://kaggle.com/,</a>，然后点击右上角的个人资料图片，从菜单中选择“我的账户”。</p><p id="64be" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">2.向下滚动到“API”部分，然后单击“创建新的API令牌”。这将下载一个包含以下内容的文件kaggle.json:</p><p id="081c" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">{"username ":"您的_KAGGLE_USERNAME "，" key ":"您的_KAGGLE_KEY"}</p><p id="31ac" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">3.当您运行opendatsets.download时，会要求您输入您的用户名&amp; <a class="ae ku" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggle </a> API，您可以从步骤2中下载的文件中获得该用户名。注意，您只需要下载kaggle.json文件一次。在Google Colab上，还可以使用files选项卡上传kaggle.json文件，所需的凭证会被自动读取。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="5e62" class="le jp hp la b fi lf lg l lh li">!pip install opendatasets --upgrade --quiet</span><span id="e9f2" class="le jp hp la b fi lj lg l lh li">import opendatasets as od</span><span id="d9c7" class="le jp hp la b fi lj lg l lh li">dataset_url = 'https://www.kaggle.com/ikarus777/best-artworks-of-all-time'</span><span id="486b" class="le jp hp la b fi lj lg l lh li">od.download(dataset_url)</span></pre><h1 id="7e34" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">将数据集导入PyTorch:</h1><h2 id="1811" class="le jp hp bd jq lk ll lm ju ln lo lp jy ko lq lr kc kq ls lt kg ks lu lv kk lw bi translated">将数据集导入PyTorch</h2><p id="0ffc" class="pw-post-body-paragraph ip iq hp is b it km iv iw ix kn iz ja ko kp jd je kq kr jh ji ks kt jl jm jn hb bi translated">该数据集由两个文件夹组成:</p><p id="ac0e" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">图像:该文件夹包含艺术家文件夹中的所有图像文件。</p><p id="c65f" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">调整大小:该文件夹包含一个文件夹中的所有图像文件。</p><p id="572b" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">由于GAN将数据分类为真实或虚假，因此不需要艺术家类别，所以我只使用了调整过大小的文件夹中的文件。使用torchvision的ImageFolder类。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="13f5" class="le jp hp la b fi lf lg l lh li">data_dir = '/content/best-artworks-of-all-time/resized/'</span><span id="7d8d" class="le jp hp la b fi lj lg l lh li">import os</span><span id="b21d" class="le jp hp la b fi lj lg l lh li">for cls in os.listdir(data_dir):</span><span id="e32b" class="le jp hp la b fi lj lg l lh li">print(cls, ':', len(os.listdir(data_dir + '/' + cls)))</span><span id="748b" class="le jp hp la b fi lj lg l lh li">from torchvision.datasets import ImageFolder</span><span id="e1fd" class="le jp hp la b fi lj lg l lh li">dataset = ImageFolder(data_dir)</span><span id="ba76" class="le jp hp la b fi lj lg l lh li">len(dataset)</span></pre><h1 id="ff00" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">让我们来看看数据集中的一些艺术图片:</h1><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="2d0f" class="le jp hp la b fi lf lg l lh li">import matplotlib.pyplot as plt</span><span id="be1c" class="le jp hp la b fi lj lg l lh li">%matplotlib inline</span><span id="445b" class="le jp hp la b fi lj lg l lh li">img, label = dataset[0]</span><span id="9fbb" class="le jp hp la b fi lj lg l lh li">plt.imshow(img)</span><span id="4e7c" class="le jp hp la b fi lj lg l lh li">img, label = dataset[500]</span><span id="1433" class="le jp hp la b fi lj lg l lh li">plt.imshow(img)</span><span id="4b2d" class="le jp hp la b fi lj lg l lh li">img, label = dataset[741]</span><span id="b803" class="le jp hp la b fi lj lg l lh li">plt.imshow(img)</span></pre><figure class="kv kw kx ky fd hk er es paragraph-image"><div class="er es lx"><img src="../Images/c0852d380ce6a8ad8f80e06e6889249c.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*_GCROft6s6j3OnnqqgHHrQ.png"/></div></figure><figure class="kv kw kx ky fd hk er es paragraph-image"><div class="er es ly"><img src="../Images/5c2e0bc0228d35986cace580f27c6941.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*rRu8zU8EbtX8nSWxxOIaxQ.png"/></div></figure><figure class="kv kw kx ky fd hk er es paragraph-image"><div class="er es lz"><img src="../Images/a773579f07e34b9f5cad2bd927fd3ecd.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*UDC6pjMLEN8_Y7i84nd61Q.png"/></div></figure><h1 id="598b" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">制作批量图片:</h1><p id="d4b5" class="pw-post-body-paragraph ip iq hp is b it km iv iw ix kn iz ja ko kp jd je kq kr jh ji ks kt jl jm jn hb bi translated">1.首先，我将调整所有图像的大小并居中裁剪，以确保它们都具有相同的形状和大小。</p><p id="648c" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">2.然后转换成张量，归一化。</p><p id="1562" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">3.然后创建数据加载器。</p><p id="50ae" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">4.然后看一些样品。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="3436" class="le jp hp la b fi lf lg l lh li">import torchvision.transforms as tt</span><span id="4eb1" class="le jp hp la b fi lj lg l lh li">from torch.utils.data import DataLoader</span><span id="4364" class="le jp hp la b fi lj lg l lh li">image_size = 64</span><span id="bab7" class="le jp hp la b fi lj lg l lh li">batch_size = 128</span><span id="3f57" class="le jp hp la b fi lj lg l lh li">stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)</span><span id="ca47" class="le jp hp la b fi lj lg l lh li">train_ds = ImageFolder(data_dir, transform=tt.Compose([</span><span id="3081" class="le jp hp la b fi lj lg l lh li">tt.Resize(image_size),</span><span id="2554" class="le jp hp la b fi lj lg l lh li">tt.CenterCrop(image_size),</span><span id="6cb6" class="le jp hp la b fi lj lg l lh li">tt.ToTensor(),</span><span id="d350" class="le jp hp la b fi lj lg l lh li">tt.Normalize(*stats)]))</span><span id="7641" class="le jp hp la b fi lj lg l lh li">train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)</span><span id="9f19" class="le jp hp la b fi lj lg l lh li">from torchvision.utils import make_grid</span><span id="c723" class="le jp hp la b fi lj lg l lh li">def denorm(img_tensors):</span><span id="61d1" class="le jp hp la b fi lj lg l lh li">return img_tensors * stats[1][0] + stats[0][0]</span><span id="d327" class="le jp hp la b fi lj lg l lh li">def show_images(images, nmax=64):</span><span id="0d8b" class="le jp hp la b fi lj lg l lh li">fig, ax = plt.subplots(figsize=(8, 8))</span><span id="be1e" class="le jp hp la b fi lj lg l lh li">ax.set_xticks([]); ax.set_yticks([])</span><span id="0cf4" class="le jp hp la b fi lj lg l lh li">ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))</span><span id="79a0" class="le jp hp la b fi lj lg l lh li">def show_batch(dl, nmax=64):<br/>  for images, _ in dl:<br/>    show_images(images, nmax)<br/>    break</span><span id="1044" class="le jp hp la b fi lj lg l lh li">show_batch(train_dl)</span></pre><figure class="kv kw kx ky fd hk er es paragraph-image"><div class="er es hg"><img src="../Images/b66ffb24262bec0381e0f4a4bdbaf7f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*zeFgL4qLzvrdl9BE8PP8cA.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">一批图像</figcaption></figure><h1 id="da13" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">使用GPU:</h1><p id="9e96" class="pw-post-body-paragraph ip iq hp is b it km iv iw ix kn iz ja ko kp jd je kq kr jh ji ks kt jl jm jn hb bi translated">为了无缝地使用GPU(如果有可用的话)，我们定义了两个助手函数(get_default_device &amp; to_device)和一个助手类“DeviceDataLoader ”,以便将模型和数据移动到GPU(如果有可用的话)。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="7aae" class="le jp hp la b fi lf lg l lh li">def get_default_device():</span><span id="6d62" class="le jp hp la b fi lj lg l lh li">"""Pick GPU if available, else CPU"""</span><span id="3383" class="le jp hp la b fi lj lg l lh li">if torch.cuda.is_available():</span><span id="76f0" class="le jp hp la b fi lj lg l lh li">return torch.device('cuda')</span><span id="d183" class="le jp hp la b fi lj lg l lh li">else:</span><span id="6b11" class="le jp hp la b fi lj lg l lh li">return torch.device('cpu')</span><span id="8c8f" class="le jp hp la b fi lj lg l lh li">def to_device(data, device):</span><span id="57d2" class="le jp hp la b fi lj lg l lh li">"""Move tensor(s) to chosen device"""</span><span id="bfaf" class="le jp hp la b fi lj lg l lh li">if isinstance(data, (list,tuple)):</span><span id="b7ee" class="le jp hp la b fi lj lg l lh li">return [to_device(x, device) for x in data]</span><span id="9df8" class="le jp hp la b fi lj lg l lh li">return data.to(device, non_blocking=True)</span><span id="edf1" class="le jp hp la b fi lj lg l lh li">class DeviceDataLoader():</span><span id="060d" class="le jp hp la b fi lj lg l lh li">"""Wrap a dataloader to move data to a device"""</span><span id="631c" class="le jp hp la b fi lj lg l lh li">def __init__(self, dl, device):</span><span id="b747" class="le jp hp la b fi lj lg l lh li">self.dl = dl</span><span id="0b3c" class="le jp hp la b fi lj lg l lh li">self.device = device</span><span id="be9a" class="le jp hp la b fi lj lg l lh li">def __iter__(self):</span><span id="08b3" class="le jp hp la b fi lj lg l lh li">"""Yield a batch of data after moving it to device"""</span><span id="b049" class="le jp hp la b fi lj lg l lh li">for b in self.dl:</span><span id="f3ec" class="le jp hp la b fi lj lg l lh li">yield to_device(b, self.device)</span><span id="9d9b" class="le jp hp la b fi lj lg l lh li">def __len__(self):</span><span id="2de5" class="le jp hp la b fi lj lg l lh li">"""Number of batches"""</span><span id="447f" class="le jp hp la b fi lj lg l lh li">return len(self.dl)</span><span id="5032" class="le jp hp la b fi lj lg l lh li">import torch</span><span id="1d72" class="le jp hp la b fi lj lg l lh li">device = get_default_device()</span><span id="80af" class="le jp hp la b fi lj lg l lh li">device</span><span id="881c" class="le jp hp la b fi lj lg l lh li">train_dl = DeviceDataLoader(train_dl, device)</span></pre><h1 id="4e17" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">鉴频器网络:</strong></h1><p id="8f85" class="pw-post-body-paragraph ip iq hp is b it km iv iw ix kn iz ja ko kp jd je kq kr jh ji ks kt jl jm jn hb bi translated">鉴别器网络顾名思义，它鉴别被操纵的图片，将图像作为输入，并试图将它们分类为“真实的”或“生成的”。在这个意义上，它就像任何其他神经网络一样。我们将使用卷积神经网络(CNN ),它为每幅图像输出一个数字。此外，我们将使用步长2来逐渐减小输出要素地图的大小。因此，给我们的项目一个更清晰的视图。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="bbbd" class="le jp hp la b fi lf lg l lh li">import torch.nn as nn</span><span id="ad63" class="le jp hp la b fi lj lg l lh li">discriminator = nn.Sequential(</span><span id="f36d" class="le jp hp la b fi lj lg l lh li"># in: 3 x 64 x 64</span><span id="f849" class="le jp hp la b fi lj lg l lh li">nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),</span><span id="1c58" class="le jp hp la b fi lj lg l lh li">nn.BatchNorm2d(64),</span><span id="1f0a" class="le jp hp la b fi lj lg l lh li">nn.LeakyReLU(0.2, inplace=True),</span><span id="67cc" class="le jp hp la b fi lj lg l lh li"># out: 64 x 32 x 32</span><span id="273f" class="le jp hp la b fi lj lg l lh li">nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),</span><span id="f917" class="le jp hp la b fi lj lg l lh li">nn.BatchNorm2d(128),</span><span id="512f" class="le jp hp la b fi lj lg l lh li">nn.LeakyReLU(0.2, inplace=True),</span><span id="a05a" class="le jp hp la b fi lj lg l lh li"># out: 128 x 16 x 16</span><span id="b5ae" class="le jp hp la b fi lj lg l lh li">nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),</span><span id="d410" class="le jp hp la b fi lj lg l lh li">nn.BatchNorm2d(256),</span><span id="9947" class="le jp hp la b fi lj lg l lh li">nn.LeakyReLU(0.2, inplace=True),</span><span id="9c91" class="le jp hp la b fi lj lg l lh li"># out: 256 x 8 x 8</span><span id="dfcb" class="le jp hp la b fi lj lg l lh li">nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),</span><span id="be06" class="le jp hp la b fi lj lg l lh li">nn.BatchNorm2d(512),</span><span id="e536" class="le jp hp la b fi lj lg l lh li">nn.LeakyReLU(0.2, inplace=True),</span><span id="c36b" class="le jp hp la b fi lj lg l lh li"># out: 512 x 4 x 4</span><span id="b85d" class="le jp hp la b fi lj lg l lh li">nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),</span><span id="9d31" class="le jp hp la b fi lj lg l lh li"># out: 1 x 1 x 1</span><span id="c794" class="le jp hp la b fi lj lg l lh li">nn.Flatten(),</span><span id="1260" class="le jp hp la b fi lj lg l lh li">nn.Sigmoid())</span><span id="86a9" class="le jp hp la b fi lj lg l lh li">discriminator = to_device(discriminator, device)</span></pre><h1 id="89fa" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">发电机网络:</h1><p id="e867" class="pw-post-body-paragraph ip iq hp is b it km iv iw ix kn iz ja ko kp jd je kq kr jh ji ks kt jl jm jn hb bi translated">生成器的输入通常是向量或随机数矩阵(称为潜在张量)，用作生成图像的种子。生成器会将形状为(128，1，1)的潜在张量转换为形状为3 x 28 x 28的图像张量。为了实现这一点，我们将使用PyTorch中的ConvTranspose2d图层，该图层被执行为转置卷积(也称为反卷积)。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="671c" class="le jp hp la b fi lf lg l lh li">latent_size = 128</span><span id="24eb" class="le jp hp la b fi lj lg l lh li">generator = nn.Sequential(</span><span id="3f19" class="le jp hp la b fi lj lg l lh li"># in: latent_size x 1 x 1</span><span id="dbf8" class="le jp hp la b fi lj lg l lh li">nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),</span><span id="2664" class="le jp hp la b fi lj lg l lh li">nn.BatchNorm2d(512),</span><span id="1809" class="le jp hp la b fi lj lg l lh li">nn.ReLU(True),</span><span id="5f97" class="le jp hp la b fi lj lg l lh li"># out: 512 x 4 x 4</span><span id="e2db" class="le jp hp la b fi lj lg l lh li">nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),</span><span id="b647" class="le jp hp la b fi lj lg l lh li">nn.BatchNorm2d(256),</span><span id="12f6" class="le jp hp la b fi lj lg l lh li">nn.ReLU(True),</span><span id="a594" class="le jp hp la b fi lj lg l lh li"># out: 256 x 8 x 8</span><span id="170b" class="le jp hp la b fi lj lg l lh li">nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),</span><span id="439f" class="le jp hp la b fi lj lg l lh li">nn.BatchNorm2d(128),</span><span id="c29c" class="le jp hp la b fi lj lg l lh li">nn.ReLU(True),</span><span id="7db9" class="le jp hp la b fi lj lg l lh li"># out: 128 x 16 x 16</span><span id="b28d" class="le jp hp la b fi lj lg l lh li">nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),</span><span id="5b95" class="le jp hp la b fi lj lg l lh li">nn.BatchNorm2d(64),</span><span id="02a9" class="le jp hp la b fi lj lg l lh li">nn.ReLU(True),</span><span id="f707" class="le jp hp la b fi lj lg l lh li"># out: 64 x 32 x 32</span><span id="e3f7" class="le jp hp la b fi lj lg l lh li">nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),</span><span id="849c" class="le jp hp la b fi lj lg l lh li">nn.Tanh()</span><span id="5e84" class="le jp hp la b fi lj lg l lh li"># out: 3 x 64 x 64</span><span id="3b84" class="le jp hp la b fi lj lg l lh li">)</span></pre><h1 id="ff6a" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">让我们看看我们的第一个创成式图像是什么样子的:</h1><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="a02a" class="le jp hp la b fi lf lg l lh li">xb = torch.randn(batch_size, latent_size, 1, 1) # random latent tensors</span><span id="2ea1" class="le jp hp la b fi lj lg l lh li">fake_images = generator(xb)</span><span id="4303" class="le jp hp la b fi lj lg l lh li">print(fake_images.shape)</span><span id="b188" class="le jp hp la b fi lj lg l lh li">show_images(fake_images)</span></pre><figure class="kv kw kx ky fd hk er es paragraph-image"><div class="er es hg"><img src="../Images/89f909e8ee72398d68521da80f9851e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*KtN6KHxQK7sFwcJFyAyt4A.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">第一张创成式图片</figcaption></figure><p id="13e5" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">好像真的很穷。因此，现在我们必须训练我们的模型。</p><h1 id="35a8" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">甄别培训:</h1><p id="cfa4" class="pw-post-body-paragraph ip iq hp is b it km iv iw ix kn iz ja ko kp jd je kq kr jh ji ks kt jl jm jn hb bi translated">由于鉴别器是二进制分类模型，我们可以使用二进制交叉熵损失函数来量化它在真实图像和生成图像之间的区分程度。</p><h2 id="5ed4" class="le jp hp bd jq lk ll lm ju ln lo lp jy ko lq lr kc kq ls lt kg ks lu lv kk lw bi translated">以下是训练鉴别器的步骤</h2><p id="42d1" class="pw-post-body-paragraph ip iq hp is b it km iv iw ix kn iz ja ko kp jd je kq kr jh ji ks kt jl jm jn hb bi translated">1.如果图像是从真实的MNIST数据集中选取的，我们希望鉴别器输出1，如果是使用生成器网络生成的，则输出0。</p><p id="66d8" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">2.我们首先传递一批真实图像，并计算损失，将目标标签设置为1。</p><p id="87db" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">3.然后，我们将一批假图像(使用生成器生成)传递给鉴别器，并计算损失，将目标标签设置为0。</p><p id="5219" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">4.最后，我们将两个损失相加，并使用总损失来执行梯度下降，以调整鉴别器的权重。</p><p id="3f49" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">值得注意的是，在训练鉴别器时，我们不改变生成器模型的权重(opt_d只影响鉴别器. parameters())</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="e1e6" class="le jp hp la b fi lf lg l lh li">def train_discriminator(real_images, opt_d):</span><span id="3f1a" class="le jp hp la b fi lj lg l lh li"># Clear discriminator gradients</span><span id="547e" class="le jp hp la b fi lj lg l lh li">opt_d.zero_grad()</span><span id="f224" class="le jp hp la b fi lj lg l lh li"># Pass real images through discriminator</span><span id="a8fd" class="le jp hp la b fi lj lg l lh li">real_preds = discriminator(real_images)</span><span id="4833" class="le jp hp la b fi lj lg l lh li">real_targets = torch.ones(real_images.size(0), 1, device=device)</span><span id="0659" class="le jp hp la b fi lj lg l lh li">real_loss = F.binary_cross_entropy(real_preds, real_targets)</span><span id="642f" class="le jp hp la b fi lj lg l lh li">real_score = torch.mean(real_preds).item()</span><span id="fe4a" class="le jp hp la b fi lj lg l lh li"># Generate fake images</span><span id="322d" class="le jp hp la b fi lj lg l lh li">latent = torch.randn(batch_size, latent_size, 1, 1, device=device)</span><span id="365b" class="le jp hp la b fi lj lg l lh li">fake_images = generator(latent)</span><span id="1965" class="le jp hp la b fi lj lg l lh li"># Pass fake images through discriminator</span><span id="30c9" class="le jp hp la b fi lj lg l lh li">fake_targets = torch.zeros(fake_images.size(0), 1, device=device)</span><span id="1c69" class="le jp hp la b fi lj lg l lh li">fake_preds = discriminator(fake_images)</span><span id="b4e0" class="le jp hp la b fi lj lg l lh li">fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)</span><span id="c6b0" class="le jp hp la b fi lj lg l lh li">fake_score = torch.mean(fake_preds).item()</span><span id="ab7e" class="le jp hp la b fi lj lg l lh li"># Update discriminator weights</span><span id="3558" class="le jp hp la b fi lj lg l lh li">loss = real_loss + fake_loss</span><span id="643b" class="le jp hp la b fi lj lg l lh li">loss.backward()</span><span id="2be3" class="le jp hp la b fi lj lg l lh li">opt_d.step()</span><span id="082c" class="le jp hp la b fi lj lg l lh li">return loss.item(), real_score, fake_score</span></pre><h1 id="43d3" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">发电机培训:</h1><p id="6f06" class="pw-post-body-paragraph ip iq hp is b it km iv iw ix kn iz ja ko kp jd je kq kr jh ji ks kt jl jm jn hb bi translated">由于生成器的输出是图像，我们如何训练生成器并不明显。这就是我们使用一个相当优雅的技巧的地方，即使用鉴别器作为损失函数的一部分。它是这样工作的:</p><p id="9e61" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">1.我们使用生成器生成一批图像，并将其传递给鉴别器。</p><p id="e47e" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">2.我们通过将目标标签设置为1(即实数)来计算损失。我们这样做是因为生成器的目的是“愚弄”鉴别器。</p><p id="3bf8" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">3.我们使用损失来执行梯度下降，即改变生成器的权重，因此它在生成类似真实的图像方面变得更好，以“愚弄”鉴别器。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="0c62" class="le jp hp la b fi lf lg l lh li">def train_generator(opt_g):</span><span id="45d4" class="le jp hp la b fi lj lg l lh li"># Clear generator gradients</span><span id="c25d" class="le jp hp la b fi lj lg l lh li">opt_g.zero_grad()</span><span id="eba7" class="le jp hp la b fi lj lg l lh li"># Generate fake images</span><span id="014e" class="le jp hp la b fi lj lg l lh li">latent = torch.randn(batch_size, latent_size, 1, 1, device=device)</span><span id="f870" class="le jp hp la b fi lj lg l lh li">fake_images = generator(latent)</span><span id="9a39" class="le jp hp la b fi lj lg l lh li"># Try to fool the discriminator</span><span id="194a" class="le jp hp la b fi lj lg l lh li">preds = discriminator(fake_images)</span><span id="8905" class="le jp hp la b fi lj lg l lh li">targets = torch.ones(batch_size, 1, device=device)</span><span id="0603" class="le jp hp la b fi lj lg l lh li">loss = F.binary_cross_entropy(preds, targets)</span><span id="a2d1" class="le jp hp la b fi lj lg l lh li"># Update generator weights</span><span id="279c" class="le jp hp la b fi lj lg l lh li">loss.backward()</span><span id="250c" class="le jp hp la b fi lj lg l lh li">opt_g.step()</span><span id="e5cc" class="le jp hp la b fi lj lg l lh li">return loss.item()</span><span id="9590" class="le jp hp la b fi lj lg l lh li">from torchvision.utils import save_image</span><span id="b46e" class="le jp hp la b fi lj lg l lh li">sample_dir = 'generated'</span><span id="781a" class="le jp hp la b fi lj lg l lh li">os.makedirs(sample_dir, exist_ok=True)</span><span id="3e1f" class="le jp hp la b fi lj lg l lh li">def save_samples(index, latent_tensors, show=True):</span><span id="3c19" class="le jp hp la b fi lj lg l lh li">fake_images = generator(latent_tensors)</span><span id="0601" class="le jp hp la b fi lj lg l lh li">fake_fname = 'generated-images-{0:0=4d}.png'.format(index)</span><span id="84a5" class="le jp hp la b fi lj lg l lh li">save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)</span><span id="b0b4" class="le jp hp la b fi lj lg l lh li">print('Saving', fake_fname)</span><span id="bcf1" class="le jp hp la b fi lj lg l lh li">if show:</span><span id="b828" class="le jp hp la b fi lj lg l lh li">fig, ax = plt.subplots(figsize=(8, 8))</span><span id="a161" class="le jp hp la b fi lj lg l lh li">ax.set_xticks([]); ax.set_yticks([])</span><span id="0e37" class="le jp hp la b fi lj lg l lh li">ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))</span><span id="762d" class="le jp hp la b fi lj lg l lh li">fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)</span><span id="5fa9" class="le jp hp la b fi lj lg l lh li">save_samples(0, fixed_latent)</span></pre><h1 id="bdaa" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">完整训练循环:</h1><p id="d57f" class="pw-post-body-paragraph ip iq hp is b it km iv iw ix kn iz ja ko kp jd je kq kr jh ji ks kt jl jm jn hb bi translated">让我们定义一个拟合函数，为每批训练数据一前一后地训练鉴别器和生成器。我们将使用带有一些定制参数(betas)的Adam优化器，这些参数对于GANs非常有效。我们还将定期保存一些样本生成的图像，以供检查。</p><figure class="kv kw kx ky fd hk er es paragraph-image"><div class="er es me"><img src="../Images/69d726debf383b222eeef65c7943b1ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*iRaLM7ErzwSNxGK7y0oZ-g.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">GAN架构</figcaption></figure><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="d89a" class="le jp hp la b fi lf lg l lh li">from tqdm.notebook import tqdm</span><span id="669d" class="le jp hp la b fi lj lg l lh li">import torch.nn.functional as F</span><span id="2f02" class="le jp hp la b fi lj lg l lh li">def fit(epochs, lr, start_idx=1):</span><span id="15c0" class="le jp hp la b fi lj lg l lh li">torch.cuda.empty_cache()</span><span id="ad42" class="le jp hp la b fi lj lg l lh li"># Losses &amp; scores</span><span id="3618" class="le jp hp la b fi lj lg l lh li">losses_g = []</span><span id="409f" class="le jp hp la b fi lj lg l lh li">losses_d = []</span><span id="91aa" class="le jp hp la b fi lj lg l lh li">real_scores = []</span><span id="93b8" class="le jp hp la b fi lj lg l lh li">fake_scores = []</span><span id="9dec" class="le jp hp la b fi lj lg l lh li"># Create optimizers</span><span id="f604" class="le jp hp la b fi lj lg l lh li">opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))</span><span id="3844" class="le jp hp la b fi lj lg l lh li">opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))</span><span id="0bde" class="le jp hp la b fi lj lg l lh li">for epoch in range(epochs):</span><span id="5567" class="le jp hp la b fi lj lg l lh li">for real_images, _ in tqdm(train_dl):</span><span id="246a" class="le jp hp la b fi lj lg l lh li"># Train discriminator</span><span id="3748" class="le jp hp la b fi lj lg l lh li">loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)</span><span id="001b" class="le jp hp la b fi lj lg l lh li"># Train generator</span><span id="fdcc" class="le jp hp la b fi lj lg l lh li">loss_g = train_generator(opt_g)</span><span id="284f" class="le jp hp la b fi lj lg l lh li"># Record losses &amp; scores</span><span id="fb25" class="le jp hp la b fi lj lg l lh li">losses_g.append(loss_g)</span><span id="e30a" class="le jp hp la b fi lj lg l lh li">losses_d.append(loss_d)</span><span id="b98b" class="le jp hp la b fi lj lg l lh li">real_scores.append(real_score)</span><span id="7af8" class="le jp hp la b fi lj lg l lh li">fake_scores.append(fake_score)</span><span id="455f" class="le jp hp la b fi lj lg l lh li"># Log losses &amp; scores (last batch)</span><span id="aa17" class="le jp hp la b fi lj lg l lh li">print("Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}".format(</span><span id="928d" class="le jp hp la b fi lj lg l lh li">epoch+1, epochs, loss_g, loss_d, real_score, fake_score))</span><span id="c0dd" class="le jp hp la b fi lj lg l lh li"># Save generated images</span><span id="fc3d" class="le jp hp la b fi lj lg l lh li">save_samples(epoch+start_idx, fixed_latent, show=False)</span><span id="d74d" class="le jp hp la b fi lj lg l lh li">return losses_g, losses_d, real_scores, fake_scores</span><span id="80eb" class="le jp hp la b fi lj lg l lh li">lr = 0.0002</span><span id="c814" class="le jp hp la b fi lj lg l lh li">epochs = 300</span><span id="d17c" class="le jp hp la b fi lj lg l lh li">history = fit(epochs, lr)</span><span id="3265" class="le jp hp la b fi lj lg l lh li">losses_g, losses_d, real_scores, fake_scores = history</span><span id="509c" class="le jp hp la b fi lj lg l lh li"># Save the model checkpoints</span><span id="9920" class="le jp hp la b fi lj lg l lh li">torch.save(generator.state_dict(), 'G.pth')</span><span id="4ae5" class="le jp hp la b fi lj lg l lh li">torch.save(discriminator.state_dict(), 'D.pth')</span></pre><h1 id="a6ac" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">现在开始观看一些创成式图片…随着时代数量的增加，我们看到了进步:</h1><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="dd21" class="le jp hp la b fi lf lg l lh li">from IPython.display import Image</span><span id="9e58" class="le jp hp la b fi lj lg l lh li">Image('./generated/generated-images-0001.png')</span></pre><figure class="kv kw kx ky fd hk er es paragraph-image"><div class="er es mf"><img src="../Images/765eeaab7195605ec11367544f4250de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*lZtHjYNCAadgYsHHFhS6tg.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">第一时代图片</figcaption></figure><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="3455" class="le jp hp la b fi lf lg l lh li">Image('./generated/generated-images-0031.png')</span></pre><figure class="kv kw kx ky fd hk er es paragraph-image"><div class="er es mf"><img src="../Images/63de9b5d8c6eb39f2acdbc5c0388ca07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*0NT58-m03x4SJV8wzqsBdQ.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">第31时代图片</figcaption></figure><p id="33c0" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">图像质量在第30代之后有了很大的提高。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="d5a6" class="le jp hp la b fi lf lg l lh li">Image('./generated/generated-images-0090.png')</span></pre><figure class="kv kw kx ky fd hk er es paragraph-image"><div class="er es mf"><img src="../Images/5e2c82b2911bb4704e1d0e903f376604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*YzkB8WClXXO4ChnX-Nd5Ug.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">第90代图片</figcaption></figure><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="6507" class="le jp hp la b fi lf lg l lh li">Image('./generated/generated-images-0201.png')</span></pre><figure class="kv kw kx ky fd hk er es paragraph-image"><div class="er es mf"><img src="../Images/0392783d5f3a55a04cd0cd164704c0f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*2uaYiXJfqVTTiOCfFs48WA.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">第201个时代图片</figcaption></figure><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="ff1c" class="le jp hp la b fi lf lg l lh li">Image('./generated/generated-images-0300.png')</span></pre><figure class="kv kw kx ky fd hk er es paragraph-image"><div class="er es mf"><img src="../Images/5f58fc7684c4e2829868960d65ec9c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*4p3QhSLNcv5wxPE1wXli3Q.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">最后一个纪元</figcaption></figure><h1 id="0464" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结果:</h1><p id="bba4" class="pw-post-body-paragraph ip iq hp is b it km iv iw ix kn iz ja ko kp jd je kq kr jh ji ks kt jl jm jn hb bi translated">1.制作一个关于创成式图片的视频。</p><p id="9b4b" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">2.图表</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="eea8" class="le jp hp la b fi lf lg l lh li">import cv2</span><span id="85e1" class="le jp hp la b fi lj lg l lh li">import os</span><span id="d1b5" class="le jp hp la b fi lj lg l lh li">vid_fname = 'art_gans_training.avi'</span><span id="22e8" class="le jp hp la b fi lj lg l lh li">files = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if 'generated' in f]</span><span id="df35" class="le jp hp la b fi lj lg l lh li">files.sort()</span><span id="8192" class="le jp hp la b fi lj lg l lh li">out = cv2.VideoWriter(vid_fname,cv2.VideoWriter_fourcc(*'MP4V'), 5, (530,530))</span><span id="b1a6" class="le jp hp la b fi lj lg l lh li">[out.write(cv2.imread(fname)) for fname in files]</span><span id="813e" class="le jp hp la b fi lj lg l lh li">out.release()</span></pre><p id="f0a2" class="pw-post-body-paragraph ip iq hp is b it iu iv iw ix iy iz ja ko jc jd je kq jg jh ji ks jk jl jm jn hb bi translated">视频制作代码，看到随着时代的进步图片。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="3ce5" class="le jp hp la b fi lf lg l lh li">plt.plot(losses_d, '-')</span><span id="0e20" class="le jp hp la b fi lj lg l lh li">plt.plot(losses_g, '-')</span><span id="cd31" class="le jp hp la b fi lj lg l lh li">plt.xlabel('epoch')</span><span id="4eae" class="le jp hp la b fi lj lg l lh li">plt.ylabel('loss')</span><span id="207a" class="le jp hp la b fi lj lg l lh li">plt.legend(['Discriminator', 'Generator'])</span><span id="89ca" class="le jp hp la b fi lj lg l lh li">plt.title('Losses');</span></pre><figure class="kv kw kx ky fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/67f3e261aa611a765e3ac0a1f7fe47ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*MO4GnLnoUYR_g_JkG4U8rw.png"/></div></figure><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="120d" class="le jp hp la b fi lf lg l lh li">plt.plot(real_scores, '-')</span><span id="976d" class="le jp hp la b fi lj lg l lh li">plt.plot(fake_scores, '-')</span><span id="a7e8" class="le jp hp la b fi lj lg l lh li">plt.xlabel('epoch')</span><span id="dd6f" class="le jp hp la b fi lj lg l lh li">plt.ylabel('score')</span><span id="ef5c" class="le jp hp la b fi lj lg l lh li">plt.legend(['Real', 'Fake'])</span><span id="7f06" class="le jp hp la b fi lj lg l lh li">plt.title('Scores');</span></pre><figure class="kv kw kx ky fd hk er es paragraph-image"><div class="er es mh"><img src="../Images/78c8fa8dcf7fa090f793fa3f4d34f267.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*WlKvy-XDOTgDXbTG5SpcXw.png"/></div></figure><h1 id="8201" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">视频链接:</h1><blockquote class="im in io"><p id="b8b1" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae ku" href="https://github.com/soham2707/ARTGAN/blob/master/art_gans_training.avi" rel="noopener ugc nofollow" target="_blank">https://github . com/soham 2707/art gan/blob/master/art _ gans _ training . avi</a></p></blockquote><h1 id="d96b" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结论:</h1><p id="804f" class="pw-post-body-paragraph ip iq hp is b it km iv iw ix kn iz ja ko kp jd je kq kr jh ji ks kt jl jm jn hb bi translated">这里我们看到，伪图像的质量是好的，但是伪图像的预测值不是很好。因此，现在我们可以使用较低的学习率(lr)来运行它，大约80-100个时期可能能够增加伪图像的预测值。这是所有关于执行甘在绘画操纵创造完全不同的打印出来。同样可以在人类图片上实现或生成艺术。</p><h1 id="2050" class="jo jp hp bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">项目链接:</h1><blockquote class="im in io"><p id="bc0f" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">https://github.com/soham2707/ARTGAN.git<a class="ae ku" href="https://github.com/soham2707/ARTGAN.git" rel="noopener ugc nofollow" target="_blank"/></p></blockquote></div></div>    
</body>
</html>