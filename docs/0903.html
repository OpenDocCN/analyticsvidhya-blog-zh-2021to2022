<html>
<head>
<title>Vulnerabilities in Convolutional Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积神经网络中的漏洞</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/vulnerabilities-in-convolutional-neural-network-11edfa36a38b?source=collection_archive---------13-----------------------#2021-02-05">https://medium.com/analytics-vidhya/vulnerabilities-in-convolutional-neural-network-11edfa36a38b?source=collection_archive---------13-----------------------#2021-02-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="a5cd" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">婴儿的脚步:)</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/74d45e353924276d75743c77adc03e1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iO-Fdm2iyFwKk5GL"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">艾莉娜·格鲁布尼亚克在<a class="ae jm" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h2 id="1518" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">放弃</h2><p id="0bfb" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi translated">在这篇博文中，我们将讨论<strong class="kn hi">快速梯度符号方法(FGSM)，</strong>一种可以用来对付CNN的特定攻击媒介。如果你是一个像我一样的安全研究人员，是AI / ML领域的新手，我强烈建议你去读一下先决条件部分。如果你是那个机器学习向导，请随意跳过同样的问题</p><h2 id="b117" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">先决条件</h2><p id="2bcc" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi translated">令人高兴的是，我们不需要设置任何环境来执行下面的代码。相反，我们可以直接运行Google的合作实验室或Colab中的代码片段。 <br/>但是要理解攻击，你需要对机器学习概念和术语有一个基本的了解，像<strong class="kn hi">卷积神经网络，梯度下降，损失函数，MobileNetV2，ImageNet，Tensorflow，Keras。</strong></p><p id="756b" class="pw-post-body-paragraph kl km hh kn b ko le ii kq kr lf il kt jy lg kv kw kc lh ky kz kg li lb lc ld ha bi translated"><strong class="kn hi">系好安全带，然后……<br/></strong>首先，我们从一个攻击者的角度来看待CNN，在这个案例中，他可以完全访问模型的架构、输入和输出。简而言之，FGSM可以被归类为白盒攻击，我们的目标是欺骗图像分类器对输入图像做出错误的预测。</p><p id="fcb7" class="pw-post-body-paragraph kl km hh kn b ko le ii kq kr lf il kt jy lg kv kw kc lh ky kz kg li lb lc ld ha bi translated">分类图像。现在我们需要理解梯度下降和损失函数。当涉及到训练神经网络时，梯度是你如何确定轻推你的权重的方向，以减少损失值，从而获得准确的预测。</p><p id="1f47" class="pw-post-body-paragraph kl km hh kn b ko le ii kq kr lf il kt jy lg kv kw kc lh ky kz kg li lb lc ld ha bi translated">在FSGM中，我们做的与:D相反..FSGM通过使用神经网络的梯度来创建一个对立的例子。对于输入图像，该方法使用损失相对于输入图像的梯度来创建使损失最大化的新图像。这个新形象被称为敌对形象。这可以用下面的表达式来概括:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lj"><img src="../Images/b860fcd55839a498b3c5dbbd11c11192.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*psHJY5OgZPlw81YgOzwAaQ.png"/></div></figure><p id="1237" class="pw-post-body-paragraph kl km hh kn b ko le ii kq kr lf il kt jy lg kv kw kc lh ky kz kg li lb lc ld ha bi translated">在哪里</p><ul class=""><li id="5303" class="lk ll hh kn b ko le kr lf jy lm kc ln kg lo ld lp lq lr ls bi translated">adv_x:对抗性的形象。</li><li id="e31d" class="lk ll hh kn b ko lt kr lu jy lv kc lw kg lx ld lp lq lr ls bi translated">x:原始输入图像。</li><li id="8a8c" class="lk ll hh kn b ko lt kr lu jy lv kc lw kg lx ld lp lq lr ls bi translated">y:原始输入标签。</li><li id="6389" class="lk ll hh kn b ko lt kr lu jy lv kc lw kg lx ld lp lq lr ls bi translated">ϵ:确保扰动小的乘数。</li><li id="68c4" class="lk ll hh kn b ko lt kr lu jy lv kc lw kg lx ld lp lq lr ls bi translated">θ:模型参数。</li><li id="89c0" class="lk ll hh kn b ko lt kr lu jy lv kc lw kg lx ld lp lq lr ls bi translated">j:损失。</li></ul><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ly"><img src="../Images/2c891f842a5198e716f265f0b02ebaed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hJNcPBqZid-aQoMHiYL06A.png"/></div></div></figure><blockquote class="lz ma mb"><p id="f50b" class="kl km mc kn b ko le ii kq kr lf il kt md lg kv kw me lh ky kz mf li lb lc ld ha bi translated">在上图中，为了理解左边的方程，你需要有梯度下降。</p></blockquote><p id="204b" class="pw-post-body-paragraph kl km hh kn b ko le ii kq kr lf il kt jy lg kv kw kc lh ky kz kg li lb lc ld ha bi translated">嗯，就是这个概念。让我们开始编码吧。请随意将下面的代码片段复制粘贴到Colab来运行并观察结果。</p><pre class="ix iy iz ja fd mg mh mi mj aw mk bi"><span id="80f8" class="jn jo hh mh b fi ml mm l mn mo">import tensorflow as tf<br/>import matplotlib as mpl<br/>import matplotlib.pyplot as plt<br/><br/>mpl.rcParams['figure.figsize'] = (8, 8)<br/>mpl.rcParams['axes.grid'] = False<br/>pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,<br/>                                                     weights='imagenet')<br/>pretrained_model.trainable = False<br/><br/># ImageNet labels<br/>decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions<br/># Helper function to preprocess the image so that it can be inputted in MobileNetV2<br/>def preprocess(image):<br/>  image = tf.cast(image, tf.float32)<br/>  image = tf.image.resize(image, (224, 224))<br/>  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)<br/>  image = image[None, ...]<br/>  return image<br/><br/># Helper function to extract labels from probability vector<br/>def get_imagenet_label(probs):<br/>  return decode_predictions(probs, top=1)[0][0]<br/>image_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')<br/>image_raw = tf.io.read_file(image_path)<br/>image = tf.image.decode_image(image_raw)<br/><br/>image = preprocess(image)<br/>image_probs = pretrained_model.predict(image)<br/>plt.figure()<br/>plt.imshow(image[0]*0.5+0.5) # To change [-1, 1] to [0,1]<br/>_, image_class, class_confidence = get_imagenet_label(image_probs)<br/>plt.title('{} : {:.2f}% Confidence'.format(image_class, class_confidence*100))<br/>plt.show()</span></pre><p id="435d" class="pw-post-body-paragraph kl km hh kn b ko le ii kq kr lf il kt jy lg kv kw kc lh ky kz kg li lb lc ld ha bi translated">现在让我们休息一下，你在Colab上执行代码了吗？好吧，如果是的话，你应该已经看到程序以41.82%的置信度将拉布拉多犬的图像分类为拉布拉多犬本身。没什么好惊讶的，这是一个普通的图像分类程序。无论如何，让我们分解代码。所以程序从导入TensorFlow和其他需要的库开始。之后加载MobileNetV2模型。接下来定义<code class="du mp mq mr mh b">preprocess</code>功能。这么说吧，在机器学习中，我们需要在输入模型之前对输入进行预处理。每个模型可能需要以不同的方式对输入进行预处理，以便特定的模型可以有效地预测输出。现在我们给程序提供一张拉布拉多的图片。程序获取该图像，对其进行预处理(<code class="du mp mq mr mh b">image = preprocess(image)</code>)并对预处理后图像进行预测(<code class="du mp mq mr mh b">image_probs = pretrained_model.predict(image)</code>)。最后，程序将预测与图像一起显示。</p><p id="8e71" class="pw-post-body-paragraph kl km hh kn b ko le ii kq kr lf il kt jy lg kv kw kc lh ky kz kg li lb lc ld ha bi translated">是啊，是啊，所以这都是机器学习101。所以让我们通过编写对抗性的东西来使事情变得有趣一点。</p><pre class="ix iy iz ja fd mg mh mi mj aw mk bi"><span id="1dce" class="jn jo hh mh b fi ml mm l mn mo">loss_object = tf.keras.losses.CategoricalCrossentropy()<br/><br/>def create_adversarial_pattern(input_image, input_label):<br/>  with tf.GradientTape() as tape:<br/>    tape.watch(input_image)<br/>    prediction = pretrained_model(input_image)<br/>    loss = loss_object(input_label, prediction)<br/><br/>  # Get the gradients of the loss w.r.t to the input image.<br/>  gradient = tape.gradient(loss, input_image)<br/>  # Get the sign of the gradients to create the perturbation<br/>  signed_grad = tf.sign(gradient)<br/>  return signed_grad</span></pre><p id="0489" class="pw-post-body-paragraph kl km hh kn b ko le ii kq kr lf il kt jy lg kv kw kc lh ky kz kg li lb lc ld ha bi translated">上面的代码只是一个函数，它返回FSGM方程的一部分，即</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ms"><img src="../Images/dc4f0874963570c9d4b27e9592706e34.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*-rkkjY6Z6HoJmte1ukg7RQ.png"/></div></figure><p id="efac" class="pw-post-body-paragraph kl km hh kn b ko le ii kq kr lf il kt jy lg kv kw kc lh ky kz kg li lb lc ld ha bi translated">让我们来想象一下上面的结果/ <code class="du mp mq mr mh b">perturbations</code></p><pre class="ix iy iz ja fd mg mh mi mj aw mk bi"><span id="de9e" class="jn jo hh mh b fi ml mm l mn mo"># Get the input label of the image.<br/>labrador_retriever_index = 208<br/>label = tf.one_hot(labrador_retriever_index, image_probs.shape[-1])<br/>label = tf.reshape(label, (1, image_probs.shape[-1]))<br/><br/>perturbations = create_adversarial_pattern(image, label)<br/>plt.imshow(perturbations[0]*0.5+0.5); # To change [-1, 1] to [0,1]</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mt"><img src="../Images/332fe5bc309731be785224640e3e25f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*52VdhcO22qtjqL8vSVM4NA.png"/></div></figure><p id="b513" class="pw-post-body-paragraph kl km hh kn b ko le ii kq kr lf il kt jy lg kv kw kc lh ky kz kg li lb lc ld ha bi translated">现在我们有了扰动(输入图像的损耗w.r.t .的有符号梯度)，我们可以在与梯度方向相反的方向上轻推图像像素。这意味着我们必须在最大化损失的方向上推动图像像素，以便CNN对图像进行错误的分类。为此，我们将对不同的ε值运行攻击，ε= 0表示没有运行攻击。请注意，随着ε值的增加，欺骗网络变得更加容易。然而，这是一种折衷，导致扰动在输入图像中变得更容易识别。</p><pre class="ix iy iz ja fd mg mh mi mj aw mk bi"><span id="258c" class="jn jo hh mh b fi ml mm l mn mo">def display_images(image, description):<br/>  _, label, confidence = get_imagenet_label(pretrained_model.predict(image))<br/>  plt.figure()<br/>  plt.imshow(image[0]*0.5+0.5)<br/>  plt.title('{} \n {} : {:.2f}% Confidence'.format(description,<br/>                                                   label, confidence*100))<br/>  plt.show()</span><span id="a3a0" class="jn jo hh mh b fi mu mm l mn mo">epsilons = [0, 0.01, 0.1, 0.15]<br/>descriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input')<br/>                for eps in epsilons]<br/><br/>for i, eps in enumerate(epsilons):<br/>  adv_x = image + eps*perturbations<br/>  adv_x = tf.clip_by_value(adv_x, -1, 1)<br/>  display_images(adv_x, descriptions[i])</span></pre><p id="6e04" class="pw-post-body-paragraph kl km hh kn b ko le ii kq kr lf il kt jy lg kv kw kc lh ky kz kg li lb lc ld ha bi translated">让我们看看结果</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mv"><img src="../Images/8d9e83b9082c6344ac65816e4c2dd9f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*ZBaGM5YgL0EVr--ZNjJjwg.png"/></div></figure><p id="4032" class="pw-post-body-paragraph kl km hh kn b ko le ii kq kr lf il kt jy lg kv kw kc lh ky kz kg li lb lc ld ha bi translated">还有瓦拉！！:MobileNetV2模型被欺骗了，它不能准确地预测/分类图像。</p><p id="c439" class="pw-post-body-paragraph kl km hh kn b ko le ii kq kr lf il kt jy lg kv kw kc lh ky kz kg li lb lc ld ha bi translated">感谢您浏览博客。直到下一次，测试呈阴性并保持阳性。</p><h2 id="1827" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">参考</h2><ul class=""><li id="08d9" class="lk ll hh kn b ko kp kr ks jy mw kc mx kg my ld lp lq lr ls bi translated"><a class="ae jm" href="https://arxiv.org/abs/1412.6572" rel="noopener ugc nofollow" target="_blank">解释和利用对立的例子</a></li><li id="1d9c" class="lk ll hh kn b ko lt kr lu jy lv kc lw kg lx ld lp lq lr ls bi translated"><a class="ae jm" href="https://www.tensorflow.org/tutorials/generative/adversarial_fgsm" rel="noopener ugc nofollow" target="_blank">使用FGSM的对抗示例</a></li></ul></div></div>    
</body>
</html>