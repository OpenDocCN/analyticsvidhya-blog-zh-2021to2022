<html>
<head>
<title>Activation Functions and Loss Functions for neural networks — How to pick the right one?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的激活函数和损失函数——如何选择正确的函数？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-and-loss-functions-for-neural-networks-how-to-pick-the-right-one-542e1dd523e0?source=collection_archive---------1-----------------------#2021-07-21">https://medium.com/analytics-vidhya/activation-functions-and-loss-functions-for-neural-networks-how-to-pick-the-right-one-542e1dd523e0?source=collection_archive---------1-----------------------#2021-07-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="f74e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">得到神经网络激活函数和损失函数的正确组合的备忘单</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/b67f3ba06961c5f27e212421742b6a66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*mgcnpP5WG8ZbKcvrc2Or_A.jpeg"/></div></figure><p id="f48c" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">当一个人开始开发自己的神经网络时，很容易被模型中每个参数的各种选项所淹没。每个隐藏层使用哪个激活函数？哪个激活函数用于输出层？什么时候使用二元交叉熵vs分类交叉熵？</p><p id="cdb8" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这样的问题会不断出现，直到我们对每个选项的作用、利弊以及何时应该使用它没有一个明确的理解。博客的目的正是如此。我们将通过流行的激活函数和损失函数的关键特征，以及理解什么时候应该使用哪个。如果你需要复习一下神经网络是如何工作的，或者什么是激活或损失函数，请参考这个<a class="ae kb" href="https://indraneeldb1993ds.medium.com/understanding-the-basics-of-neural-networks-for-beginners-9c26630d08" rel="noopener">博客</a>。所以不要再耽搁了，让我们开始吧！</p><h1 id="1d1c" class="kc kd hi bd ke kf kg kh ki kj kk kl km io kn ip ko ir kp is kq iu kr iv ks kt bi translated">激活功能</h1><p id="662a" class="pw-post-body-paragraph jf jg hi jh b ji ku ij jk jl kv im jn jo kw jq jr js kx ju jv jw ky jy jz ka hb bi translated">给定输入，神经元的激活函数定义其输出。我们将讨论4种流行的激活功能:</p><ol class=""><li id="11c4" class="kz la hi jh b ji jj jl jm jo lb js lc jw ld ka le lf lg lh bi translated"><strong class="jh hj">乙状结肠功能:</strong></li></ol><p id="e532" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">描述:</strong> <em class="li">取一个实数值，在0和1之间缩放。大负数变成0，大正数变成1 </em> <br/> <strong class="jh hj">公式:</strong> 1 /(1 + e^-x) <br/> <strong class="jh hj">范围:</strong> (0，1) <br/> <strong class="jh hj">优点:</strong> <em class="li">由于它的范围在0到1之间，所以非常适合需要预测某个事件的概率作为输出的情况。</em> <br/> <strong class="jh hj">缺点:</strong> <em class="li">梯度值对于范围-3和3是显著的，但是在这个范围之外变得更加接近零，这几乎消除了神经元对最终输出的影响。此外，sigmoid输出不是以零为中心的(它以0.5为中心)，这导致权重</em> <br/> <strong class="jh hj">图</strong>的梯度更新中出现不希望的曲折动态</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lj"><img src="../Images/398484fa660e0a4a9cbd6fdb190975fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*Igjh54wDEekixTAv3Ngihw.png"/></div></figure><p id="e60c" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> 2。双曲正切函数:</strong></p><p id="cfbe" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">描述:</strong> <em class="li">类似于sigmoid，但取一个实数值并在-1和1之间缩放。它比sigmoid更好，因为它以0为中心，这导致更好的收敛</em> <br/> <strong class="jh hj">公式:</strong>(e^x-e^-x)/(e^x+e^-x)<br/><strong class="jh hj">范围:</strong> (-1，1) <br/> <strong class="jh hj">优点:</strong><em class="li">tanh的导数大于sigmoid的导数，这有助于我们更快地最小化成本函数</em> <br/> <strong class="jh hj">缺点:</strong> <em class="li">与sigmoid类似，梯度值变得接近</em>  <em class="li">因此，网络拒绝学习或者以非常小的速率保持学习。</em> <br/> <strong class="jh hj">剧情:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lk"><img src="../Images/5595d27e2e42ed68d6b568e929e68763.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*4qN5X7XMS5AUXHqpiX14tw.png"/></div></div></figure><p id="7569" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> 3。Softmax功能:</strong></p><p id="adf8" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">描述:</strong> <em class="li"> Softmax函数可以想象为多个sigmoids的组合，它可以返回属于多类分类问题</em> <br/> <strong class="jh hj">中每个单独类的数据点的概率公式:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lp"><img src="../Images/feb2e6c3373428c3578f4b1312f32e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*d68qXc650Gm4w_RT0EteYQ.png"/></div></figure><p id="9afb" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> Range: </strong> (0，1)，sum of output = 1 <br/> <strong class="jh hj">优点:</strong> <em class="li">可以处理多个类并给出属于每个类的概率</em> <br/> <strong class="jh hj">缺点:</strong> <em class="li">不应该用在隐藏层中因为我们希望神经元是独立的。如果我们应用它，那么它们将是线性相关的。</em></p><p id="0562" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">剧情:</strong>不适用</p><p id="fcfc" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> 4。ReLU功能:</strong></p><p id="96fe" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">说明:</strong> <em class="li">整流线性激活函数，简称ReLU，是一个分段线性函数，如果是正的就直接输出输入，否则输出零。这是默认功能，但修改默认参数允许我们使用非零阈值，并对低于阈值的值使用非零倍数的输入(称为泄漏ReLU)。</em> <br/> <strong class="jh hj">公式:</strong> max(0，x) <br/> <strong class="jh hj">范围:</strong> (0，inf) <br/> <strong class="jh hj">优点:</strong> <em class="li">虽然RELU看起来和行为像线性函数，但它是一个非线性函数，允许学习复杂的关系，并且能够通过具有大的导数来允许学习深度网络中的所有隐藏层。</em> <br/> <strong class="jh hj">缺点:</strong> <em class="li">它不应该作为分类/回归任务的最终输出层</em> <br/> <strong class="jh hj">图:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lq"><img src="../Images/4f70b7d167b0b0e6c82fc0491f4ffcf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*2-_YrU2R2e1nsOB_r2CxjQ.png"/></div></figure><h1 id="78cc" class="kc kd hi bd ke kf kg kh ki kj kk kl km io kn ip ko ir kp is kq iu kr iv ks kt bi translated">损失函数</h1><p id="dff6" class="pw-post-body-paragraph jf jg hi jh b ji ku ij jk jl kv im jn jo kw jq jr js kx ju jv jw ky jy jz ka hb bi translated">建立神经网络基础设施的另一个关键方面是选择正确的损失函数。对于神经网络，我们寻求最小化由损失函数计算的误差(实际值和预测值之间的差异)。我们将讨论3种流行的损失函数:</p><p id="f231" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> 1。均方差，L2损失</strong></p><p id="dcda" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">描述:</strong> <em class="li"> MSE loss用于回归任务。顾名思义，这种损失是通过取实际值(目标值)和预测值之间的平方差的平均值来计算的。</em> <br/> <strong class="jh hj">公式:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lr"><img src="../Images/c6720f652c0bc91f63f1c02b2bc33c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*3Ae_vEivsVSvMIx5cK64WA.png"/></div></figure><p id="a773" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">范围:</strong> (0，inf) <br/> <strong class="jh hj">优点:</strong> <em class="li">如果目标变量的分布为高斯型，则首选损失函数，因为它具有良好的导数，有助于模型快速收敛</em> <br/> <strong class="jh hj">缺点:</strong> <em class="li">对数据中的异常值不稳健(与平均绝对误差等损失函数不同)并且指数地惩罚高低预测(与均方对数误差损失等损失函数不同)</em></p><p id="0587" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> 2。二元交叉熵</strong></p><p id="840f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">说明:</strong> <em class="li"> BCE损失是用于二分类任务的默认损失函数。它需要一个输出图层将数据分为两类，输出范围为(0–1)，即应使用sigmoid函数。</em> <br/> <strong class="jh hj">公式:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ls"><img src="../Images/b9da30da44d8f0b1adc61fc457f23cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*Wm5DH4S98PzcUdkKTB_PeA.png"/></div></figure><p id="266b" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><em class="li">其中y是实际标签，ŷ是分类器预测一个类别的预测概率分布，m是记录的数量。</em> <br/> <strong class="jh hj">范围:</strong> (0，inf) <br/> <strong class="jh hj">优点:</strong> <em class="li">损失函数的连续性质有助于训练过程很好地收敛</em> <br/> <strong class="jh hj">缺点:</strong> <em class="li">只能与sigmoid激活函数一起使用。其他损失函数，如铰链或方形铰链损失，可与双曲正切激活函数一起工作</em></p><p id="7732" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> 3。分类交叉熵</strong></p><p id="252c" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">描述:</strong>当我们有多类分类任务时，它是默认的损失函数。它需要与最后一层经历<em class="li"> softmax </em>激活的类相同数量的输出节点，以便每个输出节点具有介于(0–1)之间的概率值。<br/> <strong class="jh hj">公式:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lt"><img src="../Images/104c163c51064230bdefae2e3470f97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/0*jPZkyyGNN62FWI9R.png"/></div></figure><p id="4290" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><em class="li">其中y是实际标签，p是分类器预测的概率分布，用于预测类j </em> <br/> <strong class="jh hj">范围:</strong> (0，inf) <br/> <strong class="jh hj">优点:</strong> <em class="li">类似于二进制交叉熵，损失函数的连续性质有助于训练过程很好地收敛</em> <br/> <strong class="jh hj">缺点:</strong> <em class="li">如果有许多类，可能需要一个具有许多零值的热编码向量，需要大量的存储器(在这种情况下应该使用稀疏分类交叉熵</em></p><h1 id="0374" class="kc kd hi bd ke kf kg kh ki kj kk kl km io kn ip ko ir kp is kq iu kr iv ks kt bi translated">摘要</h1><p id="61d8" class="pw-post-body-paragraph jf jg hi jh b ji ku ij jk jl kv im jn jo kw jq jr js kx ju jv jw ky jy jz ka hb bi translated">读完这篇博客后，读者应该能够为大多数流行的机器学习问题建立正确的架构(就激活和损失函数而言)。下表简要介绍了在何处使用这些功能:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lu"><img src="../Images/9944dc66ab2db0032c8229f52869c48c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B24qtkjFBD43ClulZtLmVA.png"/></div></div></figure><p id="e066" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">深度学习包/库中有<a class="ae kb" href="https://keras.io/api/layers/activations/" rel="noopener ugc nofollow" target="_blank">几个其他激活函数</a>如softplus和elu，以及<a class="ae kb" href="https://keras.io/api/losses/" rel="noopener ugc nofollow" target="_blank">其他损失函数</a>如hinge和huber。我肯定会鼓励感兴趣的读者仔细阅读并尝试这些其他函数(特别是，如果这里讨论的函数不能产生正确的结果)。</p><p id="f56a" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">构建健壮和精确的神经网络的下一步是理解训练神经网络时可能面临的各种问题以及如何处理它们。请务必阅读本系列的第三部分以了解更多相关信息。</p><p id="01a4" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">你对这个博客有什么问题或建议吗？请随时留言。</p><h1 id="4255" class="kc kd hi bd ke kf kg kh ki kj kk kl km io kn ip ko ir kp is kq iu kr iv ks kt bi translated">感谢您的阅读！</h1><p id="5625" class="pw-post-body-paragraph jf jg hi jh b ji ku ij jk jl kv im jn jo kw jq jr js kx ju jv jw ky jy jz ka hb bi translated">如果你和我一样，对人工智能、数据科学或经济学充满热情，请随时在<a class="ae kb" href="http://www.linkedin.com/in/indraneel-dutta-baruah-ds" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae kb" href="https://github.com/IDB-FOR-DATASCIENCE" rel="noopener ugc nofollow" target="_blank"> Github </a>和<a class="ae kb" rel="noopener" href="/@indraneeldb1993ds"> Medium </a>上添加/关注我。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lv"><img src="../Images/5802a4a351bc0208015e1af660106676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gy6yBNKVdyKB_Dpi.jpg"/></div></div></figure></div></div>    
</body>
</html>