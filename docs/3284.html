<html>
<head>
<title>DEEP DETERMINISTIC POLICY GRADIENT FOR CONTINUOUS ACTION SPACE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">连续动作空间的深度确定性策略梯度</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-deterministic-policy-gradient-for-continuous-action-space-9b2b9bacd555?source=collection_archive---------4-----------------------#2021-06-23">https://medium.com/analytics-vidhya/deep-deterministic-policy-gradient-for-continuous-action-space-9b2b9bacd555?source=collection_archive---------4-----------------------#2021-06-23</a></blockquote><div><div class="ds gz ha hb hc hd"/><div class="he hf hg hh hi"><div class=""/><p id="badd" class="pw-post-body-paragraph ii ij hl ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf he bi translated">在之前关于策略梯度方法的<a class="ae jg" rel="noopener" href="/analytics-vidhya/policy-gradients-in-deep-reinforcement-learning-83d99575cfca">文章</a>中，我们讨论了基于PG方法的缺点。它们不是样本有效的，因为它们在每次迭代中丢弃了先前学习的策略。我们可以说它们是基于策略的学习方法，因为相同的策略生成动作并更新价值函数。这些方法不利用价值函数信息，这需要非策略设置。所以我们有一套新的算法，叫做演员-评论家方法，DDPG就是其中之一。</p><ul class=""><li id="a02a" class="jh ji hl ik b il im ip iq it jj ix jk jb jl jf jm jn jo jp bi translated">政策梯度是…</li></ul></div></div>    
</body>
</html>