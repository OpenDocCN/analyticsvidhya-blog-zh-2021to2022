<html>
<head>
<title>Why Regularization?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么要正规化？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/why-regularization-a2ca00f8612c?source=collection_archive---------2-----------------------#2021-08-21">https://medium.com/analytics-vidhya/why-regularization-a2ca00f8612c?source=collection_archive---------2-----------------------#2021-08-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="ebf9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文将涵盖广泛使用的技术，以避免过度拟合。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/63c7cb6dba30fe7de507f1a590c208e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5i2EKJwVZGYIDSJWPE8cHw.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">深沉的风格</figcaption></figure><h1 id="2448" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">需要正规化？</h1><p id="0e53" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">深度神经网络由于其复杂性、大量的隐藏层而易于过度拟合，其中训练误差非常小，但是测试误差可能上升。</p><p id="6234" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以参考这篇文章中的这个概念:</p><div class="kv kw ez fb kx ky"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/handle-bias-variance-ea01b9c166f3"><div class="kz ab dw"><div class="la ab lb cl cj lc"><h2 class="bd hi fi z dy ld ea eb le ed ef hg bi translated">处理偏差和差异？</h2><div class="lf l"><p class="bd b fp z dy ld ea eb le ed ef dx translated">数字识别欠拟合和过拟合？</p></div></div><div class="lg l"><div class="lh l li lj lk lg ll jm ky"/></div></div></a></div><p id="ed0b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正则化有助于模型更好地进行概化，从而更好地处理看不见的数据。正则化给学习算法引入了不确定性或随机性，它也简化了神经网络。一些正则化技术由于太大而惩罚权重度量，一些技术减少神经网络中隐藏单元的数量。</p><h1 id="c2a9" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">不同类型的正规化技术。</h1><p id="fbf8" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">有不同类型的正则化技术对模型的影响非常不同。以下是其中的一些:</p><h2 id="6256" class="lm jt hh bd ju ln lo lp jy lq lr ls kc ip lt lu kg it lv lw kk ix lx ly ko lz bi translated">L1和L2正规化</h2><p id="d9d6" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">使用L1正则化技术的回归模型被称为<strong class="ig hi"> <em class="ma">套索回归</em> </strong>，使用L2的模型被称为<strong class="ig hi"> <em class="ma">岭回归</em> </strong>。如果权重矩阵很大，L1和L2正则化倾向于增加成本。我们将L1或L2的正则化部分添加到实际成本函数中。它惩罚大权重矩阵。</p><blockquote class="mb"><p id="cdaf" class="mc md hh bd me mf mg mh mi mj mk jb dx translated">ost =实际成本+调整项(L1或L2)。</p></blockquote><blockquote class="ml mm mn"><p id="b501" class="ie if ma ig b ih mo ij ik il mp in io mq mr ir is ms mt iv iw mu mv iz ja jb ha bi translated">L1<strong class="ig hi">或<em class="hh">拉索</em>或</strong></p></blockquote><p id="8666" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种情况下，正则化项将度量的绝对值添加到实际成本中(例如，二进制交叉熵)。</p><blockquote class="mb"><p id="c507" class="mc md hh bd me mf mg mh mi mj mk jb dx translated"><em class="mw">成本=实际成本+λ/2m</em>∫σ∨<em class="mw">W</em>∩</p></blockquote><p id="adcf" class="pw-post-body-paragraph ie if hh ig b ih mo ij ik il mp in io ip mr ir is it mt iv iw ix mv iz ja jb ha bi translated">同样，如果<em class="ma">λ</em>为零，那么我们将得到实际成本，而一个非常大的值将使系数为零，因此它将欠拟合。压缩模型是有用的。它有助于将不太重要的功能推到零。它也有助于特征选择过程。</p><blockquote class="ml mm mn"><p id="b473" class="ie if ma ig b ih ii ij ik il im in io mq iq ir is ms iu iv iw mu iy iz ja jb ha bi translated">L2还是<strong class="ig hi">死板</strong></p></blockquote><p id="4763" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个正则化项将平方值添加到实际成本中，因为成本大而受到惩罚。</p><blockquote class="mb"><p id="159e" class="mc md hh bd me mf mg mh mi mj mk jb dx translated">成本=实际成本+λ/2m∫σ∨W∸</p></blockquote><p id="cdb0" class="pw-post-body-paragraph ie if hh ig b ih mo ij ik il mp in io ip mr ir is it mt iv iw ix mv iz ja jb ha bi translated">这里，如果<em class="ma">λ</em>为零，那么你可以想象我们得到的是实际成本。然而，如果<em class="ma">λ</em>非常大，那么它将增加太多的重量，并且将导致装配不足。话虽如此，如何选择<em class="ma">λ</em>很重要。这种技术可以很好地避免过度拟合的问题。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="mx my l"/></div></figure><h2 id="07de" class="lm jt hh bd ju ln lo lp jy lq lr ls kc ip lt lu kg it lv lw kk ix lx ly ko lz bi translated">拒绝传统社会的人</h2><p id="36f9" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">这是深度学习中使用最多的技术，并且产生了良好的结果。它减少了模型中隐藏层的数量，从而降低了模型的复杂性。</p><p id="a288" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这背后的一般思想是，我们设置一些保留节点和删除节点的概率，让我们说0.5的保留和0.5的删除。这将从训练过程中随机排除节点。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="mx my l"/></div></figure><p id="aefe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随着每次迭代，基于保持节点的概率，一些不同的节点集被激活。这种技术给模型引入了随机性，从而减少了过度拟合的机会。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mz"><img src="../Images/ae6b5dde5de4ee0c2a15d2d003281557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gJaT-8bnbjHpei0_3tdlpA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">(a)不存在压差时,( b)实施压差时，防止过度拟合的压差。</figcaption></figure><p id="030d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当您认为某些层更容易过度拟合时，应该使用Dropout。</p><h2 id="569b" class="lm jt hh bd ju ln lo lp jy lq lr ls kc ip lt lu kg it lv lw kk ix lx ly ko lz bi translated">提前停止</h2><p id="4ff3" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">早期停止是一个监视验证错误以获得最适合数据的巧妙概念。在早期停止中，我们将训练数据分为训练和验证数据集，在每个时期我们交叉验证模型。每当模型向高方差移动时，它就停止进一步交互。这种停止取决于耐心变量(让我们假设耐心= 10)，也就是说，如果你想在10次类似的高方差迭代后停止。这种耐心是一个必须仔细选择的超参数。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es na"><img src="../Images/ee78e811dfee772cfbc2c4c79c9b27e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wM7oGSqfTGIUeZniCgq8cg.png"/></div></div></figure><p id="0521" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可能会有这样的情况，模型在几个时期后开始执行得更好，但是我们将耐心设置为10，所以它会停止，这就是为什么我们必须小心选择这个参数。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="mx my l"/></div></figure><blockquote class="ml mm mn"><p id="bbf8" class="ie if ma ig b ih ii ij ik il im in io mq iq ir is ms iu iv iw mu iy iz ja jb ha bi translated">在计算机视觉这一跨学科的科学领域中，还有一种技术可以实现随机性，那就是数据扩充。在这种技术中，我们通过引入一些失真，如移动、翻转、缩放、旋转、亮度等，产生了与标记数据有些相似的图像。</p></blockquote></div></div>    
</body>
</html>