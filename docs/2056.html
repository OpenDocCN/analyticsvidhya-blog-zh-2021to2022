<html>
<head>
<title>Implementing the Cox model in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在R中实现Cox模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementing-the-cox-model-in-r-b1292d6ab6d2?source=collection_archive---------7-----------------------#2021-04-03">https://medium.com/analytics-vidhya/implementing-the-cox-model-in-r-b1292d6ab6d2?source=collection_archive---------7-----------------------#2021-04-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="4c60" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">弥合数学和代码之间的鸿沟</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/bd8220f5f524e4b6290c725b96d1d116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Oqpan8QmOE9t3z55"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">弗洛里安·奥利佛在Unsplash<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">上的照片</a></figcaption></figure><h1 id="49e7" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">动机</h1><p id="33f8" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">有许多教学资源解释了考克斯模型背后的理论。也有许多资源提供了使用现有统计软件包进行生存分析的Cox模型应用指南。然而，很少有资料来源通过解释Cox模型的程序化实现来弥合理论和实践之间的差距。在本文中，我1)解释实现Cox模型所必需的关键数学结果，2)举例说明可用于实现Cox模型的样本代码，以及3)比较手动实现的函数与<code class="du lc ld le lf b">survival::coxph()</code>的输出。</p><h1 id="ba35" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">介绍</h1><p id="eda7" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">估计几乎任何(半)参数回归模型都遵循类似的过程:</p><ol class=""><li id="d594" class="lg lh hi ki b kj li km lj kp lk kt ll kx lm lb ln lo lp lq bi translated">用我们想要估计的参数<em class="lr"> β </em>指定一个模型</li><li id="c851" class="lg lh hi ki b kj ls km lt kp lu kt lv kx lw lb ln lo lp lq bi translated">确定a)最小化的损失函数，或b)最大化的可能性</li><li id="fb85" class="lg lh hi ki b kj ls km lt kp lu kt lv kx lw lb ln lo lp lq bi translated">通过对感兴趣的参数求导并将其设置为零(使用微积分),找到上述表达式的最大值(对于可能性)或最小值(对于损失函数)</li><li id="2d14" class="lg lh hi ki b kj ls km lt kp lu kt lv kx lw lb ln lo lp lq bi translated">找出产生零导数的感兴趣参数值(<em class="lr"> β̂ </em>)(使用代数或数值优化)</li></ol><p id="8f56" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">每个回归模型都有自己的怪癖，可能会使其中任何一个步骤变得或多或少的困难。例如，线性回归的损失函数(平方误差)具有“干净的”导数，这意味着步骤(4)只是通过代数求解最佳参数值的问题。另一方面，逻辑回归的损失函数具有“混乱的”导数，这意味着步骤(4)需要使用数值优化算法，而不是简单的代数。</p><p id="4a89" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">下面是上述五个步骤的概要，具体到Cox比例风险模型。</p><ol class=""><li id="9e71" class="lg lh hi ki b kj li km lj kp lk kt ll kx lm lb ln lo lp lq bi translated">我们的模型是:λ(<em class="lr">t</em>|<em class="lr">x</em>)= λ₀(<em class="lr">t</em>e<em class="lr">ˣᵝ</em></li><li id="5ec1" class="lg lh hi ki b kj ls km lt kp lu kt lv kx lw lb ln lo lp lq bi translated">我们将最大化可能性(而不是最小化损失函数)。考克斯模型的一个怪癖是它的估计使用了所谓的“部分似然”而不是完全似然。</li><li id="b02e" class="lg lh hi ki b kj ls km lt kp lu kt lv kx lw lb ln lo lp lq bi translated">我们将通过设置导数为零来最大化部分可能性。</li><li id="9121" class="lg lh hi ki b kj ls km lt kp lu kt lv kx lw lb ln lo lp lq bi translated">导数是“混乱的”，所以我们将使用牛顿-拉夫森算法来计算得出参数估计，从而产生零导数(而不是用代数方法求解)。</li></ol><h1 id="c8fa" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">1.模型设定</h1><p id="9f8f" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">Cox模型是λ(<em class="lr">t</em>|<em class="lr">x</em>)= λ₀(<em class="lr">t</em>)<em class="lr">eˣᵝ</em>，其中x是协变量矩阵，<em class="lr"> β </em>是我们要估计的系数的向量，λ₀是所谓的基线风险函数。危险函数可以解释为患者在时间<em class="lr"> t </em>的失败“风险”。数学上，它定义如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ma"><img src="../Images/bb96cbd04624fda99c30e34b1c68c7aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*gC_-juHx6oaASJwVV2ZWMA.png"/></div></figure><p id="04a3" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">其中<em class="lr"> f </em> ( <em class="lr"> t </em>)是<em class="lr"> T </em>的PDF，即所有患者的生存时间。</p><p id="002c" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">当Cox模型中的协变量为零时，患者的风险函数λ( <em class="lr"> t </em> | <em class="lr"> X </em>)等于基线风险函数λ₀( <em class="lr"> t </em>)。非零协变量通过将基线风险乘以<em class="lr"> eˣᵝ </em>项来影响患者的风险。</p><p id="6a83" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">对我们来说，这个解释就足够了。许多其他来源已经彻底地解释了Cox模型规范的更多细节(例如，参见Therneau和Grambsch 2000)。</p><h1 id="a2b7" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">2.指定可能性</h1><p id="25dd" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">Cox模型的一个关键特征是它可以表达如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ma"><img src="../Images/c4e2bcaaf5f3e0166ff9b4845f8de18e.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*SRFYIn5QChF0dciqfzssFw.png"/></div></figure><p id="d562" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">其中<em class="lr"> g </em>是基线风险函数，而<em class="lr"> h </em>是协变量和β的指数。重要的是，<em class="lr"> g </em>不依赖于<em class="lr"> X </em>和<em class="lr"> h </em>不依赖于<em class="lr"> t </em>。这允许我们比较两个不同患者的风险函数，而不必估计<em class="lr"> g </em> ( <em class="lr"> t </em>):</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mb"><img src="../Images/8da04c9138cfa6a544240dc229fc6a58.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*o8EseN7PqSTjMmUliOEtNw.png"/></div></figure><p id="ff6c" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">这允许所谓的半参数推断，其中我们将<em class="lr"> h </em> ( <em class="lr"> X </em>)参数化，但是允许<em class="lr"> g </em> ( <em class="lr"> t </em>)更加自由。</p><p id="1d2e" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">当我们指定可能性<em class="lr"> L </em> (λ₀，<em class="lr"> β </em>)时，我们看到了这个想法。考克斯证明:<em class="lr"> L </em> (λ₀，<em class="lr"> β </em>)可以表示为<em class="lr"> L </em> ₁(λ₀，<em class="lr">β</em>)*<em class="lr">l</em>₂(<em class="lr">β</em>，其中<em class="lr"> L </em> ₂'s最大值<em class="lr"> β̂ </em>是<em class="lr"> β </em>的渐近正态无偏估计量，而考克斯实际上建议忽略₁，把所有的推论建立在最大化₂.的基础上因此，<em class="lr"> L </em> ₂可以称为偏似然。参见<a class="ae jn" href="https://web.stanford.edu/~lutian/coursepdf/unitcox1.pdf" rel="noopener ugc nofollow" target="_blank">这里的</a>可以很好地了解这一点。</p><p id="8908" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">为了简单起见，我们将把<em class="lr"> L </em> ₂简称为<em class="lr"> L </em>，但是要理解的是<em class="lr"> L </em>代表部分可能性。Cox将患者<em class="lr"> j </em>对部分可能性的贡献指定如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mc"><img src="../Images/5e62e37e0a67164875234e99baf9dfe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*Es4AZRmca6q7bjpo_R41eQ.png"/></div></figure><p id="654e" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">分子是病人<em class="lr"> j </em>在时间<em class="lr"> Tⱼ </em>失败的概率(病人<em class="lr"> j </em>的失败时间)，假定病人<em class="lr"> j </em>在时间<em class="lr"> Tⱼ </em>有死亡风险。处于风险集<em class="lr"> R </em> ( <em class="lr"> Tⱼ </em>)本质上意味着患者还没有失败，或者他们的审查日期还没有过去。分母是患者<em class="lr"> k </em>在<em class="lr"> Tⱼ </em>失败的概率，假设他们在<em class="lr"> R </em> ( <em class="lr"> Tⱼ </em>中所有患者<em class="lr"> k </em>的风险集合<em class="lr"> R </em> ( <em class="lr"> Tⱼ </em>)中。</p><p id="a184" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">在数学上，这可以表示如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es md"><img src="../Images/4818877710b6fb1496d9dfab7e754773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*2pu5ORngPR8yHLTui1pkNg.png"/></div></figure><p id="28b0" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">整个部分可能性可以写成:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es me"><img src="../Images/bfdd2a8d3224139f6f0c263dab574a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*zNmG10MmmAWW-bP_t9Khww.png"/></div></figure><p id="6cde" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">如果数据集有删截，似然表达式可以修改:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mf"><img src="../Images/f9752fd3743700efe4c44fe4b1b8816f.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*j20K6P_uZxd2fNEjeJKKCQ.png"/></div></div></figure><p id="06d7" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">其中<em class="lr"> δⱼ </em>在患者被审查时为0，否则为1。为简单起见，我们不考虑对其余的数学结果或代码进行审查。</p><h1 id="d30a" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">3.最大化可能性</h1><p id="1448" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated"><strong class="ki hj">在继续之前，关于符号的一个注意事项:</strong> <br/>因为我们最终将在代码中实现下面的数学，所以清楚符号是很重要的，特别是矩阵/向量代数:</p><p id="fc63" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">1.<em class="lr"> X </em>是一个<em class="lr"> n </em> × <em class="lr"> p </em>矩阵，代表<em class="lr"> p </em>对于<em class="lr"> n </em>患者<br/> 2的协变量。<em class="lr"> xⱼ </em>是一个1 × <em class="lr"> p </em>向量，代表患者<em class="lr">j</em>t52】3的<em class="lr"> p </em>协变量。<em class="lr"> β </em>是一个<em class="lr"> p </em> × 1向量，代表<em class="lr"> p </em>协变量<br/> 4的系数。基于上面的符号，<em class="lr"> Xβ </em>产生一个<em class="lr"> n </em> × 1的向量，而<em class="lr"> xⱼβ </em>产生一个长度为1的向量。</p><p id="5a2e" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">正如在许多MLE的情况下，最大化对数似然更容易，并且等价于最大化似然，因为<em class="lr">f</em>(<em class="lr">x</em>)=<em class="lr">log</em>(<em class="lr">x</em>)是单调递增的。Cox模型的部分对数似然性为:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mg"><img src="../Images/442971acc53db496411dc2392e428ec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*KL8CD1HvUVLa36ZKw0Dqnw.png"/></div></figure><p id="b349" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">对<em class="lr"> β </em>求导:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mg"><img src="../Images/ed91193c9cb92ee2bd53507eab7a2c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*4-an12rV3SjsoBT18XR5Ag.png"/></div></figure><p id="12e1" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">注意，由于我们对<em class="lr"> β </em>求导，这是一个<em class="lr"> p </em> × 1的向量，我们期望得到一个<em class="lr"> p </em> × 1的维导数，或梯度。由于<em class="lr"> xⱼ </em>的尺寸为<em class="lr"> p </em> × 1，x_ke^{x_k\beta}$的尺寸为<em class="lr"> p </em> × 1，而$e^{x_k\beta}$的尺寸为1 × 1，因此整体渐变的尺寸为<em class="lr"> p </em> × 1。</p><p id="e3d5" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">此时，我们已经从技术上完成了实现Cox模型所需的所有数学计算。然而，上面的梯度公式只允许我们估计<em class="lr"> β̂ </em>，而不是它的方差。为了估计<em class="lr"> β̂ </em>的方差，我们可以使用似然二阶导数的负逆。这涉及到商法则(和一些链式法则):</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mh"><img src="../Images/ef51cbf230030d3c0787a70c2e17ea3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*IXaywjzBf5rL9GnkRydS3A.png"/></div></figure><p id="b442" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">可能性相对于<em class="lr"> β </em>的二阶导数为:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mi"><img src="../Images/4a236a1f4e55b25332e13fdc0e268479.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sc6Maw3S1gyCqO2imsLJZw.png"/></div></div></figure><p id="2f24" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">对<em class="lr"> β </em>求导需要注意矩阵维数。计算部分\beta}x_ke^{x_k\beta}$需要将$x_ke^{x_k\beta}$乘以x_k美元。因为二阶导数必须有维度<em class="lr"> p </em> × 1，所以R(T_j)}x_ke^{x_k\beta}$中的$ \ frac { \ partial } { \ partial \ beta } \ sum _ { k \也必须有维度<em class="lr"> p </em> × 1。记住这一点，我们可以看到＄\ frac { \ partial } \beta}x_ke^{x_k\beta}$必须是＄x _ k e^{x_k\beta}$，其中＄x _ k＄不是$x_k^Tx_k$或$x_kx_k^T$，而是＄x _ k＄的元素方。这是我们保持理想尺寸的唯一方法。</p><p id="58d0" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">我们可以把这个二阶导数代入方差公式来估计方差。现在我们有了实现我们的模型所需的所有数学结果！</p><h1 id="e3d6" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">4.找到最小化梯度(和最大化可能性)的参数值</h1><p id="f1fc" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">我们关于部分可能性的零导数的等式是</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mj"><img src="../Images/8190a7e159461a253e6f4519a01a2d96.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*dkURrud-G4FVBBfsbFT_2g.png"/></div></figure><p id="8dab" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">解析求解<em class="lr"> β </em>是不可能的，因此我们将采用牛顿-拉夫森算法来寻找<em class="lr"> β </em>的最优值。</p><p id="2df8" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">牛顿-拉夫森算法是标准梯度下降程序的变体:</p><p id="860f" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">1.先说一些初步的猜测<em class="lr">β̂₀</em>2<br/>。计算<em class="lr"> β </em> = <em class="lr"> β̂₀ </em> <br/> 3处的似然梯度。更新你最初的猜测<em class="lr">β̂₁</em>=<em class="lr">β̂₀</em>+<em class="lr">θ</em>*<em class="lr">u</em>(<em class="lr">β̂₀</em>)其中<em class="lr"> U </em>是梯度<em class="lr"> θ </em>是学习率<br/> 4。重复步骤2-3，直到<em class="lr"> U </em> ( <em class="lr"> β̂n </em>)收敛(希望为零！)</p><p id="869b" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">梯度下降背后的直觉如下:我们可以将似然函数想象成具有某个最大值的曲线。你最初的猜测<em class="lr"> β̂₀ </em>会把你放在曲线上的某个地方，但你可能不会在最大值(除非你最初的猜测非常幸运)。为了弄清楚你是在曲线向上倾斜的部分还是向下倾斜的部分，求梯度。如果它是正的，你在向上倾斜的部分，你应该继续向正的方向移动以达到最大值。如果它是负的，你在向下倾斜的部分，你应该向负方向移动以达到最大值。步骤3通过将缩放的梯度添加到初始猜测来更新初始猜测。比例参数$\theta$是学习率，通常是0到1之间的一个小数字。</p><p id="e138" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">牛顿-拉夫逊的唯一区别是<em class="lr"> θ </em> = Vâr( <em class="lr"> β̂n </em> ₋₁).这在计算上更昂贵，因为你必须在每一步计算方差，但导致更快的收敛。</p><h1 id="af97" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">履行</h1><h2 id="169a" class="mk jp hi bd jq ml mm mn ju mo mp mq jy kp mr ms ka kt mt mu kc kx mv mw ke mx bi translated">设置参数</h2><pre class="iy iz ja jb fd my lf mz na aw nb bi"><span id="e34e" class="mk jp hi lf b fi nc nd l ne nf">library(survival)<br/>library(ggplot2)<br/>library(dplyr)</span></pre><p id="593c" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">牛顿-拉夫森收敛很快，所以我们可以把迭代次数设为10。瑟瑙和格兰伯奇建议将<em class="lr"> β̂₀ </em> = <strong class="ki hj">设为0。</strong></p><pre class="iy iz ja jb fd my lf mz na aw nb bi"><span id="b669" class="mk jp hi lf b fi nc nd l ne nf">ITERS &lt;- 10<br/>INITIAL_BETA &lt;- 0</span></pre><h2 id="653f" class="mk jp hi bd jq ml mm mn ju mo mp mq jy kp mr ms ka kt mt mu kc kx mv mw ke mx bi translated">助手功能</h2><p id="b472" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">我们可以为以下项目编写助手函数:</p><ol class=""><li id="8d0a" class="lg lh hi ki b kj li km lj kp lk kt ll kx lm lb ln lo lp lq bi translated">给定时间<em class="lr"> t </em>寻找风险集<em class="lr"> R </em> ( <em class="lr"> t </em></li></ol><pre class="iy iz ja jb fd my lf mz na aw nb bi"><span id="88ad" class="mk jp hi lf b fi nc nd l ne nf">GetRiskSet &lt;- function(time_of_interest,<br/>                       entry_vector,<br/>                       time_vector,<br/>                       event_vector) {<br/>  return(which((time_of_interest &gt;= entry_vector) &amp; ((time_vector == time_of_interest &amp; event_vector == 1) | (time_vector &gt; time_of_interest))))<br/>}</span></pre><p id="0d73" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">2.计算可能性的梯度</p><pre class="iy iz ja jb fd my lf mz na aw nb bi"><span id="382a" class="mk jp hi lf b fi nc nd l ne nf">CoxGradient &lt;- function(beta,<br/>                        Xs,<br/>                        entry,<br/>                        Ts,<br/>                        event) {<br/>  p &lt;- ncol(Xs)<br/>  <br/>  gradient &lt;- apply(cbind(Ts, Xs), 1, <br/>                    function(df){<br/>                      <br/>                      df &lt;- matrix(df, nrow = 1)<br/>                      ts &lt;- df[, 1]<br/>                      xs &lt;- df[, 2:ncol(df)]<br/>                      X_risk_set &lt;- Xs[GetRiskSet(ts, entry, Ts, event),] %&gt;% <br/>                        matrix(ncol = ncol(Xs))<br/>                      <br/>                      t1 &lt;- t(X_risk_set) %*% exp(X_risk_set %*% beta)<br/>                      t2 &lt;- sum(exp(X_risk_set %*% beta))<br/>                      <br/>                      return(xs - t1 / t2)<br/>                      <br/>                    }) %&gt;% <br/>    matrix(nrow = p) %&gt;%<br/>    rowSums()<br/>  <br/>  return(gradient)<br/>  <br/>}</span></pre><p id="3ae2" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">3.计算可能性的方差</p><pre class="iy iz ja jb fd my lf mz na aw nb bi"><span id="9997" class="mk jp hi lf b fi nc nd l ne nf">CoxVariance &lt;- function(beta,<br/>                        Xs,<br/>                        entry,<br/>                        Ts,<br/>                        event) {<br/>  <br/>  p &lt;- ncol(Xs)<br/>  <br/>  variance &lt;- apply(cbind(Ts, Xs), 1, <br/>                    function(df){<br/>                      <br/>                      df &lt;- matrix(df, nrow = 1)<br/>                      ts &lt;- df[, 1]<br/>                      xs &lt;- df[, 2:ncol(df)]<br/>                      X_risk_set &lt;- Xs[GetRiskSet(ts, entry, Ts, event),] %&gt;% <br/>                        matrix(ncol = ncol(Xs))<br/>                      <br/>                      sum1 &lt;- sum(exp(X_risk_set %*% beta))<br/>                      sum2 &lt;- rowSums(t(X_risk_set^2) %*% exp(X_risk_set %*% beta))<br/>                      sum3 &lt;- rowSums(t(X_risk_set) %*% exp(X_risk_set %*% beta))^2<br/>                      <br/>                      return(- (sum1 * sum2 - sum3) / sum1^2)<br/>                      <br/>                    }) %&gt;%<br/>    matrix(nrow = p) %&gt;%<br/>    rowSums()<br/>  <br/>  return(-1 / variance)<br/>  <br/>}</span></pre><p id="0d3c" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">4.绘制我们的梯度随时间的变化来观察收敛</p><pre class="iy iz ja jb fd my lf mz na aw nb bi"><span id="19fc" class="mk jp hi lf b fi nc nd l ne nf">GradientPlot &lt;- function(gradients){<br/>  <br/>  gradient_track &lt;- reshape2::melt(gradients) %&gt;%<br/>    `names&lt;-`(c("iteration", "beta", "gradient"))<br/>  <br/>  p &lt;- ggplot(gradient_track, aes(y = gradient, x = iteration)) +<br/>    geom_line() +<br/>    facet_wrap(~factor(beta))<br/>  <br/>  return(p)<br/>  <br/>}</span></pre><p id="47f5" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">5.绘制我们估计的<em class="lr"> β̂ </em>与来自<code class="du lc ld le lf b">survival::coxph()</code>函数的那些比较</p><pre class="iy iz ja jb fd my lf mz na aw nb bi"><span id="9344" class="mk jp hi lf b fi nc nd l ne nf">BetaPlot &lt;- function(store_betas, store_variance, coxph_output){<br/>  <br/>  variance_track &lt;- reshape2::melt(store_variance) %&gt;%<br/>    `names&lt;-`(c("iteration", "variable", "coxph_AS_variance"))<br/>  <br/>  plot_df &lt;- reshape2::melt(store_betas) %&gt;%<br/>    `names&lt;-`(c("iteration", "variable", "coxph_AS_beta")) %&gt;%<br/>    left_join(coxph_output, by = "variable") %&gt;%<br/>    left_join(variance_track, by = c("variable", "iteration")) %&gt;%<br/>    mutate(coxph_AS_lci = coxph_AS_beta - 1.96 * sqrt(coxph_AS_variance),<br/>           coxph_AS_uci = coxph_AS_beta + 1.96 * sqrt(coxph_AS_variance))<br/>  <br/>  ggplot(plot_df) +<br/>    geom_line(aes(y = coxph_AS_beta, x = iteration), size = .8) + <br/>    geom_line(aes(y = coxph_AS_lci, x = iteration), linetype = "dashed", size = .8) +<br/>    geom_line(aes(y = coxph_AS_uci, x = iteration), linetype = "dashed", size = .8) +<br/>    geom_line(aes(y = coxph_beta, x = iteration, color = "red")) +<br/>    geom_line(aes(y = coxph_lci, x = iteration), linetype = "dashed", color = "red") +<br/>    geom_line(aes(y = coxph_uci, x = iteration), linetype = "dashed", color = "red") +<br/>    facet_wrap(~factor(variable_name)) +<br/>    ylab("beta estimate") +<br/>    theme(legend.position = "none")<br/>  <br/>}</span></pre><p id="3fb8" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">6.一个包装器函数把它们放在一起</p><pre class="iy iz ja jb fd my lf mz na aw nb bi"><span id="e547" class="mk jp hi lf b fi nc nd l ne nf">coxph_AS &lt;- function(formula, data){<br/> <br/>  # Fit using coxph()<br/>  model &lt;- coxph(formula, data)<br/>  cox_beta &lt;- data.frame(variable = 1:length(coef(model)), <br/>                         coxph_beta = coef(model))<br/>  cox_ci &lt;- cbind(1:length(coef(model)), confint(model)) %&gt;% <br/>    `colnames&lt;-`(c("variable", "coxph_lci", "coxph_uci")) %&gt;%<br/>    data.frame()<br/>  coxph_output &lt;- left_join(cox_beta, cox_ci, by = "variable") %&gt;%<br/>    mutate(variable_name = names(coef(model)))<br/> <br/>  # Load data<br/>  x &lt;- data %&gt;% select(all.vars(formula[-1])) %&gt;% as.matrix<br/>  lhs &lt;- setdiff(all.vars(formula), all.vars(formula[-1]))<br/>  <br/>  if (length(lhs) == 1) { # no censoring<br/>    times &lt;- data %&gt;% select(all.vars(formula)[1]) %&gt;% as.matrix<br/>    event &lt;- rep(1, dim(times)[1]) %&gt;% as.matrix<br/>    entry &lt;- rep(0, dim(times)[1]) %&gt;% as.matrix<br/>  } else if (length(lhs) == 2) { # censoring but no entry time specified<br/>    times &lt;- data %&gt;% select(all.vars(formula)[1]) %&gt;% as.matrix<br/>    event &lt;- data %&gt;% select(all.vars(formula)[2]) %&gt;% as.matrix<br/>    entry &lt;- rep(0, dim(times)[1]) %&gt;% as.matrix<br/>  } else if (length(lhs) == 3) { # censoring but no entry time specified<br/>    times &lt;- data %&gt;% select(all.vars(formula)[2]) %&gt;% as.matrix<br/>    event &lt;- data %&gt;% select(all.vars(formula)[3]) %&gt;% as.matrix<br/>    entry &lt;- data %&gt;% select(all.vars(formula)[1]) %&gt;% as.matrix<br/>  }</span><span id="bcfa" class="mk jp hi lf b fi ng nd l ne nf"># Initialize matrices<br/>  store_gradient &lt;- matrix(NA, nrow = ITERS, ncol = ncol(x))<br/>  store_betas &lt;- matrix(NA, nrow = ITERS, ncol = ncol(x))<br/>  store_variance &lt;- matrix(NA, nrow = ITERS, ncol = ncol(x))<br/>  beta &lt;- matrix(rep(INITIAL_BETA, ncol(x)), nrow = ncol(x))<br/>  <br/>  # Newton-Raphson iterations<br/>  for(i in 1:ITERS){<br/>    store_gradient[i,] &lt;- CoxGradient(beta, x, entry, times, event)<br/>    store_variance[i,] &lt;- CoxVariance(beta, x, entry, times, event)<br/>    store_betas[i,] &lt;- beta &lt;- beta + store_gradient[i,] * store_variance[i,]<br/>  } <br/>  <br/>  # Plot<br/>  beta_plot &lt;- BetaPlot(store_betas, store_variance, coxph_output)<br/>  gradient_plot &lt;- GradientPlot(store_gradient)<br/>  <br/>  # Final output data.frame<br/>  coxph_output$coxph_AS_beta &lt;- store_betas[i,]<br/>  coxph_output$coxph_AS_lci &lt;- store_betas[i,] - 1.96 * sqrt(store_variance[i,])<br/>  coxph_output$coxph_AS_uci &lt;- store_betas[i,] + 1.96 * sqrt(store_variance[i,])<br/>  <br/>  coxph_output &lt;- coxph_output %&gt;%<br/>    transmute(variable = variable_name,<br/>              beta = coxph_beta,<br/>              beta_AS = coxph_AS_beta,<br/>              LCI = coxph_lci,<br/>              LCI_AS = coxph_AS_lci,<br/>              UCI = coxph_uci,<br/>              UCI_AS = coxph_AS_uci)<br/>  <br/>  return(list(beta_plot = beta_plot,<br/>              gradient_plot = gradient_plot,<br/>              model_output = coxph_output))<br/>}</span></pre><h1 id="92a5" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">测试它</h1><pre class="iy iz ja jb fd my lf mz na aw nb bi"><span id="7a85" class="mk jp hi lf b fi nc nd l ne nf">data(veteran)<br/>set.seed(12345)<br/>sim &lt;- coxph_AS(Surv(time) ~ trt + karno + age, data = veteran)</span></pre><p id="8760" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">比较两个模型的数字输出表明，我们的估计值(带有后缀“_AS”)与<code class="du lc ld le lf b">survival::coxph()</code>非常接近。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nh"><img src="../Images/931cb4cfcb2902478d7762329fa76e41.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*2PSUCd20hvGpAQv0YLBp9g.png"/></div></figure><p id="49dd" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">我们还可以看到我们的<em class="lr"> β̂ </em>估计值和相应的置信区间与<code class="du lc ld le lf b">survival::coxph()</code>(红色)相比如何。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ni"><img src="../Images/bc8ddf9c862e2920bd0035ecaa29a92b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*ZQ8DBM-NmHo0mTViRe_fmg.png"/></div></figure><p id="4597" class="pw-post-body-paragraph kg kh hi ki b kj li ij kl km lj im ko kp lx kr ks kt ly kv kw kx lz kz la lb hb bi translated">最后，我们可以看到如何(迅速！)我们的梯度收敛到零。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ni"><img src="../Images/0a309b57f4e7ed5c7e637a0a026b4482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*t2Ze-pdozw4epDagi7jHTw.png"/></div></figure><h1 id="f5e8" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">参考</h1><ul class=""><li id="158a" class="lg lh hi ki b kj kk km kn kp nj kt nk kx nl lb nm lo lp lq bi translated"><a class="ae jn" href="https://www4.stat.ncsu.edu/~dzhang2/st745/chap6.pdf" rel="noopener ugc nofollow" target="_blank">https://www4.stat.ncsu.edu/~dzhang2/st745/chap6.pdf</a></li><li id="ac1c" class="lg lh hi ki b kj ls km lt kp lu kt lv kx lw lb nm lo lp lq bi translated"><a class="ae jn" href="http://www.math.ucsd.edu/~rxu/math284/slect5.pdf" rel="noopener ugc nofollow" target="_blank">http://www.math.ucsd.edu/~rxu/math284/slect5.pdf</a></li><li id="bfcf" class="lg lh hi ki b kj ls km lt kp lu kt lv kx lw lb nm lo lp lq bi translated"><a class="ae jn" href="https://web.stanford.edu/~lutian/coursepdf/unitcox1.pdf" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~lutian/coursepdf/unitcox1.pdf</a></li><li id="73b1" class="lg lh hi ki b kj ls km lt kp lu kt lv kx lw lb nm lo lp lq bi translated">Therneau和Grambsch 2000,《生存数据建模》, Springer</li></ul></div></div>    
</body>
</html>