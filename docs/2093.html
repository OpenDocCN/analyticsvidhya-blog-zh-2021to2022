<html>
<head>
<title>The K-Nearest Neighbor (kNN) Machine Learning algorithm-Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-最近邻(kNN)机器学习算法-第2部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-k-nearest-neighbor-knn-machine-learning-algorithm-part-2-8bdc9a05c041?source=collection_archive---------6-----------------------#2021-04-04">https://medium.com/analytics-vidhya/the-k-nearest-neighbor-knn-machine-learning-algorithm-part-2-8bdc9a05c041?source=collection_archive---------6-----------------------#2021-04-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/6e168a483a69c708ea6d329d6014e09b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mkOw8cgteG-Fulwt0cWXVw.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">妮娜·斯特雷尔在<a class="ae it" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><h1 id="0031" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">到目前为止，在上一篇文章中，我们讨论了kNN算法的某些方面，在这篇文章中，我们还将进一步讨论该算法的一些方面和行为。</h1><p id="df24" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">关键主题:</p><ul class=""><li id="3a24" class="kq kr hh ju b jv ks jz kt kd ku kh kv kl kw kp kx ky kz la bi translated">目标不平衡问题</li><li id="674e" class="kq kr hh ju b jv lb jz lc kd ld kh le kl lf kp kx ky kz la bi translated">kNN作为一种回归技术</li><li id="8df1" class="kq kr hh ju b jv lb jz lc kd ld kh le kl lf kp kx ky kz la bi translated">标准化的重要性</li><li id="36eb" class="kq kr hh ju b jv lb jz lc kd ld kh le kl lf kp kx ky kz la bi translated">输入缺失值</li></ul><p id="71ca" class="pw-post-body-paragraph js jt hh ju b jv ks jx jy jz kt kb kc kd lg kf kg kh lh kj kk kl li kn ko kp ha bi translated">让我们开始吧。</p><h1 id="87c9" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated"><strong class="ak"> 1。误导多数阶级标签— <em class="lj">目标失衡问题</em> </strong></h1><p id="adc6" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">想象一下，我们试图检测两种不同类型的信号，一种来自保险箱，另一种是敌人的信号，这是一项极其关键的任务，就像入侵检测系统一样。问题是，罕见阶层的预测能力将不如多数阶层。在这种情况下，模型更多地暴露于从多数阶级学习，因为与少数阶级相比，它占主导地位。</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es lk"><img src="../Images/ddfaa5c85d9a6f7d0ad73bb6ab9afb17.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*ogNmKWLVo3Y9NBGEMpyplQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">目标失衡问题</figcaption></figure><blockquote class="lp lq lr"><p id="5ef0" class="js jt ls ju b jv ks jx jy jz kt kb kc lt lg kf kg lu lh kj kk lv li kn ko kp ha bi translated">python中有像<strong class="ju hi"> imblearn </strong>这样的预加载库来应对这些问题。</p></blockquote><h1 id="80ec" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">2.作为回归的最近邻技术——局部加权回归</h1><p id="a5fc" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">你可能会认为这是用词不当。更常见的是，kNN以其分类用途而闻名，但不太为人所知的是，它也可以用于回归问题。这种变体被称为<strong class="ju hi"><em class="ls">【LWR】</em></strong>，因为它使用<em class="ls">距离加权</em>训练示例来形成局部近似。它在本地被称为<strong class="ju hi"/>，因为它仅基于查询点附近的数据，而<strong class="ju hi">被称为加权</strong>，因为每个训练示例的贡献都根据其与查询点的距离进行加权。</p><blockquote class="lp lq lr"><p id="902b" class="js jt ls ju b jv ks jx jy jz kt kb kc lt lg kf kg lu lh kj kk lv li kn ko kp ha bi translated">请注意，kNN回归使用与kNN分类相同的<strong class="ju hi">距离函数，如L1、L2或闵可夫斯基距离或其任何子距离。</strong></p></blockquote><h1 id="358a" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">3.标准化的重要性</h1><p id="aedc" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">规范化方面在迎合kNN的时候是极其<strong class="ju hi">非</strong> - <strong class="ju hi">琐碎</strong>的。原因是kNN算法受到<strong class="ju hi"> <em class="ls">幅度</em> </strong>的影响，尤其是在使用欧几里德距离时。例如:假设我们在一个有3个特征的数据集上应用kNN。第一个特征的范围是<em class="ls">1–10(正常)</em>，第二个特征的范围是<em class="ls">1–20(正常)</em>，最后一个特征的范围是<em class="ls">1–1000(不正常)</em>。在这种情况下，大多数聚类将基于最后一个要素生成，因为与1–1000相比，1到10和1–20之间的差异较小。为了避免这种错误分类，我们应该<strong class="ju hi">标准化特征变量</strong>。</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es lw"><img src="../Images/90c64dfaeecc60be1118878b67062e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*gySqS8Yk8l5WGEfYtYte8Q.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">三维点集；p是q查询点</figcaption></figure><blockquote class="lp lq lr"><p id="ee14" class="js jt ls ju b jv ks jx jy jz kt kb kc lt lg kf kg lu lh kj kk lv li kn ko kp ha bi translated"><em class="hh">执行标准化有不同的技术，包括</em>最小-最大标准化(更受欢迎)、标准分数标准化，甚至加权平均。</p></blockquote><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lx"><img src="../Images/001a4bf17e796639bd80264de324d83b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dROCNIa96eSoTUPqZkEzoQ.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">加权平均归一化—谢泼德法</figcaption></figure><h1 id="3647" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">4.缺失值的插补— KNNImputer</h1><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ly"><img src="../Images/1d7d999b72665721d519aa00263e0c7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pn98uyPHjwUa0TE_4Fzrpg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://unsplash.com/@theeastlondonphotographer" rel="noopener ugc nofollow" target="_blank">Ehimetalor Akhere Unuabona</a>在<a class="ae it" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="c36f" class="pw-post-body-paragraph js jt hh ju b jv ks jx jy jz kt kb kc kd lg kf kg kh lh kj kk kl li kn ko kp ha bi translated">直接到如何表演的地步？</p><ul class=""><li id="a83e" class="kq kr hh ju b jv ks jz kt kd ku kh kv kl kw kp kx ky kz la bi translated">如果一个样本有<strong class="ju hi">个以上的特征缺失</strong>，那么该样本的相邻特征可能不同，这取决于被估算的特定特征。</li><li id="44cf" class="kq kr hh ju b jv lb jz lc kd ld kh le kl lf kp kx ky kz la bi translated">当可用的邻居数量较少且没有定义到训练集的距离时，在插补期间使用该特征的<strong class="ju hi">训练集平均值</strong>。</li><li id="7c7f" class="kq kr hh ju b jv lb jz lc kd ld kh le kl lf kp kx ky kz la bi translated">如果至少有一个邻居具有规定的距离，则在插补期间将使用剩余邻居的<strong class="ju hi">加权或未加权平均值</strong>。</li><li id="1e24" class="kq kr hh ju b jv lb jz lc kd ld kh le kl lf kp kx ky kz la bi translated">如果某个特征在训练中总是缺失，它将被删除。</li></ul><blockquote class="lp lq lr"><p id="9c2e" class="js jt ls ju b jv ks jx jy jz kt kb kc lt lg kf kg lu lh kj kk lv li kn ko kp ha bi translated">感谢sklearn的ML库，它有<code class="du lz ma mb mc b"><a class="ae it" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute" rel="noopener ugc nofollow" target="_blank">sklearn.impute</a></code>。KNNImputer可以作为解决这个问题的方法。</p></blockquote><p id="4fbe" class="pw-post-body-paragraph js jt hh ju b jv ks jx jy jz kt kb kc kd lg kf kg kh lh kj kk kl li kn ko kp ha bi translated">这是奖金…</p><figure class="ll lm ln lo fd ii"><div class="bz dy l di"><div class="md me l"/></div></figure><figure class="ll lm ln lo fd ii"><div class="bz dy l di"><div class="mf me l"/></div></figure><p id="6a42" class="pw-post-body-paragraph js jt hh ju b jv ks jx jy jz kt kb kc kd lg kf kg kh lh kj kk kl li kn ko kp ha bi translated">这就是最近邻算法的全部内容。感谢您的宝贵时间:)</p></div></div>    
</body>
</html>