<html>
<head>
<title>Random Forest Classifier and its Hyperparameters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林分类器及其超参数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/random-forest-classifier-and-its-hyperparameters-8467bec755f6?source=collection_archive---------0-----------------------#2021-02-23">https://medium.com/analytics-vidhya/random-forest-classifier-and-its-hyperparameters-8467bec755f6?source=collection_archive---------0-----------------------#2021-02-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3baf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">了解随机森林分类器的工作原理</em></p><p id="551e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据科学提供了大量的分类算法，如支持向量机、朴素贝叶斯分类器、逻辑回归、决策树等。但是在分类器层次的顶端附近是<strong class="ih hj">随机森林分类器</strong>(也有随机森林回归器，但是那是另一天的主题)。</p><p id="38f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了理解随机森林分类器的工作，我们需要首先理解决策树的概念。</p><p id="1abb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您不知道决策树分类器的概念，请在开始学习随机森林算法的工作性质之前，花一些时间了解一下<a class="ae je" href="https://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">决策树分类器</strong> </a>是如何工作的。如果你想学习决策树分类器的实现，你可以从下面的文章中找到它。</p><p id="97fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae je" href="https://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/" rel="noopener ugc nofollow" target="_blank">用Python实现决策树分类器</a></p><p id="2c2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae je" href="https://dataaspirant.com/2017/02/03/decision-tree-classifier-implementation-in-r/" rel="noopener ugc nofollow" target="_blank">用R编程语言构建决策树分类器</a></p><p id="ba1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae je" href="https://dataaspirant.com/2017/04/21/visualize-decision-tree-python-graphviz/" rel="noopener ugc nofollow" target="_blank">如何可视化建模的决策树分类器</a></p><h1 id="f3fd" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">基本决策树概念:</h1><p id="adfe" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">决策树的概念更适合于基于规则的系统。给定具有目标和特征的训练数据集，决策树算法将得出某个<strong class="ih hj">规则集</strong>。同一组规则可用于对测试数据集执行预测。</p><p id="5f01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设你想预测你的朋友会不会喜欢新上映的动画电影。为了对决策树进行建模，您将使用训练数据集，就像您的朋友在过去的电影中喜欢的动画卡通人物一样。</p><p id="e8ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，一旦你将目标数据集传递给决策树分类器，你的朋友就会<strong class="ih hj">喜欢或不喜欢这部电影，</strong>。决策树将开始建立规则，将你朋友喜欢的角色作为节点，将目标喜欢或不喜欢作为叶节点。通过考虑从根节点到叶节点的路径。你可以得到规则。</p><p id="814a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单的规则可能是，如果某个<strong class="ih hj"> <em class="jd"> x </em> </strong>角色扮演主角，那么你的朋友会喜欢这部电影。你可以根据这个例子想出更多的规则。</p><p id="cf0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后预测<strong class="ih hj">你的朋友</strong> <strong class="ih hj">是否会喜欢这部电影</strong>。你只需要检查由决策树创建的规则来预测你的朋友是否会喜欢新上映的电影。</p><p id="0b5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在决策树算法中，使用<strong class="ih hj"> <em class="jd">信息增益和基尼系数计算</em> </strong>来计算这些节点并形成规则。</p><p id="7052" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在随机森林算法中，寻找根节点和分裂特征节点的过程将随机发生，而不是使用信息增益或基尼指数来计算根节点。将在下一节中详细讨论它。</p><p id="9b43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">随机森林分类器:</strong></p><p id="f37e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">具有低偏差和高方差的决策树倾向于过度拟合数据。因此<a class="ae je" href="../Blog%201%20-%20Ensemble%20Learning/ENSEMBLE%20METHODS.docx" rel="noopener ugc nofollow" target="_blank">装袋技术</a>成为降低决策树方差的一个非常好的解决方案。除了使用底层模型作为决策树的bagging模型之外，我们还可以使用随机森林，这对于决策树来说更加方便和优化。bagging的主要问题是采样数据集之间没有太多的独立性，即存在相关性。</p><p id="050c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林相对于bagging模型的优势在于，随机森林对bagging模型的工作算法进行了<strong class="ih hj">调整</strong>，以降低树中的相关性。这个想法是在创建树的时候引入更多的随机性，这将有助于减少相关性。</p><p id="25e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林是一种<strong class="ih hj">监督学习算法</strong>，用于分类和回归。但是，它主要用于分类问题。众所周知，森林是由树组成的，更多的树意味着更健壮的森林。类似地，随机森林算法在数据样本上创建决策树，然后从每个样本中获得预测，最后选择最佳解决方案。</p><p id="ea03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一般来说，森林里的树越多，森林看起来就越茂盛。同样，在随机森林分类器中，<strong class="ih hj">森林中的树木数量</strong>越高，则<strong class="ih hj">结果的准确度</strong>越高。</p><p id="6ac3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林背后的基本概念是简单而强大的——群体的智慧。用数据科学的话说，随机森林模型如此有效的原因是:</p><p id="e93f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">大量相对不相关的模型(树)作为一个委员会运作，将胜过任何一个单独的组成模型。</em> </strong></p><p id="c12b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模型之间的低相关性是关键。</p><p id="af57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不相关的模型可以产生比任何单个预测都更准确的集合预测。产生这种奇妙效果的原因是，这些树相互保护，不受各自错误的影响(只要它们不总是朝着同一个方向出错)。虽然有些树可能是错误的，但许多其他的树将是正确的，所以作为一个群体，这些树可以朝着正确的方向前进。因此，随机森林运行良好的先决条件是:</p><p id="774c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.我们的特征中需要有一些实际的信号，以便使用这些特征建立的模型比随机猜测做得更好。</p><p id="8dde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.由单个树做出的预测(以及因此产生的误差)需要彼此具有低相关性。</p><p id="461e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林算法的伪代码可以分为两个阶段。</p><ul class=""><li id="df96" class="ki kj hi ih b ii ij im in iq kk iu kl iy km jc kn ko kp kq bi translated"><strong class="ih hj">随机森林</strong>创建伪代码。</li><li id="5775" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated"><strong class="ih hj">根据创建的随机森林分类器执行预测</strong>的伪代码。</li></ul><p id="2cb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，让我们从随机森林创建伪代码开始</p><h2 id="ff31" class="kw jg hi bd jh kx ky kz jl la lb lc jp iq ld le jt iu lf lg jx iy lh li kb lj bi translated">随机森林伪代码:</h2><p id="1e2d" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">1.就像bagging一样，使用bootstrapping从训练数据集中收集不同的样本。</p><p id="56d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.在每个样本上，我们训练我们的树模型，并且我们允许树以很高的深度生长。</p><p id="e688" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">现在，随机森林的区别在于树木是如何形成的。在bootstrapping中，我们允许所有样本数据用于分割节点，但不包括随机森林。当构建决策树时，每次发生分裂时，从总的“p”个预测值中选择“m”个预测值的随机样本。只有那些“m”个预测值被允许用于分割。</em></p><p id="aab5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">m&lt;T29】p</em></p><p id="b0e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">为什么会这样？</em></p><p id="95a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">假设在那些‘p’个预测因子中，1个预测因子很强。现在在每个样本中，这个预测器将保持最强。因此，每当为这些采样数据构建树时，所有树都会选择该预测值进行分割，从而为每个引导模型生成类似的树。这在数据集中引入了相关性，并且平均相关的数据集结果不会导致低方差。这就是为什么在随机森林中，选择分裂节点的选择是有限的，并且在树的形成中也引入了随机性。</em></p><p id="b638" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">大部分预测因子不允许考虑拆分。</em></p><p id="eeaa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">一般情况下，取‘m’的值为m ≈√p，其中‘p’为样本中预测值的个数。</em></p><p id="1f32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">当m=p时，随机森林模型变成了bagging模型。</em></p><p id="2a57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.在<strong class="ih hj">“m”</strong>特征中，使用最佳分割点计算节点<strong class="ih hj">“d”</strong>。</p><p id="7134" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.使用<strong class="ih hj">最佳分割</strong>将节点分割成<strong class="ih hj">子节点</strong>。</p><p id="1d6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.重复<strong class="ih hj"> 1到4 </strong>步骤，直到达到“l”个节点。</p><p id="7aa8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.通过重复步骤<strong class="ih hj"> 1到5</strong>n次来建立森林，以创建<strong class="ih hj">“n”个树</strong>。</p><p id="3ade" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林算法的开始是从全部<strong class="ih hj">“p”</strong>个特征中随机选择<strong class="ih hj">“m”</strong>个特征。</p><p id="a500" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下一阶段，我们将使用随机选择的<strong class="ih hj">“m”</strong>特征，通过使用<a class="ae je" href="https://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/" rel="noopener ugc nofollow" target="_blank">最佳分裂</a>方法来找到根节点。</p><p id="af92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下一阶段，我们将使用相同的最佳分割方法计算子节点。我们将重复前4个阶段，直到我们形成一个根节点，目标节点为叶节点的树。</p><p id="65af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们重复1到5个阶段来创建<strong class="ih hj">“n”</strong>随机创建的树。这些随机创建的树形成了<strong class="ih hj">随机森林。</strong></p><h2 id="8a8a" class="kw jg hi bd jh kx ky kz jl la lb lc jp iq ld le jt iu lf lg jx iy lh li kb lj bi translated">随机森林预测伪代码:</h2><p id="9fab" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">为了使用训练的随机森林算法执行预测，使用下面的伪代码。</p><p id="ad7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.采用<strong class="ih hj">测试特征</strong>并使用每个随机创建的决策树的<strong class="ih hj">规则</strong>来预测结果并存储预测结果(目标)。</p><p id="2dac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.计算每个预测目标的<strong class="ih hj">票数</strong>。</p><p id="113c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.将<strong class="ih hj">高票</strong>预测目标视为来自随机森林算法的<strong class="ih hj">最终预测</strong>。</p><p id="ecda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了使用经过训练的随机森林算法来执行预测，我们需要通过每个随机创建的树的规则来传递测试特征。假设我们从随机森林中形成了100棵随机决策树。</p><p id="f372" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个随机树将预测相同测试特征的不同目标(结果)。然后通过考虑每个预测目标，计算票数。假设100个随机决策树预测了大约3个独特的目标<strong class="ih hj"> x，y，z </strong>那么x的投票什么也不是，但是在100个随机决策树中有多少个树预测是<strong class="ih hj"> x. </strong></p><p id="84ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其他两个目标(y，z)也是如此。如果x获得最高票数。假设100个随机决策树<strong class="ih hj">中有60个</strong>树预测目标将是x。然后最终的随机树返回x作为预测目标。</p><p id="ddaa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种投票的概念被称为<strong class="ih hj">多数投票</strong>。</p><p id="b510" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林分类器的可视化表示:</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lk"><img src="../Images/e62cedd9197ea0de942c4ac566c4770d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hmtbIgxoflflJqMJ_UHwXw.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">图片提供:谷歌</figcaption></figure><p id="4157" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">随机森林分类器的Python实现:</strong></p><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es ma"><img src="../Images/00011b3280a07d198a91a19b1b2fc1fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_f8u5V9ICmD4i7clIjqdrw.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">导入所有必需的库</figcaption></figure><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mb"><img src="../Images/079ba2edbe411c93ee44a28ba32b588d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UnmRbu3yev8651ajSWX2hQ.png"/></div></div></figure><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mc"><img src="../Images/4f2165cd1d494a41048a0addcc3b0240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hBTY00600W3bLsrk0uJ0tg.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">拆分训练和测试数据</figcaption></figure><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es md"><img src="../Images/0db640b1b5fcb41aea72d1118f0b84c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KW0n3F3YiiCa1SjORlm-dg.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">将模型拟合到训练数据</figcaption></figure><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es me"><img src="../Images/72d46989349917906a66ff43f043662d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bnm4b2d_XUkaS593ZUZ22g.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">计算精确度</figcaption></figure><p id="12a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"/><strong class="ih hj">随机森林分类器的超参数:</strong></p><p id="8aa4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.<strong class="ih hj"> max_depth: </strong>随机森林中一棵树的<em class="jd"> max_depth </em>定义为根节点和叶节点之间的最长路径。</p><p id="592b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj"> min_sample_split: </strong>参数<em class="jd"><strong class="ih hj"> <em class="jd"> </em>默认= 2 </strong></em></p><p id="deca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.<strong class="ih hj"> max_leaf_nodes: </strong> <strong class="ih hj">该超参数设置了树中节点分裂的条件，从而限制了树的增长。</strong></p><p id="2c02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.<strong class="ih hj"> min_samples_leaf: </strong>这个随机森林超参数指定在分割一个节点之后，在叶节点<strong class="ih hj">中应该出现的最小样本数。<strong class="ih hj">默认= 1 </strong></strong></p><p id="06bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.<strong class="ih hj"> n_estimators: </strong>森林中的树木数量。</p><p id="724a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.<strong class="ih hj">max _ sample:</strong><em class="jd">max _ samples</em>超参数决定了原始数据集的多少部分被分配给任何一棵树。</p><p id="a288" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7.这类似于提供给随机森林中每棵树的最大特征数量。</p><p id="9d18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">8.<strong class="ih hj"> bootstrap </strong>:采样数据点的方法(有无替换)。<strong class="ih hj">默认=真</strong></p><p id="3bcc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">9.<strong class="ih hj">判据:</strong>衡量分割质量的函数。支持的标准是基尼杂质的“基尼”和信息增益的“熵”。</p><p id="c893" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">现在，手动设置超参数，并使用GridSearchCV进行超参数调整:</em></p><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mf"><img src="../Images/fafbb89543857bda558a16df7815c1f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F0Lhw_0kLQSTa7lyA-N-HA.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">使用网格搜索寻找最佳参数</figcaption></figure><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mg"><img src="../Images/e2511efeb54bb9e4d1bfe1f579ec661f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RM35FRbr-A1bFLtzqVwlYQ.png"/></div></div></figure><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es md"><img src="../Images/71d22b31de2825d8b6b4cf10f7ebe579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jb00ce7jISqJs50v2erp9Q.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">用最佳参数拟合训练数据并计算新的精度</figcaption></figure><p id="868c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">超参数调整后的最终精度有所提高。</p><p id="6888" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">随机森林分类器的优势:</strong></p><p id="168d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它通过平均或组合不同决策树的结果来克服过拟合问题。</p><p id="c44d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林比单个决策树更适用于大范围的数据项。</p><p id="9b53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林比单一决策树具有更小的方差。</p><p id="d924" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林非常灵活，并且具有非常高的准确性。</p><p id="7bcf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在随机森林算法中不需要缩放数据。即使在提供未经缩放的数据后，它也能保持良好的准确性。</p><p id="6ab3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">即使大部分数据丢失，随机森林算法也能保持良好的准确性。</p><p id="fcdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">随机森林分类器的缺点:</strong></p><p id="964f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">复杂性是随机森林算法的主要缺点。</p><p id="76b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林的构建比决策树要困难和耗时得多。</p><p id="8be3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实现随机森林算法需要更多的计算资源。</p><p id="1b4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们有一个大的决策树集合时，它就不那么直观了。</p><p id="66fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与其他算法相比，使用随机森林的预测过程非常耗时。</p><p id="3b6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结论:</strong></p><p id="86d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">至此，我们结束了对随机森林分类器的工作以及如何调整随机森林的各种超参数的讨论。为了更好地理解超参数调谐，您可以尝试自己处理“葡萄酒质量数据集”。</p><p id="63bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下载数据集—【https://www.kaggle.com/rajyellow46/wine-quality T2】</p><p id="c827" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">随机森林分类器的YouTube链接:</strong></p><p id="3317" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.统计任务:<a class="ae je" href="https://www.youtube.com/watch?v=J4Wdy0Wc_xQ" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=J4Wdy0Wc_xQ</a></p><p id="594d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.https://www.youtube.com/watch?v=nxFG5xdpDto的克里斯·纳伊克</p><p id="0f7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">看看我之前的文章！</strong></p><p id="8b29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae je" rel="noopener" href="/analytics-vidhya/ensemble-methods-bagging-boosting-and-stacking-28d006708731">综合方法——装袋、助推和堆叠</a>。</p><p id="e5ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望你喜欢这篇文章。如果你有任何问题，欢迎在下面评论。如果你想让我写一个关于机器学习的特定话题，请在下面的评论中告诉我。</p><p id="18cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读！</p><p id="1643" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献:</strong></p><p id="c571" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.<a class="ae je" href="https://ineuron.ai/home" rel="noopener ugc nofollow" target="_blank">伊内乌龙</a></p><p id="4a53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<a class="ae je" href="https://towardsdatascience.com/" rel="noopener" target="_blank">走向数据科学</a></p><p id="7abc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.<a class="ae je" href="https://dataaspirant.com/" rel="noopener ugc nofollow" target="_blank">数据野心家</a></p><p id="2ef5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.<a class="ae je" href="https://github.com/Ankit-c2104/Machine-Learning-Notes" rel="noopener ugc nofollow" target="_blank">https://github.com/Ankit-c2104/Machine-Learning-Notes</a></p></div></div>    
</body>
</html>