<html>
<head>
<title>ResNet for Image Classification.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图像分类的ResNet。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-your-own-simple-resnets-with-0-99-accuracy-on-mnsit-2fd10d1955de?source=collection_archive---------1-----------------------#2021-06-02">https://medium.com/analytics-vidhya/building-your-own-simple-resnets-with-0-99-accuracy-on-mnsit-2fd10d1955de?source=collection_archive---------1-----------------------#2021-06-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/94c37e3a28039e23e9ca13f587869875.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/1*T-AyPeCdLDzHI0R952EmYg.gif"/></div><figcaption class="il im et er es in io bd b be z dx translated">预测数字(<a class="ae ip" rel="noopener" href="/analytics-vidhya/applying-ann-digit-and-fashion-mnist-13accfc44660">来源</a>)</figcaption></figure><p id="27cf" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">ResNets或剩余网络是由微软研究团队的、何、、任少勤、提出的(<a class="ae ip" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">链接到论文</a>)。通过引入跳跃连接或快捷连接，解决了神经网络过深时的退化问题。</p><p id="32b3" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">退化问题— </strong>当一个模型越来越深，在某一点之后，模型的精度开始下降。发生这种情况是因为随着模型变得太深，各层从浅层传播信息变得困难，并且信息丢失。</p><p id="3f72" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">摘自<a class="ae ip" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank"> ResNet论文</a>:</p><blockquote class="jo jp jq"><p id="6217" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn ha bi translated">当更深的网络能够开始收敛时，退化问题就暴露出来了:随着网络深度的增加，精度达到饱和(这可能不足为奇)，然后迅速退化。出乎意料的是，这种退化不是由过度拟合引起的，并且向适当深度的模型添加更多的层会导致更高的训练误差。</p></blockquote><p id="3fd5" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">解决方案— </strong>使用跳过连接或快捷方式。它使用身份网络直接连接浅层和深层。这使得数据可以在两层之间轻松流动。如果一个层妨碍了模型的性能，该层将被跳过。因此得名跳过连接。</p><figure class="jw jx jy jz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es jv"><img src="../Images/dc2cea21ff56e9b1d323c562d579b37a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vq8kei-AblmWd23aOZlEqQ.png"/></div></div></figure><p id="271d" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">残差网络—y = f(x)+x。</strong></p><p id="40a0" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这里+x是跳过连接。f(x)表示要学习的残差映射。<strong class="is hi"> f(x) + x </strong>通过跳跃连接的元素相加得到。</p><p id="9fca" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">现在让我们看看ResNet架构。</p><figure class="jw jx jy jz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es ke"><img src="../Images/9b97e4d13ae4807e6f0b77a52cb9c8fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lxlHkQX9f5Gby2qkdQ9w6Q.png"/></div></div></figure><p id="55b6" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">第一层是7×7内核，然后是3×3内核的重复块，在每个包括2个3×3卷积层的块之后有一个跳跃连接。每当特征图减半时，滤波器的数量加倍以保持每层的时间复杂度。</p><p id="fb41" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">现在让我们来看看使Resnets与普通网络不同的部分——跳过连接。这些是内核大小为1x1的身份快捷方式。如果输入和输出维度相同，可以直接使用恒等式快捷方式。在输入和输出维度不同的情况下，我们有两种选择-</p><ol class=""><li id="8685" class="kf kg hh is b it iu ix iy jb kh jf ki jj kj jn kk kl km kn bi translated">我们可以添加零填充来保持尺寸</li><li id="57ed" class="kf kg hh is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">投影快捷方式用于匹配尺寸。</li></ol><p id="181d" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">两者都以步幅2使用。</p><p id="f40e" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">现在让我们看一个例子。你可以在这里找到MNSIT数据集的完整代码<a class="ae ip" href="https://github.com/anki08?tab=overview&amp;from=2021-05-01&amp;to=2021-05-31" rel="noopener ugc nofollow" target="_blank">。</a></p><div class="kt ku ez fb kv kw"><a href="https://github.com/anki08/MNSIT-using-ResNets" rel="noopener  ugc nofollow" target="_blank"><div class="kx ab dw"><div class="ky ab kz cl cj la"><h2 class="bd hi fi z dy lb ea eb lc ed ef hg bi translated">anki08/MNSIT-using-ResNets</h2><div class="ld l"><h3 class="bd b fi z dy lb ea eb lc ed ef dx translated">这篇文章的代码——在MNSIT上构建您自己的精度约为0.99的简单结果以运行代码——进入…</h3></div><div class="le l"><p class="bd b fp z dy lb ea eb lc ed ef dx translated">github.com</p></div></div><div class="lf l"><div class="lg l lh li lj lf lk ij kw"/></div></div></a></div><figure class="jw jx jy jz fd ii er es paragraph-image"><div class="er es ll"><img src="../Images/e7620493a3de60425cfd89c7d99be1f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*Ft2rLuO82eItlvJn5HOi9A.png"/></div><figcaption class="il im et er es in io bd b be z dx translated"><a class="ae ip" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNSIT </a></figcaption></figure><p id="c78b" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">让我们为我们的模型创建一个类。我们将分别定义每个模块，以便于理解和调试。</p><figure class="jw jx jy jz fd ii"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="b8ec" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">第一块——也就是我们自己。主块是一个7x7卷积层。我添加了3的填充和2的步幅来保持图像的尺寸。然后，我们添加一个非线性ReLU，并根据本文的架构使用maxpool将图像的维度减少2倍。</p><p id="4a3b" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">接下来，我们有3个2卷积层的块，具有3×3的内核大小，使用BatchNorm和ReLU来稳定学习过程。自我缩减采样层是我们的<strong class="is hi">跳过连接。</strong>如您所见，这些是1x1身份层。我们以线性层结束。在向前，在主层之后，在每一层，我们<strong class="is hi">添加</strong>该层的结果和前一层通过身份跳过连接传递的结果。你也可以使用<strong class="is hi"> CONCAT </strong>代替加法来添加层。</p><p id="f60a" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">现在我们有了模型，您可以继续加载和训练图像。我还添加了tensorboard日志。您可以使用以下命令运行tensorboard</p><pre class="jw jx jy jz fd lo lp lq lr aw ls bi"><span id="c5b3" class="lt lu hh lp b fi lv lw l lx ly"> tensorboard — logdir log_directory — reload_interval 1</span></pre><p id="64d2" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">火车文件由—</p><pre class="jw jx jy jz fd lo lp lq lr aw ls bi"><span id="5249" class="lt lu hh lp b fi lv lw l lx ly">python train.py --log_dir=log_directory</span></pre><p id="b70d" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">运行训练文件应该可以在10个历元内得到0.99的精度:)</p><figure class="jw jx jy jz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es lz"><img src="../Images/a4a6d0e33ff71f0386011b3c9b895085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*INuYAU-K5vSkZcchE2YJUw.png"/></div></div></figure></div></div>    
</body>
</html>