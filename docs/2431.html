<html>
<head>
<title>Improving credit card detection fraud by data oversampling using PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PySpark通过数据过采样改进信用卡欺诈检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/improving-credit-card-detection-fraud-by-data-oversampling-using-pyspark-73293a60177?source=collection_archive---------6-----------------------#2021-04-23">https://medium.com/analytics-vidhya/improving-credit-card-detection-fraud-by-data-oversampling-using-pyspark-73293a60177?source=collection_archive---------6-----------------------#2021-04-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="7eab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">欺诈活动被视为不常见或异常交易，这可能是欺诈的主要特征之一。正如《使用描述性、预测性和社交网络技术的欺诈分析:欺诈检测数据科学指南》一书的作者所指出的:</p><blockquote class="jd je jf"><p id="1d9b" class="ie if jc ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">“这使得[由于异常值的特征]很难检测欺诈，因为欺诈案例被非欺诈案例所覆盖，同时也很难从历史案例中学习，以建立一个强大的欺诈检测系统，因为只有很少的例子可用”</p></blockquote><p id="4f03" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，考虑到数据的不平衡性质，必须解决这个问题。在本帖中，我们将使用在https://www.kaggle.com/mlg-ulb/creditcardfraud的<a class="ae jj" href="https://www.kaggle.com/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank">可用的信用卡数据。此外，我们将使用PySpark，这是Python中Apache Spark的接口，因为它是处理大数据的优秀工具。</a></p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="133f" class="jt ju hh jp b fi jv jw l jx jy"># Importing some libraries</span><span id="7ea8" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">import</strong> numpy <strong class="jp hi">as</strong> np<br/><strong class="jp hi">from</strong> pyspark.sql <strong class="jp hi">import</strong> functions <strong class="jp hi">as</strong> F<br/><strong class="jp hi">from</strong> pyspark.sql.types <strong class="jp hi">import</strong> DoubleType</span><span id="b562" class="jt ju hh jp b fi jz jw l jx jy"># Reading credit card fraud transactions</span><span id="53c2" class="jt ju hh jp b fi jz jw l jx jy">df1 = spark.read.format("csv").options(header='true').load("dbfs:/FileStore/shared_uploads/eric.fontes.databricks@gmail.com/creditcard.csv")</span><span id="03ee" class="jt ju hh jp b fi jz jw l jx jy"># Some basic statistics</span><span id="c39d" class="jt ju hh jp b fi jz jw l jx jy">df_describe = df1.describe()<br/>display(df_describe)</span></pre><figure class="jk jl jm jn fd kb er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es ka"><img src="../Images/137e10d17a3f4602836a65345b8f88f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8jnooylaYYzMD1tejXX-mA.png"/></div></div></figure><p id="d3b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">的。describe方法对于显示数据的一些基本统计很重要。这个spark DataFrame对象有31列和284807行。时间特征表示本次交易与数据集中第一次交易之间经过的秒数，V1 → V28列可能是PCA降维的结果，以保护用户身份和敏感特征，金额特征是交易金额，类目标变量1代表欺诈性交易，0代表非欺诈性交易。</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="c569" class="jt ju hh jp b fi jv jw l jx jy">df2_rdd = (df_describe<br/>.rdd.filter(<br/><strong class="jp hi">lambda</strong> x: x.summary == 'mean')<br/>.map(<strong class="jp hi">lambda</strong> x: x.asDict()<br/>))</span><span id="c127" class="jt ju hh jp b fi jz jw l jx jy"># Mean<br/>float(df2_rdd.collect()[0]['V1'])<br/>Out[5]: 9.516248586879277e-16</span><span id="9d84" class="jt ju hh jp b fi jz jw l jx jy">df3_rdd = (df_describe<br/>.rdd.filter(<br/><strong class="jp hi">lambda</strong> x: x.summary == 'stddev')<br/>.map(<strong class="jp hi">lambda</strong> x: x.asDict()<br/>))</span><span id="298f" class="jt ju hh jp b fi jz jw l jx jy"># Std-deviation<br/>float(df3_rdd.collect()[0]['V1'])<br/>Out[7]: 1.9586958038574889</span></pre><p id="128f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上面的代码是使用RDD API为V1要素提取df_describe中的平均值和标准差而编写的。这是一个例子，说明我们将如何提取所有Vk，k=1…28的这些值。现在，我们将使用其他一些库，包括考拉。</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="4278" class="jt ju hh jp b fi jv jw l jx jy">#Importing some other libraries</span><span id="2f0e" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">from</strong> random <strong class="jp hi">import</strong> randrange, uniform<br/><strong class="jp hi">from</strong> pyspark.sql <strong class="jp hi">import</strong> SparkSession, Row<br/><strong class="jp hi">from</strong> pyspark.sql.types <strong class="jp hi">import</strong> ArrayType, StructField, StructType, StringType, IntegerType, DecimalType, FloatType<br/><strong class="jp hi">from</strong> decimal <strong class="jp hi">import</strong> Decimal<br/><strong class="jp hi">import</strong> databricks.koalas <strong class="jp hi">as</strong> ks</span></pre><p id="f76a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们可以创建新的过采样数据帧了。为此，我们必须注意，所有Vk特征都类似于正态分布(这不是我们在此证明这一假设的目的)，并且金额特征可以被均匀地过采样。我们的过采样方法包括使用正态和均匀分布中的随机数对原始数据进行上采样。我们已经为正常特征的每个新特征使用了“原始”平均值和标准差。这个讨论可以转换成下面的代码:</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="2632" class="jt ju hh jp b fi jv jw l jx jy"># Oversampling the initial data with random numbers within normal distributions for the PCA variables and</span><span id="d969" class="jt ju hh jp b fi jz jw l jx jy"># random numbers within uniform distribution for the Amount feature</span><span id="d9c3" class="jt ju hh jp b fi jz jw l jx jy">dict1 = {'V1':np.random.normal(loc=float(df2_rdd.collect()[0]['V1']), scale=float(df3_rdd.collect()[0]['V1']), size=(283823)).tolist(), 'V2': np.random.normal(loc=float(df2_rdd.collect()[0]['V2']), scale=float(df3_rdd.collect()[0]['V2']), size=(283823)).tolist(), 'V3': np.random.normal(loc=float(df2_rdd.collect()[0]['V3']), scale=float(df3_rdd.collect()[0]['V3']), size=(283823)).tolist(), 'V4': np.random.normal(loc=float(df2_rdd.collect()[0]['V4']), scale=float(df3_rdd.collect()[0]['V4']), size=(283823)).tolist(), 'V5': np.random.normal(loc=float(df2_rdd.collect()[0]['V5']), scale=float(df3_rdd.collect()[0]['V5']), size=(283823)).tolist(), 'V6': np.random.normal(loc=float(df2_rdd.collect()[0]['V6']), scale=float(df3_rdd.collect()[0]['V6']), size=(283823)).tolist(), 'V7': np.random.normal(loc=float(df2_rdd.collect()[0]['V7']), scale=float(df3_rdd.collect()[0]['V7']), size=(283823)).tolist(), 'V8': np.random.normal(loc=float(df2_rdd.collect()[0]['V8']), scale=float(df3_rdd.collect()[0]['V8']), size=(283823)).tolist(), 'V9': np.random.normal(loc=float(df2_rdd.collect()[0]['V9']), scale=float(df3_rdd.collect()[0]['V9']), size=(283823)).tolist(), 'V10': np.random.normal(loc=float(df2_rdd.collect()[0]['V10']), scale=float(df3_rdd.collect()[0]['V10']), size=(283823)).tolist(), 'V11': np.random.normal(loc=float(df2_rdd.collect()[0]['V11']), scale=float(df3_rdd.collect()[0]['V11']), size=(283823)).tolist(), 'V12': np.random.normal(loc=float(df2_rdd.collect()[0]['V12']), scale=float(df3_rdd.collect()[0]['V12']), size=(283823)).tolist(), 'V13': np.random.normal(loc=float(df2_rdd.collect()[0]['V13']), scale=float(df3_rdd.collect()[0]['V13']), size=(283823)).tolist(), 'V14': np.random.normal(loc=float(df2_rdd.collect()[0]['V14']), scale=float(df3_rdd.collect()[0]['V14']), size=(283823)).tolist(), 'V15': np.random.normal(loc=float(df2_rdd.collect()[0]['V15']), scale=float(df3_rdd.collect()[0]['V15']), size=(283823)).tolist(), 'V16': np.random.normal(loc=float(df2_rdd.collect()[0]['V16']), scale=float(df3_rdd.collect()[0]['V16']), size=(283823)).tolist(), 'V17': np.random.normal(loc=float(df2_rdd.collect()[0]['V17']), scale=float(df3_rdd.collect()[0]['V17']), size=(283823)).tolist(), 'V18': np.random.normal(loc=float(df2_rdd.collect()[0]['V18']), scale=float(df3_rdd.collect()[0]['V18']), size=(283823)).tolist(), 'V19': np.random.normal(loc=float(df2_rdd.collect()[0]['V19']), scale=float(df3_rdd.collect()[0]['V19']), size=(283823)).tolist(), 'V20': np.random.normal(loc=float(df2_rdd.collect()[0]['V20']), scale=float(df3_rdd.collect()[0]['V20']), size=(283823)).tolist(), 'V21': np.random.normal(loc=float(df2_rdd.collect()[0]['V21']), scale=float(df3_rdd.collect()[0]['V21']), size=(283823)).tolist(), 'V22': np.random.normal(loc=float(df2_rdd.collect()[0]['V22']), scale=float(df3_rdd.collect()[0]['V22']), size=(283823)).tolist(), 'V23': np.random.normal(loc=float(df2_rdd.collect()[0]['V23']), scale=float(df3_rdd.collect()[0]['V23']), size=(283823)).tolist(), 'V24': np.random.normal(loc=float(df2_rdd.collect()[0]['V24']), scale=float(df3_rdd.collect()[0]['V24']), size=(283823)).tolist(), 'V25': np.random.normal(loc=float(df2_rdd.collect()[0]['V25']), scale=float(df3_rdd.collect()[0]['V25']), size=(283823)).tolist(), 'V26': np.random.normal(loc=float(df2_rdd.collect()[0]['V26']), scale=float(df3_rdd.collect()[0]['V26']), size=(283823)).tolist(), 'V27': np.random.normal(loc=float(df2_rdd.collect()[0]['V27']), scale=float(df3_rdd.collect()[0]['V27']), size=(283823)).tolist(), 'V28': np.random.normal(loc=float(df2_rdd.collect()[0]['V28']), scale=float(df3_rdd.collect()[0]['V28']), size=(283823)).tolist(), 'Amount':[], 'Class':[]}</span><span id="3527" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">for</strong> k <strong class="jp hi">in</strong> range(283823):</span><span id="3486" class="jt ju hh jp b fi jz jw l jx jy">dict = {'Amount_1': uniform(0, 25691), 'Class_1': int((k+1)/(k+1))} dict1['Amount'].append(dict['Amount_1']) dict1['Class'].append(dict['Class_1'])</span><span id="5e45" class="jt ju hh jp b fi jz jw l jx jy">df2 = ks.DataFrame(dict1).to_spark()</span></pre><p id="46eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，棘手的部分是使用考拉，然后转换回spark数据帧，为新的数据帧自动生成新的模式。因此，现在我们将连接原始数据帧和新数据帧。在原始数据帧(df1)中，我们将删除时间列，因为这个特性对我们的模型并不重要。</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="f7da" class="jt ju hh jp b fi jv jw l jx jy">df1_final = df1.drop('Time')<br/>concatenado_koalas = df2.union(df1_final).to_koalas()<br/>concatenado_spark = concatenado_koalas.to_spark()<br/>concatenado_spark.createOrReplaceTempView('concatenado_spark_var')</span><span id="9f52" class="jt ju hh jp b fi jz jw l jx jy"># New balanced target variable</span><span id="7e28" class="jt ju hh jp b fi jz jw l jx jy">%sql<br/>select Class, count(Class) <strong class="jp hi">from</strong> concatenado_spark_var group by Class</span></pre><p id="f3e5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用databricks中的SQL magic命令，我们可以轻松地使用SQL进行查询。因此，对于类目标变量，我们现在有了一个完全平衡的集合。</p><figure class="jk jl jm jn fd kb er es paragraph-image"><div class="er es ki"><img src="../Images/8faef41d6460b15b5c14fdaaaa28d840.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*7Y2Iv3SE2Ehdh8BaLv_U8g.png"/></div><figcaption class="kj kk et er es kl km bd b be z dx translated">“类”目标变量现在完全平衡了</figcaption></figure><p id="2db4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们将通过逻辑回归模型评估我们的过采样数据，然后将其与原始数据进行比较。此外，我们还将过采样数据与通过欠采样程序获得的新数据进行比较。为此，我们需要导入pandas和scikit-learn库:</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="e9a9" class="jt ju hh jp b fi jv jw l jx jy"><strong class="jp hi">import</strong> pandas <strong class="jp hi">as</strong> pd<br/><strong class="jp hi">from</strong> sklearn.model_selection <strong class="jp hi">import</strong> train_test_split<br/><strong class="jp hi">from</strong> sklearn.linear_model <strong class="jp hi">import</strong> LogisticRegression<br/><strong class="jp hi">from</strong> sklearn.metrics <strong class="jp hi">import</strong> classification_report, confusion_matrix, roc_auc_score<br/><strong class="jp hi">from</strong> sklearn.datasets <strong class="jp hi">import</strong> make_classification<br/><strong class="jp hi">from</strong> sklearn.pipeline <strong class="jp hi">import</strong> make_pipeline<br/><strong class="jp hi">from</strong> sklearn.preprocessing <strong class="jp hi">import</strong> StandardScaler</span><span id="a9d7" class="jt ju hh jp b fi jz jw l jx jy">concatenado_pandas = concatenado_koalas.to_pandas()</span></pre><p id="71c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">定义我们的一组特征和目标变量:</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="a788" class="jt ju hh jp b fi jv jw l jx jy">features_eric = concatenado_pandas.drop('Class', axis=1)<br/>target_eric = concatenado_pandas['Class'].ravel()</span></pre><p id="3bf3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们有，</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="7f41" class="jt ju hh jp b fi jv jw l jx jy">X, y = make_classification(random_state=42)</span><span id="2faf" class="jt ju hh jp b fi jz jw l jx jy">X_treino, X_teste, y_treino, y_teste = train_test_split(features_eric, target_eric, test_size=0.4, random_state=42)</span><span id="e306" class="jt ju hh jp b fi jz jw l jx jy">pipe = make_pipeline(StandardScaler(), LogisticRegression())</span><span id="59ad" class="jt ju hh jp b fi jz jw l jx jy">pipe.fit(X_treino, y_treino)</span></pre><p id="6e78" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">模型结果如下所示:</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="3b06" class="jt ju hh jp b fi jv jw l jx jy">y_pred=pipe.predict(X_teste)</span></pre><figure class="jk jl jm jn fd kb er es paragraph-image"><div class="er es kn"><img src="../Images/0b47de2ea2536b73886eca25d6e938ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*zVV5JBpU8vSnQeUGwfwAUA.png"/></div></figure><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="c102" class="jt ju hh jp b fi jv jw l jx jy">print("AUC score is: ", roc_auc_score(y_teste, y_pred))</span><span id="1dad" class="jt ju hh jp b fi jz jw l jx jy">AUC score is:  0.9884036712683695</span></pre><p id="1c7e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们将我们的结果与不平衡的“原始”数据进行比较:</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="67a9" class="jt ju hh jp b fi jv jw l jx jy">df1_pandas = df1_final.toPandas()</span><span id="a952" class="jt ju hh jp b fi jz jw l jx jy"># Unbalanced data<br/>features_original = df1_pandas.drop('Class', axis=1)<br/>target_original = df1_pandas['Class'].ravel()</span><span id="46a5" class="jt ju hh jp b fi jz jw l jx jy">K, k = make_classification(random_state=42)</span><span id="1be2" class="jt ju hh jp b fi jz jw l jx jy">K_treino, K_teste, k_treino, k_teste = train_test_split(features_original, target_original, test_size=0.4, random_state=42)</span><span id="15dd" class="jt ju hh jp b fi jz jw l jx jy">pipe.fit(K_treino, k_treino)</span><span id="6e95" class="jt ju hh jp b fi jz jw l jx jy">k_pred=pipe.predict(K_teste)</span></pre><p id="f547" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">模型结果如下所示:</p><figure class="jk jl jm jn fd kb er es paragraph-image"><div class="er es ko"><img src="../Images/88b92f50d95860581aa5925556b5ec9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*K9M2VgjM1iii4scnsTveuA.png"/></div></figure><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="b7da" class="jt ju hh jp b fi jv jw l jx jy">print("AUC score is: ", roc_auc_score(k_teste, k_pred))</span><span id="0f04" class="jt ju hh jp b fi jz jw l jx jy">AUC score is:  0.8035989769648607</span></pre><p id="45e7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们现在将我们的结果与通过欠采样程序获得的另一个数据进行比较，该程序的代码最初可在<a class="ae jj" href="https://www.kdnuggets.com/2019/05/fix-unbalanced-dataset.html" rel="noopener ugc nofollow" target="_blank">https://www . kdnugges . com/2019/05/fix-unbalanced-dataset . html</a>找到:</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="6db7" class="jt ju hh jp b fi jv jw l jx jy"># Undersampling data</span><span id="8330" class="jt ju hh jp b fi jz jw l jx jy">L = df1_pandas.drop('Class', axis=1).sample(n=984, random_state=42)<br/># Shuffling the data<br/>shuffled_df = df1_pandas[['Class']].sample(frac=1, random_state=42)</span><span id="272b" class="jt ju hh jp b fi jz jw l jx jy"># Put all the fraud class in a separate dataset.<br/>fraud_df = shuffled_df.loc[shuffled_df['Class'] == 1]</span><span id="fbcb" class="jt ju hh jp b fi jz jw l jx jy">#Randomly select 492 observations from the non-fraud (majority class)<br/>non_fraud_df = shuffled_df.loc[shuffled_df['Class'] == '0'].sample(n=492, random_state=42)</span><span id="9698" class="jt ju hh jp b fi jz jw l jx jy"># Concatenate both dataframes again<br/>l = normalized_df = pd.concat([fraud_df, non_fraud_df]).squeeze().ravel()</span></pre><p id="8fa8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将该模型应用于欠采样数据，我们得到:</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="7d9a" class="jt ju hh jp b fi jv jw l jx jy">L, l = make_classification(random_state=42)</span><span id="9e5a" class="jt ju hh jp b fi jz jw l jx jy">L_treino, L_teste, l_treino, l_teste = train_test_split(L, l, test_size=0.4, random_state=42)</span><span id="6432" class="jt ju hh jp b fi jz jw l jx jy">pipe.fit(L_treino, l_treino)</span><span id="9333" class="jt ju hh jp b fi jz jw l jx jy">l_pred=pipe.predict(L_teste)</span></pre><p id="e775" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">模型结果如下所示:</p><figure class="jk jl jm jn fd kb er es paragraph-image"><div class="er es kp"><img src="../Images/5b3d267555eb818796f59435b2904cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*UzEPBUfE_vM1eVPeRk2LSw.png"/></div></figure><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="03a9" class="jt ju hh jp b fi jv jw l jx jy">print("AUC score is: ", roc_auc_score(l_teste, l_pred))</span><span id="6f87" class="jt ju hh jp b fi jz jw l jx jy">AUC score is:  0.9285714285714286</span></pre><p id="a316" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">结论:与不平衡数据集相比，适当的过采样程序可以提供更好的结果。我们已经证明了不平衡数据的统计知识对于数据的上采样是很重要的。通过这样做，与原始数据和欠采样数据相比，我们获得了过采样数据的更好结果。这里需要强调的是，我们无意优化模型的超参数，也无意证明本文中提到的统计假设。</p></div></div>    
</body>
</html>