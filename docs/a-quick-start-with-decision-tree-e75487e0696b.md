# 决策树快速入门

> 原文：<https://medium.com/analytics-vidhya/a-quick-start-with-decision-tree-e75487e0696b?source=collection_archive---------13----------------------->

**决策树**是基于特定条件做出决策的各种方法的图解。这些条件通常是 if-then 语句，或者我们一般可以说是或否。树越健壮，条件越困难。每种情况都是一个问题，它的解决方案将帮助我们量化问题。

让我们用一个实时例子来说明这一点

*   假设我们想在某一天打羽毛球，比如周六，你将如何决定是否打。
*   假设你出去检查天气是热还是冷，检查风速和湿度，天气如何，例如是晴天、多云还是下雨。你要考虑所有这些因素来决定你是否想玩。

树状结构将代表以下属性:

*   决策节点:它定义了对单个属性的测试。
*   叶节点:显示目标属性的值。
*   边缘:它是一个属性的分裂。
*   路径:这是一个析取的测试，以作出最后的决定。

**节点杂质**是节点内的相关性。如果事例有多个响应值，则节点是不纯的。如果所有实例都具有相同的响应值或目标变量或杂质值= 0，则节点是纯的。

这是两种最常用的测量节点杂质的方法:

*   熵
*   基尼

**熵**

在决策树中，熵是一种无序或不确定性。它是对一堆数据中杂质、无序或不确定性的度量。

```
def get_entropy(data):
label_col = data[:, -1]
a, counts = np.unique(label_col, return_counts=True)
prob = counts / counts.sum()
entropy = sum(probabilities * -np.log2(probabilities))return entropy
```

熵的一个简单例子:

假设有一个包描述了两种不同的场景:

*   袋子 A 有 100 个绿色的球。彼得想从这个包里选一个绿色的球。这里，袋 A 的熵为 0，因为它意味着 0 杂质或总纯度。
*   我们用红球替换 A 袋中的 40 个绿球，同样，我们用黑球替换 10 个绿球。现在，约翰想从这个包里选择一个绿色的球。在这种情况下，由于袋子杂质的增加，抽取绿色球的概率将从 1.0 下降到 0.5。

**信息增益**

它是决策树算法建立决策树所接受的主键。决策树将改进以最大化信息增益。具有最高信息增益的属性将首先被测试或分割。

**基尼指数**

像熵一样，基尼指数也是决策树中计算信息增益的一种标准。决策树使用信息增益来分割节点，而基尼系数测量节点的杂质。基尼系数的范围在 0 到 0.5 之间。与熵相比，基尼系数更适合于选择最佳特征。

希望它能帮助你理解决策树的基础&与决策树相关的一些其他方法。像熵计算这样的方法越来越少，需要克服的问题也越来越少，比如过度拟合，我会在我即将到来的博客中解释。

快乐学习！

**参考资料:**【https://scikit-learn.org/stable/modules/tree.html】T2

*原载于 2021 年 6 月 7 日 https://www.numpyninja.com*[](https://www.numpyninja.com/post/a-quick-start-with-decision-tree)**。**