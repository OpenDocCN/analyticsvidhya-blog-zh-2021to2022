<html>
<head>
<title>SOFT ACTOR-CRITIC ALGORITHMS IN DEEP REINFORCEMENT LEARNING</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度强化学习中的软行动者批评算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/soft-actor-critic-algorithms-in-deep-reinforcement-learning-a11bedd9aa20?source=collection_archive---------1-----------------------#2021-07-19">https://medium.com/analytics-vidhya/soft-actor-critic-algorithms-in-deep-reinforcement-learning-a11bedd9aa20?source=collection_archive---------1-----------------------#2021-07-19</a></blockquote><div><div class="ds hc hd he hf hg"/><div class="hh hi hj hk hl"><div class=""/><p id="cc68" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hh bi translated">在之前的系列文章中，我们谈到了<a class="ae jj" rel="noopener" href="/analytics-vidhya/policy-gradients-in-deep-reinforcement-learning-83d99575cfca">政策梯度法</a>、<a class="ae jj" rel="noopener" href="/analytics-vidhya/deep-deterministic-policy-gradient-for-continuous-action-space-9b2b9bacd555">、</a>和<a class="ae jj" rel="noopener" href="/analytics-vidhya/trust-region-methods-for-deep-reinforcement-learning-e7e2a8460284">信赖域法</a>。这里我们也讨论了每种方法相应的缺点。</p><ul class=""><li id="9b70" class="jk jl ho in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated">像基于PG的方法是样本低效的，因为它们在一次梯度更新后丢弃样本。在复杂的任务中，丢弃样本会导致更新缓慢，并且经常收敛到次优策略。</li><li id="6762" class="jk jl ho in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated">DDPG试图通过一个重放缓冲数据结构来解决这个问题，该数据结构存储转换元组。我们抽样了一批…</li></ul></div></div>    
</body>
</html>