<html>
<head>
<title>How does Linear Regression work? Implementation with sklearn.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归是如何工作的？用sklearn实现。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-does-linear-regression-work-implementation-with-sklearn-e5a850eddc89?source=collection_archive---------9-----------------------#2021-06-14">https://medium.com/analytics-vidhya/how-does-linear-regression-work-implementation-with-sklearn-e5a850eddc89?source=collection_archive---------9-----------------------#2021-06-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/3a45c72a8d8fb830e67a0926c936d81d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*Tv4iNJpwcs4mYAq18OSQdA.png"/></div></figure><h1 id="6181" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">介绍</h1><p id="81e4" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">观众好，我是穆斯塔法，在这篇博客中，你将学习一种简单的机器学习算法，叫做<strong class="jl hi">线性回归。</strong>我们会经过一些数学，然后跳转到编码部分。相信我，你只需要几行代码就可以实现任何机器学习模型，但你真正需要理解的是它是如何工作的，以及这些机器学习算法背后的数学原理。我们开始吧！</p><p id="47ec" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">那么什么是线性回归呢？它用什么数学方程来预测数值？。我将在这篇博客中尝试回答这些问题。</p><h1 id="8c68" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">什么是回归？</h1><p id="41e4" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">在继续之前，让我们了解一下什么是回归？。回归是一种借助独立值或称特征来预测未来值或目标值的方法。</p><h1 id="766c" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">什么是线性回归？</h1><p id="0934" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">顾名思义，它是一个<strong class="jl hi">线性模型</strong>，所以如果我们有线性数据，基本上我们可以得到好的结果。什么是线性数据？简而言之，当我们绘制数据时，我们会得到一条有一定角度的直线(假设是二维数据)。因此，回归模型试图以最小的损失拟合训练数据的最佳可能线。</p><figure class="km kn ko kp fd ii"><div class="bz dy l di"><div class="kq kr l"/></div></figure><p id="60ef" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">线性模型通过简单计算输入要素的加权和以及称为偏差或截距的常数项来进行预测</p><figure class="km kn ko kp fd ii er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ks"><img src="../Images/e6fd7badfd45ad53e543d75e8f8b270e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YbL3gyJZh4DmRKSArL87rQ.jpeg"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">线性回归模型预测方程。</figcaption></figure><p id="b15a" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">在这个等式中:</p><ul class=""><li id="0281" class="lb lc hh jl b jm kh jq ki ju ld jy le kc lf kg lg lh li lj bi translated">y-hat是预测值。</li><li id="38ec" class="lb lc hh jl b jm lk jq ll ju lm jy ln kc lo kg lg lh li lj bi translated">n是特征的数量。</li><li id="719e" class="lb lc hh jl b jm lk jq ll ju lm jy ln kc lo kg lg lh li lj bi translated">x(i)是第I个特征值。</li><li id="5da5" class="lb lc hh jl b jm lk jq ll ju lm jy ln kc lo kg lg lh li lj bi translated">θ是模型参数，或者我们可以说是特征权重。</li></ul><h1 id="6285" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">线性回归模型如何训练？</h1><p id="71c5" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">这就是线性模型的方程，现在我们如何训练它？。训练模型意味着设置其参数，使模型最适合训练集。为此，我们首先需要衡量模型与训练数据的吻合程度。因此，回归任务最常见的性能度量是应用均方误差(MSE)。为了训练回归模型，我们需要找到最小化MSE的值θ。</p><figure class="km kn ko kp fd ii er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lp"><img src="../Images/c2b19227d0fba50f7fcfb9b1a1d0ddae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYN5oCEje5Z7XK02FBiubQ.jpeg"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">MSE成本函数</figcaption></figure><ul class=""><li id="eaac" class="lb lc hh jl b jm kh jq ki ju ld jy le kc lf kg lg lh li lj bi translated">x是特征向量。</li><li id="5ee8" class="lb lc hh jl b jm lk jq ll ju lm jy ln kc lo kg lg lh li lj bi translated">h是系统的预测函数，也称为假设。</li><li id="53ff" class="lb lc hh jl b jm lk jq ll ju lm jy ln kc lo kg lg lh li lj bi translated">θ是权重矩阵。</li><li id="392c" class="lb lc hh jl b jm lk jq ll ju lm jy ln kc lo kg lg lh li lj bi translated">x(i)是x的第I个实例。</li><li id="e17b" class="lb lc hh jl b jm lk jq ll ju lm jy ln kc lo kg lg lh li lj bi translated">y(i)是第I个实例的目标值。</li></ul><p id="0a96" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">为了更新θ的值，我们先来看看基本方程，叫做<strong class="jl hi">正规方程。</strong></p><figure class="km kn ko kp fd ii er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lq"><img src="../Images/bf37a3271e3587c0ddefc0c9db8378d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8VjHTki8XSutojMjJe-yeA.jpeg"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">正态方程</figcaption></figure><ul class=""><li id="b502" class="lb lc hh jl b jm kh jq ki ju ld jy le kc lf kg lg lh li lj bi translated">θ是使成本函数最小的θ值。</li><li id="0c65" class="lb lc hh jl b jm lk jq ll ju lm jy ln kc lo kg lg lh li lj bi translated">y是包含y(1)到y(m)的目标值的向量。</li></ul><p id="c12c" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">让我们生成一些数据来测试上面的等式。</p><pre class="km kn ko kp fd lr ls lt lu aw lv bi"><span id="24fe" class="lw im hh ls b fi lx ly l lz ma">import numpy as np</span><span id="b6d0" class="lw im hh ls b fi mb ly l lz ma"># generating data</span><span id="ebf5" class="lw im hh ls b fi mb ly l lz ma">X = 2 * np.random.rand(100, 1)<br/>y = 4 + 3 * X + np.random.randn(100, 1)</span><span id="1e00" class="lw im hh ls b fi mb ly l lz ma">X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance</span><span id="cb9e" class="lw im hh ls b fi mb ly l lz ma"># normal equation<br/>theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span></pre><p id="e8b7" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">为了生成数据，我们使用了numpy . rand . rand()，它为我们提供了具有一些随机值的100大小的向量，为了生成我们的目标值，我们使用了4x+3+(一些高斯噪声)。为了实现法线方程以获得θ的最佳值，我们使用了numpy inv()函数来计算矩阵的逆矩阵，使用dot()函数来计算点积。</p><p id="262e" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">请记住，我们用来生成数据的等式是4+3x+(一些高斯噪声)。让我们看看使用法线方程会得到什么:</p><pre class="km kn ko kp fd lr ls lt lu aw lv bi"><span id="fcc9" class="lw im hh ls b fi lx ly l lz ma">print(theta_best)</span><span id="a855" class="lw im hh ls b fi mb ly l lz ma"># output:<br/># array([[4.21509616],<br/>#       [2.77011339]])</span></pre><p id="48f8" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">我们希望得到值4和3，但我们得到了足够接近的4.215和2.7701。</p><p id="4119" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">让我们预测新的值并把它们画在图上。</p><pre class="km kn ko kp fd lr ls lt lu aw lv bi"><span id="506c" class="lw im hh ls b fi lx ly l lz ma">X_new = np.array([[0], [2]])<br/># add x0 = 1 to each instance<br/>X_new_b = np.c_[np.ones((2, 1)), X_new]</span><span id="b596" class="lw im hh ls b fi mb ly l lz ma">y_predict = X_new_b.dot(theta_best)<br/>print(y_predict)<br/># output<br/># array([[4.21509616],<br/>#        [9.75532293]])</span><span id="9c98" class="lw im hh ls b fi mb ly l lz ma"># plotting data<br/>plt.plot(X_new, y_predict, "r-")<br/>plt.plot(X, y, "b.")<br/>plt.axis([0, 2, 0, 15])<br/>plt.show()</span></pre><figure class="km kn ko kp fd ii er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mc"><img src="../Images/403df52c3a53645817aa3448a6d76652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LMI1KVlStiCX87pW8Lfdow.png"/></div></div></figure><p id="cfac" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">在上面的代码中，我们生成2个随机值，并将它们传递给predict方法以获得目标值。在matplotlib库的帮助下，我们绘制了训练数据的散点图和我们生成的一行数据。</p><p id="7e9a" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">LinearRegression类基于<strong class="jl hi"> scipy.linalg.lstsq() </strong>函数(该名称代表“最小二乘”)。它返回线性矩阵方程的最小二乘解。</p><pre class="km kn ko kp fd lr ls lt lu aw lv bi"><span id="0b5c" class="lw im hh ls b fi lx ly l lz ma">theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)<br/>print(theta_best_svd)<br/># ouput<br/># array([[4.21509616],<br/>#       [2.77011339]])</span></pre><p id="5c92" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">此函数计算θ = (X+) y，其中X+是X的伪逆。您可以使用np.linalg.pinv()直接计算伪逆:</p><pre class="km kn ko kp fd lr ls lt lu aw lv bi"><span id="8409" class="lw im hh ls b fi lx ly l lz ma">np.linalg.pinv(X_b).dot(y)</span></pre><p id="69d4" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">伪逆计算如下:X+= vσ+UT(T代表转置)。v和U是正交矩阵。为了计算矩阵σ+，该算法采用σ，并将小于一个微小阈值的所有值设置为零，然后用它们的逆矩阵替换所有非零值，最后，转置得到的矩阵。但是为什么要用伪逆代替正规方程呢？如果矩阵XTX是不可逆的(即奇异的)，例如如果m &lt; n或者如果一些特征是冗余的，则正规方程可能不起作用，但是伪逆总是被定义的。</p><h1 id="5e40" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">使用sklearn实现线性回归</h1><p id="5960" class="pw-post-body-paragraph jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg ha bi translated">让我们使用sklearn库实现线性回归。</p><pre class="km kn ko kp fd lr ls lt lu aw lv bi"><span id="7434" class="lw im hh ls b fi lx ly l lz ma">from sklearn.linear_model import LinearRegression<br/>lin_reg =LinearRegression()<br/>lin_reg.fit(X,y) # data we genrated above<br/>print(lin_reg.intercept_, lin_reg.coef_)</span><span id="b306" class="lw im hh ls b fi mb ly l lz ma"># output<br/># (array([4.21509616]), array([[2.77011339]]))</span></pre><p id="0a9f" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">因此，使用sklearn实现线性回归是一项非常简单的任务。我们只需要3行代码来实现它，首先从sklearn.linear_model导入模型，然后初始化一个对象，最后调用fit方法，将特征值和目标值作为参数。</p><p id="69bf" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">此外，您可以评估模型并尝试GridSearchCV进行超参数调优。</p><p id="3084" class="pw-post-body-paragraph jj jk hh jl b jm kh jo jp jq ki js jt ju kj jw jx jy kk ka kb kc kl ke kf kg ha bi translated">如果你觉得有用或者你认为需要改进，请在下面评论。谢谢:)</p></div></div>    
</body>
</html>