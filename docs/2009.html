<html>
<head>
<title>Machine Learning Models- Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习模型-逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-models-logistic-regression-3b71d8a0f3b8?source=collection_archive---------11-----------------------#2021-03-31">https://medium.com/analytics-vidhya/machine-learning-models-logistic-regression-3b71d8a0f3b8?source=collection_archive---------11-----------------------#2021-03-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="3467" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">简介</strong></h1><p id="bb0c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">本博客涵盖的主题:</p><ol class=""><li id="ef5e" class="ka kb hh je b jf kc jj kd jn ke jr kf jv kg jz kh ki kj kk bi translated">什么是逻辑回归？</li><li id="0a07" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">为什么不使用线性回归</li><li id="eb91" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">关于逻辑回归的更多信息</li><li id="d969" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">最大似然估计。</li><li id="ecda" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">逻辑回归中的成本函数</li><li id="a1f9" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">梯度下降</li><li id="e680" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">Python实现</li></ol><p id="1dc7" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">为了理解逻辑回归，有必要了解一下我们之前已经介绍过的<a class="ae kt" rel="noopener" href="/analytics-vidhya/machine-learning-models-linear-regression-58855efb2355">线性回归</a>。</p><h1 id="b5ab" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">什么是逻辑回归？</h1><p id="0a61" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">逻辑回归是广义线性模型。与根据数字数据预测值的线性回归不同，逻辑回归用于<strong class="je hi">分类问题</strong>。分类问题的一些例子是电子邮件垃圾分类器、虹膜种类分类、信用卡欺诈检测。</p><p id="0af0" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">那为什么叫逻辑回归呢？</strong></p><p id="52f2" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">逻辑回归实际上预测从属特征的概率值，其范围在[0，1]之间。如果这是一个二元分类问题，并且如果该值大于或等于0.5，则它被分类为真，否则它被分类为假。然而，我们知道在线性回归中，范围是从负无穷大到正无穷大，但这里我们有[0，1]之间的值。为了解决这个问题，我们有一个函数叫做<strong class="je hi"> Sigmoid函数。</strong> Sigmoid函数给出一条曲线。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ku"><img src="../Images/e8ec733a2b5283441e862a89f7ae78d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*dx1CiQFaqS7KvcqLYV7wGw.jpeg"/></div></figure><p id="cc4d" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">为什么不用线性回归？</strong></p><p id="addf" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">第一个显而易见的原因是，与线性回归不同，逻辑回归是一种分类模型。线性回归的范围是从负无穷大到正无穷大，因此它可能会生成负预测。而范围在[0，1]之间的逻辑回归则没有这个问题。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lc"><img src="../Images/83cf923458f733025e8c28b27bb9aa01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i8ZpQEQQgRcpbzI8_tS_1g.jpeg"/></div></figure><p id="4a43" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">线性回归的另一个问题是，当您有一些异常值时，最佳拟合线将向异常值倾斜，从而导致错误的预测。</p><h2 id="9432" class="ld if hh bd ig le lf lg ik lh li lj io jn lk ll is jr lm ln iw jv lo lp ja lq bi translated"><strong class="ak">关于逻辑回归的更多信息。</strong></h2><p id="f0d7" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">如前所述，逻辑回归有点类似于线性回归。为了预测最佳拟合曲线，我们使用Logit函数将该曲线转换为直线，该函数将概率作为输入，给出概率的对数作为输出。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lr"><img src="../Images/3f0b7f9a94df3a8495db0eed134bcc11.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*ruru4_XTGwS3s-iJsActbw.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">图一</figcaption></figure><p id="a2b2" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">对于每一个点，我们将计算奇数值的对数，这些数值将在图中的y轴上使用。这样我们就可以为曲线生成一条直线。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lw"><img src="../Images/45f10f200de2d1391b3892ea27e16384.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*SmBoSa0nxlE-3KlCX6CDPQ.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">图二</figcaption></figure><p id="999b" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">最大似然估计</strong></p><p id="419b" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">在线性回归中，我们使用误差平方和的最小二乘法来寻找最佳拟合线。在图-2中，由于大部分点都趋于负无穷大和正无穷大，所以误差值也会趋于无穷大。我们不能使用最小二乘法，而是使用最大似然估计。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lx"><img src="../Images/90df6a981e61fdc763508cdecf6826f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*J8-w-6jrH7INJ2-EdOBxIw.png"/></div></figure><p id="c501" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">我们计算每个对数(比值)点的最大似然，并将所有似然相乘，以获得完整数据集的似然。获得完整数据集的最大似然值的曲线被认为是最佳曲线。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ly"><img src="../Images/850a10500b825a760f3998014622361b.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*O1EHcTTx1mBVcuBzqjLEuQ.png"/></div></figure><p id="4ba7" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">如何最大化对数似然？</strong></p><p id="7dc0" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">在统计学中，最大似然估计(MLE)被广泛用于获得分布的参数。在这个范例中，最大化对数似然等于最小化成本函数<em class="lz"> J </em>。它是凸优化中的一个对偶问题。</p><h1 id="c7b3" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">逻辑回归中的成本函数</strong></h1><p id="1f25" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们学习了线性回归中的成本函数<em class="lz"> J </em> ( <em class="lz"> θ </em>)，成本函数代表优化目标，即我们创建成本函数，并尝试使用梯度下降来最小化成本函数，以便我们可以获得全局最小值。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ma"><img src="../Images/2dbfcd9e4074d13a46cd3f27d6147a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*D8vOnNligEGX7U-ZmyEbKQ.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">成本函数线性回归</figcaption></figure><p id="3c9d" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">因此，如果我们尝试将线性回归的成本函数用于逻辑回归的假设函数(sigmoid方程),那么可以观察到它给出了非凸函数。在非凸函数中，除了全局极小值之外，我们还会得到局部极小值，而寻找全局极小值将是一项困难的任务。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mb"><img src="../Images/55c92c73225a98aef8dd9796b3874c07.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*NXUKuVbR31YDHTUjRgcOcA.png"/></div></figure><p id="faa0" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">对于逻辑回归，成本函数定义为:-</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lc"><img src="../Images/bf3e73d0466019781bc1354887e21eaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ERPuNCmF72snxKVIUUDXBA.png"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mc"><img src="../Images/427660befbfae7929ba70027d48a3411.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*LlddwGoXvTq_v47xIbImew.png"/></div></figure><p id="a3e1" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">上述两个方程可以合并得到一个新方程</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es md"><img src="../Images/b61f379166ade4695af86217f37379d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*_OOJfgx2ajZR0FOUxRR9Wg.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">价值函数</figcaption></figure><p id="52a0" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">现在使用这个成本函数，我们可以使用梯度下降的全局最小值。</p><h1 id="7c4f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">梯度下降</strong></h1><p id="10aa" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">逻辑回归的梯度下降与线性回归相同。现在，为了最小化成本函数，我们需要对每个参数运行梯度下降函数，即</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es me"><img src="../Images/6bb0557f2e646ec17c7ad27221b58dd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*eeHBmeE_pTjl50KhASXUtg.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">为了最小化成本函数，我们必须对每个参数运行梯度下降函数</figcaption></figure><h1 id="e1f1" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak"> Python实现</strong></h1><p id="474e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">所以，重要的是看看如何使用我们所获得的。我在鸢尾上应用了逻辑回归，它可以根据萼片和花瓣的长度和宽度对物种进行分类。</p><p id="7045" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">点击<a class="ae kt" href="https://www.kaggle.com/ashishkumarbehera/iris-species-classification" rel="noopener ugc nofollow" target="_blank">此处</a>查看完整代码，看看机器学习的世界在逻辑回归方面有多简单。</p><h1 id="2328" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">结论</strong></h1><p id="6e67" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在这篇博客中，我已经向你介绍了我们必须知道的逻辑回归的基本概念。我希望这是有帮助的，让你保持动力。</p><p id="3b8a" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">感谢阅读。<br/>:)<em class="lz">还有，❤这本书读得真好。尽情享受吧！</em></p><p id="aa0c" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">编辑:<a class="mf mg ge" href="https://medium.com/u/f709789d3ba4?source=post_page-----3b71d8a0f3b8--------------------------------" rel="noopener" target="_blank"> Ashishkumar </a></p></div></div>    
</body>
</html>