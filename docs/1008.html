<html>
<head>
<title>Intro to Expectation Maximization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">期望值最大化简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/intro-to-expectation-maximization-f02849db1f89?source=collection_archive---------25-----------------------#2021-02-09">https://medium.com/analytics-vidhya/intro-to-expectation-maximization-f02849db1f89?source=collection_archive---------25-----------------------#2021-02-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/db8cd9e8b193492675ead5dd6cf1554e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ApApJllf65Em_KUtIEhEEw.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">迪伦·诺尔特在<a class="ae it" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="96a3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我写过几篇关于参数估计的帖子。第一个帖子是关于最大似然估计(MLE ),我们希望找到某个参数θ的值，使训练数据最有可能，这也可以称为给定训练数据时最有可能的参数。贝叶斯参数估计(BPE)扩展了最大似然估计，因为关于θ的先验信息是给定的或假设的。资料不全怎么办？如果有丢失的数据，丢失可能意味着噪音、丢失或隐藏数据，该怎么办？这就是期望最大化(EM)的用武之地。EM算法是一种迭代方法，用于边缘化缺失数据以寻找最大似然估计。目标是迭代地找到更好的参数θ，使得l(θ) &gt; l(θ')，其中θ'是当前估计，θ。</p><h1 id="2884" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">定义EM算法</h1><p id="63f5" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">EM算法由两个步骤组成。第一步是计算期望值，或计算Q(θ；θ').第二步是最大化第一步计算的期望值。我们也用Q的arg max(θ；θ').由于这是一个迭代算法，当Q变化不大时，该算法在数据上循环，将终止。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kv"><img src="../Images/d9c5df0ea2724d8479dad233faa4e971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xAL-1NUqmkR7uho_RVoZiQ.jpeg"/></div></div></figure><h1 id="57bb" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">推导EM算法</strong></h1><p id="7963" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">我们有一个数据集Z = (x，y ),其中x是可用的，y是潜在的数据，不可观察的数据。在MLE中，我们想求P(X|θ)其中X={x_1，x_2，…，x_n}，但由于我们既有可观测数据又有不可观测数据，所以我们想求P(Z|θ)。在我们想求P(Z|θ)的同时，如何在y为潜伏时最大化P(x，y|θ)？由于只知道x，所以只能最大化P(x|θ)其中P(x|θ)=P(x，y|θ)-P(y|x，θ)。让我们更深入一点来推导期望。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es la"><img src="../Images/88cd403c9920d2926c0881c0b07f904c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rnoV_DTh_fSyGhQp8G_Ggw.jpeg"/></div></div></figure><p id="d80c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，我们对如何用公式表示Q(θ，有了更好的想法。θ’)，让我们看一个例子来进一步理解。</p><h1 id="7496" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">EM示例</h1><p id="cf7f" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">假设你和一个朋友在掷两枚不同的硬币。你把你的硬币抛10次，你的朋友也把他的硬币抛10次。你们两个总共重复3次试验。然后你们交换硬币，但是在这个过程中，你们两个人都把硬币掉了，导致两个人都不知道他们有谁的硬币。你想算出两个硬币正面概率的最佳估计值，但是你不知道你在掷哪个硬币。在这里，我们可以使用EM来估计硬币正面的概率，而不完全知道哪个硬币正在被投掷。我们决定把硬币放在一个袋子里，我们从袋子里随机选择一枚硬币，并翻转10次。作为EM算法的第一次迭代，我们再执行2次。这是第一次迭代的结果。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lb"><img src="../Images/b8436c010ca4432719ea18e82f73a493.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EHNOAN80DR7oMjRx_yjjUg.jpeg"/></div></div></figure><p id="b414" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里我们展示了EM算法的一次迭代。理想情况下，我们会执行更多次迭代，直到新θ的变化小于或等于某个停止条件值。你可能想知道为什么这看起来一点也不像EM算法方程，你是对的。让我们来看看我们是如何将θ最大化简化为归因头数乘以归因翻转数的。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lc"><img src="../Images/57173ebbce15855b8badd70d4b02949b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dg43nRhj92duPewhHvDfbg.jpeg"/></div></div></figure><p id="da40" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">虽然做了很多转换，但我认为这有点直观。我们有y_n，其中n在{1，2，3}中，是每个试验，y是代表试验n的硬币的潜在数据，如果我们最大化硬币1的θ，y_n将是试验n是硬币1的偶数。我们最多只进行3次试验，因为这是我们在第一次迭代中定义的。理论上，可以进行任意数量的试验。从这个方程中，我们可以解出n_H和n_T，它们是每次试验的头和尾。这看起来应该很熟悉。从MLE的帖子中我们知道，如果我们对Q求导，就可以找到使θ最大化的值。我们在这里得到的问题的EM公式和问题的MLE公式的结果之间唯一真正的区别是，我们不知道抛的是什么硬币。我们必须根据人头来自某个硬币的概率来衡量我们观察到的人头的数量。</p><h1 id="80b9" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">结论</h1><p id="c30c" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">EM算法有一些缺点。EM算法中的E步骤在计算上可能非常昂贵，尤其是该算法接近最大值。这主要是由于求解p(y|x，θ’)可能的复杂性。EM算法的另一个缺点是它会收敛到局部最大值。尽管EM算法有缺点，但它是一种解决具有潜在数据的数据集的最大似然估计的强大算法。如果你喜欢这篇文章，一定要击碎鼓掌按钮。如果你觉得有帮助，我会看看我的其他帖子。下次见！</p></div></div>    
</body>
</html>