<html>
<head>
<title>Gradient Descent Part 2: The Math</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降第2部分:数学</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-part-2-the-math-c23060a96a13?source=collection_archive---------18-----------------------#2021-01-12">https://medium.com/analytics-vidhya/gradient-descent-part-2-the-math-c23060a96a13?source=collection_archive---------18-----------------------#2021-01-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8305" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降的数学原理。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/a468e616762ff0bc55d256cc55fa1fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*laN3aseisIU3T9QTIlob4Q.gif"/></div></figure><p id="97bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文的第一部分，我们看到了对梯度下降的直观理解，以及对其进行数学理解所需的一些概念。在这篇文章中，我们将深入梯度下降的数学细节。如果你还没有阅读这个博客的第一个直观部分，我建议你通过<a class="ae jl" href="https://harshjadhav100.medium.com/gradient-descent-part-1-the-intuition-a154a6d43c2e" rel="noopener"> <strong class="ih hj"> <em class="jm">点击这里</em> </strong> </a> <em class="jm">来阅读这个博客。</em></p><p id="172f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本博客概述:</p><ol class=""><li id="ab29" class="jn jo hi ih b ii ij im in iq jp iu jq iy jr jc js jt ju jv bi translated">什么是机器学习中的优化？</li><li id="7c81" class="jn jo hi ih b ii jw im jx iq jy iu jz iy ka jc js jt ju jv bi translated">我们如何使用梯度下降进行优化？</li></ol><p id="7d24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在准备好出发了，</p><h1 id="06fa" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">1.什么是机器学习中的优化？</h1><p id="fcdf" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">你一定想知道为什么我们会读到这个优化的东西，原因很简单，梯度下降是一个优化算法，要理解它，我们必须先知道什么是优化。简单来说，优化就是“最大限度或最有效地利用环境或资源的行为。”  <em class="jm"> </em>但是在机器学习方面那些情况或者资源是什么？当我们使用机器学习算法来执行一些预测任务时，每个算法都有一些损失函数来衡量该算法执行预测任务的性能。这个损失函数帮助我们了解机器学习算法预测期望输出的效果。损失越低，算法越好，因此任何机器学习任务的目标都是实现最小的损失，从而实现最大的准确性。损失基本上是一种误差测量技术，也称为成本函数。我们可以称之为计算期望输出和我们的模型预测之间的差异的函数。</p><p id="9e7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">优化的目标是找到最小化损失函数的特定算法的最佳参数。我们已经在之前的博客中看到了什么是最小值和最大值，优化试图找到损失函数的最小值，即'<em class="jm"> x' </em>的值，为此我们得到了'<em class="jm"> f(x) </em>'的最小值。</p><p id="d655" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">酷，这并不复杂。</p><h1 id="59b6" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak"> 2。我们如何使用梯度下降进行优化？</strong></h1><p id="635b" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">正如我们上面讨论的，我们建立了一个机器学习模型来执行预测任务，因此它总是具有成本/损失函数。现在在这一部分，我们将看到如何使用梯度下降进行优化。看看下面这张漂亮的图表，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es le"><img src="../Images/d9eb4b9cb7887622a63cd7c0f4139225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hZMyUoh5eas8tvTYTH3QPQ.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图片提供:<a class="ae jl" href="https://paperswithcode.com/method/sgd" rel="noopener ugc nofollow" target="_blank">https://paperswithcode.com/method/sgd</a></figcaption></figure><p id="e4fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，在“X”轴上，我们有“w”变量，在“Y”轴上，我们有J(w)，即“w”的某个函数。</p><p id="b905" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是执行优化的步骤:</p><p id="5810" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，初始化函数J(w)的参数，即随机设置“w”的值，</p><p id="c52e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，对于每次迭代，通过使用以下公式计算新的“w ”,</p><p id="a4ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">w _ new = w _ old-r *(<em class="jm">dJ(w)/dw)</em></p><p id="d3a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">并迭代直到我们找到最小值J(w ),即直到我们得到给出如上图所示的最小值J(w)的值“w”。这里(<em class="jm"> dJ(w)/dw) </em>是斜率或梯度(损失函数w.r.t .对参数的导数)，而‘r’是学习速率或步长。</p><p id="a862" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学习速率或步长“r”是一个常数值，我们用它来定义成本函数在每次迭代中应该收敛多少值。</p><p id="a764" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数学也没那么复杂，对吧？无论我们在上面看到了什么，我们都将以代码的形式看到，</p><pre class="je jf jg jh fd ln lo lp lq aw lr bi"><span id="e675" class="ls kc hi lo b fi lt lu l lv lw">weights <strong class="lo hj">= </strong>0.01 # intial value we can keep any random value<strong class="lo hj"><br/></strong>step_size <strong class="lo hj">=</strong> 0.001</span><span id="9ee6" class="ls kc hi lo b fi lx lu l lv lw"><strong class="lo hj">for </strong>epoch<strong class="lo hj"> in range</strong>(<strong class="lo hj">100</strong>)<strong class="lo hj">:<br/>    </strong>weights_grad <strong class="lo hj">=</strong> evaluate_gradient(loss_fun, data, weights)<br/>    weights <strong class="lo hj">=</strong> weights <strong class="lo hj">-</strong> step_size <strong class="lo hj">*</strong> weights_grad <em class="jm"># parameter update</em></span></pre><p id="3c74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就像它是数学一样，代码也很简单。相信我，这是许多机器学习算法中使用的优化的核心思想，如果你理解了这一点，你就能够很容易地理解其他方法。</p><p id="d7e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好的，我们上面看到的梯度下降也称为批量梯度下降，因为它考虑了数据集中的所有数据点，用于每次迭代的计算，并在迭代后用整个数据集更新权重，这对于小数据集来说是好的，但在大数据集的情况下，考虑所有数据点在计算上变得非常昂贵。</p><p id="a97d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，为了解决这个问题，研究人员提出了一个很好的GD变体，称为随机梯度下降(SGD)。在SGD中，唯一的区别是我们使用一个小的随机数据子集，而不是整个数据集，并且权重更新发生在每个数据点，而不是在整个迭代之后。这使得该算法的计算成本更低，即使对于大型数据集。其余部分同批次GD。让我们看看SGD的代码，</p><pre class="je jf jg jh fd ln lo lp lq aw lr bi"><span id="14d9" class="ls kc hi lo b fi lt lu l lv lw">weights <strong class="lo hj">= </strong>0.0<!-- -->1<!-- --> # intial value we can keep any random value<strong class="lo hj"><br/></strong>step_size <strong class="lo hj">=</strong> 0.001<strong class="lo hj"><br/>for </strong>epoch <strong class="lo hj">in range</strong>(<strong class="lo hj">100</strong>):   <br/>    np.random.shuffle(data)   <br/>    <strong class="lo hj">for</strong> example <strong class="lo hj">in</strong> data:     <br/>        <!-- -->weights_grad <strong class="lo hj">= </strong>evaluate_gradient(loss_fun, example, params)<br/>        <!-- -->weights <strong class="lo hj">= </strong>weights <strong class="lo hj">-</strong> <!-- -->step_size <strong class="lo hj">*</strong> <!-- -->weights_grad</span></pre><p id="5474" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GD还有另一种变体，即小批量梯度下降。我们将直接看到它的代码，你自己就会明白它与其他两个变体的不同之处。</p><pre class="je jf jg jh fd ln lo lp lq aw lr bi"><span id="2630" class="ls kc hi lo b fi lt lu l lv lw">weights <strong class="lo hj">= </strong>0.0<!-- -->1<!-- --> # intial value we can keep any random value<strong class="lo hj"><br/></strong>step_size <strong class="lo hj">=</strong> 0.001<br/>batch_size <strong class="lo hj">= </strong>50<strong class="lo hj"><br/>for</strong> <!-- -->epoch <strong class="lo hj">in</strong> <strong class="lo hj">range</strong>(<strong class="lo hj">100</strong>):<br/>    <strong class="lo hj">for</strong> batch <strong class="lo hj">in</strong> <!-- -->np.random.choice(<!-- -->data<!-- -->, <!-- -->batch_size<!-- -->)<!-- -->: <br/>        <!-- -->weights<!-- -->_grad <strong class="lo hj">=</strong> evaluate_gradient(loss_fun, batch, <!-- -->weights)<br/>        weights <strong class="lo hj">=</strong> <!-- -->weights <strong class="lo hj">— </strong>step_size <strong class="lo hj">*</strong> <!-- -->weights<!-- -->_grad</span></pre><p id="1627" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我知道你在这种类型的GD中得到了不同，是的，你很聪明，猜对了，小批量梯度下降对每个小批量训练样本执行权重更新。</p><p id="22a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">很好，到目前为止，您已经了解了机器学习领域中非常常见且最重要的优化算法。</p><p id="70fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么下一步是什么？我们应该检查这种优化对于实际的机器学习算法是如何工作的。我肯定会在以后的博客中报道这件事。非常感谢你坚持到最后。下一篇博客再见。</p><blockquote class="ly lz ma"><p id="6818" class="if ig jm ih b ii ij ik il im in io ip mb ir is it mc iv iw ix md iz ja jb jc hb bi translated">参考资料:</p></blockquote><ul class=""><li id="b37a" class="jn jo hi ih b ii ij im in iq jp iu jq iy jr jc me jt ju jv bi translated">【https://cs231n.github.io/optimization-1/#optimization T4】</li><li id="86e0" class="jn jo hi ih b ii jw im jx iq jy iu jz iy ka jc me jt ju jv bi translated"><a class="ae jl" href="https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants" rel="noopener ugc nofollow" target="_blank">https://ruder.io/optimizing-gradient-descent/index.html</a></li></ul></div></div>    
</body>
</html>