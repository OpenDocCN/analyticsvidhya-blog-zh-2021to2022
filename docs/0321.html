<html>
<head>
<title>Text Classification — From Bag-of-Words to BERT — Part 6( BERT )</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本分类—从词袋到BERT —第六部分(BERT)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-6-bert-2c3a5821ed16?source=collection_archive---------8-----------------------#2021-01-12">https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-6-bert-2c3a5821ed16?source=collection_archive---------8-----------------------#2021-01-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a0af3f3b28d64dcdb258a595b4716e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nxd9cSeI8vUH4qnB"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://unsplash.com/@samule?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Samule孙</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="48ec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个故事是一系列文本分类的一部分——从词袋到BERT在名为“<a class="ae iu" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge" rel="noopener ugc nofollow" target="_blank"> <em class="jt">有毒评论分类挑战”</em> </a> <strong class="ix hj"> <em class="jt">的Kaggle比赛上实施多种方法。</em> </strong>在这场比赛中，我们面临的挑战是建立一个多头模型，能够检测不同类型的毒性，如<em class="jt">威胁、淫秽、侮辱和基于身份的仇恨。如果你还没看过之前的报道，那就去看看吧</em></p><p id="d3b7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9" rel="noopener">第一部分(BagOfWords) </a></p><p id="7fe8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-2-word2vec-35c8c3b34ee3" rel="noopener">第二部分(Word2Vec) </a></p><p id="c85b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-3-fasttext-8313e7a14fce" rel="noopener">第三部分(快速文本)</a></p><p id="bdbf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-4-convolutional-neural-network-53aa63941ade" rel="noopener">第四部分(卷积神经网络)</a></p><p id="de2d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" rel="noopener" href="/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-5-recurrent-neural-network-b825ffc8cb26">第五部分(递归神经网络)</a></p><p id="5deb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在前面的故事中(<a class="ae iu" rel="noopener" href="/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-5-recurrent-neural-network-b825ffc8cb26">第5部分(递归神经网络)</a>)，我们将使用相同的Keras库来创建LSTMs，这是对用于多标签文本分类的常规RNNs的改进。</p><p id="54bf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这一次，我们将使用预训练的BERT模型，并使用迁移学习在我们的数据集上对其进行微调。我们将首先通过对变压器和BERT如何工作的一点直觉，然后使用最小化的单输出层(具有6个神经元)来实现多标签分类。在仅仅一个时期的微调中，它在排行榜上给出了大约98个AUC。这一次会比平时长，但请耐心等待，因为这是值得的。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="90ce" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">直觉</h1><h2 id="e995" class="kz kc hi bd kd la lb lc kh ld le lf kl jg lg lh kp jk li lj kt jo lk ll kx lm bi translated">变形金刚(电影名)</h2><p id="a94c" class="pw-post-body-paragraph iv iw hi ix b iy ln ja jb jc lo je jf jg lp ji jj jk lq jm jn jo lr jq jr js hb bi translated">在引入变压器之前，大多数最先进的NLP系统都依赖门控rnn，如LSTMs和GRUs，这一点引起了更多关注。Transformer建立在这种注意力的基础上，没有使用RNN结构，突出了这样一个事实，即没有重复的顺序处理，注意力本身就足以实现带注意力的RNNs的性能，并且更具并行性，需要的训练时间也少得多。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ls"><img src="../Images/54c9c1eece91680ccb5197de8a8dd2c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bTNcS9h5zIzL0e5tobkIRg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">变压器整体架构</figcaption></figure><p id="9aae" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在机器翻译应用程序中，它会将一种语言的句子转换成另一种语言输出。在里面，我们看到一个编码组件，一个解码组件，以及它们之间的联系。编码组件由N = 6个相同层的堆叠组成。解码组件是相同数量的解码器的堆栈。</p><p id="afe8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">编码器的输入首先流经自我关注层，该层帮助编码器在编码特定单词时查看输入句子中的其他单词。自我注意层的输出被馈送到一个前馈神经网络。完全相同的前馈网络独立地应用于每个位置。解码器有这两层，但在它们之间有一个注意力层，帮助解码器关注输入句子的相关部分</p><p id="a1a7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">就像NLP中的一般情况一样，我们使用嵌入算法将每个输入单词转换成一个向量。嵌入只发生在最底层的编码器中。但是在其他编码器中，它是直接在下面的编码器的输出。</p><p id="edbf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> N.B. </strong>:每个位置的字在编码器中流经自己的路径。在自我关注层，这些路径之间存在依赖关系。前馈层没有这些依赖性，因此各种路径可以在流经FF层时并行执行。</p><p id="820a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">自我关注:</em> </strong>自我关注是变形者用来把对其他相关词汇的“理解”烘焙到我们当前正在处理的一个中的方法。我们遵循以下步骤:</p><p id="61b2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1.从编码器的每个输入向量中创建3个向量。因此，对于每个单词，我们创建一个查询向量、一个键向量和一个维数为64的值向量。这些向量是通过将输入乘以我们在训练过程中训练的三个矩阵而创建的。</p><p id="a84a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.算一个分数。假设我们在计算第一个单词的自我关注度。我们需要将输入句子中的每个单词与这个单词进行比较。分数决定了当我们在某个位置对一个单词进行编码时，对输入句子的其他部分的关注程度。分数是通过查询向量与我们正在评分的相应单词的关键向量的点积来计算的。因此，如果我们正在处理位置#1的单词的自我注意，第一个分数将是q1和k1的点积。第二个分数是q1和k2的点积。</p><p id="6823" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.&amp; 4.将分数除以8(所用关键向量维数的平方根— 64。这导致具有更稳定的梯度。然后通过softmax操作传递结果。Softmax将分数标准化，因此它们都是正数，加起来等于1。</p><p id="4df1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">5.将每个值向量乘以softmax分数。这里的直觉是保持我们想要关注的词的价值不变，淹没无关的词</p><p id="5071" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">6.对加权值向量求和。这就在这个位置产生了自我关注层的输出。</p><p id="1c7f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">多头自我关注:</em> </strong>论文通过增加“多头”关注，进一步细化了自我关注层。这从两个方面提高了注意力层的性能:</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/d9467e0ecb2dd4d67e905cd1df27f788.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PNdtPg2KINh9-lhrVswa0w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">多头自我关注摘要</figcaption></figure><ol class=""><li id="2061" class="ly lz hi ix b iy iz jc jd jg ma jk mb jo mc js md me mf mg bi translated">它扩展了模型关注不同位置的能力。</li><li id="e466" class="ly lz hi ix b iy mh jc mi jg mj jk mk jo ml js md me mf mg bi translated">它给了注意力层多个“表示子空间”</li></ol><p id="c874" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们连接多个矩阵，然后将它们乘以附加的权重矩阵WO，以便它们可以被发送到前馈网络。</p><p id="00d8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">位置编码:</strong>转换器为每个输入嵌入添加一个向量，以说明单词在输入序列中的顺序。这里的直觉是，将这些值添加到嵌入中提供了嵌入向量之间有意义的距离，一旦它们被投影到Q/K/V向量中和在点积注意力期间。左半部分的值由正弦函数生成，右半部分的值由余弦函数生成。然后将它们连接起来，形成每个位置编码向量。</p><p id="d072" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">残差:</strong>每个编码器中的每个子层(自关注，ffnn)周围都有一个残差连接，然后是层归一化步骤。例如，将输入矩阵X (2 * 4)与输出矩阵Z (2 * 4)相加，并按行进行归一化。</p><p id="5bea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">解码器端</strong></p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/4336e8334a34ba69a8a8d9ad47e92967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*8jTqC2HMbBVVgVM63UOWyQ.gif"/></div></div></figure><p id="6fd2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">编码器从处理输入序列开始。顶部编码器的输出然后通过乘以权重向量被转换成一组注意力向量K和V。这些将由每个解码器在其“编码器-解码器关注”层中使用，这有助于解码器关注输入序列中的适当位置。以下步骤重复该过程，直到到达指示变压器解码器已经完成其输出的特殊符号。每一步的输出在下一个时间步被馈送到底部的解码器，解码器像编码器一样冒泡它们的解码结果。就像我们对编码器输入所做的一样，我们将位置编码嵌入并添加到这些解码器输入中，以指示每个单词的位置。</p><p id="65cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">解码器中的自我关注层的工作方式与编码器中的略有不同:在解码器中，自我关注层只被允许关注输出序列中较早的位置。这是通过在自我关注计算中的softmax步骤之前屏蔽未来位置(将它们设置为-inf)来实现的。“编码器-解码器关注”层的工作方式类似于多头自我关注，只是它从其下一层创建查询矩阵，并从编码器堆栈的输出中获取键和值矩阵。</p><p id="df87" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">最终的线性和Softmax层</strong>线性层是一个简单的全连接神经网络，它将解码器堆栈产生的向量投影到一个非常非常大的向量中，称为logits向量，其中每个单元都是词汇表中的一个单词。然后softmax层将这些分数转化为概率。选择概率最高的像元，并产生与之相关的单词作为该时间步长的输出。</p><p id="3b48" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">训练</strong>通过使用交叉熵损失将预测值与实际值进行比较来完成。因为，模型一次输出一个单词，我们可以使用束搜索来找到最佳的输出字符串</p><h2 id="d89d" class="kz kc hi bd kd la lb lc kh ld le lf kl jg lg lh kp jk li lj kt jo lk ll kx lm bi translated">来自变压器的双向编码器表示</h2><p id="eb94" class="pw-post-body-paragraph iv iw hi ix b iy ln ja jb jc lo je jf jg lp ji jj jk lq jm jn jo lr jq jr js hb bi translated">BERT基本上是一个训练有素的变压器编码器堆栈。我们预先训练伯特理解语言，并微调伯特学习特定的任务。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/ed214cf45b3621228c22ebfe9d89eec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iEI29ygldRiyc1KWml8bMQ.png"/></div></div></figure><p id="c628" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">预训练:</strong>这是通过同时做两件事来完成的:</p><ol class=""><li id="cb6a" class="ly lz hi ix b iy iz jc jd jg ma jk mb jo mc js md me mf mg bi translated">屏蔽语言建模(MLM):MLM从输入中随机屏蔽一些标记，目标是仅根据上下文预测屏蔽单词的原始词汇id。这只棕色的狐狸跳过了那只戴面具的狗。MLM目标使得表示能够融合左和右上下文，这允许我们预先训练深度双向转换器。训练数据生成器随机选择15%的标记位置进行预测。如果选择了第I个令牌，我们用(1)80%时间的[掩码]令牌(2)10%时间的随机令牌(3)10%时间的未改变的第I个令牌来替换第I个令牌。</li><li id="660d" class="ly lz hi ix b iy mh jc mi jg mj jk mk jo ml js md me mf mg bi translated">下一个句子预测(NSP):在这个任务中，取两个句子，如果这两个句子是一个接一个的或者不是，则进行二元分类。阿杰是个很酷的人，他住在俄亥俄州。</li></ol><p id="6547" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">输入:</strong></p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/19593d5c62e26df2785cb1595bd347f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*0c4xv6Agek7uKbjLwyc4ng.png"/></div></figure><p id="2678" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们为输入向量传入嵌入向量。这个嵌入向量是通过将相应的标记、段和位置嵌入相加而从输入向量中计算出来的。分段和位置编码用于模型以理解时间定位，因为这些输入是同时给出的，而不是像LSTMs中那样在一个时间步长给出的。对于令牌嵌入，我们使用具有30，000个令牌词汇的单词块嵌入。具体来说，在为每个预训练示例选择句子A和B时，B有50%的时间是跟随A的实际下一个句子(标记为IsNext)，有50%的时间是来自语料库的随机句子(标记为NotNext)。</p><p id="136f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">输出:</strong>MLM的输出是语句向量的形式，其中屏蔽的记号填充有预测的记号。然后，对词汇表中的所有单词进行软最大化，并根据实际值计算交叉熵损失。这样做是为了屏蔽文字。</p><p id="9ee0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">NSP的输出是输出中[CLS]令牌的结果。</p><p id="f0a7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">微调:</strong></p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/fea99de19088934112de24e59c6d59b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CBjZ9EpmORs4_BRHi-bChg.jpeg"/></div></div></figure><p id="95a2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于每个任务，我们只需将特定于任务的输入和输出插入到BERT中，并对所有参数进行端到端的微调。相比于预训练，微调相对便宜且快速。该论文给出了BERT的两种模型尺寸(对于，层数(即，变换器块)为L，隐藏尺寸为H，自关注头的数量为A):</p><p id="5347" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BERTBASE (L=12，H=768，A=12，总参数=110M)</p><p id="fcd1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">伯特拉奇(L=24，H=1024，A=16，总参数=340M)。</p><p id="89bd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于预训练语料库，我们使用了图书语料库(800M单词)和英语维基百科(2,500M单词)。</p><p id="a2d7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这些文本主要取自以下资源:【http://jalammar.github.io/illustrated-transformer/<a class="ae iu" href="https://www.youtube.com/watch?v=xI0HHN5XKDo" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=xI0HHN5XKDo</a>T4</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="85ad" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">履行</h1><p id="66ec" class="pw-post-body-paragraph iv iw hi ix b iy ln ja jb jc lo je jf jg lp ji jj jk lq jm jn jo lr jq jr js hb bi translated">你可以在这里找到完整的代码</p><h2 id="5ee2" class="kz kc hi bd kd la lb lc kh ld le lf kl jg lg lh kp jk li lj kt jo lk ll kx lm bi translated">1.读取数据集</h2><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/db99fdeb4f7e3db1767c0c6a683535fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*e1Iu83wD0WsoXjPZ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">提醒一下，这是训练数据的样子</figcaption></figure><h2 id="90a7" class="kz kc hi bd kd la lb lc kh ld le lf kl jg lg lh kp jk li lj kt jo lk ll kx lm bi translated"><strong class="ak"> 2。文本预处理</strong></h2><p id="4a0f" class="pw-post-body-paragraph iv iw hi ix b iy ln ja jb jc lo je jf jg lp ji jj jk lq jm jn jo lr jq jr js hb bi translated">首先，我们来清理一下数据:1。使用BeautifulSoup和2清理HTML标签。删除非字母数字数据</p><pre class="lt lu lv lw fd mr ms mt mu aw mv bi"><span id="6990" class="kz kc hi ms b fi mw mx l my mz">def strip(text):<br/> soup = BeautifulSoup(text, ‘html.parser’)<br/> text = re.sub(‘\[[^]]*\]’, ‘’, soup.get_text())<br/> pattern=r”[^a-zA-z0–9\s,’]”<br/> text=re.sub(pattern,’’,text)<br/> return text<br/>df_train[“comment_text”] = df_train[“comment_text”].apply(strip)<br/>df_test[“comment_text”] = df_test[“comment_text”].apply(strip)</span></pre><p id="6c1c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BERT输入的预处理:</p><pre class="lt lu lv lw fd mr ms mt mu aw mv bi"><span id="ccbd" class="kz kc hi ms b fi mw mx l my mz">#Adding [CLS] and [SEP] for each sentence<br/>train_sentences = df_train[“comment_text”]<br/>test_sentences = df_test[“comment_text”]<br/>train_sentences = [“[CLS] “+ i + “ [SEP]”for i in train_sentences]<br/>test_sentences = [“[CLS] “+ i + “ [SEP]”for i in test_sentences]<br/>print(train_sentences[0])<br/>#Tokenizing the input sequence<br/>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)<br/>train_tokenizer_texts = list(map(lambda t: tokenizer.tokenize(t)[:510], tqdm(train_sentences)))<br/>test_tokenizer_texts = list(map(lambda t: tokenizer.tokenize(t)[:510], tqdm(test_sentences)))<br/>print(np.array(train_tokenizer_texts[0]))<br/>#Converting Input words to corresponding ids<br/>input_ids = [tokenizer.convert_tokens_to_ids(x) for x <strong class="ms hj">in</strong> tqdm(train_tokenizer_texts)]<br/>input_ids = pad_sequences(sequences = input_ids, maxlen = MAX_LEN, dtype = 'long', padding='post', truncating='post')<br/>test_input_ids = [tokenizer.convert_tokens_to_ids(x) for x <strong class="ms hj">in</strong> <br/>tqdm(test_tokenizer_texts)]<br/>#Padding the input sequences to a fixed length<br/>MAX_LEN = 128<br/>test_input_ids = pad_sequences(sequences = test_input_ids, maxlen = MAX_LEN, dtype = 'long', padding='post', truncating='post')<br/><em class="jt">#Creating an attention mask - For actual tokens its set to 1, for padding tokens its set to 0</em><br/>def create_attention_masks(input_ids):<br/>    attention_masks = []<br/>    for seq <strong class="ms hj">in</strong> tqdm(input_ids):<br/>        seq_mask = [float(i&gt;0) for i <strong class="ms hj">in</strong> seq]<br/>        attention_masks.append(seq_mask)<br/>    return np.array(attention_masks)<br/><br/>attention_masks = create_attention_masks(input_ids)<br/>test_attention_masks = create_attention_masks(test_input_ids)<br/>attention_masks[0], test_attention_masks[0]</span></pre><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es na"><img src="../Images/f8df9ff8fd95b4715d4d9818ff542373.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*Nrr5KDaidsaYPgitONvMrg.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">预处理的结果</figcaption></figure><ol class=""><li id="7807" class="ly lz hi ix b iy iz jc jd jg ma jk mb jo mc js md me mf mg bi translated">每个序列的第一个标记总是一个特殊的分类标记([CLS])，我们用一个特殊的标记([SEP])来分隔句子</li><li id="c2b4" class="ly lz hi ix b iy mh jc mi jg mj jk mk jo ml js md me mf mg bi translated">输入必须被分割成标记，然后这些标记必须被映射到它们在标记化器词汇表中的索引。</li><li id="9142" class="ly lz hi ix b iy mh jc mi jg mj jk mk jo ml js md me mf mg bi translated">填充输入的符号化索引序列</li><li id="7e87" class="ly lz hi ix b iy mh jc mi jg mj jk mk jo ml js md me mf mg bi translated">创建一个注意力面具</li></ol></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h2 id="5e81" class="kz kc hi bd kd la lb lc kh ld le lf kl jg lg lh kp jk li lj kt jo lk ll kx lm bi translated">3.创建训练和验证数据加载器</h2><p id="f216" class="pw-post-body-paragraph iv iw hi ix b iy ln ja jb jc lo je jf jg lp ji jj jk lq jm jn jo lr jq jr js hb bi translated">首先，我们将预训练数据集分为训练和验证两部分。然后，我们将所有数据集转换为torch类型，以便PyTorch可以使用它。最后，我们将创建训练和验证数据加载器，它们可用于在训练/预测时生成批量数据</p><pre class="lt lu lv lw fd mr ms mt mu aw mv bi"><span id="222c" class="kz kc hi ms b fi mw mx l my mz"><em class="jt"># Use train_test_split to split our data and attention masks into train and validation sets for training</em><br/>X_train, X_val, y_train, y_val = train_test_split(input_ids, labels, random_state = 123, test_size = 0.20)<br/>attention_masks_train, attention_masks_val = train_test_split(attention_masks, random_state = 123, test_size = 0.20)<em class="jt"># Convert all inputs and labels into torch tensors, the required datatype </em><br/>X_train = torch.tensor(X_train)<br/>X_val = torch.tensor(X_val)<br/>y_train = torch.tensor(y_train) <br/>y_val = torch.tensor(y_val)<br/>attention_masks_train = torch.tensor(attention_masks_train)<br/>attention_masks_val = torch.tensor(attention_masks_val)<br/>test_input_ids = torch.tensor(test_input_ids)<br/>test_attention_masks = torch.tensor(test_attention_masks)</span><span id="5ebc" class="kz kc hi ms b fi nb mx l my mz">BATCH_SIZE = 32<br/><em class="jt">#Dataset wrapping tensors.</em><br/>train_data = TensorDataset(X_train, attention_masks_train, y_train)<br/>val_data = TensorDataset(X_val, attention_masks_val, y_val)<br/>test_data = TensorDataset(test_input_ids, test_attention_masks)<br/><em class="jt">#Samples elements randomly. If without replacement(default), then sample from a shuffled dataset.</em><br/>train_sampler = RandomSampler(train_data)<br/>val_sampler = SequentialSampler(val_data)<br/>test_sampler = SequentialSampler(test_data)<br/><em class="jt">#represents a Python iterable over a dataset</em><br/>train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE)<br/>val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = BATCH_SIZE)<br/>test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = BATCH_SIZE)</span></pre><h2 id="42a3" class="kz kc hi bd kd la lb lc kh ld le lf kl jg lg lh kp jk li lj kt jo lk ll kx lm bi translated">4.加载预训练的BERT并设置微调参数</h2><p id="8803" class="pw-post-body-paragraph iv iw hi ix b iy ln ja jb jc lo je jf jg lp ji jj jk lq jm jn jo lr jq jr js hb bi translated">首先，我们加载用于序列分类的BERT模型。我们将输出神经元设置为6，因为我们有6种毒性类型要预测为是或否。</p><p id="2e8f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我们获得不同层的权重，并将它们放入一个列表中。完成后，我们将权重参数(需要更新)与偏差、伽玛和贝塔参数(不需要更新)分开。</p><pre class="lt lu lv lw fd mr ms mt mu aw mv bi"><span id="3b5f" class="kz kc hi ms b fi mw mx l my mz"><em class="jt">#Inititaing a BERT model</em><br/>model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 6)<br/>model.cuda()<br/><em class="jt">#Dividing the params into those which needs to be updated and rest</em><br/>param_optimizer = list(model.named_parameters())<br/>no_decay = ['bias', 'gamma', 'beta']<br/>optimizer_grouped_parameters = [{<br/>        'params': [p for n, p <strong class="ms hj">in</strong> param_optimizer if <strong class="ms hj">not</strong> any(nd <strong class="ms hj">in</strong> n for nd <strong class="ms hj">in</strong> no_decay)],<br/>        'weight_decay_rate': 0.01 },<br/>{       'params': [p for n, p <strong class="ms hj">in</strong> param_optimizer if any(nd <strong class="ms hj">in</strong> n for nd <strong class="ms hj">in</strong> no_decay)],<br/>        'weight_decay_rate': 0.0 }]<br/>optimizer = BertAdam(optimizer_grouped_parameters, lr = 2e-5, warmup = .1)</span></pre><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es nc"><img src="../Images/c7f25004c211cbef96557b639c9b6e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*hVU4XaOhyKwPo8SdkGXsEQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">伯特模型</figcaption></figure><h2 id="0d06" class="kz kc hi bd kd la lb lc kh ld le lf kl jg lg lh kp jk li lj kt jo lk ll kx lm bi translated">5.调整BERT模型</h2><p id="2949" class="pw-post-body-paragraph iv iw hi ix b iy ln ja jb jc lo je jf jg lp ji jj jk lq jm jn jo lr jq jr js hb bi translated">在这个阶段，我们根据需要对数据集上的BERT模型进行多次微调，并根据验证数据验证模型性能。我们将使用BCEWithLogitLoss函数来计算预测值和实际值之间的多标签损失。这类似于在网络末端添加一个sigmoid函数，并计算二元交叉熵损失，就像我们在Keras为LSTM和CNN所做的那样。</p><pre class="lt lu lv lw fd mr ms mt mu aw mv bi"><span id="163f" class="kz kc hi ms b fi mw mx l my mz"><em class="jt">#Empty the GPU memory as it might be memory and CPU intensive while training</em><br/>torch.cuda.empty_cache()<br/><em class="jt">#Number of times the whole dataset will run through the network and model is fine-tuned</em><br/>epochs = 2<br/><em class="jt">#Iterate over number of epochs</em><br/>for _ <strong class="ms hj">in</strong> trange(epochs, desc = "Epoch"):<br/>    <em class="jt">#Switch model to train phase where it will update gradients</em><br/>    model.train()<br/>    <em class="jt">#Initaite train and validation loss, number of rows passed and number of batches passed</em><br/>    tr_loss = 0<br/>    nb_tr_examples, nb_tr_steps = 0, 0<br/>    val_loss = 0<br/>    nb_val_examples, nb_val_steps = 0, 0<br/>    <em class="jt">#Iterate over batches within the same epoch</em><br/>    for batch <strong class="ms hj">in</strong> tqdm(train_dataloader):<br/>        <em class="jt">#Shift the batch to GPU for computation</em><br/>        batch = tuple(t.to(device) for t <strong class="ms hj">in</strong> batch)<br/>        <em class="jt">#Load the input ids and masks from the batch</em><br/>        b_input_ids, b_input_mask, b_labels = batch<br/>        <em class="jt">#Initiate gradients to 0 as they tend to add up</em><br/>        optimizer.zero_grad()<br/>        <em class="jt">#Forward pass the input data</em><br/>        logits = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)<br/>        <em class="jt">#We will be using the Binary Cross entropy loss with added sigmoid function after that in BCEWithLogitsLoss</em><br/>        loss_func = BCEWithLogitsLoss()<br/>        <em class="jt">#Calculate the loss between multilabel predicted outputs and actuals</em><br/>        loss = loss_func(logits, b_labels.type_as(logits))<br/>        <em class="jt">#Backpropogate the loss and calculate the gradients</em><br/>        loss.backward()<br/>        <em class="jt">#Update the weights with the calculated gradients</em><br/>        optimizer.step()<br/>        <em class="jt">#Add the loss of the batch to the final loss, number of rows and batches</em><br/>        tr_loss += loss.item()<br/>        nb_tr_examples += b_input_ids.size(0)<br/>        nb_tr_steps += 1<br/>    <em class="jt">#Print the current training loss </em><br/>    print("Train Loss: <strong class="ms hj">{}</strong>".format(tr_loss/nb_tr_examples))<br/>    <em class="jt">#Switch the model to evaluate stage at which the gradients wont be updated</em><br/>    model.eval()<br/>    <em class="jt">#Iterate over the validation data</em><br/>    for step, batch <strong class="ms hj">in</strong> enumerate(val_dataloader):<br/>        <em class="jt">#Shift the validation data to GPUs for computation</em><br/>        batch = tuple(t.to(device) for t <strong class="ms hj">in</strong> batch)<br/>        <em class="jt">#We dont want to update the gradients</em><br/>        with torch.no_grad():<br/>            <em class="jt">#Load the input ids and masks from the batch</em><br/>            b_input_ids, b_input_mask, b_labels = batch<br/>            <em class="jt">#Forward pass the input data</em><br/>            logits = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)<br/>            <em class="jt">#We will be using the Binary Cross entropy loss with added sigmoid function after that in BCEWithLogitsLoss</em><br/>            loss_func = BCEWithLogitsLoss()<br/>            <em class="jt">#Calculate the loss between multilabel predicted outputs and actuals</em><br/>            loss = loss_func(logits, b_labels.type_as(logits))<br/>            <em class="jt">#Add the loss of the batch to the final loss, number of rows and batches</em><br/>            val_loss += loss.item()<br/>            nb_val_examples += b_input_ids.size(0)<br/>            nb_val_steps += 1<br/>    <em class="jt">#Print the current validation loss     </em><br/>    print("Valid Loss: <strong class="ms hj">{}</strong>".format(val_loss/nb_val_examples))</span></pre><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/28946b7ed00a8ea0b5581b2c359decfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*N6N1XZGrNw3f8v7RgAAZMw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">第一时段的平均损失</figcaption></figure><h2 id="607d" class="kz kc hi bd kd la lb lc kh ld le lf kl jg lg lh kp jk li lj kt jo lk ll kx lm bi translated">6.改进的结果和范围</h2><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/40d11afac8a69a626afeead275c8db82.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*R5rPgDrIXs_BFuMkx8Rd2Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">Kaggle排行榜分数(相当惊人的一个时代吧？)</figcaption></figure><ul class=""><li id="fe30" class="ly lz hi ix b iy iz jc jd jg ma jk mb jo mc js nf me mf mg bi translated">尝试不同版本的BERT — RoBERTa、DistilBERT和ALBERT</li><li id="830b" class="ly lz hi ix b iy mh jc mi jg mj jk mk jo ml js nf me mf mg bi translated">时期、学习率、批量大小、提前停止的超参数调整</li></ul><p id="a51f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">同样，这个博客的全部代码呈现在<a class="ae iu" href="https://www.kaggle.com/anirbansen3027/jtcc-multilabel-bert-pytorch" rel="noopener ugc nofollow" target="_blank">(这里)</a>。请以回答和鼓掌的形式提供您的反馈:)</p></div></div>    
</body>
</html>