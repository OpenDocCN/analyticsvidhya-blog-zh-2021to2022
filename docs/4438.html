<html>
<head>
<title>Exploratory Data Analysis (EDA), Feature Selection, and machine learning prediction on time series data.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索性数据分析(EDA)、特征选择和对时间序列数据的机器学习预测。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/descriptive-predictive-and-feature-selection-on-time-series-data-813a202312b1?source=collection_archive---------0-----------------------#2021-10-13">https://medium.com/analytics-vidhya/descriptive-predictive-and-feature-selection-on-time-series-data-813a202312b1?source=collection_archive---------0-----------------------#2021-10-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/200b8fe7201720b3d59b2b2ec3b9a88e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QMpWncRrNbe1wmxznxSpSA.jpeg"/></div></div></figure><div class=""/><h1 id="f641" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">介绍</h1><p id="73e3" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><strong class="jq hu">数据集描述:【1950年至2015年瑞典犯罪统计。</strong></p><p id="a461" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hu">数据集属性信息</strong> : <strong class="jq hu">犯罪总数</strong>:报案总数。<strong class="jq hu"> crimes.penal.code </strong>:举报的违反刑法的犯罪总数。<strong class="jq hu"> crimes.person </strong>:针对某个人的举报犯罪总数。<strong class="jq hu">谋杀</strong>:报告的谋杀总数。<strong class="jq hu">性犯罪</strong>:报告的性犯罪总数。<strong class="jq hu">强奸</strong>:举报的强奸总数。<strong class="jq hu">殴打</strong>:举报的严重殴打总数。<strong class="jq hu">偷窃。概述</strong>:报告的偷窃或抢劫犯罪总数。抢劫:报告的武装抢劫总数。<strong class="jq hu">入室盗窃</strong>:报告的持械入室盗窃总数。<strong class="jq hu">车辆失窃</strong>:车辆失窃报告总数。<strong class="jq hu"> house.theft </strong>:房屋内报告的盗窃总数。商店盗窃:商店内报告的盗窃总数。<strong class="jq hu">车外盗窃</strong>:报告的车辆盗窃总数。<strong class="jq hu">刑事损害</strong>:报告的刑事损害总数。<strong class="jq hu">其他刑事犯罪</strong>:其他刑事犯罪数量。<strong class="jq hu">欺诈</strong>:报告的欺诈总数。<strong class="jq hu">麻醉品</strong>:报告的麻醉品滥用总数。<strong class="jq hu">酒驾</strong>:举报酒驾事件总数。<strong class="jq hu">年份</strong>:年份。人口:当时估计的瑞典人口总数</p><p id="89a1" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hu">下载数据集:</strong><a class="ae kr" href="https://www.kaggle.com/mguzmann/swedishcrime" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/mguzmann/swedishcrime</a></p><p id="ec3e" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hu">目标:</strong>在这个项目中，我们将使用瑞典犯罪率数据集训练一个机器学习模型来预测瑞典的muder率，并通过在瑞典犯罪数据集上完成以下任务来执行探索性数据分析(EDA)和特征选择:</p><blockquote class="ks kt ku"><p id="67bd" class="jo jp kv jq b jr km jt ju jv kn jx jy kw ko kb kc kx kp kf kg ky kq kj kk kl hb bi translated"><em class="ht"> 1。加载并查看数据集</em></p><p id="44b4" class="jo jp kv jq b jr km jt ju jv kn jx jy kw ko kb kc kx kp kf kg ky kq kj kk kl hb bi translated"><em class="ht"> 2。数据可视化</em></p><p id="9c85" class="jo jp kv jq b jr km jt ju jv kn jx jy kw ko kb kc kx kp kf kg ky kq kj kk kl hb bi translated"><em class="ht"> 3。数据预处理(数据编码、处理缺失值、处理异常值(检测、移除和替换)和标准化)</em></p><p id="ccd4" class="jo jp kv jq b jr km jt ju jv kn jx jy kw ko kb kc kx kp kf kg ky kq kj kk kl hb bi translated"><em class="ht"> 4。使用过滤、嵌入和包装方法进行特征选择。</em></p><p id="cb8a" class="jo jp kv jq b jr km jt ju jv kn jx jy kw ko kb kc kx kp kf kg ky kq kj kk kl hb bi translated"><em class="ht"> 5。比较没有特征选择的训练和有特征选择的训练(过滤方法(卡方)、包装方法(RFE)和嵌入方法(套索))</em></p><p id="37ea" class="jo jp kv jq b jr km jt ju jv kn jx jy kw ko kb kc kx kp kf kg ky kq kj kk kl hb bi translated"><em class="ht"> 6。时间序列或回归算法比较(朴素贝叶斯、k-最近邻、支持向量机、卷积神经网络和递归神经网络(RNN)(LSTM) </em></p><p id="e8b7" class="jo jp kv jq b jr km jt ju jv kn jx jy kw ko kb kc kx kp kf kg ky kq kj kk kl hb bi translated"><em class="ht"> 7。保存训练好的模型</em></p></blockquote><h1 id="c2ac" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">1.<strong class="ak">加载并查看数据集</strong></h1><p id="cad4" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">查看前五行和数据集的形状，同时检查是否有丢失的数据。最后检查数据集数据类型</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es kz"><img src="../Images/14d7f249610cc5582bcbe1d5c0708884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*E3tsZ6_0kc1bvOF1IbzBug.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">查看前五行和数据集的形状，同时检查是否有丢失的数据。</figcaption></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es li"><img src="../Images/3a7b2a29b87f9d3afcf58942c6798583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*gwnwVqdFUhsX-N-6ajrxiw.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">检查数据集数据类型</figcaption></figure><h1 id="6ca1" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 2。数据可视化</strong></h1><p id="729b" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">我们可以使用折线图、密度图、散点图、条形图和直方图绘制一些可视化图形。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lj"><img src="../Images/af80cc1d59b977a56f98fc9166b3624d.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*JOsyTObViOfPPfEC0Q3sBA.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">显示每年犯罪和入室盗窃的折线图</figcaption></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es lk"><img src="../Images/b49ffd8b3899ef3e8a507d1a49f9d4f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*dfzQMLfHsPbVw6xEaSyntg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">显示每年攻击事件的密度图</figcaption></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es ll"><img src="../Images/ec6059a5efd6867047023f76f374463c.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*uZvl6_RqNCfO8lW5BG1_WA.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">散点图显示每年的欺诈</figcaption></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lm"><img src="../Images/c7ab0861d16ca721e5a937bf5d293750.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l8huEvRxl71Z2nM5YBZNaw.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">显示每年入室盗窃的条形图</figcaption></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ln"><img src="../Images/f20523bfc779fec5ba69ac30eed1a3d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ibCyxVqdO1jJQSRNJXzEoQ.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">显示每年强奸案的柱状图</figcaption></figure><h1 id="93c1" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 3。数据预处理(数据编码、处理缺失值、处理异常值(检测、移除和替换)和标准化)</strong></h1><p id="53de" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">3.1处理缺失值:</p><p id="49e9" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">用列的平均值替换缺失的数据</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es lo"><img src="../Images/f9b5f261b0bcbd713650535ae8bb06d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*N0xgj4__xcbAQ7iXHyiKJA.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">用列的平均值替换缺失的数据</figcaption></figure><p id="bb2c" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">3.2编码数据</p><p id="accd" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因为数据集中没有对象列，所以不需要对列进行编码</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es lk"><img src="../Images/bdb481ec7446567fca3f883cbc7f684a.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*MEgcyNwP8LICYtdvEh5eBA.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">检查列数据类型</figcaption></figure><p id="f940" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">3.3异常值检测</p><p id="02fe" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">可视化方法</p><p id="d2a5" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">可视化方法可以用箱线图和分布图来完成，下面使用其他图、箱线图和分布图。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lp"><img src="../Images/da4cbe20f08f7839a3501a0d4d3d4d76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NfuPIr0bB-6FIO6Be3I15A.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">强奸和犯罪列的箱线图异常值检测。</figcaption></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es lq"><img src="../Images/ade82ba03a312ae44507ed6616416a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*NKFTjIpHSy3_xEyWuziKCA.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">分布图异常检测</figcaption></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lr"><img src="../Images/4dcb450c0ceed5cee4bb1b613b9e68f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Op_AIc-T6BVxZZjy-poXKQ.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">犯罪和强奸列的分布图异常检测</figcaption></figure><p id="4448" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">Z分数检测</p><p id="2fee" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">另一种异常值检测方法是Z值:</p><p id="e99d" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">z分数也称为标准分数。z得分告诉我们一个数据点离平均值有多少标准偏差，它有助于理解一个数据值是大于还是小于平均值，以及它离平均值有多远。更确切地说。</p><p id="c8e9" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们计算每一列的Z分数。并且设置阈值，该阈值指示该数据点与其他数据点非常不同</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es ls"><img src="../Images/46c7d0ce9bb7a4693b1dfd4bbe4bfc79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*qiYwUPEIAQl_IqLIM7PKEw.png"/></div></figure><p id="6849" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">3.3.3四分位距</p><p id="d4da" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">IQR通过将数据集分成四分位数来衡量可变性。Q1，Q2，Q3称为第一，第二和第三四分位数是分割数据集的值。</p><ul class=""><li id="b0d0" class="lt lu ht jq b jr km jv kn jz lv kd lw kh lx kl ly lz ma mb bi translated">Q1代表数据的第25个百分点。</li><li id="dc91" class="lt lu ht jq b jr mc jv md jz me kd mf kh mg kl ly lz ma mb bi translated">Q2代表数据的第50个百分点。</li><li id="3271" class="lt lu ht jq b jr mc jv md jz me kd mf kh mg kl ly lz ma mb bi translated">Q3代表数据的第75个百分点。</li></ul><p id="e518" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">IQR是第一和第三个四分位数之间的范围，即Q1和Q3: <em class="kv"> IQR = Q3 — Q1 </em>。低于<em class="kv">Q1-1.5 IQR</em>或高于<em class="kv"> Q3 + 1.5 IQR </em>的数据点为异常值。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/5b9470fc8aba10fbbd568d7ebb4f86cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h6WQMym9DjOxbC_16mTUew.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">犯罪数据集的四分位数范围显示存在异常值</figcaption></figure><p id="e4e6" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">移除异常值</p><p id="7f1b" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在上一节中，我们看到了如何使用Z得分和内部四分位数范围来检测异常值，但现在我们想要移除或过滤异常值并获得干净的数据。</p><p id="11e1" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们现在用Z-score移除后得到(47，21)的新形状。</p><p id="7fd6" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们现在用四分位数间距去除(51，21)后的新形状。</p><p id="83f9" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们还可以用列的中值替换离群值，如下所示。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/a2602e3741a43844cab9b345550f13e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*Zip-S3eSlUfX4fjya3oOww.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">离群点去除和替换</figcaption></figure><p id="5af0" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">3.4最小最大标量归一化</p><p id="9bbb" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">最小-最大归一化是归一化数据最常用的方法之一。对于每个特性，该特性的最小值被转换为0，最大值被转换为1，其他值被转换为0到1之间的小数。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mj"><img src="../Images/1065cb9152825af4da6b3acf3528a92c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9qBo2hJjwaJnUrni4lrSsg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">犯罪数据集中用中值替换的异常值的最小最大归一化</figcaption></figure><h1 id="c5ff" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 4。使用过滤、嵌入和包装方法进行特征选择。</strong></h1><p id="7c45" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">特征选择也称为属性选择，是一个<em class="kv">从数据集中提取最相关的特征</em>的过程，然后应用机器学习算法以获得更好的模型性能。特征选择通常可以导致更好的学习性能、更高的学习精度、更低的计算成本和更好的模型可解释性。</p><p id="e02a" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">4.1过滤方法</p><p id="90f8" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在过滤方法中，基于统计测量来选择特征。它独立于学习算法，需要较少的计算时间。例如，信息增益、卡方检验、费希尔评分、相关系数和方差阈值或方差分析。</p><p id="5aa5" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">4.1.1卡方<br/>计算每个特征和目标之间的卡方，并选择所需数量的具有最佳卡方得分的特征。它确定样本的两个分类变量之间的关联是否反映了它们在总体中的真实关联。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mk"><img src="../Images/977888e37a8946f12d5cfe8acff07f57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JS05p8RnRqIPVEDOy-D2rA.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">卡方特征选择</figcaption></figure><p id="10d7" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">皮尔逊相关性</p><p id="6ac8" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">相关性是对两个或更多变量的线性关系的度量。通过相关性，我们可以从一个变量预测另一个变量。使用相关性进行特征选择背后的逻辑是好的变量与目标高度相关。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ml"><img src="../Images/6b181cf6370e711034753cad4499df76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kbtB_ZLzmKqOdOajMN2VdA.png"/></div></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es mm"><img src="../Images/ba0e8e157bae300c53b3f7f56e1057b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*dGDxDkDtLsmUMvt40bVeGA.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">将特征与谋杀列相关联</figcaption></figure><p id="1335" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">信息增益</p><p id="1f7f" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">它通常用于从训练数据集构建决策树，方法是评估每个变量的信息增益，并选择使信息增益最大化的变量，从而使熵最小化，并最好地将数据集分成多个组，以便进行有效分类。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mn"><img src="../Images/2ec07165a0092093651ba21e94d496c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aGJnn81bMHNKOtaAIoiEDA.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">每个栏目的信息增益以及它们与谋杀栏目的关系</figcaption></figure><p id="7b4f" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">方差分析</p><p id="b0e7" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">ANOVA是方差分析的首字母缩写，用于确定两个或多个数据样本(通常是三个或更多)的平均值是否来自同一分布。该测试的结果可用于特征选择，其中独立于目标变量的那些特征可从数据集中移除。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mo"><img src="../Images/82375382c5113a077eba67e31ec63bc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rqk9XMC2siREyIeghuuVsg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">犯罪数据集上的方差分析特征选择</figcaption></figure><p id="de23" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">4.2包装方法</p><p id="438c" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">包装器方法将特征集的选择视为一个搜索问题，其中不同的组合被准备、评估并与其他组合进行比较。预测模型用于评估特征的组合并分配模型性能分数。对数据集使用RFE、向前选择和向后排除。</p><p id="2425" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">4.2.1 RFE</p><p id="6aed" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">递归特征消除(RFE)是一种特征选择方法，可拟合模型并移除最弱的特征，直到达到指定的特征数量。通过模型的系数或特征重要性属性对特征进行排序，并通过递归消除每个循环中的少量特征，RFE试图消除模型中可能存在的依赖性和共线性。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mp"><img src="../Images/8e4c4b5ea597a949b9bf0898f0165421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oXkxATvm7AcNiNf_zZXpQQ.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">犯罪数据集上的RFE特征选择</figcaption></figure><p id="22c1" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">向前选择</p><p id="08fe" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">前向选择是一种迭代方法，我们从模型中没有特征开始。在每一次迭代中，我们不断地添加最能改进我们模型的特性，直到添加一个新变量不能改进模型的性能。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es mq"><img src="../Images/a192858b1d5c91b4d1fc5ca474cb2339.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*U27SoCSPza03wg_X_tOjlg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">犯罪数据集上的正向选择特征选择</figcaption></figure><p id="9f4b" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">4.2.3逆向淘汰</p><p id="60ca" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在向后消除中，我们从所有特征开始，并在每次迭代中移除最不重要的特征，这提高了模型的性能。我们重复这一过程，直到在删除特征时没有观察到改进。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es mr"><img src="../Images/4b016fa03bd923766ae18c63b657e2bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*N3cR4JhvUJEQYk_vPRw3PQ.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">犯罪数据集上的逆向淘汰特征选择</figcaption></figure><p id="3db8" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">4.3嵌入式方法</p><p id="e585" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这些方法包含了包装器和过滤器方法的优点，既包含了功能的交互，又保持了合理的计算成本。嵌入式方法是迭代的，它负责模型训练过程的每次迭代，并仔细提取那些对特定迭代的训练贡献最大的特征。</p><p id="f97d" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">套索正规化</p><p id="bd72" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">拉索或L1正则化包括向机器学习模型的不同参数添加惩罚以避免过度拟合。在线性模型正则化中，惩罚应用于乘以每个预测值的系数。从不同类型的正则化，拉索或L1的属性，能够缩小一些系数为零。因此，可以从模型中删除该特征。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ms"><img src="../Images/71c301a1aec1bff7e2a8b18f6587d23f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*29QNg_s2ckE47fpiRpJxIg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">犯罪数据集上的L1正则化特征选择</figcaption></figure><p id="b3f1" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">岭回归</p><p id="8108" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">另一方面，当具有共线/相互依存的要素时，L2或岭回归非常有用。岭回归将系数的“平方值”作为惩罚项添加到损失函数中。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es mt"><img src="../Images/92d52948fb1037c3d4db9c0e49110bc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*e34YALPedm_yhqNf7etefQ.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">犯罪数据集上的岭回归特征选择</figcaption></figure><p id="67ae" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">4.3.3随机森林重要性</p><p id="7be2" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">特征重要性指的是一类用于为预测模型的输入特征分配分数的技术，该预测模型在进行预测时指示每个特征的相对重要性。</p><p id="5fd8" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">随机森林是一种Bagging算法，它聚集了指定数量的决策树。随机森林使用的基于树的策略自然会根据它们提高节点纯度的程度进行排序，或者换句话说，根据所有树的杂质(<strong class="jq hu"> Gini杂质</strong>)的减少程度进行排序。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mu"><img src="../Images/e850dea99830f660e0dfad2a8ffd0644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rTh3UiKbbYpXvn0giHJgKg.png"/></div></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mv"><img src="../Images/d92a506d32e0e039f7961b8db3199023.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*Xs80cEik-LkrdnoMTeyR3A.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">犯罪数据集中的随机森林重要性</figcaption></figure><p id="3f8b" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">4.3.4主成分分析</p><p id="22af" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">PCA是一种降维方法。PCA方法可以使用线性代数工具来描述和实现。使用PCA作为特征选择工具的基本思想是根据变量系数的大小(绝对值从最大到最小)(<em class="kv">负载</em>)来选择变量。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es mw"><img src="../Images/be29d291d352fcb9a5c5c7cdb903d447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*Sd1_ck4qbFpMuZr-otJ-Zg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">犯罪数据集的主成分分析</figcaption></figure><h1 id="5786" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 5。比较没有特征选择的训练和有特征选择的训练(过滤方法(卡方)、包装方法(RFE)和嵌入方法(套索))</strong></h1><p id="5cf4" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">5.1没有特征选择的朴素贝叶斯训练</p><p id="8a18" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在没有特征选择的情况下，用朴素贝叶斯进行训练后，我们的准确度为0.7。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es mx"><img src="../Images/b8a0ca9f4b5ce93021280054fea84447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*qGtMu6kmyNs-qr_CLWYfxw.png"/></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es my"><img src="../Images/de48c634f6fd402d4cf0124d15520019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*t_9YrQbRn6rWV7KPYaABRA.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">无特征选择的朴素贝叶斯训练</figcaption></figure><p id="04d7" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">5.2带特征选择的朴素贝叶斯训练</p><p id="ec96" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">5.2.1卡方检验</p><p id="3948" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在使用具有过滤方法(卡方)特征选择的朴素贝叶斯进行训练之后，我们具有0.714的准确度。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es mz"><img src="../Images/df0b0d7e26354b2e9c6901db56cdcd6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*ZXj7AMZqojZAd0SdM-fI1g.png"/></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es na"><img src="../Images/5044e276c1984312604d9895015bfc5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4q9uDjjwriX_18JrMo22iQ.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">卡方特征选择的朴素贝叶斯训练</figcaption></figure><p id="7bb9" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">RFE</p><p id="8578" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在用具有包装器方法(RFE)特征选择的朴素贝叶斯训练之后，我们具有0.643的准确度。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es nb"><img src="../Images/b43a352594462e32f5eb04e13dd5a52c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*Vf0ssWFZ4fFBqcOmxPhj9Q.png"/></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es nc"><img src="../Images/479a5fb54f197af53deb071262752993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*S6MPRmYbVHvWPO5bIujWcw.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">基于RFE特征选择的朴素贝叶斯训练</figcaption></figure><p id="fe75" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">套索</p><p id="fe08" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在使用具有嵌入方法(LASSO)特征选择的朴素贝叶斯进行训练之后，我们具有0.785的准确度。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es mt"><img src="../Images/c3878936a10c317ebfae8788817cbbf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*UjtdbM4-5hku36_FlocfhQ.png"/></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es nd"><img src="../Images/c0e88d6590db9f2e2b71dc289a170791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*U46unVMR52EdoF3x9npA7Q.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">使用套索特征选择的朴素贝叶斯训练</figcaption></figure><h1 id="95dd" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 6。时间序列或回归算法比较(朴素贝叶斯、k近邻、支持向量机、卷积神经网络和RNN(LSTM) </strong></h1><p id="4c04" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">6.1朴素贝叶斯</p><p id="f5c4" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这是一种基于贝叶斯定理的算法，假设预测器之间是独立的。简而言之，朴素贝叶斯分类器假设一个类中特定特征的存在与任何其他特征的存在无关。与逻辑回归等其他模型相比，朴素贝叶斯分类器的性能更好，并且需要的训练数据更少</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es ne"><img src="../Images/48a30addf3fd6ca093ad8a58853eda15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*nRG9X6t1qT81HyL9OcJK-Q.png"/></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nf"><img src="../Images/3069e29026ea42ed988c0985b740e474.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JuhZic8fbGqEKx6NcOi6YA.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">基于套索特征选择的朴素贝叶斯训练</figcaption></figure><p id="a348" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">6.2 K最近邻</p><p id="509e" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hu"> KNN </strong>算法既可以用于分类问题，也可以用于回归问题。KNN算法使用“特征相似性”来预测任何新数据点的值。这意味着根据新点与训练集中的点的相似程度为其赋值。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ng"><img src="../Images/a576de7d9dc03bafa6a77b0ec33780df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pwaHfPnVtzBzA80OAmjp_Q.png"/></div></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nh"><img src="../Images/d3427af59fdf4b612ca9105f9d235c0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Ixd4dcvzphASxsq0PL0Tw.png"/></div></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es ni"><img src="../Images/f9098114ad7edd8f276e47aabcf1a5bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*LmOaVxXbJw-olOUobRsyQA.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">使用套索特征选择的KNN训练</figcaption></figure><p id="b6a2" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">6.3支持向量机(SVM)</p><p id="e152" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">支持向量机也可以用作回归方法，保持表征算法的所有主要特征(最大间隔)。在回归的情况下，一个容限(ε)被设置为接近问题已经要求的SVM。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nj"><img src="../Images/05817fac24f0744ce9813f41de85e8c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JHo-m5VrPS9bx9NH0cuI0Q.png"/></div></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es et"><img src="../Images/8dda0037f8c83e666bb9d03f1c6fcb6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dq9DkjvzKkvPcvXePqaaIA.png"/></div></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es nk"><img src="../Images/065e1e3fc74cf228a110c0bc74c96276.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*59QfX8u2PoP2Y9IjIFv69g.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">使用套索特征选择的SVM训练</figcaption></figure><p id="ba19" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">6.4卷积神经网络</p><p id="7c2f" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">卷积神经网络(CNN)模型主要用于像图像数据这样的二维数组。但是，我们也可以应用CNN进行回归数据分析。在这种情况下，我们应用一维卷积网络，并根据它对输入数据进行整形。Keras提供了Conv1D类来将一维卷积层添加到模型中。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es nl"><img src="../Images/e5e3237eaf09eccbf0cf4e6e586131b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*p5cK82CUHr1g2TETaHEHUg.png"/></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nm"><img src="../Images/bded647f0561bf497f3bc77b93cd620d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N2_bVIm5-Qioc228GTAW4A.png"/></div></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es nn"><img src="../Images/c6cfa3111d6b72d0c6075c64b094deb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*f6l5DIQw1HbBxeOgxplWKg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">使用套索特征选择的CNN训练</figcaption></figure><p id="8489" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">6.5 LSTM RNN</p><p id="3c33" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">RNN(LSTM)非常擅长在输入特征空间中提取模式，其中输入数据跨越长序列。鉴于LSTM的门控结构具有操纵其记忆状态的能力，它们是回归或时间序列问题的理想选择。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es no"><img src="../Images/ea2af6be6daa5868e27268a8e80c7cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*8a_l9-pDgSLR9rP5PMj1Hg.png"/></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es np"><img src="../Images/ecaa04532db8b325934f46512b7e67e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YsRm_M_l277lSPwL2kucrg.png"/></div></div></figure><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es nq"><img src="../Images/0b6effc6f32359d09b7fac4e921b2479.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*9nIgS6pKkUpJLBk9utxK9g.png"/></div></figure><h1 id="273d" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">7.保存已训练的模型</h1><p id="3d39" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">既然机器学习模型已经训练好了，我们现在可以用pickle保存这个模型了。</p><figure class="la lb lc ld fd hk er es paragraph-image"><div class="er es nr"><img src="../Images/6ac02578add53c91f1af796b8c1f4823.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*RDIy-5ScjIcdZRhQuoykHQ.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">用pickle保存训练好的模型</figcaption></figure><h1 id="eaf2" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结论</h1><p id="3887" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">这个项目解释了在瑞典犯罪率数据集上进行EDA的过程。我们讨论了如何执行可视化，通过处理缺失数据、异常值、归一化来进行数据预处理，解释了特征选择方法，并比较了卡方检验。rfe和lasso训练的准确性，最后比较了SVM、KNN、朴素贝叶斯、CNN和LSTM。</p><p id="d7c1" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hu">作家</strong>:奥卢耶德·塞贡。(小)</p><p id="7cdf" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hu">所用资源(参考文献)及延伸阅读:</strong></p><div class="hh hi ez fb hj ns"><a href="https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hu fi z dy nx ea eb ny ed ef hs bi translated">机器学习中的特征选择技术</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">在现实生活中构建机器学习模型时，数据集中的所有变量都有用的情况几乎是罕见的…</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">www.analyticsvidhya.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og hp ns"/></div></div></a></div><div class="hh hi ez fb hj ns"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/feature-selection-for-dimensionality-reduction-embedded-method-e05c74014aa"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hu fi z dy nx ea eb ny ed ef hs bi translated">降维的特征选择(嵌入式方法)</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">在机器学习中，选择数据中的重要特征是整个周期的重要部分。</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">medium.com</p></div></div><div class="ob l"><div class="oh l od oe of ob og hp ns"/></div></div></a></div><div class="hh hi ez fb hj ns"><a href="https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hu fi z dy nx ea eb ny ed ef hs bi translated">k最近邻算法| KNN回归Python</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">在我遇到的所有机器学习算法中，KNN算法是最容易上手的…</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">www.analyticsvidhya.com</p></div></div><div class="ob l"><div class="oi l od oe of ob og hp ns"/></div></div></a></div><div class="hh hi ez fb hj ns"><a href="https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hu fi z dy nx ea eb ny ed ef hs bi translated">学习朴素贝叶斯算法|朴素贝叶斯分类器示例</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">了解一个最流行和简单的机器学习分类算法，朴素贝叶斯算法它…</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">www.analyticsvidhya.com</p></div></div><div class="ob l"><div class="oj l od oe of ob og hp ns"/></div></div></a></div><div class="hh hi ez fb hj ns"><a href="https://www.datatechnotes.com/2019/12/how-to-fit-regression-data-with-cnn.html" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hu fi z dy nx ea eb ny ed ef hs bi translated">Python中如何用CNN模型拟合回归数据</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">使用R、Python和C#进行机器学习、深度学习和数据分析</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">www.datatechnotes.com</p></div></div><div class="ob l"><div class="ok l od oe of ob og hp ns"/></div></div></a></div><div class="hh hi ez fb hj ns"><a href="https://www.saedsayad.com/support_vector_machine_reg.htm" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hu fi z dy nx ea eb ny ed ef hs bi translated">支持向量回归</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">支持向量机也可以作为一种回归方法，保持所有的主要特征，表征…</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">www.saedsayad.com</p></div></div><div class="ob l"><div class="ol l od oe of ob og hp ns"/></div></div></a></div><p id="9c60" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hu">领英简介</strong>:<a class="ae kr" href="https://www.linkedin.com/in/oluyede-segun-adedeji-jr-a5550b167/" rel="noopener ugc nofollow" target="_blank">https://www . LinkedIn . com/in/oluyede-segun-ade deji-Jr-a 5550 b 167/</a></p><p id="c991" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hu">说明笔记本链接:</strong><a class="ae kr" href="https://github.com/juniorboycoder/TIME_SEREIS_EDA_FEATURE_SELECTION_AND_PREDICITVE_ANALYSIS/blob/main/eda_and_feature_Selection_timeseries_project.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/juniorboycoder/TIME _ SEREIS _ EDA _ FEATURE _ SELECTION _ AND _ predictive _ ANALYSIS/blob/main/EDA _ AND _ FEATURE _ SELECTION _ TIME series _ project . ipynb</a></p><p id="d4db" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">https://twitter.com/oluyedejun1的推特简介:<a class="ae kr" href="https://twitter.com/oluyedejun1" rel="noopener ugc nofollow" target="_blank">T4</a></p><p id="5845" class="pw-post-body-paragraph jo jp ht jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hu">标签</strong>:# feature selection # Outlier # time series # regression # CNN # SVM # KNN # LSTM # naive Bayes #过滤器#包装器#嵌入式#EDA</p></div></div>    
</body>
</html>