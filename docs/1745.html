<html>
<head>
<title>Activation Functions (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">激活功能(第1部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-part-1-d80b0e8ee89d?source=collection_archive---------20-----------------------#2021-03-15">https://medium.com/analytics-vidhya/activation-functions-part-1-d80b0e8ee89d?source=collection_archive---------20-----------------------#2021-03-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="2c0f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">激活是一种应用于神经元输出的功能，随着我们在神经网络中的深入，它可以学习更复杂的功能。它们也可以被认为是修改函数范围的映射。例如，我们可以使用一个激活函数来确保所有的输出都在0和1之间，或者在-1和1之间。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/242441d75e46c5c55925e3279e48ebd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*2Ma2kdyPRjZ5Js59sZxjbg.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图1:神经元输出的表达式，然后应用激活函数</figcaption></figure><ul class=""><li id="b9e0" class="jo jp hh ig b ih ii il im ip jq it jr ix js jb jt ju jv jw bi translated">图1中的第一个等式是用于计算神经元输出的公式。该方程采用直线y = mx + c的形式</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jx"><img src="../Images/bbfcf78cfaef9678c800c8684afcd8f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*HFtL1iFBv1wDYAHVFBio3w.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图2:y方程的曲线图</figcaption></figure><ul class=""><li id="1078" class="jo jp hh ig b ih ii il im ip jq it jr ix js jb jt ju jv jw bi translated">随着我们增加神经网络的层数，我们希望学习更复杂的特征来更好地表示数据。如果我们在神经网络中使用线性函数作为激活，学习更复杂的特征将变得非常具有挑战性。看下图，我们很难用一条直线把这些点分割开来。</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jy"><img src="../Images/e28909c9edab1bcebe5331f428f360fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*Bs-tOAKqUl7cygXrvU_zWg.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图3:非线性特征</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es jz"><img src="../Images/8db0ada17d9f7d04818d59bd4e895092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n8okVmw6vPJ6Tytx8D48ug.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图4:两层神经网络</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ke"><img src="../Images/a384750ebe13837568ec46ba9f71e8e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*6xX9BDUzBee1v_N1UeSo7Q.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图5:定义使用的术语</figcaption></figure><ul class=""><li id="ae69" class="jo jp hh ig b ih ii il im ip jq it jr ix js jb jt ju jv jw bi translated">如果我们使用线性激活函数，如图5中定义的方程，其中f_activation = <strong class="ig hi"> a*y </strong>，我们最终可以将整个网络表示为一个单层。</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kf"><img src="../Images/d51799eeb8cb311ea75260b12e04d250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*WS3MbVpuymG2DJV4S37KNQ.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图6:扩展衍生品</figcaption></figure><ul class=""><li id="986a" class="jo jp hh ig b ih ii il im ip jq it jr ix js jb jt ju jv jw bi translated">通过图6中的等式，<strong class="ig hi"> y </strong>表示线性激活后网络的最终输出。<strong class="ig hi"> x1 </strong>表示线性激活后第一层的输出。</li><li id="0efc" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">我们现在可以用第二个等式中的<strong class="ig hi"> x1 </strong>的表达式替换第一个等式中的<strong class="ig hi"> x1 </strong>项，然后展开该等式。</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kl"><img src="../Images/5505f799047272d780e069a8655e6183.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*PSuP6Pd_6JgLabiDNNLSgw.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图7:简化导数</figcaption></figure><ul class=""><li id="9790" class="jo jp hh ig b ih ii il im ip jq it jr ix js jb jt ju jv jw bi translated">简化方程后，由于a2和a1是常数，我们可以把<strong class="ig hi">写成a2*W2*a1*W1 </strong>作为新的权重矩阵<strong class="ig hi"> W3 </strong>。</li><li id="a8a0" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">(<strong class="ig hi"> a2*W2*a1*b1 + a2*b </strong>)本质上再次表示可能是偏差<strong class="ig hi"> b3 </strong>的新矩阵。</li><li id="062b" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">这基本上意味着最后一层可以用一个等式代替<strong class="ig hi"> y = W3*x0 + b3 </strong></li><li id="e278" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">这是为什么使用非线性激活函数来打破输入到输出的直接线性关系的主要原因之一。</li></ul><h1 id="8926" class="km kn hh bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated"><strong class="ak">常用激活:</strong></h1><ul class=""><li id="98ea" class="jo jp hh ig b ih lk il ll ip lm it ln ix lo jb jt ju jv jw bi translated"><strong class="ig hi">乙状结肠功能</strong></li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lp"><img src="../Images/f42c445288cedd0def290a9069353978.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*YcpxHdJ2qXjtg8Xdd9cEmA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图8: Sigmoid函数</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/4cea0328619a594a01724836f6606316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*4pjjwMmt7iO0J23DHHE6dg.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图sigmoid函数及其导数的图形</figcaption></figure><p id="927b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">乙状结肠神经元的正向传递:</strong></p><ul class=""><li id="13cb" class="jo jp hh ig b ih ii il im ip jq it jr ix js jb jt ju jv jw bi translated">sigmoid函数的范围是从0到+1</li><li id="052b" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">如果我们分析sigmoid图，对于大于5或小于-5的输入值，我们会得到几乎相同的输出。(也就是说，对于10、100或1000的输入，我们将得到几乎相同的输出1)。</li><li id="64d8" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">与其他一些激活函数相比，公式中指数计算的存在也使得计算速度变慢，但是这并不是一个明显的缺点。</li></ul><p id="180f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">乙状结肠神经元的反向传递:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/f4232a82159a3e8c398eb5ae96efa32c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*7zCe917iKj5YY6-TO6WbBA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图10:乙状结肠神经元的衍生物</figcaption></figure><ul class=""><li id="1075" class="jo jp hh ig b ih ii il im ip jq it jr ix js jb jt ju jv jw bi translated">这个导数可以重写为:</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ls"><img src="../Images/be888dcc23f860c80015522892b1bc11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*Xkps8hAY1EyvtaMgr4z6FQ.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图11:乙状结肠神经元的简化衍生物</figcaption></figure><ul class=""><li id="18c1" class="jo jp hh ig b ih ii il im ip jq it jr ix js jb jt ju jv jw bi translated">导数的图形由图9中的反向曲线表示</li><li id="f680" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">从图中可以看出，正向通道在较高值时饱和，导数也在较高值时饱和。这是因为函数在较高值处的梯度可以忽略不计，导致在较高值处导数几乎为0。这就是通常所说的<strong class="ig hi">消失梯度</strong>问题。</li><li id="cc1d" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">一旦使用sigmoid神经元的层的权重变得太大，其正向输出饱和为0或1，并且其导数变为0，导致我们不能以任何显著的量更新其值。</li><li id="9c8f" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">由于sigmoid函数在0和1之间缩放其输出，因此它是<strong class="ig hi">而不是以零为中心的</strong>(即，输入为0时sigmoid的值不等于0，并且它不输出任何负值)。</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lt"><img src="../Images/ffa9e83de096a953bfe691fd4d161015.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*QoevW9gODYl9HoBLhUo1OQ.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图12:使用sigmoid神经元的权重导数</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lu"><img src="../Images/d1ec125d29f722c44f443380937b231e.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*WnmxpWqxdRkI2i1TzrcQIQ.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图13:y相对于重量的导数</figcaption></figure><ul class=""><li id="5852" class="jo jp hh ig b ih ii il im ip jq it jr ix js jb jt ju jv jw bi translated">从图12和13中，我们可以看到，损耗相对于层权重的导数取决于层本身的输入<br/>(即<strong class="ig hi"> x </strong>)。</li><li id="1526" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">现在，例如，如果我们有一个2层神经网络，并在第1层后使用sigmoid激活函数，我们知道第1层的输出将始终位于0和1之间。</li><li id="93e5" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">如果我们要计算层2相对于其权重的导数，即<strong class="ig hi"> dy/d(权重)</strong>，从图13中我们可以看到它等于<strong class="ig hi"> x </strong>(即层1的输出)。由于我们在第1层使用了sigmoid激活，<strong class="ig hi"> x </strong>将总是0和1之间的正结果。</li><li id="6629" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">这也意味着层2的权重的导数的符号将完全取决于项<strong class="ig hi"> d(损失)/dy </strong>的符号，因为<strong class="ig hi"> dy/d(权重)</strong>总是正的。</li><li id="dd78" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">这导致我们约束第2层的所有权重要么正要么负地更新。Sigmoid不允许用正值更新某些权重，用负值更新某些权重。这可能导致很多曲折，因为我们将首先在第一步中正向更新权重，然后在下一步中负向更新权重。</li><li id="9b76" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated"><strong class="ig hi">双曲正切函数</strong></li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lv"><img src="../Images/cf8bf34ab562231226e1ccc4743a7a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*zsl4ToKbVoPU1bdl2Cjm5w.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图14:双曲正切函数</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/041e35b4f485bb6f11bdf12ad05bd538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*GmCydDsR1VvIdDBAfBWUwg.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图15:双曲正切函数及其导数的图表</figcaption></figure><p id="d7d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">一个双曲正切神经元的正向传递:</strong></p><ul class=""><li id="1141" class="jo jp hh ig b ih ii il im ip jq it jr ix js jb jt ju jv jw bi translated">双曲正切函数的范围是-1到+1</li><li id="56b4" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">如果我们分析双曲正切图，我们会注意到来自乙状结肠神经元的相同的持续问题，即，在大于3或小于-3的值，我们最终会得到相同的输出+1或-1，这导致了<strong class="ig hi">饱和</strong>。</li><li id="a617" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">它的公式中存在多个指数，这使得它的计算速度比sigmoid神经元慢。</li></ul><p id="366c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">一个双曲正切神经元的反向传递:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lw"><img src="../Images/0cc4e3b28c80a48468c1aa43cf6daa58.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*I4ea4UAVX1LtuRSWHIeS4w.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图16:函数u/v的导数</figcaption></figure><ul class=""><li id="7309" class="jo jp hh ig b ih ii il im ip jq it jr ix js jb jt ju jv jw bi translated">双曲正切函数的导数如图16所示</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lx"><img src="../Images/3803d97d09f52b5a5a5e5c016ed9f2c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*ZFXKJg8xqD-EQSqANJr-_Q.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图17:计算双曲正切导数</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ly"><img src="../Images/ef7402f5ef5b743aac6a1529e2caa29e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*K6mY1WZfkCRbmE8V1i3J3g.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图18: Tanh导数(续)</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lz"><img src="../Images/ed77b1421db5800529d2efad3aac3784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*CymwbJIRlsNpTLqHnb7E6w.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图19:双曲正切导数(续)</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ma"><img src="../Images/560613ac02ba3190129794d83fbed1b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*2LxZ9WeRzDZS3iVpmO-HGA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">图20:简化的双曲正切导数</figcaption></figure><ul class=""><li id="2fb0" class="jo jp hh ig b ih ii il im ip jq it jr ix js jb jt ju jv jw bi translated">方程式的图形由图15中的反向图表示</li><li id="452b" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">在<strong class="ig hi">消失梯度</strong>方面，双曲正切函数的<strong class="ig hi">饱和</strong>提出了与sigmoid神经元相同的问题(即，导数在大的正值或负值处变得接近0，导致没有权重更新)。</li><li id="e9af" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">与sigmoid函数相比，tanh函数的优势在于它以零为中心。</li><li id="cc91" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">回到图12和图13的解释，由于层权重的导数(即<strong class="ig hi"> dy/d(weight) </strong>)直接取决于该层本身的输入(即<strong class="ig hi"> x </strong>)，使用双曲正切函数将给出包含正值和负值的输出。</li><li id="b425" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">这将允许我们摆脱用正值更新所有权重或者用负值更新所有权重的约束，因为<strong class="ig hi"> x </strong>将位于(-1，1)之间。</li></ul><h2 id="19e0" class="mb kn hh bd ko mc md me ks mf mg mh kw ip mi mj la it mk ml le ix mm mn li mo bi translated"><strong class="ak">总结</strong>:</h2><ul class=""><li id="cf0d" class="jo jp hh ig b ih lk il ll ip lm it ln ix lo jb jt ju jv jw bi translated">在下一篇文章中，我将介绍一些更常用的激活方式，比如Relu和Leaky Relu。</li><li id="2756" class="jo jp hh ig b ih kg il kh ip ki it kj ix kk jb jt ju jv jw bi translated">下面是一个简单代码的<a class="ae mp" href="https://github.com/vineeth2309/Activation-Functions" rel="noopener ugc nofollow" target="_blank">链接</a>,用于实现激活函数及其衍生函数，并可视化它们的输出。</li></ul></div></div>    
</body>
</html>