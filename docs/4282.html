<html>
<head>
<title>PyTorch Hooks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch挂钩</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pytorch-hooks-5909c7636fb?source=collection_archive---------0-----------------------#2021-09-17">https://medium.com/analytics-vidhya/pytorch-hooks-5909c7636fb?source=collection_archive---------0-----------------------#2021-09-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/decd4227e045b203277e865cbad68700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lb3tpny5RUc4t9ued6KovA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">ref:<a class="ae iu" href="https://jhui.github.io/2017/03/16/CNN-Convolutional-neural-network/" rel="noopener ugc nofollow" target="_blank">https://jhui . github . io/2017/03/16/CNN-卷积-神经网络/ </a></figcaption></figure><p id="7dc3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有时候有很多方法可以完成同样的任务。你如何做取决于可用的工具和你使用它们的效率。Pytorch Hook就是那个工具，没有它你可能会做一个完整的神经网络，也可以训练它，但是当你知道它有多强大的时候，你就无法把手从它身上拿开了。</p><p id="0715" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">那么什么是钩子呢？钩子是帮助动态更新梯度、输入或输出的函数。也就是说，我可以改变神经网络的行为，即使是在我训练它的时候。</p><p id="ed4d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">钩子用在两个地方</p><ol class=""><li id="a9c8" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">关于张量</li><li id="25f7" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">在torch.nn .模块上</li></ol><p id="b392" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">更重要的一点是，要应用一个钩子，我们必须首先“注册”我们想要应用它的地方。现在听起来可能有点复杂，我们将在后面的例子中理解它。</p><p id="c0ce" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">挂钩有三种应用方式</p><ul class=""><li id="a0a4" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js kh jz ka kb bi translated">向前预瞄(在向前传球之前执行)，</li><li id="c1a0" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js kh jz ka kb bi translated">向前钩住(在向前传球之后执行)，</li><li id="9efd" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js kh jz ka kb bi translated">向后挂钩(在向后传递后执行)。</li></ul><p id="7734" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，前向传递是指输入被用来计算下一个隐藏神经元的值，使用权重等等，直到它到达末端并返回输出。使用输出值和真实值计算损失后，会发生反向传递，然后使用链规则在输出到输入的方向上(因此是向后)计算每个层的每个权重和偏差的梯度。基本上，反向传播发生的步骤</p><p id="5d19" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">论张量</strong></p><p id="da92" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于张量来说，只有向后弯钩是可能的。为了记录钩子的张量，我们可以</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="d74a" class="kr ks hi kn b fi kt ku l kv kw">x.register_hook( your_hook_func ) #x is a tensor</span></pre><p id="5a2a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个钩子函数与梯度一起工作，并且它将在每次计算张量的梯度时被激活。</p><p id="8c62" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">钩子函数要么返回更新的渐变，要么不返回。我们不应该做任何就地操作，这可能会改变计算图形中与之相连的张量的梯度。</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="3250" class="kr ks hi kn b fi kt ku l kv kw">   #Correct way                         #Inplace (wrong)<br/>   def func(grad):                       def func(grad):<br/>      return grad+100                       grad+=100</span></pre><p id="bc41" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看一个完整的例子，看看输出。首先让我们定义一个钩子，它将为计算的梯度加2</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="a3f5" class="kr ks hi kn b fi kt ku l kv kw">def hook(grad):<br/>   return grad + 2</span></pre><p id="bac6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们写一个简单的乘法代码，首先，我们将不使用钩子来计算梯度</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="31cd" class="kr ks hi kn b fi kt ku l kv kw">import torch</span><span id="3b48" class="kr ks hi kn b fi kx ku l kv kw">#initializing two tensors(requires_grad = True is necessary to calculate gradients)</span><span id="b4aa" class="kr ks hi kn b fi kx ku l kv kw">a = torch.tensor(7.0, requires_grad=True)<br/>b = torch.tensor(13.0, requires_grad=True)</span><span id="e6b7" class="kr ks hi kn b fi kx ku l kv kw">c = a * b</span><span id="2cfe" class="kr ks hi kn b fi kx ku l kv kw">c.retain_grad()#to store the gradient of C</span><span id="213d" class="kr ks hi kn b fi kx ku l kv kw">c.backward()</span><span id="8bfc" class="kr ks hi kn b fi kx ku l kv kw">print(a.grad)<br/>print(b.grad)<br/>print(c.grad)</span></pre><p id="7eaf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出将是</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="e191" class="kr ks hi kn b fi kt ku l kv kw">tensor(13.) tensor(7.) tensor(1.)</span></pre><p id="02ad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以现在如果我们使用钩子，c的梯度应该增加2也就是应该是3，类似地，a和b的梯度也会改变，它们的新梯度将是旧梯度乘以3，分别是39和21。让我们看看是否匹配。</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="447b" class="kr ks hi kn b fi kt ku l kv kw">c = a * b</span><span id="30f8" class="kr ks hi kn b fi kx ku l kv kw">#registering the tensor c with the hook<br/>c.register_hook(lambda grad: hook(grad))</span><span id="422c" class="kr ks hi kn b fi kx ku l kv kw">c.retain_grad()<br/>c.backward()</span><span id="ef47" class="kr ks hi kn b fi kx ku l kv kw">print(a.grad)<br/>print(b.grad)<br/>print(c.grad)</span></pre><p id="949e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="8d77" class="kr ks hi kn b fi kt ku l kv kw">tensor(39.) tensor(21.) tensor(3.)</span></pre><p id="9f5e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，它与我们之前讨论的内容相匹配。(如果你对这些梯度是如何计算的感到困惑，你最好看看亲笔签名的库)</p><p id="5dc0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要移除挂钩，请执行以下操作</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="3d0b" class="kr ks hi kn b fi kt ku l kv kw">d = c.register_hook(c_hook)<br/>d.remove()</span></pre><p id="b4db" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我不会在<strong class="ix hj">模块上讨论整个钩子系统。</strong>但是在这里我们可以使用所有的三个钩子，即向前预钩、向前和向后钩。</p><p id="cf07" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看模块上前向挂钩的一个很好的应用。</p><h1 id="c9c8" class="ky ks hi bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">使用钩子寻找层激活</strong></h1><p id="11dc" class="pw-post-body-paragraph iv iw hi ix b iy lv ja jb jc lw je jf jg lx ji jj jk ly jm jn jo lz jq jr js hb bi translated">如果我们想要计算模型学习的激活，向前挂钩会非常有用。假设你制作了一个可以检测皮肤癌的模型，通过激活模型，我们可以看到模型实际上聚焦在图像的什么地方。这是描述模型可解释性的一个很好的工具，因为我们可以看到激活图。</p><p id="436e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们建立一个简单的CNN模型，由3层组成。首先是卷积层，然后是平均池层，最后是线性层。我们将尝试从池层获取激活。(如果你愿意，你可以从每一层激活)</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="90dc" class="kr ks hi kn b fi kt ku l kv kw">import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F</span><span id="a3cd" class="kr ks hi kn b fi kx ku l kv kw">class Net(nn.Module):<br/>   def __init__(self):<br/>      super().__init__() <br/>      self.conv = nn.Conv2d(3,8,2)<br/>      self.pool = nn.AdaptiveAvgPool2d((4,4))<br/>      self.fc = nn.Linear(8*4*4 , 1)</span><span id="c31c" class="kr ks hi kn b fi kx ku l kv kw">def forward(self, x):<br/>      x = F.relu(self.conv(x))<br/>      x = self.pool(x)<br/>      x = x.view(x.shape[0] , -1)<br/>      x = self.fc(x)<br/>      return x<br/>net = Net()</span></pre><p id="e30c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">forward hook函数有3个参数:模块、输入和输出。它根据函数或None返回更新的输出。它应该具有以下签名:</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="e1f9" class="kr ks hi kn b fi kt ku l kv kw"><strong class="kn hj">hook(module,</strong> input<strong class="kn hj">,</strong> <strong class="kn hj">output)</strong> <strong class="kn hj">-&gt;</strong> <strong class="kn hj">None</strong> <strong class="kn hj">or</strong> <strong class="kn hj">modified</strong> <strong class="kn hj">output</strong></span></pre><p id="bfbd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们做一个钩子，它可以收集激活。我们将使用字典数据结构来收集它们。</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="2579" class="kr ks hi kn b fi kt ku l kv kw">feats = {} #an empty dictionary<br/>def hook_func(m , inp ,op):<br/>   feats['feat'] = op.detach()</span></pre><p id="b3d5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在通常我们首先训练一个模型。(显然)我不会在这里做。让我们假设我们之前制作的模型已经根据一些数据进行了训练，我们现在需要它学习的功能。</p><p id="4eac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在池层上注册向前挂钩</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="5de7" class="kr ks hi kn b fi kt ku l kv kw">net.pool.register_forward_hook(hook_func)</span></pre><p id="2567" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设我们已经输入了一个尺寸为1x3x10x10的图像(一个尺寸为10x10的RGB图像),现在我们需要这些特征。</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="ab97" class="kr ks hi kn b fi kt ku l kv kw">x= torch.randn(1,3,10,10)<br/>output = net(x)</span></pre><p id="8406" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这样做将使激活保存在专长字典中。让我展示一下字典的形状，因为它太大了，不可能展示里面的内容。</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="c436" class="kr ks hi kn b fi kt ku l kv kw">print(feats['feat'].shape)</span><span id="f18e" class="kr ks hi kn b fi kx ku l kv kw">#output -&gt; torch.Size([1, 8, 4, 4])</span></pre><p id="f209" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此我们的激活得到了保存。</p><p id="8c7f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以总结一下，我们已经看到了钩子是什么，是怎么用的。这是一种特殊的工具，可以有多种用途。我们可以在训练时控制梯度，我们可以存储层的激活，我们可以改变输出的计算方式等等。我还没有发现一个关于预挂钩的很好的应用，如果你有什么可以告诉我。希望读完这篇文章后，你已经有了某种程度的好奇心去进一步探索。</p><p id="6de1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">快乐学习！</p><p id="9a73" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">参考资料:</p><ol class=""><li id="3ffa" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated"><a class="ae iu" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html?highlight=register_full_backward_hook#torch.nn.Module.register_full_backward_hook" rel="noopener ugc nofollow" target="_blank">https://py torch . org/docs/master/generated/torch . nn . module . html？highlight = register _ full _ backward _ hook # torch . nn . module . register _ full _ backward _ hook</a></li><li id="e8d8" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/" rel="noopener ugc nofollow" target="_blank">https://blog . paper space . com/py torch-hooks-gradient-clipping-debug/</a></li><li id="f68d" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://www.youtube.com/watch?v=syLFCVYua6Q&amp;t=27s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=syLFCVYua6Q&amp;t = 27s</a></li></ol></div></div>    
</body>
</html>