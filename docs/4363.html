<html>
<head>
<title>Support Vector Machine (SVM) — Theory and Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机(SVM)——理论与实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/support-vector-machine-svm-theory-and-implementation-bbef18daf5?source=collection_archive---------0-----------------------#2021-09-29">https://medium.com/analytics-vidhya/support-vector-machine-svm-theory-and-implementation-bbef18daf5?source=collection_archive---------0-----------------------#2021-09-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/7878d0d46bd5319c3d3f2f654495ff64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rSkTO2zS93-OhjWX"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">马克·柯尼希在<a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/><p id="69da" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我使用的案例中，支持向量机(SVM)模型几乎总能产生好的结果。这是一个优秀的分类模型。算法逻辑是合理的，相当容易实现，需要调整的重要参数很少，并且在其线性模型中，具有模型解释能力的系数。在这篇文章中，我将解释SVM的机制，并通过它的实现。</p><h2 id="1318" class="jt ju hy bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">定义</h2><p id="7dc8" class="pw-post-body-paragraph iv iw hy ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">支持向量机或SVM是一种机器学习模型，它基于使用一个超平面，该超平面可以将n维空间中的数据点最佳地划分为多个类别。在Scikit-Learn库中，这是一个可靠的分类或回归模型。超平面(在n维欧几里得空间中)是前述欧几里得空间的平坦的、<strong class="ix hz"> n-1 </strong>维子空间，其将空间分成两个独立的部分。超平面很难想象比我们传统的3-D空间更高的任何n值(但在数学上是可能的，所以不要担心),只要知道它将我们的数据分成几类。支持向量是靠近超平面的边缘，该超平面将我们的数据分成它的类。如果我们最大限度地提高利润率，我们就能获得数据点的最佳分类。很简单，明白了吗？</p><h2 id="2f8a" class="jt ju hy bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">内核变换</h2><p id="ba01" class="pw-post-body-paragraph iv iw hy ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">好吧，但是，如果我们的数据点以这样一种方式被分开，以至于不可能拟合一个具有最大限度的边界的线性超平面…例如，如果我们的数据点以这样的方式被排列:</p><figure class="ku kv kw kx fd hk er es paragraph-image"><div class="er es kt"><img src="../Images/efb6ba3fd2d9d54bb0c46fad38faee20.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*69iGbLIDscqZXRKZjqPGDA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图为Lilly Chen在<a class="ae hv" href="https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496" rel="noopener" target="_blank">https://towards data science . com/support-vector-machine-simple-explained-fee 28 EBA 5496</a></figcaption></figure><p id="bfd7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">核变换或“核技巧”是对我们的数据点进行变换，从而使超平面成为可能。这是SVM的症结所在，也是SVM模式的微妙之处、独特性、独特性和有效性。在sklearn中，可能的转换有:<code class="du ky kz la lb b">‘<strong class="ix hz"><em class="lc">linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed'</em></strong></code> <strong class="ix hz"> <em class="lc"> </em> </strong>其中预计算是一个自己创建的转换。在建模过程中，人们会“尝试”不同的内核，并为给定的数据点找到性能最佳的内核。然而，我已经提供了内核选择的指南。解释这些转换需要更高的数学知识(这超出了本文的范围)，但是这里是我自己的建议，我愿意接受关于内核转换的学术建议和讨论。其实我会在这个环节给出转换公式/解释:</p><p id="936e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae hv" href="https://data-flair.training/blogs/svm-kernel-functions/" rel="noopener ugc nofollow" target="_blank">https://data-flair.training/blogs/svm-kernel-functions/</a></p><p id="f44f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果您的数据点具有线性1:1关系(我的意思是数据以线性x:y模式映射)，我会推荐线性。sklearn实现中带有“degree”的多项式变换(其中degree为变换提供了“bends ”)创建了一个带有曲线的超平面。Rbf代表径向基函数，它给超平面一个径向(圆形)形状。当数据点具有清晰的决策边界但未显示在数据中时，应使用sigmoid变换。其他可以做更多研究的核有:方差分析核、高斯核和贝塞尔函数核。</p><h2 id="e900" class="jt ju hy bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">超参数</h2><p id="bd13" class="pw-post-body-paragraph iv iw hy ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">SVM类中最重要的参数是C和γ。c指的是超平面在类之间分隔的边缘的距离。默认值为1，但较高的C值意味着减少误分类的较小裕度，而较低的C值意味着增加误分类率的较大裕度。c是正则化参数，因为该值将在模型过拟合中起作用。更高的C导致过度拟合。</p><p id="b6e9" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Gamma仅用于rbf和sigmoid核。该参数设置每个点对相邻点的影响程度。低gamma允许每个点对其相邻点具有较高的影响，这意味着分类更加自由，相似点之间的距离也更大。高灰度系数意味着每个点的影响较低，这意味着点必须靠得更近才能被分类在一起。较大的灰度系数会导致较低的泛化能力和更多的过拟合，而较低的灰度系数会导致更高的泛化能力。</p><h2 id="21b5" class="jt ju hy bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">系数</h2><p id="024a" class="pw-post-body-paragraph iv iw hy ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">仅适用于线性内核。这些系数赋予模型可解释性。因为这些系数与线性模型中类别的预测相关，所以它解释了模型的特征重要性。SVM类实例中的<code class="du ky kz la lb b">.<a class="ae hv" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.coef_" rel="noopener ugc nofollow" target="_blank">coef_</a></code> <strong class="ix hz"> </strong>属性会列出它的系数，而<code class="du ky kz la lb b">.get_feature_names()</code>会给出特性标签。</p><h2 id="3b36" class="jt ju hy bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">结论</h2><p id="ad8d" class="pw-post-body-paragraph iv iw hy ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">SVM在概念上比其他ML模型更复杂。SVM适用于包含许多要素的较小数据集，因为它适用于较高的维度。当我们的数据点没有清晰的划分/分离时，SVM表现不佳。必须使用定标器对要素进行定标，而SVM计算量很大，因此不适合大型数据集。SVM不容易过拟合，因为它具有良好的正则化参数(C，γ)。它也有一个回归模型。我认为SVM是一个多才多艺的、通用的、多用途的模型，它做得很好。</p></div></div>    
</body>
</html>