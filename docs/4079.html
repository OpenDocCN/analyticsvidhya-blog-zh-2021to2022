<html>
<head>
<title>Grammatical Error Correction : Machines understanding and correcting errors in text!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语法纠错:机器理解和纠正文本中的错误！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/grammatical-error-correction-machines-understanding-and-correcting-errors-in-text-21638203ca9f?source=collection_archive---------5-----------------------#2021-08-24">https://medium.com/analytics-vidhya/grammatical-error-correction-machines-understanding-and-correcting-errors-in-text-21638203ca9f?source=collection_archive---------5-----------------------#2021-08-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/5eb8a344e53cd5996f6472ac097e36e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mM2J7_mst_U-fFLZ"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">纳迪·博罗迪纳在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="f6a9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">语法错误纠正(GEC) </strong>是指<strong class="iw hi">检测和纠正语法错误句子的语法</strong>中的错误的任务。这些错误可能包括各种语法错误，如<strong class="iw hi">拼写错误、冠词、介词、代词、名词等的错误使用，甚至是糟糕的句子结构</strong>。GEC本质上是一项NLP任务，它通过学习如何像人类一样纠正语法，使我们的机器像我们一样思考。语法纠错有广泛的应用，用于测试输入文本的语法，例如在电子邮件应用、文本编辑器、记事本等中。</p><p id="295a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个博客旨在解决使用神经网络纠正语法错误的任务，并试图分析改进它所需的步骤。</p><h2 id="1723" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated"><strong class="ak">以下是本博客的内容:</strong></h2><ol class=""><li id="17a6" class="kn ko hh iw b ix kp jb kq jf kr jj ks jn kt jr ku kv kw kx bi translated">文献调查</li><li id="8a83" class="kn ko hh iw b ix ky jb kz jf la jj lb jn lc jr ku kv kw kx bi translated">问题的ML公式</li><li id="4a0c" class="kn ko hh iw b ix ky jb kz jf la jj lb jn lc jr ku kv kw kx bi translated">业务限制</li><li id="b7de" class="kn ko hh iw b ix ky jb kz jf la jj lb jn lc jr ku kv kw kx bi translated">电子设计自动化(Electronic Design Automation)</li><li id="25c4" class="kn ko hh iw b ix ky jb kz jf la jj lb jn lc jr ku kv kw kx bi translated">基线模型</li><li id="08d2" class="kn ko hh iw b ix ky jb kz jf la jj lb jn lc jr ku kv kw kx bi translated">最终模型</li><li id="6f2d" class="kn ko hh iw b ix ky jb kz jf la jj lb jn lc jr ku kv kw kx bi translated">结果</li></ol></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><h1 id="aeb1" class="lk jt hh bd ju ll lm ln jy lo lp lq kc lr ls lt kf lu lv lw ki lx ly lz kl ma bi translated"><strong class="ak">文献调查</strong></h1><p id="5ecb" class="pw-post-body-paragraph iu iv hh iw b ix kp iz ja jb kq jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">Naghshnejad等人。艾尔。谈论语法错误检测和纠正系统的最新趋势和以前的工作，以及这些工作面临的挑战。解决这个问题的方法主要是基于规则、基于语法、基于机器学习和基于深度神经网络。此外，资源较少的语言，如阿姆哈拉语、丹麦语、希腊语、拉脱维亚语、旁遮普语、菲律宾语、阿拉伯语也得到解决。</p><h2 id="e817" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated"><strong class="ak">基于规则、基于语法vs机器学习和深度神经网络</strong></h2><p id="690c" class="pw-post-body-paragraph iu iv hh iw b ix kp iz ja jb kq jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated"><em class="me">基于规则的系统</em>使用一组预定义的规则(错误模式)来匹配文本，所有规则都是手工开发的。<em class="me">另一方面，</em>基于语法的系统，使用文本的语法以及词汇数据库、词法分析器和语法分析器。解析器用给定的语法创建解析树，如果完整的树创建不成功，则认为文本是错误的。</p><p id="cfd6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这些方法对于一些语法错误是有利的，但是当我们想要覆盖许多不同种类的语法错误时，这些方法是低效的。此外，由于它们需要大量的存储和大量的劳动力，所以探索了其他方法。</p><p id="ba6f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="me">机器学习方法</em>主要包括两种方法，即基于分类和基于统计的方法。这些方法在GEC的任务中非常成功，但需要大量的功能工程，反过来，需要大量的时间和专业知识。</p><p id="e85e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最近的工作表明，在语法错误纠正和检测任务中使用<em class="me">神经网络和神经机器翻译</em>的结果非常令人鼓舞。许多这样的NMT体系结构已经使用了字符级翻译模型或单词级翻译模型。</p><blockquote class="mf"><p id="e184" class="mg mh hh bd mi mj mk ml mm mn mo jr dx translated">为什么要使用神经网络？</p><p id="487f" class="mg mh hh bd mi mj mk ml mm mn mo jr dx translated">-不需要特征工程，因为深度学习算法会自己学习最重要的特征</p><p id="f41f" class="mg mh hh bd mi mj mk ml mm mn mo jr dx translated">-已经证明在GEC任务中提供了非常强大的基线</p></blockquote><p id="3bfc" class="pw-post-body-paragraph iu iv hh iw b ix mp iz ja jb mq jd je jf mr jh ji jj ms jl jm jn mt jp jq jr ha bi translated">用于GEC的几个常用数据集是CoNLL-13、CoNLL-14(来自CoNLL GEC竞赛)、FCE数据集等。</p><h2 id="8dd5" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated"><strong class="ak">字符级与单词级翻译模型</strong></h2><p id="b8d2" class="pw-post-body-paragraph iu iv hh iw b ix kp iz ja jb kq jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated"><a class="ae it" href="https://aclanthology.org/N16-1042/" rel="noopener ugc nofollow" target="_blank">袁与布里斯科等人。艾尔。</a>使用了一个<strong class="iw hi">词级</strong>神经机器翻译模型，使用RNNs进行从不正确的句子到格式良好的句子的序列到序列映射，在FCE数据集上取得了53.49的F0.5分数，在CoNLL-2014测试集上取得了39.9的分数，这是一个相当不错的分数。</p><p id="e695" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">他们的模型用UNK符号取代了不常用的单词，并在NMT模型的输出中应用了GIZA++和METEOR等无监督对齐模型，以找到未知单词的来源，然后他们使用在平行句子上训练的单词级翻译模型进行翻译。</p><p id="b8ee" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这种方法的局限性是，该模型没有考虑词汇之外或上下文中的OOV词来做出正确的决策，并且不能很好地概括以纠正在并行训练数据中未看到的词。</p><p id="923f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">另一方面，<a class="ae it" href="https://www.researchgate.net/publication/301879441_Neural_Language_Correction_with_Character-Based_Attention" rel="noopener ugc nofollow" target="_blank">谢等人</a>使用了<strong class="iw hi">字符级</strong>翻译模型，而不是单词级模型，避免了单词的问题，在CoNLL 14测试集上取得了40.56 F0.5的成绩。</p><p id="5f32" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这种方法的局限性在于，即使与基于单词的语言模型一起使用，该模型也不能利用单词级信息。</p><h2 id="8758" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated"><strong class="ak">CNN的使用</strong></h2><p id="ee71" class="pw-post-body-paragraph iu iv hh iw b ix kp iz ja jb kq jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated"><a class="ae it" href="http://nlp.seas.harvard.edu/papers/aesw2016.pdf" rel="noopener ugc nofollow" target="_blank">金等人。艾尔。</a>给出了一个模型，该模型是由<strong class="iw hi">三个基于字符的编码器-解码器模型、一个基于单词的编码器-解码器模型和在AESW 2016二进制预测共享任务(识别句子中是否有错误)上执行最高的句子级CNN </strong>的组合。</p><h2 id="f029" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated"><strong class="ak">其他途径</strong></h2><ol class=""><li id="d08f" class="kn ko hh iw b ix kp jb kq jf kr jj ks jn kt jr ku kv kw kx bi translated"><a class="ae it" href="https://aclanthology.org/D12-1052.pdf" rel="noopener ugc nofollow" target="_blank">达尔迈尔等人。al </a>。提出了一种<strong class="iw hi">波束搜索解码器，它从当前假设中迭代生成新的假设修正，并基于语法正确性和流畅性的特征对其进行评分</strong>。</li><li id="3edc" class="kn ko hh iw b ix ky jb kz jf la jj lb jn lc jr ku kv kw kx bi translated"><a class="ae it" href="https://arxiv.org/pdf/1906.01733.pdf" rel="noopener ugc nofollow" target="_blank"> Alikaniotis等人</a>提出了<strong class="iw hi">在大型语言语料库上训练的最新变压器语言模型，并在没有任何监督训练的情况下在GEC任务上评估了它们的性能</strong>。使用语言模型完成GEC任务背后的核心思想是，语法错误的句子出现的概率应该很低。我们需要学习如何将这种低概率序列转换成高概率序列或者语法正确的序列。</li><li id="9c2d" class="kn ko hh iw b ix ky jb kz jf la jj lb jn lc jr ku kv kw kx bi translated"><a class="ae it" href="https://aclanthology.org/2020.findings-emnlp.275.pdf" rel="noopener ugc nofollow" target="_blank">拉赫贾等人。艾尔。</a>展示了<strong class="iw hi">在使用生成器鉴别器架构的GEC任务中使用对抗学习方法</strong>。该生成器是一个转换器模型，被训练成在给出语法不正确的句子的情况下产生语法正确的句子。鉴别器是一个句子对分类模型，用于从语法上判断句子对。这两个模型都是在平行文本上进行预训练，然后使用策略梯度方法进行进一步微调，该方法为可能是语法错误文本的真正更正的句子分配高奖励。</li></ol><h1 id="e9b9" class="lk jt hh bd ju ll mu ln jy lo mv lq kc lr mw lt kf lu mx lw ki lx my lz kl ma bi translated"><strong class="ak">问题的ML公式化</strong></h1><p id="8af0" class="pw-post-body-paragraph iu iv hh iw b ix kp iz ja jb kq jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">我们使用NMT将GEC任务建模为序列到序列学习任务，其中来自源语言的文本或向量被映射到目的语言的向量，这只是给定输入的输出的最大似然估计。</p><h1 id="f2b3" class="lk jt hh bd ju ll mu ln jy lo mv lq kc lr mw lt kf lu mx lw ki lx my lz kl ma bi translated"><strong class="ak">业务限制:</strong></h1><ul class=""><li id="5495" class="kn ko hh iw b ix kp jb kq jf kr jj ks jn kt jr mz kv kw kx bi translated">系统应该具有高精度，并且优先考虑具有更高精度的系统，而不是召回。</li><li id="2c16" class="kn ko hh iw b ix ky jb kz jf la jj lb jn lc jr mz kv kw kx bi translated">低延迟总是首选。</li><li id="00b3" class="kn ko hh iw b ix ky jb kz jf la jj lb jn lc jr mz kv kw kx bi translated">此外，由于问题涉及将语法不正确的文本作为输入，当部署在服务器上时，我们需要优化的管道来处理输入数据量。</li></ul><h1 id="bc9d" class="lk jt hh bd ju ll mu ln jy lo mv lq kc lr mw lt kf lu mx lw ki lx my lz kl ma bi translated"><strong class="ak">数据集</strong>:</h1><p id="f263" class="pw-post-body-paragraph iu iv hh iw b ix kp iz ja jb kq jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">有多个数据集可用于这项任务，如CoNLL-13、CoNLL-14、Lang-8、FCE、JFLEG等。每个聚焦于不同种类的误差和目标。</p><p id="e01b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于这个任务，我使用了Lang-8数据集。</p><h1 id="3863" class="lk jt hh bd ju ll mu ln jy lo mv lq kc lr mw lt kf lu mx lw ki lx my lz kl ma bi translated"><strong class="ak"> EDA: </strong></h1><div class="na nb nc nd fd ab cb"><figure class="ne ii nf ng nh ni nj paragraph-image"><img src="../Images/386b1f9620e45c6562ed367d3d3f0dd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*o4NTXPcRBhvOAGbV4z_wbw.png"/></figure><figure class="ne ii nk ng nh ni nj paragraph-image"><img src="../Images/0806b2d4312944ec83f6c783f1abe6eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*1bbwyc-pnKb-kWDwyYESNw.png"/></figure></div><p id="6bf4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上图显示了输入的句子长度直方图，即<strong class="iw hi"> </strong>语法错误的句子和输出的语法正确的句子。每个语料库的句子长度分布似乎是相似的。</p><div class="na nb nc nd fd ab cb"><figure class="ne ii nl ng nh ni nj paragraph-image"><img src="../Images/871bdbe18c0ad1bf91e44cf0f29779b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*tnZkhJfd4dgGNouu8gz-vw.png"/></figure><figure class="ne ii nl ng nh ni nj paragraph-image"><img src="../Images/25d1ec458a4ddb6e1de1cfbc5ba369d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*i_dLA7ucdZh2AyLpOY45hw.png"/><figcaption class="ip iq et er es ir is bd b be z dx nm di nn no translated">语法正确的句子(左)和语法不正确的句子(右)的单词云</figcaption></figure></div><p id="0f27" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上面的图显示了单词云，以找出语法正确和语法不正确的句子中最常见的单词，这些单词有些相似(think，today，friend等)。</p><p id="504d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们还可以通过向从任何来源获得的文本中添加错误来创建自己的数据集。下面是一个使用Spacy的简单代码。</p><figure class="na nb nc nd fd ii"><div class="bz dy l di"><div class="np nq l"/></div></figure><h2 id="7019" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated"><strong class="ak">损失:</strong></h2><p id="f9c1" class="pw-post-body-paragraph iu iv hh iw b ix kp iz ja jb kq jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">我们使用<strong class="iw hi">屏蔽稀疏分类交叉熵</strong>损失作为损失函数。</p><h2 id="215c" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated"><strong class="ak">指标:</strong></h2><p id="ca6c" class="pw-post-body-paragraph iu iv hh iw b ix kp iz ja jb kq jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated"><strong class="iw hi"> Fbeta分数</strong>(beta = 0.5)用作训练指标，而<strong class="iw hi"> BLEU分数</strong>用于判断最终表现。</p><h1 id="b737" class="lk jt hh bd ju ll mu ln jy lo mv lq kc lr mw lt kf lu mx lw ki lx my lz kl ma bi translated"><strong class="ak">基线模型:</strong></h1><p id="39fc" class="pw-post-body-paragraph iu iv hh iw b ix kp iz ja jb kq jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">对于基线模型，我专门训练了一个普通的编码器解码器模型和一个编码器解码器模型。</p><h2 id="908f" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated"><strong class="ak">编码器解码器和注意力模型</strong></h2><p id="e50d" class="pw-post-body-paragraph iu iv hh iw b ix kp iz ja jb kq jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">由<a class="ae it" href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> Sutskever等人</a>介绍的<strong class="iw hi">编码器解码器模型</strong>，广泛用于序列到序列的映射，基本上由编码器和解码器两部分组成，其中<strong class="iw hi">编码器</strong> <em class="me">将输入向量</em>编码为<em class="me">上下文向量</em>，而<strong class="iw hi">解码器</strong> <em class="me">将该上下文向量解码为输出</em>。<strong class="iw hi">上下文向量</strong>被期望<em class="me">捕获输入</em>的本质，其基本上是编码器的输出。rnn被广泛用作编码器和解码器，因为它们非常好地捕获序列信息。</p><p id="644d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">编码器/解码器模型执行得非常好，但是一旦输入向量开始变长，就很难捕捉到本质。为了解决这个问题，Bahdanau等人引入了注意机制。艾尔。改进编码器/解码器模型，成功翻译较长的输入句子。</p><p id="0fc5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">直观地说，<strong class="iw hi">注意机制</strong> <em class="me">获取每一个时间步长</em>的上下文向量和编码器输出，<em class="me">为每一个编码器时间步长</em>计算一个 <strong class="iw hi">注意权重</strong> <em class="me">。这个权重告诉我们<em class="me">对每个编码器输出</em>的重视程度，并与每个时间步的编码器输出相乘，然后在每个时间步馈送给解码器。因此，在确定每个时间步</em>的解码器输出时，解码器知道<em class="me">对哪个输入字给予最大关注。下图解释了注意力网络。</em></p><figure class="na nb nc nd fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nr"><img src="../Images/5892681363e9e501e75cfa2bfe0c1e22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FBWBW6PxWJvpk81TUyb1vQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">注意机制:<a class="ae it" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="a846" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">左下方由hs条表示的蓝色块代表所有时间步长的<strong class="iw hi">编码器输出</strong>。我们可以想象，五个蓝色模块代表编码器输入，或者换句话说，五个字(假设字级转换)，在每个时间步长都被输入到编码器模块，从而得到编码器在每个时间步长的输出。</p><p id="9249" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由ht表示的右下方的栗色块表示每个时间步长的<strong class="iw hi">解码器隐藏状态</strong>，因此为了简单起见，我们假设最后一个块正在解码输入句子的第二个单词。</p><p id="825d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如左图所示，解码时，对于每一步，从编码器输出创建一个<strong class="iw hi">上下文向量</strong>，ct。如果使用<strong class="iw hi">全局注意力</strong>，所有编码器输出(即在所有时间步长)用于创建该向量。这个向量实际上就是每个输出与一个<strong class="iw hi">注意力权重</strong>相乘的和。at表示包含每个时间步长的所有注意力权重的向量。</p><p id="4952" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了获得这个注意力权重，我们使用评分函数，并将其应用于编码器输出和解码器隐藏状态。例如，如果我们使用点积评分函数，我们简单地取编码器状态和解码器隐藏状态的点积来得到注意力权重。这个权重告诉我们每个编码器状态对于当前解码器状态给予我们多少重要性或上下文。</p><p id="0b99" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于右侧所示的<strong class="iw hi">局部注意</strong>,我们仅使用解码器所处时间步长附近的编码器状态。</p><p id="3ab2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，我们将当前解码器隐藏状态ht与上下文向量ct连接起来，并将其馈送给解码器，以获得下一个解码器隐藏状态ht~。</p><h2 id="30e6" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated"><strong class="ak">培训</strong></h2><p id="f707" class="pw-post-body-paragraph iu iv hh iw b ix kp iz ja jb kq jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">我使用Adam优化器训练模型，学习率为0.01。对于嵌入，我参考<a class="ae it" href="https://github.com/mridul1012/Grammatical-Error-Correction-with-Neural-Networks" rel="noopener ugc nofollow" target="_blank"> this </a> github使用了FastText嵌入。为了达到最佳效果，我将句子的最大长度控制在15个。</p><figure class="na nb nc nd fd ii"><div class="bz dy l di"><div class="np nq l"/></div></figure><figure class="na nb nc nd fd ii"><div class="bz dy l di"><div class="np nq l"/></div></figure><h2 id="3b77" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">结果:</h2><p id="ea91" class="pw-post-body-paragraph iu iv hh iw b ix kp iz ja jb kq jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">注意图:</p><figure class="na nb nc nd fd ii er es paragraph-image"><div class="er es ns"><img src="../Images/e282c2772264e3b5db6e563f804bb444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*dC9c4T4sTdbO3YSBUycDbw.png"/></div></figure><figure class="na nb nc nd fd ii er es paragraph-image"><div class="er es nt"><img src="../Images/ac14fdc24fa90157929695a337e4db01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*66P9OH1F6oKdhHm_8pZeqw.png"/></div></figure><figure class="na nb nc nd fd ii er es paragraph-image"><div class="er es nu"><img src="../Images/bb708c1881338cb07fb4ea0f4a62db15.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*ludo9iNm2Ph79kCbYA0gGw.png"/></div></figure><p id="aa60" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">翻译结果:</p><figure class="na nb nc nd fd ii"><div class="bz dy l di"><div class="np nq l"/></div></figure><p id="490b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最终BLEU分数:0.68</p><p id="5b8e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="me">感谢阅读！</em></p><p id="0bbe" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所有的代码都可以在<a class="ae it" href="https://github.com/aishani691/Grammar_Error_Correction" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="0539" class="lk jt hh bd ju ll mu ln jy lo mv lq kc lr mw lt kf lu mx lw ki lx my lz kl ma bi translated">进一步的工作</h1><p id="004d" class="pw-post-body-paragraph iu iv hh iw b ix kp iz ja jb kq jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">通过利用最先进的NLP架构(如GPT架构、BERT等)的能力，现有方法还有很大的改进空间。gan也已经被证明是许多NLP应用的游戏规则改变者，并且可以进一步提高输出。</p></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><h2 id="57d9" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated"><strong class="ak">参考文献:</strong></h2><ol class=""><li id="533b" class="kn ko hh iw b ix kp jb kq jf kr jj ks jn kt jr ku kv kw kx bi translated"><a class="ae it" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/lil-log/2018/06/24/attention-attention . html</a></li><li id="0238" class="kn ko hh iw b ix ky jb kz jf la jj lb jn lc jr ku kv kw kx bi translated"><a class="ae it" href="https://www.appliedaicourse.com/course/11/Applied-Machine-learning-course" rel="noopener ugc nofollow" target="_blank">https://www . Applied ai course . com/course/11/Applied-Machine-learning-course</a></li></ol></div></div>    
</body>
</html>