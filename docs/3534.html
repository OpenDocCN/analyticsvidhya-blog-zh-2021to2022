<html>
<head>
<title>Neural Network Guide for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">初学者神经网络指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-network-guide-for-beginners-d3457bbb7e34?source=collection_archive---------8-----------------------#2021-07-07">https://medium.com/analytics-vidhya/neural-network-guide-for-beginners-d3457bbb7e34?source=collection_archive---------8-----------------------#2021-07-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="3b15" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇神经网络指南是为那些刚开始学习神经网络，并想很好地理解如何用Python自己构建一个神经网络的人准备的。随着你对神经网络的基本拓扑结构越来越熟悉，所有复杂的数学和复杂的算法都会随着你对这个主题的深入而变得更有意义。因此，我试图把这个话题保持在一个很高的水平，而不是深入细节。我个人的信念是，加速你对神经网络理解的最好方法是把手弄脏，而不是试图理解引擎盖下的所有理论概念。</p><p id="114c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了帮助您理解神经网络模型的一般拓扑或结构，我将探索三种不同的情况(回归、二元分类和多类分类)来演示和强调细微的差异。</p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><h1 id="3444" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">1.回归</h1><p id="4dd4" class="pw-post-body-paragraph ie if hh ig b ih kh ij ik il ki in io ip kj ir is it kk iv iw ix kl iz ja jb ha bi translated">提醒一下，在回归问题中，我们的目标是预测连续的值。在现实生活中，这个目标变量可以代表无数的事物。出于本练习的目的，我将使用scikit-learn的数据集模块生成自己的回归数据集，而不是使用真实的数据。</p><p id="6cec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">首先，我们导入以下:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="a42b" class="kv jk hh kr b fi kw kx l ky kz"># Imports<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span><span id="aedd" class="kv jk hh kr b fi la kx l ky kz">from sklearn.datasets import make_classification, make_regression<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.model_selection import train_test_split</span><span id="a11f" class="kv jk hh kr b fi la kx l ky kz">from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense<br/>from tensorflow.keras.utils import to_categorical</span></pre><p id="6393" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">导入后，让我们生成自己的回归数据集:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="1371" class="kv jk hh kr b fi kw kx l ky kz">X, y = make_regression(n_samples=10000, n_features=20, random_state=42)</span></pre><p id="24a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，我正在生成一个包含10，000个样本和20个特征的回归数据集。简单地说，这是一个包含10，000行和20列的数据集。鉴于这是一个虚拟数据，这些特征代表什么并不重要。在这一点上，我们所关心的是，我们有一个可以用来建立和训练我们的第一个神经网络模型的回归数据集。</p><p id="bd03" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">训练/测试分割:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="cfec" class="kv jk hh kr b fi kw kx l ky kz">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)</span></pre><p id="b99d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个重要的注意事项！由于神经网络使用梯度下降(优化技术)来最小化损失函数，所以我们首先缩放我们的数据是至关重要的。</p><p id="b81d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">缩放数据:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="f291" class="kv jk hh kr b fi kw kx l ky kz">ss = StandardScaler()</span><span id="ed91" class="kv jk hh kr b fi la kx l ky kz">Z_train = ss.fit_transform(X_train)<br/>Z_test = ss.transform(X_test)</span></pre><p id="f002" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们已经对数据进行了缩放，让我们最终跳到构建我们的第一个神经网络模型！</p><p id="ff26" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">添加图层:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="3173" class="kv jk hh kr b fi kw kx l ky kz">model = Sequential()</span><span id="3b42" class="kv jk hh kr b fi la kx l ky kz">model.add(Dense(32, activation='relu', input_shape=(Z_train.shape[1],)))<br/>model.add(Dense(8, activation='relu'))</span><span id="5e96" class="kv jk hh kr b fi la kx l ky kz">model.add(Dense(1, activation=None))</span></pre><p id="c813" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这几行代码中发生了几个大的活动，所以让我们花点时间来分析一下这里发生了什么。</p><p id="5223" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，当我们构建神经网络模型时，我们从所谓的“输入层”开始，以“输出层”结束。夹在中间的是“隐藏层”。每一层都有许多节点或“权重”。例如，在下图中，输入层有3个节点，隐藏层有4个节点，输出层有2个节点。</p><figure class="km kn ko kp fd lc er es paragraph-image"><div class="er es lb"><img src="../Images/5171d63bfed11c2a75192553edb26b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*nevKs6306VMnE3aP-C0zbg.jpeg"/></div></figure><p id="c19f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们上面的代码本质上是从左到右构建这个一般流程。</p><ul class=""><li id="6ee3" class="lf lg hh ig b ih ii il im ip lh it li ix lj jb lk ll lm ln bi translated">我们的输入图层有20个结点，因为我们的虚拟数据集有20个要素</li><li id="e1aa" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">我们的隐藏层有32个节点，因为这是我们在这个例子中使用的数字。我可以使用任何数字，但我决定使用32，因为32是一个很好的基数2的数字。</li><li id="3a0a" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">我正在添加第二个隐藏层，有8个节点。同样，除了8也是一个很好的基数2之外，没有什么特别的原因。</li><li id="6698" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">对于我的输出层，我只用一个节点结束它。对于输出层，我们只有一个节点是很关键的，因为这是一个回归问题，我们希望模型的实际输出没有任何转换。</li></ul><p id="c019" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第二，注意每一层都有一个激活功能。现在，注意到行业标准是出于计算原因使用ReLU就足够了。您可能会遇到sigmoid或双曲线正切激活函数，但请记住ReLU是目前最常见的激活函数选择。</p><p id="abf9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">我们来编译:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="297c" class="kv jk hh kr b fi kw kx l ky kz">model.compile(optimizer='adam', loss='mse', metrics=['mae'])</span></pre><p id="88fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们花点时间来分析一下这行代码。</p><ul class=""><li id="2931" class="lf lg hh ig b ih ii il im ip lh it li ix lj jb lk ll lm ln bi translated">如前所述，神经网络使用梯度下降来寻找最优解。事实证明，梯度下降是一个非常复杂的过程，有许多不同的技术可以执行这一方法。到目前为止，最常见的优化器是“adam”。然而，还有其他可用的技术，如AdaGrad和RMSProp。作为一个通用的经验法则，当有疑问时，我建议你总是默认为“adam”。</li><li id="6aa7" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">由于这是一个回归问题，我们的损失函数是mse(均方误差)。</li><li id="c1b2" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">除了mse，我们还可以测量和比较其他回归指标。在这个特殊的例子中，我选择了mae(平均绝对误差)。</li></ul><p id="e22d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">拟合模型:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="7e6e" class="kv jk hh kr b fi kw kx l ky kz">first_model = model.fit(Z_train, y_train, batch_size=256, epochs=50, validation_data=(Z_test, y_test))</span></pre><p id="9eb5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">同样，这里发生了很多事情，所以让我们也将它分解成各个组件。</p><ul class=""><li id="a040" class="lf lg hh ig b ih ii il im ip lh it li ix lj jb lk ll lm ln bi translated">Batch_size为256意味着我们的模型将通过256个样本传播以计算误差，然后对接下来的256个样本重复该过程，依此类推。</li><li id="6a46" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">Epochs为50意味着模型会看到整个数据集50次。每当整个数据集通过神经网络模型时，我们都将其计为一个时期。</li><li id="01b6" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">请注意，在拟合模型时，我还能够传入验证数据(测试数据)。这是一个很棒的附加功能，因为我可以同时拟合和测试模型。</li></ul><p id="23ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了直观地显示损失函数在每个时期是如何改进的，我们可以绘制训练和测试数据的损失函数:</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="63d2" class="kv jk hh kr b fi kw kx l ky kz">plt.figure(figsize=(15,8))<br/>plt.plot(first_model.history['loss'])<br/>plt.plot(first_model.history['val_loss'])<br/>plt.legend(['loss', 'val loss']);</span></pre><figure class="km kn ko kp fd lc er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es lt"><img src="../Images/65fb0d7270942217c1f0cd9490859541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wX02xioXBkqqCVsYdfGU5w.jpeg"/></div></div></figure><p id="5d69" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们可以产生我们的预测！</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="39ab" class="kv jk hh kr b fi kw kx l ky kz">first_model.predict(Z_test)</span></pre><h1 id="efbf" class="jj jk hh bd jl jm ly jo jp jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg bi translated">2.二元分类</h1><p id="b1d5" class="pw-post-body-paragraph ie if hh ig b ih kh ij ik il ki in io ip kj ir is it kk iv iw ix kl iz ja jb ha bi translated"><strong class="ig hi">类似于上面的回归，让我们创建一个虚拟数据集，训练/测试分割，并缩放数据:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="8666" class="kv jk hh kr b fi kw kx l ky kz">X, y = make_classification(n_samples=10_000, n_features=20, random_state=42)</span><span id="0270" class="kv jk hh kr b fi la kx l ky kz">X, y = make_classification(n_samples=10_000, n_features=20, random_state=42)</span><span id="7c78" class="kv jk hh kr b fi la kx l ky kz">X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)</span><span id="6844" class="kv jk hh kr b fi la kx l ky kz">ss = StandardScaler()<br/>Z_train = ss.fit_transform(X_train)<br/>Z_test = ss.transform(X_test)</span></pre><p id="5a26" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">让我们来构建模型:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="37ed" class="kv jk hh kr b fi kw kx l ky kz">model = Sequential()</span><span id="adef" class="kv jk hh kr b fi la kx l ky kz">model.add(Dense(32, activation='relu', input_shape=(Z_train.shape[1],)))<br/>model.add(Dense(8, activation='relu'))</span><span id="92e9" class="kv jk hh kr b fi la kx l ky kz">model.add(Dense(1, activation='sigmoid'))</span></pre><p id="d49a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与上面的回归拓扑非常相似，我们保持一切不变，除了注意输出层，我们使用sigmoid作为我们的激活函数。这是因为sigmoid函数采用曲率并将其限制在0和1之间，这对于二元分类问题是完美的，因为我们本质上是预测概率。这与logit函数如何在逻辑回归中绘制最佳拟合线以限制0和1之间的值基本相同。</p><p id="adc1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">我们来编译:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="7aaf" class="kv jk hh kr b fi kw kx l ky kz">model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])</span></pre><p id="9177" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">再一次，一些改变适应了现在这是一个二进制分类问题的事实。</p><ul class=""><li id="b22d" class="lf lg hh ig b ih ii il im ip lh it li ix lj jb lk ll lm ln bi translated">二元分类问题的损失函数是“二元交叉熵”</li><li id="f13d" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">注意“adam”仍然是首选的优化器。</li><li id="986c" class="lf lg hh ig b ih lo il lp ip lq it lr ix ls jb lk ll lm ln bi translated">对于其他指标，我增加了准确性。我们可以添加其他分类指标，如灵敏度、特异性、精确度等。</li></ul><p id="182c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">拟合模型:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="086d" class="kv jk hh kr b fi kw kx l ky kz">model_2 = model.fit(Z_train, y_train, batch_size=256, epochs=50, validation_data=(Z_test, y_test))</span></pre><p id="b3d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">装配过程保持不变。</p><p id="f317" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如上所述，您可以生成预测、图形损失函数和度量等。</p><h1 id="b46e" class="jj jk hh bd jl jm ly jo jp jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg bi translated">3.多分类</h1><p id="e758" class="pw-post-body-paragraph ie if hh ig b ih kh ij ik il ki in io ip kj ir is it kk iv iw ix kl iz ja jb ha bi translated">类似的方法也适用于多分类问题，有三个主要的调整。</p><p id="353d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">让我们重复这个过程创建一个虚拟数据集，训练/测试分割，并缩放数据:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="bfb3" class="kv jk hh kr b fi kw kx l ky kz">X, y = make_classification(n_samples=10000, n_features=20, n_informative=4, n_classes=3)</span><span id="db78" class="kv jk hh kr b fi la kx l ky kz">y = to_categorical(y)</span><span id="be6b" class="kv jk hh kr b fi la kx l ky kz">X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)</span><span id="6542" class="kv jk hh kr b fi la kx l ky kz">ss = StandardScaler()<br/>Z_train = ss.fit_transform(X_train)<br/>Z_test = ss.transform(X_test)</span></pre><p id="7a16" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是第一个主要的调整。注意，我使用Keras中可用的to _ categorical函数将我们的目标类向量转换成二进制类矩阵。你可以认为这是对我们的目标变量的一次性编码。</p><p id="64fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，在我们的虚拟数据集中，我们创建了3个独立的类。因此，当我们对y进行一次性编码时，转换结果如下:</p><p id="b6d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">0 = [1, 0, 0]</p><p id="32ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">1 = [0, 1, 0]</p><p id="b64c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">2 = [0, 0, 1]</p><p id="ecc9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">让我们来建造:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="7da4" class="kv jk hh kr b fi kw kx l ky kz">model = Sequential()</span><span id="6fee" class="kv jk hh kr b fi la kx l ky kz">model.add(Dense(32, activation='relu', input_shape=(Z_train.shape[1],)))<br/>model.add(Dense(16, activation='relu'))</span><span id="ff6d" class="kv jk hh kr b fi la kx l ky kz">model.add(Dense(3, activation='softmax'))</span></pre><p id="a4e6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是第二个调整。对于多分类，我们的输出层的激活函数将是softmax。同样，不用深入兔子洞，注意到softmax激活函数只是使我们的输出正常化就足够了。</p><p id="42d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">我们来编译:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="2e0c" class="kv jk hh kr b fi kw kx l ky kz">model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])</span></pre><p id="884f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是第三个主要的调整。对于多分类，我们的损失函数将是分类交叉熵。我们将坚持使用adam作为我们的优化器，并使用准确性作为我们的分类标准来进行验证。</p><p id="0802" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">让我们来拟合模型:</strong></p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="161d" class="kv jk hh kr b fi kw kx l ky kz">model_3 = model.fit(Z_train, y_train, batch_size=256, epochs=50, validation_data=(Z_test, y_test))</span></pre><h1 id="5aab" class="jj jk hh bd jl jm ly jo jp jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg bi translated">摘要</h1><p id="ff16" class="pw-post-body-paragraph ie if hh ig b ih kh ij ik il ki in io ip kj ir is it kk iv iw ix kl iz ja jb ha bi translated">我们探索了如何为回归、二元分类和多分类数据集构建神经网络模型。虽然三者的整体拓扑保持不变，但我们探讨了三者之间的主要差异，主要涉及输出层激活函数和损耗函数。</p><p id="8502" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不用说，还有很多本指南没有涉及的主题。例如，我们还没有涵盖一些可以应用于神经网络的正则化方法，如l2、漏失层和早期停止。然而，我认为拥有这些基础知识将有助于你在数据科学之旅中成长和学习更多关于神经网络的知识。</p></div></div>    
</body>
</html>