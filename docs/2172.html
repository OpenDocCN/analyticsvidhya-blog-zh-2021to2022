<html>
<head>
<title>ML Code Dive: K-Nearest Neighbors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML代码潜水:K-最近邻</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ml-code-dive-k-nearest-neighbors-c183e3349ddd?source=collection_archive---------7-----------------------#2021-04-09">https://medium.com/analytics-vidhya/ml-code-dive-k-nearest-neighbors-c183e3349ddd?source=collection_archive---------7-----------------------#2021-04-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="177d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc"> ML Code Dive是在没有任何ML库的情况下，深度挖掘机器学习算法的系列文章。纯粹用Python和numpy！它不仅可以让你更好地了解ML模型的细节，还可以帮助你在数据科学家/机器学习工程师面试中胜出！你可以在这里</em>  <em class="jc">找到所有的</em> <a class="ae jd" href="https://github.com/Lou1sWang/MLCodeDive" rel="noopener ugc nofollow" target="_blank"> <em class="jc">。</em></a></p></div><div class="ab cl je jf go jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="ha hb hc hd he"><p id="4eb2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">k近邻，又名KNN，是最简单的机器学习模型之一。距离计算需要数值，并且模型不需要训练，因为它只是使用距离来获得k个最相似的数据点或最近邻，并基于它们将标签分配给未观察到的样本。</p><p id="873e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNN具有很多优点:高精度、对异常值不敏感、对数据没有任何假设。然而，对于较大的数据集，计算量很大，并且需要大量内存。它适用于数值和标称值。</p><p id="32cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">说到它的工作原理，它只有三个步骤。</p><ul class=""><li id="4a9c" class="jl jm hh ig b ih ii il im ip jn it jo ix jp jb jq jr js jt bi translated">距离计算。</li><li id="d563" class="jl jm hh ig b ih ju il jv ip jw it jx ix jy jb jq jr js jt bi translated">最低k距离投票</li><li id="31fe" class="jl jm hh ig b ih ju il jv ip jw it jx ix jy jb jq jr js jt bi translated">对投票结果进行排序</li></ul><h2 id="3520" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">距离计算</h2><p id="6547" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">距离公式有不同的选项可供选择:</p><p id="3dd1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">欧几里德距离:它是欧几里德空间中两点间真实直线距离的度量。</p><figure class="la lb lc ld fd le er es paragraph-image"><div class="er es kz"><img src="../Images/b392378b761dc7e6ead4189d04db9912.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/0*AV08V6og51opRYru.jpg"/></div></figure><p id="48be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">曼哈顿距离:两点之间的距离是它们的笛卡尔坐标的绝对差之和。<strong class="ig hi">当我们遇到高维度的情况时，该距离优于欧几里德距离。</strong></p><figure class="la lb lc ld fd le er es paragraph-image"><div class="er es lh"><img src="../Images/99e13e65ff4a220a211173fba520c45e.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/0*oPgyOun5utdR_lAD.jpg"/></div></figure><p id="c033" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">余弦距离:这个距离度量主要用于计算两个向量之间的相似性。它通过两个向量之间的夹角余弦来测量，并确定两个向量是否指向同一方向。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es li"><img src="../Images/3532a2f46d14c760afa8f1813ab72746.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/0*9FmQbC4iwOfI2ODP.png"/></div></div></figure><p id="4498" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Jaccard距离:Jaccard系数是一种与余弦相似性相似的比较方法，因为这两种方法都比较分布在所有数据中的一种属性。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ln"><img src="../Images/ddf6a3ba9efeb76546d4235c6ab21f96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zwB69V3W4C6LSPZS.jpg"/></div></div></figure><p id="5667" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们用欧几里德距离编码。我们可以使用numpy中的array来表示数据集，并使用numpy.tile来获取输入(未观察到的)数据和所有其他数据点之间的距离</p><pre class="la lb lc ld fd lo lp lq lr aw ls bi"><span id="b8cf" class="jz ka hh lp b fi lt lu l lv lw">import numpy as np</span><span id="7433" class="jz ka hh lp b fi lx lu l lv lw">def getDistance(x, dataset):</span><span id="dd7c" class="jz ka hh lp b fi lx lu l lv lw">    rows = dataset.shape[0]<br/>    diff = np.tile(x, (dataset,1)) - dataset<br/>    distances = (diff**2.sum(axis=1)) **0.5<br/>    return distances</span></pre><p id="4aee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">值得注意的一点是，我们应该归一化数据集，因为如果要素处于不同的比例，一些较大的数字将在距离计算中占主导地位。这是最小-最大归一化的代码，我们减去最小值，然后除以范围</p><pre class="la lb lc ld fd lo lp lq lr aw ls bi"><span id="601e" class="jz ka hh lp b fi lt lu l lv lw">def minMaxNorm(dataset):<br/>    minVals = dataset.min(0)<br/>    maxVals = dataset.max(0)<br/>    ranges = maxVals - minVals<br/>    normDataset = np.zeros(dataset.shape)<br/>    rows = dataset.shape[0]<br/>    normDataset = dataset - np.tile(minVals,(m,1))<br/>    normDataset = normDataset/np.tile(ranges,(m,1))<br/>    return normDataset</span></pre><h2 id="b5fd" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">最低k距离投票和排序</h2><p id="df14" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">获得所有距离后，我们只选择k个最近的数据点，并根据这k个点的标签分配标签。输入k应该总是正整数。然后我们返回投票最多的标签。</p><pre class="la lb lc ld fd lo lp lq lr aw ls bi"><span id="dbae" class="jz ka hh lp b fi lt lu l lv lw">def getVote(distances, labels, k):<br/>    # sort the distances and get The index with argsort<br/>    sortedDisIndex = distances.argsort()</span><span id="1953" class="jz ka hh lp b fi lx lu l lv lw">    # count the class for the k points<br/>    classCount = {}<br/>    for i in range(k):<br/>        voteLabel = labels<br/>        classCount[voteLabel] = classCount.get(voteLabel,0) + 1<br/>    sortedClassCount = sorted(classCount.items(), key = lambda x:x[0], reverse = True)</span><span id="1fc1" class="jz ka hh lp b fi lx lu l lv lw">   # return the first result after voting<br/>   return sortedClassCount[0][0]</span></pre><h2 id="9799" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">关于选择K的一些讨论</h2><p id="e52d" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">在机器学习中，我们应该始终考虑偏差-方差权衡，在KNN，k对模型的性能起着非常重要的作用。当k=1时，方差很高，因为模型“记忆”数据，这导致过度拟合。随着k的增加，偏差增加，而方差减少。该模型对于未观察到的(测试)数据变得更加稳健。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ly"><img src="../Images/11475c1c3f0a689d214c44170f333af0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*B4_0HXIFReFlVisA.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">差错率和k . Credit:<a class="ae jd" href="http://sameersingh.org/courses/gml/fa17/sched.html" rel="noopener ugc nofollow" target="_blank">http://sameersingh.org/courses/gml/fa17/sched.html</a></figcaption></figure><h1 id="5bd4" class="md ka hh bd kb me mf mg kf mh mi mj kj mk ml mm km mn mo mp kp mq mr ms ks mt bi translated">摘要</h1><p id="6a62" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">KNN是一种简单有效的数据分类方法。这是一个实例学习的例子，其中你需要手头有数据实例来执行机器学习算法。它是一种<strong class="ig hi"><em class="jc"/></strong><em class="jc"/><strong class="ig hi"><em class="jc">的非参数懒惰学习，并且不需要对数据分布的假设。</em> </strong>因此，对于大型数据集来说，存储成本很高。</p></div><div class="ab cl je jf go jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="ha hb hc hd he"><p id="240a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">你可以在</em><a class="ae jd" href="https://www.linkedin.com/in/yu-feng/" rel="noopener ugc nofollow" target="_blank"><em class="jc">LinkedIn</em></a><em class="jc">联系我，觉得文章有用就给文章鼓掌！</em></p></div></div>    
</body>
</html>