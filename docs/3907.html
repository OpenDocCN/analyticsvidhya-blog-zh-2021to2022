<html>
<head>
<title>Regression Trees | Decision Tree for Regression | Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回归树|回归决策树|机器学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/regression-trees-decision-tree-for-regression-machine-learning-e4d7525d8047?source=collection_archive---------0-----------------------#2021-08-08">https://medium.com/analytics-vidhya/regression-trees-decision-tree-for-regression-machine-learning-e4d7525d8047?source=collection_archive---------0-----------------------#2021-08-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="ebc6" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">回归树如何用于解决回归问题？如何建立一个？</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/ad1dc7a04399e99185c7077271bf6014.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E2br87UjCErSE2eqJ56DWQ.png"/></div></div></figure><p id="74d2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这篇博客假设读者熟悉决策树和回归的概念。如果没有，请参考下面的博客。</p><ul class=""><li id="50f3" class="kf kg hi jl b jm jn jp jq js kh jw ki ka kj ke kk kl km kn bi translated"><a class="ae ko" rel="noopener" href="/analytics-vidhya/decision-trees-for-classification-id3-machine-learning-6844f026bf1a">决策树</a></li><li id="377c" class="kf kg hi jl b jm kp jp kq js kr jw ks ka kt ke kk kl km kn bi translated"><a class="ae ko" rel="noopener" href="/analytics-vidhya/introduction-to-machine-learning-and-artificial-intelligence-cebb3528c823">机器学习入门</a></li><li id="ac20" class="kf kg hi jl b jm kp jp kq js kr jw ks ka kt ke kk kl km kn bi translated"><a class="ae ko" rel="noopener" href="/analytics-vidhya/linear-regression-with-gradient-descent-derivation-c10685ddf0f4">带梯度下降的线性回归</a></li></ul><h1 id="b4e4" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">什么是回归树？</h1><p id="29c0" class="pw-post-body-paragraph jj jk hi jl b jm lm ij jo jp ln im jr js lo ju jv jw lp jy jz ka lq kc kd ke hb bi translated">阅读了上面的博客或者已经熟悉了相关的主题，现在你有希望理解什么是决策树(我们用于分类任务的那个)。回归树基本上是用于回归任务的决策树，可用于预测连续值输出而不是离散输出。</p><h1 id="5af3" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">均方误差</h1><p id="4429" class="pw-post-body-paragraph jj jk hi jl b jm lm ij jo jp ln im jr js lo ju jv jw lp jy jz ka lq kc kd ke hb bi translated">在用于分类的决策树中，我们看到了树如何在正确的节点提出正确的问题，以便给出准确有效的分类。在分类树中实现这一点的方法是使用两种度量，即熵和信息增益。但是既然我们预测的是连续变量，我们就不能计算熵，也不能经历同样的过程。我们现在需要一个不同的衡量标准。一个告诉我们我们的预测偏离原始目标多少的度量，这是均方差的切入点。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/24b4fbd40ba30ced744c0b4feebd9fe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*WZysd8kAnlTU_Ec1mxNgJA.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">图1.1:均方值</figcaption></figure><p id="5185" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> Y </strong>是实际值<strong class="jl hj"> Y_hat </strong>是预测值，我们只关心预测值与目标值相差多少。不是哪个方向。因此，我们计算差值的平方，然后将总和除以记录总数。</p><p id="bde9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在回归树算法中，我们做与分类树相同的事情。但是，我们试图减少每个孩子的均方差，而不是熵。</p><h1 id="a030" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">构建回归树</h1><p id="ba37" class="pw-post-body-paragraph jj jk hi jl b jm lm ij jo jp ln im jr js lo ju jv jw lp jy jz ka lq kc kd ke hb bi translated">让我们考虑一个数据集，其中有两个变量，如下所示</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lw"><img src="../Images/a58a8d2daa1257f5e33097e81dca2e26.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*5X3Icrvsn9_huVbTfoPlKA.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">图2.1:数据集，X是一个连续变量，Y是另一个连续变量</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lx"><img src="../Images/7c15fdfb0d9ed27bb46f7ce085e34203.png" data-original-src="https://miro.medium.com/v2/resize:fit:228/format:webp/1*1P4T_H7eSHgUzREo4CMszA.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">图2.2:实际的数据集表</figcaption></figure><p id="5367" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们需要建立一个回归树，在给定x的情况下最好地预测Y。</p><h2 id="f3e6" class="ly kv hi bd kw lz ma mb la mc md me le js mf mg lg jw mh mi li ka mj mk lk ml bi translated"><strong class="ak">第一步</strong></h2><p id="c102" class="pw-post-body-paragraph jj jk hi jl b jm lm ij jo jp ln im jr js lo ju jv jw lp jy jz ka lq kc kd ke hb bi translated">第一步是基于X对数据进行排序(本例中的<em class="mm">，已经排序了</em>)。然后，取变量X中前2行的平均值(根据给定数据集为(1+2)/2 = 1.5)。将数据集分成2部分<em class="mm">(A部分和B部分)</em>，由x &lt; 1.5和X ≥ 1.5分隔。</p><p id="e3ac" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，A部分仅包含一个点，即第一行(1，1)，所有其他点都在B部分中。现在，分别取A部分中所有Y值的平均值和B部分中所有Y值的平均值。这两个值是x &lt; 1.5 and x ≥ 1.5 respectively. Using the predicted and original values, calculate the mean square error and note it down.</p><h2 id="504b" class="ly kv hi bd kw lz ma mb la mc md me le js mf mg lg jw mh mi li ka mj mk lk ml bi translated">Step 2</h2><p id="07c9" class="pw-post-body-paragraph jj jk hi jl b jm lm ij jo jp ln im jr js lo ju jv jw lp jy jz ka lq kc kd ke hb bi translated">In step 1, we calculated the average for the first 2 numbers of sorted X and split the dataset based on that and calculated the predictions. Then, we do the same process again but this time, we calculate the average for the second 2 numbers of sorted X ( (2+3)/2 = 2.5 ). Then, we split the dataset again based on X &lt; 2.5 and X ≥ 2.5 into Part A and Part B again and predict outputs, find mean square error as shown in step 1. This process is repeated for the third 2 numbers, the fourth 2 numbers, the 5th, 6th, 7th till n-1th 2 numbers (<em class="mm">的决策树的预测输出，其中n是数据集</em>中的记录或行数。</p><h2 id="52c8" class="ly kv hi bd kw lz ma mb la mc md me le js mf mg lg jw mh mi li ka mj mk lk ml bi translated">第三步</h2><p id="f2ab" class="pw-post-body-paragraph jj jk hi jl b jm lm ij jo jp ln im jr js lo ju jv jw lp jy jz ka lq kc kd ke hb bi translated">现在我们已经计算了n-1个均方误差，我们需要选择分割数据集的点。这个点是分裂时产生最小均方误差的点。在这种情况下，点是x=5.5。因此，这棵树将被分成两部分。x &lt;5.5 and x≥ 5.5. The Root node is selected this way and the data points that go towards the left child and right child of the root node are further recursively exposed to the same algorithm for further splitting.</p><h1 id="0cd2" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">Brief Explanation of What the algorithm is doing</h1><p id="7e42" class="pw-post-body-paragraph jj jk hi jl b jm lm ij jo jp ln im jr js lo ju jv jw lp jy jz ka lq kc kd ke hb bi translated">The basic idea behind the algorithm is to find the point in the independent variable to split the data-set into 2 parts, so that the mean squared error is the minimised at that point. The algorithm does this in a repetitive fashion and forms a tree-like structure.</p><p id="5674" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">A regression tree for the above shown dataset would look like this</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mn"><img src="../Images/14a1ef21bacc913ac47426950dd1f75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*Xiz5EiXm0L1_i08seiEP-g.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">fig 3.1: The resultant Decision Tree</figcaption></figure><p id="5959" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">and the resultant prediction visualisation would be this</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mo"><img src="../Images/5f4744420a20193c220bc4c73b511e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*fLy0JUqjVijnQbgqEqtjtA.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">fig 3.2: The Decision Boundary</figcaption></figure><p id="43f7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">well, The logic behind the algorithm itself is not rocket science. All we are doing is splitting the data-set by selecting certain points that best splits the data-set and minimises the mean square error. <br/>我们选择这些点的方式是通过一个迭代过程来计算所有分割的均方误差，并选择具有最小<em class="mm"> mse </em>值的分割。所以，这很自然。</p><h1 id="2158" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">当有多个自变量时会发生什么？</h1><p id="f21c" class="pw-post-body-paragraph jj jk hi jl b jm lm ij jo jp ln im jr js lo ju jv jw lp jy jz ka lq kc kd ke hb bi translated">让我们考虑有3个变量类似于图2.2中的自变量<strong class="jl hj"> X </strong>。<br/>在每个节点，所有3个变量都将经历与上面例子中<strong class="jl hj"> X </strong>所经历的相同的过程。数据将根据3个变量分别排序。<br/>计算所有3个变量中最小化mse的点。从3个变量和为它们计算的点数中，选择mse最小的一个。</p><h1 id="8629" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">分类变量是如何处理的？</h1><p id="0646" class="pw-post-body-paragraph jj jk hi jl b jm lm ij jo jp ln im jr js lo ju jv jw lp jy jz ka lq kc kd ke hb bi translated">当我们使用连续变量作为独立变量时，我们使用如上所述的迭代算法选择具有最小mse的点。当给定一个分类变量时，我们简单地通过询问一个二元问题来分割它(<em class="mm">通常是</em>)。例如，假设我们有一个以分类术语指定肿瘤大小的列。说<strong class="jl hj">小号，中号</strong>和<strong class="jl hj">大号。</strong></p><p id="4108" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">该树将根据<strong class="jl hj">肿瘤大小=小</strong>或<strong class="jl hj">肿瘤大小=大</strong>或<strong class="jl hj">肿瘤大小=中等</strong>来分割数据集，或者在某些情况下也可以根据哪个问题最大程度地减少<em class="mm"> mse </em>来合并多个值。这成为这个变量<em class="mm">(肿瘤大小)的首要竞争者。</em>将其他变量的前<em class="mm"> </em>竞争者与此进行比较，选择过程类似于<em class="mm">中提到的情况“当有多个自变量时会发生什么？”</em></p><h1 id="768f" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">处理过度拟合以及何时停止构建树？</h1><p id="9409" class="pw-post-body-paragraph jj jk hi jl b jm lm ij jo jp ln im jr js lo ju jv jw lp jy jz ka lq kc kd ke hb bi translated">在阅读之前的博客时，人们可能会理解过度拟合的问题以及它如何影响机器学习模型。回归树容易出现这个问题。</p><p id="f819" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">当我们希望减少均方误差时，决策树可以递归地将数据集分成大量的子集，直到一个集合只包含一行或一条记录。尽管这可能会将mse降低到零，但这显然不是一件好事。</p><p id="f281" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这就是著名的过拟合问题，也是它自己的一个话题。基本要点是，这些模型与现有数据的拟合过于完美，以至于无法用新数据进行概括。我们可以使用交叉验证方法来避免这种情况。</p><p id="588e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">对于回归树，防止这种情况的一种方法是预先指定Aleaf节点可以拥有的最小记录数或行数。当涉及到大型数据集时，准确的数字并不容易知道。但是，交叉验证可以用于此目的。</p><h1 id="921e" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">结论</h1><p id="9624" class="pw-post-body-paragraph jj jk hi jl b jm lm ij jo jp ln im jr js lo ju jv jw lp jy jz ka lq kc kd ke hb bi translated">总之，回归树是调用用于回归的决策树的另一种方式，它在许多变量之间的关系是非线性的领域非常有用。需要记住的一点是，算法容易过拟合。因此，最好总是预先指定每个叶节点的最小子节点数，并使用交叉验证来找到这个值。</p><h1 id="de0a" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">谢谢你</h1></div></div>    
</body>
</html>