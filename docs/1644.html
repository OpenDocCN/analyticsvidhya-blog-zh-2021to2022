<html>
<head>
<title>Encoders-Decoders, Sequence to Sequence Architecture.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">编码器-解码器，序列到序列架构。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/encoders-decoders-sequence-to-sequence-architecture-5644efbb3392?source=collection_archive---------0-----------------------#2021-03-11">https://medium.com/analytics-vidhya/encoders-decoders-sequence-to-sequence-architecture-5644efbb3392?source=collection_archive---------0-----------------------#2021-03-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9e4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理解深度学习中的编码器-解码器、序列到序列架构。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/34548865e05c0093ab859d9d9ace7638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JWfrWj5R_qAYKVffpQKPwg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">从一种语言翻译成另一种语言。</figcaption></figure><p id="2056" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在深度学习中，很多复杂的问题可以通过构建更好的神经网络架构来解决。RNN(递归神经网络)及其变体在序列对序列学习中非常有用。RNN变体LSTM(长短期记忆)是seq-seq学习任务中使用最多的细胞。</p><blockquote class="ju jv jw"><p id="b0a7" class="if ig jx ih b ii ij ik il im in io ip jy ir is it jz iv iw ix ka iz ja jb jc hb bi translated">递归神经网络的编码器-解码器架构是标准的神经<strong class="ih hj">机器翻译方法</strong>，它可以与经典的统计机器翻译方法相媲美，在某些情况下甚至更胜一筹。</p></blockquote><p id="66bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种架构非常新，仅在2014年被开发出来，尽管它已经被用作<a class="ae jt" href="https://translate.google.com/" rel="noopener ugc nofollow" target="_blank">谷歌翻译服务</a>的核心技术。</p><h1 id="e3b1" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">编码器-解码器模型</h1><p id="2ed1" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">编码器-解码器模型中有三个主要模块，</p><ul class=""><li id="0699" class="le lf hi ih b ii ij im in iq lg iu lh iy li jc lj lk ll lm bi translated">编码器</li><li id="bfd7" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">隐藏向量</li><li id="80f4" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">解码器</li></ul><p id="0854" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">编码器会将输入序列转换成一维向量(隐藏向量)。解码器将把隐藏向量转换成输出序列。</p><blockquote class="ju jv jw"><p id="cee4" class="if ig jx ih b ii ij ik il im in io ip jy ir is it jz iv iw ix ka iz ja jb jc hb bi translated">编码器-解码器模型被联合训练以在给定输入序列的情况下最大化目标序列的条件概率。</p></blockquote><h1 id="ae5c" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">序列对序列模型如何工作？</h1><p id="f954" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">为了充分理解模型的基本逻辑，我们将浏览下图:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ls"><img src="../Images/41e66f534fb4802ceac025c50125becd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R8p3Xj4GHpfJy_p3MPqAVg.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">编码器-解码器序列到序列模型</figcaption></figure><h1 id="6a27" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">编码器</h1><ul class=""><li id="88fc" class="le lf hi ih b ii kz im la iq lt iu lu iy lv jc lj lk ll lm bi translated">多个RNN单元可以堆叠在一起形成编码器。RNN按顺序读取每个输入</li><li id="94e5" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">对于每个时间步长(每个输入)t，隐藏状态(隐藏向量)h根据该时间步长X[i]处的输入进行更新。</li><li id="439c" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">在编码器模型读取所有输入之后，模型的最终隐藏状态表示整个输入序列的上下文/摘要。</li><li id="4633" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">示例:考虑要编码的输入序列“我是学生”。编码器模型总共有4个时间步长(4个令牌)。在每个时间步长，隐藏状态h将使用先前的隐藏状态和当前输入来更新。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/70e7502e61b9a908e19df4fc6dc7d188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dCSSWPzbK4sD4lSYfEoGhQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">示例:编码器</figcaption></figure><ul class=""><li id="7041" class="le lf hi ih b ii ij im in iq lg iu lh iy li jc lj lk ll lm bi translated">在第一时间步t1，先前的隐藏状态h0将被认为是零或随机选择的。所以第一个RNN单元将用第一个输入和h0更新当前隐藏状态。每一层输出两件事—更新的隐藏状态和每个阶段的输出。每一级的输出都被拒绝，只有隐藏状态将被传播到下一层。</li><li id="bfd9" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">隐藏状态<em class="jx"> h_i </em>使用以下公式计算:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lx"><img src="../Images/dd26e5bf9a8fca1ad6466901f1d3fa19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KiPH5x4WHWpZbn_tSdesWg.png"/></div></div></figure><ul class=""><li id="8ec5" class="le lf hi ih b ii ij im in iq lg iu lh iy li jc lj lk ll lm bi translated">在第二时间步t2，隐藏状态h1和第二输入X[2]将作为输入给出，并且隐藏状态h2将根据这两个输入更新。那么隐藏状态h1将用新的输入更新，并将产生隐藏状态h2。在所举的例子中，所有四个阶段都会发生这种情况。</li><li id="41e2" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">几个循环单元(LSTM单元或GRU单元，性能更好)的堆栈，其中每个单元接受输入序列的单个元素，收集该元素的信息，并将其向前传播。</li><li id="cf53" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">在问答问题中，输入序列是问题中所有单词的集合。每个单词被表示为<em class="jx"> x_i </em>，其中<em class="jx"> i </em>是该单词的顺序。</li></ul><blockquote class="ju jv jw"><p id="b6b0" class="if ig jx ih b ii ij ik il im in io ip jy ir is it jz iv iw ix ka iz ja jb jc hb bi translated">这个简单的公式代表了普通递归神经网络的结果。正如你所看到的，我们只是将适当的权重应用到先前隐藏的状态<em class="hi"> h_(t-1) </em>和输入向量<em class="hi"> x_t. </em></p></blockquote><h1 id="e222" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">编码器向量</h1><ul class=""><li id="3ff6" class="le lf hi ih b ii kz im la iq lt iu lu iy lv jc lj lk ll lm bi translated">这是从模型的编码器部分产生的最终隐藏状态。它是用上面的公式计算的。</li><li id="5ab6" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">该向量旨在封装所有输入元素的信息，以帮助解码器做出准确的预测。</li><li id="ec6f" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">它充当模型解码器部分的初始隐藏状态。</li></ul><h1 id="1a06" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">解码器</h1><ul class=""><li id="ae74" class="le lf hi ih b ii kz im la iq lt iu lu iy lv jc lj lk ll lm bi translated">给定隐藏状态ht，解码器通过预测下一个输出Yt来产生输出序列。</li><li id="5263" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">解码器的输入是在编码器模型结束时获得的最终隐藏向量。</li><li id="a9af" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">每一层将有三个输入，来自前一层的隐藏向量ht-1和前一层输出yt-1，原始隐藏向量h。</li><li id="d77d" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">在第一层，编码器的输出向量和随机符号开始，空隐藏状态ht-1将作为输入给出，获得的输出将是y1和更新的隐藏状态h1(输出的信息将从隐藏向量中减去)。</li><li id="ebb8" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">第二层将更新的隐藏状态h1和先前的输出y1以及原始隐藏向量h作为当前输入，产生隐藏向量h2和输出y2。</li><li id="3ac9" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">解码器每个时间步长的输出是实际输出。模型将预测输出，直到结束符号出现。</li><li id="c1c7" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">几个循环单元的堆栈，每个循环单元在一个时间步<em class="jx"> t </em>预测一个输出<em class="jx"> y_t </em>。</li><li id="ecb6" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">每个递归单元接受前一个单元的隐藏状态，并产生一个输出和它自己的隐藏状态。</li><li id="8099" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">在问答问题中，输出序列是答案中所有单词的集合。每个单词表示为<em class="jx"> y_i </em>，其中<em class="jx"> i </em>是该单词的顺序。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ly"><img src="../Images/be09251af1392b38b34616bf8be896c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jIXPXCh-xuFPUyAtkxdS3A.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">例如:解码器。</figcaption></figure><ul class=""><li id="3665" class="le lf hi ih b ii ij im in iq lg iu lh iy li jc lj lk ll lm bi translated">任何隐藏状态<em class="jx"> h_i </em>都是使用公式<em class="jx"> </em>计算的:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lz"><img src="../Images/eb9c5a43e6cc75474e62eef79982ea7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*969LQQeUB97d3WYt1I3bVQ.png"/></div></div></figure><p id="e896" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如你所见，我们只是使用前一个隐藏状态来计算下一个。</p><h1 id="ae38" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">输出层</h1><ul class=""><li id="b68e" class="le lf hi ih b ii kz im la iq lt iu lu iy lv jc lj lk ll lm bi translated">我们在输出层使用Softmax激活函数。</li><li id="25ce" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">它用于从具有高概率目标类的值向量中生成概率分布。</li><li id="23d2" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">使用以下公式计算时间步长<em class="jx"> t </em>的输出<em class="jx"> y_t </em>:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ma"><img src="../Images/1389335f437c805636478251f27874ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ukwDodB7Jws7mY46FTzXIQ.png"/></div></div></figure><p id="0aff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用当前时间步长的隐藏状态以及相应的权重W(S)来计算输出。Softmax用于创建一个概率向量，该向量将帮助我们确定最终输出(例如问答问题中的word)。</p><p id="52b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个模型的强大之处在于它可以将不同长度的序列相互映射<strong class="ih hj">。</strong>如您所见，输入和输出不相关，它们的长度可能不同。这开启了一系列全新的问题，现在可以使用这种架构来解决。</p><h1 id="8cce" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">应用程序</h1><p id="fcd5" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">它拥有许多应用，例如</p><ul class=""><li id="9c1e" class="le lf hi ih b ii ij im in iq lg iu lh iy li jc lj lk ll lm bi translated">谷歌的机器翻译</li><li id="9709" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">问答聊天机器人</li><li id="5b30" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">语音识别</li><li id="c4a2" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">时间序列应用等。,</li></ul><h1 id="d92c" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">序列对序列模型的用例</h1><p id="6691" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">您每天面对的众多系统背后都有一个序列对序列模型。例如，seq2seq模型支持谷歌翻译、语音设备和在线聊天机器人等应用。一般来说，这些应用程序包括:</p><ul class=""><li id="5513" class="le lf hi ih b ii ij im in iq lg iu lh iy li jc lj lk ll lm bi translated"><em class="jx">机器翻译</em> —谷歌2016年的一篇<a class="ae jt" href="https://arxiv.org/pdf/1409.3215.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>展示了seq2seq模型的翻译质量如何“接近或超过目前所有公布的结果”。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mb"><img src="../Images/28114fa528684aa9184f5a754a638627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SPgApM7BwFgusifokzn7oA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">谷歌翻译截图。</figcaption></figure><h1 id="808c" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">摘要</h1><p id="47ed" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">在这篇文章中，你发现了神经机器翻译的编码器-解码器模型。编码器和解码器模型如何工作</p><p id="5368" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你有什么问题吗？<br/>在下面的评论里提出你的问题，我会尽力回答。</p><h1 id="c00f" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">进一步阅读</h1><p id="b6d7" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">如果您想更深入地了解这个主题，本节提供了更多的资源。</p><ul class=""><li id="2975" class="le lf hi ih b ii ij im in iq lg iu lh iy li jc lj lk ll lm bi translated"><a class="ae jt" href="https://arxiv.org/abs/1609.08144" rel="noopener ugc nofollow" target="_blank">谷歌的神经机器翻译系统:弥合人类与机器翻译的鸿沟</a>，2016。</li><li id="aac7" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated"><a class="ae jt" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">用神经网络进行序列对序列学习</a>，2014。</li><li id="a8c2" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated"><a class="ae jt" href="https://www.youtube.com/watch?v=-uyXE7dY5H0" rel="noopener ugc nofollow" target="_blank">用神经网络进行序列到序列学习的演示</a>，2016。</li><li id="18c5" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated"><a class="ae jt" href="http://www.cs.toronto.edu/~ilya/" rel="noopener ugc nofollow" target="_blank">伊利亚·苏茨科夫主页</a></li><li id="c0f8" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated"><a class="ae jt" href="https://arxiv.org/abs/1406.1078" rel="noopener ugc nofollow" target="_blank">使用统计机器翻译的RNN编码器-解码器学习短语表示</a>，2014年。</li><li id="c118" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated"><a class="ae jt" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">联合学习对齐和翻译的神经机器翻译</a>，2014。</li><li id="1348" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated"><a class="ae jt" href="https://arxiv.org/abs/1409.1259" rel="noopener ugc nofollow" target="_blank">关于神经机器翻译的性质:编码器-解码器方法</a>，2014。</li><li id="235d" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated"><a class="ae jt" href="http://www.kyunghyuncho.me/" rel="noopener ugc nofollow" target="_blank">京畿道町主页</a></li><li id="e6bd" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">GPU神经机器翻译入门(<a class="ae jt" href="https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/" rel="noopener ugc nofollow" target="_blank"> part1 </a>、<a class="ae jt" href="https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/" rel="noopener ugc nofollow" target="_blank"> part2 </a>、<a class="ae jt" href="https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/" rel="noopener ugc nofollow" target="_blank"> part3 </a>)，2015。</li></ul></div></div>    
</body>
</html>