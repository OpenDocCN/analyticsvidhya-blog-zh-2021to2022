<html>
<head>
<title>MULTI LAYER PERCEPTRON explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多层感知器解释</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multi-layer-perceptron-explained-318df92b3efb?source=collection_archive---------4-----------------------#2021-04-27">https://medium.com/analytics-vidhya/multi-layer-perceptron-explained-318df92b3efb?source=collection_archive---------4-----------------------#2021-04-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7646" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以我从今天开始我的博客之旅。对于我的第一篇文章，我将解释一个简单但非常重要的概念来研究深度学习<br/>，这是多层感知器。</p><h2 id="1eae" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">数据集</h2><p id="36c0" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">在这篇博客中，我们将使用来自sklearn的<strong class="ih hj"> make_moons </strong>数据集。<br/> 选择这个数据集是因为它不能用直线分开。<br/>执行以下脚本来生成我们将要使用的数据集，以便训练和测试我们的神经网络。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/a1f12f9d9acba2aeccf05dbcbdb7d974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3qk3ndT2tRXIFPj7-nTENA.png"/></div></div></figure><p id="4057" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的脚本中，我们从<code class="du kp kq kr ks b">sklearn</code>库中导入了<code class="du kp kq kr ks b">datasets</code>类。为了创建100个数据点的非线性数据集，我们使用了<code class="du kp kq kr ks b">make_moons</code>方法，并将100作为第一个参数传递给它。该方法返回一个数据集，该数据集在绘制时包含两个交错的半圆，如下图所示:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kt"><img src="../Images/6a0858251c08defe68d7b89458163ea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/0*J_M5kp4nALvrxYaQ.png"/></div></figure><p id="b671" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以清楚地看到，这些数据不能用一条直线分开，因此<strong class="ih hj">感知器</strong>不能用来正确地分类这些数据。</p><p id="56aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们来验证一下这个概念。为此，我们将使用一个具有一个输入层和一个输出层的简单<strong class="ih hj">感知器</strong>,因为这种结构基本上是一个具有输入和输出的逻辑回归函数，并且我们知道我们不能仅通过逻辑回归来对这些数据进行分类(我的意思是我们不能，但是误差不会从某个数字开始下降)</p><h1 id="a77a" class="ku je hi bd jf kv kw kx jj ky kz la jn lb lc ld jq le lf lg jt lh li lj jw lk bi translated">单隐层神经网络</h1><p id="7d29" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">在本节中，我们将创建一个具有一个输入层、一个隐藏层和一个输出层的神经网络(我更喜欢称之为<strong class="ih hj">单层感知器</strong>)。我们的神经网络的架构将会是这样的</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ll"><img src="../Images/d41c25d11589902dcbfd8067b665151f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mhzehd9X3n84PK9a.png"/></div></div></figure><p id="6175" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，我们有一个具有两个输入、一个隐藏层和一个输出层的神经网络。隐藏层有4个节点。输出层有1个节点，因为我们正在解决一个二元分类问题，其中可能只有两个可能的输出。这种神经网络结构能够发现非线性边界。</p><p id="b1fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">神经网络的基本原理是3个步骤</p><ol class=""><li id="78f4" class="lm ln hi ih b ii ij im in iq lo iu lp iy lq jc lr ls lt lu bi translated">正向输送</li><li id="1617" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated">计算损失</li><li id="b414" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated">反向传播</li><li id="4377" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated">改变重量</li></ol><p id="f7a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我们今天在博客中要做的四件事。</p><h1 id="821e" class="ku je hi bd jf kv kw kx jj ky kz la jn lb lc ld jq le lf lg jt lh li lj jw lk bi translated"><strong class="ak">前馈</strong></h1><p id="28c0" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">对于每一行，我们都有特征(<strong class="ih hj"> x1 </strong>和<strong class="ih hj"> x2 </strong>)。为了计算隐藏层中每个节点的值，我们必须将输入乘以我们正在计算值的节点的相应权重。然后，我们将点积传递给一个激活函数，以获得最终值。最初权重是随机选择的。因为输入(<strong class="ih hj"> 2个节点</strong>)连接到隐藏层<br/>中的<strong class="ih hj"> 4个节点</strong>，所以我们对于层1的权重矩阵将是形状(2，8)，因为每个输入_节点连接到<strong class="ih hj">隐藏层的4个节点。</strong></p><blockquote class="ma mb mc"><p id="e23c" class="if ig md ih b ii ij ik il im in io ip me ir is it mf iv iw ix mg iz ja jb jc hb bi translated">weights _ layer 1 = NP . random . rand(features . shape[1]，features.shape[1]*4) #卫星数据的权重形状(2，8)</p></blockquote><p id="b6ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，要计算隐藏层中第一个节点的最终值，用“ah1”表示，您需要执行以下计算:</p><blockquote class="ma mb mc"><p id="f97b" class="if ig md ih b ii ij ik il im in io ip me ir is it mf iv iw ix mg iz ja jb jc hb bi translated">zh1=x1w1+x2w2 <strong class="ih hj"> →等式a</strong><br/>ah1 = 1/(1+NP . exp(-zh1))→<strong class="ih hj">等式b </strong></p></blockquote><p id="fc34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是隐藏层中最顶层节点的结果值。同样，您可以计算隐藏层的第2、第3和第4个节点的值。</p><p id="0625" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，为了计算输出图层的值，隐藏图层节点中的值被视为输入。因此，为了计算输出，将隐藏层节点的值与它们相应的权重相乘，并通过<strong class="ih hj">激活</strong>函数传递结果。</p><p id="edf3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个操作可以用下面的等式来数学表达:</p><blockquote class="ma mb mc"><p id="38c0" class="if ig md ih b ii ij ik il im in io ip me ir is it mf iv iw ix mg iz ja jb jc hb bi translated">zo = ah1 * w9+ah2 * w10+ah3 * w11+ah4 * w12→<strong class="ih hj">方程c</strong><br/>ao = 1/(1+NP . exp(-zo))→<strong class="ih hj">方程d </strong></p></blockquote><h1 id="2b6c" class="ku je hi bd jf kv kw kx jj ky kz la jn lb lc ld jq le lf lg jt lh li lj jw lk bi translated"><strong class="ak">反向传播</strong></h1><p id="819d" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">在<strong class="ih hj">反向传播</strong>阶段，我们将首先定义我们的损失函数。我们将使用<a class="ae mh" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方差</a>成本函数。它在数学上可以表示为:<br/>这是我们的<strong class="ih hj">成本函数</strong></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mi"><img src="../Images/845e0f68ae74c7ba5a0d6f43dc831762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*hCPgfyUrFh1fZq5dchuZvw.png"/></div></figure><h2 id="bfb1" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">这里的<strong class="ak"> n </strong>是观察次数</h2><h2 id="74ad" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">第一相</h2><p id="7245" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">在反向传播的第一阶段，我们需要更新输出层的权重，即w9、w10、w11和w12。所以暂时就考虑我们的神经网络有以下部分:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mj"><img src="../Images/af517fd9926f6cbda0471fd88a67c94e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/0*3knzM8BBFvGtMpV_.png"/></div></figure><p id="0ff0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">反向传播的第一阶段的目的是以最小化最终误差的方式更新权重w9、w10、w11和w12。这是一个<a class="ae mh" href="https://en.wikipedia.org/wiki/Optimization_problem" rel="noopener ugc nofollow" target="_blank">优化问题</a>，我们必须为我们的成本函数找到<a class="ae mh" href="https://en.wikipedia.org/wiki/Maxima_and_minima" rel="noopener ugc nofollow" target="_blank">函数最小值</a>。</p><p id="8ca2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了找到函数的最小值，我们可以使用<a class="ae mh" href="https://en.wikipedia.org/wiki/Optimization_problem" rel="noopener ugc nofollow" target="_blank">梯度下降</a>算法。梯度下降算法可以数学表示如下:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mk"><img src="../Images/3633e9c5ba94264cc535a9539079618a.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*jyrriSy4aZ3PczIwf38Acg.png"/></div></figure><p id="f03f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的神经网络中，预测输出用“ao”表示。这意味着我们必须基本上最小化这个函数:</p><blockquote class="ma mb mc"><p id="e965" class="if ig md ih b ii ij ik il im in io ip me ir is it mf iv iw ix mg iz ja jb jc hb bi translated"><strong class="ih hj">成本=((1/2)*(NP . power((ao-labels)，2))) </strong></p></blockquote><ol class=""><li id="81c1" class="lm ln hi ih b ii ij im in iq lo iu lp iy lq jc lr ls lt lu bi translated"><strong class="ih hj">标签</strong>为<strong class="ih hj">真值</strong></li><li id="aef5" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated"><strong class="ih hj"> ao </strong>是<strong class="ih hj">的预测值</strong></li></ol><p id="01d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们必须更新重量值，以便降低成本。为此，我们需要对每个权重的成本函数求导。由于在此阶段，我们要处理输出层的权重，因此我们需要区分关于w9、w10、w11和w2的成本函数。</p><p id="c7f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">利用链式法则计算成本函数的微分。</p><h2 id="13cb" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">为我们的博客</h2><p id="010c" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">我们写<br/> <strong class="ih hj"> dao/dwo = dao_dwo(非常重要)</strong></p><p id="0497" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dcost _ dwo = dcost _ Dao * Dao _ dzo * dzo _ dwo→<strong class="ih hj">等式1 </strong></p><p id="f6ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里“wo”指的是输出层中的权重。每个术语开头的字母“d”表示导数。</p><p id="74d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">dcost _ Dao</strong>= 2 *(T19)ao—<strong class="ih hj">标签</strong> )/ <strong class="ih hj">观察总数</strong></p><p id="a977" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的<strong class="ih hj"> 2 </strong>和<strong class="ih hj">总观察次数</strong>是常数，所以我们可以忽略它们。<br/>所以<strong class="ih hj">dcost _ Dao =(ao-labels)→等式2 </strong></p><p id="7851" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在ao wrt zo(sigmoid的导数是等式3 ) <br/>所以<strong class="ih hj">Dao _ dzo</strong>=<strong class="ih hj">sigmoid(zo)*(1-sigmoid(zo)→等式3 </strong></p><p id="dcc2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们需要找到关于“dwo”的“dzo”。导数就是来自隐藏层的输入，如下所示:</p><p id="ebec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dzo_dwo = ah → <strong class="ih hj">等式4 </strong></p><p id="1365" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的“ah”指的是隐藏层的4个输入。<em class="md">等式1 </em>可用于找到输出层权重的更新权重值。为了找到新的权重值，由<em class="md">等式1 </em>返回的值可以简单地乘以学习率并从当前权重值中减去。这很简单，我们以前也这样做过。</p><p id="7c7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第二阶段</strong></p><p id="c7b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上一节中，我们了解了如何找到输出图层权重的更新值，即w9、w10、w11和12。在本节中，我们将误差反向传播到前一层，并找到隐藏层权重的新权重值，即权重w1至w8。</p><p id="f076" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将隐藏层权重表示为“wh”。我们基本上必须区分关于“wh”的成本函数。数学上我们可以用链式微分法则来表示它为:</p><p id="c9fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">dcost _ dwh = dcost _ dah * dah _ dzh * dzh _ dwh→等式7 </strong></p><p id="4d78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将再次把方程式<strong class="ih hj"> 5 </strong>分解成几个部分</p><p id="ac8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一项“dcost”可以使用微分链规则相对于“dah”进行微分，如下所示:</p><p id="2d99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dcost _ dah = dcost _ dzo * dzo _ dah→<strong class="ih hj">方程7.1 </strong></p><p id="679a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们再次将<em class="md">等式7.1 </em>分解成单独的项。再次使用链式法则，我们可以将“dcost”与“dzo”区分如下:</p><p id="c377" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dcost _ dzo = dcost _ Dao * Dao _ dzo→<strong class="ih hj">等式7.2 </strong></p><p id="b307" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经计算了<strong class="ih hj"> dcost/dao </strong>在<em class="md">等式5 </em>中的值和<strong class="ih hj"> dao/dzo </strong>在<em class="md">等式6 </em>中的值。</p><p id="2e29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们需要从<strong class="ih hj"> <em class="md">方程a </em> </strong>中找到<strong class="ih hj"> dzo/dah </strong>。如果我们看一下<strong class="ih hj"> zo </strong>，它有以下值:</p><blockquote class="ma mb mc"><p id="2377" class="if ig md ih b ii ij ik il im in io ip me ir is it mf iv iw ix mg iz ja jb jc hb bi translated">zo = ah1 * w9+ah2 * w10+ah3 * w11+ah4 * w12</p></blockquote><p id="8aa5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们相对于来自隐藏层的所有输入来区分它，用“ao”表示，那么我们剩下来自输出层的所有权重，用“wo”表示。因此</p><p id="3fae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dzo_dah = wo → <strong class="ih hj">方程7.3 </strong></p><p id="4d8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们可以通过替换下式中的值来找到dcost/dah的值</p><p id="24f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="md">方程式7.2 </em> </strong>和<strong class="ih hj"> <em class="md">中的<strong class="ih hj"> 7.3 </strong>方程式7.1 </em> </strong></p><p id="bfce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回到<em class="md">等式2 </em>，我们还没有找到dah_dzh和dzh_dwh。</p><p id="fcc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dah _ dzh = sigmoid(zh)*(1-sigmoid(zh))→<strong class="ih hj">方程7.4 </strong></p><p id="5219" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dzh_dwh =输入特征→ <strong class="ih hj">方程7.5 </strong></p><p id="856d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们替换等式7中的7.4和7.5，7.1，我们可以得到隐藏层权重的更新矩阵。为了找到隐藏层权重“wh”的新权重值，由<em class="md">等式7 </em>返回的值可以简单地乘以学习率，并从当前权重值中减去。差不多就是这样。有很多计算，但是是的，你学到了一些非常漂亮的东西</p><p id="fbb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代码的链接将是<br/><a class="ae mh" href="https://github.com/sahibpreetsingh12/100daysofmlcode/blob/main/D8-SLP/D8-SLP.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/sahibpreetsingh 12/100 daysofmlcode/blob/main/D8-SLP/D8-SLP . ipynb</a></p><p id="674a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回购环节是<a class="ae mh" href="https://github.com/sahibpreetsingh12/100daysofmlcode" rel="noopener ugc nofollow" target="_blank">https://github.com/sahibpreetsingh12/100daysofmlcode</a></p><p id="843c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望你喜欢:)</p></div></div>    
</body>
</html>