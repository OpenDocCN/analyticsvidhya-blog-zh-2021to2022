<html>
<head>
<title>Create a Tokenizer and Train a Huggingface RoBERTa Model from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">创建一个分词器，从头开始训练一个Huggingface RoBERTa模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/create-a-tokenizer-and-train-a-huggingface-roberta-model-from-scratch-f3ed1138180c?source=collection_archive---------1-----------------------#2021-08-16">https://medium.com/analytics-vidhya/create-a-tokenizer-and-train-a-huggingface-roberta-model-from-scratch-f3ed1138180c?source=collection_archive---------1-----------------------#2021-08-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/429c47837c3ff0a6d7a50843781067cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M5pCg5RLzy1t6gEYVbnPRg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">凯利·西克玛在<a class="ae iu" href="https://unsplash.com/s/photos/learn-scratch?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="4ef1" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">简介</h1><p id="5da4" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">这篇博文是我们希望使用transformer模型创建产品名称生成器的系列文章的第一部分。有几个星期，我在Huggingface中研究不同的模型和替代方案，以训练一个文本生成模型。我们有一个产品及其描述的候选名单，我们的目标是获得产品的名称。我用Tensorflow中的Transformer模型和T5摘要器做了一些实验。最后，为了深化Huggingface transformers的使用，我决定用一种更复杂的方法来解决这个问题，一种编码器-解码器模型。也许这不是最好的选择，但我想学习拥抱变形金刚的新知识。在本系列的下一篇文章中，我们将向您更深入地介绍这个概念。</p><p id="a88e" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在第一部分中，我们将展示如何从头开始训练一个标记器，以及如何使用屏蔽语言建模技术来创建一个RoBERTa模型。这个个性化模型将成为我们未来编码器-解码器模型的基础模型。</p><h1 id="4bad" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">我们自己的解决方案</h1><p id="9b33" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在我们的实验中，我们将从头开始训练一个RoBERTa模型，它将成为未来模型的编码器和解码器。但是我们的领域是非常具体的，关于衣服、形状、颜色的单词和概念……因此，我们感兴趣的是从我们的特定词汇中定义我们自己的标记器，避免包括来自其他领域或用例的与我们的最终目的无关的更常见的单词。</p><p id="08fb" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><em class="kw">我们可以用三个主要步骤来描述我们的培训阶段</em>:</p><ul class=""><li id="319a" class="kx ky hi jv b jw kr ka ks ke kz ki la km lb kq lc ld le lf bi translated">使用与RoBERTa相同的特殊标记创建并训练一个字节级的字节对编码标记器</li><li id="efad" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated">使用MLM<strong class="jv hj">屏蔽语言建模</strong>从头开始训练一个<strong class="jv hj">罗伯塔模型</strong>。</li></ul><p id="00b2" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">代码可以在这个<a class="ae iu" href="https://github.com/edumunozsala/RoBERTa_Encoder_Decoder_Product_Names/blob/03c0456f03d8cff62e2d1b04f03029130694e18b/RoBERTa%20MLM%20and%20Tokenizer%20train%20for%20Text%20generation.ipynb" rel="noopener ugc nofollow" target="_blank"> Github库</a>中找到。在这篇文章中，我们将只向你展示主要的代码部分和一些解释。</p><h1 id="1654" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">数据集</h1><p id="fbcc" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">正如我们之前提到的，我们的数据集包含大约31，000个商品，涉及一家重要零售商的服装，包括一个长的产品描述和一个短的产品名称，这是我们的目标变量。首先，我们执行一个探索性的数据分析，我们可以观察到具有异常值的行的数量很少。单词数看起来像一个左偏分布，75%的行在50-60个单词的范围内，最多约125个单词。目标变量包含大约3到6个单词。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/f5d5d7944880f7d9efd52c34b8c51b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OuPA-37-qI-O0zMLTCiHRQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@popnzebra?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">波普&amp;斑马</a>在<a class="ae iu" href="https://unsplash.com/s/photos/token?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">挡泥板</a>上拍摄</figcaption></figure></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h1 id="7dda" class="iv iw hi bd ix iy lx ja jb jc ly je jf jg lz ji jj jk ma jm jn jo mb jq jr js bi translated">训练分词器</h1><p id="849b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">斯坦福NLP小组将标记化定义为:</p><blockquote class="mc md me"><p id="cfdf" class="jt ju kw jv b jw kr jy jz ka ks kc kd mf kt kg kh mg ku kk kl mh kv ko kp kq hb bi translated">给定一个字符序列和一个已定义的文档单元，标记化就是将它分割成若干部分，称为<em class="hi">标记</em>，也许同时丢弃某些字符，如标点符号</p></blockquote><p id="2fe4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">一个<em class="kw">记号分解器将一串字符，通常是文本的句子，分解成记号</em>，记号的整数表示，通常通过寻找空白(制表符、空格、换行符)。它通常将一个句子拆分成单词，但也有很多像子词这样的选项。</p><p id="937d" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">"<em class="kw">我们将使用一个</em> <strong class="jv hj"> <em class="kw">字节级字节对编码标记器</em> </strong> <em class="kw">，字节对编码(BPE)是一种简单的数据压缩形式，其中最常见的一对连续数据字节被替换为一个不在该数据中出现的字节</em>，"<a class="ae iu" href="https://en.wikipedia.org/wiki/Byte_pair_encoding" rel="noopener ugc nofollow" target="_blank">字节对编码</a>"，维基百科定义。这种方法的好处是，它将从单个字符的字母表开始构建词汇，因此所有的单词都可以分解成记号。我们可以避免未知(UNK)令牌的出现。</p><p id="8102" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在Huggingface文档<a class="ae iu" href="https://huggingface.co/transformers/tokenizer_summary.html" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/transformers/tokenizer_summary.html</a>中可以找到对记号赋予器的很好的解释。</p><p id="4a41" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">为了训练一个分词器，我们需要将数据集保存在一堆文本文件中。我们为每个描述值创建一个纯文本文件。</p><figure class="lm ln lo lp fd ij"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="83bb" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在，我们可以在创建的文本文件上训练我们的标记器并包含我们的词汇表，我们需要指定词汇表的大小、要包含的标记的最小频率以及特殊标记。我们选择的vocab大小为8，192，最小频率为2(您可以根据自己的最大词汇量来调整这个值)。<strong class="jv hj">特殊标志</strong>取决于型号，对于RoBERTa，我们提供了一个候选名单:</p><ul class=""><li id="95af" class="kx ky hi jv b jw kr ka ks ke kz ki la km lb kq lc ld le lf bi translated"><s>或BOS，句首</s></li><li id="ec3d" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated">或EOS，句子结束</li><li id="7690" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated"><pad>填充令牌</pad></li><li id="0e6b" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated"><unk>未知令牌</unk></li><li id="c7a6" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated"><mask>掩蔽令牌。</mask></li></ul><p id="fdc6" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">样本数量很少，标记器训练非常快。现在我们可以将标记器保存到磁盘，稍后我们将使用它来训练语言模型。</p><figure class="lm ln lo lp fd ij"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="6e12" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们现在有了一个<code class="du mk ml mm mn b">vocab.json</code>，它是一个按频率排列的最频繁的标记列表，用于将标记转换成id，还有一个<code class="du mk ml mm mn b">merges.txt</code>文件，用于将文本映射成标记。</p><pre class="lm ln lo lp fd mo mn mp mq aw mr bi"><span id="4037" class="ms iw hi mn b fi mt mu l mv mw"># vocab.json<br/>{<br/>    "&lt;s&gt;": 0,<br/>    "&lt;pad&gt;": 1,<br/>    "&lt;/s&gt;": 2,<br/>    "&lt;unk&gt;": 3,<br/>    "&lt;mask&gt;": 4,<br/>    "!": 5,<br/>    "\"": 6,<br/>    "#": 7,<br/>    "$": 8,<br/>    "%": 9,<br/>    "&amp;": 10,<br/>    "'": 11,<br/>    "(": 12,<br/>    ")": 13,<br/>    # ...<br/>}<br/> <br/># merges.txt<br/>l a<br/>Ġ k<br/>o n<br/>Ġ la<br/>t a<br/>Ġ e<br/>Ġ d<br/>Ġ p<br/># ...</span></pre><p id="4fa4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">最棒的是，我们的标记器针对我们非常特殊的词汇进行了优化，而不是针对普通英语训练的通用标记器。</p><p id="9ddb" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们可以用这两个文件实例化我们的记号化器，并用数据集中的一些文本测试它。</p><figure class="lm ln lo lp fd ij"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="ed9f" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">稍后，当训练模型时，我们将使用<code class="du mk ml mm mn b">from_pretrained</code>方法初始化记号赋予器。</p></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/ffd883349f80168bd4ddf04fd7a683dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zsq_z59LBK6YNKCq8glqaA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">由<a class="ae iu" href="https://unsplash.com/@ninjason?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Jason Leung </a>在<a class="ae iu" href="https://unsplash.com/s/photos/language?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h1 id="d520" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">从头开始训练语言模型</h1><p id="9f80" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我们将训练一个<strong class="jv hj"> RoBERTa模型</strong>，它类似于BERT，有一些变化(查看<a class="ae iu" href="https://huggingface.co/transformers/model_doc/roberta.html" rel="noopener ugc nofollow" target="_blank">文档</a>了解更多细节)。简而言之:<em class="kw">“它建立在BERT的基础上，并修改了关键的超参数，删除了下一句话的预训练目标，并以更大的小批量和学习率进行训练”，</em>hugging face documentation on RoBERTa<em class="kw">。</em></p><p id="be6e" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">由于该模型类似于BERT，我们将在<strong class="jv hj"> <em class="kw">掩蔽语言建模</em> </strong>的任务上训练它。它涉及屏蔽部分输入，大约10–20%的标记，然后学习一个模型来预测丢失的标记。MLM经常在预训练任务中使用，以<strong class="jv hj">给模型从未标记数据中学习文本模式的机会</strong>。它可以针对特定的下游任务进行微调。主要的好处是我们不需要带标签的数据(很难获得)，为了预测丢失的值，不需要人工标注文本。</p><p id="a391" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们将从头开始训练这个模型，而不是从一个预先训练好的模型开始。我们为RoBERTa模型创建一个模型配置，设置主要参数:</p><ul class=""><li id="6bca" class="kx ky hi jv b jw kr ka ks ke kz ki la km lb kq lc ld le lf bi translated">词汇量</li><li id="a4e6" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated">注意头</li><li id="772e" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated">隐藏层</li></ul><p id="f2b1" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">最后，<em class="kw">让我们使用配置文件</em>初始化我们的模型。由于我们是从零开始训练，我们从定义模型架构的配置开始初始化，但不恢复先前训练的权重。权重将被随机初始化。</p><figure class="lm ln lo lp fd ij"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="a309" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们使用上一步中训练和保存的记号赋予器重新创建记号赋予器。我们将使用一个<code class="du mk ml mm mn b">RoBERTaTokenizerFast</code>对象和<code class="du mk ml mm mn b">from_pretrained</code>方法来初始化我们的记号赋予器。</p><figure class="lm ln lo lp fd ij"><div class="bz dy l di"><div class="mi mj l"/></div></figure><h2 id="aeea" class="ms iw hi bd ix my mz na jb nb nc nd jf ke ne nf jj ki ng nh jn km ni nj jr nk bi translated">构建训练数据集</h2><p id="a6f1" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我们将构建一个Pytorch数据集，子类化dataset类。<code class="du mk ml mm mn b">CustomDataset</code>接收带有<code class="du mk ml mm mn b">description</code>变量值的Pandas系列和对这些值进行编码的标记器。数据集为系列中的每个产品说明返回一个令牌列表。</p><p id="4960" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">为了在训练期间评估模型，我们将生成训练数据集和评估数据集。</p><figure class="lm ln lo lp fd ij"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="9cde" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">一旦我们有了数据集，一个<strong class="jv hj">数据整理器将帮助我们屏蔽我们的训练文本</strong>。这只是一个小助手，它将帮助我们将数据集的不同样本一起批处理到PyTorch知道如何对其执行反向投影的对象中。数据排序规则是通过使用数据集元素列表作为输入来形成一个批处理的对象，并且可以应用一些处理，如填充或随机屏蔽。<code class="du mk ml mm mn b">DataCollatorForLanguageModeling</code>方法允许我们设置在输入中随机屏蔽标记的概率。</p><figure class="lm ln lo lp fd ij"><div class="bz dy l di"><div class="mi mj l"/></div></figure></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h1 id="6850" class="iv iw hi bd ix iy lx ja jb jc ly je jf jg lz ji jj jk ma jm jn jo mb jq jr js bi translated">初始化并培训我们的培训师</h1><p id="1029" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">当我们想要训练一个transformer模型时，基本的方法是创建一个Trainer类，它提供一个用于功能完整训练的API，并且包含基本的训练循环。首先，我们定义培训参数，它们有很多，但更相关的是:</p><ul class=""><li id="6c2d" class="kx ky hi jv b jw kr ka ks ke kz ki la km lb kq lc ld le lf bi translated"><code class="du mk ml mm mn b">output_dir</code>保存模型工件的位置</li><li id="df6d" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated"><code class="du mk ml mm mn b">evaluation_strategy</code>何时计算验证损失</li><li id="9cf2" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated"><code class="du mk ml mm mn b">num_train_epochs</code></li><li id="f330" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated"><code class="du mk ml mm mn b">per_device_train_batch_size</code>是训练的批量</li><li id="6b60" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated"><code class="du mk ml mm mn b">per_device_eval_batch_size</code>是评估的批量</li><li id="8c29" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated"><code class="du mk ml mm mn b">learning_rate</code>，初始化为1e-4</li><li id="5f89" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated"><code class="du mk ml mm mn b">weight_decay</code>，0.01</li></ul><p id="aaf2" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">最后，我们使用参数、输入数据集、评估数据集和定义的数据排序器创建一个<code class="du mk ml mm mn b">Trainer</code>对象。现在我们准备训练我们的模型。</p><figure class="lm ln lo lp fd ij"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="9ad3" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">因此，我们可以在训练时观察损失是如何减少的。</p><p id="a8f0" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">最后，我们保存模型和记号赋予器，以便它们可以被恢复用于未来的下游任务，我们的编码器-解码器模型。</p><h1 id="6341" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">使用管道检查已训练的模型</h1><p id="fcfd" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">看着训练和评估损失下降是不够的，我们想应用我们的模型来检查我们的语言模型是否正在学习任何有趣的东西。一个简单的方法是通过<code class="du mk ml mm mn b">FillMaskPipeline</code>。</p><p id="5598" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">管道是标记化器和模型的简单包装器。<strong class="jv hj">我们可以使用“填充-屏蔽”管道</strong>，在这里我们输入一个包含屏蔽标记(<code class="du mk ml mm mn b">&lt;mask&gt;</code>)的序列，它返回一个最可能的填充序列列表，以及它们的概率。但只对只有一个标记被屏蔽的输入有效(见<a class="ae iu" href="https://huggingface.co/transformers/main_classes/pipelines.html" rel="noopener ugc nofollow" target="_blank">拥抱脸官方文档</a>)。</p><figure class="lm ln lo lp fd ij"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="16ca" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这就是第一部分的全部内容，我希望你已经为你未来的项目找到了灵感，我希望几天后我们会带着第二部分回来。</p><p id="3ce0" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">代码可以在这个<a class="ae iu" href="https://github.com/edumunozsala/RoBERTa_Encoder_Decoder_Product_Names/blob/03c0456f03d8cff62e2d1b04f03029130694e18b/RoBERTa%20MLM%20and%20Tokenizer%20train%20for%20Text%20generation.ipynb" rel="noopener ugc nofollow" target="_blank"> Github库</a>中找到。</p><h1 id="dbaa" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">参考</h1><p id="a2f7" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">2020年2月，“如何使用变形器和标记器从零开始训练一个新的语言模型”，Huggingface博客。</p><p id="11ed" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">“<a class="ae iu" href="https://huggingface.co/transformers/model_doc/encoderdecoder.html" rel="noopener ugc nofollow" target="_blank">编码器-解码器型号</a>”，Huggingface官方文档</p><p id="d5a9" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">来自Huggingface的<a class="ae iu" href="https://huggingface.co/transformers/model_doc/roberta.html" rel="noopener ugc nofollow" target="_blank"> RoBERTa文档</a></p><p id="126b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Byte_pair_encoding" rel="noopener ugc nofollow" target="_blank">字节对编码</a>，维基百科定义。</p></div></div>    
</body>
</html>