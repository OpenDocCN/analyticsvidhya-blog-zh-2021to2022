<html>
<head>
<title>Building Machine Learning Models to Detect Scams in Email</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">建立机器学习模型来检测电子邮件中的诈骗</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-machine-learning-models-to-detect-scams-in-email-41ef3bd86bdb?source=collection_archive---------4-----------------------#2021-05-08">https://medium.com/analytics-vidhya/building-machine-learning-models-to-detect-scams-in-email-41ef3bd86bdb?source=collection_archive---------4-----------------------#2021-05-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/827af699cd5c4a54950a3d6812575cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6LGa3oBFtEbwLsnZ-G3YjQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">这是我们将要构建的随机森林算法中的一棵决策树。决策树对电子邮件是否是骗局进行分类。如你所见，决策树会变得非常复杂。</figcaption></figure><p id="a866" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">你曾经想学习如何检测电子邮件中的骗局吗？在这篇文章中，我深入研究了如何构建不同的机器学习模型来实现这一点。本文要求具备Python和Pandas的基础知识，以便能够构建模型。我们将使用Sci-kit Learn的机器学习库来构建算法，以检测该电子邮件是否是一个骗局。我们将使用UCI的<a class="ae jr" href="https://archive.ics.uci.edu/ml/datasets/spambase" rel="noopener ugc nofollow" target="_blank"> Scambase数据集</a>，它包括一个特征列表和一个分类，如果它是一个骗局(1)或不是(0)。你可以通过Scambase网站看到数据和数据定义。让我们首先探索数据以了解更多信息。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="67ef" class="ki kj hh ke b fi kk kl l km kn"># importing some packages</span><span id="476f" class="ki kj hh ke b fi ko kl l km kn">import pandas as pd</span><span id="9f65" class="ki kj hh ke b fi ko kl l km kn">import matplotlib.pyplot as plt</span><span id="5fca" class="ki kj hh ke b fi ko kl l km kn">import seaborn as sns</span><span id="3e33" class="ki kj hh ke b fi ko kl l km kn">from sklearn.metrics import accuracy_score</span></pre><p id="a279" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">好的，亲爱的。我们来看看数据框。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="dac3" class="ki kj hh ke b fi kk kl l km kn">df = pd.read_csv('spambase.csv')</span><span id="53c6" class="ki kj hh ke b fi ko kl l km kn">print(df)</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kp"><img src="../Images/886a238f293da69fa1b2c53b588e2842.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ql08Jr7VKUouDFDtXn0v1w.png"/></div></div></figure><p id="5e2c" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">接下来，看看有多少诈骗邮件(1)对非诈骗邮件(0)。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="6691" class="ki kj hh ke b fi kk kl l km kn"># finding how many spam vs non-spam emails there are</span><span id="e924" class="ki kj hh ke b fi ko kl l km kn">sns.countplot(x = 'class', data = df)</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div class="er es kq"><img src="../Images/34e09c4c29b7dcc2d20b1a790783da1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*3eyeTKkuSvrZmvhpmmQjbQ.png"/></div></figure><p id="70f9" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">如你所见，非诈骗邮件比诈骗邮件多。我们必须在以后对我们的数据集进行分层，以确保一个好的结果。接下来，让我们看一些关于数据集的统计数据。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="8db1" class="ki kj hh ke b fi kk kl l km kn">print(df.describe())<br/>print(df.info())</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kr"><img src="../Images/970c33241dba691d4323420c182c9277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n1RSpPH7qB55k1UAp9a87g.png"/></div></div></figure><figure class="jz ka kb kc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ks"><img src="../Images/66eb64359ab946a3d0a63d9b15085ac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DOs-BTXqpQfAO5ZlHxrZqg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">注:共有4601个条目和51列。</figcaption></figure><p id="aa0f" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">好极了。我们准备开始使用机器学习。让我们建立我们的训练和测试集。出于再现性目的，我将继续将“random_set”设置为42。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="7a7f" class="ki kj hh ke b fi kk kl l km kn">X = df.iloc[:,1:]</span><span id="0179" class="ki kj hh ke b fi ko kl l km kn">y = df.iloc[:,0]</span><span id="e343" class="ki kj hh ke b fi ko kl l km kn"># partition data into training and testing sets</span><span id="7674" class="ki kj hh ke b fi ko kl l km kn">X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,stratify=y,random_state=42)</span></pre><p id="bda8" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在让我们来计算基线预测。这仅仅是基于最频繁发生的预测。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="fb1c" class="ki kj hh ke b fi kk kl l km kn"># computing baseline accuracy</span><span id="0c7f" class="ki kj hh ke b fi ko kl l km kn">from sklearn.dummy import DummyClassifier</span><span id="e438" class="ki kj hh ke b fi ko kl l km kn">dummy_classifier = DummyClassifier(strategy='most_frequent')</span><span id="9f10" class="ki kj hh ke b fi ko kl l km kn">dummy_classifier.fit(X_train,y_train)</span><span id="2adb" class="ki kj hh ke b fi ko kl l km kn">baseline_acc = dummy_classifier.score(X_test,y_test)</span><span id="d32a" class="ki kj hh ke b fi ko kl l km kn">### For verifying answer:</span><span id="d529" class="ki kj hh ke b fi ko kl l km kn">print("Baseline Accuracy = ", baseline_acc)</span><span id="e19f" class="ki kj hh ke b fi ko kl l km kn"><strong class="ke hi">Baseline Accuracy =  0.6060825488776249</strong></span></pre><p id="d3bd" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">先试试什么机器学习方法？</strong>让我们试试自助抽样方法。这是一个随机的替换样本。这意味着我们实际上可以对同一个样本进行两次采样。我们可以试试Skikit Learn的“装袋分类器”。这种方法根据数据的随机子集拟合基本分类器，然后聚合它们各自的预测。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="14fd" class="ki kj hh ke b fi kk kl l km kn">from sklearn.ensemble import BaggingClassifier<br/>from sklearn.metrics import accuracy_score</span><span id="c1b1" class="ki kj hh ke b fi ko kl l km kn"><br/>baggingclassifer = BaggingClassifier(random_state= 42)<br/>baggingclassifer.fit(X_train,y_train)predict_bagging_score = baggingclassifer.predict(X_test)</span><span id="6db6" class="ki kj hh ke b fi ko kl l km kn">accuracy_score_baggingclassifer = accuracy_score(y_test,predict_bagging_score)</span><span id="bf20" class="ki kj hh ke b fi ko kl l km kn">print(accuracy_score_baggingclassifer)</span><span id="24e3" class="ki kj hh ke b fi ko kl l km kn"><strong class="ke hi">0.9312092686459088</strong></span></pre><p id="b137" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">哇，对于我们的第一个模型来说，这已经很不错了！<strong class="iv hi">接下来，让我们试试随机森林分类。</strong>这也是另一种套袋方式。这个方法很棒，因为算法中的树是<em class="kt">去相关的</em>。通过在算法中去相关树，我们减少了方差。这是因为集成方法中的高相关树可能具有权重非常大的特征，从而导致低偏差和高方差。在随机决策森林中，每棵树都会吐出一个预测，得票最多的类成为输出。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="b1d5" class="ki kj hh ke b fi kk kl l km kn">from sklearn.ensemble import RandomForestClassifier</span><span id="a3a6" class="ki kj hh ke b fi ko kl l km kn">randomForestClassifer = RandomForestClassifier(n_estimators=10,max_features=10, random_state= 42)</span><span id="2e8a" class="ki kj hh ke b fi ko kl l km kn">randomForestClassifer.fit(X_train,y_train)</span><span id="40da" class="ki kj hh ke b fi ko kl l km kn">predict_randomforest_score = randomForestClassifer.predict(X_test)</span><span id="a7e1" class="ki kj hh ke b fi ko kl l km kn">accuracy_score_randomforest = accuracy_score(y_test,predict_randomforest_score)</span><span id="e6a9" class="ki kj hh ke b fi ko kl l km kn">print(accuracy_score_randomforest)</span><span id="656c" class="ki kj hh ke b fi ko kl l km kn"><strong class="ke hi">0.939174511223751</strong></span></pre><p id="b38a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">如果我们决定增加树的数量，减少特征的数量会怎么样？</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="9f9c" class="ki kj hh ke b fi kk kl l km kn">randomForestClassifer = RandomForestClassifier(n_estimators=100,max_features =5, random_state= 42)</span><span id="8141" class="ki kj hh ke b fi ko kl l km kn">randomForestClassifer.fit(X_train,y_train)</span><span id="2a8d" class="ki kj hh ke b fi ko kl l km kn">predict_randomforest_score = randomForestClassifer.predict(X_test)</span><span id="2a95" class="ki kj hh ke b fi ko kl l km kn">accuracy_score_randomforest = accuracy_score(y_test,predict_randomforest_score)</span><span id="f78f" class="ki kj hh ke b fi ko kl l km kn">print(accuracy_score_randomforest)</span><span id="37d4" class="ki kj hh ke b fi ko kl l km kn"><strong class="ke hi">0.9543808834178131</strong></span></pre><p id="6f2a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">让我们看看我们的随机森林决策树的报告。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="9297" class="ki kj hh ke b fi kk kl l km kn">from sklearn.metrics import classification_report, confusion_matrix, accuracy_score</span><span id="f7f1" class="ki kj hh ke b fi ko kl l km kn">print(confusion_matrix(y_test,predict_randomforest_score))</span><span id="24d3" class="ki kj hh ke b fi ko kl l km kn">print(classification_report(y_test,predict_randomforest_score))</span><span id="926b" class="ki kj hh ke b fi ko kl l km kn">print(accuracy_score(y_test, predict_randomforest_score))</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ku"><img src="../Images/ca8225fce237697dacade9cc3f4bca71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lhtjobDZVryWszKGYwcHnA.png"/></div></div></figure><p id="2bb3" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">让我们想象一下随机森林中的一棵决策树是什么样子的。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="aa46" class="ki kj hh ke b fi kk kl l km kn">from sklearn import tree</span><span id="f0f6" class="ki kj hh ke b fi ko kl l km kn">from matplotlib import pyplot as plt</span><span id="e788" class="ki kj hh ke b fi ko kl l km kn">plt.figure(figsize=(20,20))</span><span id="79d4" class="ki kj hh ke b fi ko kl l km kn"># plotting the second Decision Tree</span><span id="80a5" class="ki kj hh ke b fi ko kl l km kn">_ = tree.plot_tree(randomForestClassifer.estimators_[1], feature_names=X.columns, filled=True)</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/b30be14a6e052f3c487628dbe2992385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BLEILhllNMZHLpDgfnaUaA.png"/></div></div></figure><p id="051c" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">接下来，让我们看看随机森林分类器的特征重要性。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="a947" class="ki kj hh ke b fi kk kl l km kn">feature_importances = randomForestClassifer.feature_importances_</span><span id="af0a" class="ki kj hh ke b fi ko kl l km kn">features = X_train.columns</span><span id="2667" class="ki kj hh ke b fi ko kl l km kn">df1 = pd.DataFrame({'features': features, 'importance': feature_importances}).nlargest</span><span id="1ca4" class="ki kj hh ke b fi ko kl l km kn">print(df1)</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kv"><img src="../Images/35d9d2ab18df34e9795085a6ec695c2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_jTQ6eSfORNdzKTTp4GzNw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">注意:此屏幕截图没有显示所有功能。#1预测因子是要素#51，即char_freq_%21。</figcaption></figure><p id="4173" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">接下来让我们尝试一些助推方法。</strong> Boosting仍然用替换进行采样；然而，仍然有一些根本的不同。Boosting允许我们选择在后续包中建模不佳的数据实例，并根据这个错误对它们进行加权。我们先来试试Ada boost算法。该算法在开始时对所有数据点具有相等的权重。当Adaboost通过每个包时，它将增加错误分类观察的权重。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="874d" class="ki kj hh ke b fi kk kl l km kn">from sklearn.ensemble import AdaBoostClassifier</span><span id="eaee" class="ki kj hh ke b fi ko kl l km kn">from sklearn.tree import DecisionTreeClassifier</span><span id="b8c5" class="ki kj hh ke b fi ko kl l km kn">base_est = DecisionTreeClassifier (max_depth =2)</span><span id="3ee1" class="ki kj hh ke b fi ko kl l km kn">ada_boost = AdaBoostClassifier(base_est, n_estimators=500, random_state=42, learning_rate=.05)</span><span id="81e9" class="ki kj hh ke b fi ko kl l km kn">ada_boost.fit(X_train, y_train)</span><span id="7e03" class="ki kj hh ke b fi ko kl l km kn">predict_adaboost_score = ada_boost.predict(X_test)</span><span id="be29" class="ki kj hh ke b fi ko kl l km kn">accuracy_score_adaboost = accuracy_score(y_test,predict_adaboost_score)</span><span id="5261" class="ki kj hh ke b fi ko kl l km kn">print(accuracy_score_adaboost)</span><span id="ab5a" class="ki kj hh ke b fi ko kl l km kn"><strong class="ke hi">0.9551049963794352</strong></span></pre><p id="e65c" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">让我们试试渐变助推树(GBT)。</strong>这也是一种bagging方法，试图修复前面模型的错误。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="5723" class="ki kj hh ke b fi kk kl l km kn">from sklearn.ensemble import GradientBoostingClassifier</span><span id="d3c7" class="ki kj hh ke b fi ko kl l km kn">gradientBoostingClassifier = GradientBoostingClassifier()<br/>gradientBoostingClassifier.fit(X_train,y_train)</span><span id="c97f" class="ki kj hh ke b fi ko kl l km kn">predict_gradientBoostingClassifier = gradientBoostingClassifier.predict(X_test)</span><span id="15fb" class="ki kj hh ke b fi ko kl l km kn">accuracy_score_gradientBoostingClassifier = accuracy_score(y_test,predict_gradientBoostingClassifier)</span><span id="ea31" class="ki kj hh ke b fi ko kl l km kn">print(accuracy_score_gradientBoostingClassifier)</span><span id="a51e" class="ki kj hh ke b fi ko kl l km kn"><strong class="ke hi">0.946415640839971</strong></span></pre><p id="8883" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">最后，我们来看看XGBoost。</strong>这是另一个很棒的boosting算法，因为它的出色表现而广为人知。它允许简单的交叉验证和规范化。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="d752" class="ki kj hh ke b fi kk kl l km kn">import xgboost as xgb</span><span id="706c" class="ki kj hh ke b fi ko kl l km kn"># create an optimized structure for xgb<br/>data_dmatrix = xgb.DMatrix(data=X,label=y)</span><span id="3c02" class="ki kj hh ke b fi ko kl l km kn">xg_reg = xgb.XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 100)</span><span id="4f39" class="ki kj hh ke b fi ko kl l km kn">xg_reg.fit(X_train,y_train)<br/>xgb_predict = xg_reg.predict(X_test)</span><span id="80c7" class="ki kj hh ke b fi ko kl l km kn">xgb_accuracy_score = accuracy_score(y_test,xgb_predict)</span><span id="4702" class="ki kj hh ke b fi ko kl l km kn">print(xgb_accuracy_score)</span><span id="0b23" class="ki kj hh ke b fi ko kl l km kn"><strong class="ke hi">0.9507603186097031</strong></span></pre><p id="c5af" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">太好了。现在我们可以使用XGBoost进行k重交叉验证。这导致偏差较小的结果。我们可以通过将数据集分成k个组，细分一个测试数据组，并将剩余的组用于训练来实现这一点。让我们建立五重交叉验证。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="5ebd" class="ki kj hh ke b fi kk kl l km kn">params = {"objective":"reg:linear",'colsample_bytree': 0.3,'learning_rate': 0.1,'max_depth': 5, 'alpha': 10}</span><span id="66db" class="ki kj hh ke b fi ko kl l km kn">cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=5,num_boost_round=50,early_stopping_rounds=10,metrics="rmse", as_pandas=True, seed=42)</span><span id="6954" class="ki kj hh ke b fi ko kl l km kn">print(cv_results.head())</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kw"><img src="../Images/0a96256be09d0bba68e2a40fe22ebcf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xnDArgdOrXdfl6XQdO_jQg.png"/></div></div></figure><p id="cd11" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在总结一下:</p><figure class="jz ka kb kc fd ii er es paragraph-image"><div class="er es kx"><img src="../Images/d096ad8e4d2e1a28506117096e7d1adf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*R9UpONipABMaSoSw5NlXdQ.png"/></div></figure><p id="0b5a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">如您所见，第二个随机森林分类XGBoost和AdaBoost分类器非常接近，准确率为95%。我们发现，将电子邮件分类为垃圾邮件的第一预测指标是“char_freq_%21”。</p><p id="e94d" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">想了解更多代码吗？这里是</strong> <a class="ae jr" href="https://github.com/drewm8080/Detecting-Scam-Emails-Using-Machine-Learning-" rel="noopener ugc nofollow" target="_blank"> <strong class="iv hi"> Github库</strong> </a> <strong class="iv hi">。</strong></p><p id="8542" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">请关注我的</strong><a class="ae jr" href="http://www.linkedin.com/in/andrewlmoore2020" rel="noopener ugc nofollow" target="_blank"><strong class="iv hi">Linkedin</strong></a><strong class="iv hi">。</strong></p></div></div>    
</body>
</html>