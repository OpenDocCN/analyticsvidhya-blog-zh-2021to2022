<html>
<head>
<title>Understanding difference between Regularization methods Ridge, Lasso, and ElasticNet in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解Python中正则化方法Ridge、Lasso和ElasticNet之间的差异</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-difference-between-regularization-methods-ridge-lasso-and-elasticnet-in-python-996185296ed2?source=collection_archive---------4-----------------------#2021-02-14">https://medium.com/analytics-vidhya/understanding-difference-between-regularization-methods-ridge-lasso-and-elasticnet-in-python-996185296ed2?source=collection_archive---------4-----------------------#2021-02-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/25361a898747d3d91f3337bf6838067b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZfPDur0W_2BBI-DDwCoZTg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Haneul Kim摄</figcaption></figure><p id="a38f" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">今天，我们将讨论如何正则化线性回归，即在训练时使我们的模型更普遍地适应过度拟合。</p><p id="cce9" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">线性回归是一种简单而强大的方法，因为它在训练后为我们提供了快速的预测时间，这是开发机器学习模型时要考虑的最重要的特征之一，因为在现实世界中，有客户在等待预测，他们等待的时间越长，客户体验就会下降。</p><p id="cb94" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">当线性回归不适合时，除了增加模型的复杂性(假设您无法添加更多数据)之外，没有其他方法可以采用多项式回归(三次、二次等)或使用其他复杂模型来获取线性回归因其简单性而无法获取的数据。</p><p id="017f" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">当线性回归过度拟合，列数(独立变量)接近观察数时，有两种方法可以减轻</p><ol class=""><li id="fb95" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated">添加更多观察</li><li id="e14b" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">正规化</li></ol><blockquote class="kf kg kh"><p id="3115" class="it iu ki iv b iw ix iy iz ja jb jc jd kj jf jg jh kk jj jk jl kl jn jo jp jq ha bi translated">U <!-- -->与多项式回归等已经很复杂的模型不同，我们可以通过降低模型复杂性(降低多项式的次数)来避免过度拟合，线性模型没有更简单的方法，因此它需要正则化。</p></blockquote><p id="2b0d" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">由于添加更多的观察值是耗时的，并且经常不被提供，我们将使用正则化技术来减轻过拟合。有多种正则化技术，它们都有相同的概念，即<strong class="iv hi">在独立变量(除了θ_ 0)的权重</strong>上添加约束，但是它们在约束方式上有所不同。我们将介绍三种最流行的正则化技术:</p><ul class=""><li id="619b" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq km jx jy jz bi translated">岭回归(L2)</li><li id="4229" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq km jx jy jz bi translated">拉索回归(L1)</li><li id="2daf" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq km jx jy jz bi translated">弹性网回归(L1和L2的混合)</li></ul></div><div class="ab cl kn ko go kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ha hb hc hd he"><p id="1276" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我们使用来自Kaggle 的<a class="ae ku" href="https://www.kaggle.com/anthonypino/melbourne-housing-market" rel="noopener ugc nofollow" target="_blank">墨尔本房屋数据集尝试不同的正则化。</a></p><p id="e9d9" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">下载数据集后，让我们导入数据、依赖项并做一点清理。</p><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="kz la l"/></div></figure><p id="c62f" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在让我们使用线性回归作为我们的基础模型，注意我们的X变量包含的列比观察值多。</p><p id="62c4" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">正如我们所料，训练集的性能很高，但测试集的性能很差，这表明过度拟合。</p><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="kz la l"/></div></figure><p id="e369" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我们将看到正则化技术如何解决这个问题→缩小两者之间的差距并降低两者的性能分数。</p><p id="19d1" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">首先，让我们看看<strong class="iv hi">岭回归</strong>也称为<strong class="iv hi">吉洪诺夫正则化</strong>。它所做的只是简单地将正则化项添加到成本函数中，使模型权重尽可能小，您希望保持多小的权重取决于超参数<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code> <strong class="iv hi"> </strong> (alpha)</p><ul class=""><li id="1773" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq km jx jy jz bi translated"><code class="du lb lc ld le b"><strong class="iv hi">α</strong></code> = 0，对权重没有限制这只是线性回归</li><li id="0de7" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq km jx jy jz bi translated"><code class="du lb lc ld le b"><strong class="iv hi">α</strong></code> &gt; 0，权重变得接近于零。</li></ul><p id="378c" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">请注意，过高的<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code>可能会导致拟合不足，因此您需要始终牢记“偏差和方差权衡”的概念。</p><figure class="kv kw kx ky fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/052fb5b2073d5097b852a71281ce52aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*L6ywF4Lxo_epDTBrPlDKJA.png"/></div></figure><p id="b4af" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">下面是使用岭回归的代码。</p><p id="372d" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在尝试不同的<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code>值后，我们使用产生最佳性能的<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code>值来训练岭回归。<a class="ae ku" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.idxmin.html" rel="noopener ugc nofollow" target="_blank"> pd。Series.idxmin() </a>输出序列中最小值的指数，即具有最小RMSE(我们使用的成本函数，成本函数可以是MSE或任何其他)得分的指数。</p><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="kz la l"/></div></figure><p id="7b8f" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我们可以看到测试性能下降，即使RMSE很高，训练和测试之间的差距也减小了。我们可以说，岭回归能够比以前的线性模型更好地概括，但是我们的模型仍然在预测方面做得很差，因此我们可以考虑使用不同的模型或进行更多的特征清洗和工程设计。</p></div><div class="ab cl kn ko go kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ha hb hc hd he"><p id="81a2" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi"> L </strong>东<strong class="iv hi">A</strong>b<strong class="iv hi">S</strong>h<strong class="iv hi">S</strong>选举<strong class="iv hi"> O </strong>操作员(<strong class="iv hi">套索</strong> ) <strong class="iv hi">回归</strong>，又称“L1”正规化。岭和套索正则化项之间的唯一区别在于，在套索回归中，它将权重向量的L1范数添加到成本函数中，从而允许套索回归消除最不重要的特征，即，它执行自动特征选择。超参数<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code>的行为方式相同，因此<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code> =0是线性回归。</p><figure class="kv kw kx ky fd ii er es paragraph-image"><div class="er es lg"><img src="../Images/e738be4018e5c9e9a1ca51f77d18d809.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*UHssG3zRxhds89jA7vtxWQ.png"/></div></figure><p id="cf8b" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">Lasso也在sklearn中作为Lasso提供，只需将前面代码块中的Ridge()替换为Lasso()(没有解算器参数)，我们将设置为使用Lasso回归。</p><p id="478d" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我们不像上面那样将Ridge改为Lasso，而是运行完全相同的代码，我们将看看在Ridge和Lasso回归中，独立变量的权重如何随着<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code>的增加而变化，以了解它们的差异。</p><p id="9917" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我们将使用多个<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code>值运行Lasso和Ridge，看看随着<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code>的增加，特征的权重会发生什么变化。</p><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="kz la l"/></div></figure><p id="0571" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">上面的代码创建了两个数据帧:</p><figure class="kv kw kx ky fd ii er es paragraph-image"><div class="er es lh"><img src="../Images/18585a6f6c8a000eed237e685549ad09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*NDGmsd2FonlzBTxP8RODiA.png"/></div></figure><p id="51e1" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">对于每个<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code>列，其行表示每个特征的权重，例如第一行表示在<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code> =0.05时，房间特征的权重是278369，表示它是一个信息特征，因为它相对于其他特征较高。</p><p id="ebe3" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">为了看出套索和山脊之间的区别，让我们随着<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code>的增加来数一下<strong class="iv hi">零重量</strong>的数量。概述:套索减少了不重要的功能，使其权重为0，而山脊减少了权重，但从未达到0。</p><p id="bd87" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">事实上，随着<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code>的增加，Lasso将越来越多的特征权重减少到零，然而对于山脊，即使有很高的<code class="du lb lc ld le b"><strong class="iv hi">α</strong></code>，零的数量保持不变(权重将减少，但从未达到零)。</p><figure class="kv kw kx ky fd ii er es paragraph-image"><div class="er es li"><img src="../Images/63334a302a53f15ef0dc3b854e1b4398.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*vkEIdyRZum-dFIGq7dS3gA.png"/></div></figure><p id="0da5" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">因此，山脊是一个很好的默认，如果你已经从EDA中知道大多数功能是不重要的，那么套索应该是首选，因为它会自动摆脱无用的功能。</p></div><div class="ab cl kn ko go kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ha hb hc hd he"><p id="0077" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">最后<strong class="iv hi">弹性网</strong>具有正则项，该正则项是脊和套索正则项的简单混合，并且它们的比例由混合比例<code class="du lb lc ld le b">r</code>控制。添加到成本函数的弹性网络正则化项如下所示。</p><figure class="kv kw kx ky fd ii er es paragraph-image"><div class="er es lj"><img src="../Images/c636e8cb1ef8847024236f7ba80f1d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*Qsj6vGvksvrJ1cvynmydbA.png"/></div></figure><p id="596b" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">你可以看到什么时候</p><ul class=""><li id="4644" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq km jx jy jz bi translated">r = 0，则套索正则项变为0。弹性网=脊</li><li id="9765" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq km jx jy jz bi translated">r =1，岭正则项= 0。弹性网=套索</li></ul><p id="9b98" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">由Sklearn作为ElasticNet()提供，它包含参数<code class="du lb lc ld le b">l1_ratio</code>，即混合比例<code class="du lb lc ld le b">r</code>。只需用多了一个参数<code class="du lb lc ld le b">l1_ratio</code>的ElasticNet()替换Ridge()即可使用。</p></div><div class="ab cl kn ko go kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ha hb hc hd he"><p id="a4f9" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">总之，我们已经介绍了不同类型的正则化技术来减少过度拟合。像这样对基础有坚实的掌握是非常重要的，因为这将是深度学习的基础，对这些基础有清晰的理解将在学习更复杂的主题时节省你很多时间。请注意，这些不是唯一的正则化技术，其他一些常见的技术包括早期停止(当验证误差达到最小值时停止训练)和退出(在深度学习中)。</p><p id="13c9" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">希望你喜欢我的博客，如果有任何错误信息，请纠正我，谢谢！:)</p></div><div class="ab cl kn ko go kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ha hb hc hd he"><p id="a1ab" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">参考资料:</p><ul class=""><li id="846d" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq km jx jy jz bi translated"><a class="ae ku" href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646" rel="noopener ugc nofollow" target="_blank">使用Scikit-Learn进行机器实践学习&amp; Tensorflow Ch.4 </a></li><li id="b83e" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq km jx jy jz bi translated"><a class="ae ku" href="https://www.kaggle.com/anthonypino/melbourne-housing-market" rel="noopener ugc nofollow" target="_blank">墨尔本住房数据集</a></li><li id="7c3b" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq km jx jy jz bi translated"><a class="ae ku" href="https://datascience.stackexchange.com/questions/80868/overfitting-in-linear-regression" rel="noopener ugc nofollow" target="_blank">线性回归如何过拟合？</a></li><li id="dc89" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq km jx jy jz bi translated"><a class="ae ku" href="https://www.youtube.com/watch?v=VqKq78PVO9g&amp;ab_channel=codebasics" rel="noopener ugc nofollow" target="_blank"> L1，L2 python教程视频</a></li><li id="c20f" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq km jx jy jz bi translated"><a class="ae ku" href="https://www.kaggle.com/apapiu/regularized-linear-models" rel="noopener ugc nofollow" target="_blank">正则化线性模型——ka ggle笔记本</a></li><li id="5354" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq km jx jy jz bi translated"><a class="ae ku" href="https://stats.stackexchange.com/questions/151304/why-is-ridge-regression-called-ridge-why-is-it-needed-and-what-happens-when" rel="noopener ugc nofollow" target="_blank">岭回归为什么叫“岭”？</a></li><li id="919c" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq km jx jy jz bi translated"><a class="ae ku" href="https://www.youtube.com/watch?v=NGf0voTMlcs&amp;ab_channel=StatQuestwithJoshStarmer" rel="noopener ugc nofollow" target="_blank"> Lasso回归StatsQuest youtube视频</a></li><li id="60f3" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq km jx jy jz bi translated"><a class="ae ku" href="https://www.youtube.com/watch?v=9lRv01HDU0s" rel="noopener ugc nofollow" target="_blank">克里斯·纳伊克的《套索与山脊》youtube深度视频</a></li></ul></div></div>    
</body>
</html>