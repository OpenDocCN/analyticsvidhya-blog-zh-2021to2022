<html>
<head>
<title>Word Embedding : Text Analysis : NLP : Part-1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入:文本分析:NLP:第1部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/word-embedding-text-analysis-nlp-part-1-c6c3f161e69f?source=collection_archive---------10-----------------------#2021-02-24">https://medium.com/analytics-vidhya/word-embedding-text-analysis-nlp-part-1-c6c3f161e69f?source=collection_archive---------10-----------------------#2021-02-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/a289bf65aa12ce931e62c76d902171a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uK_JHWwrC_eUobmS.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图片取自<a class="ae it" href="https://www.google.co.in/url?sa=i&amp;url=https%3A%2F%2Fwww.socher.org%2Findex.php%2FMain%2FImprovingWordRepresentationsViaGlobalContextAndMultipleWordPrototypes&amp;psig=AOvVaw3-fLyF-MMpkCZFBinRitGj&amp;ust=1614160655352000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCPD1kJjf_-4CFQAAAAAdAAAAABAm" rel="noopener ugc nofollow" target="_blank">此处</a>。</figcaption></figure><p id="6476" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js">写这篇文章是为了理解不同的单词嵌入技术，这些技术是解决任何自然语言处理问题所必需的。</em></p><h1 id="8621" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">简介</strong></h1><p id="d62d" class="pw-post-body-paragraph iu iv hh iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr ha bi translated">我们都被不同种类的数据所包围，这些数据可能是文本、图像、语音或任何其他格式的。在这里，我们将讨论报纸、电子邮件、书籍甚至任何产品评论形式的文本数据。为了理解这些文本数据，我们必须深入了解它，为此我们需要以容易理解的方式处理数据。如今，自然语言处理(NLP)使得理解这些文本数据和开发各种应用变得更加容易，如语言翻译、文本分类、文本摘要等。</p><p id="faaa" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所有上述NLP技术都需要将文本数据转换成机器可以学习的数字数据。将文本数据转换成数字数据的技术称为单词嵌入。</p><h1 id="c05b" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">文字嵌入</strong></h1><p id="2b82" class="pw-post-body-paragraph iu iv hh iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr ha bi translated">单词嵌入将文本数据转换成数字，这对于学习单词的语义和句法上下文是有用的。同样，任何单词的相似性都可以用这个数字数据来检查。一些重要的嵌入技术有。</p><ol class=""><li id="f2e1" class="kw kx hh iw b ix iy jb jc jf ky jj kz jn la jr lb lc ld le bi translated">一个热编码(BOW)</li><li id="dba7" class="kw kx hh iw b ix lf jb lg jf lh jj li jn lj jr lb lc ld le bi translated">TF-IDF</li><li id="53d8" class="kw kx hh iw b ix lf jb lg jf lh jj li jn lj jr lb lc ld le bi translated">Word2Vec</li><li id="e0a0" class="kw kx hh iw b ix lf jb lg jf lh jj li jn lj jr lb lc ld le bi translated">手套</li><li id="62f6" class="kw kx hh iw b ix lf jb lg jf lh jj li jn lj jr lb lc ld le bi translated">快速文本</li></ol><p id="2ab0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js">本教程将分为两部分，第一部分将介绍一种热编码，第二部分将介绍Word2Vec和GloVe。</em></p><p id="9ca6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">一个热编码(弓形/计数矢量器)</strong></p><p id="f456" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">顾名思义，一种热编码将文本数据转换为0和1表示。它只是简单地计算语料库中的字数，并将数据转换成二进制形式，这就是为什么它也被称为二进制嵌入。文本数据的最终表示将是矩阵，并且它将是数据的向量表示。下面列出了我们可以用来转换数据的步骤。</p><ul class=""><li id="93d7" class="kw kx hh iw b ix iy jb jc jf ky jj kz jn la jr lk lc ld le bi translated">将文本标记成单词</li><li id="f8ca" class="kw kx hh iw b ix lf jb lg jf lh jj li jn lj jr lk lc ld le bi translated">将数据转换为更低</li><li id="3994" class="kw kx hh iw b ix lf jb lg jf lh jj li jn lj jr lk lc ld le bi translated">预处理数据，去除标点符号和停用词</li><li id="03ad" class="kw kx hh iw b ix lf jb lg jf lh jj li jn lj jr lk lc ld le bi translated">使用计数矢量器创建单词的频率分布</li></ul><p id="c61b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">考虑具有Y个文档的语料库X。首先，我们将提取N个唯一的单词，它将形成一个Y×N维的矩阵。</p><p id="18a2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">代码表示</strong></p><p id="8396" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这段代码中，我们将使用sklearn库中的<strong class="iw hi"> <em class="js"> CountVectorizer、</em> </strong> it <strong class="iw hi"> <em class="js"> </em> </strong>对文本文档进行标记，构建word包并转换一个热编码数据。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="c530" class="lu ju hh lq b fi lv lw l lx ly">from sklearn.feature_extraction.text import CountVectorizer<br/>from nltk.tokenize import sent_tokenize<br/>from nltk.tokenize import word_tokenize<br/>import pandas as pd<br/>from nltk.corpus import stopwords<br/>stopwords = set(stopwords.words('english'))<br/>text = ['He is a boy and he is from India.','The boy is Playing The Cricket.','The Cricket is the most popular game in India.']<br/>vectorizer = CountVectorizer(text,stop_words=stopwords)<br/>sentence_vectors = vectorizer.fit_transform(text)<br/>feature_names = vectorizer.get_feature_names()<br/>dense = sentence_vectors.todense()<br/>denselist = dense.tolist()<br/>df = pd.DataFrame(denselist, columns=feature_names)</span></pre><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es lz"><img src="../Images/749fb30edb620b3f41635f2772df4a5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*9si1lV9Szuek2ZyJlbhsKA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">测向输出</figcaption></figure><p id="19ae" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以看到，删除停用词后，唯一单词列表长度为6。所以我们的矩阵输出是3x6。我们还可以看到，在每个raw =句子中，哪些单词出现在唯一列表中，用一个热编码值表示。在第一句中，唯一的单词take是boy和india，因此这些单词以值1显示在first raw中。我们使用单词袋模型从文本中提取特征，将文本转换成文档中单词出现的矩阵。</p><p id="fd4d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">计数矢量化的问题</strong></p><p id="7c13" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里我们可以看到生成的矩阵是稀疏矩阵。此外，它不能迎合语义信息。句子中使用的所有单词的值都是1，因此我们无法从句子中找到有意义的单词。此外，矩阵具有高维数，因此计算量很大。为了解决这个问题，我们将研究另一种叫做TF-IDF的方法</p><h1 id="23fd" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak"> TF-IDF </strong></h1><p id="63c8" class="pw-post-body-paragraph iu iv hh iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr ha bi translated">TF-IDF是词频的缩写——逆文档频率。</p><p id="69d8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">术语频率，定义为术语在文档中出现的次数。该公式如下所示，其计算方式为术语在文档中出现的次数除以文档中的总术语数。</p><blockquote class="ma mb mc"><p id="4cf0" class="iu iv js iw b ix iy iz ja jb jc jd je md jg jh ji me jk jl jm mf jo jp jq jr ha bi translated"><strong class="iw hi"> TF(t) =(术语t在文档中出现的次数)/(文档中的总术语数)</strong></p></blockquote><p id="8b62" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过这种方式，我们可以了解文档中特定术语的重要性。然而，当我们处理一些停用词时，如a，an，the等。被频繁使用，因此实际单词的重要性没有被发现。为了处理这类单词，我们使用内置的python库来删除停用词。然而，我们已经删除了停用词，但如果这个词在句子中频繁使用，我们仍然无法了解它的重要性。为了解决这个问题，我们检查下一个术语逆文档频率。</p><p id="2777" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">逆文档频率用下面的公式计算。其中我们将文档总数除以具有特定单词的文档数，并记录该值的对数。因此，如果频繁使用该词，IDF值将向0移动，否则将向正无穷大移动。</p><blockquote class="ma mb mc"><p id="ad5f" class="iu iv js iw b ix iy iz ja jb jc jd je md jg jh ji me jk jl jm mf jo jp jq jr ha bi translated"><strong class="iw hi"> IDF(t) = log_e(文档总数/其中包含术语t的文档数+1)</strong><em class="hh">(+1在此公式中取值，以避免此公式中分母为0的值)</em></p></blockquote><p id="0d33" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，为了得到TF的字乘法的TF-IDF表示，采用IDF。最后，我们可以针对句子中的重要单词获得一些值。如果这个词在所有的句子中被默认使用，它将被表示为0值。因此，用这种方法我们可以得到一个词在句子中的一些语义信息。</p><p id="e858" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"><em class="js">TF-IDF(t)= TF(t)* IDF(t)</em></strong></p><p id="525d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如，考虑包含100个单词的文档，其中单词<em class="js"> cat </em>出现了3次。那么<em class="js">猫</em>的项频率(即tf)就是(3 / 100) = 0.03。现在，假设我们有1000万个文档，单词<em class="js"> cat </em>出现在其中的1000个文档中。然后，逆文档频率(即idf)计算为log(10，000，000 / 1，000) = 4。因此，Tf-idf重量是这些量的乘积:0.03 * 4 = 0.12。</p><p id="8936" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">代码表示</strong></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="cec3" class="lu ju hh lq b fi lv lw l lx ly">from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer<br/>from nltk.tokenize import sent_tokenize<br/>from nltk.tokenize import word_tokenize<br/>from nltk.corpus import stopwords<br/>stopwords = set(stopwords.words(‘english’))<br/>text = [‘He is a boy and he is from India.’,’The boy is Playing The Cricket.’,’The Cricket is the most popular game in India.’]<br/>vectorizer = TfidfVectorizer(text,stop_words=stopwords)<br/>vectors = vectorizer.fit_transform(text)<br/>feature_names = vectorizer.get_feature_names()<br/>dense = vectors.todense()<br/>denselist = dense.tolist()<br/>df = pd.DataFrame(denselist, columns=feature_names)</span></pre><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mg"><img src="../Images/35c2e5b5c119a59ef516dc489ffd260c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ja59FdQTNIh83h4fWd-haQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">测向输出</figcaption></figure><p id="66b5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以看到普通countvectorizer和TF-IDF的区别。例如，在第二个句子中，playing的值为0.68，这个值在这个句子中较高，这意味着它在这个句子中有一些重要的信息。而在countvectorizer中，playing的值为1，与本句中的其他单词相同，因此我们没有本句的语义信息。</p><p id="80d3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里我们也导入了TfidfTransformer，它与TfidfVectorizer相同，唯一的区别是TfidfTransformer与countvectorizer一起使用。下面是它的代码实现。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="e3b7" class="lu ju hh lq b fi lv lw l lx ly">from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer<br/>from nltk.tokenize import sent_tokenize<br/>from nltk.tokenize import word_tokenize<br/>from nltk.corpus import stopwords<br/>stopwords = set(stopwords.words(‘english’))<br/>text = [‘He is a boy and he is from India.’,’The boy is Playing The Cricket.’,’The Cricket is the most popular game in India.’]<br/>tfIdfTransformer = TfidfTransformer()<br/>countVectorizer = CountVectorizer(text,stop_words=stopwords)<br/>wordCount = countVectorizer.fit_transform(text)<br/>newTfIdf = tfIdfTransformer.fit_transform(wordCount)<br/>feature_names = countVectorizer.get_feature_names()<br/>dense = newTfIdf.todense()<br/>denselist = dense.tolist()<br/>df = pd.DataFrame(denselist, columns=feature_names)</span></pre><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mh"><img src="../Images/959883f39d57046223640f687db70904.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MloZxpu7429PI81RhiOaKg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">与tfidf矢量化结果相同</figcaption></figure><h1 id="143a" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">总结</strong></h1><p id="860c" class="pw-post-body-paragraph iu iv hh iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr ha bi translated">在上面的博客中，我们讨论了单词嵌入的热编码和TFIDF。在接下来的第2部分中，我们将讨论另一种嵌入Word2Vec、GloVe和FastText的方法。希望我在这个博客中涵盖了所有需要的信息。建议是非常必要的，如果有的话。</p><p id="ff0d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">参考文献</strong></p><ol class=""><li id="4a67" class="kw kx hh iw b ix iy jb jc jf ky jj kz jn la jr lb lc ld le bi translated"><a class="ae it" href="http://www.tfidf.com/#:~:text=TF(t)%20%3D%20(Number,how%20important%20a%20term%20is.&amp;text=IDF(t)%20%3D%20log_e(,with%20term%20t%20in%20it)." rel="noopener ugc nofollow" target="_blank">http://www . tfi df . com/#:~:text = TF(t)% 20% 3D % 20(Number，how % 20 important % 20a % 20 term % 20 is。&amp; text=IDF(t)%20%3D%20log_e(，with%20term%20t%20in%20it)。</a></li><li id="198a" class="kw kx hh iw b ix lf jb lg jf lh jj li jn lj jr lb lc ld le bi translated"><a class="ae it" rel="noopener" href="/sfu-cspmp/nlp-word-embedding-techniques-for-text-analysis-ec4e91bb886f">https://medium . com/sfu-cspmp/NLP-word-embedding-techniques-for-text-analysis-ec4e 91 bb 886 f</a></li></ol></div></div>    
</body>
</html>