<html>
<head>
<title>Regularized Linear Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正则化线性模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/regularized-linear-models-d0d8964bf23f?source=collection_archive---------15-----------------------#2021-05-31">https://medium.com/analytics-vidhya/regularized-linear-models-d0d8964bf23f?source=collection_archive---------15-----------------------#2021-05-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie"><p id="3014" class="if ig hh bd ih ii ij ik il im in io dx translated">“也许故事只是有灵魂的数据。”<br/> —布琳·布朗</p></blockquote><figure class="iq ir is it iu iv er es paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="er es ip"><img src="../Images/4962277a38af67f1e86fe023d3ff0ceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L42OHVIY_96Xenr1mhbDRw.jpeg"/></div></div><figcaption class="jc jd et er es je jf bd b be z dx translated">来源:<a class="ae jg" href="https://elearningindustry.com/how-design-thinking-transforming-learning-experience-free-ebook" rel="noopener ugc nofollow" target="_blank">https://elearningindustry . com/how-design-thinking-transforming-learning-experience-free-ebook</a></figcaption></figure><p id="3ff8" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">你好学习者，在这篇文章中我将向你介绍三种正则化线性模型:</p><ol class=""><li id="75a5" class="ke kf hh jj b jk jl jo jp js kg jw kh ka ki io kj kk kl km bi translated">里脊回归</li><li id="a4fc" class="ke kf hh jj b jk kn jo ko js kp jw kq ka kr io kj kk kl km bi translated">套索</li><li id="5a0c" class="ke kf hh jj b jk kn jo ko js kp jw kq ka kr io kj kk kl km bi translated">弹性网</li></ol><p id="e59a" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated"><strong class="jj hi">首先，你所说的正则化术语是什么意思？</strong>这意味着模型在对数据进行训练时存在一些约束。这些约束被称为正则化。它们有助于避免模型过度拟合训练数据。</p><h1 id="55f2" class="ks kt hh bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">里脊回归</h1><blockquote class="lq lr ls"><p id="ae20" class="jh ji lt jj b jk jl jm jn jo jp jq jr lu jt ju jv lv jx jy jz lw kb kc kd io ha bi translated">岭回归也称为吉洪诺夫正则化</p></blockquote><p id="893b" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">岭回归是线性回归的正则化版本:正则化项(等式1)被添加到成本函数中。这迫使学习算法适应数据，并使模型权重尽可能小。</p><figure class="ly lz ma mb fd iv er es paragraph-image"><div class="er es lx"><img src="../Images/ba91d4f1f341a2547c4c21e7cb1ec532.png" data-original-src="https://miro.medium.com/v2/resize:fit:280/format:webp/1*6WOOPqtGyzWqr0ngTyDSpA.png"/></div><figcaption class="jc jd et er es je jf bd b be z dx translated">等式1</figcaption></figure><p id="13f3" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated"><strong class="jj hi">请注意，在评估模型时，我们没有在成本函数中使用正则项。</strong></p><p id="ecc4" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">超参数α控制您想要正则化模型的程度。如果α = 0，那么岭回归就是线性回归。如果α非常大，那么所有权重最终都非常接近于零，结果是一条穿过数据平均值的平线。</p><p id="7110" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">我们来看岭回归的损失函数。</p><figure class="ly lz ma mb fd iv er es paragraph-image"><div class="er es mc"><img src="../Images/96cba59443e21b1a38c1d477ad130e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*0rrSkXvdWYrZUmAmKsPy0g.png"/></div><figcaption class="jc jd et er es je jf bd b be z dx translated">损失函数</figcaption></figure><p id="c04e" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">因此，精确训练模型的一个非常简单的等式，我们可以说是校正模型。</p><p id="a7a5" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">接下来，让我们看看如何使用sklearn实现它。是的，使用sklearn实现模型没什么大不了的，你只需要3行代码来为预测准备模型，但重要的是超参数调整，这是数据科学家或机器学习工程师需要发展的技能。</p><p id="c450" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">我们来看看实现。请注意，我们的目标是理解模型，而不是超参数调整。</p><p id="5e4c" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">首先，我们将使用numpy生成一些虚拟数据。</p><pre class="ly lz ma mb fd md me mf mg aw mh bi"><span id="272c" class="mi kt hh me b fi mj mk l ml mm">import numpy as np<br/>m= 100 # number of data points<br/>X = 6* np.random.rand(m,1)-3 # our feature vector<br/>y = 0.5 * X**2 + X +2+np.random.randn(m,1) # our target value</span></pre><p id="0240" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">导入岭回归</p><pre class="ly lz ma mb fd md me mf mg aw mh bi"><span id="eca5" class="mi kt hh me b fi mj mk l ml mm">from sklearn.linear_model import Ridge</span><span id="8dc3" class="mi kt hh me b fi mn mk l ml mm">ridge_reg = Ridge(alpha=1,solver='cholesky')<br/>ridge_reg.fit(X,y)<br/>ridge_reg.predict([[1.5]])</span></pre><p id="916a" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">下图显示了使用不同α值对一些线性数据进行训练的几种岭模型。在左边，使用简单的山脊模型，导致线性预测。在右侧，首先使用多项式要素(阶数=10)扩展数据，然后使用标准缩放器缩放数据，最后将岭模型应用于生成的要素:这是使用岭正则化的多项式回归。</p><figure class="ly lz ma mb fd iv er es paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="er es mo"><img src="../Images/891cd6d72ee07d16a9d0ef00f84d95dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZHmNfdt9rdKB3zPAFlxv5w.png"/></div></div><figcaption class="jc jd et er es je jf bd b be z dx translated">具有不同alpha值的岭模型。</figcaption></figure><h1 id="4ce8" class="ks kt hh bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">套索回归</h1><blockquote class="lq lr ls"><p id="90c1" class="jh ji lt jj b jk jl jm jn jo jp jq jr lu jt ju jv lv jx jy jz lw kb kc kd io ha bi translated">最小绝对收缩和选择算子回归</p></blockquote><p id="1832" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">Lasso回归是线性回归的另一个正则化版本:就像岭回归一样，它在成本函数中添加了一个正则化项，但它使用权重向量的l1范数，而不是l2范数的一半平方。</p><blockquote class="lq lr ls"><p id="45bd" class="jh ji lt jj b jk jl jm jn jo jp jq jr lu jt ju jv lv jx jy jz lw kb kc kd io ha bi translated"><strong class="jj hi"><em class="hh">L1范数计算为矢量的绝对值之和。L2范数计算为矢量值平方之和的平方根。</em>T3】</strong></p></blockquote><p id="78de" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">以下是Lasso回归的成本函数:</p><figure class="ly lz ma mb fd iv er es paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="er es mp"><img src="../Images/ff2b4ea72c37f21dfe282f620372b920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Adj7stdmDIt-3DoxLENmLA.png"/></div></div><figcaption class="jc jd et er es je jf bd b be z dx translated">套索成本函数。</figcaption></figure><p id="7a45" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">Lasso回归的一个重要特征是，它倾向于消除最不重要的要素的权重。换句话说，Lasso回归自动执行特征选择并输出稀疏模型。</p><p id="eb38" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">让我们用sklearn实现套索回归。</p><pre class="ly lz ma mb fd md me mf mg aw mh bi"><span id="03f7" class="mi kt hh me b fi mj mk l ml mm">from sklearn.linear_model import Lasso</span><span id="5014" class="mi kt hh me b fi mn mk l ml mm">lasso_reg = Lasso(alpha=0.1)<br/>lasso_reg.fit(X, y)<br/>lasso_reg.predict([[1.5]])</span></pre><p id="2306" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">下图显示了不同alpha值的套索回归。左侧图是简单数据的结果，右侧图是添加多项式要素和在数据中执行标准缩放的结果。</p><figure class="ly lz ma mb fd iv er es paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="er es mq"><img src="../Images/9939c33dc2d1df379ff5cbfd71f2ae41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AgiXLWWLTMn0Iu-0C9slKg.png"/></div></div><figcaption class="jc jd et er es je jf bd b be z dx translated">不同α值的套索回归。</figcaption></figure><h1 id="6d7b" class="ks kt hh bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">弹性网</h1><p id="0fec" class="pw-post-body-paragraph jh ji hh jj b jk mr jm jn jo ms jq jr js mt ju jv jw mu jy jz ka mv kc kd io ha bi translated">弹性网是岭回归和套索回归的中间地带。正则项是岭和Lasso正则项的简单混合，你可以控制混合比r，当r = 0时，弹性网相当于岭回归，当r = 1时，相当于Lasso回归</p><figure class="ly lz ma mb fd iv er es paragraph-image"><div class="er es mw"><img src="../Images/6967886e62b3cea72044f51e66d7a360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*t-8UhRZQXi_rPMgaQKN7sw.png"/></div></figure><p id="3039" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">下面是一个使用Scikit-Learn的ElasticNet的简短示例:</p><pre class="ly lz ma mb fd md me mf mg aw mh bi"><span id="7965" class="mi kt hh me b fi mj mk l ml mm">from sklearn.linear_model import ElasticNet</span><span id="cafd" class="mi kt hh me b fi mn mk l ml mm">elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)<br/>elastic_net.fit(X, y)<br/>elastic_net.predict([[1.5]])</span></pre><h2 id="7d57" class="mi kt hh bd ku mx my mz ky na nb nc lc js nd ne lg jw nf ng lk ka nh ni lo nj bi translated">那么什么时候应该使用简单线性回归(即，没有任何正则化)，岭，套索，或弹性网？</h2><p id="e8ef" class="pw-post-body-paragraph jh ji hh jj b jk mr jm jn jo ms jq jr js mt ju jv jw mu jy jz ka mv kc kd io ha bi translated">至少有一点点<br/>正则化几乎总是更可取的，所以一般来说，你应该避免简单的线性回归。山脊是一个很好的默认值，但如果您怀疑只有少数要素实际上是有用的，您应该更喜欢套索或弹性网，因为它们往往会将无用要素的权重降低到零。通常，弹性网优于套索，因为当要素数量大于训练实例数量或多个要素高度相关时，套索可能会表现不稳定。</p><p id="baa6" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">这是对<strong class="jj hi">正则化线性模型</strong>的简单介绍，你可以在互联网上探索更多关于它们的信息，因为学习永无止境，你一定会发现一些新的东西，如果你发现了除此之外的东西或你事先知道的东西，请在评论中与我分享。</p><p id="882e" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated">谢谢你</p><p id="5e34" class="pw-post-body-paragraph jh ji hh jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd io ha bi translated"><a class="nk nl ge" href="https://medium.com/u/1a16144ee274?source=post_page-----d0d8964bf23f--------------------------------" rel="noopener" target="_blank">创始人基金</a> <a class="nk nl ge" href="https://medium.com/u/78cbadd3304e?source=post_page-----d0d8964bf23f--------------------------------" rel="noopener" target="_blank">莫拉莱斯</a> <a class="nk nl ge" href="https://medium.com/u/449acda51a61?source=post_page-----d0d8964bf23f--------------------------------" rel="noopener" target="_blank">黄一福</a> <a class="nk nl ge" href="https://medium.com/u/ee56d0bac1b7?source=post_page-----d0d8964bf23f--------------------------------" rel="noopener" target="_blank">雷切尔·托马斯</a> <a class="nk nl ge" href="https://medium.com/u/e53b1a2a902f?source=post_page-----d0d8964bf23f--------------------------------" rel="noopener" target="_blank">欧亨尼奥·库鲁尔切洛</a> <a class="nk nl ge" href="https://medium.com/u/7ff56d802184?source=post_page-----d0d8964bf23f--------------------------------" rel="noopener" target="_blank">劳拉·范德卡姆</a> <a class="nk nl ge" href="https://medium.com/u/d382998cf5b3?source=post_page-----d0d8964bf23f--------------------------------" rel="noopener" target="_blank">佩曼·塔伊</a> <a class="nk nl ge" href="https://medium.com/u/e53b1a2a902f?source=post_page-----d0d8964bf23f--------------------------------" rel="noopener" target="_blank">欧亨尼奥·库鲁尔切洛</a> <a class="nk nl ge" href="https://medium.com/u/d382998cf5b3?source=post_page-----d0d8964bf23f--------------------------------" rel="noopener" target="_blank">佩曼·塔伊</a> <a class="nk nl ge" href="https://medium.com/u/1b1fb9c5ea70?source=post_page-----d0d8964bf23f--------------------------------" rel="noopener" target="_blank">亚历山大·巴甫洛夫·洪查尔</a> <a class="nk nl ge" href="https://medium.com/u/84b9db1628b3?source=post_page-----d0d8964bf23f--------------------------------" rel="noopener" target="_blank">马塞尔·穆斯伯格</a> <a class="nk nl ge" href="https://medium.com/u/99b221f5f824?source=post_page-----d0d8964bf23f--------------------------------" rel="noopener" target="_blank"> IOS基金会</a> <a class="nk nl ge" href="https://medium.com/u/97d04284d471?source=post_page-----d0d8964bf23f--------------------------------" rel="noopener" target="_blank">马修</a></p></div></div>    
</body>
</html>