<html>
<head>
<title>Math behind Logistic Regression that will make you a Data Scientist(hopefully)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归背后的数学将使你成为一名数据科学家(希望如此)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/math-behind-logistic-regression-that-will-make-you-a-data-scientist-2bce20ea53fd?source=collection_archive---------12-----------------------#2021-01-31">https://medium.com/analytics-vidhya/math-behind-logistic-regression-that-will-make-you-a-data-scientist-2bce20ea53fd?source=collection_archive---------12-----------------------#2021-01-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/bc94d59e6cd77596d5aec07a8ca03b2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8UPUWxkjS3sVFCrx4kXVXg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">由<a class="ae it" href="https://unsplash.com/@qwitka?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Maksym Kaharlytskyi </a>在<a class="ae it" href="https://unsplash.com/s/photos/logistic?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="b7a4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在多次数据科学家访谈中，有几个问题是我经常被问到的，其中一个是“解释逻辑回归是如何工作的”。我在很高的层次上理解这个概念，并且知道如何实现它，但是当我被要求解释“什么是sigmoid函数”，“逻辑回归使用什么成本函数，为什么？”时，我被卡住了等等……我无法回答他们。此外，即使你不同意面试中经常被问到的事实，深入了解逻辑回归的工作原理也是有帮助的，因为它是复杂算法如多神经网络的基础。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="e13a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将按照以下步骤进行逻辑回归:</p><ol class=""><li id="4f4b" class="jz ka hh iw b ix iy jb jc jf kb jj kc jn kd jr ke kf kg kh bi translated">什么是逻辑回归</li><li id="5534" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">如何训练模型</li><li id="23b9" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">如何用训练好的模型进行预测</li></ol></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="8e85" class="kn ko hh bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">什么是逻辑回归？</h1><ul class=""><li id="ccc3" class="jz ka hh iw b ix ll jb lm jf ln jj lo jn lp jr lq kf kg kh bi translated">二元分类算法→标签有“是或否”、“真或假”、“1或0”等…</li><li id="752e" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr lq kf kg kh bi translated">类似于线性回归，它们首先计算输入的加权和(z ),但是不同之处在于，<code class="du lr ls lt lu b">z</code>不是直接输出，而是传递到sigmoid函数，该函数输出给定x属于正类的概率(你想要的类，例如:如果你想要对癌症进行分类，则标记为“癌症”的类是正类)。</li><li id="0c14" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr lq kf kg kh bi translated">当数据集很大时，它对相关特征是鲁棒的，因此用在朴素贝叶斯分类器上。</li></ul></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="e3fb" class="kn ko hh bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">模特培训</h1><p id="7cc3" class="pw-post-body-paragraph iu iv hh iw b ix ll iz ja jb lm jd je jf lv jh ji jj lw jl jm jn lx jp jq jr ha bi translated">基本上，它的工作方式与线性回归相似，最重要的部分是为每个特征(x变量)找到正确的<strong class="iw hi">权重</strong>，即在给定成本函数时输出最小成本的权重(x0w0 + x1w1 + … + xnwn)，注意x0w0是<strong class="iw hi">偏差项(又名截距)</strong>。</p><p id="a336" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">训练逻辑回归可以分为两个部分:</p><ol class=""><li id="dda4" class="jz ka hh iw b ix iy jb jc jf kb jj kc jn kd jr ke kf kg kh bi translated">成本函数:计算预测产量和真实产量之间的距离。在我们的例子中，我们将使用<strong class="iw hi">交叉熵损失</strong>作为我们的成本函数。</li><li id="6f66" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">梯度下降:迭代更新权重以最小化成本函数的优化算法。</li></ol><p id="fc66" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们导入依赖项并创建示例数据集，该数据集简单地用对角线分隔，右上角的学生“通过(1)”，左下角的学生“未通过(0)”。我们将使用定制的逻辑回归模型来预测学生的学习时间和睡眠时间是否合格。</p><figure class="ly lz ma mb fd ii"><div class="bz dy l di"><div class="mc md l"/></div></figure><p id="c487" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上面创建的数据如下所示:</p><figure class="ly lz ma mb fd ii er es paragraph-image"><div class="er es me"><img src="../Images/788a88c3046f1615897776d295c72c94.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*LSaE1pvrn231FN3NOFWn1w.png"/></div></figure><p id="9212" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，我们需要计算每个数据的交叉熵损失，但是什么是<strong class="iw hi">交叉熵损失</strong>？</p><figure class="ly lz ma mb fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/79960ea592e6fe9eb531616b855d3e43.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*O5BIHruXlmai_CO-nZSjMw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">交叉熵损失</figcaption></figure><p id="f4b8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">注意，<code class="du lr ls lt lu b">(w⋅x +b) </code>其中<code class="du lr ls lt lu b">b=w0x0</code>是输入到<strong class="iw hi"> sigmoid函数</strong>的特征的加权和，该函数然后输出属于类1的给定X(数据/学生)的预测概率，为了找到属于类0的X的概率，我们做<code class="du lr ls lt lu b">1-prob(class 1 | X)</code>。为了计算成本，我们将类别1和0的对数似然性相加，然后乘以-1，类别1的对数似然性= <code class="du lr ls lt lu b">log(<strong class="iw hi">σ</strong>(z)) </code>和类别0= <code class="du lr ls lt lu b">log(1-<strong class="iw hi">σ</strong>(z))</code>。</p><p id="d0ca" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在用python实现之前，下面是<strong class="iw hi"> sigmoid函数</strong>的样子:</p><figure class="ly lz ma mb fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/6dda4c0479c363dd001053ba6ee0362b.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*uF4rW1TEsBFmpdD4tZoi7A.png"/></div></figure><p id="3241" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">它允许我们将<code class="du lr ls lt lu b">z</code>的[-inf，+inf]值映射到范围[0，1]。</p><p id="e8a0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上述等式应用于每行数据，在应用于所有行之后，它们的平均值就是一次迭代的<strong class="iw hi">成本，我们反复迭代，直到找到全局最小值(收敛)。</strong></p><figure class="ly lz ma mb fd ii"><div class="bz dy l di"><div class="mc md l"/></div></figure></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="de9c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">概述:<strong class="iw hi">梯度下降</strong>是一种通过计算出函数的斜率在哪个方向(在n_feature空间中)上升最陡，然后向相反方向移动直到斜率= 0，从而找到函数最小值的方法。</p><p id="6345" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">坡度</strong>是一个矢量，它指向坡度增加最大的方向，而<strong class="iw hi">下降</strong>意味着在坡度的相反方向下降。</p><p id="7995" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过对成本函数相对于其特征进行偏导数来计算每个特征的斜率。梯度被定义为这些偏导数的向量。</p><figure class="ly lz ma mb fd ii er es paragraph-image"><div class="er es mh"><img src="../Images/9947d0d31a77d45335b51de5f51fa908.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*zwGy6gFF8JIheJqfa7MSPQ.png"/></div></figure><p id="e0a0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们用(learning_rate * gradient_vector)减去前一个权重向量，然后得到新的权重向量，输入到下一次迭代过程中，重复这个过程。</p><p id="7c40" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在代码中实现梯度下降，以下代码参考自<a class="ae it" href="https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#training" rel="noopener ugc nofollow" target="_blank">https://ml-cheat sheet . readthedocs . io/en/latest/logistic _ regression . html # training</a></p><figure class="ly lz ma mb fd ii"><div class="bz dy l di"><div class="mc md l"/></div></figure><p id="f0a6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在让我们用目前所学来训练逻辑回归模型。因为我们的数据很小，所以我们用90%的数据进行100次迭代的训练。</p><figure class="ly lz ma mb fd ii"><div class="bz dy l di"><div class="mc md l"/></div></figure><p id="6077" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">运行以上代码输出权重= [-0.33614273，0.00188673，0.10463643]也就是我们的<code class="du lr ls lt lu b">z = -0.336(bias) + 0.0019(studied) + 0.105(slept)</code>。现在，当给定新生的睡眠和学习时间时，我们可以使用以下三个步骤进行预测:</p><ol class=""><li id="6435" class="jz ka hh iw b ix iy jb jc jf kb jj kc jn kd jr ke kf kg kh bi translated">用我们学习的重量计算<code class="du lr ls lt lu b">z</code></li><li id="75a5" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">将<code class="du lr ls lt lu b">z</code>传递到<strong class="iw hi"> sigmoid函数</strong>中，以获得学生通过的概率</li><li id="e8da" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">通过的概率被输入到<strong class="iw hi">决策函数(阈值)</strong>，该函数输出标签(“通过或失败”)。例如:给定0.5阈值，低于0.5的概率将输出“失败”标签，大于0.5的概率将输出“通过”标签。</li></ol></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="a6b1" class="kn ko hh bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">预言；预测；预告</h1><p id="bd2a" class="pw-post-body-paragraph iu iv hh iw b ix ll iz ja jb lm jd je jf lv jh ji jj lw jl jm jn lx jp jq jr ha bi translated">现在有了10%的数据，我们将计算模型的表现，在给定学生睡眠和学习时间的情况下，我们的模型预测“通过或失败”有多好。</p><figure class="ly lz ma mb fd ii"><div class="bz dy l di"><div class="mc md l"/></div></figure><p id="23bc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">运行上面的代码使用0.9的精度值，这意味着我们的模型在90%的时间是正确的。</p><p id="9be9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">请注意，accuracy_score不是衡量分类模型性能的合适指标，尤其是在不平衡数据中。例如，在尝试识别癌症是恶性还是良性时，由于癌症的罕见性，仅仅通过猜测所有患者都是良性的就可以很好地进行预测。在这种情况下，我们必须考虑精确度、召回率、f1分数和混淆矩阵(另一个博客的主题)。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="6f5e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">就是这样，我们已经编码了从训练逻辑回归到使用学习到的特征权重和偏差进行预测的所有内容。</p><p id="8c40" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">总之，我认为逻辑回归在幕后有非常简洁的数学运算。我希望每个人都花时间真正理解正在发生的事情，不仅仅是为了采访，而是因为理解经常使用的工具中发生的事情很有趣。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="23af" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">参考资料:</p><ul class=""><li id="f18f" class="jz ka hh iw b ix iy jb jc jf kb jj kc jn kd jr lq kf kg kh bi translated"><a class="ae it" href="https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html" rel="noopener ugc nofollow" target="_blank">关于逻辑回归的ml-cheatsheet文档</a>(推荐)</li><li id="aec8" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr lq kf kg kh bi translated">斯坦福的教科书(推荐)</li><li id="126c" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr lq kf kg kh bi translated">安德鲁·吴的斯坦福演讲</li><li id="257f" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr lq kf kg kh bi translated"><a class="ae it" href="https://www.youtube.com/watch?v=uFfsSgQgerw&amp;ab_channel=KrishNaik" rel="noopener ugc nofollow" target="_blank">KrishNaik的Youtube视频</a></li><li id="9c45" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr lq kf kg kh bi translated"><a class="ae it" href="https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training" rel="noopener ugc nofollow" target="_blank">https://developers . Google . com/machine-learning/速成班/逻辑回归/模型培训</a></li><li id="abdd" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr lq kf kg kh bi translated"><a class="ae it" href="https://gist.github.com/avidale/a640f7a8e353d9efdd79385e277caef1" rel="noopener ugc nofollow" target="_blank">如何选择梯度下降的初始权重</a></li></ul></div></div>    
</body>
</html>