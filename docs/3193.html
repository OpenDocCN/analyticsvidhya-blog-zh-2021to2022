<html>
<head>
<title>Character awareness for language models using CNNs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用细胞神经网络的语言模型的字符意识</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/character-awareness-for-language-models-using-cnns-6b1c1a331042?source=collection_archive---------1-----------------------#2021-06-17">https://medium.com/analytics-vidhya/character-awareness-for-language-models-using-cnns-6b1c1a331042?source=collection_archive---------1-----------------------#2021-06-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="b88b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文将通过PyTorch和Python上实现的实际细节来介绍将CNN用于NLP的用例。它基于论文“<a class="ae jc" href="https://arxiv.org/pdf/1508.06615.pdf" rel="noopener ugc nofollow" target="_blank">字符感知神经语言模型”</a>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/d22bffed4f0f43bd31086fe6ab7d3db4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*r-YscSrImdpqYdmF"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jc" href="https://unsplash.com/@alinnnaaaa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿丽娜·格鲁布尼亚</a>在<a class="ae jc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="5a4a" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">背景:</strong></h1><p id="bcc6" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated"><strong class="ig hi">语言模型:</strong>语言模型为句子或短语分配概率。给定一些单词，我们可以预测下一个单词，它将赋予该短语最高的概率。我们经常看到它们被用于消息服务的文本补全</p><p id="e5da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">孩子们在_ _ _ _ _ _ _ _ _ _</p><p id="5348" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一门好的语言应该能够预测下一个单词是“park”还是其他有意义的词。</p><p id="f059" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">更正式地说，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kw"><img src="../Images/ef0434d67b131825fe434c60c5e71b20.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*Sob6Q1NCFVa5JotZB2cjeQ.png"/></div></figure><p id="016a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以对一个给定的“m”个单词序列的概率进行建模，方法是接受每个单词，并根据之前出现过的单词计算概率。</p><p id="1a66" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在前Transformer时代，大多数论文使用<strong class="ig hi"> RNNs </strong>来完成这样的任务，论文中使用的模型就是这种情况。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kx"><img src="../Images/dd90b47ab811e061d54d82fa5e935b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*kKILs8Fnw0X8rKXWIjcNzg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">用于语言建模任务的典型RNN架构</figcaption></figure><p id="e2ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">给定一个单词，在预定义的词汇表中查找该单词的嵌入，并使用权重矩阵<strong class="ig hi"> W_e </strong>进行线性变换。循环部分来自隐藏状态<strong class="ig hi">‘h’，</strong>，其中我们再次在每个时间步长<strong class="ig hi">使用相同的权重矩阵<strong class="ig hi"> W_h </strong>。然后，它被非线性压缩，通常是一个<strong class="ig hi"> tanh </strong>。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ky"><img src="../Images/3720a064ded7f5cb73cc2497f9ce58cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/0*IDe024Cqr0RKccIZ.png"/></div></figure><p id="fe17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，在预测下一个单词时，我们使用softmax来输出词汇的分布。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kz"><img src="../Images/e1b50608250e2ca6a3ce67a3178a5339.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/0*sua1-a56k3PYk4Px.png"/></div></figure><p id="ce86" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个矩阵的形状如下:</p><ol class=""><li id="82e3" class="la lb hh ig b ih ii il im ip lc it ld ix le jb lf lg lh li bi translated"><strong class="ig hi"> h_t </strong> : (D_h，)→隐藏状态形状</li><li id="db39" class="la lb hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><strong class="ig hi"> W_h </strong> : (D_h x D_h) →隐藏状态的权重矩阵的形状</li><li id="d09e" class="la lb hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><strong class="ig hi"> x_t </strong> : ( E，)→每个单词的嵌入形状</li><li id="552e" class="la lb hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><strong class="ig hi"> W_e: </strong> ( D_h x E ) →嵌入的权重矩阵的形状</li></ol><p id="5848" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">概括地说，在训练过程中，我们逐字逐句地接受每个输入，并查找它们的单词级表示，通过RNN输入它们以生成预测。然后，我们根据词汇中标签的真实概率分布来训练我们的网络。</p><p id="eed4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">尽管这些单词级嵌入在实践中非常有效；当我们遇到不认识的单词或生僻的单词时，我们的预测可能不那么准确。</p><p id="9a98" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通常，对于我们的词汇表中没有的任何未知单词，我们使用一个“unk”标记，它本身具有某种嵌入表示。</p><p id="62ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上述论文中，使用了每个单词的字符级表示，这是通过对字符级嵌入进行1d卷积而获得的。</p><p id="f165" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">文字的字符表示:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/990f8981d87dfaeeb3e5f4735e0a9fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*8bWo9HLeQzxI_rRV2D8Mdg.png"/></div></figure><p id="8bae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将每个字符表示为一个1D向量，并将它们连接起来表示每个单词。我还想补充一些实际的实现细节。所以让我们一步一步来。最初，我们需要每个字符的一个表示作为索引。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="107d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们用左右括号({，})来表示一个单词在句子中的开始和结束。我们还必须定义一个函数，该函数接收句子列表并输出字符索引(我们之前映射的那些)</p><p id="01dd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/48bdd2d300ddb9a11b10c3ba9605f103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*zcQGaHCOGXzC1VKV2vlfnQ.png"/></div></figure><p id="91ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用我们之前定义的字典将每个单词转换成索引。此外，它们以1 ( {)开始，以2 ( })结束。但是句子和单词的长度会不同。<strong class="ig hi">因此，我们必须填充每个单词以获得相等的长度</strong>。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lp lq l"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/3473ea8f6e36377ee4616750941d7360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*EizaedP4uOKpthr30Ncqgg.png"/></div></figure><p id="2a19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于这些索引中的每一个，我们查找特定的字符嵌入并将它们连接在一起，以使它们准备好通过卷积层。</p><p id="2c75" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以如果最大的字的大小是<strong class="ig hi">‘m’</strong>&amp;字符嵌入维数是<strong class="ig hi">‘d’</strong>，那么在CNN层上传递的矩阵的大小就是<strong class="ig hi">(m×d)</strong>。我们可以把这个叫做X_emb。</p><p id="4ee2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> Conv图层:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/ef31fc1a7cbd328c0938ef07b9492cf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*EnI94A3D662hQ4oFW-qQKw.png"/></div></figure><p id="311a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们已经得到了我们的串联字符嵌入，我们通过1D卷积和最大池来获得一个固定维度的单词的嵌入。</p><p id="df84" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在最初的论文中，作者使用了具有<strong class="ig hi">单独过滤器宽度</strong>的多个过滤器，其中每个过滤器将捕捉不同的表示。根据作者的说法，“过滤器本质上是挑选一个字符n-gram，其中n-gram的大小对应于过滤器宽度”。例如，如果我们有单词“无政府状态”，大小为3的过滤器宽度可能集中在“ana”、“nar”或“arc”上，以查看它们是否有意义。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/d675c89cfeaf171fd8da26320379ee41.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*HJIb6Y7JjjaEeIAAvhFatQ.png"/></div></figure><p id="d03a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从(<strong class="ig hi"> m x d) X_emb </strong>矩阵中，我们必须遵循以下步骤:</p><ol class=""><li id="924a" class="la lb hh ig b ih ii il im ip lc it ld ix le jb lf lg lh li bi translated">整形为(<strong class="ig hi"> d x m) </strong>:这只是因为PyTorch在矩阵的最后一个维度上执行卷积。所以我们想沿着列移动卷积(即每个字符)。</li><li id="35b2" class="la lb hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">我们选择“过滤器”的数量，这最终将是我们想要的单词嵌入维度的大小。所以我们可以称这个数为“<strong class="ig hi"> E”，</strong>如上面RNN例子中提到的。另一个超参数是宽度<strong class="ig hi">“k”</strong>。</li><li id="564a" class="la lb hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">利用X_emb的整形后的矩阵，我们计算每个宽度为“k”的滤波器的元素乘积和。</li><li id="8461" class="la lb hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">对于每个字，我们计算每个滤波器的<strong class="ig hi">(m k+1)长向量。我们有E个过滤器，我们最终得到形状为<strong class="ig hi"> E x (m-k+1)的最终矩阵。</strong></strong></li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/41fd35b47d6cea5b5bf5e7149ca662ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/0*N9_WMArt1jJ3XY8E.png"/></div></figure><p id="9ed3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上面的等式只是表示我们如何计算m-k+1长向量的第I个元素。我们用大小为<strong class="ig hi">(d×k)的滤波器H取大小为k的窗口的Frobenius内积(这就像矩阵的点积，逐元素相乘+求和)。</strong></p><p id="ff05" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">5.贴上这个我们添加一个非线性像tanh或Relu。在论文中他们使用了tanh。</p><p id="3053" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">6.最后，我们在第二维上使用最大池，以得到大小为e的单个向量。</p><p id="d855" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">概括一下，对于每个单词，我们从一个矩阵<strong class="ig hi"> (m x d) → E x (m-k+1) → E </strong></p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="f2ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以，最后对于每个单词，我们都有一个从字符派生的单词嵌入。现在在报纸上，他们把这个传递给一个“高速公路网”。关于这项技术的论文可以在<a class="ae jc" href="https://arxiv.org/abs/1505.00387" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="fce2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">公路网:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/b5d0bdccefad1881f32c5bc259509dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*2FPsFeEpkt04foOBWaixsg.png"/></div></figure><p id="694f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用一个“门”来控制有多少输入被送入RNN，而不是将从conv层得到的单词_embedding直接传递给RNN。这似乎有助于改善梯度流，并优化一般。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/fe46a89e5fd3efa7ae30772ece42e2c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*0Ie-EYEivMY5JRw8xblA3Q.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/2651d2b38fc91e905d74db62678c6ac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*s61p0WEGniYNknZaZRRfFA.png"/></div></figure><p id="a05d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由sigmoid控制的门具有需要传递“多少”信息的含义。投影是用非线性系数计算的&amp;乘以门。其余的直接以(1-x_gate)的因子传递。</p><p id="9a7a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">权重W_proj &amp; W_gate的形状为<strong class="ig hi">(E x E)ie word _ embedding x word embedding。</strong></p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="23f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">最终层和预测</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ma"><img src="../Images/1f6419fdbcd2fa22ba23ffcb35103ff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*Tm_f3SM0Jl7t_KmIeSKfhg.png"/></div></figure><p id="d37a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">公路图层的输出被传递到RNN(本例中为LSTM)。而且是用下一个词的真实标签训练出来的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mb"><img src="../Images/1f0ecbec56366c5757ac143b18fa1514.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*DqjzrAudK9b5eqxfQ23I9A.png"/></div></figure><p id="bdac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据该论文，该模型能够实现比具有少量参数的单词级模型更好的准确性。这里PPL指的是“困惑”，一种评估语言模型的度量。(越低越好)</p></div></div>    
</body>
</html>