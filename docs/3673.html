<html>
<head>
<title>Creating MobileNetsV2 with TensorFlow from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TensorFlow从头开始创建MobileNetsV2</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/creating-mobilenetsv2-with-tensorflow-from-scratch-c85eb8605342?source=collection_archive---------2-----------------------#2021-07-17">https://medium.com/analytics-vidhya/creating-mobilenetsv2-with-tensorflow-from-scratch-c85eb8605342?source=collection_archive---------2-----------------------#2021-07-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f645" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MobileNet模型非常小，延迟很低。MobileNet模型可以轻松地部署在移动和嵌入式边缘设备上。</p><p id="6f6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇博客中，我们将看看MobileNet的改进版本，即版本2- MobileNetV2。第一个版本<a class="ae jd" rel="noopener" href="/@sumeet.bb/creating-mobilenets-with-tensorflow-from-scratch-c34ec79a59d2"> MobileNet解释和用Tensorflow </a>创建在我以前的帖子中解释过。在版本1中，作者使用深度方向可分离卷积来减少计算。在版本2中，作者通过减少可训练参数进一步减少了计算时间，而没有任何显著或没有降低精度。</p><p id="3547" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MobileNetV2论文链接:【http://export.arxiv.org/pdf/1801.04381 T2</p><p id="639d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">好，那V2有什么变化？</strong></p><p id="bfd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个新的卷积块称为反向残差和线性瓶颈。在该块中，来自较低维度表示的特征被放大。然后应用深度方向的卷积，然后将特征压缩回先前的低维表示。</p><p id="e362" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里有一张积木的图片，让你更容易理解积木结构背后的直觉。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/b7ebee6f5793fc33559a62dd9c6e426c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*KeSmxGxWaJCPucirlXsrVw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">图1:块的表示。</figcaption></figure><blockquote class="jq jr js"><p id="64d2" class="if ig jt ih b ii ij ik il im in io ip ju ir is it jv iv iw ix jw iz ja jb jc hb bi translated">层的扩展由标量变量<strong class="ih hj"> t </strong>决定。这个变量是过滤器大小的倍数，以获得所需的扩展。压缩率取决于我们想要的输出层，即滤波器大小。</p></blockquote><p id="cbc2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们用Python创建这些层。这次我们将使用Relu(6)作为激活函数。Relu(6)是在许多模型和论文中使用的事实上的激活函数。怎么做，为什么？那是另一个时间的讨论。</p><pre class="jf jg jh ji fd jx jy jz ka aw kb bi"><span id="f37b" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">def</strong> expansion_block(x,t,filters,block_id):</span><span id="8186" class="kc kd hi jy b fi ki kf l kg kh">    prefix = 'block_{}_'.format(block_id)<br/>    total_filters = t*filters<br/>    x = Conv2D(total_filters,1,padding='same',use_bias=False, name =    prefix +'expand')(x)</span><span id="663d" class="kc kd hi jy b fi ki kf l kg kh">    x = BatchNormalization(name=prefix +'expand_bn')(x)<br/>    x = ReLU(6,name = prefix +'expand_relu')(x)<br/>    <strong class="jy hj">return</strong> x</span><span id="db05" class="kc kd hi jy b fi ki kf l kg kh"><strong class="jy hj">def</strong> depthwise_block(x,stride,block_id):</span><span id="6574" class="kc kd hi jy b fi ki kf l kg kh">    prefix = 'block_{}_'.format(block_id)<br/>    x = DepthwiseConv2D(3,strides=(stride,stride),padding ='same', use_bias = False, name = prefix + 'depthwise_conv')(x)</span><span id="cf6a" class="kc kd hi jy b fi ki kf l kg kh">    x = BatchNormalization(name=prefix +'dw_bn')(x)<br/>    x = ReLU(6,name = prefix +'dw_relu')(x)<br/>    <strong class="jy hj">return</strong> x</span><span id="833e" class="kc kd hi jy b fi ki kf l kg kh"><strong class="jy hj">def</strong> projection_block(x,out_channels,block_id):</span><span id="9bc1" class="kc kd hi jy b fi ki kf l kg kh">    prefix = 'block_{}_'.format(block_id)<br/>    x = Conv2D(filters=out_channels,kernel_size = 1,   padding='same',use_bias=False,name= prefix + 'compress')(x)</span><span id="cf3d" class="kc kd hi jy b fi ki kf l kg kh">    x = BatchNormalization(name=prefix +'compress_bn')(x)<br/>    <strong class="jy hj">return</strong> x</span></pre><p id="a6e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为它是一个剩余块，意味着应该有一个添加，所以添加的层是块的输入和输出。残差背后的直觉是，它在第一层和最后一层之间创建了一条高速公路，使数据流(即梯度)更容易。</p><p id="a102" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是MobileNetV2中使用的残差块的图片。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kj"><img src="../Images/18f713159cfcaa74208299aa94226313.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*VSCcDX8MI6InkJFl_61Dzw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">图2:瓶颈剩余块</figcaption></figure><p id="e9bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可能会注意到，在投影图层中的批处理规范化图层之后没有激活图层。这是因为作者发现，由于它将数据压缩到低维，对其应用非线性会破坏有用的信息。</p><p id="dce7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用低维张量是减少计算量的关键。毕竟，张量越小，卷积层需要做的乘法就越少。</p><p id="c331" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">只有当通道尺寸相同时，才能添加层。为此，我们将实现一个<strong class="ih hj"> if </strong>函数来检查通道大小。现在是它的Python代码。让我们看看。</p><pre class="jf jg jh ji fd jx jy jz ka aw kb bi"><span id="86c2" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">def</strong> Bottleneck(x,t,filters, out_channels,stride,block_id):<br/>    y = expansion_block(x,t,filters,block_id)<br/>    y = depthwise_block(y,stride,block_id)<br/>    y = projection_block(y, out_channels,block_id)<br/>    <strong class="jy hj">if</strong> y.shape[-1]==x.shape[-1]:<br/>       y = add([x,y])<br/>    <strong class="jy hj">return</strong> y</span></pre><p id="1fec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们看看所有这些块和层是什么样子，以及如何用python实现它们。</p><p id="d590" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">MobileNetV2的架构:</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kk"><img src="../Images/a117f33ae97d6c83ce9a417d34c3677a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*3_h-odA8FjRZ1SOlvB5sOg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">图3:MobileNetV2架构(来源:最初的MobileNetV2论文)</figcaption></figure><p id="26c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MobileNetV2从基本的2D卷积层开始。然后是一系列的瓶颈层一个接一个的附着，有膨胀率(<strong class="ih hj"> t </strong>，不同的步幅(<strong class="ih hj"> s </strong>，输出通道(<strong class="ih hj"> c </strong>，重复的次数(<strong class="ih hj"> n </strong>)。</p><p id="fa7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">定义卷积块</strong> —输入后的每个卷积块有如下顺序:BatchNormalization，后跟ReLU activation，然后传递给下一个块。</p><p id="032b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一卷积块具有32个内核大小(3×3)的滤波器，步长为2。如前所述，随后是批处理标准化层和ReLU激活。这三行可以用下面的代码表示。</p><pre class="jf jg jh ji fd jx jy jz ka aw kb bi"><span id="1a4c" class="kc kd hi jy b fi ke kf l kg kh">input = Input (input_shape)<br/>x = Conv2D(32,3,strides=(2,2),padding='same', use_bias=False)(input)<br/>x = BatchNormalization(name='conv1_bn')(x)<br/>x = ReLU(6, name='conv1_relu')(x)</span></pre><p id="0032" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来是一连串的17个瓶颈。瓶颈的称呼如下。x =瓶颈(x，t = 6，过滤器= x.shape[-1]，out_channels = 24，stride = 2，block_id = 2)</p><pre class="jf jg jh ji fd jx jy jz ka aw kb bi"><span id="c27c" class="kc kd hi jy b fi ke kf l kg kh">x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 24, stride = 2,block_id = 2)</span></pre><p id="e7e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后还有步幅1的1x1卷积层。接着是一层GlobalAveragePooling，最后是输出层。输出层是一个密集层，在这里类将被提及。如果类是3，那么它应该是密集的(3)。使用的激活函数是Softmax。</p><pre class="jf jg jh ji fd jx jy jz ka aw kb bi"><span id="e6d4" class="kc kd hi jy b fi ke kf l kg kh">x = Conv2D(filters = 1280,kernel_size = 1,padding='same',use_bias=False, name = 'last_conv')(x)</span><span id="4739" class="kc kd hi jy b fi ki kf l kg kh">x = BatchNormalization(name='last_bn')(x)<br/>x = ReLU(6,name='last_relu')(x)</span><span id="edf5" class="kc kd hi jy b fi ki kf l kg kh">x = GlobalAveragePooling2D(name='global_average_pool')(x)output = Dense(n_classes,activation='softmax')(x)</span></pre><p id="58cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们已经把所有的模块都放在一起了，让我们把它们合并起来，看看整个MobileNet架构。</p><p id="5301" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">完整的MobileNet架构:</strong></p><pre class="jf jg jh ji fd jx jy jz ka aw kb bi"><span id="d92e" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">def</strong> MobileNetV2(input_image = (224,224,3), n_classes=1000):</span><span id="adb1" class="kc kd hi jy b fi ki kf l kg kh">    input = Input (input_shape)<br/>    x = Conv2D(32,3,strides=(2,2),padding='same', use_bias=False)(input)<br/>    x = BatchNormalization(name='conv1_bn')(x)<br/>    x = ReLU(6, name='conv1_relu')(x)</span><span id="b091" class="kc kd hi jy b fi ki kf l kg kh">    # 17 Bottlenecks</span><span id="165c" class="kc kd hi jy b fi ki kf l kg kh">    x = depthwise_block(x,stride=1,block_id=1)<br/>    x = projection_block(x, out_channels=16,block_id=1)</span><span id="b5a3" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 24, stride = 2,block_id = 2)</span><span id="f8bf" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 24, stride = 1,block_id = 3)</span><span id="50ca" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 2,block_id = 4)</span><span id="29f8" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 1,block_id = 5)</span><span id="722f" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 1,block_id = 6)</span><span id="29a6" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 2,block_id = 7)</span><span id="df1f" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 8)</span><span id="60a2" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 9)</span><span id="7ed0" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 10)</span><span id="381d" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 11)</span><span id="4286" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 12)</span><span id="861c" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 13)</span><span id="5188" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 160, stride = 2,block_id = 14)</span><span id="3616" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 160, stride = 1,block_id = 15)</span><span id="edf8" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 160, stride = 1,block_id = 16)</span><span id="ec83" class="kc kd hi jy b fi ki kf l kg kh">    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 320, stride = 1,block_id = 17)</span><span id="9cb0" class="kc kd hi jy b fi ki kf l kg kh">    x = Conv2D(filters = 1280,kernel_size = 1,padding='same',use_bias=False, name = 'last_conv')(x)<br/>    x = BatchNormalization(name='last_bn')(x)<br/>    x = ReLU(6,name='last_relu')(x)</span><span id="2064" class="kc kd hi jy b fi ki kf l kg kh">    x = GlobalAveragePooling2D(name='global_average_pool')(x)<br/>    output = Dense(n_classes,activation='softmax')(x)</span><span id="e882" class="kc kd hi jy b fi ki kf l kg kh">    model = Model(input, output)<br/>    <strong class="jy hj">return</strong> model</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kl"><img src="../Images/650977ee939e73ab776a7489dfbdf6b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YdWvMVh34ZW3GZMIBf7ngQ.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">图4:模型总结最后几层</figcaption></figure><p id="f893" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是我们实现MobileNetV2架构的方式。</p><p id="3ab0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要以更好的方式查看代码，请查看github上的jupyter <a class="ae jd" href="https://github.com/Haikoitoh/paper-implementation/blob/main/MobileNetV2.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><p id="1b29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献:</strong></p><p id="5069" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">@ article { Sandler 2018 mobilenetv2ir，title={MobileNetV2:反向残差和线性瓶颈}，作者= { m . Sandler and Andrew g . Howard and Meng long Zhu and a . Zhmoginov and Liang-Chieh Chen }，期刊={2018 IEEE/CVF计算机视觉和模式识别会议}，年份={2018}，页数= { 4510–4520 } }</p></div></div>    
</body>
</html>