<html>
<head>
<title>Convolutional Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/cnn-32fbc1706a89?source=collection_archive---------9-----------------------#2021-06-28">https://medium.com/analytics-vidhya/cnn-32fbc1706a89?source=collection_archive---------9-----------------------#2021-06-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="a0e0" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">概观</h1><p id="adca" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在这篇文章中，我想分享我对卷积神经网络的基础知识。理解任何概念的基础会使学习这些概念变得容易。所以，让我们开始以更好的方式理解CNN或Convnets。</p><h1 id="be5b" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">目录:</h1><ol class=""><li id="c142" class="ka kb hh je b jf jg jj jk jn kc jr kd jv ke jz kf kg kh ki bi translated">介绍</li><li id="5c3e" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">CNN如何从图像中学习？</li><li id="bf05" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">过滤器和过滤深度</li><li id="9805" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">因素</li><li id="d356" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">填料</li><li id="28de" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">维度</li><li id="8d73" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">联营</li><li id="7f09" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">CNN基本架构</li><li id="482f" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">激活功能</li><li id="3d94" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">使用反向传播训练CNN</li><li id="2184" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">现有架构</li></ol><h1 id="ec7e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak"> 1。简介:</strong></h1><p id="922d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">卷积神经网络(CNN或Convnet)是专门为视觉任务构建的一类神经网络。图像识别和分类、物体识别等领域的视觉任务。</p><p id="5dd8" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">ConvNets是当今大多数机器学习实践者的重要工具。然而，理解ConvNets并第一次学习使用它们有时会是一种令人生畏的经历。</p><p id="ba5e" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">卷积神经网络(CNN)在生物学上受到人脑视觉皮层的启发，它由一个或多个卷积层组成，然后是一个或多个完全连接的层</p><h1 id="0aad" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak"> 2。CNN是如何运作的？</strong></h1><p id="d7cc" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">让我们对卷积神经网络(CNN)如何工作有一个更好的直觉。我们将研究人类如何对图像进行分类，然后看看CNN如何使用类似的方法。</p><p id="5293" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">图像是值得注意的，但像素。像素包含构成图像的RGB颜色值。我用<a class="ae kt" href="https://csfieldguide.org.nz/en/interactives/pixel-viewer/" rel="noopener ugc nofollow" target="_blank">像素浏览器</a>查看下图的像素范围。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/997f028032d47dbb2a9d9557fc8cd9db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kZhWUL0_Htuj9xjkFdRLRw.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">猫的原始图像</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lk"><img src="../Images/fce41f6d50e225ab335140d1195bdf43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wtwlXI8s0LT_7S6VDqcldg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">上面猫图像的像素RGB值</figcaption></figure><p id="d9e9" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">假设我们想将下面的一只猫的图像归类到该猫的一个特定品种:</p><p id="a18e" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">作为人类，我们如何做到这一点？</p><p id="f3ac" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">我们这样做是为了识别猫的某些部位，比如鼻子、眼睛和嘴巴。我们本质上是将图像分解成更小的片段，识别更小的片段，然后将这些片段组合起来，以获得猫的整体概念。</p><p id="590c" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">在这种情况下，我们可以将图像分解为以下内容的组合:</p><ul class=""><li id="ddc1" class="ka kb hh je b jf ko jj kp jn ll jr lm jv ln jz lo kg kh ki bi translated">一个鼻子</li></ul><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lp"><img src="../Images/45a1b1135f72a79cd1031620abd0fe5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*nng3WNdRLKvqwpOJ-fZ3LQ.jpeg"/></div></figure><ul class=""><li id="7640" class="ka kb hh je b jf ko jj kp jn ll jr lm jv ln jz lo kg kh ki bi translated">眼睛</li></ul><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lq"><img src="../Images/7374ed7a72108e91d54c8ef3204131a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*oBarLlUcRxKyR3gqCdzKTg.jpeg"/></div></figure><ul class=""><li id="f439" class="ka kb hh je b jf ko jj kp jn ll jr lm jv ln jz lo kg kh ki bi translated">口</li></ul><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lr"><img src="../Images/087dfa77644e18079d6fa08e433ff36a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jxfnaB0db2jxJ7FzbPxHCg.jpeg"/></div></div></figure><p id="9558" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">更进一步</strong></p><p id="eb5d" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">我们如何确定鼻子到底是什么？</p><p id="5a19" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">鼻子可以看做一个椭圆形，里面有两个黑洞。因此，对鼻子进行分类的一种方法是将其分成更小的部分，并寻找黑洞(鼻孔)和定义椭圆的曲线，如下所示:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ls"><img src="../Images/ffb76fa2da51b738acad9320ded1943a.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*vAuTIMgGjtNPkSJN7XXbzg.jpeg"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">我们可以用来确定鼻子的曲线</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lt"><img src="../Images/0ffb1619281e7d9c6887d7ce3307da2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*AGTiz1BwN4yIkvAi4vMFug.jpeg"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">一个鼻孔，我们可以用它来给狗的鼻子分类</figcaption></figure><p id="84e3" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">从广义上讲，这是一个CNN学会做的事情。它学习识别基本的线条和曲线，然后是形状和斑点，然后是图像中日益复杂的对象。最后，CNN通过组合更大、更复杂的对象来对图像进行分类。</p><p id="bd46" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">在我们的例子中，层次结构中的级别是:</p><ul class=""><li id="f3af" class="ka kb hh je b jf ko jj kp jn ll jr lm jv ln jz lo kg kh ki bi translated">简单的形状，如椭圆形和黑色圆形</li><li id="eb60" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated">复杂物体(简单形状的组合)，如眼睛、鼻子和嘴</li><li id="e4ec" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated">猫是复杂物体的组合。</li></ul><p id="8186" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">通过深度学习，我们实际上并没有对CNN进行编程来识别这些特定的特征。更确切地说，CNN通过前向传播和反向传播自己学习识别这样的物体！</p><p id="34e4" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">很神奇吧..CNN可以学习如何对图像进行分类，即使我们从来没有用特定特征的信息对CNN进行编程。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lu"><img src="../Images/81ff0842d7a951499eec69ce149c14cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*pNystPJT7ycSq2rPTULLJw.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">CNN如何一层一层分割图像的层次结构</figcaption></figure><p id="dd81" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">CNN可能有几层，每层可能捕获对象层次结构中的不同级别。第一层是层次中的最低层，CNN通常将图像的小部分分类为简单的形状，如水平线和垂直线以及简单的颜色块。</p><p id="30e3" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">随后的层往往是层次中更高的层次，通常对更复杂的概念进行分类，如形状(线条的组合)，最终是完整的对象，如猫。</p><p id="0425" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">CNN <strong class="je hi">再一次靠自己</strong>了解了所有这些。我们甚至不需要告诉CNN去寻找线条或曲线或鼻子或嘴。CNN只是从训练集中学习，发现猫的哪些特征值得寻找。</p><h1 id="b5d2" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">3.过滤</h1><p id="79c2" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">对CNN来说，第一步是把图像分成更小的片段。我们通过选择定义过滤器的宽度和高度来做到这一点。</p><p id="b143" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">滤波器/内核:</strong>滤波器或内核是预先选择的m*n矩阵，它扫描输入的图像矩阵，并通过矩阵乘法产生一些结果，这些结果给出关于各种图像特征的想法。过滤器查看图像的小块或小块。这些补丁与过滤器的大小相同。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lv"><img src="../Images/2f2e9f4f1810eb45345c0980835d919b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/0*l4TcDlJpVVyKW9eT.gif"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图片来源:<a class="ae kt" href="https://towardsdatascience.com/cnn-part-i-9ec412a14cb1" rel="noopener" target="_blank">中</a></figcaption></figure><p id="b92b" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">在上面的gif中，绿色矩阵是5X5的图像，黄色矩阵是3X3的核/过滤器。通过计算核，在图像矩阵上，我们得到卷积的特征矩阵。过滤器/内核只是水平或垂直滑动，聚焦在图像的不同部分。</p><p id="2c60" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">步幅:</strong>过滤器滑动的量称为<strong class="je hi">‘步幅’</strong>。通过减少每层观察的总面片数，增加步幅可以减小模型的大小。</p><p id="2505" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">让我们看一个例子。在这张猫的放大图中，我们首先从红色的补丁开始。我们的过滤器的宽度和高度决定了这个正方形的大小。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lw"><img src="../Images/a56ad36f08defc4b7be69030a437eb0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FLN--2BzGdRMneBxaplLkA.png"/></div></div></figure><p id="dff7" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">然后，我们将方块向右移动给定的步幅(在本例中为2 ),以获得另一个面片。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lx"><img src="../Images/76a96e6ea03fefb68efd0d6b0ee13fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NOyiOLeOQO-Gj13Ueu6RGg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">我们将正方形向右移动两个像素，创建另一个小块。</figcaption></figure><p id="2c01" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">这里重要的是，我们将相邻的像素分组，并把它们作为一个整体。</p><p id="cf82" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">在正常的非卷积神经网络中，我们会忽略这种邻接关系。在正常的网络中，我们会将输入图像中的每个像素连接到下一层的神经元。在这样做的时候，我们没有利用图像中的像素由于某种原因靠近在一起并且具有特殊意义的事实。</p><p id="18f6" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">通过利用这种局部结构，我们的CNN学会了对图像中的局部模式进行分类，如形状和物体。</p><p id="b397" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">点记:</strong>在CNN的，没有定义滤镜。在训练过程中学习每个滤波器的值。</p><p id="1f80" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">为什么过滤器/内核是可学习的，内核是如何学习的？</strong></p><p id="bccd" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">在1988年之前的卷积神经网络的初始方法中，使用硬编码滤波器意味着滤波器不是可学习的滤波器。使用硬编码过滤器的问题在于，这些过滤器无法帮助full提取不同类型数据的要素地图。</p><p id="44f4" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">出于这个原因，一个可学习的内核开始使用。过滤器/内核在反向传播期间学习并改变它们的值，因此，它们的工作方式类似于多层感知器中的权重。</p><h2 id="f5d7" class="ly if hh bd ig lz ma mb ik mc md me io jn mf mg is jr mh mi iw jv mj mk ja ml bi translated">过滤深度</h2><p id="1dd4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">通常会有一个以上的过滤器。不同的滤镜拾取不同质量的补丁。例如，一个过滤器可能寻找特定的颜色，而另一个过滤器可能寻找一种特定形状的对象。卷积层中的滤波器数量被称为<strong class="je hi">滤波器深度</strong>。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mm"><img src="../Images/e392832a3ad3f5679b79d846c16062ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_wti-kB_HUmHzhJztzXrYQ.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">贴片连接到下一层的神经元</figcaption></figure><p id="84e5" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">每个贴片连接多少个神经元？</p><p id="9905" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">这取决于我们的过滤深度。如果我们有“n”个深度，我们将每个像素块连接到下一层的“n”个神经元。这给了我们下一层中“n”数的高度。实际上，“n”是我们调整的超参数，大多数CNN倾向于选择相同的起始值。</p><p id="2318" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">但为什么要将单个贴片连接到下一层的多个神经元呢？一个神经元还不够好吗？</strong></p><p id="941d" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">多个神经元可能是有用的，因为一个补丁可以有多个我们想要捕捉的有趣特征。</p><p id="2199" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">例如，一个补丁可能包括一些有趣的不同功能，如下图所示:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mn"><img src="../Images/262cbfeb09499f6ade052aac78e0790d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*P1jciADMR8Xzefll3XcEkg.jpeg"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">猫的这一部分包含许多有趣的特征，包括眼睛、鼻子、胡须等</figcaption></figure><p id="8748" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">给定的小块有多个神经元确保了我们的CNN可以学习捕捉CNN学习到的任何重要特征。</p><p id="8f2a" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">请记住，CNN的“编程”并不是为了寻找某些特征。相反，它<strong class="je hi">自己学习</strong>注意哪些特征。</p><h1 id="9efd" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">4.参数共享</h1><p id="e7ce" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">当我们试图对一张猫的图片进行分类时，我们并不关心猫在图片中的位置。如果在左上或者右下，在我们眼里还是猫。我们希望我们的CNN也拥有这种被称为翻译不变性的能力。如何才能实现这一点？</p><p id="53f2" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">正如我们前面看到的，图像中给定块的分类是由对应于该块的权重和偏差决定的。</p><p id="be63" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">如果我们想让一只在左上方的猫和一只在右下方的猫以同样的方式被分类，我们需要对应于这些补丁的权重和偏差是相同的，这样它们就以同样的方式被分类。</p><p id="685b" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">这正是我们在CNN所做的。我们为给定输出层学习的权重和偏差在给定输入层的所有面片中共享。请注意，随着我们增加滤波器的深度，我们需要学习的权重和偏差数量仍会增加，因为权重不会在输出通道之间共享。</p><p id="0d95" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">分享我们的参数还有一个额外的好处。如果我们不在所有片上重复使用相同的权重，我们将不得不为每个单独的片和隐藏层神经元对学习新的参数。这不能很好地缩放，尤其是对于高保真图像。因此，共享参数有助于我们保持平移不变性，并为我们提供一个更小、更可伸缩的模型。</p><h1 id="41e7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak"> 5。填充</strong></h1><p id="45e4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">填充是在输入图像上添加额外图层的过程。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mo"><img src="../Images/9404daac28bc2915393f6e9da7ad6752.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*oHgvFo-htU6YPKdtVXp91g.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图片来源:<a class="ae kt" href="https://www.geeksforgeeks.org/cnn-introduction-to-padding/" rel="noopener ugc nofollow" target="_blank"> GeeksforGeeks </a></figcaption></figure><p id="f45d" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">填充类型:</p><ol class=""><li id="554f" class="ka kb hh je b jf ko jj kp jn ll jr lm jv ln jz kf kg kh ki bi translated"><strong class="je hi">有效填充:</strong>这里我们不会对输入图像应用任何填充，这意味着原始输入图像的大小将保持不变，以供进一步处理。</li><li id="ab94" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated"><strong class="je hi">相同的填充:</strong>这里，我们添加填充层，使得输出图像与输入图像具有相同的尺寸。</li></ol><h1 id="7bb0" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">6.维度</h1><p id="58aa" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">从我们目前所了解到的，如何计算我们CNN中每一层的神经元数量？</p><p id="6fd2" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">假设我们的输入层的体积为<strong class="je hi"> V </strong>，我们的过滤器的体积(高度*宽度*深度)为<strong class="je hi"> F </strong>，我们的步幅为<strong class="je hi"> S </strong>，填充为<strong class="je hi"> P </strong></p><p id="26a8" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">下面的公式给我们下一层的体积:<strong class="je hi"> (V-F+2P)/S+1 </strong></p><p id="fa73" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">了解每个附加层的维度有助于我们了解模型有多大，以及我们围绕过滤器大小和步长的决策如何影响我们网络的规模</p><h1 id="8497" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">7.联营</h1><p id="2405" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">池化图层独立操作每个要素地图。池化通过降低要素地图的高度和宽度来降低要素地图的分辨率，但保留分类所需的地图要素。这叫做<strong class="je hi"><em class="mp"/></strong><em class="mp">。</em></p><p id="382f" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">联营的类型:</p><ol class=""><li id="ddde" class="ka kb hh je b jf ko jj kp jn ll jr lm jv ln jz kf kg kh ki bi translated"><strong class="je hi">最大池</strong>:当在跨度=2、过滤器尺寸为2X2的4X4特征图上进行最大池时，它会将特征图缩小到尺寸为2X2，并通过拾取过滤器在特征图上滑动时形成的度量中的最大数字来计算2X2特征图的值。</li></ol><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mq"><img src="../Images/b5347089a3462e19822517bd1e548c04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mozNNmxksWdnAQPiQZDHtQ.png"/></div></div></figure><p id="bdd4" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">2.平均池化:当平均池化在步幅=2且过滤器大小为2X2的4X4特征图上进行时，它将特征图缩小到大小为2X2，并且通过拾取过滤器在特征图上滑动时形成的度量的平均数来计算2X2特征图的值。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mr"><img src="../Images/0952b7f7028a98d2f27d7e0cab047653.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YGK49T1iKXDfWHmnoX3sIg.png"/></div></div></figure><p id="239c" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">3.全局池:它将特征图中的每个通道减少到单个值，它可以是全局最大池或全局平均池。</p><h1 id="bc20" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">9.神经网络中的卷积层</h1><p id="b24e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">上面我们学习了卷积层的重要部分，现在让我们建立一个卷积层结构</p><p id="4aa4" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">让我们通过创建一个简单的模型来理解CNN的架构:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ms"><img src="../Images/ba9a92826547653dcb31d8e04ec95828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bp2XcgtqlBKzGbLUxtvOsA.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">无填充和有填充的单卷积层结构。</figcaption></figure><p id="a32a" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">在这里，假设当应用大小为3×3的核并且步幅为1时，图像大小为5×5。可学习的核提取特征并输出3×3卷积矩阵或特征图。步幅=1的2×2池应用于卷积矩阵的顶部，并且度量值被展平为称为展平层的单个向量。</p><p id="4708" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">在上面的结构中，我在卷积图像或特征地图上应用填充，知道特征地图的大小是4X4，填充后应用合并，步幅=1，并使用展平层展平值</p><p id="1498" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">为什么需要汇集、填充？</strong></p><p id="81f7" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">当我们取stride =1时，在特征提取中会有重叠，这导致更多的数据是相关的，因此，通过应用汇集，我们可以在不丢失重要特征的情况下减少数据的大小。在我们的示例中，我们通过应用stride = 1的汇集，将3×3卷积矩阵减少为2×2数据</p><p id="2a9f" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">在我们的例子中，我们通过应用3X3内核/滤波器将5X5图像分解为3X3卷积矩阵。在某些情况下，我们不想这样做，因为这样做可能会丢失图像边缘的特征，或者我们必须保持相同的大小度量来进行矩阵乘法，因此，我们使用一种填充。在上述示例中，卷积特征被填充，大小为4X4。当我们在4X4卷积特征之上应用池化时，它会给出一个3X3的数据矩阵。这意味着我们保持与先前矩阵相同的大小。</p><p id="67d2" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">注:</strong></p><p id="4c9d" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">只能在要素地图/卷积图像上进行池化，但也可以在输入图像和要素地图上进行填充。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mt"><img src="../Images/6c9b4cc5c75942025f88f5fd07457e78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oi-H1M28ybM2lEU-8PBopg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">卷积神经网络的结构</figcaption></figure><p id="7767" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">CNN层试图使用内核、填充和池来学习从图像中提取什么。在CNN层之后，我们添加一个完全或前向连接的层，它学习CNN层根据权重提取的模式之间的关系，并给出输出。</p><p id="bdeb" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">图像只不过是一个RGB值的矩阵(就彩色图像而言)，当我们在图像上应用内核/过滤器时，内核开始提取图像的边缘或特征。如果我们应用多个滤波器，我们得到多个卷积图像/矩阵或在那些卷积矩阵之上的特征图。</p><p id="2dad" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">我们可以应用更多的卷积层、最大池化、批量归一化来提取特征图，最后，我们展平这些特征图，并添加完全连接的神经元，以获得图像的最终输出或分类。</p><p id="d1ec" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">我试着在上面介绍了单个卷积层的基本术语和架构。</p><h1 id="82ce" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">10.使用反向传播训练CNN</h1><ul class=""><li id="1dac" class="ka kb hh je b jf jg jj jk jn kc jr kd jv ke jz lo kg kh ki bi translated">我们用随机值初始化CNN层中的所有滤波器和全连接网络中的参数/权重</li><li id="15f8" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated">该网络将训练图像作为输入，经历正向传播步骤(卷积、ReLU和汇集操作以及全连接层中的正向传播)，并找到每一类的输出概率。</li><li id="f0b8" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated">在CNN架构中，我们使用卷积层和最大池，因此，我们必须确保卷积层和最大池都需要是可区分的，才能进行反向传播。</li><li id="d3db" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated">卷积层包含卷积算子和激活函数，卷积算子和激活函数都是可微的，最大池也是可微的。</li><li id="4c39" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated">得到产量后，我们可以计算实际产量(y)和预测output(y^之间的损失。通过得到损失函数，我们可以得到微分损失函数。</li><li id="bcdd" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated">由于权重是为第一个训练示例随机分配的，因此输出概率也是随机的。</li><li id="ef87" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated">计算输出图层的总误差。</li><li id="4cc8" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated">使用反向传播来计算网络中所有权重的误差的<em class="mp">梯度</em>，并使用<em class="mp">梯度下降</em>来更新所有滤波器值/权重和参数值，以最小化输出误差。</li></ul><p id="27e2" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">权重按照它们对总误差的贡献成比例地进行调整。</p><p id="5193" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">像过滤器数量、过滤器尺寸、网络结构等参数。在步骤1之前已经全部固定，并且在训练过程中不会改变——只有过滤器矩阵和连接权重的值得到更新。</p><ul class=""><li id="77a1" class="ka kb hh je b jf ko jj kp jn ll jr lm jv ln jz lo kg kh ki bi translated">对训练集中的所有图像重复反向传播，直到错误率变得最优。</li></ul><p id="248f" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">当一个新的(看不见的)图像被输入到ConvNet中时，网络将经历正向传播步骤，并输出每一类的概率(对于一个新的图像，使用已经被优化以正确分类所有先前训练示例的权重来计算输出概率)。如果我们的训练集足够大，网络将(有希望)很好地归纳新图像，并将它们分类到正确的类别中。</p><p id="9c51" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">我用简单的英语单词用反向传播解释了训练。如果你有兴趣详细了解训练的数学原理，请点击<a class="ae kt" href="https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><h1 id="31a9" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">11.CNN架构</h1><p id="606b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">卷积神经网络自20世纪90年代初就已经存在。</p><ul class=""><li id="e386" class="ka kb hh je b jf ko jj kp jn ll jr lm jv ln jz lo kg kh ki bi translated"><strong class="je hi">莱内特(上世纪90年代)</strong></li><li id="4c4b" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated"><strong class="je hi"> AlexNet (2012) </strong></li><li id="687c" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated"><strong class="je hi"> ZF网(2013) </strong></li><li id="e3cf" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated"><strong class="je hi">谷歌网(2014) </strong></li><li id="301b" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated"><strong class="je hi"> VGGNet (2014) </strong></li><li id="e4db" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated"><strong class="je hi">雷斯内特(2015) </strong></li><li id="6879" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz lo kg kh ki bi translated"><strong class="je hi">dense net(2016年8月)</strong></li></ul><h2 id="6934" class="ly if hh bd ig lz ma mb ik mc md me io jn mf mg is jr mh mi iw jv mj mk ja ml bi translated"><strong class="ak">完成这篇文章的人的额外参考资料:- </strong></h2><p id="2383" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">如果你是初学者，想直观地了解CNN，请访问这个网站<a class="ae kt" href="https://poloclub.github.io/cnn-explainer/" rel="noopener ugc nofollow" target="_blank"> CNN解说</a></p><p id="61fd" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">一旦你熟悉了CNN的基本架构，你就可以去参观<a class="ae kt" href="https://tensorspace.org/html/playground/lenet.html" rel="noopener ugc nofollow" target="_blank"> TensorSpace游乐场</a>。</p><p id="9d11" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">感谢您阅读这篇文章…</p><h1 id="f094" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考:</h1><ol class=""><li id="9317" class="ka kb hh je b jf jg jj jk jn kc jr kd jv ke jz kf kg kh ki bi translated"><a class="ae kt" href="https://github.com/nehal96/Deep-Learning-ND-Exercises/blob/master/Convolutional%20Neural%20Networks/convolutional-neural-networks-notes.md" rel="noopener ugc nofollow" target="_blank">https://github . com/nehal 96/Deep-Learning-ND-Exercises/blob/master/convolutionary % 20 neural % 20 networks/convolutionary-neural-networks-notes . MD</a></li></ol><div class="mu mv ez fb mw mx"><a href="https://www.geeksforgeeks.org/cnn-introduction-to-padding/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hi fi z dy nc ea eb nd ed ef hg bi translated">CNN |填充介绍- GeeksforGeeks</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">CNN |简单卷积层填充问题简介填充输入图像填充只是一个过程…</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">www.geeksforgeeks.org</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl le mx"/></div></div></a></div><p id="c25e" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">3.<a class="ae kt" href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" rel="noopener ugc nofollow" target="_blank">https://ujjwalkarn . me/2016/08/11/直观-解释-convnets/ </a></p><p id="ece8" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">4.<a class="ae kt" href="https://www.tensorflow.org/tutorials/images/cnn" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/images/cnn</a></p><p id="a462" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">5.<a class="ae kt" href="https://developersbreach.com/convolution-neural-network-deep-learning/" rel="noopener ugc nofollow" target="_blank">https://developer breach . com/convolution-neural-network-deep-learning/</a></p><p id="57b9" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">5.<a class="ae kt" href="https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">CNN中的反向传播</a></p><p id="22b9" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">6.<a class="ae kt" href="https://poloclub.github.io/cnn-explainer/" rel="noopener ugc nofollow" target="_blank"> CNN解说</a></p></div></div>    
</body>
</html>