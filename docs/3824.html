<html>
<head>
<title>Web Scraping With Python: Beginner to Advanced.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python进行Web抓取:初学者到高级。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/web-scraping-with-python-beginner-to-advanced-10daaca021f3?source=collection_archive---------1-----------------------#2021-07-30">https://medium.com/analytics-vidhya/web-scraping-with-python-beginner-to-advanced-10daaca021f3?source=collection_archive---------1-----------------------#2021-07-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="7227" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">更多的数据更多的机器学习。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/818aaf360faf9cb5a0f9cbcb099f79b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*idMHrJ2Njbuup2yr.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://www.smallbizdaily.com/7-ways-how-web-scraping-helps-retailers/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="281e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">副标题引用是一个事实，如果你有更多关于你的问题的数据，你可以提取更多关于问题的信息，可以更准确地解决它。所以网络抓取是一种从网络中提取数据的技术。</p><h1 id="e1ac" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">目录</h1><ol class=""><li id="ebee" class="lc ld hi jq b jr le ju lf jx lg kb lh kf li kj lj lk ll lm bi translated">什么是网页抓取？</li><li id="d48b" class="lc ld hi jq b jr ln ju lo jx lp kb lq kf lr kj lj lk ll lm bi translated">我们如何从网站上删除数据？</li><li id="c834" class="lc ld hi jq b jr ln ju lo jx lp kb lq kf lr kj lj lk ll lm bi translated">在python中执行web抓取</li></ol><ul class=""><li id="61ab" class="lc ld hi jq b jr js ju jv jx ls kb lt kf lu kj lv lk ll lm bi translated">使用硒和美丽素的网刮</li><li id="e634" class="lc ld hi jq b jr ln ju lo jx lp kb lq kf lr kj lv lk ll lm bi translated">使用request_html的Web抓取</li><li id="2be7" class="lc ld hi jq b jr ln ju lo jx lp kb lq kf lr kj lv lk ll lm bi translated">使用scrapy进行网页抓取</li></ul><h1 id="ca08" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">1.什么是网页抓取？</h1><p id="578e" class="pw-post-body-paragraph jo jp hi jq b jr le ij jt ju lf im jw jx lw jz ka kb lx kd ke kf ly kh ki kj hb bi translated">形式定义</p><blockquote class="lz ma mb"><p id="41cc" class="jo jp mc jq b jr js ij jt ju jv im jw md jy jz ka me kc kd ke mf kg kh ki kj hb bi translated"><strong class="jq hj">网络抓取是一种用于从网站中提取大量数据的自动化方法。</strong></p></blockquote><p id="2e75" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">网站上的数据通常是非结构化的。Web抓取是一种收集非结构化数据并将其以结构化形式存储的技术。</p><h1 id="1c15" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">2.现在的问题是我们如何从网站上获取数据？</h1><p id="153a" class="pw-post-body-paragraph jo jp hi jq b jr le ij jt ju lf im jw jx lw jz ka kb lx kd ke kf ly kh ki kj hb bi translated">当运行任何web抓取代码时，请求会被发送到您提到的URL。网站通过发送数据来响应请求，并允许它读取XML或HTML页面。然后，代码将从XML或HTML页面中提取所需的数据。</p><p id="4741" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">任何网页抓取代码都需要遵循基本步骤:</p><ol class=""><li id="c6e7" class="lc ld hi jq b jr js ju jv jx ls kb lt kf lu kj lj lk ll lm bi translated">找到你想要抓取的网页的URL(地址)</li><li id="a478" class="lc ld hi jq b jr ln ju lo jx lp kb lq kf lr kj lj lk ll lm bi translated">检查页面并找到要提取的数据</li><li id="231e" class="lc ld hi jq b jr ln ju lo jx lp kb lq kf lr kj lj lk ll lm bi translated">编写提取数据的逻辑</li><li id="93d6" class="lc ld hi jq b jr ln ju lo jx lp kb lq kf lr kj lj lk ll lm bi translated">将提取的数据存储为结构化形式(例如熊猫数据帧)</li></ol><p id="2d44" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们将使用各种库和框架来执行web抓取。</p><h1 id="ac7e" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">3.使用多个库执行web抓取</h1><h2 id="7134" class="mg kl hi bd km mh mi mj kq mk ml mm ku jx mn mo kw kb mp mq ky kf mr ms la mt bi translated">网刮使用硒和美丽的汤</h2><p id="71eb" class="pw-post-body-paragraph jo jp hi jq b jr le ij jt ju lf im jw jx lw jz ka kb lx kd ke kf ly kh ki kj hb bi translated">让我来介绍一下每个库</p><ul class=""><li id="b7a2" class="lc ld hi jq b jr js ju jv jx ls kb lt kf lu kj lv lk ll lm bi translated"><strong class="jq hj"> Selenium: </strong> Selenium是一个web测试库。它用于自动化浏览器活动。</li><li id="40fb" class="lc ld hi jq b jr ln ju lo jx lp kb lq kf lr kj lv lk ll lm bi translated"><strong class="jq hj">Beautiful Soup:</strong>Beautiful Soup是一个解析HTML和XML文档的Python包。它创建解析树，有助于轻松提取数据。</li><li id="a8ce" class="lc ld hi jq b jr ln ju lo jx lp kb lq kf lr kj lv lk ll lm bi translated">Pandas:  Pandas是一个用于数据操作和分析的库。它用于提取数据并以所需的格式存储数据。</li></ul><p id="9686" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们从flipkart网站获取关于笔记本电脑的信息。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mu"><img src="../Images/79cf702035e1c40391886368176adcde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fJnfd9zFUxncYDGgv0hBhg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">Flipkart网页</figcaption></figure><p id="284a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">假设我们需要笔记本电脑名称和价格等信息</p><p id="7fe8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们通过导入所需的库来开始编写代码</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="53da" class="mg kl hi mw b fi na nb l nc nd">from selenium import webdriver<br/>from bs4 import BeautifulSoup<br/>import pandas as pd</span></pre><p id="20e0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">要配置webdriver使用Chrome浏览器，我们必须设置chromedriver的路径</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="3347" class="mg kl hi mw b fi na nb l nc nd">driver = webdriver.Chrome('/usr/lib/chromium-browser/chromedriver')</span></pre><p id="fe0e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在让我们列出一些存储数据和获取内容的列表</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="114b" class="mg kl hi mw b fi na nb l nc nd">products=[] #List to store name of the product<br/>prices=[] #List to store price of the product<br/>driver.get('<a class="ae jn" href="https://www.flipkart.com/search?q=Laptops&amp;otracker=search&amp;otracker1=search&amp;marketplace=FLIPKART&amp;as-show=off&amp;as=off&amp;as-pos=1&amp;as-type=HISTORY&amp;as-backfill=on'" rel="noopener ugc nofollow" target="_blank">https://www.flipkart.com/search?q=Laptops&amp;otracker=search&amp;otracker1=search&amp;marketplace=FLIPKART&amp;as-show=off&amp;as=off&amp;as-pos=1&amp;as-type=HISTORY&amp;as-backfill=on'</a>)<br/>content = driver.page_source<br/>soup = BeautifulSoup(content)</span></pre><p id="bbb7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们有了网页的内容，所以我们需要从中获取有用的数据。</p><p id="d480" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这里要注意的是，我们需要类名来从网页中找到特定的内容，我们可以通过检查网页来找到它</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mu"><img src="../Images/96a0933e22bd63489dd8ba9d89349ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W31Z_a4-cuyHnRgbk0ED0g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">正在查找类ID</figcaption></figure><p id="a559" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在上面的图像中，显示了特定笔记本电脑内容的id，类似地，您可以找到所有的id，即名称和价格。</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="15d8" class="mg kl hi mw b fi na nb l nc nd">for a in soup.findAll(attrs={'class':'_1fQZEK'}):<br/>    name=a.find('div', attrs={'class':'_4rR01T'})<br/>    price=a.find('div', attrs={'class':'_30jeq3 _1_WHN1'})<br/>    products.append(name.text)<br/>    prices.append(price.text)</span></pre><p id="9b1e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这里，循环在所有类别id为<strong class="jq hj"> _1fQZEK </strong>的<div>标签上迭代，在那个&lt; div &gt;中，它们是名称和价格的其他子&lt; div &gt;标签。此外，我们会将该内容添加到我们的受尊重列表中。</div></p><p id="7985" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，为了使这些数据成为结构化的形式，我们将它存储到pandas dataframe中，并进一步保存为csv文件。</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="6f68" class="mg kl hi mw b fi na nb l nc nd">df = pd.DataFrame({'Product Name':products,'Price':prices})<br/>df.to_csv('products.csv', index=False, encoding='utf-8')</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mu"><img src="../Images/9983beeeb1140255e7979728f5b664c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q_beT1_k9otCyLCEqclIFg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">数据</figcaption></figure><h2 id="05e4" class="mg kl hi bd km mh mi mj kq mk ml mm ku jx mn mo kw kb mp mq ky kf mr ms la mt bi translated">使用requests_html的Web抓取</h2><p id="5301" class="pw-post-body-paragraph jo jp hi jq b jr le ij jt ju lf im jw jx lw jz ka kb lx kd ke kf ly kh ki kj hb bi translated">我们可以通过使用单个库从网站抓取数据，即我们将使用request_html抓取数据</p><p id="9bc9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">假设我们想从github repositories页面获取存储库名称、使用的语言和日期。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ne"><img src="../Images/0a7d78bba0e5c371b3df44ffd5527122.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sc079RWJNPj4MPLwB9mgHA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">Github页面</figcaption></figure><p id="2cb6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们直接进入代码</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="cff7" class="mg kl hi mw b fi na nb l nc nd">import requests_html<br/>from requests_html import HTMLSession<br/>import pandas as pd</span></pre><p id="0b53" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这些只是我们需要的进口货。</p><p id="fec6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在让我们创建html会话对象，设置URL并获取内容</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="cd8f" class="mg kl hi mw b fi na nb l nc nd">session = HTMLSession()<br/>url = '<a class="ae jn" href="https://github.com/kamlesh11?tab=repositories'" rel="noopener ugc nofollow" target="_blank">https://github.com/kamlesh11?tab=repositories'</a><br/>response = session.get(url)</span></pre><p id="aa43" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们需要从网页中找到对我们真正有用的数据。</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="42e8" class="mg kl hi mw b fi na nb l nc nd">container = response.html.find('#user-repositories-list',first=True)<br/># #user-repositories-list represents id of &lt;div&gt; tag</span></pre><p id="6d47" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">所以我们提取id = user-repositories-list的<div>标签中的所有内容。</div></p><p id="9b44" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">此外，我们只需要Li标签</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="42d4" class="mg kl hi mw b fi na nb l nc nd">list = container.find('li')</span></pre><p id="e89c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在让我们创建存储信息的列表。</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="7f78" class="mg kl hi mw b fi na nb l nc nd">name = [] # for storing name of repository<br/>lang = [] # language used in code<br/>date = [] # date updated<br/>for item in list:    <br/>    tmp = item.text.split('\n')<br/>    name.append(tmp[0])<br/>    lang.append(tmp[1])<br/>    date.append(tmp[2])</span></pre><p id="4768" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在创建数据框来组织数据，并进一步保存为csv文件。</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="1f1d" class="mg kl hi mw b fi na nb l nc nd">df = pd.DataFrame({'Name':name,'language':lang,'date':date})<br/>df.to_csv('repositories.csv')</span></pre><p id="54f0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">已创建csv文件</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mu"><img src="../Images/359484b3d2036e9fe0b4117cc99dfb0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x7C_hQCIIZUozWwAx1zs5g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">存储库csv文件</figcaption></figure><p id="497f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们将学习Scrapy一个网页抓取框架。</p><h2 id="5d04" class="mg kl hi bd km mh mi mj kq mk ml mm ku jx mn mo kw kb mp mq ky kf mr ms la mt bi translated">使用scrapy进行网页抓取</h2><p id="0544" class="pw-post-body-paragraph jo jp hi jq b jr le ij jt ju lf im jw jx lw jz ka kb lx kd ke kf ly kh ki kj hb bi translated">Scrapy是一个用于大规模网页抓取的<a class="ae jn" href="http://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&amp;utm_medium=WebScrapinginPythonarticle" rel="noopener ugc nofollow" target="_blank"> Python </a>框架。它为您提供了高效地从网站中<strong class="jq hj"> <em class="mc">提取</em> </strong>数据所需的所有工具，<strong class="jq hj"> <em class="mc">根据您的需要处理</em> </strong>，并以您喜欢的<strong class="jq hj"> <em class="mc">结构</em> </strong>和格式存储它们。</p><p id="58ee" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">设置您的系统</strong></p><p id="2725" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">Scrapy支持Python 2和3两个版本。如果你使用的是Anaconda，你可以从conda-forge频道安装这个包，那里有针对Linux、Windows和OS X的最新包。</p><p id="f2da" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">要使用conda安装Scrapy，请运行:</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="6837" class="mg kl hi mw b fi na nb l nc nd">conda install -c conda-forge scrapy</span></pre><p id="f171" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">或者，如果你使用的是Linux或Mac OSX，你可以通过以下方式直接安装scrapy:</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="b4b7" class="mg kl hi mw b fi na nb l nc nd">pip install scrapy</span></pre><p id="a33d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们已经设置好了我们的系统，让我们用scrapy来执行网页抓取。</p><h2 id="0bb3" class="mg kl hi bd km mh mi mj kq mk ml mm ku jx mn mo kw kb mp mq ky kf mr ms la mt bi translated">用Scrapy刮网</h2><p id="dace" class="pw-post-body-paragraph jo jp hi jq b jr le ij jt ju lf im jw jx lw jz ka kb lx kd ke kf ly kh ki kj hb bi translated">我们再从flipkart中提取一些数据。这次我们将提取iphones的数据。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nf"><img src="../Images/4fb10d7290cf715e5a9750c65f09cbc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ce--xO9c0A9JQoRimzWGVA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">Flipkart iphone页面</figcaption></figure><p id="d5f6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们将提取iphones的名称和评级。</p><p id="9b7a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了提取数据，我们必须启动scrapy shell，我们可以通过以下命令启动它</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="3f16" class="mg kl hi mw b fi na nb l nc nd">scrapy shell</span></pre><p id="1cf8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们需要添加我们想要提取的网页的URL。在这种情况下，我们希望提取Flipkart iphone页面。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ng"><img src="../Images/f4f1557ae39af0eb489736d46988e192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EnM3MZmd-wrWPud--5NbrQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">取得</figcaption></figure><p id="6155" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在让我们看看回应</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="f8bd" class="mg kl hi mw b fi na nb l nc nd">response.text</span></pre><p id="3c6c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">以上代码将以XML或HTML格式打印页面内容。现在我们有了所有的内容，让我们开始收集所需的数据。让我们先刮一下iphones的名字。</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="7b5b" class="mg kl hi mw b fi na nb l nc nd">response.css('._4rR01T::text').extract()</span></pre><p id="ad6b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">给你。_ 4rR01T是具有phones标题的类名。你可以通过检查得到这个东西。您可以在本文的第一部分了解这些信息。</p><h2 id="b383" class="mg kl hi bd km mh mi mj kq mk ml mm ku jx mn mo kw kb mp mq ky kf mr ms la mt bi translated">输出:-</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nh"><img src="../Images/a4472d5b4ce8b8915a806f4e9745fc34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*srPonFcYJBQ0tpox5sjaHA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">Iphone名称</figcaption></figure><p id="2840" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在让我们提取所有iphones的评级。</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="0480" class="mg kl hi mw b fi na nb l nc nd">response.css('._3LWZlK::text').extract()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ni"><img src="../Images/cea432a6b3fcd36a7c6ec948dd92e9c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*afVFPQ1ZxQh8wUdweCCRFA.png"/></div></div></figure><p id="25e3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">同样，我们也可以提取其他数据。</p><p id="6a2f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，我们已经在shell中完成了所有这些工作，但是我们实际上需要csv文件中的数据，而不是shell中的数据。为此，我们需要开始新的项目。</p><p id="8777" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们开始新的项目。</p><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="c64e" class="mg kl hi mw b fi na nb l nc nd">scrapy startproject Iphone</span></pre><p id="f5c3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们需要编写抓取数据的蜘蛛。</p><h2 id="60b1" class="mg kl hi bd km mh mi mj kq mk ml mm ku jx mn mo kw kb mp mq ky kf mr ms la mt bi translated">等等，蜘蛛是什么？</h2><p id="e4c9" class="pw-post-body-paragraph jo jp hi jq b jr le ij jt ju lf im jw jx lw jz ka kb lx kd ke kf ly kh ki kj hb bi translated">蜘蛛是一种从网站或给定的URL下载内容的程序。当提取大规模的数据时，你需要为不同的网站编写定制的蜘蛛，因为由于网站设计的多样性，没有“一刀切”的方法。您还需要编写代码，将提取的数据转换为结构化格式，并将其存储为可重用的格式，如CSV、JSON、excel等。那要写很多代码，幸运的是<strong class="jq hj"> <em class="mc"> scrapy自带了</em> </strong>中内置的大部分功能。</p><h2 id="fb66" class="mg kl hi bd km mh mi mj kq mk ml mm ku jx mn mo kw kb mp mq ky kf mr ms la mt bi translated">创造一只蜘蛛</h2><pre class="iy iz ja jb fd mv mw mx my aw mz bi"><span id="c75a" class="mg kl hi mw b fi na nb l nc nd">scrapy genspider iphone <a class="ae jn" href="http://www.flipkart.com/search?q=iphone" rel="noopener ugc nofollow" target="_blank">www.flipkart.com/search?q=iphone</a></span></pre><p id="69e8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这里，我们正在创建iphone蜘蛛。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nj"><img src="../Images/ffdf244cb315007c4acfe33dc69be1bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S77cpg7KbmwFI8ZgEj9u8Q.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">创建iphone蜘蛛</figcaption></figure><p id="287e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在你会看到一个名为Iphone的文件夹，里面有很多文件，你还会发现一个名为spiders的文件夹，蜘蛛就存储在那里。在我们的例子中，我们只创建了一个名为iphone的蜘蛛。因此，这里的iphone.py文件将被发现，为每一个新的蜘蛛，它将创建新的蜘蛛文件。</p><p id="0127" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们来看看iphone spider里面有哪些代码。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nk"><img src="../Images/e29c3d58b9f068f99e920fe52247c3b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vRY_PmX6H6Sm_I_LgiIzsA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">Iphone蜘蛛</figcaption></figure><p id="6efd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这里，每次我们抓取项目蜘蛛将运行，数据将从网站或网站刮。现在我们将编写一些代码来收集数据。</p><p id="2777" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们需要从flipkart页面提取iphones的名称和评级。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nl"><img src="../Images/f459b9491f39156b6c14cf7c2789dcf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*n01jYZGRNF04ljttcSX_Sg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">添加的代码</figcaption></figure><p id="c267" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这里，我们抓取的名称和评级与我们在scrapy shell中编写的相同，并添加了一些以csv格式保存数据的代码。</p><p id="e3dc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">请注意，在抓取数据时，我们可能会获得或多或少的内容，因为数据是非结构化形式的，所以我们会获得24个iphone名称和39个评级。这可能会发生，因为该类也可能在其他一些地方使用，所以数量会有所不同。</p><p id="bdf3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在让我们看看csv文件</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mu"><img src="../Images/ebc847b1611c652e66e777f8fcf9d701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NTGUf9UVjxpkTa2dbqya4Q.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">Iphone CSV</figcaption></figure><p id="f116" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果你想获得更多关于刺儿头的信息，我建议你阅读这篇<a class="ae jn" href="https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#:~:text=Scrapy%20is%20a%20Python%20framework,your%20preferred%20structure%20and%20format." rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj"> <em class="mc">文章</em> </strong> </a>。</p><p id="d566" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">祝贺你走了这么远，我希望你在这篇文章中获得了很多信息。</p><p id="e30f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">感谢阅读。😃</p></div></div>    
</body>
</html>