<html>
<head>
<title>Sentiment Analysis of Movie Reviews pt.3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">电影评论的情感分析第三部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sentiment-analysis-of-movie-reviews-b5241ca736b7?source=collection_archive---------18-----------------------#2021-01-26">https://medium.com/analytics-vidhya/sentiment-analysis-of-movie-reviews-b5241ca736b7?source=collection_archive---------18-----------------------#2021-01-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="160f" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">第3部分— n-gram</h2></div><p id="a53f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">更多代码链接到我的Github:<a class="ae js" href="https://github.com/charliezcr/Sentiment-Analysis-of-Movie-Reviews/blob/main/sa_p1.ipynb" rel="noopener ugc nofollow" target="_blank">https://Github . com/charliezcr/情操分析-电影评论/blob/main/sa_p3.ipynb </a></p><h2 id="c379" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">n元语法</h2><p id="df92" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">在<a class="ae js" href="https://crzheng97.medium.com/sentiment-analysis-of-movie-reviews-pt-1-1a52daa90cdc" rel="noopener"> part.1 </a>的文本预处理步骤中，我们对评论中的单词进行了逐个分词。比如<em class="kt">‘非常无聊的电影’</em>会被令牌化为<em class="kt">【非常’，【无聊】，【电影】】</em>。</p><p id="1d30" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这种模型被称为unigram模型，因为我们一次只接受一个令牌。然而，在n-gram模型中还有其他的单词标记方法。我们可以一次取一系列的记号。例如，在bigram (2-gram)模型中，<em class="kt">'非常无聊的电影'</em>将被标记为<em class="kt"> ['非常无聊'，'无聊的电影'] </em>。</p><p id="e0dd" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在三元模型中，<em class="kt">“非常无聊的电影”</em>将被标记为单个标记<em class="kt">“非常无聊的电影”</em>。</p><p id="4bb7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">N-gram模型有助于我们的情感分析，因为单词序列可能包含更重要的分类语义。例如，单字very本身并不包含任何情感。“无聊”意味着评论讨厌这部电影。然而，“非常无聊”传达了评论者真的讨厌这部电影，更多的只是“无聊”。“非常无聊”应与“无聊”区别对待，因为它包含了更强烈的情感。所以我们需要找到好的n元模型来做情感分析。</p><h1 id="ce11" class="ku ju hh bd jv kv kw kx jz ky kz la kd in lb io kg iq lc ir kj it ld iu km le bi translated">调谐参数</h1><p id="3b57" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">在Scikit-learn的TfidfVectorizer中，我们可以通过传入参数(最小n和最大n的元组)来选择n元模型。例如，(1，1)意味着我们只使用unigram模型，因为最小n和最大n都是1。(1，3)意味着我们一起使用一元模型、二元模型和三元模型。比如<em class="kt">'非常无聊的电影'</em>会被令牌化为<em class="kt"> ['非常'，'无聊'，'电影'，'非常无聊'，'无聊的电影'，'非常无聊的电影']。</em></p><p id="1391" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，我们可以将第1部分中的预处理和分类功能细化如下:</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="9460" class="jt ju hh lk b fi lo lp l lq lr"><strong class="lk hi">from</strong> nltk.stem <strong class="lk hi">import</strong> PorterStemmer    <em class="kt"># stem the words</em><br/><strong class="lk hi">from</strong> nltk.tokenize <strong class="lk hi">import</strong> word_tokenize <em class="kt"># tokenize the sentences into tokens</em><br/><strong class="lk hi">from</strong> string <strong class="lk hi">import</strong> punctuation<br/><strong class="lk hi">from</strong> sklearn.feature_extraction.text <strong class="lk hi">import</strong> TfidfVectorizer <em class="kt"># vectorize the texts</em><br/><strong class="lk hi">from</strong> sklearn.model_selection <strong class="lk hi">import</strong> train_test_split <em class="kt"># split the testing and training sets</em></span><span id="a758" class="jt ju hh lk b fi ls lp l lq lr"><strong class="lk hi">def</strong> preprocess(path, ngram):<br/>    '''generate cleaned dataset<br/>    <br/>    Args:<br/>        path(string): the path of the file of testing data<br/>        ngram(tuple (min_n, max_n)): the range of n-gram model</span><span id="b33a" class="jt ju hh lk b fi ls lp l lq lr">    Returns:<br/>        X_train (list): the list of features of training data<br/>        X_test (list): the list of features of test data<br/>        y_train (list): the list of targets of training data ('1' or '0')<br/>        y_test (list): the list of targets of training data ('1' or '0')<br/>    '''<br/>    <br/>    <em class="kt"># text preprocessing: iterate through the original file and </em><br/>    <strong class="lk hi">with</strong> open(path, encoding<strong class="lk hi">=</strong>'utf-8') <strong class="lk hi">as</strong> file:<br/>        <em class="kt"># record all words and its label</em><br/>        labels <strong class="lk hi">=</strong> []<br/>        preprocessed <strong class="lk hi">=</strong> []<br/>        <strong class="lk hi">for</strong> line <strong class="lk hi">in</strong> file:<br/>            <em class="kt"># get sentence and label</em><br/>            sentence, label <strong class="lk hi">=</strong> line<strong class="lk hi">.</strong>strip('\n')<strong class="lk hi">.</strong>split('\t')<br/>            labels<strong class="lk hi">.</strong>append(int(label))<br/>            <br/>            <em class="kt"># remove punctuation and numbers</em><br/>            <strong class="lk hi">for</strong> ch <strong class="lk hi">in</strong> punctuation<strong class="lk hi">+</strong>'0123456789':<br/>                sentence <strong class="lk hi">=</strong> sentence<strong class="lk hi">.</strong>replace(ch,' ')<br/>            <em class="kt"># tokenize the words and stem them</em><br/>            words <strong class="lk hi">=</strong> []<br/>            <strong class="lk hi">for</strong> w <strong class="lk hi">in</strong> word_tokenize(sentence):<br/>                words<strong class="lk hi">.</strong>append(PorterStemmer()<strong class="lk hi">.</strong>stem(w))<br/>            preprocessed<strong class="lk hi">.</strong>append(' '<strong class="lk hi">.</strong>join(words))<br/>    <br/>    <em class="kt"># vectorize the texts</em><br/>    vectorizer <strong class="lk hi">=</strong> TfidfVectorizer(stop_words<strong class="lk hi">=</strong>'english', sublinear_tf<strong class="lk hi">=True</strong>, ngram_range<strong class="lk hi">=</strong>ngram)<br/>    X <strong class="lk hi">=</strong> vectorizer<strong class="lk hi">.</strong>fit_transform(preprocessed)<br/>    <em class="kt"># split the testing and training sets</em><br/>    X_train, X_test, y_train, y_test <strong class="lk hi">=</strong> train_test_split(X, labels, test_size<strong class="lk hi">=</strong>0.2)<br/>    <strong class="lk hi">return</strong> X_train, X_test, y_train, y_test</span><span id="257e" class="jt ju hh lk b fi ls lp l lq lr"><strong class="lk hi">from</strong> sklearn.metrics <strong class="lk hi">import</strong> accuracy_score<br/><strong class="lk hi">def</strong> classify(clf, todense<strong class="lk hi">=False</strong>):<br/>    '''to classify the data using machine learning models<br/>    <br/>    Args:<br/>        clf: the model chosen to analyze the data<br/>        todense(bool): whether to make the sparse matrix dense<br/>        <br/>    '''<br/>    clf<strong class="lk hi">.</strong>fit(X_train, y_train)<br/>    y_pred <strong class="lk hi">=</strong> clf<strong class="lk hi">.</strong>predict(X_test)<br/>    accuracy <strong class="lk hi">=</strong> accuracy_score(y_test,y_pred)<br/>    <strong class="lk hi">return</strong> accuracy</span></pre><h1 id="6696" class="ku ju hh bd jv kv kw kx jz ky kz la kd in lb io kg iq lc ir kj it ld iu km le bi translated">朴素贝叶斯分类器</h1><p id="9e34" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">因为从第1部分来看，多项式朴素贝叶斯分类器是快速而准确的。我们将使用MultinomialNB作为调整参数的基准模型。我们可以将不同的参数元组(从(1，1)到(3，3)传递给分类器，并在熊猫数据帧中记录性能，如下所示:</p><p id="7c64" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在[40]:</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="c960" class="jt ju hh lk b fi lo lp l lq lr"><strong class="lk hi">from</strong> sklearn.naive_bayes <strong class="lk hi">import</strong> MultinomialNB<br/><strong class="lk hi">import</strong> pandas <strong class="lk hi">as</strong> pd<br/><em class="kt"># create a dictionary to record the accuracy for each ngram_range</em><br/>d <strong class="lk hi">=</strong> {}<br/><em class="kt"># iterate through each ngram_range</em><br/><strong class="lk hi">for</strong> ngram <strong class="lk hi">in</strong> [(1,1),(1,2),(1,3),(2,2),(2,3),(3,3)]:<br/>    X_train, X_test, y_train, y_test <strong class="lk hi">=</strong> preprocess('imdb_labelled.txt',ngram)<br/>    d[str(ngram)] <strong class="lk hi">=</strong> [classify(MultinomialNB())]<br/>df <strong class="lk hi">=</strong> pd<strong class="lk hi">.</strong>DataFrame(data<strong class="lk hi">=</strong>d)</span></pre><figure class="lf lg lh li fd lu er es paragraph-image"><div class="er es lt"><img src="../Images/e916515c58a34d576962235b7b7e38e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*EINEpSP-UMutWXfg5DFqQw.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">结果</figcaption></figure><p id="61e0" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以看到，我们必须包含unigram，因为(1，1)，(1，2)和(1，3)取得了很大的结果。(2，2)的表现一般。(2，3)，(3，3)的准确率都降到0.5了，也就是说没用了。</p><h1 id="f5ec" class="ku ju hh bd jv kv kw kx jz ky kz la kd in lb io kg iq lc ir kj it ld iu km le bi translated">缓和</h1><p id="77dc" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">在多项式lNB模型中，我们可以通过调整拉普拉斯平滑的平滑参数αα来探索更好的结果。关于拉普拉斯平滑更详细的介绍，可以参考这篇<a class="ae js" href="https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece" rel="noopener" target="_blank">文章</a>。我们可以从列表[0.1，0.5，1，1.5，2，2.5]中选择αα，从(1，1)，(1，2)，(1，3)中选择n元模型。然后，运行情感分析并在熊猫数据框架中记录准确性。这样，我们就可以找到最佳的一对参数。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="42f5" class="jt ju hh lk b fi lo lp l lq lr">alpha_list <strong class="lk hi">=</strong> [0.1,0.5,1,1.5,2,2.5]<br/>d <strong class="lk hi">=</strong> {'alpha':alpha_list}<br/><strong class="lk hi">for</strong> ngram <strong class="lk hi">in</strong> [(1,1),(1,2),(1,3)]:<br/>    acc <strong class="lk hi">=</strong> []<br/>    <strong class="lk hi">for</strong> value <strong class="lk hi">in</strong> alpha_list:<br/>        X_train, X_test, y_train, y_test <strong class="lk hi">=</strong> preprocess('imdb_labelled.txt',ngram)<br/>        acc<strong class="lk hi">.</strong>append(classify(MultinomialNB(alpha <strong class="lk hi">=</strong> value)))<br/>    d[ngram] <strong class="lk hi">=</strong> acc<br/>df <strong class="lk hi">=</strong> pd<strong class="lk hi">.</strong>DataFrame(data<strong class="lk hi">=</strong>d)</span></pre><figure class="lf lg lh li fd lu er es paragraph-image"><div class="er es mb"><img src="../Images/d9379f674c47e5938884a7129fa02255.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*AxdwW736Au4XT_fAhMByqQ.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">结果</figcaption></figure></div></div>    
</body>
</html>