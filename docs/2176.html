<html>
<head>
<title>Capturing the Elusive World of Sound: Fetch and Transcribe Google Podcasts with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">捕捉难以捉摸的声音世界:用Python获取和转录谷歌播客</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/capturing-the-elusive-world-of-sound-fetch-and-transcribe-google-podcasts-with-python-76dcf87b6eff?source=collection_archive---------3-----------------------#2021-04-10">https://medium.com/analytics-vidhya/capturing-the-elusive-world-of-sound-fetch-and-transcribe-google-podcasts-with-python-76dcf87b6eff?source=collection_archive---------3-----------------------#2021-04-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="cb85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了继续寻找更多的数据，我开始研究声音的短暂世界。我曾经对电视上的自动字幕感到惊讶。它们是如此准确，几乎没有任何延迟，这让我怀疑它是自动化的，但我们不能让某人为每个程序键入所有这些单词，对吗？显然，我们早就有这项技术了。如果我们能记下人们说的每一句话，那将意味着有更多的数据可以利用！</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/5fe9c2a0f3f71da61bb50caa4e8f9d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Gk-ur9lUI11CgztHiW8Mw.jpeg"/></div></div></figure><p id="6a9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">电视节目似乎有点难以企及，所以对于初学者来说，播客看起来就像一束束很好的音频数据，我们可以转录它，以便固定每个单词进行进一步的文本分析。(显然，现在你留下的任何数字痕迹都可能被用来对付你😨连随便说的话都不安全…)</p><p id="408f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我的目标是批量获取和转录文件，所以我希望简化这个过程。这包括两个部分。首先是数据收集。我从谷歌播客开始收集音频剪辑，因为它有一个可访问的文件下载结构。MP3文件链接被很好地包装在页面源代码中—</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jp"><img src="../Images/ba88cc4a020f421694027aea94ffd4bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-GSZOeHPzBX8jd8DFBXYJQ.png"/></div></div></figure><p id="f472" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二部分是转录。我选择使用<a class="ae jq" href="https://cloud.google.com/speech-to-text/docs/basics" rel="noopener ugc nofollow" target="_blank"> Google Cloud语音转文本</a> API和SpeechRecognition库(各种API的包装器)来访问它。</p><h1 id="0ac4" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">1.抓取播客</h1><p id="0eee" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">这是我们在这部分需要的:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="868c" class="kz js hi kv b fi la lb l lc ld">import requests<br/>from bs4 import BeautifulSoup<br/>import wget # to download files<br/>import pandas as pd</span></pre><p id="2d1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于我们希望下载播客的节目，我们可以简单地将它们的主页URL放入一个列表中，然后逐个进行请求。为了一次有效地获取所有播客，我们需要收藏页面，而不是特定剧集的页面。这种URL的一个例子是<a class="ae jq" href="https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9YQV84NTFrMw?sa=X&amp;ved=0CAkQlvsGahcKEwjwqt2f9u3vAhUAAAAAHQAAAAAQAQ" rel="noopener ugc nofollow" target="_blank">堆栈溢出播客</a>。</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="f7c6" class="kz js hi kv b fi la lb l lc ld">URLs = [&lt;a list of homepage links&gt;]</span><span id="7309" class="kz js hi kv b fi le lb l lc ld">info_df_list = []</span><span id="0492" class="kz js hi kv b fi le lb l lc ld">for url in URLs:<br/>    soup = BeautifulSoup(requests.get(url).text, 'lxml')<br/>    title = soup.find('div', {'class':'ZfMIwb'}).text # This is the name of the show<br/>    os.mkdir(title) # make a new folder to contain podcasts from the same show<br/>    df = download_podcasts(soup, title) # function details below<br/>    info_df_list.append(df)</span><span id="818e" class="kz js hi kv b fi le lb l lc ld">info_all_podcasts = pd.concat(info_df_list)</span></pre><p id="280e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，对于每个节目，将有三个功能代表三个步骤——下载、转换/组织和转录。</p><p id="1e91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一个函数不仅下载文件，还将所有相应的元数据收集到一个数据帧中——</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="3ea6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为中间步骤，我们需要将文件格式从MP3转换成WAV。我遇到了错误，<a class="ae jq" href="https://towardsdatascience.com/how-to-use-google-speech-to-text-api-to-transcribe-long-audio-files-1c886f4eb3e9" rel="noopener" target="_blank">这个提示</a>救了我。所以转换函数来自Sundar Krishnan，在这里被复制了——</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="0da3" class="kz js hi kv b fi la lb l lc ld"># still, title is the name of the show/channel and we've made it into a folder</span><span id="d8f9" class="kz js hi kv b fi le lb l lc ld">from pydub import AudioSegment</span><span id="c45a" class="kz js hi kv b fi le lb l lc ld">def mp3_to_wav(audio_file_name, title):<br/>    if audio_file_name.split('.')[1] == 'mp3':    <br/>        sound = AudioSegment.from_mp3(title+'/'+audio_file_name)<br/>        audio_file_name = audio_file_name.split('.')[0] + '.wav'<br/>        sound.export(title+'/'+audio_file_name, format="wav")</span></pre><p id="46a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，我没有使用标准的SpeechClient类，因为不知何故它花费了太长的时间来完成，而且你将不得不(付费)使用云存储桶来进行长时间的音频转录。除此之外，我还没有找到一种方法来标记完整的文本，尽管他们的<a class="ae jq" href="https://cloud.google.com/speech-to-text/docs/async-time-offsets" rel="noopener ugc nofollow" target="_blank">单词出现时间标记</a>似乎工作得很好。所以，要想直接使用谷歌语音转文本API，请参考<a class="ae jq" href="https://towardsdatascience.com/how-to-use-google-speech-to-text-api-to-transcribe-long-audio-files-1c886f4eb3e9" rel="noopener" target="_blank"> Sundar的文章</a>。</p><h1 id="90f6" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">2.抄本</h1><p id="f129" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">使用Google Cloud Speech，我记录了一个110分钟长的播客的转录过程，耗时23分钟！为了更有效地转录长音频文件，我发现了一个聪明的变通办法<a class="ae jq" href="https://techtldr.com/transcribing-audio-file-to-text-with-google-cloud-speech-api-and-python/#4-break-up-audio-file-into-smaller-parts" rel="noopener ugc nofollow" target="_blank">这里</a>。我们将一段很长的音频分成30秒的片段，使用多线程并行处理这些声音片段，最后将它们重新组合成一个文本文件。</p><p id="574e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将使用ffmpeg来分割音频，但首先，让我们为以后将包含所有块的每集创建文件夹:</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="dbfa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，在终端中运行以下命令，遍历<title>文件夹中的每个WAV文件，将每一集分成音频块，并将它们分类到相应的文件夹中(这就是为什么将剧集文件名作为文件夹名称很重要)。</title></p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="98d4" class="kz js hi kv b fi la lb l lc ld">for file in &lt;title&gt;/*.wav; do ffmpeg -i "$file" -f segment -segment_time 30 -c copy "${file%.wav}"/out%09d.wav; done</span></pre><p id="98b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在是实际转录的时候了。虽然我们使用的是SpeechRecognition wrapper，但我们仍然需要在Google云平台上注册，启动一个项目，并获得凭据。这一过程在本文中有更详细的说明。</p><p id="2942" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我花了很长时间才弄明白如何设置环境变量。有各种各样的方法。我所做的是，首先，点击进入<a class="ae jq" href="https://console.cloud.google.com/apis/credentials" rel="noopener ugc nofollow" target="_blank">凭证页面</a>上的服务帐户。其次，点击钥匙标签—</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lh"><img src="../Images/da01c0a79e8b91776def5e0f8252b585.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dkatglPnOQVoMpjLfOFnGg.png"/></div></div></figure><p id="ddc3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，下载密钥文件—</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es li"><img src="../Images/2501109ffdff6ede0a5047a816b7b95b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tIcQ6nEp7wqheRy0c6vL2w.png"/></div></div></figure><p id="6053" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在终端中使用导出来创建一个临时的环境变量，或者在你的Jupyter笔记本中使用这种显式的方式——</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="9b6d" class="kz js hi kv b fi la lb l lc ld">os.environ["GOOGLE_APPLICATION_CREDENTIALS"]="&lt;path to the credential file&gt;/key.json"</span></pre><p id="8d31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这样，我们就可以实例化识别器，并选择使用recognize_google_cloud作为转录模型:</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="1cff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用这种方法，每个2小时的播客大约需要3-5分钟才能完成，这与谷歌云语音的23分钟相比是一个巨大的进步。我比较了产品，差异可以忽略不计。</p><p id="de19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们有越来越丰富的语料库来进行自然语言处理实验，检测口语语言模式，或者许多更实际的目的。例如，我可能只想靠近人们在技术相关的播客中谈论Python或计算机视觉的地方。或者，我可以跟踪某个品牌的提及率，并评估其情绪。我也在考虑为我自己的语言学习转录一些德语播客。或者你想在比较分析中识别和调查某些关键词——例如，某个政治节目在播客中提到“欺诈”的频率。</p><p id="c1de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以使用正则表达式来匹配单词，然后在它周围搜索以包含一点上下文，在这种情况下，可以是包含关键字的组块前后30秒。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="afe4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将它们放入一个数据框中，每一次提及占一行。另一种方法是将每个播客作为一个独特的行，并将提及的内容连接在一起——无论哪个对项目目标更有意义。</p><p id="c412" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的例子中，我想要具有重复元数据的唯一提及，因此是正确的连接—</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="7555" class="kz js hi kv b fi la lb l lc ld">result_df = meta_df.merge(finding_df, on='ID', how='right') # meta_df would be a subset of info_all_podcasts limited to one show/channel</span></pre><p id="d37a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以收集一些例子，比如—</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="9510" class="kz js hi kv b fi la lb l lc ld">00:46:30 darebee contention would not produce a win their they're asking for georgia recount is coming out to happy counties account already is coming out the exact same way the only thing that could really do substantial damage is to find some problem with the minion is got to be legitimate problem obviously sidney powell is on the record pushing this she sings this is where it's all about and she has a few different issues here she is cut 14% of all i never say anything i can't prove <br/>00:47:00 bentley the evidence is coming in so fast i can't even process it all millions of americans have written i would say by now i definitely hundreds of thousands of step forward with their different experiences of voter <strong class="kv hj">fraud</strong> but this is a massive election <strong class="kv hj">fraud</strong> and i'm very concerned it involves not only dominion and it's smartmatic software but that the software essentially was used by other election machine <br/>00:47:30 and it's mostly in in 27 states rejected by texas and we see that there was and i see that for democratic senators actually in the spring of 2019 came out and said why we using some of these systems that are cruelly flawed it would make our system boulder bowl one was peters in michigan and what was amy klobuchar of minnesota so this has been pointed out as an issue before but the question is what is the problem and how can you prove it</span></pre><p id="7723" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有了时间戳，您就可以返回到音频文件，并实际聆听出现关键字的片段。</p><p id="5d75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当然，在语音识别领域还有更多值得探索的地方。但是就我的目的而言，我对我收集的数据很满意，然后可以继续进行文本分析！</p></div></div>    
</body>
</html>