<html>
<head>
<title>Part I Machine Learning Granularity by Splitting Neurons</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第一部分通过分裂神经元的机器学习粒度</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-granularity-by-splitting-neurons-fd2f02e07817?source=collection_archive---------20-----------------------#2021-01-18">https://medium.com/analytics-vidhya/machine-learning-granularity-by-splitting-neurons-fd2f02e07817?source=collection_archive---------20-----------------------#2021-01-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="1a07" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">问题是</h1><ol class=""><li id="9042" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">训练机器学习模型可能需要很长时间。</li><li id="95ab" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">存在死神经元的问题，其中模型可能开始时有太多的神经元和/或层，但没有有效的方法来提前知道这一点或稍后移除那些额外的参数。</li><li id="9dee" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">有时，一个模型开始稳定在我们想要的精度以下，如果不在一个更大的新模型上开始训练，就没有办法进一步提高精度。</li></ol><h1 id="22b3" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">解决方案</h1><p id="739e" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">如果我们可以从一个较小的神经网络模型开始，在训练过程中将其增长到问题的大小，会怎么样？</p><p id="8451" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">假设我们可以从两个神经元的隐藏层开始，当它开始达到稳定的精确度时，保持两倍的大小。这就是我们今天要使用TensorFlow和Keras开始探索的内容。</p><p id="0187" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我们首先认识一个简单的矩阵恒等式:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/c484659cb78d7886f7bd3af70e9f1240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G-WIob-MXxdOmVEggVaXwg.png"/></div></div></figure><p id="aa0f" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我们在上面所做的就是在左边取x₁和y₁，在右边把它们分成两个元素。x₁被重复，而y₁被重复<strong class="je hi">和</strong>除以二。然而，这一变化使矩阵点积保持不变。</p><p id="eae1" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">考虑神经网络中的一个神经元。如果我们能把那个神经元一分为二，同时保持输出不变，会怎么样？这将给予损失函数新的参数来调整反向传播。如果我们可以对每个神经元都这样做，从而使我们的网络翻倍，而不用从头开始，会怎么样？这正是这次行动的可能。让我们通过一个模型来演示如何在TensorFlow中实现这一点。</p><h1 id="8d08" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">一个工作实例</h1><p id="e95f" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">让我们从调用我们的模块并加载MNIST数据集开始:</p><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="d777" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><em class="lh">注意:这段代码适用于Tensorflow 2.4.0、Python 3.8和Numpy 1.18.5。</em></p><p id="c209" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我们将使用<em class="lh"> num_neurons </em>来定义我们将从多少个神经元开始。在这个演示中，我们将只使用两个神经元。现在来看看我们的模型架构。</p><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="lf lg l"/></div></figure><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es li"><img src="../Images/454f04aa0d4fc1a5cd5077d84dcffe93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*CBWfwL_7EUDT5aWo-DVC8w.png"/></div></figure><p id="1453" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">这只是一个简单的序列模型，有一个隐藏的密集层和1600个参数。现在让我们编译并拟合模型。</p><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="1131" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">您可能会注意到我们的fit方法中的<a class="ae lj" href="https://keras.io/api/callbacks/early_stopping/" rel="noopener ugc nofollow" target="_blank"> <em class="lh">提前停止</em>回调</a>。这将确保一旦在两个连续时期内验证准确度的变化小于1%,模型就停止。现在我们想从模型中得到权重和偏差。我们可以通过Keras内置的<a class="ae lj" href="https://keras.io/api/models/model_saving_apis/#get_weights-method" rel="noopener ugc nofollow" target="_blank"><em class="lh">get _ weights</em></a><em class="lh"/>方法来实现。</p><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="b6b2" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">每层包含两个数组，一个用于权重，一个用于偏差。权重通过矩阵乘法相乘。偏差然后被添加到结果向量中。我们的模型包含一个密集层和一个输出层。权重和偏好的结构是这样的:</p><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="1ca8" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我们不想分割输出，因为我们仍然应该有10个类别。我们需要做以下工作:</p><ol class=""><li id="95fe" class="jc jd hh je b jf ko jh kp jj lk jl ll jn lm jp jq jr js jt bi translated">复制层1密集权重、层1偏差和输出层权重中的所有元素。</li><li id="728b" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">将输出图层权重的元素除以2。</li><li id="4312" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">层1中的新神经元对将是相同的。所以损失函数最终会把它们调整得完全一样。因此，我们将希望在相反的方向上稍微推动它们的偏差，这样每个相同的神经元对都有一个主导神经元和一个被动神经元，它们可以在进一步的训练中继续多样化。</li></ol><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="fd40" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">这为我们提供了一个新的数组列表:</p><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="cfaf" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我们可以看到上面的数组就是我们想要的形状。现在让我们用两倍的神经元创建第二个模型架构。</p><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="4b97" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">使用<a class="ae lj" href="https://keras.io/api/models/model_saving_apis/#set_weights-method" rel="noopener ugc nofollow" target="_blank"><em class="lh">set _ weights</em></a><em class="lh"/>方法<em class="lh"> </em>将我们所有处理过的权重和偏差传递给新模型。让我们来总结一下:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es ln"><img src="../Images/5a46d30a0bdf4f358bf4662a744fe6db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*UzqQeWKS9eEQs6xmV-EfFA.png"/></div></figure><p id="0787" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我们可以看到我们的新模型的参数增加了近一倍(1，600到3，190)。现在让我们继续使我们的参数适合新模型。</p><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="464c" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">首先，让我们在较小的双神经元模型上进行训练。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lo"><img src="../Images/21f1177d1e1be005f2fb3a43852cbeab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aNiMJnt7X4zgGsUGoKnh-A.png"/></div></div></figure><p id="dc14" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我们可以看到，在由于回调而停止之前，我们的2个神经元达到了68%的验证准确率。看起来是个不错的增长目标！让我们看看4个神经元是如何工作的:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lp"><img src="../Images/e8b6421660d91940e2301eca26055544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hn1aozAciWNPYd91PTe2ig.png"/></div></div></figure><p id="1c8a" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">正如我们所希望的！新的更大的模型接受了分开的权重和偏差，开始时准确率约为68%，损失量与小模型相同。到了第六纪元，在我们停止回调之前，整体准确率提高到了87%。当然，您可以对更大的模型重复上述操作，直到模型达到期望的精度。任务完成！</p><h1 id="8026" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">模块化生长神经网络</h1><p id="2a76" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">虽然上面演示了神经元确实可以被分裂以及如何进行分裂，但每次分裂所有神经元并不一定是最有效的方法。理想情况下，我们希望将损失最大的神经元作为分裂的目标。假设我们建立了一个阈值损失，其中任何高于特定损失的东西都会在反向传播后分裂其神经元。</p><p id="0c8a" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我们之所以希望以损失最大的神经元为目标，是因为这些神经元在每一批中都经历了最大的变化，并且是增加粒度的理想目标。</p><p id="1686" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">根据正在处理的数据的类型以及有多少数据可用，您可能也希望这种情况只发生在这么多的时期之后。这样，在我们继续以神经元为分裂目标之前，模型有机会解决一些权重和偏差。</p><p id="d24d" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">分裂神经元对于具有昂贵训练机制的大型模型来说尤其理想，例如生成式对抗网络(GANs)和自然语言处理器(NLP)。这可以让这些模型增长到数据，并大大减少死亡神经元的积累。</p><p id="9b1e" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">更新(5/9/21):下一篇文章将介绍一个为Pytorch开发的模块，该模块可以根据线性层和Conv2D层的每个神经元的活跃程度，在不同时期之间自动分割单个神经元。我们将运行一些测试，看看性能是否有所提高。</p><h1 id="392e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">最后</h1><p id="b765" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">我希望你和我一样对此感兴趣！欢迎留下评论/问题，感谢您的阅读。</p><p id="a196" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">下面是Github上完整python脚本的链接:</p><p id="9bbc" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><a class="ae lj" href="https://github.com/therealjjj77/Splitting-Neurons" rel="noopener ugc nofollow" target="_blank">https://github.com/therealjjj77/Splitting-Neurons</a></p></div></div>    
</body>
</html>