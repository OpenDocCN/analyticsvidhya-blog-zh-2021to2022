<html>
<head>
<title>Reducing data dimensions in a non-linear subspace: t-SNE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降低非线性子空间中的数据维数:t-SNE</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reducing-data-dimensions-in-a-non-linear-subspace-t-sne-f6925c34281b?source=collection_archive---------16-----------------------#2021-09-21">https://medium.com/analytics-vidhya/reducing-data-dimensions-in-a-non-linear-subspace-t-sne-f6925c34281b?source=collection_archive---------16-----------------------#2021-09-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/6ef1aeddca37d361149d466bc4b8f5e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xNbc4QQmYju7gq8T"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">卢卡斯·布拉塞克在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="d604" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">T-分布式随机邻居嵌入(t-SNE) </strong>是一项获奖的非线性降维技术，特别适合于高维数据集的可视化。但我们先讨论一些背景。</p><p id="e749" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">维度的诅咒:</strong>看看下面一些随机的数据点，绘制在一维(线)上，然后投射到二维平面上，再进一步投射到三维空间。</p><p id="404b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你意识到上面的数字发生了什么吗？随着维数的增加，空间的体积增加，同样的数据随着维数的增加变得越来越稀疏。<em class="js">随着维度数量的增加，开发有意义的机器学习模型所需的数据量呈指数增长</em>。这种现象被称为维数灾难。收集&amp;过程中更多的数据是昂贵的，&amp;有时根本不可能得到。</p><p id="1c29" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js">但高维数据并不总是坏的，</em>更多的维度或特征也意味着更多的信息，更多的方式来看待数据&amp;挑选你想要的。事实上，最好的方法是获得尽可能多的特性，然后根据用例选择合适的特性。</p><p id="bed6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js">有两种减少维数的常用方法，也称为特征:</em></p><p id="8d7e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">方法1:特性选择:</strong>最直接的方法是删除一些特性。可以基于a)用例&amp;领域知识或b)基于这些度量中的任何一个来选择要删除的特性:<em class="js">缺失值比率、低方差过滤器、高相关性过滤器、随机森林模型、向前&amp;向后逐步回归。</em></p><p id="a504" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">方法2:降维:</strong>不是丢弃任何特征，而是通过组合现有特征来创建新特征。然后，原始数据点被投影到这个新的坐标系上&amp;，包含有意义信息的尺寸被保留。根据我们是希望将数据投影到线性还是非线性子空间的决定，降维技术进一步分为线性还是非线性。</p><p id="18e1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">线性降维:</strong>用原始维度的线性组合来创建新维度。<em class="js">把线性变换想象成平移&amp;拉伸尺度。</em> PCA是应用最广泛的线性技术。它利用特征值分解或奇异值分解来正交旋转原始轴，因此新的坐标系将自己对准最大方差的方向。要详细了解PCA和SVD，请参考我的另一篇文章:<a class="ae it" href="https://www.linkedin.com/pulse/bare-all-interview-pca-arvind-shukla/?trackingId=7libxUqRwziP0F2PLC4uyg%3D%3D" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hi"> A bare it all采访PCA </strong> </a>。</p><p id="efdc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">非线性降维:</strong>当原始维度的非线性组合更有意义时使用。非线性转换会对数据产生更大的变化。</p><p id="5dd8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如，从<a class="ae it" href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction" rel="noopener ugc nofollow" target="_blank">维基百科</a>看下图。它显示了将二维数据从一个区域减少到一条线的效果。蓝线是创建的线性尺寸，而红线是从数据点创建的非线性尺寸。正如我们所看到的，将所有灰色数据点减少到蓝线上将是垂直轴上信息的巨大损失。在这种情况下，创建非线性维度(如红色线)比创建线性蓝色线能捕捉更多的数据差异。</p><p id="4d0e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> t-SNE: </strong>是一种非线性投影技术。它支持分布式随机邻居嵌入。</p><p id="522b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">t-SNE的直觉:</strong>正如我们在本文开头所看到的，点在低维中趋向于变得密集&amp;这导致了过度拥挤。t-SNE背后的想法很简单，<em class="js">我们希望保留数据的原始结构&amp;避免过度拥挤。这意味着，我们希望确保相似的对象保持相似&amp;不相似的对象在将数据从高维映射到低维平面时保持不相似。</em>为此，我们首先定义一个函数，该函数表示高维数据点之间的相似性。然后，我们定义另一个函数来表示低维数据点之间的相似性。然后，我们迭代&amp;在更低的维度上重新排列我们的数据点，这样这两个函数之间的差异就可以最小化。这将在两个维度上给我们相似的数据点。</p><p id="84c4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们把它分成3个步骤&amp;看看每一步:</p><p id="4cf1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">第一步:</strong> <em class="js">为高维数据创建相似度函数:</em>计算高维平面上所有数据点之间的欧氏距离。假设该距离遵循高斯分布，计算概率分布函数。这是数据点之间相似性(距离)的条件概率分布函数。根据这个条件分布函数，计算每一对数据点的联合概率分布函数。</p><p id="26a4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">第二步:</strong> <em class="js">为低维数据创建相似度函数:</em>在低维空间中随机取n (n =高维数据点的原始个数)个数据点。按照步骤1为这个数据集创建联合概率分布，但是这次不使用高斯分布，而是使用t分布。这是因为t分布是一种重尾分布&amp;在低维空间使用它可以缓解拥挤问题。</p><p id="0f00" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">第三步:</strong> <em class="js">最小化两个分布的差异:</em>使用<a class="ae it" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> Kullback-Leiber散度</a> (KL散度)计算两个联合概率分布的差异。计算两个分布之间的KL散度，然后以进一步减少KL散度的方式重新组织低维数据集。使用梯度下降迭代优化这些嵌入&amp;成本函数是两个分布之间的KL散度。然而，成本函数是非凸的，这意味着存在陷入局部最小值的风险。</p><p id="77ed" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一旦KL散度最小，这意味着两个分布彼此相似，换句话说，这意味着较低维度中的数据点与较高维度中的数据点相似。</p><p id="1f0b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">结论:</strong>简而言之，t-SNE就是为高维数据点定义一个基于高斯的联合概率分布函数，为低维数据点定义一个基于t-分布的联合概率分布函数&amp;，然后重新排列低维数据点以减少两个分布之间的差异(就KL散度而言)。从高斯分布到t分布的映射用于利用t分布的重尾特性&amp;，因此可以避免过度拥挤问题。</p><p id="ac37" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是文章的结尾。我希望你喜欢阅读它&amp;现在对t-SNE算法有了更好的直觉。</p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><p id="80b8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js">最初发表于</em><a class="ae it" href="https://www.linkedin.com/pulse/reducing-data-dimensions-non-linear-subspace-t-sne-arvind-shukla/" rel="noopener ugc nofollow" target="_blank"><em class="js">【https://www.linkedin.com】</em></a><em class="js">。</em></p></div></div>    
</body>
</html>