<html>
<head>
<title>Image2Image translation with cGAN, Pix2Pix…</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图2使用cGAN、Pix2Pix进行图像翻译…</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/image2image-translation-with-cgan-pix2pix-86f2f327fece?source=collection_archive---------6-----------------------#2021-09-05">https://medium.com/analytics-vidhya/image2image-translation-with-cgan-pix2pix-86f2f327fece?source=collection_archive---------6-----------------------#2021-09-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="3ad1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文重点介绍使用条件GANs(pix 2 pix)实现图像到图像转换的深度卷积网络。该模型是在<a class="ae jc" href="https://www.tensorflow.org/tutorials/generative/pix2pix" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>的帮助下建立的。</p><p id="cfdf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">完整的代码和结果可以在<a class="ae jc" href="https://github.com/Devesan/pix2pix" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="00e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">文章概述:</strong></p><p id="d053" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一、Pix2Pix cGAN介绍？</p><p id="c8c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">二关于数据集</p><p id="51a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">构建U-net生成器和鉴别器</p><p id="cb06" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">定义损失函数</p><p id="206a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">五.结论</p><h1 id="af06" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">一、Pix2Pix cGAN简介:</strong></h1><p id="6f1d" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">GAN是生成模型，它学习从随机噪声向量z到输出图像y，G: z → y的映射。相反，条件gan学习从观察到的图像x和随机噪声向量z，到y，G: {x，z} → y的映射。在pix2pix cGAN中，我们调节输入图像并生成相应的输出图像。cGANs最初是在<a class="ae jc" href="https://arxiv.org/abs/1411.1784" rel="noopener ugc nofollow" target="_blank">条件生成对抗网</a>中提出的(Mirza和Osindero，2014)</p><p id="714a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该架构如下所示:</p><p id="0649" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1)一个U网生成器</p><p id="cd64" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2)卷积PatchGAN鉴别器</p><p id="534e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">发生器G被训练成产生不能被对抗性训练的鉴别器D从“真实”图像中区分出来的输出，鉴别器D被训练成尽可能好地检测发生器的“假货”。这个训练过程如图2所示。</p><p id="f089" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Pix2Pix GAN已经在一系列图像到图像的翻译任务中得到验证，例如将地图转换为卫星照片，将黑白照片转换为彩色照片，以及将产品草图转换为产品照片。</p><h1 id="c70c" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">二关于数据集:</strong></h1><p id="dfef" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">已经创建了包含超过3500张手绘图像及其相应CAD图像的数据集。这些图像是从SketchGraphs数据集构建的，该数据集可在此处找到<a class="ae jc" href="https://github.com/PrincetonLIPS/SketchGraphs#sketchgraphs-a-large-scale-dataset-for-modeling-relational-geometry-in-computer-aided-design" rel="noopener ugc nofollow" target="_blank">、<strong class="ig hi"> SketchGraphs:一个用于在计算机辅助设计中对关系几何建模的大规模数据集</strong>、</a></p><p id="251f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">训练数据集中的每个图像都包含用shape (288，864，3)连接在一起的两个图像。在数据输入管道中，此图像的大小调整为(256，512，3)。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/dacfd8281f77cacf560307a0097c9ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NkwLb7g7ws-o4MyTZlMWsw.png"/></div></div></figure><h1 id="39e6" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">三、建立模型:</strong></h1><p id="1115" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated"><strong class="ig hi"> 1)构建一个U-net生成器:</strong></p><p id="1f29" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">pix2pix cGAN的生成器是一个<em class="ks">修改的</em> <a class="ae jc" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> U-Net </a>。U-Net由编码器(下采样器)和解码器(上采样器)组成。</p><ul class=""><li id="2d2a" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">编码器中的每个块是:卷积-&gt;批量归一化-&gt;泄漏ReLU</li><li id="f2d6" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">解码器中的每个块是:转置卷积-&gt;批量归一化-&gt;丢弃(应用于前3个块)-&gt; ReLU</li><li id="c8fd" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">编码器和解码器之间有跳跃连接(如在U-Net中)。</li></ul><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lh"><img src="../Images/431e51cd9ae12d7cb606fc103c297cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*qckzBmbO9vW__8JF0os_Rw.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">U-net架构</figcaption></figure><p id="c64f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">发电机:</strong></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lm"><img src="../Images/fa4bbd7edc9d97aae2297c3881b02544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*hMh9TL1lRsBlXDL9FTsdFw.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">u网生成器</figcaption></figure><p id="16bf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 2)构建PatchGAN鉴别器:</strong></p><p id="3e5e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">pix2pix中使用的PatchGAN鉴别器是该设计的另一个独特组件。PatchGAN / Markovian鉴别器的工作原理是将图像中的单个(N x N)斑块分类为“真实与虚假”，而不是将整个图像分类为“真实与虚假”，如<a class="ae jc" href="https://arxiv.org/abs/1611.07004" rel="noopener ugc nofollow" target="_blank"> pix2pix论文</a>中所述。作者认为，这加强了更多的约束，鼓励尖锐的高频细节。此外，PatchGAN的参数更少，运行速度比分类整个图像更快。</p><p id="a1cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">鉴别器中的每个模块是:</p><ul class=""><li id="d50d" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">卷积-&gt;批量归一化-&gt;泄漏ReLU。</li><li id="0302" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">最后一层之后输出的形状是(batch_size，30，30，1)。</li><li id="6586" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">输出的每个30 x 30图像块对输入图像的70 x 70部分进行分类。</li><li id="a441" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">鉴频器接收2个输入:</li><li id="1687" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">输入图像和目标图像，它应该分类为真实的。</li><li id="7761" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">输入图像和生成的图像(生成器的输出)，它应该将其归类为假的。</li><li id="00ea" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">使用tf.concat([inp，tar]，axis=-1)将这两个输入连接在一起。</li></ul><h1 id="40bc" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">四定义损失函数:</strong></h1><p id="aa91" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated"><strong class="ig hi">发电机损耗:</strong></p><p id="f533" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">gan学习适应数据的损失，而cGANs学习惩罚与网络输出和目标图像不同的可能结构的结构化损失，如<a class="ae jc" href="https://arxiv.org/abs/1611.07004" rel="noopener ugc nofollow" target="_blank"> pix2pix论文</a>中所述。</p><ul class=""><li id="6b03" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">生成器损失是所生成图像的sigmoid交叉熵损失和一个的<strong class="ig hi">阵列。</strong></li><li id="8dab" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">pix2pix论文还提到了L1损失，这是生成的图像和目标图像之间的MAE(平均绝对误差)。</li><li id="fe60" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">这使得生成的图像在结构上变得与目标图像相似。</li><li id="8ef8" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">计算发电机总损耗的公式为gan_loss + LAMBDA * l1_loss，其中LAMBDA = 100。这个值是由论文作者决定的。</li></ul><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lm"><img src="../Images/f84e60ecf45660144cc9109839260556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*XjPW3JhJComj_T9zr1W7ZA.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">发电机的培训程序</figcaption></figure><p id="80c9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">鉴别器损耗:</strong></p><ul class=""><li id="b485" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">discriminator_loss函数有两个输入:<strong class="ig hi">真实图像</strong>和<strong class="ig hi">生成图像</strong>。</li><li id="fe08" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">real_loss是<strong class="ig hi">实像</strong>和一个数组<strong class="ig hi">(因为这些是实像)</strong>的sigmoid交叉熵损失。</li><li id="71e4" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">generated_loss是<strong class="ig hi">生成的图像</strong>和一个零的<strong class="ig hi">阵列(因为这些是假图像)</strong>的sigmoid交叉熵损失。</li><li id="6e83" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">总损失是实际损失和产生损失的总和。</li></ul><p id="3c4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">条件-对抗损失(生成器对鉴别器)非常普遍地格式化如下:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ln"><img src="../Images/2c8599d7c5c78c505ab3c7ea08dc7092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/0*gLPr-lIGi8fxubsK.png"/></div></figure><p id="1ece" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">前面提到的L1损失函数如下所示:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lo"><img src="../Images/69c6744dfc9bc8e6038a19abca5ae776.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/0*t6UrHlHOZFr_4Isq.png"/></div></figure><p id="3f72" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">结合这些功能会产生:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lp"><img src="../Images/48d10af73127294ec77d6291b6fe3b3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/0*14rJGOvb_IJcC1ui.png"/></div></figure><p id="c319" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在实验中，作者报告说他们发现λ参数等于100时最成功。所以我们也选择了λ为100。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lq"><img src="../Images/1b2868e863c3cfe7eba58f2c9bcdcc5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G4YNchkNISM_TAmX8YOEZA.png"/></div></div></figure><p id="3e19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">结论:</strong></p><p id="5965" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用手绘图像训练模型，以创建具有16k步骤的CAD图像。</p><p id="5dd8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最终的模型可以从拥抱脸下载，这里是模型的结果。</p><p id="f2f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一些结果如下所示。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lq"><img src="../Images/b26daec8b578bf430de2903ab030bb02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qMiJMqyYK3GuzvckILPlSA.png"/></div></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lq"><img src="../Images/c1ff77dfe212af6af796f954f026ef73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UispT1moM9zR76Wp3JHIZw.png"/></div></div></figure><p id="9918" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">谢谢你…</p></div></div>    
</body>
</html>