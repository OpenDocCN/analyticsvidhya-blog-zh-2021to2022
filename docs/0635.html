<html>
<head>
<title>Support Vector Regression using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python支持向量回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/support-vector-regression-using-python-fdf3ed48f282?source=collection_archive---------6-----------------------#2021-01-25">https://medium.com/analytics-vidhya/support-vector-regression-using-python-fdf3ed48f282?source=collection_archive---------6-----------------------#2021-01-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/6c941e5b22144f31c7b07b485a58e20d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqDZU-x8AMljs291wpRV7w.jpeg"/></div></div></figure><p id="b34a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">通常，我们大多数人会混淆<strong class="ir hi">支持向量机(SVM) </strong>和<strong class="ir hi">支持向量回归机(SVR) </strong>。基本的区别是SVM用于分类，支持向量回归用于回归。</p><p id="2158" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">SVR到底是什么？为了理解什么是SVR，我们将把它与线性回归进行比较。</p><p id="03df" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">众所周知，在线性回归中，我们通过数据点绘制直线，然后将数据点垂直投影到回归线上，以找到最小距离。</p><blockquote class="jn jo jp"><p id="dfa1" class="ip iq jq ir b is it iu iv iw ix iy iz jr jb jc jd js jf jg jh jt jj jk jl jm ha bi translated">我们将做类似的事情，但不是只画回归线，我们将画一个<strong class="ir hi">管</strong>(你可以看到上面图像中的虚线)。我们不必担心管内或虚线上的点。现在，管外的点，我们将把它们的垂直投影投影到虚线上，而不是管的主轴线上。这将减少我们的错误，我们将有最小的距离。</p></blockquote><p id="d29b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这种方法被称为<strong class="ir hi">支持向量</strong>，因为管外的点被称为<strong class="ir hi">向量</strong>。</p><blockquote class="ju"><p id="f758" class="jv jw hh bd jx jy jz ka kb kc kd jm dx translated">我们可以使用不同类型的<strong class="ak">核</strong>对非线性数据点进行支持向量回归。</p></blockquote><h2 id="0fb0" class="ke kf hh bd kg kh ki kj kk kl km kn ko ja kp kq kr je ks kt ku ji kv kw kx ky bi translated">内核:</h2><p id="2111" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">它是我们使用线性回归来解决非线性回归问题的方法。不同类型的内核，</p><ol class=""><li id="8890" class="le lf hh ir b is it iw ix ja lg je lh ji li jm lj lk ll lm bi translated">线性的</li><li id="4d72" class="le lf hh ir b is ln iw lo ja lp je lq ji lr jm lj lk ll lm bi translated">多项式</li><li id="6990" class="le lf hh ir b is ln iw lo ja lp je lq ji lr jm lj lk ll lm bi translated">高斯径向基函数</li><li id="0773" class="le lf hh ir b is ln iw lo ja lp je lq ji lr jm lj lk ll lm bi translated">指数径向基函数，等等</li></ol></div><div class="ab cl ls lt go lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ha hb hc hd he"><p id="e535" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">最后我会提供一个我的Kaggle笔记本的链接！！！</strong></p><h2 id="8582" class="ke kf hh bd kg kh lz kj kk kl ma kn ko ja mb kq kr je mc kt ku ji md kw kx ky bi translated">数据:</h2><p id="2acb" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我们将使用包含葡萄酒质量的红酒数据集。它有13列1599行。数据集中没有任何缺失值。</p><p id="c5df" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">链接:<a class="ae me" href="https://www.kaggle.com/piyushgoyal443/red-wine-dataset?select=wineQualityReds.csv" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/piyushgoyal443/red-wine-dataset?select = winequilityreds . CSV</a></p><figure class="mg mh mi mj fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mf"><img src="../Images/b03437c944765a57245e557dbab9be4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5w7qwrOA4NLWTuOJBc4rLg.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated">红酒数据集</figcaption></figure><h2 id="86a9" class="ke kf hh bd kg kh lz kj kk kl ma kn ko ja mb kq kr je mc kt ku ji md kw kx ky bi translated">导入库和数据集:</h2><p id="4fca" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我们将使用Numpy、Pandas和Scikit-learn库。导入数据集后，我们将删除数据集的第一列，因为它包含索引，所以我们不需要它。</p><p id="8c42" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们将把数据集分成独立(X)和非独立(y)变量。</p><pre class="mg mh mi mj fd mo mp mq mr aw ms bi"><span id="d3f3" class="ke kf hh mp b fi mt mu l mv mw">#importing the libraries <br/>import numpy as np<br/>import pandas as pd</span><span id="6551" class="ke kf hh mp b fi mx mu l mv mw">#importing the dataset<br/>dataset = pd.read_csv('Name_of_the_dataset.csv')<br/># Removing the unnecessary column<br/>dataset.drop(['Unnamed: 0'], axis = 1, inplace = True)</span><span id="3516" class="ke kf hh mp b fi mx mu l mv mw"># seprating the dataset<br/>X = X = dataset.iloc[:, :-1].values<br/>y = dataset.iloc[:, -1:].values</span></pre><h2 id="15ee" class="ke kf hh bd kg kh lz kj kk kl ma kn ko ja mb kq kr je mc kt ku ji md kw kx ky bi translated">将数据集分成训练集和测试集:</h2><p id="1a48" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我们将使用scikit-learn库来分割数据集。训练集包含大部分数据。</p><pre class="mg mh mi mj fd mo mp mq mr aw ms bi"><span id="7748" class="ke kf hh mp b fi mt mu l mv mw"># Spliting the datdset<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2, random_state=0)</span></pre><h2 id="7a35" class="ke kf hh bd kg kh lz kj kk kl ma kn ko ja mb kq kr je mc kt ku ji md kw kx ky bi translated">特征缩放:</h2><p id="423a" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">特征缩放指的是将值放在相同的范围或相同的比例中，以便没有一个变量受另一个变量支配。我们不会对测试数据集应用特征缩放。如果列包含虚拟变量(二进制值)，那么我们不会对它们应用特性缩放。</p><pre class="mg mh mi mj fd mo mp mq mr aw ms bi"><span id="a231" class="ke kf hh mp b fi mt mu l mv mw">from sklearn.preprocessing import StandardScaler<br/>X_sc = StandardScaler()<br/>y_sc = StandardScaler()<br/>X_train = X_sc.fit_transform(X_train)<br/>y_train = y_sc.fit_transform(y_train)</span></pre><h2 id="1355" class="ke kf hh bd kg kh lz kj kk kl ma kn ko ja mb kq kr je mc kt ku ji md kw kx ky bi translated">训练数据集:</h2><p id="8197" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我们将使用训练集来训练具有SVR模型的数据集。你可以使用所有的内核，看看哪个给你最好的结果。</p><pre class="mg mh mi mj fd mo mp mq mr aw ms bi"><span id="7293" class="ke kf hh mp b fi mt mu l mv mw">from sklearn.svm import SVR<br/>regrassor = SVR(kernel = 'rbf')<br/>regrassor.fit(X_train, y_train)</span></pre><h2 id="4eb7" class="ke kf hh bd kg kh lz kj kk kl ma kn ko ja mb kq kr je mc kt ku ji md kw kx ky bi translated">预测结果:</h2><p id="40e3" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我们必须使用StandardScaler类中的inverse_transform()方法来反转我们的值。</p><pre class="mg mh mi mj fd mo mp mq mr aw ms bi"><span id="cdac" class="ke kf hh mp b fi mt mu l mv mw">y_pred = regrassor.predict(X_sc.transform(X_test))<br/>y_pred = y_sc.inverse_transform(y_pred)</span></pre><h2 id="53df" class="ke kf hh bd kg kh lz kj kk kl ma kn ko ja mb kq kr je mc kt ku ji md kw kx ky bi translated">将预测值和实际值可视化:</h2><pre class="mg mh mi mj fd mo mp mq mr aw ms bi"><span id="1c05" class="ke kf hh mp b fi mt mu l mv mw">y_test = y_test.flatten()<br/>df = pd.DataFrame({'Predicted value': y_pred, 'Real Value': y_test})<br/>df</span></pre><p id="6ad9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Kaggle笔记本链接:<a class="ae me" href="https://www.kaggle.com/rahulkadam0909/suppoer-vector-regression" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/rahulkadam 0909/sup poer-vector-regression</a></p><blockquote class="ju"><p id="9999" class="jv jw hh bd jx jy jz ka kb kc kd jm dx translated">编码快乐！！！</p><p id="4b2a" class="jv jw hh bd jx jy jz ka kb kc kd jm dx translated">继续努力吧！！！</p></blockquote></div></div>    
</body>
</html>