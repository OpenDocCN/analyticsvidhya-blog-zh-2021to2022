<html>
<head>
<title>Why Activation Functions?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么要激活函数？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/why-activation-functions-8328f3f21120?source=collection_archive---------19-----------------------#2021-02-15">https://medium.com/analytics-vidhya/why-activation-functions-8328f3f21120?source=collection_archive---------19-----------------------#2021-02-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/653ba35c81649419ba67a5042a578c16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F8SVj1DckQ70wix1oklS8A.png"/></div></div></figure><p id="f83e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在本文中，我们将讨论以下主题:</p><ul class=""><li id="5253" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">为什么我们需要激活函数</li><li id="ca56" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">不同种类的激活功能</li><li id="7976" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">不同激活功能的优缺点</li></ul><p id="b9b2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为什么我们需要激活功能？</p><p id="3c1f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们以单个神经元为例。每个神经元执行两种功能:</p><ul class=""><li id="490e" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">计算所有输入要素的加权和</li><li id="4cd7" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">将它传递给一个激活函数。</li></ul><pre class="kb kc kd ke fd kf kg kh ki aw kj bi"><span id="8b54" class="kk kl hh kg b fi km kn l ko kp">z = w1*x1 + w2*x2 + w3*x3 - (Eq.1)(where x1,x2,x3 are input features and w1,w2,w3 are weights)<br/>a = f(z) where f is the activation function.</span></pre><p id="aa55" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以激活函数基本上提供了z的非线性，这有助于学习复杂函数。如果我们移除所有的激活函数，我们的网络将只学习线性函数，这不会有任何帮助。为了更清楚地了解这一点，你可以参考我关于<a class="ae kq" rel="noopener" href="/analytics-vidhya/neural-networks-and-the-power-of-universal-approximation-theorem-9b8790508af2">神经网络</a>的文章。</p><p id="fc9a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当我们讨论不同的激活函数时，你会对此有更好的理解。</p></div><div class="ab cl kr ks go kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ha hb hc hd he"><h1 id="a801" class="ky kl hh bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">不同种类的激活功能</strong></h1><p id="3c58" class="pw-post-body-paragraph ip iq hh ir b is lv iu iv iw lw iy iz ja lx jc jd je ly jg jh ji lz jk jl jm ha bi translated">有各种各样的激活函数，我们可以根据我们要解决的问题的类型，为我们的模型选择其中任何一个</p><h2 id="64ca" class="kk kl hh bd kz ma mb mc ld md me mf lh ja mg mh ll je mi mj lp ji mk ml lt mm bi translated">Sigmoid函数</h2><figure class="kb kc kd ke fd ii er es paragraph-image"><div class="er es mn"><img src="../Images/269f88b55ca3ac6a9cd150ab4bb88310.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*arc2k1bBbxesOfu4PV1WwQ.png"/></div></figure><ul class=""><li id="4ce8" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">sigmoid函数由上述等式给出，有点像S形图。</li><li id="a75c" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">对于任何输入Z，它将转换0和1之间的值。由于sigmoid函数的这种性质，它们被用作二进制分类问题中输出层的激活函数。因为它给出了特定类别出现的概率。</li></ul><p id="cd3c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">sigmoid函数的问题</strong></p><ul class=""><li id="fd42" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><strong class="ir hi">消失梯度问题</strong>:假设我们的网络变得很深，我们计算梯度来更新我们网络的权重。在sigmoid中，我们可以获得的梯度的最大值约为0.25，现在如果将太多的梯度相乘(使用链式法则)，最终梯度将变得非常小，并且由于梯度非常小，权重更新将变得可以忽略不计(因为w = w-<strong class="ir hi">α</strong>δw)。因此，我们的初始层停止更新(想想吧！).</li><li id="f0f4" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi"> sigmoid不在零中心</strong>:通过查看Sigmoid的图形，我们可以看到Sigmoid函数只能给出正值作为输出。因此，每当我们在sigmoid中传递z (z = w*x + b)时，我们都会得到一组有限的正值，这限制了sigmoid函数输出的可能值范围。</li><li id="21c2" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">指数函数的计算成本很高。</li></ul><h1 id="12d6" class="ky kl hh bd kz la mo lc ld le mp lg lh li mq lk ll lm mr lo lp lq ms ls lt lu bi translated">双曲正切</h1><figure class="kb kc kd ke fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mt"><img src="../Images/2a0eaef70084e240b22ba431ad2b9dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3H1SdpvzugfNCulNSSjkGw.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">Tanh激活函数</figcaption></figure><ul class=""><li id="4aa3" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">Tanh函数最初用于LeNet，它的性能比sigmoid函数好得多。</li><li id="a3cc" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">它以零为中心，因此双曲正切函数的输出值范围很广，即1<z/></li></ul><h2 id="e510" class="kk kl hh bd kz ma mb mc ld md me mf lh ja mg mh ll je mi mj lp ji mk ml lt mm bi translated">双曲正切函数的问题</h2><ul class=""><li id="1887" class="jn jo hh ir b is lv iw lw ja my je mz ji na jm js jt ju jv bi translated"><strong class="ir hi">饱和问题:</strong>类似于对于z的大值的sigmoid函数，梯度变为零，因此一些神经元可能表现为死的，并且对预测输出没有贡献。看上面这张图，想一想为什么会这样。</li><li id="38a3" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">指数函数的计算成本很高。</li></ul><p id="7aea" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">热卢</strong></p><figure class="kb kc kd ke fd ii er es paragraph-image"><div class="er es nb"><img src="../Images/61feb87faaae38518f47d08b8658e26c.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*njuH4XVXf-l9pR_RorUOrA.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">ReLu激活功能</figcaption></figure><ul class=""><li id="79d0" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">ReLU代表整流线性单元，是业内使用最广泛的激活功能之一。</li><li id="5c8a" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">relu解决了梯度消失的问题，因为ReLu函数的梯度最大值是1。</li><li id="c8df" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">它还解决了神经元饱和的问题，因为relu函数的斜率从不为零。</li></ul><p id="8f0e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">【ReLU的问题</p><ul class=""><li id="915e" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">假设我们给我们的relu函数以下输入，z = w*x + b，并且在训练过程中“b”变成非常大的负值，现在这反过来将使“z”的值为负。在我们的relu函数中，我们可以看到，对于负值，它给出的输出为零，这又导致了我们网络中神经元死亡的问题。</li><li id="9ea3" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">大的学习率可能导致这个问题，因为大的L.R .将导致b中更大的更新，因此更有可能变成负值。</li></ul><p id="5291" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">泄漏继电器</strong></p><figure class="kb kc kd ke fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nc"><img src="../Images/9e8d46e579b5e63ca48a957c4ff9f220.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xP31TATV4R-IowGxHahrmw.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">泄漏ReLU激活功能</figcaption></figure><ul class=""><li id="e75f" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">漏ReLU是Relu函数的一个微小修改。</li><li id="b7ba" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">它通过为x&lt;0设置0.01x解决了我们面临的ReLU(负值作为输入)问题。</li></ul><p id="30e0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这就是本文的全部内容。您还可以探索其他激活功能，其中一些是</p><ul class=""><li id="df7e" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">Softmax激活功能</li><li id="d822" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">格鲁</li><li id="b9ff" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">ELU</li></ul></div></div>    
</body>
</html>