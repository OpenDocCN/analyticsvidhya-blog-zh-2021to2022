<html>
<head>
<title>Linear Regression Made Simple</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归变得简单</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-made-simple-d5315db78a14?source=collection_archive---------1-----------------------#2021-04-24">https://medium.com/analytics-vidhya/linear-regression-made-simple-d5315db78a14?source=collection_archive---------1-----------------------#2021-04-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="7f24" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">机器学习背后的基础数学</h2></div><p id="3273" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">许多人，尤其是该领域的新手，经常对机器学习领域表现出极大的兴趣，但回避管理它的数学。我希望通过用简单的术语解释机器学习背后的数学来为你揭开它的神秘面纱。线性回归是监督学习中最基本的数学算法之一。</p><blockquote class="js jt ju"><p id="57b0" class="iw ix jv iy b iz ja ii jb jc jd il je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated"><em class="hh">你可能会问，</em>“什么是线性回归？”</p></blockquote><p id="c2af" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在英语中，回归意味着回到欠发达状态。技术定义不会偏离太远。回归是一种分析方法，用于理解两个或多个变量之间的关系。这是预测和优化的一个有价值的工具。</p><p id="7b64" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们开始用图形来理解线性回归和梯度下降背后的原理。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es jz"><img src="../Images/9e67e4f5054397360ea014b166407d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*paxK2DHCyFZkehm2Rka8Aw.jpeg"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">作者图片</figcaption></figure><h1 id="1852" class="kp kq hh bd kr ks kt ku kv kw kx ky kz in la io lb iq lc ir ld it le iu lf lg bi translated">简单线性回归</h1><p id="848c" class="pw-post-body-paragraph iw ix hh iy b iz lh ii jb jc li il je jf lj jh ji jj lk jl jm jn ll jp jq jr ha bi translated">让我们任意假设，我们有如下所示的点分布:</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es lm"><img src="../Images/d79d750bcc78e7d0876c84028f077514.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mERNIwhgRjucLJmqLPMsYg.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">作者图片</figcaption></figure><p id="ad37" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">线性回归寻找这种分布的近似值——最好地解释给定分布的直线，最好地描述数据模式的直线。让我们称之为<em class="jv">最佳拟合线</em>。直觉上，线性回归解释了这个确切的事情。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es ln"><img src="../Images/9ff52bbf1cd1ee30e66bc7fc068c646d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pj2KPBiT68PxxOwrBvzwPg.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">作者图片</figcaption></figure><p id="165c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以用代数方法将上面的线表示为<em class="jv"> y = h(x) </em>，其中:</p><blockquote class="lo"><p id="df31" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated"><em class="ly"> h(x) = </em> θ₀ + θ₁ <em class="ly"> x </em></p></blockquote><p id="592b" class="pw-post-body-paragraph iw ix hh iy b iz lz ii jb jc ma il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated"><em class="jv"> h </em>代表<em class="jv">假设</em> , <em class="jv"> θ </em>代表参数，<em class="jv"> x </em>是特征。对数据集执行线性回归的目的是找到<em class="jv"> θ₀ </em>和<em class="jv"> θ₁ </em>的最佳值，使得<em class="jv"> h(x </em>)最适合解释数据的行为。</p><p id="13bd" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上面我们看到了什么叫做<em class="jv">简单线性回归</em>，只涉及一个<em class="jv">解释变量</em>。这种线性回归方法有其使用案例，但更多的时候，我们发现自己需要分析不止一个特征。这就引出了<em class="jv">多元线性回归。</em></p><h1 id="f00b" class="kp kq hh bd kr ks kt ku kv kw kx ky kz in la io lb iq lc ir ld it le iu lf lg bi translated">多元线性回归</h1><p id="51c9" class="pw-post-body-paragraph iw ix hh iy b iz lh ii jb jc li il je jf lj jh ji jj lk jl jm jn ll jp jq jr ha bi translated">多元线性回归类似于简单线性回归，与只考虑一个不同的特征相比，多元线性回归更加复杂。最佳拟合线(假设函数)的等式现在看起来像这样:</p><blockquote class="lo"><p id="63e1" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">h(x) = θ₀ + θ₁ x₁ + θ₂ x₂ + θ₃ x₃ + … + θₙ xₙ</p></blockquote><p id="bca3" class="pw-post-body-paragraph iw ix hh iy b iz lz ii jb jc ma il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated"><em class="jv"> θ </em>项代表参数，也称为权重，很容易理解为什么。从等式<em class="jv"> h(x) </em>中可以清楚地看出，参数决定了特性对系统行为的影响程度。我们也可以说<em class="jv"> θᵢ </em>定义了特定特性<em class="jv"> xᵢ </em>在最终输出中的重要性。</p><p id="b32a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了更好地理解我们如何在现实世界中应用多元线性回归，让我们以著名的房价算法为例。这里，我们使用房子的特征(卧室数量、浴室数量、面积和位置)来确定房子的价格。我们也可以考虑其他特征，但是为了这个例子，让我们考虑这四个。</p><figure class="ka kb kc kd fd ke"><div class="bz dy l di"><div class="me mf l"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">在上面的图表中，我们可以说位置与基于一些任意标准的星等相关联。</figcaption></figure><p id="1871" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用我们的方程计算第一座房子的最佳拟合线，我们得到:</p><blockquote class="lo"><p id="76c6" class="lp lq hh bd lr ls lt lu lv lw lx jr dx">30 = θ₀ + 1 . θ₁ + 1 . θ₂ + 500 . θ₃ + 3 . θ₄</p></blockquote><p id="899b" class="pw-post-body-paragraph iw ix hh iy b iz lz ii jb jc ma il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">同样，我们可以为所有房屋创建一个方程，从而找到最佳参数。</p><h2 id="c90b" class="mg kq hh bd kr mh mi mj kv mk ml mm kz jf mn mo lb jj mp mq ld jn mr ms lf mt bi translated">损失函数</h2><p id="bb14" class="pw-post-body-paragraph iw ix hh iy b iz lh ii jb jc li il je jf lj jh ji jj lk jl jm jn ll jp jq jr ha bi translated">为了简单起见，让我们假设一个只有一个解释变量的系统。两个变量的系统需要我们画一个二维平面，三个解释变量需要一个超平面来表示。超平面是不可能可视化的，所以让我们坚持一个解释变量。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es mu"><img src="../Images/b05612db6e476f54ad70710746795d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xfgCIXfyEC9DkSzHLtlP2A.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">作者图片</figcaption></figure><p id="6f1e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以清楚地看到，我们的近似最佳拟合线与许多点相距甚远。我们把一个点到最佳拟合线的垂直偏差叫做<em class="jv"> e </em>。这个<em class="jv"> e </em>是预测值和实际值之差。然后，我们的目标是最小化σ<em class="jv">e</em>。为此，让我们引入一个函数:</p><blockquote class="lo"><p id="0eb6" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">j(θ)=σ<em class="ly">e</em></p></blockquote><p id="7cb5" class="pw-post-body-paragraph iw ix hh iy b iz lz ii jb jc ma il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated"><em class="jv"> J(θ </em>)称为损失函数，或误差函数。为了进一步分析和最小化损失函数，让我们展开<em class="jv"> e. </em></p><p id="8cf7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="jv"> e </em>等于实际点和最佳拟合估计线上的点之间的垂直距离。因为我们不希望距离在求和时相互抵消，所以我们求这个值的平方。</p><blockquote class="lo"><p id="a7a0" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated"><em class="ly">e</em>₁<em class="ly">=(h(x</em>₁<em class="ly">)-y</em>₁)，e₂ = (h(x₂) - y₂)等等…</p></blockquote><p id="99d9" class="pw-post-body-paragraph iw ix hh iy b iz lz ii jb jc ma il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">因此，我们有:</p><blockquote class="lo"><p id="9ef4" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">j(θ)=(1/2m)σ(h(xᵢ)-yᵢ)，其中I从0到m，m是例子的总数。</p></blockquote><p id="c881" class="pw-post-body-paragraph iw ix hh iy b iz lz ii jb jc ma il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">我们取<em class="jv"> 1/m </em>的原因是因为我们的求和值会不必要的大，我们取<em class="jv"> 1/2 </em>是为了简化计算，你将在后面看到。这就是我们如何得出损失函数的。这种计算误差的方法叫做<em class="jv">均方误差。</em></p><p id="07de" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">外推我们的损失函数以考虑多个额外维度，我们有:</p><blockquote class="lo"><p id="5c28" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">j(θ)=(1/2m)σ(θ₀+θ₁x₁<em class="ly">ᵢ</em>+θ₂x₂<em class="ly">ᵢ</em>+θ₃x₃<em class="ly">ᵢ</em>+…+θₙxₙ<em class="ly">ˇ</em>-y<em class="ly">ˇ</em>)，i: 0</p></blockquote><p id="4b51" class="pw-post-body-paragraph iw ix hh iy b iz lz ii jb jc ma il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">为了找到最佳参数，我们采用了一种称为<em class="jv">梯度下降</em>的方法。</p><h2 id="b176" class="mg kq hh bd kr mh mi mj kv mk ml mm kz jf mn mo lb jj mp mq ld jn mr ms lf mt bi translated">寻找曲线上一点的斜率</h2><p id="7afd" class="pw-post-body-paragraph iw ix hh iy b iz lh ii jb jc li il je jf lj jh ji jj lk jl jm jn ll jp jq jr ha bi translated">要使用梯度下降法寻找最优参数，我们需要知道如何找到曲线上每一点的斜率。为了图形化地理解这个过程，让我们再次假设一个简单的线性回归的例子，其中<em class="jv"> h(x) = θ₀ + θ₁ x </em>。</p><blockquote class="lo"><p id="c20a" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">j(θ)=(1/2m)σ(θ₀+θ₁x₁ᵢ-yᵢ)，i: 0 ➡ m</p></blockquote><p id="c985" class="pw-post-body-paragraph iw ix hh iy b iz lz ii jb jc ma il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">根据<em class="jv"> J(θ) </em>的方程，我们可以知道<em class="jv"> J(θ) </em>和θ₁之间的曲线将是抛物线，因为抛物线的方程是<em class="jv"> y = ax + bx + c. </em></p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es mv"><img src="../Images/eac9544d8e2067df1b59a3d9c6097b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4w5oDsc59cgXQ04FyLlrcw.jpeg"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">作者图片</figcaption></figure><p id="8ffe" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">再一次，这清楚地说明了为什么我选择仅仅用图形来表示一个简单的线性回归的例子，因为任何更多的东西都需要超越三维。</p><p id="042f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">显而易见，为了最小化<em class="jv"> J(θ) </em>，我们必须找到曲线的局部最小值。使用基本微积分，要找到曲线上特定点的切线斜率，我们必须对函数进行微分，并代入对应于该特定点的值。在这里应用这些知识，使用偏导数，因为<em class="jv"> J(θ) </em>是<em class="jv"> θ₀ </em>和<em class="jv"> θ₁ </em>的函数，我们得到:</p><blockquote class="lo"><p id="a474" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated"><strong class="ak">∂</strong>j(θ)/<strong class="ak">∂</strong>θ₀=(1/2m)2σ(θ₀+θ₁x₁ᵢ-yᵢ)(1+0–0)，i: 0 ➡ m</p><p id="5de2" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">∂j(θ)/∂θ₀=(1/m)σ(θ₀+θ₁x₁ᵢ-yᵢ)，i: 0 ➡ m</p><p id="0180" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">∂j(θ)/∂θ₀=(1/m)σ(h(xᵢ)-yᵢ)，i: 0 ➡ m</p></blockquote><p id="f60d" class="pw-post-body-paragraph iw ix hh iy b iz lz ii jb jc ma il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">现在我们明白为什么我在定义损失函数时在分母中引入了一个<em class="jv"> 2 </em>了。</p><p id="73a9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">相对于<em class="jv"> θ₁ </em>对<em class="jv"> J(θ) </em>进行部分微分，我们得到:</p><blockquote class="lo"><p id="ef56" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">∂j(θ)/∂θ₁=(1/2m)2σ(θ₀+θ₁x₁ᵢ-yᵢ)(0+x₁ᵢ-0)，i: 0 ➡ m</p><p id="2106" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">∂j(θ)/∂θ₁=(1/m)σ(θ₀+θ₁x₁ᵢ-yᵢ)(x₁ᵢ)，i: 0 ➡ m</p><p id="57c5" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">∂j(θ)/∂θ₁=(1/m)σ(h(xᵢ)-yᵢ)(x₁ᵢ)，i: 0 ➡ m</p></blockquote><p id="afde" class="pw-post-body-paragraph iw ix hh iy b iz lz ii jb jc ma il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">我们可以引入一个特征<em class="jv"> x₀ </em>到<em class="jv"> h(x) </em>，其中<em class="jv"> x₀ = 1。</em></p><blockquote class="lo"><p id="a68d" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">h(x)=θ₀x₀+θ₁x₁+θ₂x₂+θ₃x₃+…+θₙxₙ，其中x₀ = 1</p></blockquote><p id="d1d0" class="pw-post-body-paragraph iw ix hh iy b iz lz ii jb jc ma il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">那么我们可以把<em class="jv"> ∂J(θ)/∂θ₀ </em>改写成:</p><blockquote class="lo"><p id="9b6d" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">∂j(θ)/∂θ₀=(1/m)σ(h(xᵢ)-yᵢ)(x₀ᵢ)，i: 0 ➡ m</p></blockquote><p id="1a40" class="pw-post-body-paragraph iw ix hh iy b iz lz ii jb jc ma il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">观察<em class="jv"> ∂J(θ)/∂θ₀和</em>∂j(θ)/∂θ₁，我们可以做出这样的概括:</p><blockquote class="lo"><p id="14b3" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">∂j(θ)/∂θₙ=(1/m)σ(h(xᵢ)-yᵢ)(xₙᵢ)，i: 0 ➡ m</p></blockquote><p id="da87" class="pw-post-body-paragraph iw ix hh iy b iz lz ii jb jc ma il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">这就是我们如何计算曲线上一点处切线的斜率。</p><h2 id="67dc" class="mg kq hh bd kr mh mi mj kv mk ml mm kz jf mn mo lb jj mp mq ld jn mr ms lf mt bi translated">梯度下降</h2><p id="1357" class="pw-post-body-paragraph iw ix hh iy b iz lh ii jb jc li il je jf lj jh ji jj lk jl jm jn ll jp jq jr ha bi translated">梯度下降是一种寻找函数局部极小值的迭代优化算法。让我们通过一步一步的可视化来理解它。</p><p id="05ca" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们在曲线上选择任意一点，求该点切线的斜率。上面我们已经有了它的方程式。该点的斜率告诉我们最小值在哪个方向。</p><p id="acd1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最小值是<em class="jv"> J(θ) </em>的值最小的点，这意味着在这个点我们将找到我们的最佳参数值。</p><p id="7ecc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如，如果这一点的斜率是负的，这就告诉我们，最小值是向右的，如果斜率是正的，最小值是向左的。换句话说，切线的斜率将我们指向局部最小值。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es mw"><img src="../Images/681d9e7fef2cf07297ae7bc44da7c223.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9w0S515Md-vA8LH5tGlzFQ.jpeg"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">作者图片</figcaption></figure><p id="0e1e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">从理论上讲，现在我们知道了前进的方向，通过在正确的方向上逐步迭代，我们最终一定会接近最优解。</p><blockquote class="js jt ju"><p id="7642" class="iw ix jv iy b iz ja ii jb jc jd il je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated"><em class="hh">现在你可能会问，</em>“我们如何知道每次迭代我们必须移动多少？”</p></blockquote><p id="1d8c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这就把我们带到了本文的最后一个主题，即<em class="jv">学习率。</em></p><h2 id="8b31" class="mg kq hh bd kr mh mi mj kv mk ml mm kz jf mn mo lb jj mp mq ld jn mr ms lf mt bi translated">学习率</h2><p id="a8c2" class="pw-post-body-paragraph iw ix hh iy b iz lh ii jb jc li il je jf lj jh ji jj lk jl jm jn ll jp jq jr ha bi translated">梯度下降使用一个称为学习率的超参数<em class="jv"> α </em>，来控制我们向最优解迈进的每一步的大小。学习率太小将导致达到最优解所需的时期太多，而学习率太大将有超过最小值和发散的风险。</p><p id="cc13" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">完美的<em class="jv"> α </em>将从相对较高的位置开始，并且随着我们接近最小值而逐渐变小，以便在历元数和精度之间找到完美的平衡。这可以通过使α与该点的斜率成比例来实现。斜率越大，我们在最小值的方向上迈出的步伐就越大。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es mx"><img src="../Images/98a0975b88fcf01d6a7840ec4a8be046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*An4tZEyQAYgPAZl396JzWg.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">图片由<a class="ae my" href="https://www.jeremyjordan.me/nn-learning-rate/" rel="noopener ugc nofollow" target="_blank">杰瑞米·乔登</a></figcaption></figure><p id="851e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在数学上，我们将学习率纳入我们的最小值方法，如下所示:</p><blockquote class="lo"><p id="3faf" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">θₙ:=θₙ-<em class="ly">α(</em>∂j(θₙ)/∂θₙ<em class="ly">)，或者</em></p><p id="635f" class="lp lq hh bd lr ls lt lu lv lw lx jr dx translated">θₙ := θₙ - α(斜率)</p></blockquote><h2 id="ad5e" class="mg kq hh bd kr mh mz mj kv mk na mm kz jf nb mo lb jj nc mq ld jn nd ms lf mt bi translated">梯度下降优化</h2><p id="c93a" class="pw-post-body-paragraph iw ix hh iy b iz lh ii jb jc li il je jf lj jh ji jj lk jl jm jn ll jp jq jr ha bi translated">当误差函数不像抛物线并且具有多个最小值时，梯度下降概念的一个问题出现了。在这种情况下，我们总是希望找到函数的全局极小值。</p><p id="39ba" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">梯度下降带来了陷入局部极小值的风险，这取决于我们在哪里随机初始化我们的权重(从哪里开始梯度下降过程)。</p><p id="171a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对此的解决方案之一是用不同的初始化权重运行该算法几次，希望其中一次找到的最小值是全局的。</p><p id="6286" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">另一个解决方案是使用动量，这是梯度下降的优化，可以想象为我们把球推下山(函数)。原则上，球会滚过局部最小值(假设它从足够高的点开始)，如果我们加入空气阻力，球最终会到达全局最小值。</p></div><div class="ab cl ne nf go ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ha hb hc hd he"><p id="58e3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你一路阅读，并发现这很有用，谢谢！请随意留下建议或评论。</p></div></div>    
</body>
</html>