<html>
<head>
<title>Comparing PCA with Autoencoders for reducing dimensions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA与自动编码器在降维方面的比较</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/comparing-pca-with-deep-learning-for-reducing-dimensions-d291b965c454?source=collection_archive---------6-----------------------#2021-09-08">https://medium.com/analytics-vidhya/comparing-pca-with-deep-learning-for-reducing-dimensions-d291b965c454?source=collection_archive---------6-----------------------#2021-09-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/5fa6b5a07ec179b64a2f5d098be2b9f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H2tDYUacW5QJRxBk_xBpUQ.jpeg"/></div></div></figure><p id="8a1b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们经常需要在监督和非监督设置中降低特征空间的维度，然而，评估监督问题中的维度降低技术相对更容易，因为我们可以根据目标目标(最大化准确度、最小化验证损失、精确度、召回率等)来处理投影编码的性能。</p><p id="f994" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">由于其计算效率，PCA经常被用作优选的降维技术。然而，简单的神经网络可以胜过基于PCA的编码。我玩过基于玩具神经网络的自动编码器，并将输出编码与使用PCA生成的编码进行了比较。</p><p id="c242" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">此分析的代码可在此处找到:<a class="ae jn" href="https://github.com/NaveenBansal-creater/projects/blob/master/DimensionalityReduction/dimensionality_reduction.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/NaveenBansal-creater/projects/blob/master/DimensionalityReduction/dimensionality _ reduction . ipynb</a></p><h1 id="b17b" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">为实验搭建平台</h1><p id="c271" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated"><strong class="ir hi">数据</strong> : <em class="kr">使用scikit生成合成数据用于分类目的。共有50个要素的10000个样本，其中20个信息要素具有两个输出类。下面是每个特性的方差图。请注意，特征中存在的信息与特征中存在的方差相关。做这个练习是为了把这个50维的数据压缩到5维。</em></p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es ks"><img src="../Images/08a9f35deaee764545b8cadfa1d3b624.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*O-WfWR7yKW3kvEtHH4g3gg.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">每个特征的信息量</figcaption></figure><h1 id="8764" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">目标:</strong></h1><blockquote class="lb lc ld"><p id="19b9" class="ip iq kr ir b is it iu iv iw ix iy iz le jb jc jd lf jf jg jh lg jj jk jl jm ha bi translated"><strong class="ir hi">重构误差:</strong>比较压缩到5维空间时丢失信息的差异。</p><p id="c119" class="ip iq kr ir b is it iu iv iw ix iy iz le jb jc jd lf jf jg jh lg jj jk jl jm ha bi translated"><strong class="ir hi">准确性:</strong>如果与目标相关，则比较编码的性能。</p><p id="c542" class="ip iq kr ir b is it iu iv iw ix iy iz le jb jc jd lf jf jg jh lg jj jk jl jm ha bi translated"><strong class="ir hi">解释:</strong>比较输出特征之间的相关性</p></blockquote><p id="30d5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">出于实验目的，选择了三种神经网络变体。</p><ol class=""><li id="06ad" class="lh li hh ir b is it iw ix ja lj je lk ji ll jm lm ln lo lp bi translated"><strong class="ir hi">普通自动编码器(AE) </strong></li></ol><p id="80c4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">从50维到5维执行线性变换，即在编码器或解码器层之后不使用激活。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/298afe3b26b8b0fc542470afe66b996f.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*vktAlvDkJBOZXHHgWHOk8Q.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">没有激活的普通自动编码器</figcaption></figure><p id="09b3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 2。堆叠式自动编码器(Stacked_AE) </strong></p><p id="4989" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用深度自动编码器，在编码器和解码器侧堆叠一个额外的层。隐藏层1的大小是20，隐藏层2的大小是5(将作为编码器输出)。这种架构将需要更多的时间来训练，因为与上面的普通网络相比，需要更多的参数。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lr"><img src="../Images/0ae182639f420bb3a10583353bfeefd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZkZnPCZjcL4cefIN81BNw.png"/></div></div></figure><p id="b414" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 3。带非线性激活的堆叠式自动编码器(NL_AE) </strong></p><p id="5f63" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该网络的架构与上面的堆叠AE相同，除了在隐藏层1和隐藏层2之后应用了relu激活单元。注意，在该模型中，参数的数量与模型2相同，但是收敛时间可能因激活单元而不同。</p><h1 id="34cb" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">培养</h1><p id="d713" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">网络被训练以产生相同的输入，该输入馈入输入层。均方误差用作损失标准。相同的训练参数用于训练具有SGD优化器和恒定学习率0.01的每个网络。使用早期停止的耐心为5，以避免网络过度适应，并立即停止训练。</p><h1 id="0062" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结果</h1><h2 id="a7d2" class="ls jp hh bd jq lt lu lv ju lw lx ly jy ja lz ma kc je mb mc kg ji md me kk mf bi translated">重建误差</h2><p id="d139" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">重构误差给出了在数据压缩期间信息丢失的概念。<em class="kr">重建误差越大，信息损失越多</em>。这是原始数据和编码数据之间的均方误差。相同的输入通过经过训练的PCA来比较重建误差。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/af618566ead915fb7e509e0dcf90a09c.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*yeVYPxNHvGt50vIsqQ4Z3w.png"/></div></figure><p id="c836" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">即使对于小网络，重构误差也几乎与PCA相同。如果我们使用更深的网络或微调一些更高的参数和激活，神经网络将很容易在重建相同数据时击败PCA。</p><h2 id="39ef" class="ls jp hh bd jq lt lu lv ju lw lx ly jy ja lz ma kc je mb mc kg ji md me kk mf bi translated">输出编码中的信息内容</h2><p id="8d6b" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">信息内容与编码的单个组成部分中存在的方差(或std)相关。在PCA的情况下，最大方差由第一个主分量解释，随后是下一个主分量，但是在自动编码器中没有这样的关系。在PCA中，方差/标准差将始终按排序顺序排列。下面是每个输出分量的标准偏差的比较。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es mh"><img src="../Images/9fe316d37e9818b362678869f69c3e78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*A-FOaPX5xyn3K3k0GUT9uA.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">输出特征的标准偏差比较</figcaption></figure><h2 id="edae" class="ls jp hh bd jq lt lu lv ju lw lx ly jy ja lz ma kc je mb mc kg ji md me kk mf bi translated">分类性能</h2><p id="aff2" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">根据来自每个模型的输出编码、PCA和原始未压缩数据(50维)来训练单独的随机森林模型。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es mi"><img src="../Images/e4c6f5456221357d73c762bc2a14d784.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*GKXuNzg_BG9Tuc7TTinVPg.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">性能比较</figcaption></figure><p id="84ac" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">具有激活的自动编码器的分类性能比所有三个模型都好，事实上它几乎与原始数据集相当，表明最小的信息损失。</p><h2 id="8b45" class="ls jp hh bd jq lt lu lv ju lw lx ly jy ja lz ma kc je mb mc kg ji md me kk mf bi translated">相关性检验</h2><p id="066e" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">观察PCA和自动编码器中输出编码的不同分量之间的相关性是很有趣的。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mj"><img src="../Images/d01cc52a317cb532738fa4fc2f975f5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sgWWeima29OCXZp1ZbU6ZA.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">特征关联热图</figcaption></figure><p id="8a2f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">PCA中的所有分量都是相互正交的，这意味着分量之间没有相关性。在第一个情节中也可以看到同样的情况。然而，在自动编码器的情况下没有这样的正交约束，这就是为什么在不同的组件之间有一些相关性。</p><h1 id="7246" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">裁决</h1><ol class=""><li id="b2bb" class="lh li hh ir b is km iw kn ja mk je ml ji mm jm lm ln lo lp bi translated">自动编码器的编码可以是相关的，因为它们被训练来无任何约束地重建数据，但是PCA的编码由于正交性约束而不具有任何相关性。</li><li id="12db" class="lh li hh ir b is mn iw mo ja mp je mq ji mr jm lm ln lo lp bi translated">PCA训练起来快多了。</li><li id="d38d" class="lh li hh ir b is mn iw mo ja mp je mq ji mr jm lm ln lo lp bi translated">一个普通的自动编码器和基于复杂数学的PCA一样好。</li></ol><p id="4eb9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">4.如果数据是非线性的，PCA会非常失败，但是自动编码器在这种情况下也能很好地工作。</p></div></div>    
</body>
</html>