<html>
<head>
<title>A Guide to Building Your First Data Science Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建首个数据科学项目的指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-guide-to-building-your-first-data-science-project-e5f814b098a7?source=collection_archive---------1-----------------------#2021-05-04">https://medium.com/analytics-vidhya/a-guide-to-building-your-first-data-science-project-e5f814b098a7?source=collection_archive---------1-----------------------#2021-05-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="b9f1" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">从头到尾</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/2032f7bbe003ed51e57cba18fbcec272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A8Qv88LHuGO8yhvTWqm3kg.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">“步入”数据科学领域</figcaption></figure><p id="0320" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我们听说过一些流行词汇:<em class="ki">数据科学、机器学习、预测建模</em>但是它们是什么意思呢？我们如何在现实世界中使用这项技术来做出有影响力的决策？</p><p id="aab6" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">这个端到端项目就是为了展示这一点而创建的。</p></div><div class="ab cl kj kk go kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="ha hb hc hd he"><p id="3ea3" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated"><a class="ae kq" href="https://www.ibm.com/cloud/learn/data-science-introduction" rel="noopener ugc nofollow" target="_blank">数据科学</a>可以定义为科学方法、数学、专业编程、高级分析、人工智能和讲故事的结合，以揭示隐藏在数据中的商业洞察力。让我们简化这个定义:我将数据科学描述为从更大的数据源中获取有用信息的过程。</p><p id="1fd9" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">那么，我们如何知道哪些信息是有用的呢？这就是机器学习的用武之地。机器学习为系统和用户(像我们一样)提供了使用<em class="ki">算法根据我们的数据捕捉隐藏见解的能力。</em>算法通过接受输入数据，使用统计建模来预测输出值。</p><p id="7dee" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">有两种机器学习任务，它们是根据输入数据的类型来区分的。<em class="ki">监督学习</em>在开发模型时使用标记输出来显示输入和输出数据之间的关系。<em class="ki">无监督学习</em>没有明确标记的输出，因此只使用给定的数据点集开发模型。</p><p id="83cc" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我们可以根据结果类型进一步分割监督学习:<em class="ki">分类</em>，其中我们的预测结果是二元的(0/1)或<em class="ki">回归</em>，其中我们的预测结果是连续的。当处理机器学习问题时，理解结果的类型很重要——两种问题类型之间的思维过程和方法是不同的。</p><p id="43ca" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated"><strong class="jo hi"> <em class="ki">本文将使用弗雷明汉心脏研究<a class="ae kq" href="https://www.kaggle.com/amanajmera1/framingham-heart-study-dataset" rel="noopener ugc nofollow" target="_blank">数据集</a>详细介绍端到端项目</em> </strong>的步骤。该数据来自一项正在进行的对马萨诸塞州弗雷明汉个人的心血管研究，研究参与者根据15个不同的变量监测冠心病(CHD)的风险。利用这个数据集，我们将确定与结果最相关的变量，并预测被诊断为冠心病的总体风险。</p><p id="9cee" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">让我们开始吧。</p><blockquote class="kr ks kt"><p id="afbd" class="jm jn ki jo b jp jq ii jr js jt il ju ku jw jx jy kv ka kb kc kw ke kf kg kh ha bi translated">注意:本文假设您具备Python IDE的工作知识。这篇文章的所有代码和可视化都是在Jupyter笔记本上创建的，可以在我的<a class="ae kq" href="https://github.com/Hpanuganty/Heart_Disease_Classification" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到</p></blockquote><h2 id="dc11" class="kx ky hh bd kz la lb lc ld le lf lg lh jv li lj lk jz ll lm ln kd lo lp lq lr bi translated"><strong class="ak">第一步:定义问题</strong></h2><p id="d52b" class="pw-post-body-paragraph jm jn hh jo b jp ls ii jr js lt il ju jv lu jx jy jz lv kb kc kd lw kf kg kh ha bi translated">由于输出标签是提供给我们的(TenYearCHD)，我们知道这是一个<em class="ki">监督的</em>问题。我们希望将个人分为两类:患冠心病的人(1)和不患冠心病的人(0)，因此这是一个<em class="ki">分类</em>问题。</p><h2 id="7534" class="kx ky hh bd kz la lb lc ld le lf lg lh jv li lj lk jz ll lm ln kd lo lp lq lr bi translated"><strong class="ak">第二步:数据加载</strong></h2><p id="a17e" class="pw-post-body-paragraph jm jn hh jo b jp ls ii jr js lt il ju jv lu jx jy jz lv kb kc kd lw kf kg kh ha bi translated">我们将使用Python库NumPy、Pandas和Seaborn进行数据加载、探索和可视化。Seaborn library建立在Matplotlib的基础上，产生了清晰易读的可视化效果。</p><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="be25" class="kx ky hh ly b fi mc md l me mf"><em class="ki">#Data loading and visualization </em><br/>import numpy as np <br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span></pre><p id="0292" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">读入数据后(数据集可以从Kaggle下载为CSV文件)，下一步是在进入数据清理之前检查数据——我们希望了解数据集的形状、不同的数据类型、包含的变量和目标变量。</p><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="d3ff" class="kx ky hh ly b fi mc md l me mf"><em class="ki">#Reading in data</em><br/>fhs = pd.read_csv('/Users/harikapanuganty/Desktop/framingham.csv')<br/>fhs.head()</span><span id="b18c" class="kx ky hh ly b fi mg md l me mf"><em class="ki">#Shape of data</em><br/>fhs.shape</span><span id="20c5" class="kx ky hh ly b fi mg md l me mf"><em class="ki">#Information about a DataFrame<br/></em>fhs.info()</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mh"><img src="../Images/075aa61c88d712c6283122905b314998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VwKpdniLpqY0wve7o7C3kg.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">数据集中15个变量和目标的描述</figcaption></figure><h2 id="1462" class="kx ky hh bd kz la lb lc ld le lf lg lh jv li lj lk jz ll lm ln kd lo lp lq lr bi translated"><strong class="ak">第三步:数据清理</strong></h2><p id="833b" class="pw-post-body-paragraph jm jn hh jo b jp ls ii jr js lt il ju jv lu jx jy jz lv kb kc kd lw kf kg kh ha bi translated">在现实世界的数据集和项目中，我们不会得到一个整洁干净的CSV文件——会有不一致的数据类型、缺失/空值和重复。俗话说，“垃圾进，垃圾出”:机器学习模型只能和它给定的输入数据一样好。</p><p id="7379" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">这个Kaggle数据集相对干净，但我们将检查和处理空值。</p><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="dc2a" class="kx ky hh ly b fi mc md l me mf"><em class="ki">#Looking for null values<br/></em>fhs.isnull().sum() </span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mi"><img src="../Images/2de32542f4f506c288898b43a6d0a0fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*IbZCYuaN2eCDT3qWrSDa8A.jpeg"/></div></figure><p id="a895" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我们注意到有几列有一个或多个缺失值。<a class="ae kq" href="https://towardsdatascience.com/how-to-deal-with-missing-data-in-python-1f74a9112d93" rel="noopener" target="_blank">处理缺失数据</a>的两种最流行的方法是<em class="ki">删除受影响的行</em>(或全部列)或<em class="ki">输入缺失值</em>。基于该列和缺失值的数量，我选择在删除行、用平均值和插值填充空值之间交替进行。</p><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="48cb" class="kx ky hh ly b fi mc md l me mf">fhs = fhs.dropna(subset = ['heartRate'])</span><span id="3e7c" class="kx ky hh ly b fi mg md l me mf">fhs['cigsPerDay'].fillna(int(fhs['cigsPerDay'].mean()), inplace=True)</span><span id="8646" class="kx ky hh ly b fi mg md l me mf">fhs['totChol'].fillna(int(fhs['totChol'].mean()), inplace=True)</span><span id="791c" class="kx ky hh ly b fi mg md l me mf">fhs['BPMeds'].fillna(int(fhs['BPMeds'].mean()), inplace=True)</span><span id="9bbc" class="kx ky hh ly b fi mg md l me mf">fhs['BMI'].interpolate(method='pad', direction = 'forward', inplace=True)</span><span id="987f" class="kx ky hh ly b fi mg md l me mf">fhs['glucose'].interpolate(method='pad', direction = 'forward', inplace=True)</span><span id="50fd" class="kx ky hh ly b fi mg md l me mf">fhs.dropna(subset = ['education'], inplace=True)</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mj"><img src="../Images/e3dadf5d395360bbb47aa414613c9470.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*XAIiMSF9F6rbhQ0G1z_xgQ.jpeg"/></div></figure><h2 id="0e2d" class="kx ky hh bd kz la lb lc ld le lf lg lh jv li lj lk jz ll lm ln kd lo lp lq lr bi translated"><strong class="ak">第四步:探索性数据分析</strong></h2><p id="9d44" class="pw-post-body-paragraph jm jn hh jo b jp ls ii jr js lt il ju jv lu jx jy jz lv kb kc kd lw kf kg kh ha bi translated">之前，我们做了一些简短的探索，以更好地理解我们的数据集。在这一步中，我们将使用6种不同的图表深入研究每个变量以及变量与结果之间的关系。</p><ul class=""><li id="055f" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi">箱线图</strong></li></ul><p id="9c36" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我们可以通过绘制箱线图来识别异常值(T21)。在盒子的上下线之外的任何数据点都是明显的异常值(像在<code class="du mt mu mv ly b">totChol </code>和<code class="du mt mu mv ly b">sysBP</code>列中的极端数据点),并且需要被去除。</p><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="bb2f" class="kx ky hh ly b fi mc md l me mf">plt.figure(figsize=(20,35))<br/>sns.boxplot(data=fhs)<br/>plt.show()</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mw"><img src="../Images/1d971edba7b2ff443c43a8f46a32389f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*XZlKbHoyNe_2hiDYHosxkw.jpeg"/></div></figure><ul class=""><li id="1cdd" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi">关联热图</strong></li></ul><p id="54b8" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">热图显示了所有变量与目标变量之间的<em class="ki">相关性。阅读热图很简单，我们需要做的就是将网格中方块的颜色与边栏上的值进行比较。</em></p><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="3f7d" class="kx ky hh ly b fi mc md l me mf">fhs_corr = fhs.corr()<br/>plt.figure(figsize=(20,10))<br/>sns.heatmap(fhs_corr)<br/>plt.title("Correlation between features", size=20)<br/>fhs_corr</span></pre><p id="45c3" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">当侧栏上的值为:<br/>接近0时，两个变量之间没有线性相关性<br/>接近+1时，两个变量之间有正相关性<br/>接近-1时，两个变量之间有负相关性</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mx"><img src="../Images/369669568d95af37b6b8923302cf57bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MK6ZXZIsBbIBfu6lOSc9cg.jpeg"/></div></div></figure><p id="8bc5" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">例如，与x轴上的<code class="du mt mu mv ly b">TenYearCHD</code>正方形相比，y轴上的<code class="du mt mu mv ly b">sysBP </code>正方形的颜色是浅粉紫色，对应于侧边栏上的~0.3。这表明<code class="du mt mu mv ly b">sysBP</code>与<code class="du mt mu mv ly b">TenYearCHD</code>变量正相关。</p><ul class=""><li id="1f12" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi"> Distplots </strong></li></ul><p id="8f30" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">Displots显示每个变量的频率分布和潜在偏差，这些信息在以后的步骤中会派上用场。变量分布在我们为最终特征选择的方法类型和特征缩放中起作用。</p><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="d816" class="kx ky hh ly b fi mc md l me mf">numeric_vars_fhs = ['cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']<br/>for var in numeric_vars_fhs:<br/>    plt.figure(figsize=(8,8))<br/>    sns.distplot(fhs[var])<br/>    plt.title('{} Distribution'.format(var))<br/>    plt.show()</span></pre><div class="ix iy iz ja fd ab cb"><figure class="my jb mz na nb nc nd paragraph-image"><img src="../Images/69959fd08af84ae0aa5d14261ee1fcf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*YekpjP1RBe4WtcoPApH__g.jpeg"/></figure><figure class="my jb ne na nb nc nd paragraph-image"><img src="../Images/ffea78b2046a6250e9864a7d3fdd4528.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*gxe8ehhze1jXHrMAYqyr8A.jpeg"/></figure></div><p id="a9b7" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">正态分布变量的一个例子是<code class="du mt mu mv ly b">sysBP</code>，而<code class="du mt mu mv ly b">cigsPerDay</code>是高度偏斜和向右倾斜的。</p><ul class=""><li id="0ba7" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi">柱状图</strong></li></ul><p id="90c7" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">柱状图通常用于绘制两个分类变量之间的关系。以变量<code class="du mt mu mv ly b">gender</code>和<code class="du mt mu mv ly b">TenYearCHD</code>为例，我们可以清楚地看到，与女性相比，男性患冠心病的风险<em class="ki">略高</em>。</p><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="11e1" class="kx ky hh ly b fi mc md l me mf">plt.figure(figsize=(8,6))<br/>sns.barplot(x=gender_graph["male"], y=gender_graph["TenYearCHD"])<br/>plt.title("Graph showing which gender has more risk of coronary heart disease CHD", size=15)<br/>plt.xlabel("Gender\n0 is female and 1 is male",size=15)<br/>plt.ylabel("TenYearCHD cases", size=15)</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nf"><img src="../Images/5d53a4a5dc668a14240ce5acdc617706.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*161NdqNS4A26xiRd3m9u4g.jpeg"/></div></figure><ul class=""><li id="c0ef" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi">计数图</strong></li></ul><p id="b29c" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">计数图在使用条形显示分类“bin”中数值观察的<em class="ki">计数</em>时非常有效。这些图可以用来显示数字变量和分类变量之间的关系。</p><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="8968" class="kx ky hh ly b fi mc md l me mf">plt.figure(figsize=(30,12), facecolor='w')<br/>sns.countplot(x='TenYearCHD', data=fhs, hue='cigsPerDay')<br/>plt.legend(title = 'cigsPerDay', fontsize='large')<br/>plt.title("Graph showing the relationship between cigsPerDay and risk of developing CHD", size=28)<br/>plt.xlabel("Risk of developing, TenYearCHD", size=25)<br/>plt.ylabel("Count, TenYearCHD", size=25)<br/>plt.show()</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ng"><img src="../Images/71bf0937a58ce00740f826af426dbbbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_EbSPzslCLHBgivZA_TOjQ.jpeg"/></div></div></figure><p id="6a7f" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">从该图中，我们观察到<code class="du mt mu mv ly b">cigsPerDay</code>(数字)与<code class="du mt mu mv ly b">TenYearCHD</code>(分类)正相关，即一个人一天吸烟越多，就越有可能患冠心病。</p><ul class=""><li id="a52e" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi"> Regplots </strong></li></ul><p id="1f7e" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">Regplots用于绘制数据和线性回归模型拟合。这种图包含一个数字变量和一个分类变量，并输出一条趋势线，展示两个变量之间的关系。</p><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="9f30" class="kx ky hh ly b fi mc md l me mf">plt.figure(figsize=(8,8))<br/>sns.regplot(x=sysbp_graph["TenYearCHD"], y=sysbp_graph["sysBP"])<br/>plt.title("Distribution of sysBP in relation to developing CHD", size=15)</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nh"><img src="../Images/2d651498ad881460d5a100473b6ad907.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*Y0y9HRbNkyR8H6MH7C5mCw.jpeg"/></div></figure><p id="7123" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">查看<code class="du mt mu mv ly b">sysBP</code>(数值)和<code class="du mt mu mv ly b">TenYearCHD</code>(分类)，我们可以看到一条线性增长的线，表示两个变量之间的正相关关系。随着<code class="du mt mu mv ly b">sysBP</code>的增加，发展<code class="du mt mu mv ly b">TenYearCHD</code>的风险也增加。</p><h2 id="766d" class="kx ky hh bd kz la lb lc ld le lf lg lh jv li lj lk jz ll lm ln kd lo lp lq lr bi translated"><strong class="ak">第五步:特征选择</strong></h2><p id="ecb0" class="pw-post-body-paragraph jm jn hh jo b jp ls ii jr js lt il ju jv lu jx jy jz lv kb kc kd lw kf kg kh ha bi translated">既然我们已经探索了我们的变量以及这些变量和结果之间的关系，我们就可以为我们的机器学习模型选择功能了。如上图所示，并非每个变量都会直接影响输出，我们希望确保模型中包含的变量会对模型性能产生积极影响。</p><p id="0fb9" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我们可以使用各种特征选择<a class="ae kq" href="https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/" rel="noopener ugc nofollow" target="_blank">技术</a>，但是对于这个数据集，我们将限制为两种方法:</p><ul class=""><li id="813d" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi"> SelectKBest </strong>:计算X和y类标签的每个特征之间的chi统计量，并返回得分最高的前k个特征。</li></ul><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="5890" class="kx ky hh ly b fi mc md l me mf">from sklearn.feature_selection import SelectKBest, chi2</span><span id="a28a" class="kx ky hh ly b fi mg md l me mf">bestfeatures_skb = SelectKBest(score_func=chi2, k=8)</span><span id="1a81" class="kx ky hh ly b fi mg md l me mf">fit_skb = bestfeatures_skb.fit(X,y) </span><span id="3717" class="kx ky hh ly b fi mg md l me mf">dfscores = pd.DataFrame(fit_skb.scores_)<br/>dfcolumns = pd.DataFrame(X.columns)</span><span id="49a0" class="kx ky hh ly b fi mg md l me mf">featureScores = pd.concat([dfcolumns,dfscores],axis=1)<br/>featureScores.columns = ['Specs','Score']  </span><span id="078b" class="kx ky hh ly b fi mg md l me mf">print(featureScores.nlargest(10,'Score'))  </span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ni"><img src="../Images/29634c1d7b7a7757c2e48affd7d5bc5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*DldVI6rXRDQVXK-GVvPJBw.jpeg"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">具有最高选择最佳得分的10个功能</figcaption></figure><ul class=""><li id="cd59" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi">互信息分类</strong>:衡量特征与目标变量的依赖关系，得分越高表明变量越依赖。</li></ul><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="3280" class="kx ky hh ly b fi mc md l me mf">from sklearn.feature_selection import mutual_info_classif</span><span id="384d" class="kx ky hh ly b fi mg md l me mf">threshold = 10  <br/>high_score_features = []</span><span id="685d" class="kx ky hh ly b fi mg md l me mf">feature_scores = mutual_info_classif(X, y, random_state=1)</span><span id="4956" class="kx ky hh ly b fi mg md l me mf">for score, f_name in sorted(zip(feature_scores, X.columns), reverse=True)[:threshold]:<br/>        print(f_name, score)<br/>        high_score_features.append(f_name)</span><span id="aeae" class="kx ky hh ly b fi mg md l me mf">feature_scores_mic = X[high_score_features]</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nj"><img src="../Images/129ab0863ec8b3f895f737a7bf217b87.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*zJAYbzXsSuOZ7VXGcyBXcg.jpeg"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">互信息分类得分最高的10个特征</figcaption></figure><p id="f971" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我们模型中的最终特征将是来自选择最佳和互信息分类结果的顶级特征的组合:<code class="du mt mu mv ly b">sysBP</code>、<code class="du mt mu mv ly b">age</code>、<code class="du mt mu mv ly b">totChol</code>、<code class="du mt mu mv ly b">diaBP</code>、<code class="du mt mu mv ly b">prevalentHyp</code>、<code class="du mt mu mv ly b">diabetes</code>、<code class="du mt mu mv ly b">BPMeds</code>和<code class="du mt mu mv ly b">male</code>。</p><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="f055" class="kx ky hh ly b fi mc md l me mf">fhs = fhs[['sysBP', 'age', 'totChol', 'diaBP', 'prevalentHyp', 'diabetes', 'BPMeds', 'male','TenYearCHD' ]]</span><span id="a51f" class="kx ky hh ly b fi mg md l me mf">fhs.head()</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nk"><img src="../Images/65a63e8942c413c0bb3145a94130b313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*sEwaeWmuSENU1gSyXzRcpA.jpeg"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">最终数据集中包含的列</figcaption></figure><h2 id="f5a6" class="kx ky hh bd kz la lb lc ld le lf lg lh jv li lj lk jz ll lm ln kd lo lp lq lr bi translated"><strong class="ak">第六步:数据预处理</strong></h2><p id="ceae" class="pw-post-body-paragraph jm jn hh jo b jp ls ii jr js lt il ju jv lu jx jy jz lv kb kc kd lw kf kg kh ha bi translated">这一步骤被称为将数据转换为机器学习模型可读形式的过程，包括将数据集分为训练和测试，缩放特征以及平衡不平衡的变量。</p><ul class=""><li id="b695" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi">训练-测试分离</strong>:我们将一个数据集分成两个子集，第一个子集(训练数据集)适合模型，第二个子集(测试数据集)用于评估从训练数据到测试数据的预测。如果我们不拆分数据集，模型将“看到”所有数据，并且无法准确预测新数据的性能。</li></ul><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="1aa8" class="kx ky hh ly b fi mc md l me mf">from sklearn.model_selection import train_test_split</span><span id="1a6b" class="kx ky hh ly b fi mg md l me mf">X = fhs.drop(['TenYearCHD'], axis=1) <br/>y = fhs['TenYearCHD'] <br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=25)</span></pre><ul class=""><li id="3e02" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi">特征缩放</strong>:我们希望特征中的每个数据点具有相同的<em class="ki">权重。</em>特征缩放方法取决于我们数据的分布，在我们的例子中，分布是正态的，所以我们将使用最小最大缩放器。</li></ul><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="cef5" class="kx ky hh ly b fi mc md l me mf">from sklearn.preprocessing import MinMaxScaler</span><span id="d91e" class="kx ky hh ly b fi mg md l me mf">scaler = MinMaxScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_test = scaler.transform(X_test)</span></pre><ul class=""><li id="a83f" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi">重采样不平衡变量</strong>:在重采样之前看一下数据集的形状，我们看到我们的目标变量<code class="du mt mu mv ly b">TenYearCHD</code>高度不平衡。如果我们使用这个变量来预测，我们的模型将偏向多数类而忽略少数类，从而导致模型具有<em class="ki">高准确性但低召回率。</em>有几种方法可以解决这个问题，但我们将使用<a class="ae kq" href="https://towardsdatascience.com/how-to-effortlessly-handle-class-imbalance-with-python-and-smote-9b715ca8e5a7" rel="noopener" target="_blank"> SMOTE </a>方法，该方法通过从现有样本生成新样本来对少数类进行过采样。</li></ul><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="634e" class="kx ky hh ly b fi mc md l me mf">from imblearn.over_sampling import SMOTE</span><span id="8bc0" class="kx ky hh ly b fi mg md l me mf">sm = SMOTE(random_state=1)<br/>X_sm, y_sm = sm.fit_resample(X,y)</span><span id="edaf" class="kx ky hh ly b fi mg md l me mf">print(f'''Shape of X before SMOTE: {X.shape}<br/>Shape of X after SMOTE: {X_sm.shape}''')</span><span id="bbb3" class="kx ky hh ly b fi mg md l me mf">print('\nNumber of positive and negative instances in both classes (%):')<br/>y_sm.value_counts(normalize=True) * 100</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nl"><img src="../Images/0e52f26417e6b5a2f41bc6568970d6b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*bEAJmfmfHRfGs6BFgw9Ahw.jpeg"/></div></figure><p id="892d" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">到目前为止，我们已经探索了数据集，确定并删除了异常值，深入分析了我们的分类和数值变量，选择了我们的特征，将我们的数据适当地划分为测试和训练数据集，调整了我们的特征，并平衡了我们的目标变量。</p><p id="9ba3" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我们现在已经为机器学习算法做好了准备。</p><h2 id="df06" class="kx ky hh bd kz la lb lc ld le lf lg lh jv li lj lk jz ll lm ln kd lo lp lq lr bi translated"><strong class="ak">第七步:预测建模</strong></h2><p id="0549" class="pw-post-body-paragraph jm jn hh jo b jp ls ii jr js lt il ju jv lu jx jy jz lv kb kc kd lw kf kg kh ha bi translated">有几种算法非常适合分类问题。这个项目将实现这些算法中的四个(和它们的超调对应物):逻辑回归、随机森林、K-最近邻和支持向量机。</p><ul class=""><li id="3601" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi"> Logistic回归:</strong>期望输出为二进制(是/否或0/1)时广泛使用的分类算法。当理解一个或多个独立变量对单个结果变量的影响时，该算法被证明是有帮助的。</li></ul><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="abc4" class="kx ky hh ly b fi mc md l me mf">from sklearn.linear_model import Logistic Regression<br/>from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report</span><span id="4181" class="kx ky hh ly b fi mg md l me mf">lr = LogisticRegression(random_state=25, max_iter=100)<br/>lr_model = fhs_lr.fit(X_train, y_train) <br/>lr_predict = fhs_lr.predict(X_test)</span><span id="7f37" class="kx ky hh ly b fi mg md l me mf">lr_accuracy = accuracy_score(y_test, lr_predict) <br/>lr_cm = confusion_matrix(y_test, lr_predict)</span><span id="8a58" class="kx ky hh ly b fi mg md l me mf">print("Accuracy of Logistic Regression:", lr_accuracy)<br/>print("Logistic Regression Confusion Matrix:", lr_cm)<br/>print(classification_report(y_test, lr_predict))</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nm"><img src="../Images/3b700bff841f7804496392c30f7a06fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*dSCx5M06GjJa4cMNjnc1Wg.jpeg"/></div></figure><ul class=""><li id="5052" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi">随机森林:</strong>对分类和回归问题都有效，随机森林是几个决策树放在一起。什么是决策树？类似于流程图，决策树将数据分解成更小的子集，直到算法找到适合数据的最小的树。尽管单个树易于很好地解释和处理数据，但它们容易过度拟合，产生的结果准确度较低。将多个树组合到一个模型中，即随机森林，增强了每个单独的树模型到一个强树模型中的性能。</li></ul><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="630b" class="kx ky hh ly b fi mc md l me mf">from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report</span><span id="0783" class="kx ky hh ly b fi mg md l me mf">rf = RandomForestClassifier(n_estimators=100, random_state=25, max_depth=12)<br/>rf.fit(X_train, y_train)<br/>rf_predict = rf.predict(X_test)</span><span id="c1a5" class="kx ky hh ly b fi mg md l me mf">rf_accuracy = accuracy_score(y_test, rf_predict)<br/>rf_cm = confusion_matrix(y_test, rf_predict)</span><span id="2da9" class="kx ky hh ly b fi mg md l me mf">print("Accuracy of Random Forest:", rf_accuracy)<br/>print("Random Forest Confusion Matrix:", rf_cm)<br/>print(classification_report(y_test, rf_predict))</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nn"><img src="../Images/16f636b74efd06399ca264227279d0ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*MiMJ4TUqwboRVTUQvMh8cw.jpeg"/></div></figure><ul class=""><li id="0bbf" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi">K-最近邻:</strong>该算法在相似数据点彼此靠近的假设下运行。KNN结合了“接近”的概念来计算数据点之间的距离。取K的一个特定值，例如K = 5，我们将考虑离未知数据点最近的5个数据点，并且这些点之间的多数标记将被分配给未知数据点。</li></ul><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="26f7" class="kx ky hh ly b fi mc md l me mf">from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report</span><span id="f3d0" class="kx ky hh ly b fi mg md l me mf">knn = KNeighborsClassifier(n_neighbors=5)<br/>knn_model = knn.fit(X_train, y_train)<br/>knn_predict = knn.predict(X_test)</span><span id="e011" class="kx ky hh ly b fi mg md l me mf">knn_cm = confusion_matrix(y_test, knn_predict)<br/>knn_accuracy = accuracy_score(y_test, knn_predict)</span><span id="759d" class="kx ky hh ly b fi mg md l me mf">print("Accuracy of KNN Classification:", knn_accuracy)<br/>print("KNN Classification Confusion Matrix:", knn_cm)<br/>print(classification_report(y_test, knn_predict))</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es no"><img src="../Images/ed3c04982f1c3919e82852332b6f73e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*vnjZJxFZHthxV3GueMWFHQ.jpeg"/></div></figure><ul class=""><li id="f7b8" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><strong class="jo hi">支持向量机:</strong>该算法创建一个线性分隔符，将一组数据点分成两类(用于分类)，将每个数据点归入两类中的一类。</li></ul><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="a74a" class="kx ky hh ly b fi mc md l me mf">from sklearn.svm import SVC<br/>from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report</span><span id="5223" class="kx ky hh ly b fi mg md l me mf">svm = SVC()<br/>svm_model = svm.fit(X_train, y_train)<br/>svm_predict = svm.predict(X_test)</span><span id="e622" class="kx ky hh ly b fi mg md l me mf">svm_cm = confusion_matrix(y_test, svm_predict)<br/>svm_accuracy = accuracy_score(y_test, svm_predict)</span><span id="b637" class="kx ky hh ly b fi mg md l me mf">print("Accuracy of SVM Classification:", svm_accuracy)<br/>print("SVM Classification Confusion Matrix:", svm_cm)<br/>print(classification_report(y_test, svm_predict))</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es np"><img src="../Images/9bf20c4b8f6af957c75a42be84d1ffbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*VrGDiqifCYTVuY9umZfIcQ.jpeg"/></div></figure><h2 id="51ea" class="kx ky hh bd kz la lb lc ld le lf lg lh jv li lj lk jz ll lm ln kd lo lp lq lr bi translated"><strong class="ak">步骤8:超参数调整</strong></h2><p id="c545" class="pw-post-body-paragraph jm jn hh jo b jp ls ii jr js lt il ju jv lu jx jy jz lv kb kc kd lw kf kg kh ha bi translated">对于大多数模型，我们还可以<a class="ae kq" href="https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74" rel="noopener" target="_blank">调整超参数</a>(把它想象成算法的设置)来优化模型性能。Scikit-learn包括一组适用于所有模型的默认超参数，但这些值不能保证产生最佳结果。GridSearch和RandomizedSearch是用于寻找最优值的常用调优方法；这个项目中的所有超调模型都使用了随机搜索(使用这种方法，我们根据一系列值随机选择超参数组合)</p><p id="4af3" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated"><strong class="jo hi">超调随机森林</strong>:</p><p id="8f45" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">调整后的超参数:</p><ul class=""><li id="35be" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><em class="ki"> n_estimators </em>:森林中的树木数量</li><li id="cc16" class="mk ml hh jo b jp nq js nr jv ns jz nt kd nu kh mp mq mr ms bi translated"><em class="ki"> max_features </em>:每次分割前需要的最大特征数</li><li id="35e1" class="mk ml hh jo b jp nq js nr jv ns jz nt kd nu kh mp mq mr ms bi translated"><em class="ki"> max_depth </em>:每棵树的最大层数</li><li id="dc82" class="mk ml hh jo b jp nq js nr jv ns jz nt kd nu kh mp mq mr ms bi translated"><em class="ki"> min_samples_split </em>:分割一个节点所需的最小样本数</li><li id="dee5" class="mk ml hh jo b jp nq js nr jv ns jz nt kd nu kh mp mq mr ms bi translated"><em class="ki"> min_samples_leaf </em>:每个节点需要的最小样本数</li><li id="9b7c" class="mk ml hh jo b jp nq js nr jv ns jz nt kd nu kh mp mq mr ms bi translated"><em class="ki"> bootstrap </em>:为训练每棵树选择样本的方法</li></ul><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="a1cb" class="kx ky hh ly b fi mc md l me mf">from sklearn.model_selection import RandomizedSearchCV<br/>from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report</span><span id="9660" class="kx ky hh ly b fi mg md l me mf"># Number of trees in forest <br/>n_estimators = [int(x) for x in np.linspace(start= 100, stop = 1000, num = 10)]</span><span id="be8b" class="kx ky hh ly b fi mg md l me mf"># Max number of features needed before each split<br/>max_features = ["auto", "sqrt"]</span><span id="2b81" class="kx ky hh ly b fi mg md l me mf"># Max no. of levels in each tree<br/>max_depth = [int(x) for x in np.linspace(start=10, stop=100, num = 10)] <br/>max_depth.append(None)</span><span id="2c64" class="kx ky hh ly b fi mg md l me mf"># Min no. of samples needed to split a node  <br/>min_samples_split = [2, 5, 10]</span><span id="f2f3" class="kx ky hh ly b fi mg md l me mf"># Min no. of samples needed at each node <br/>min_samples_leaf = [1, 2, 4]</span><span id="22e8" class="kx ky hh ly b fi mg md l me mf"># Method of choosing samples for training each tree<br/>bootstrap = [True, False]</span><span id="d8b2" class="kx ky hh ly b fi mg md l me mf"># Create the random grid to sample from during fitting<br/>rf_grid = {'n_estimators': n_estimators,<br/>               'max_features': max_features,<br/>               'max_depth': max_depth,<br/>               'min_samples_split': min_samples_split,<br/>               'min_samples_leaf': min_samples_leaf,<br/>               'bootstrap': bootstrap}</span><span id="caaa" class="kx ky hh ly b fi mg md l me mf">print(rf_grid)</span></pre><p id="947c" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">一旦我们创建了网格，我们就可以实例化对象，并像其他scikit-learn模型一样拟合。</p><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="82c8" class="kx ky hh ly b fi mc md l me mf">#instantiating object <br/>rf_hyp = RandomForestClassifier()</span><span id="4121" class="kx ky hh ly b fi mg md l me mf">#Random search of parameters through 3-Fold cross-validation <br/>rf_hyp_rs = RandomizedSearchCV(estimator=rf_hyp, param_distributions = rf_grid, n_iter=100, cv=3, verbose=2, random_state=25, n_jobs=-1)</span><span id="dd68" class="kx ky hh ly b fi mg md l me mf">#Fit model<br/>rf_hyp_rs.fit(X_train, y_train)</span><span id="54c7" class="kx ky hh ly b fi mg md l me mf">rf_hyp_model = rf_hyp_rs.best_estimator_<br/>rf_hyp_model.fit(X_train, y_train)<br/>rf_hyp_predict = rf_hyp_model.predict(X_test)<br/>rf_hyp_cm = confusion_matrix(y_test, rf_hyp_predict)<br/>rf_hyp_accuracy = accuracy_score(y_test, rf_hyp_predict)</span><span id="fbc7" class="kx ky hh ly b fi mg md l me mf">print("Accuracy of Hypertuned Random Forest:", rf_hyp_accuracy)<br/>print("Hypertuned Random Forest:", rf_hyp_cm)<br/>print(classification_report(y_test, rf_hyp_predict))</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nv"><img src="../Images/1ca3894b574967d2623adaccede53173.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*D8dedmQRnOy0vyKmoBblEQ.jpeg"/></div></figure><p id="9aa9" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated"><strong class="jo hi">超调K近邻</strong></p><p id="71da" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">调整后的超参数:</p><ul class=""><li id="daa5" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><em class="ki"> leaf_size </em>:影响查询的速度和内存，传递给算法(本例中为balltree)</li><li id="4980" class="mk ml hh jo b jp nq js nr jv ns jz nt kd nu kh mp mq mr ms bi translated"><em class="ki">n _邻居</em>:邻居数量</li><li id="7e42" class="mk ml hh jo b jp nq js nr jv ns jz nt kd nu kh mp mq mr ms bi translated"><em class="ki"> p </em>:闵可夫斯基度规功率参数</li></ul><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="a97e" class="kx ky hh ly b fi mc md l me mf">from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report</span><span id="e73c" class="kx ky hh ly b fi mg md l me mf"># affects speed and memory of query, passed on to algorithm (balltree or kdtree)<br/>leaf_size= list(range(1,50))</span><span id="81cd" class="kx ky hh ly b fi mg md l me mf"># Number of neighbors <br/>n_neighbors= list(range(1,30))</span><span id="7dad" class="kx ky hh ly b fi mg md l me mf"># minkowski metric power parameter <br/>p= [1,2]</span><span id="761a" class="kx ky hh ly b fi mg md l me mf">#creating the dict 'grid'<br/>knn_hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)</span></pre><p id="a2c1" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">一旦我们创建了网格，我们就可以实例化对象，并像其他scikit-learn模型一样拟合。</p><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="d0b4" class="kx ky hh ly b fi mc md l me mf">#Create base model to tune and use grid to find best hyperparameters <br/>knn_hyp_obj = KNeighborsClassifier()</span><span id="5122" class="kx ky hh ly b fi mg md l me mf">#Random search of parameters through 3-Fold cross-validation <br/>knn_hyp_grid = RandomizedSearchCV(knn_hyp_obj, knn_hyperparameters, random_state = 25, cv=3, n_jobs=-1)</span><span id="0876" class="kx ky hh ly b fi mg md l me mf">#Fit random search model<br/>knn_hyp_model = knn_hyp_grid.fit(X_train, y_train)</span><span id="b7f9" class="kx ky hh ly b fi mg md l me mf">knn_hyp = KNeighborsClassifier(n_neighbors=28, leaf_size=20, p=1)<br/>knn_model_hyp = knn_hyp.fit(X_train, y_train)<br/>knn_predict_hyp = knn_hyp.predict(X_test)<br/>knn_cm_hyp = confusion_matrix(y_test, knn_predict_hyp)<br/>knn_hyp_accuracy = accuracy_score(y_test, knn_predict_hyp)</span><span id="b509" class="kx ky hh ly b fi mg md l me mf">print("Accuracy of Hypertuned KNN Classification:", knn_hyp_accuracy)<br/>print("Hypertuned KNN Classification Confusion Matrix:", knn_cm_hyp)<br/>print(classification_report(y_test, knn_predict_hyp))</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nw"><img src="../Images/fcfc8c3531f93acc7da20b6da395d4cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*GZ8QCsM__w8Lply4pDmHCw.jpeg"/></div></figure><p id="180d" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated"><strong class="jo hi">超调支持向量机</strong></p><p id="c4bc" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">调整后的超参数:</p><ul class=""><li id="b25b" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated"><em class="ki"> C </em>:正则化参数</li><li id="b410" class="mk ml hh jo b jp nq js nr jv ns jz nt kd nu kh mp mq mr ms bi translated"><em class="ki">内核</em>:内核类型(可以是线性、多边形、rbf、sigmoid、预计算或可调用)</li><li id="f938" class="mk ml hh jo b jp nq js nr jv ns jz nt kd nu kh mp mq mr ms bi translated"><em class="ki">伽玛</em>:核系数(rbf，poly，sigmoid)</li></ul><pre class="ix iy iz ja fd lx ly lz ma aw mb bi"><span id="469c" class="kx ky hh ly b fi mc md l me mf">from sklearn.svm import SVC<br/>from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report</span><span id="212b" class="kx ky hh ly b fi mg md l me mf">svm_param_grid = {'C': [0.1, 1, 10, 100], <br/>                 'gamma': [1, 0.1, 0.01, 0.001, 0.0001],<br/>                 'kernel': ['rbf']}</span><span id="d583" class="kx ky hh ly b fi mg md l me mf">svm_hyp_rs = RandomizedSearchCV(SVC(), svm_param_grid, refit=True, verbose=1, n_jobs=-1)</span><span id="e623" class="kx ky hh ly b fi mg md l me mf">svm_hyp_rs.fit(X_train, y_train)</span><span id="911c" class="kx ky hh ly b fi mg md l me mf">svm_hyp_model = svm_hyp_rs.best_estimator_<br/>svm_hyp_model.fit(X_train, y_train)<br/>svm_hyp_predict = svm_hyp_model.predict(X_test)<br/>svm_hyp_cm = confusion_matrix(y_test, svm_hyp_predict)<br/>svm_hyp_accuracy = accuracy_score(y_test, svm_hyp_predict)</span><span id="9339" class="kx ky hh ly b fi mg md l me mf">print("Accuracy of Hypertuned Support Vector Machine:", svm_hyp_accuracy)<br/>print("Hypertuned Support Vector Machine:", svm_hyp_cm)<br/>print(classification_report(y_test, svm_hyp_predict))</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nx"><img src="../Images/53c6e06fe6aae129f898fbc021dc09ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*3v8P7SbNGA2SqTux0wlDOw.jpeg"/></div></figure><h2 id="f81e" class="kx ky hh bd kz la lb lc ld le lf lg lh jv li lj lk jz ll lm ln kd lo lp lq lr bi translated"><strong class="ak">第九步:模型评估</strong></h2><p id="d0da" class="pw-post-body-paragraph jm jn hh jo b jp ls ii jr js lt il ju jv lu jx jy jz lv kb kc kd lw kf kg kh ha bi translated">为了评估我们的模型，我们将使用精确度和混淆矩阵。<a class="ae kq" href="https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234" rel="noopener" target="_blank">分类准确度</a>，是正确预测的数量与输入样本总数的比率，当每个类别具有相同数量的样本时效果最佳(我们的两个类别都是如此，因为我们在前面的步骤中重新采样以达到平衡)。最终目标是获得尽可能高的准确率，罕见的100% —我们所有模型的准确率都在83%到84%之间。乍一看，这看起来不错。</p><p id="2103" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">准确性的一个问题是，它不能清楚地表明样本的错误分类，根据项目的类型和目标，这可能会成为一个问题。以我们的数据集为例，我们试图根据几个变量来预测一个人患冠心病的可能性。有两个<a class="ae kq" href="https://www.scribbr.com/statistics/type-i-and-type-ii-errors/" rel="noopener ugc nofollow" target="_blank">错误分类错误</a>可能会发生:</p><ul class=""><li id="8ff6" class="mk ml hh jo b jp jq js jt jv mm jz mn kd mo kh mp mq mr ms bi translated">1型/假阳性:模型表明个体将发展为冠心病，而实际上他们不会。当无效假设实际上是真的时，我们拒绝它。</li><li id="caed" class="mk ml hh jo b jp nq js nr jv ns jz nt kd nu kh mp mq mr ms bi translated">2型/假阴性:模型表明个体不会发展为冠心病，而实际上他们会发展为冠心病。当零假设实际上是错误的时候，我们不能拒绝它。</li></ul><p id="df46" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">回到我们的案例，哪个错误更糟糕？他们都很坏，但可能是2型。我们不想告诉研究参与者他们没有冠心病，只是为了让他们几年后再次患有晚期冠心病。</p><p id="9bf4" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated"><em class="ki">混淆矩阵</em>是对机器学习分类问题的性能测量，它(与准确性不同)考虑了真阳性(TP)、假阳性(FP/1型错误)、假阴性(FN/2型错误)和真阴性(TN)值。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ny"><img src="../Images/53d5cbb0e568c09bc619319e233eea30.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*kpcso9kQEonNGME3PGlnRw.jpeg"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">来源:Sarang Narkhede在《走向数据科学》中的《理解困惑矩阵》</figcaption></figure><p id="34b8" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">通过我们的数据集来看看上面的混淆矩阵，我们希望我们的模型输出大量的真阳性(TP)和真阴性(TN)以及少量的假阴性(FN)和假阳性(FP)。</p><p id="dabf" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">以下是我们对每个模型的准确度和混淆矩阵[[TP，FP] [FN，TN]]的结果。尽管所有模型在总体准确性和假阴性数量方面产生了类似的结果，但我认为<strong class="jo hi"> <em class="ki">超调随机森林</em> </strong>是代表我们数据集和结果最好的模型。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nz"><img src="../Images/2853ffb5d88fb6322ce34212f78f0cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*EGwHsvXzztTFvNd7YOgeDw.jpeg"/></div></figure><h2 id="d9de" class="kx ky hh bd kz la lb lc ld le lf lg lh jv li lj lk jz ll lm ln kd lo lp lq lr bi translated"><strong class="ak"> <em class="oa">最终想法</em> </strong></h2><p id="5b52" class="pw-post-body-paragraph jm jn hh jo b jp ls ii jr js lt il ju jv lu jx jy jz lv kb kc kd lw kf kg kh ha bi translated">将我们的超调随机森林模型应用于该数据集，我们可以84%确定该模型正确预测了结果。相对较低的假阴性增加了我们的信心。像这样的模型当然可以投入生产，并在现实世界中使用，以帮助心脏专家根据输出做出健康决策。</p><p id="dd1e" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">有一些提高模型精度的方法。由于弗雷明汉心脏研究仍在进行中，我们可以在收到数据时向机器学习模型提供更多数据——我们知道模型可以通过增加数据来做出更好的预测。从技术角度来看，我们可以尝试使用更先进的机器学习技术和算法，如集成和深度学习。这就是我最喜欢机器学习的地方，可能性是无穷无尽的:)</p><p id="fbf4" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">感谢您的阅读！</p></div></div>    
</body>
</html>