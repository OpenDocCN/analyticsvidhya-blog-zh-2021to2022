<html>
<head>
<title>Performance Evaluation of Text Generating NLP Models — GPT-Neo, GPT-2 and XLNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本生成自然语言处理模型——GPT-尼奥、GPT-2和XLNet的性能评估</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/performance-evaluation-of-text-generating-nlp-models-gpt-neo-vs-gpt-2-vs-bert-ddb72547956f?source=collection_archive---------1-----------------------#2021-08-31">https://medium.com/analytics-vidhya/performance-evaluation-of-text-generating-nlp-models-gpt-neo-vs-gpt-2-vs-bert-ddb72547956f?source=collection_archive---------1-----------------------#2021-08-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5a3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一种实证方法，用于测试三个巨大的最先进的语言模型在应用于相同的文本生成下游任务时的性能。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/3200dc5389366ffd2aa182ecf6d642ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nOA3skfbXNB1TpTc"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">纳迪·博罗迪纳在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h1 id="b45c" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">简介</strong></h1><p id="34d5" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">随着研究和技术的快速发展，自然语言模型已经进化了很多年。数据科学家将这些模型应用于下游任务和应用，如生成SQL代码、编写法律文档、发送自动邮件回复、聊天机器人和回答数学问题，取得了出色的成果。人们相信，<a class="ae jt" href="https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>(预训练生成式<a class="ae jt" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">变形金刚</a>的最新产品)将通过向非技术人员开放该领域，并大规模创建一个全新的少数镜头学习产品领域，在<a class="ae jt" href="https://www.technologyreview.com/2020/10/15/1010461/artificial-general-intelligence-robots-ai-agi-deepmind-google-openai/" rel="noopener ugc nofollow" target="_blank"> AGI </a>(人工通用智能)采用方面开创一个新的范式。语言建模通常利用统计和概率技术来确定句子中给定单词序列的概率。为了进行单词预测，语言模型结合上下文、语法和词汇来分析之前的文本数据。</p><h1 id="fd9c" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">常识</strong></h1><p id="4648" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">研究人员和开发人员正不知疲倦地致力于建立强大而有效的语言模型，他们认为，用互联网文本语料库对更多参数进行训练的更大的模型会产生更好的结果。毫不奇怪，预训练语言模型的参数大小越来越大，达到了175B ( <a class="ae jt" href="https://lambdalabs.com/blog/demystifying-gpt-3/" rel="noopener ugc nofollow" target="_blank"> GPT-3大型</a>)的数量，其动机是模型越大，它在来自特定领域数据的训练样本越少的情况下表现越好。显然，训练一个大型的集中式多任务模型也比单独为每个任务训练一个新模型更有效。在这篇文章中，我们努力比较三个这样的大规模语言模型来揭穿和揭开这个理论的神秘面纱。</p><blockquote class="kx"><p id="d6ef" class="ky kz hi bd la lb lc ld le lf lg jc dx translated"><em class="lh">在语言模型环境中，越大就意味着越好吗？</em></p></blockquote><h1 id="a49e" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf li kh ki kj lj kl km kn lk kp kq kr bi translated">人工智能社区的趋势是什么？</h1><p id="33b3" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">transformers体系结构和迁移学习的结合引领着自然语言处理行业的黄金标准。有许多预先训练的语言模型具有自我注意变压器块，乍看之下它们是相似的，但是由于它们不同的内在训练算法，在现实世界任务中表现不同。像<a class="ae jt" href="https://huggingface.co/transformers/model_doc/bert.html" rel="noopener ugc nofollow" target="_blank">伯特</a>、<a class="ae jt" href="https://openai.com/blog/gpt-2-1-5b-release/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>、GPT-3、<a class="ae jt" href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" rel="noopener ugc nofollow" target="_blank">图灵NLG </a>(微软)、<a class="ae jt" href="https://github.com/NVIDIA/Megatron-LM" rel="noopener ugc nofollow" target="_blank">威震天</a>(英伟达)和<a class="ae jt" href="https://github.com/google-research/text-to-text-transfer-transformer" rel="noopener ugc nofollow" target="_blank"> T5 </a>(谷歌)这样的模型已经成为头条新闻，因为他们在无人监督的语言应用程序上的最先进的表现。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ll"><img src="../Images/fdf16c72bf18d80f2b38065cab00cf2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*40VA19kG5zUmTj-AOnh47A.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">语言模型的进化。来源:<a class="ae jt" href="https://www.arxiv-vanity.com/papers/2104.04473/" rel="noopener ugc nofollow" target="_blank"> arXiv研究论文</a></figcaption></figure><p id="4894" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们努力选择三个候选模型进行比较评估的过程中，我们考虑了各个方面，例如开源代码的可用性、微调就绪性、适应性、可调参数和硬件计算时间。虽然在人工智能社区中有大量预训练的语言模型及其大小导数可用，但我们决定将自己限制在仅具有不寻常性能的基本语言模型。出于评估目的，我们最终选定了<a class="ae jt" href="https://github.com/EleutherAI/gpt-neo" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"><em class="lm">GPT-尼奥</em> </strong> </a> <strong class="ih hj"> <em class="lm"> (GPT-3的复制品)</em> </strong>、<strong class="ih hj"> <em class="lm"> GPT-2(前身)</em> </strong>和<a class="ae jt" href="https://analyticsindiamag.com/guide-to-xlnet-for-language-understanding/" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"><em class="lm">XLNet</em></strong></a><strong class="ih hj"><em class="lm">(类伯特)</em> </strong>，因为这些模型都具有相当好的通用少量学习能力。</p><h1 id="24b5" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">实验装置和数据集</h1><p id="eae1" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">我们将演示和评估上面选择的三个NLP模型的响应，方法是根据预测给定单词输入序列的下一个单词序列的简单目标来微调它们中的每一个。此外，进行综合比较以评估所有可能的预测。目标被设定为主要通过经验方法和渐进迭代来实现。</p><p id="2c3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该实验旨在与新冠肺炎疫苗推文的kaggle domain <a class="ae jt" href="https://www.kaggle.com/gpreda/all-covid19-vaccines-tweets" rel="noopener ugc nofollow" target="_blank">数据集</a>一起工作，该数据集包含来自疫苗制造商的条目，即辉瑞/BioNTech、国药控股、科兴生物、Moderna、牛津/阿斯利康、Covaxin和Sputnik V</p><p id="10a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用<a class="ae jt" href="https://www.tweepy.org/" rel="noopener ugc nofollow" target="_blank"> tweepy </a> Python包访问Twitter API来收集数据。对于每种疫苗，相关的搜索词(在Twitter中最常用来指代相应的疫苗)被用于合并所有疫苗制造商的所有来源的推文。我们将在这个数据集上微调这些语言模型，通过预测和生成符合上下文的下一个最佳单词序列来完成疫苗相关的提示或输入。</p><p id="940d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">出于迁移学习的目的，我们选择使用<a class="ae jt" href="https://colab.research.google.com/?utm_source=scs-index" rel="noopener ugc nofollow" target="_blank"> Google Collaboratory </a>免费实例和GPU/TPU运行时。此外，由于其共享的在线培训工作区，我们将只使用所有考虑中的语言模型的小型可比版本。对于本文，由于硬件限制，我们将选择<strong class="ih hj">124米</strong>大小的GPT-2、<strong class="ih hj">125米</strong>大小的GPT-尼奥和<strong class="ih hj">110米</strong>大小的XLNet进行微调。</p><h1 id="d8dd" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">候选模型1 : GPT近地天体</h1><p id="e2da" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">GPT代表“生成预训练变压器”。去年7月，Open AI发布了GPT-3，这是一个在公共数据集上训练的自回归语言模型，拥有5000亿个标记和1750亿个参数，至少比以前的非稀疏语言模型大十倍。客观地看，它的前身GPT-2只训练了15亿个参数。不幸的是，GPT-3(截至2021年4月)尚未开源。API访问GPT-3只能通过邮件的私人测试批准。然而，有一个很有前途的替代方案叫做GPT-尼奥，一个只有2.7 B参数的开源变压器模型，在设计和性能方面都类似于GPT-3。在本文中，我们将实现GPT-近地天体取代GPT-3作为替代。</p><p id="770e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Happy Transformer是建立在<a class="ae jt" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">humping Face的Transformer库</a>之上的一个包，它可以轻松地利用最先进的NLP模型进行推理，并在各种任务上训练它们，如文本生成、文本分类、问题回答、单词预测等。GPT-Neo-125M是一个变压器模型，使用伊柳瑟雷复制的GPT-3架构设计。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ln"><img src="../Images/ab85ced180b883ab8ec27d7b8c18f47b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IJUn-S_D08So_I18cqIeqw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">语言模型大小比较。来源:<a class="ae jt" href="https://www.merkleinc.com/in/blog/ai-search-what-openais-gpt-3-means-google-and-seo-0" rel="noopener ugc nofollow" target="_blank">谷歌图片</a></figcaption></figure><h1 id="c40b" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">候选模型2: GPT-2</h1><p id="fbdf" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">它是由<a class="ae jt" href="https://en.wikipedia.org/wiki/OpenAI" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>发布的原始系列语言模型的第二次迭代。GPT目前有3种型号{v1、v2和v3}。其中只有GPT 1号和GPT 2号是完全开源的，因此我们将选择最新的GPT 2号进行实验。在技术方面，GPT-2的体系结构由变压器体系结构的解码器部分组成。该块的编码器-解码器交叉关注部分被移除，因为没有编码器，并且自关注部分被替换为被屏蔽的自关注。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lo"><img src="../Images/08a06cd6374704f4e6afa5492331f84d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-P4_8w9wVhIfgYz32NSoQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">GPT-2的插图。来源:<a class="ae jt" href="https://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank"> GPT文件</a></figcaption></figure><h1 id="a7ea" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">模型候选3: <strong class="ak"> XLNet (BERT) </strong></h1><p id="d485" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">XLNet是另一种类似BERT的模型。但这是一个非常有前途和潜力的机会。XLNet结合了广义自回归(AR)预训练方法。它是一个AR语言模型，使用上下文单词来预测下一个单词。但是这里上下文词被限制在两个方向，向前或向后，而不是同时向前和向后。语言模型由两个阶段组成，训练前阶段和微调阶段。XLNet专注于训练前阶段。在预训练阶段，它提出了一个新的目标称为<strong class="ih hj">置换语言建模使用置换</strong>生成。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lp"><img src="../Images/2fdfb8aa85cd61ed4a97fcbadc6e512d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*hWj62bFOkqzT5nj7OWdjsA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">XLNet的一个图解。来源:<a class="ae jt" href="http://naveedshare-licensetoplay.blogspot.com/2019/08/xlnet-sota-model.html" rel="noopener ugc nofollow" target="_blank">SOTA XLNet</a></figcaption></figure><p id="90f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与AR语言模型不同，BERT被归类为自动编码器(AE)语言模型。AE语言模型旨在从损坏的输入中重建原始数据。损坏的输入表示在预训练阶段用于将原始令牌<strong class="ih hj"> <em class="lm">替换为</em> </strong>的【掩码】。而目标是预测<strong class="ih hj"> <em class="lm">变成</em> </strong>以得到原来的句子。AE语言模型可以等效地看到前后两个方向的上下文。</p><h1 id="1d13" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">结果&amp;观察</strong></h1><p id="3601" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">该实验包括通过迁移学习技术在疫苗数据集上微调预先训练的GPT-尼奥、GPT-2和XLNet的通用语言模型。为了实现这一点，我们分析了疫苗相关短语和短文本的少量提示，作为这些模型的输入前体，以优化最终几个神经网络层的权重和偏差。这使得模型能够通过以可理解的方式预测下一个最有意义的序列来对疫苗特异性文本提示做出响应。</p><p id="710f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了简单和快速比较，我们在评估阶段对所有三个现在已经微调的语言模型解析了相同的文本提示，以记录它们对四次迭代的响应。下面的一组表格图像以综合的方式描述了所有这些响应的比较视图。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lq"><img src="../Images/83fc664b39030f0ea247b6c54ff56dda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*KY831hnzIrZO5mjisWewnQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">第一个文本提示的四次迭代。作者图片</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/b52759cf64c548e88c5531aec7b0a0fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*0JFy5AbH1w4XqaEDw_F-Fw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">第二个文本提示的四次迭代。作者图片</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/eb94bf8c0559b996f91a2a30d9f10f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*cr4etSuQRzcoEtsMxEaNyg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">第三个文本提示的四次迭代。作者图片</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/dcfec958d97095cc416d64e0fdca8d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*ndSjCuk8pNMnxCgWm5hemg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">第四个文本提示的四次迭代。作者图片</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lq"><img src="../Images/90111b0839908c2a72c6c009fec75c9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*2yBtGdSA2hMePLzZwvHtLA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">第五个文本提示的四次迭代。作者图片</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/317c91f87eb7e12bbbadc439db584e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Ii3YT0LPWrqYo-bCOkAJ2g.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">第六个文本提示的四次迭代。作者图片</figcaption></figure><p id="9fdf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总的来说，对于所有六个提示，GPT-近地天体比其他候选模型更好地检测和响应上下文。即使使用最优化的<a class="ae jt" href="https://www.vennify.ai/gpt-neo-made-easy/" rel="noopener ugc nofollow" target="_blank"> GENSettings </a>，它也非常冗长。而GPT-2生成的上下文提示要短得多，在微调阶段需要更多的时间来优化。最后，XL-Net在排名中排名垫底，因为它经常偏离上下文，并且由于其掩蔽的自然训练范式，还挑选不相关的词。</p><p id="13a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还建立了一个相对的<a class="ae jt" href="https://en.wikipedia.org/wiki/BLEU" rel="noopener ugc nofollow" target="_blank"> BLEU </a> ( <em class="lm">双语评估替角分数</em>)分数来衡量每个提示的每次迭代的结果，并将其列表如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/503998d1447da90f3e95b2b41d3448f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*Ho9CFKO0e0wSieuRveTm8g.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">蓝色分数。作者图片</figcaption></figure><p id="52aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BLEU是一个专注于精度的指标，它计算参考文本和生成文本的n元语法重叠。这种n元语法重叠意味着除了n元语法的术语关联之外，评估方案是独立于单词位置的。在BLEU中需要注意的一点是——有一个简短的惩罚，即当生成的文本与目标文本相比太小时应用的惩罚。所有的源代码都可以从这里的<a class="ae jt" href="https://github.com/Shashank545/comparing-text-generating-NLP-models" rel="noopener ugc nofollow" target="_blank">分叉。</a></p><h1 id="42b6" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">结论</strong></h1><p id="671c" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">由此获得的结果是有趣的，因为它有助于解释迁移学习教学法，然后是预先训练的语言模型，以快速适应任何下游任务。此外，我们成功地测试了性能，并通过BLEU分数进行了验证，当然它需要通过跨更多时代的微调来改进。但是要点很清楚，因为这些模型在规模和多租户功能方面随着时间的推移会表现得更好。这些模型即使不是很好，也是很好的，当通过多次试验使用正确的配置进行微调时，可以产生有意义的结果。这使得它们成为完美的基线启动者，如果不是许多NLP应用的最先进的。</p><h1 id="2c67" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="e5e2" class="lw lx hi ih b ii ks im kt iq ly iu lz iy ma jc mb mc md me bi translated">科林·拉弗尔，诺姆·沙泽尔，Adam Roberts，凯瑟琳·李和莎兰·纳朗。用统一的文本到文本转换器探索迁移学习的局限性。2020.<a class="ae jt" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank"> arXiv:1910.10683 </a></li><li id="dd4a" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated">亚历克·拉德福德、杰弗里·吴、雷文·柴尔德、大卫·栾、达里奥·阿莫代伊、伊利亚·苏茨基弗等。语言模型是无人监督的多任务学习者。<em class="lm"> OpenAI博客</em>，1(8):9，2019。</li><li id="8954" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><a class="ae jt" href="https://github.com/EleutherAI/gpt-neo" rel="noopener ugc nofollow" target="_blank">GPT-尼奥</a></li><li id="dece" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><a class="ae jt" href="https://minimaxir.com/2019/09/howto-gpt2/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a></li></ul></div></div>    
</body>
</html>