<html>
<head>
<title>LSTM Model Tensor flow 2.4 with Azure Machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带有Azure机器学习的LSTM模型张量流2.4</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/lstm-model-tensor-flow-2-4-with-azure-machine-learning-33664fc2853f?source=collection_archive---------16-----------------------#2021-03-01">https://medium.com/analytics-vidhya/lstm-model-tensor-flow-2-4-with-azure-machine-learning-33664fc2853f?source=collection_archive---------16-----------------------#2021-03-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/d76cf0590934059f4e4cf96147e0d407.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/0*FWW0qnZdJStJ8hRV"/></div></figure><h1 id="588a" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">用例</h1><ul class=""><li id="bb4a" class="jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">在Azure机器学习中运行Tensorflow LSTM建模</li><li id="d84b" class="jj jk hh jl b jm kb jo kc jq kd js ke ju kf jw jx jy jz ka bi translated">我使用GCP时间序列样本</li><li id="a023" class="jj jk hh jl b jm kb jo kc jq kd js ke ju kf jw jx jy jz ka bi translated">创建时间序列模型</li><li id="e43e" class="jj jk hh jl b jm kb jo kc jq kd js ke ju kf jw jx jy jz ka bi translated">在规则表格数据集中使用LSTM</li><li id="6110" class="jj jk hh jl b jm kb jo kc jq kd js ke ju kf jw jx jy jz ka bi translated">为了展示这个案例，我们可以在Azure机器学习中使用Tensorflow运行LSTM模型</li></ul><h1 id="715e" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">要求</h1><ul class=""><li id="f5c2" class="jj jk hh jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">Azure帐户</li><li id="dc1f" class="jj jk hh jl b jm kb jo kc jq kd js ke ju kf jw jx jy jz ka bi translated">Azure机器学习帐户</li><li id="5f81" class="jj jk hh jl b jm kb jo kc jq kd js ke ju kf jw jx jy jz ka bi translated">创建计算实例—标准_DS14_V2</li><li id="7c10" class="jj jk hh jl b jm kb jo kc jq kd js ke ju kf jw jx jy jz ka bi translated">下载开源数据集</li><li id="872c" class="jj jk hh jl b jm kb jo kc jq kd js ke ju kf jw jx jy jz ka bi translated">使用GCP collab张量流代码</li></ul><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="be94" class="kp im hh kl b fi kq kr l ks kt">Note: the idea here is the to run GCP collab sample and show that it can run fine in Azure Machine learning with out installing anything</span></pre><h1 id="ba5d" class="il im hh bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">密码</h1><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="4857" class="kp im hh kl b fi kq kr l ks kt">import os<br/>import datetime<br/><br/>import IPython<br/>import IPython.display<br/>import matplotlib as mpl<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import tensorflow as tf<br/><br/>mpl.rcParams['figure.figsize'] = (8, 6)<br/>mpl.rcParams['axes.grid'] = False</span><span id="62e1" class="kp im hh kl b fi ku kr l ks kt">zip_path = tf.keras.utils.get_file(<br/>    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',<br/>    fname='jena_climate_2009_2016.csv.zip',<br/>    extract=True)<br/>csv_path, _ = os.path.splitext(zip_path)</span><span id="9c9b" class="kp im hh kl b fi ku kr l ks kt">df = pd.read_csv(csv_path)<br/># slice [start:stop:step], starting from index 5 take every 6th record.<br/>df = df[5::6]<br/><br/>date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')</span><span id="1ffb" class="kp im hh kl b fi ku kr l ks kt">df.head()</span><span id="94f4" class="kp im hh kl b fi ku kr l ks kt">plot_cols = ['T (degC)', 'p (mbar)', 'rho (g/m**3)']<br/>plot_features = df[plot_cols]<br/>plot_features.index = date_time<br/>_ = plot_features.plot(subplots=True)<br/><br/>plot_features = df[plot_cols][:480]<br/>plot_features.index = date_time[:480]<br/>_ = plot_features.plot(subplots=True)</span><span id="6d84" class="kp im hh kl b fi ku kr l ks kt">df.describe().transpose()</span><span id="d332" class="kp im hh kl b fi ku kr l ks kt">wv = df['wv (m/s)']<br/>bad_wv = wv == -9999.0<br/>wv[bad_wv] = 0.0<br/><br/>max_wv = df['max. wv (m/s)']<br/>bad_max_wv = max_wv == -9999.0<br/>max_wv[bad_max_wv] = 0.0<br/><br/># The above inplace edits are reflected in the DataFrame<br/>df['wv (m/s)'].min()</span><span id="ff49" class="kp im hh kl b fi ku kr l ks kt">fft = tf.signal.rfft(df['T (degC)'])<br/>f_per_dataset = np.arange(0, len(fft))<br/><br/>n_samples_h = len(df['T (degC)'])<br/>hours_per_year = 24*365.2524<br/>years_per_dataset = n_samples_h/(hours_per_year)<br/><br/>f_per_year = f_per_dataset/years_per_dataset<br/>plt.step(f_per_year, np.abs(fft))<br/>plt.xscale('log')<br/>plt.ylim(0, 400000)<br/>plt.xlim([0.1, max(plt.xlim())])<br/>plt.xticks([1, 365.2524], labels=['1/Year', '1/day'])<br/>_ = plt.xlabel('Frequency (log scale)')</span><span id="d221" class="kp im hh kl b fi ku kr l ks kt">n = len(df)<br/>train_df = df[0:int(n*0.7)]<br/>val_df = df[int(n*0.7):int(n*0.9)]<br/>test_df = df[int(n*0.9):]<br/><br/>num_features = df.shape[1]</span><span id="3c4c" class="kp im hh kl b fi ku kr l ks kt">train_mean = train_df.mean()<br/>train_std = train_df.std()<br/><br/>train_df = (train_df - train_mean) / train_std<br/>val_df = (val_df - train_mean) / train_std<br/>test_df = (test_df - train_mean) / train_std</span><span id="fbae" class="kp im hh kl b fi ku kr l ks kt">df_std = (df - train_mean) / train_std<br/>df_std = df_std.melt(var_name='Column', value_name='Normalized')<br/>plt.figure(figsize=(12, 6))<br/>ax = sns.violinplot(x='Column', y='Normalized', data=df_std)<br/>_ = ax.set_xticklabels(df.keys(), rotation=90)</span><span id="c4ec" class="kp im hh kl b fi ku kr l ks kt">class WindowGenerator():<br/>  def __init__(self, input_width, label_width, shift,<br/>               train_df=train_df, val_df=val_df, test_df=test_df,<br/>               label_columns=None):<br/>    # Store the raw data.<br/>    self.train_df = train_df<br/>    self.val_df = val_df<br/>    self.test_df = test_df<br/><br/>    # Work out the label column indices.<br/>    self.label_columns = label_columns<br/>    if label_columns is not None:<br/>      self.label_columns_indices = {name: i for i, name in<br/>                                    enumerate(label_columns)}<br/>    self.column_indices = {name: i for i, name in<br/>                           enumerate(train_df.columns)}<br/><br/>    # Work out the window parameters.<br/>    self.input_width = input_width<br/>    self.label_width = label_width<br/>    self.shift = shift<br/><br/>    self.total_window_size = input_width + shift<br/><br/>    self.input_slice = slice(0, input_width)<br/>    self.input_indices = np.arange(self.total_window_size)[self.input_slice]<br/><br/>    self.label_start = self.total_window_size - self.label_width<br/>    self.labels_slice = slice(self.label_start, None)<br/>    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]<br/><br/>  def __repr__(self):<br/>    return '\n'.join([<br/>        f'Total window size: {self.total_window_size}',<br/>        f'Input indices: {self.input_indices}',<br/>        f'Label indices: {self.label_indices}',<br/>        f'Label column name(s): {self.label_columns}'])</span><span id="bb2b" class="kp im hh kl b fi ku kr l ks kt">def plot(self, model=None, plot_col='T (degC)', max_subplots=3):<br/>  inputs, labels = self.example<br/>  plt.figure(figsize=(12, 8))<br/>  plot_col_index = self.column_indices[plot_col]<br/>  max_n = min(max_subplots, len(inputs))<br/>  for n in range(max_n):<br/>    plt.subplot(3, 1, n+1)<br/>    plt.ylabel(f'{plot_col} [normed]')<br/>    plt.plot(self.input_indices, inputs[n, :, plot_col_index],<br/>             label='Inputs', marker='.', zorder=-10)<br/><br/>    if self.label_columns:<br/>      label_col_index = self.label_columns_indices.get(plot_col, None)<br/>    else:<br/>      label_col_index = plot_col_index<br/><br/>    if label_col_index is None:<br/>      continue<br/><br/>    plt.scatter(self.label_indices, labels[n, :, label_col_index],<br/>                edgecolors='k', label='Labels', c='#2ca02c', s=64)<br/>    if model is not None:<br/>      predictions = model(inputs)<br/>      plt.scatter(self.label_indices, predictions[n, :, label_col_index],<br/>                  marker='X', edgecolors='k', label='Predictions',<br/>                  c='#ff7f0e', s=64)<br/><br/>    if n == 0:<br/>      plt.legend()<br/><br/>  plt.xlabel('Time [h]')<br/><br/>WindowGenerator.plot = plot</span><span id="3990" class="kp im hh kl b fi ku kr l ks kt">w1 = WindowGenerator(input_width=24, label_width=1, shift=24,<br/>                     label_columns=['T (degC)'])<br/>w1</span><span id="47fb" class="kp im hh kl b fi ku kr l ks kt">w2 = WindowGenerator(input_width=6, label_width=1, shift=1,<br/>                     label_columns=['T (degC)'])<br/>w2</span><span id="2dd6" class="kp im hh kl b fi ku kr l ks kt">def split_window(self, features):<br/>  inputs = features[:, self.input_slice, :]<br/>  labels = features[:, self.labels_slice, :]<br/>  if self.label_columns is not None:<br/>    labels = tf.stack(<br/>        [labels[:, :, self.column_indices[name]] for name in self.label_columns],<br/>        axis=-1)<br/><br/>  # Slicing doesn't preserve static shape information, so set the shapes<br/>  # manually. This way the `tf.data.Datasets` are easier to inspect.<br/>  inputs.set_shape([None, self.input_width, None])<br/>  labels.set_shape([None, self.label_width, None])<br/><br/>  return inputs, labels<br/><br/>WindowGenerator.split_window = split_window</span><span id="8a76" class="kp im hh kl b fi ku kr l ks kt"># Stack three slices, the length of the total window:<br/>example_window = tf.stack([np.array(train_df[:w2.total_window_size]),<br/>                           np.array(train_df[100:100+w2.total_window_size]),<br/>                           np.array(train_df[200:200+w2.total_window_size])])<br/><br/><br/>example_inputs, example_labels = w2.split_window(example_window)<br/><br/>print('All shapes are: (batch, time, features)')<br/>print(f'Window shape: {example_window.shape}')<br/>print(f'Inputs shape: {example_inputs.shape}')<br/>print(f'labels shape: {example_labels.shape}')</span><span id="7220" class="kp im hh kl b fi ku kr l ks kt">def make_dataset(self, data):<br/>  data = np.array(data, dtype=np.float32)<br/>  ds = tf.keras.preprocessing.timeseries_dataset_from_array(<br/>      data=data,<br/>      targets=None,<br/>      sequence_length=self.total_window_size,<br/>      sequence_stride=1,<br/>      shuffle=True,<br/>      batch_size=32,)<br/><br/>  ds = ds.map(self.split_window)<br/><br/>  return ds<br/><br/>WindowGenerator.make_dataset = make_dataset</span><span id="1aa4" class="kp im hh kl b fi ku kr l ks kt">@property<br/>def train(self):<br/>  return self.make_dataset(self.train_df)<br/><br/>@property<br/>def val(self):<br/>  return self.make_dataset(self.val_df)<br/><br/>@property<br/>def test(self):<br/>  return self.make_dataset(self.test_df)<br/><br/>@property<br/>def example(self):<br/>  """Get and cache an example batch of `inputs, labels` for plotting."""<br/>  result = getattr(self, '_example', None)<br/>  if result is None:<br/>    # No example batch was found, so get one from the `.train` dataset<br/>    result = next(iter(self.train))<br/>    # And cache it for next time<br/>    self._example = result<br/>  return result<br/><br/>WindowGenerator.train = train<br/>WindowGenerator.val = val<br/>WindowGenerator.test = test<br/>WindowGenerator.example = example</span><span id="c525" class="kp im hh kl b fi ku kr l ks kt"># Each element is an (inputs, label) pair<br/>w2.train.element_spec</span><span id="228a" class="kp im hh kl b fi ku kr l ks kt">for example_inputs, example_labels in w2.train.take(1):<br/>  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')<br/>  print(f'Labels shape (batch, time, features): {example_labels.shape}')</span><span id="10d4" class="kp im hh kl b fi ku kr l ks kt">single_step_window = WindowGenerator(<br/>    input_width=1, label_width=1, shift=1,<br/>    label_columns=['T (degC)'])<br/>single_step_window</span><span id="3213" class="kp im hh kl b fi ku kr l ks kt">for example_inputs, example_labels in single_step_window.train.take(1):<br/>  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')<br/>  print(f'Labels shape (batch, time, features): {example_labels.shape}')</span><span id="0421" class="kp im hh kl b fi ku kr l ks kt">wide_window = WindowGenerator(<br/>    input_width=24, label_width=24, shift=1,<br/>    label_columns=['T (degC)'])<br/><br/>wide_window</span><span id="e7b6" class="kp im hh kl b fi ku kr l ks kt">- Model configuration</span><span id="60b9" class="kp im hh kl b fi ku kr l ks kt">CONV_WIDTH = 3<br/>conv_window = WindowGenerator(<br/>    input_width=CONV_WIDTH,<br/>    label_width=1,<br/>    shift=1,<br/>    label_columns=['T (degC)'])<br/><br/>conv_window</span><span id="ad76" class="kp im hh kl b fi ku kr l ks kt">LABEL_WIDTH = 24<br/>INPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)<br/>wide_conv_window = WindowGenerator(<br/>    input_width=INPUT_WIDTH,<br/>    label_width=LABEL_WIDTH,<br/>    shift=1,<br/>    label_columns=['T (degC)'])<br/><br/>wide_conv_window</span><span id="f8db" class="kp im hh kl b fi ku kr l ks kt">class Baseline(tf.keras.Model):<br/>  def __init__(self, label_index=None):<br/>    super().__init__()<br/>    self.label_index = label_index<br/><br/>  def call(self, inputs):<br/>    if self.label_index is None:<br/>      return inputs<br/>    result = inputs[:, :, self.label_index]<br/>    return result[:, :, tf.newaxis]</span><span id="6dce" class="kp im hh kl b fi ku kr l ks kt">baseline = Baseline(label_index=column_indices['T (degC)'])<br/><br/>baseline.compile(loss=tf.losses.MeanSquaredError(),<br/>                 metrics=[tf.metrics.MeanAbsoluteError()])<br/>val_performance = {}<br/>performance = {}<br/>val_performance['Baseline'] = baseline.evaluate(single_step_window.val)<br/>performance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0)</span><span id="c92f" class="kp im hh kl b fi ku kr l ks kt">lstm_model = tf.keras.models.Sequential([<br/>    # Shape [batch, time, features] =&gt; [batch, time, lstm_units]<br/>    tf.keras.layers.LSTM(32, return_sequences=True),<br/>    # Shape =&gt; [batch, time, features]<br/>    tf.keras.layers.Dense(units=1)<br/>])</span><span id="a5d3" class="kp im hh kl b fi ku kr l ks kt">print('Input shape:', wide_window.example[0].shape)<br/>print('Output shape:', lstm_model(wide_window.example[0]).shape)</span><span id="1b51" class="kp im hh kl b fi ku kr l ks kt">MAX_EPOCHS = 20<br/><br/>def compile_and_fit(model, window, patience=2):<br/>  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',<br/>                                                    patience=patience,<br/>                                                    mode='min')<br/><br/>  model.compile(loss=tf.losses.MeanSquaredError(),<br/>                optimizer=tf.optimizers.Adam(),<br/>                metrics=[tf.metrics.MeanAbsoluteError()])<br/><br/>  history = model.fit(window.train, epochs=MAX_EPOCHS,<br/>                      validation_data=window.val,<br/>                      callbacks=[early_stopping])<br/>  return history</span><span id="0805" class="kp im hh kl b fi ku kr l ks kt">history = compile_and_fit(lstm_model, wide_window)<br/><br/>IPython.display.clear_output()<br/>val_performance['LSTM'] = lstm_model.evaluate(wide_window.val)<br/>performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)</span><span id="2212" class="kp im hh kl b fi ku kr l ks kt">x = np.arange(len(performance))<br/>width = 0.3<br/>metric_name = 'mean_absolute_error'<br/>metric_index = lstm_model.metrics_names.index('mean_absolute_error')<br/>val_mae = [v[metric_index] for v in val_performance.values()]<br/>test_mae = [v[metric_index] for v in performance.values()]<br/><br/>plt.ylabel('mean_absolute_error [T (degC), normalized]')<br/>plt.bar(x - 0.17, val_mae, width, label='Validation')<br/>plt.bar(x + 0.17, test_mae, width, label='Test')<br/>plt.xticks(ticks=x, labels=performance.keys(),<br/>           rotation=45)<br/>_ = plt.legend()</span><span id="d274" class="kp im hh kl b fi ku kr l ks kt">for name, value in performance.items():<br/>  print(f'{name:12s}: {value[1]:0.4f}')</span></pre></div><div class="ab cl kv kw go kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ha hb hc hd he"><p id="f41a" class="pw-post-body-paragraph lc ld hh jl b jm le lf lg jo lh li lj jq lk ll lm js ln lo lp ju lq lr ls jw ha bi translated"><em class="lt">最初发表于</em><a class="ae lu" href="https://github.com/balakreshnan/mlops/blob/master/tensorflow/tf2lstm.md" rel="noopener ugc nofollow" target="_blank"><em class="lt">【https://github.com】</em></a><em class="lt">。</em></p></div></div>    
</body>
</html>