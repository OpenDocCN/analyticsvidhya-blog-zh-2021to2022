<html>
<head>
<title>EfficientDet: Scalable and Efficient Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">EfficientDet:可扩展且高效的对象检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/efficientdet-scalable-and-efficient-object-detection-384a5df9011a?source=collection_archive---------3-----------------------#2021-04-14">https://medium.com/analytics-vidhya/efficientdet-scalable-and-efficient-object-detection-384a5df9011a?source=collection_archive---------3-----------------------#2021-04-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="31ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">简介:</strong></p><p id="48c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">高效检测一系列新的目标检测模型。看到一个标题“EfficientDet”后想到的第一个问题，真的高效吗？让我们探索EfficientDet背后的直觉，看看它是否真的有效。</p><p id="05b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">模型效率在计算机视觉中变得越来越重要。近年来，在更精确的物体检测方面已经取得了各种进展。但是随着我们向更精确的目标检测网络发展，网络在参数数量、FLOPS方面将变得更昂贵。例如，最新的基于AmoebaNet的NAS-FPN检测器需要167M个参数和3045B个FLOPS(比RetinaNet多30倍)才能达到最先进的精度。</p><p id="12c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">提高目标检测网络精度的通常方法是通过增加层数或增加通道数或增加模型输入图像分辨率来使网络更深。但是随机增加上述维度中的任何一个都会降低精度增益。例如，ResNet-1000与ResNet-101具有相似的精度，尽管它有更多的层。因此，在effectively det论文中，作者介绍了一种系统的模型缩放方法，并指出仔细平衡网络深度、宽度和分辨率可以获得更好的性能。</p><p id="7e5e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，在针对图像分类的EfficientNet论文中提出了模型缩放的概念(即通过增加网络的深度、宽度和分辨率来获得更好的性能)。但在看到EfficientNet的结果后，他们将这种技术应用于对象检测，并将其称为EfficientDet。在EfficientNet论文中，作者开发了一个基线网络，他们称之为EfficientNet，它是由NAS(神经结构搜索)开发的。通过NAS开发网络背后的直觉是探索网络架构的空间(空间包含Conv、Maxpool、SeparableConv等)。到目前为止，无论我们有什么架构，例如YOLO、固态硬盘等。所有这些架构都是由一些人类专家设计的，但这并不意味着我们完全探索了网络架构的空间。因此，谷歌的团队想出了一个通过NAS开发架构的想法，该架构在引擎盖下使用强化学习，并以某种程度的准确性和失败为优化目标开发了一个EfficientNET(在这篇博客中，我们不讨论NAS如何工作)，然后我们在不同的资源约束下扩大了EfficientNet，以获得efficient net模型家族(B0，B1，B2，B3，B4，B5，B6，B7)。</p><p id="01d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">【EfficientDet与其他模型的比较:</p><p id="9637" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将相同的模型缩放概念应用于EfficientDet，以获得针对不同资源约束的模型系列(D0、D1、D2、D3、D4、D5、D6、D7)。efficientdet-D0在COCO数据集上使用2.5B FLOPS实现了33.8 AP，比YOLOv3在COCO数据集上使用71B FLOPS实现33 AP小28倍。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/f12d2e659e09ff9085fb9621536a9a65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*Gb9AFezEv5WOFGFkdcZbAA.png"/></div></figure><p id="c9b1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">高效架构:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jk"><img src="../Images/d8bed2a7ff1368c89974056c91c0ff95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2PlVN8xlKTSQxKxx_mMqUA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">EfficientDet架构:采用EfficientNet作为主干网络，BiFPN作为特征网络，共享类/箱预测网络。</figcaption></figure><p id="b307" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">EfficientDet架构遵循一级检波器范式。他们采用ImageNet-pre trained efficient nets作为主干网络，并提出了BiFPN作为特征网络，它从主干网络中提取3-7级特征(P3、P4、P5、P6、P7 ),并重复应用自上而下和自下而上的双向特征融合。这些融合的特征被馈送到类和盒网络，以分别产生对象类和包围盒预测。</p><p id="b68d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">主干网络:</strong>使用与B6 efficient net-B0相同的宽度/深度缩放系数，以便可以使用ImageNet预训练的检查点。</p><p id="cc7d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> BiFPN网络:</strong>作者像在EfficientNets中所做的那样指数增长BiFPN宽度(通道数)，但是线性增加深度(层数)，因为深度需要四舍五入到小整数。在网格搜索之后，1.35被检测为宽度的最佳比例因子。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jt"><img src="../Images/74faee5a0d6cc555f355648c827e5041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*mkGm7jfdaSUUmTkptcwdtA.png"/></div></figure><p id="b184" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">盒/类预测网络:</strong>宽度保持与BiFPN相同，但深度(层数)线性增加。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ju"><img src="../Images/8e891509cb9e584e648339c0237a52e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*LM0geQmzpgdrJmiZ6Ac5Jw.png"/></div></figure><p id="87d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">输入图像分辨率:</strong>由于在BiFPN中使用特征级别3–7，输入分辨率必须除以<em class="jv"> 2^7 </em> = 128，因此我们线性增加分辨率。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jw"><img src="../Images/9f2f8b352b124027625edca42e8e4bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*tbyPq3uWEYEMUp5GhffoaA.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jx"><img src="../Images/fd64ca34c0bca22834b93d53d4fd2232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*frg6w7jY3YCAXp3Ct1t6iQ.png"/></div></figure><p id="9ec9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参考文献:</strong></p><div class="jy jz ez fb ka kb"><a href="https://arxiv.org/abs/1911.09070" rel="noopener  ugc nofollow" target="_blank"><div class="kc ab dw"><div class="kd ab ke cl cj kf"><h2 class="bd hi fi z dy kg ea eb kh ed ef hg bi translated">EfficientDet:可扩展且高效的对象检测</h2><div class="ki l"><h3 class="bd b fi z dy kg ea eb kh ed ef dx translated">模型效率在计算机视觉中变得越来越重要。本文系统地研究了神经网络</h3></div><div class="kj l"><p class="bd b fp z dy kg ea eb kh ed ef dx translated">arxiv.org</p></div></div></div></a></div><div class="jy jz ez fb ka kb"><a href="https://arxiv.org/abs/1905.11946" rel="noopener  ugc nofollow" target="_blank"><div class="kc ab dw"><div class="kd ab ke cl cj kf"><h2 class="bd hi fi z dy kg ea eb kh ed ef hg bi translated">EfficientNet:重新思考卷积神经网络的模型缩放</h2><div class="ki l"><h3 class="bd b fi z dy kg ea eb kh ed ef dx translated">卷积神经网络(ConvNets)通常是在固定的资源预算下开发的，然后按比例放大用于…</h3></div><div class="kj l"><p class="bd b fp z dy kg ea eb kh ed ef dx translated">arxiv.org</p></div></div></div></a></div></div></div>    
</body>
</html>