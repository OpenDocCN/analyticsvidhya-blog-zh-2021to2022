<html>
<head>
<title>Gradient descent in python with example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">python中的梯度下降及其示例</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/if-we-have-any-function-and-we-want-find-the-extremum-of-that-function-whether-it-is-minima-or-3dd53d89ea40?source=collection_archive---------1-----------------------#2021-02-10">https://medium.com/analytics-vidhya/if-we-have-any-function-and-we-want-find-the-extremum-of-that-function-whether-it-is-minima-or-3dd53d89ea40?source=collection_archive---------1-----------------------#2021-02-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/6eaa956bbf3b71056e2269bfcf4d8953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j6pKpGFT4A6rtdGoopEqNg.png"/></div></div></figure><div class=""/><p id="195d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们有任何函数，我们想找到这个函数的极值，不管它是最小值还是最大值，我们使用梯度下降优化技术。所以在这篇博客中，我们将看到python中的梯度下降及其变体。这篇博客假设你已经知道一点关于导数和线性回归的知识</p><p id="6fc9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们有函数y=f(x)=x**2+3</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es jo"><img src="../Images/b541343fe56584053c3e771e89ba0666.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*NE9RzgJc5Y_oHFAPmpCaFA.png"/></div></figure><p id="4120" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们可以看到，最小值出现在x=0时。现在，如果我们想找到任何函数的最小值，我们对变量(x)求导，然后使它等于0。</p><p id="f30c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Y= x**2 +3</p><p id="8b0f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">dy/dx=d(x**2+3)/dx</p><p id="0f67" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">dy/dx=2x</p><p id="6cd8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2x=0 #等于零</p><p id="52f2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">X=0</p><p id="6129" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以当x=0时，我们会找到这个函数的最小值。</p><p id="4f0c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Y=0**2+3=3</p><p id="1422" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以3是我们在这里能得到的最小值。</p><p id="ccff" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">函数可以有多个局部最小值或最大值，但它只能有一个全局最小值或最大值。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es jt"><img src="../Images/c169fcc2ae7d6be364fada3485731fb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cUVfBy47TZ4UtSqvpaqd-w.png"/></div></div></figure><p id="104a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但有时函数可能很复杂，所以如果你想找到最小值和最大值，仅仅通过求导等于零(dy/dx=0)并不总是可行的。比如说。F(x)=log (1+ exp(ax))现在解这个方程并不简单，所以梯度下降法可以解决这个问题。</p><h1 id="fc9b" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">梯度下降:</strong></h1><p id="5ef8" class="pw-post-body-paragraph iq ir ht is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">梯度下降是迭代算法。首先我们猜测变量(x)是什么。</p><p id="612a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">F(x)=x**2在这个方程中，唯一的变量是x，这个函数也可以有多个变量。为了便于理解，我仅举一个简单的例子<strong class="is hu">。</strong></p><figure class="jp jq jr js fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kx"><img src="../Images/a7fd76f3b4b4e84398b6c6df024b16ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*yKXH3yqe4sVVThqkaIgytA.png"/></div></div></figure><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es jo"><img src="../Images/00a1e32a4b5382be7b3c56eb322781bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*KZ9pICJTzWzup3DgKhNoNg.png"/></div></figure><p id="73bd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以x*是最佳值，我们可以得到函数的最小值。最初你随机选择x0点。然后我们试图找到比x0更接近x*的x1。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es ky"><img src="../Images/8bfecce2698e0cf89c529ab95157c4e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*wGDzuZFX2XT4xUtCL-Fq7w.png"/></div></figure><p id="4d22" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上述方程称为更新函数。其中，γ(gamma)是步长，而[df/dx]xo是x0处的导数，因此我们得到x1。所以我们继续这样做，直到达到我们的最佳点x*。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es jo"><img src="../Images/350e9fd129d070bfc053ab7f254bdbe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*FSo46XRgdr0s003dwtC6Jg.png"/></div></figure><p id="0c53" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于第一次迭代，它进行大的跳跃，然后跳跃的大小随着迭代而减小。这也取决于学习速度，它会有多大的跳跃。我们必须选择合适的学习速度。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es kz"><img src="../Images/1460f19e743fa24afdd6ef2321f17bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*EdqQ3Em9yD71yx1XWwROlw.png"/></div></figure><p id="5abc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以我们知道直线y= mx+ c的函数，这个函数也是我们用于线性回归的函数。为了便于理解，我们将设计一个函数，为了便于计算，我将忽略偏差项(截距)。</p><p id="0746" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以让y_pred=w * x让这里我们取值w=0.5，实际上我们必须计算w的值，但是这里我们是为了理解的目的而设计实验。</p><p id="d3e0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设给定一个人的身高，我们就得预测体重。我们有10名学生</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es la"><img src="../Images/6839dc36621a2f3196c7d2fd12d9eb81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*wkSsZkQU4LnAiQ6loyhL3Q.png"/></div></figure><p id="6454" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以我们有了身高，如果我们知道w，我们就可以预测体重。所以我们必须计算最佳w*，它可以减少实际值和预测值之间的误差。我们知道线性回归的公式。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es lb"><img src="../Images/6ffaa43bf80462c23fa4e144e7022c38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*Riyfx_cSDRSXJwuPRLCTlQ.png"/></div></figure><p id="3466" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">而我们可以通过梯度下降找到最优w*。</p><h1 id="3162" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">为什么随机梯度下降？</strong></h1><p id="38a6" class="pw-post-body-paragraph iq ir ht is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">在上面的等式中，如果数据集很大，例如我们的数据集大小以百万计，那么对于权重的每次更新，我们必须遍历所有的数据点并计算数百万次导数，所以这在计算上非常昂贵，所以这里随机梯度下降非常有用，这是梯度下降的一种变体。实际上梯度下降有三种变体。</p><p id="85bd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">设n =数据点的总数</p><p id="b3b1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> 1】随机梯度下降</strong>:批量=1</p><p id="ea55" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2] <strong class="is hu">小批量梯度下降</strong>:批量=k(其中1 &lt; k &lt; n)</p><p id="f37a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3] <strong class="is hu">批量梯度下降</strong>:批量= n</p><p id="abc1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> 1】随机梯度下降(SGD): </strong>在SGD中，权重更新发生在处理每个数据点之后，因此反向传播发生在处理每个数据点之后。</p><p id="4909" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，在每次迭代中，我们使用1个点，计算梯度并更新权重。</p><p id="50ee" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让n=1000</p><p id="1129" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以批量=1</p><p id="0428" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">像这样，我们需要多少次迭代来遍历整个数据集？</p><p id="398d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi">1000/1=1000</p><p id="f668" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们执行1000次迭代，称为一个时期。</p><p id="880b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> 2】迷你比赛梯度下降:</strong></p><p id="2133" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">n=1000</p><p id="a7b0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">迷你批量=k=5 ( k应为:1 &lt; k &lt; n )</p><p id="4a25" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">now at each iteration we use 5 data points calculate the average gradient and update the weight .</p><p id="3956" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">like this how many iteration do we need to traverse the whole dataset?</p><p id="d446" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi">1000/5=200</p><p id="2cb3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Now we perform 200 iteration and its called one epoch.</p><p id="75a5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> 3】批量梯度下降:</strong></p><p id="b876" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">N=1000</p><p id="6736" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">批量=1000</p><p id="bade" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，在每次迭代中，我们使用1000个点，计算平均梯度并更新权重。</p><p id="b12b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">像这样，我们需要多少次迭代来遍历整个数据集？</p><p id="410c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi">1000/1000=1</p><p id="af1b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这不过是梯度下降。当批量大小=数据点的数量时，它只是梯度下降。</p><p id="cacd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随机梯度下降比批量梯度下降收敛更快。可能会比较吵，但是收敛比较快。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lc"><img src="../Images/d4e884b0b4c8fc9a29f1e1d28af669a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3L2t1Da4M3ztbB0I1Torhw.png"/></div></div></figure><p id="6615" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们看看python中的代码</p><p id="fdb5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，我们创建了数据集。x =人的身高，y =人的体重</p><figure class="jp jq jr js fd hk"><div class="bz dy l di"><div class="ld le l"/></div></figure><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es lf"><img src="../Images/8aa52ffbd4b0494de30d6b95a53b9381.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*65s3Di55DhLRI_L4UrTlyQ.png"/></div></figure><p id="a288" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后我们有了成本函数。</p><figure class="jp jq jr js fd hk"><div class="bz dy l di"><div class="ld le l"/></div></figure><p id="6e40" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">计算每个点的导数</p><figure class="jp jq jr js fd hk"><div class="bz dy l di"><div class="ld le l"/></div></figure><h1 id="b7c6" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">批量梯度下降或常规梯度下降</strong></h1><p id="c03d" class="pw-post-body-paragraph iq ir ht is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">这里我取学习率非常小γ= 0.000001，因为我的数据集大小非常小n=10。通常我们把学习率定在0.01或0.001或0.1左右。</p><figure class="jp jq jr js fd hk"><div class="bz dy l di"><div class="ld le l"/></div></figure><figure class="jp jq jr js fd hk"><div class="bz dy l di"><div class="ld le l"/></div></figure><pre class="jp jq jr js fd lg lh li lj aw lk bi"><span id="437d" class="ll jv ht lh b fi lm ln l lo lp">optimal value is 0.48597498598079936</span></pre><p id="28ce" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以看到它是如何向最优w收敛的，这里是0.5，我们已经接近它了，这里是0.458585858686</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es lq"><img src="../Images/2b4a58184e32a05ed5285c4ad0884fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*c9xZwqKxyoPgiOJ3mLeAEg.png"/></div></figure><h1 id="4446" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">随机</strong>梯度下降</h1><figure class="jp jq jr js fd hk"><div class="bz dy l di"><div class="ld le l"/></div></figure><figure class="jp jq jr js fd hk"><div class="bz dy l di"><div class="ld le l"/></div></figure><pre class="jp jq jr js fd lg lh li lj aw lk bi"><span id="320e" class="ll jv ht lh b fi lm ln l lo lp">optimal value w is  0.4999999999999996</span></pre><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es lq"><img src="../Images/8d05c03db5f5f3b53227c0177d5b9df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*ZgqcmXW3xvRo7zJHkvZA9w.png"/></div></figure><h1 id="7621" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">小批量梯度下降</h1><figure class="jp jq jr js fd hk"><div class="bz dy l di"><div class="ld le l"/></div></figure><figure class="jp jq jr js fd hk"><div class="bz dy l di"><div class="ld le l"/></div></figure><pre class="jp jq jr js fd lg lh li lj aw lk bi"><span id="46d0" class="ll jv ht lh b fi lm ln l lo lp">optimal value w is  0.4999314426019738</span></pre><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es lq"><img src="../Images/f5bbb6675241ebdfa4ae08e7e8e6103f.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*UAagPiC64B8tXn4TezzmEQ.png"/></div></figure><h1 id="7259" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">比较梯度下降的所有变量</strong></h1><figure class="jp jq jr js fd hk"><div class="bz dy l di"><div class="ld le l"/></div></figure><figure class="jp jq jr js fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lr"><img src="../Images/6770917a523b8b520d42ca9182d7896e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qE23dEofjj079O95u22rLA.png"/></div></div></figure><h1 id="fe58" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">结论:</strong></h1><p id="1586" class="pw-post-body-paragraph iq ir ht is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">我们可以看到，尽管sgd可能有噪声，但它比常规梯度下降收敛得更快。我们可以将小批量梯度下降视为批量梯度下降和随机梯度下降的平均值。</p><h1 id="7793" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">参考:</strong></h1><div class="hh hi ez fb hj ls"><a href="https://www.appliedaicourse.com/" rel="noopener  ugc nofollow" target="_blank"><div class="lt ab dw"><div class="lu ab lv cl cj lw"><h2 class="bd hu fi z dy lx ea eb ly ed ef hs bi translated">应用课程</h2><div class="lz l"><h3 class="bd b fi z dy lx ea eb ly ed ef dx translated">我们知道转行是多么具有挑战性。我们的应用人工智能/机器学习课程被设计为整体学习…</h3></div><div class="ma l"><p class="bd b fp z dy lx ea eb ly ed ef dx translated">www.appliedaicourse.com</p></div></div><div class="mb l"><div class="mc l md me mf mb mg hp ls"/></div></div></a></div></div></div>    
</body>
</html>