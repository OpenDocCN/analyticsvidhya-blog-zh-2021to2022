<html>
<head>
<title>Implementing Logistic Regression with SGD From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始用SGD实现逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementing-logistic-regression-with-sgd-from-scratch-5e46c1c54c35?source=collection_archive---------1-----------------------#2021-02-01">https://medium.com/analytics-vidhya/implementing-logistic-regression-with-sgd-from-scratch-5e46c1c54c35?source=collection_archive---------1-----------------------#2021-02-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="f671" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">python中逻辑回归的自定义实现。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/f2d47a023580fced342bfc40e820f4ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NdoiWVIGX-XySYlZztgwaQ.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><a class="ae js" href="https://dimensionless.in/logistic-regression-concept-application/" rel="noopener ugc nofollow" target="_blank">https://无量纲. in/逻辑回归-概念-应用/ </a></figcaption></figure><p id="fa00" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">大家好，</p><p id="9e51" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于你只是出于好奇，在这里停下来看了一下逻辑回归这个标题，所以我将在整篇文章中满足你的好奇心。读完这篇文章后，你将对逻辑回归充满信心，你将能够自己实现它，当然你也能够向任何人解释它。不浪费你一点时间，我会慢慢满足你的好奇心，继续读下去。</p><p id="b201" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你一定听说过<strong class="ig hi">逻辑回归</strong>，它是最著名的机器学习算法。逻辑回归是一种用于解决分类任务的机器学习算法。是的，即使它的名字包含“回归”术语，它也是一种<strong class="ig hi">分类</strong>算法，而不是<strong class="ig hi">回归</strong>算法，这基本上意味着当我们有任务将图像分类时，我们使用这种算法，例如将图像分为两类，比如它是属于猫还是狗。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jt"><img src="../Images/2ebff1be15a31dfc592fe16909e42ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*biZq-ihFzq1I6Ssjz7UtdA.jpeg"/></div></div></figure><p id="2fb0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好了，我们现在对什么是逻辑回归有了一些了解，LR的另一个流行之处是它主要用于二分类问题，即有两个类别的问题。但是这也可以扩展到多类分类问题。逻辑回归假设我们将用于训练的数据点几乎或完全是线性可分的。请看下图，我们必须找到那条分隔蓝色星星和橙色圆圈的绿线。请记住，它可以是二维空间中的一条线，也可以是三维空间中的一个平面。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ju"><img src="../Images/ec05f76e7139abdf0747731b17438bc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eTR8DjEVI27ICYr01A-MLg.png"/></div></div></figure><p id="f39d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">就像逻辑回归中的线性回归一样，我们试图找到斜率和截距项。因此，这里的平面/直线方程是相似的。</p><p id="036a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y = mx + c</p><p id="12a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是，在逻辑回归的情况下，我们使用该方程的方式是不同的，不是将线拟合到点，而是试图找到正确分隔两种类型的数据点的线/平面。如果我们认为上图中的蓝色星星为1，橙色圆圈为0，我们必须预测数据点属于0或1。所以为了这个目的有一个东西叫做S <strong class="ig hi"> igmoid Function </strong>，这么一个花哨的名字。我们对我们的方程<strong class="ig hi"> " <em class="jv"> y=mx + c </em> " </strong>应用Sigmoid函数，即<strong class="ig hi">Sigmoid(<em class="jv">y = MX+c</em>)</strong>，这就是逻辑回归的核心。但是这个sigmoid函数在里面做什么，让我们看看，</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jw"><img src="../Images/0f0dc060f3f4de1e2384d94f8f014226.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*JAaWYpMl3d7e2ey_4O585A.png"/></div></figure><p id="d965" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，z = mx+c</p><p id="178a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用这个函数来预测值属于类0还是类1。但在此之前我们需要'<strong class="ig hi"> <em class="jv"> m </em> </strong>'和'<strong class="ig hi"> <em class="jv"> c </em> </strong> <em class="jv">'，</em>的广义值来对新的数据点进行预测。为此，我们使用优化算法来找到'<strong class="ig hi"> <em class="jv"> m </em> </strong>'和'<strong class="ig hi"> <em class="jv"> c </em> </strong>'的最佳值。我们将使用随机梯度下降(SGD)算法来执行优化。如果你没有太多接触渐变下降<a class="ae js" rel="noopener" href="/analytics-vidhya/gradient-descent-part-1-the-intuition-a154a6d43c2e"> <strong class="ig hi"> <em class="jv">点击这里</em></strong></a><strong class="ig hi"><em class="jv"/></strong>阅读一下。</p><p id="3c68" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，让我们定义损失函数，</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jx"><img src="../Images/477ba35a3f386aa221dc67ef2f4c99c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TZCFZOjw5cSgcDFwvh63IA.png"/></div></div></figure><p id="f7ca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，'<em class="jv"> Ytrue' </em>为真值，'<em class="jv"> Ypred' </em>为预测值，</p><p id="1c3b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jv"> Ypred </em> =乙状结肠(mx+c)</p><p id="2c1e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们根据我们想要优化的参数来区分这个损失函数。所以这里我们要区分w . r . t .<em class="jv">‘m’</em>和<em class="jv">‘c’</em>。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jy"><img src="../Images/730ebd68ea1d63a62584176e3ac0b78e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*_iDMroy0c5AZIjJNYdBV4g.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jz"><img src="../Images/e18429fad0d2eaaee10a8f26333d2d21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4yrZJLfY76ivIwPwrZ8yOw.png"/></div></div></figure><p id="1717" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">完成后，最重要的需求现在都满足了。让我们开始用python代码编写实现上述等式的代码，</p><pre class="jd je jf jg fd ka kb kc kd aw ke bi"><span id="1b4c" class="kf kg hh kb b fi kh ki l kj kk"># first let's import required libraries<br/><strong class="kb hi">import numpy as np</strong> # for mathematical operations</span><span id="ef88" class="kf kg hh kb b fi kl ki l kj kk"><strong class="kb hi">X =</strong> # data points with some features which we want to train<br/><strong class="kb hi">y =</strong> # labels of all datapoints<br/># Initialize the weights and bias i.e. 'm' and 'c'<br/><strong class="kb hi">m = np.zeros_like(X[0])</strong> # array with shape equal to no. of features<br/><strong class="kb hi">c = 0</strong><br/><strong class="kb hi">LR = 0.0001</strong>  # The learning Rate<br/><strong class="kb hi">epochs = 50</strong> # no. of iterations for optimization</span><span id="ade3" class="kf kg hh kb b fi kl ki l kj kk"># Define sigmoid function<br/><strong class="kb hi">def sigmoid(z):<br/> sig = 1/(1+np.exp(-z))<br/> return sig</strong></span><span id="d28b" class="kf kg hh kb b fi kl ki l kj kk"># Performing Gradient Descent Optimization<br/># for every epoch<br/><strong class="kb hi">for epoch in range(1,epochs+1):</strong><br/>    # for every data point(X_train,y_train)<br/>    <strong class="kb hi">for i in range(len(X)):</strong><br/>        #compute gradient w.r.t 'm' <br/>        <strong class="kb hi">gr_wrt_m = X[i] * (y[i] - sigmoid(np.dot(m.T, X[i]) + c))</strong><br/>        <br/>        #compute gradient w.r.t 'c'<br/>        <strong class="kb hi">gr_wrt_c = y[i] - sigmoid(np.dot(m.T, X[i]) + c)</strong></span><span id="d932" class="kf kg hh kb b fi kl ki l kj kk">        #update m, c<br/>        <strong class="kb hi">m = m - LR * gr_wrt_m</strong><br/>        <strong class="kb hi">c = c - LR * gr_wrt_c</strong></span><span id="287c" class="kf kg hh kb b fi kl ki l kj kk"># At the end of all epochs we will be having optimum values of '<strong class="kb hi">m</strong>' and '<strong class="kb hi">c</strong>'<br/># So by using those optimum values of 'm' and 'c' we can perform predictions</span><span id="8f46" class="kf kg hh kb b fi kl ki l kj kk"><strong class="kb hi">predictions = []</strong></span><span id="7289" class="kf kg hh kb b fi kl ki l kj kk"><strong class="kb hi">for i in range(len(X)):<br/> z = np.dot(m, X[i]) + c<br/> y_pred = sigmoid(z)<br/> if y_pred&gt;=0.5:<br/>  predictions.append(1)<br/> else:<br/>  predictions.append(0)</strong></span><span id="e954" class="kf kg hh kb b fi kl ki l kj kk"># 'predictions' list will contain all the predicted class labels using optimum 'm' and 'c'</span></pre><p id="d421" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是我们如何使用python从头开始实现逻辑回归。我希望您一定喜欢这篇文章，并且一定很高兴自己尝试一下，所以不要浪费时间，去实现您自己的逻辑回归分类器，并使用它来执行预测任务。</p><p id="2a6d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">非常感谢你一直坚持到最后，下一篇文章再见，到那时你会过得很愉快，继续学习。</p></div></div>    
</body>
</html>