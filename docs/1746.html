<html>
<head>
<title>Deep Learning Specialization Course</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习专业化课程</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-learning-specialization-course-20c115b77d0e?source=collection_archive---------21-----------------------#2021-03-15">https://medium.com/analytics-vidhya/deep-learning-specialization-course-20c115b77d0e?source=collection_archive---------21-----------------------#2021-03-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="1dd7" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">课程2:改善神经网络:超参数调整、正规化与最佳化(第二周笔记)</h2></div><p id="346c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">本周我们将学习优化算法，这将使我们更快地训练我们的神经网络。深度学习可以有效地处理海量数据，但在这些数据集上训练算法真的很慢。因此，让我们深入学习优化算法。</p><h2 id="590c" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">小批量梯度下降</h2><p id="a5cf" class="pw-post-body-paragraph iw ix hh iy b iz kn ii jb jc ko il je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">过去，我们已经了解到矢量化有助于处理M个训练示例，而无需显式for循环。但是，当示例的数量增加到一个非常高的数量时，训练过程就会变得很慢。随着梯度下降的实现，我们将不得不首先处理整个训练集，然后在梯度下降中向前移动一步。</p><p id="616a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">事实证明，如果我们让梯度下降开始取得一些进展，甚至在我们处理完整个训练集之前，我们就可以获得更快的算法。为此，我们可以将训练集分成称为<strong class="iy hi">小批量的小训练集。</strong>它们可以用花括号表示。</p><p id="da33" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">小批量t : X^{t}，Y^{t} <br/> <strong class="iy hi">其中，</strong>x的维数为(nX，小批量)<br/>y的维数为(1，小批量)</p><p id="d446" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">实现</strong></p><p id="89b4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如，小批量的大小是1000，总训练样本是五百万。将有5000个小批量。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ks"><img src="../Images/aba1e3a715f5a9a0d88d9a7d47717cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yXJ14zk28f5VmOJdhUdWkw.png"/></div></div></figure><p id="8a01" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于小批量梯度下降，单次通过训练集被称为1个时期。</p><p id="9d7a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当我们一次处理整个训练集时，我们应该期望成本在每次迭代后降低，但是当处理小批量时，我们不能期望成本在每次历元迭代后降低，如下图所示。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es le"><img src="../Images/7b90d7b67fa114042a5012f6342f2894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*OsBFZKbLMZjV9FbGKL4VEg.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">图片来源:吴恩达</figcaption></figure><p id="fa17" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果小批量的大小等于M，则它变成批量梯度下降。如果我们试图在等高线图上绘制梯度下降，可能需要相对较大的步骤才能达到最小值。</p><p id="6dd0" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果小批量的大小等于1，那么它变成随机梯度下降。如果我们试图在等高线图上绘制随机梯度下降，它可能需要非常小和嘈杂的步骤。训练集的单个例子也可能误导算法走向错误的方向。它永远不会收敛，继续振荡。在实践中，我们应该使用介于1和m之间的小批量。</p><p id="96ca" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">选择小批量规模的准则</strong></p><ol class=""><li id="b320" class="lj lk hh iy b iz ja jc jd jf ll jj lm jn ln jr lo lp lq lr bi translated">如果训练集大小小于2000，则使用梯度下降。</li><li id="45d4" class="lj lk hh iy b iz ls jc lt jf lu jj lv jn lw jr lo lp lq lr bi translated">如果训练集大小较大，则使用小批量大小，如64，128，256(2的幂)。</li></ol><h2 id="f20b" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated"><a class="ae lx" href="https://madhurijain27.medium.com/understanding-exponentially-weighted-average-5b6f5099eb01" rel="noopener">指数加权平均</a> (EWA)</h2><p id="0050" class="pw-post-body-paragraph iw ix hh iy b iz kn ii jb jc ko il je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">有比梯度下降更好的算法。为了理解它们，我们必须首先理解指数加权平均线或移动平均线。</p><p id="dff0" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了计算指数加权平均值，我们可以利用以下公式:</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es ly"><img src="../Images/5ebc95ec26fdd6ea91f5a62df3e6f15c.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*q6T1c5SxeTY_iFjhjjzRyA.png"/></div></figure><p id="d66b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">指数加权平均中的偏差校正</strong></p><p id="42b8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">偏差修正是可以更精确地增强EWA计算的技术细节。当β的值在0.5到1.0之间选择时。值V不代表初始水平的θ的正确值，因此引入了偏差。这可以通过将Vt的值除以1-β来纠正。新方程如下。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es lz"><img src="../Images/245dfed1453b4dace462c818112e53ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*2ascgF2UkA-RxBIDEmoyUA.png"/></div></figure><h2 id="0570" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">动量梯度下降</h2><p id="8c5d" class="pw-post-body-paragraph iw ix hh iy b iz kn ii jb jc ko il je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">该算法比标准的梯度下降算法工作得更快。在该算法中，我们首先对权重应用指数加权平均，然后更新权重来计算代价函数。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es ma"><img src="../Images/caa745fd032eeccdbc4741c6fd787a99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*vlLd3yocJxoxgwT1rQbFwA.png"/></div></figure><h2 id="d030" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">RMS Prop</h2><p id="80bf" class="pw-post-body-paragraph iw ix hh iy b iz kn ii jb jc ko il je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">该算法也被称为均方根prop，它也可以加速梯度下降。它的实现可以如下进行。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es mb"><img src="../Images/85ca2b334c6a6a14b2239c9926711998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*8S5aXZcmu2-kFOk3uBOIpg.png"/></div></figure><h2 id="cb4f" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">亚当优化算法</h2><p id="0d33" class="pw-post-body-paragraph iw ix hh iy b iz kn ii jb jc ko il je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">Adam一词来自自适应矩估计。该算法采用RMS Prop和带动量的梯度下降，并将它们放在一起，如以下实现所示。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es mc"><img src="../Images/9cbec5b1c455b5919920ef035e933017.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/1*vS5rS6cksr1hvGeyq01NKg.gif"/></div></figure><p id="f8fa" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">Adam优化算法中的超参数</strong></p><ol class=""><li id="2f12" class="lj lk hh iy b iz ja jc jd jf ll jj lm jn ln jr lo lp lq lr bi translated">阿尔法:学习率，它的值需要调整。</li><li id="4733" class="lj lk hh iy b iz ls jc lt jf lu jj lv jn lw jr lo lp lq lr bi translated">Beta 1:默认情况下，它的值应该是0.9。这是作者推荐的</li><li id="3ffa" class="lj lk hh iy b iz ls jc lt jf lu jj lv jn lw jr lo lp lq lr bi translated">Beta 2: 0.999，推荐值</li><li id="a589" class="lj lk hh iy b iz ls jc lt jf lu jj lv jn lw jr lo lp lq lr bi translated">ε:10^-8</li></ol><h2 id="747f" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">学习率衰减</h2><p id="c7e3" class="pw-post-body-paragraph iw ix hh iy b iz kn ii jb jc ko il je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">可能有助于加速算法的一件事情是随着时间的推移降低学习率。如果我们保持一个学习率的固定值，那么它会四处游荡，但永远不会真正收敛，如下图所示。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es md"><img src="../Images/c0972633f188a52041de4ba71f75aeff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*VJ9Yvk0sAbkRGDZkjGMNYQ.png"/></div></figure><p id="3d19" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果我们慢慢降低学习速率，我们可以在初始阶段快速学习，当学习接近收敛时，它可以采取较小的步骤来达到最小值。</p><p id="d39f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">学习率的值可以使用以下公式计算:</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es me"><img src="../Images/f8c8343f70f28a54b44927e79394734b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*bv10sJsMdgMFRtiYY8qKKA.png"/></div></figure><p id="5f27" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，如果我们希望使用学习率衰减，我们可以尝试多个值的α-零和衰减率，看看什么效果最好。</p><h2 id="6017" class="js jt hh bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated"><strong class="ak">想到了那天</strong></h2><p id="e9ac" class="pw-post-body-paragraph iw ix hh iy b iz kn ii jb jc ko il je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">阅读帮助你建立自己对事物的观点。</p><p id="b9d1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">继续读！！快乐学习:)</p></div></div>    
</body>
</html>