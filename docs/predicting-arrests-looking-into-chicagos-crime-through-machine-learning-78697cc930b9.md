# 预测逮捕:通过机器学习调查芝加哥的犯罪

> 原文：<https://medium.com/analytics-vidhya/predicting-arrests-looking-into-chicagos-crime-through-machine-learning-78697cc930b9?source=collection_archive---------2----------------------->

## 使用决策树、逻辑模型和随机森林分类器分析 2012-2017 年芝加哥的犯罪。

*作者:卡莉·威廉姆斯*

![](img/c95fece3895c24ab35e4c7c8ffd4d20d.png)

芝加哥地平线

对于这个项目，我使用了 Kaggle 的数据集:2012 年至 2017 年芝加哥犯罪。在应用任何模型之前，我首先需要清理和探索我的数据。

# 步骤 1:探索数据

为了开始探索过程，我遵循了我在 3 月 7 日发表的上一篇文章“分析芝加哥的犯罪率”中所采取的许多相同步骤。

这个数据集有 **1456714 行和 23 列**。这些列描述了由行指定的每个独特犯罪的一切。从经纬度到 FBI 代码再到逮捕，23 列中的每一列都为犯罪提供了更多的描述。

为了使用我的项目，我将事情缩小到 16 列:

> **日期:**案发日期
> 
> **街区:**发生犯罪的街区
> 
> **IUCR:** 四位数伊利诺伊州统一犯罪报告(IUCR)代码
> 
> **描述:**犯罪类型的简短描述
> 
> **地点描述:**描述犯罪发生的地点
> 
> **逮捕:**是否逮捕的布尔值(T/F)
> 
> **国内:**犯罪是否为国内的布尔值(T/V)
> 
> **社区区域:**数值表示犯罪发生的社区区域
> 
> 联邦调查局代码:表示联邦调查局犯罪分类的数字代码
> 
> **X & Y 坐标:**犯罪发生的确切地点
> 
> **年:**年犯罪发生
> 
> **纬度&经度:**犯罪的经纬度信息

# 步骤 2:清理数据

> 第 1 部分:检查空值

为了开始清理过程以准备我的数据进行分析，我想首先检查我是否有空值。为此，我首先检查哪些列包含空值。我发现有几个是这样的:位置描述、地区、社区区域、X & Y 坐标以及经度和纬度。为了确定处理这些空值的最佳方式，我想看看每一列中分别有多少百分比是空值。

![](img/61d703b98ab7af29108956bb912c0a28.png)

显示 7 列中存在空值

为了找出每一列中 null 值的比例，我打印了每一列中 null 值的百分比。我发现几乎每一列都不到 1%，但是 X & Y 坐标和经纬度各有近 20%。因此，我决定完全删除这些列，只删除其他列中的行。检查后，我没有更多的空值，并准备继续分析。

![](img/53479000bf70ecf9cd291b35879e1998.png)

在删除和清理数据之后，我不再有空值，可以继续进行分析。

> 第 2 部分:检查分类数据

接下来，我需要更改一些列数据类型，以便能够正确地分析它们。为此，我查看了哪些列是分类的，而不仅仅是数字的。通过打印列的每个唯一值，我可以看到 IUCR、社区区域、联邦调查局代码和主要类型是明确的。

![](img/9b962eb5efd33321095cfd6a01fbe6f6.png)

这显示了每列的唯一值。通过向右滚动，我可以分辨出哪些有有限数量的分类值，哪些没有。

我将这些列的数据类型转换为类别，检查它们是否被转换，然后能够继续我的分析。

> 第 3 部分:检查重复行

接下来，我想看看是否有重复的数据行，并删除它们。我找到了 3238 行。然后，我通过保留第一个实例来删除它们。

![](img/798dc32de124b13e764debc0b5305fb7.png)

这显示了找到的重复行数:3，238

> 第 4 部分:将日期列设置为 DateTimeIndex

为了能够按日期进行分析，我想将日期列更改为 DateTimeIndex。我改变了它，并检查了它的完整性。这将允许我使用 python 依赖于日期的特性。

![](img/8b3f384aa6c992e44a9ab7ba2b5db7b7.png)

这显示了数据类型和日期从字符串到对象的变化

# 步骤 3:开始分析

> 第 1 部分:创建更简洁的数据框架

在研究了我的数据之后，我决定要看看不同变量和逮捕率之间的关系。我知道这是一个布尔值，我认为这可能是有趣的事情，看看像主要类型，家庭状况，社区面积，年份等。，影响逮捕率。

首先，我将我的数据框架缩小到我感兴趣的几列。

**选择列:**逮捕、主要类型、国内、社区地区、年份

> 第 2 部分:创建虚拟变量

接下来，我为非数字数据创建了虚拟变量，以便能够对其进行分析。我为主要类型，逮捕和家庭创造了虚拟变量。

> 第 3 部分:设置目标和特性变量

因为我想探究不同变量对逮捕率的关系，所以我将目标和特征列设置为如下:

**目标列:** ['Arrest_True']

**特色栏目:** ['社区 _ 区域'，'年份'，'家庭 _ 真实'，'家庭 _ 虚假'，'初级 _ 类型 _ 盗窃'，'初级 _ 类型 _ 殴打'，'初级 _ 类型 _ 毒品'，'初级 _ 类型 _ 袭击'，'初级 _ 类型 _ 欺骗行为'，'初级 _ 类型 _ 其他犯罪'，'初级 _ 类型 _ 入室盗窃'，'初级 _ 类型 _ 机动车盗窃'，'初级 _ 类型 _ 抢劫']

这些目标和特征列将允许我预测和分析模型的能力，以预测犯罪是否会被逮捕。

> 第 4 部分:将 X 和 y 分解成训练和测试数据

最后，我可以将我的数据分成训练和测试数据。我决定在这个分析中使用 0 的随机状态和 0 . 25 的测试规模。

# 第四步:探索关系

在应用任何模型之前，我想看看变量之间的相关性。我首先查看了所有变量之间的相关性，但数据太多，无法清楚地看到任何关系。因此，我缩小了范围，看看真正的逮捕和我的数据框架中概述的其他变量之间的相关性。

![](img/f840bc289a8a4777f3ad802ab2a4e425.png)

这显示了真实逮捕和其他变量之间的相关性。我可以看到最强的相关性是毒品，到目前为止。其他密切相关的是刑事损害、欺骗行为、非法侵入、盗窃武器和卖淫。

为了形象化这一点，我创建了一个热图。然而，有太多的变量使得分析变得困难。

![](img/1aaf08fec79c951d226945b53e25bdda.png)

正如你所看到的，有这么多的变量，很难看出哪里有更强的相关性。

为了能够简化这一点，我想把主要类型缩小到最常见的犯罪。为了做到这一点，我绘制了一个最常见犯罪的 seaborn 图。

![](img/55230bdd48d9d15013ca2e5f43083855.png)

在这里你可以看到最常见的犯罪。10 种最常见的是:盗窃、殴打、毒品、刑事损害、攻击、其他犯罪、盗窃、欺骗行为、机动车盗窃和抢劫。

现在我知道了最常见的犯罪，我可以缩小我的热图。我做了一个新的数据框架，包括逮捕、社区区域、年份、家庭状况和四种最常见的犯罪:盗窃、殴打、毒品和刑事损害。现在，我的热图更容易分析和理解。看起来逮捕和其他变量与盗窃、刑事损害和毒品之间存在很强的相关性，但我想打印一些数字来复查。

![](img/d1ff776aa8483e8cd06ad47d31f2b334.png)

这是一个热图，显示了四种最常见的犯罪之间的相互关系，逮捕，家庭暴力和年度犯罪。

为此，我创建了一个新的相关性变量，并打印出这些变量和 Arrest True 之间的结果。这显示了盗窃、刑事破坏和毒品之间最强的相关性，证实了我对热图的分析。

![](img/e94e09eaf5b564defe39016cf8dd9afd.png)

这显示了逮捕真实和四个最常见的犯罪，年，和家庭之间的相关性。

# 步骤 5:应用虚拟分类器模型

我选择查看的第一个模型是一个虚拟分类器模型，为我自己提供一个基准来判断未来的模型。

> 第 1 部分:导入和拟合模型

首先，我使用策略“最频繁”导入并拟合虚拟分类器模型。这将是“不逮捕”。

> 第 2 部分:评估虚拟分类器

在拟合我的模型之后，我计算出大约 74.11%的准确率。这给了我一个基线精度，作为我的其他模型的基础，看看我是否可以改进。

![](img/d491eb138b76065b7fafef35cb959251.png)

这显示了虚拟分类器模型的基线精度

> 第 3 部分:创建可视化

为了创建虚拟树模型的可视化，我制作了一个混淆矩阵来显示模型做出的预测。

![](img/00a83d70c9045632367ffaabd2f38631.png)

这显示了虚拟分类器模型的混淆矩阵，显示了对无停滞的完全预测，因为这是最频繁的

![](img/6f82a9daceb2bd5461c358256199d86a.png)

这些是上面虚拟分类器模型的混淆矩阵后面的数字

# 步骤 6:应用逻辑模型

> 第 1 部分:导入和拟合模型

由于我的预测变量是分类变量，而我的输出变量(停止与非停止)本质上是分类变量，而不是连续数值，因此我认为对于我想要进行的分析类型，逻辑回归比线性回归更好。因此，在我的下一个模型中，我选择了逻辑回归。

首先，我首先导入 LogisticRegression 模型，然后使它适合我的数据。

![](img/92b051334acaaa5b51a3baac00ea8ff1.png)

这显示了将我的数据拟合到逻辑模型的结果

> 第二部分:评价逻辑斯蒂模型

为了评估这个模型，我创建了一个预测逮捕的准确度分数。我发现准确率 85.66%！这是从我的虚拟分类器收集的基线 74.11%的改进。

![](img/edf7e3c0c847eeda54b7059155f0bf17.png)

这表明了逻辑模型在预测逮捕方面的准确性

> 第 3 部分:创建可视化

为了更深入地研究这个模型，我创建了一个混淆矩阵来显示模型对逮捕率的预测。你可以看到最大值是一个真正的负值:一个真正的无逮捕率预测(260522)。第二大值是真阳性:真的停搏预测(51086)。有大量的假阴性(43094)。我认为，在这个商业案例中，假阳性比假阴性更糟糕:如果犯罪公司或芝加哥警察局预测有人会被逮捕，但他们没有，这可能会说服他们逮捕可能不应该被逮捕的人。在这个模型中，所有没有被逮捕的案例有 3.36%的假阳性率。

![](img/1ced56ecbe79a2fd5baff62a82a3d77e.png)

这是混淆矩阵，显示预测的停滞和真实的停滞，以显示逻辑模型的准确性

![](img/458f5adccd1b36454132441396121596.png)

这些是上述混淆矩阵背后的价值

除了我的模型的准确性之外，我想看一些更多的关于不同变量的逮捕率如何变化的可视化。为了做到这一点，我为与逮捕率最相关的变量创建了列联表和镶嵌图:主要类型盗窃、主要类型毒品和主要类型刑事损害。

> 权变表 1:盗窃犯罪与逮捕

![](img/0daadb6c863d5d42d0ed08aae4c5c348.png)

这显示了盗窃逮捕率的列联表。与逮捕相比，你可以看到大量的不逮捕。

> 应急表 2:毒品犯罪与逮捕

![](img/11f75f9c4b4305e8a2c40df089ef1c8d.png)

这是逮捕毒品犯罪的应急表。这表明，对于大多数毒品犯罪，有一个逮捕。

> 权变表 3:刑事损害罪与逮捕

![](img/5a759eb5e0c6b7236f0c6ebd1b159e18.png)

这是刑事损害犯罪的列联表。这表明，对于大多数刑事损害犯罪，没有逮捕。

# 步骤 7:应用决策树模型

> 第 1 部分:导入和拟合模型

我想研究的下一个模型是决策树。首先，我从 sklearn.tree 导入 DecisionTreeClassifier，然后将标准设置为 Gini，并拟合我的数据。

> 第 2 部分:评估决策树模型

为了评估决策树模型，我找到了它预测逮捕的准确度分数。我发现准确率(85.763%)略高于我的逻辑模型(85.66%)。这也是从 74.11%的基线虚拟分类器准确度的改进。

![](img/2fc25ec97ec861c440b38500b535482b.png)

这显示了决策树模型的准确性，略高于逻辑

> 第 3 部分:创建可视化

为了创建决策树模型的可视化，我从混淆矩阵开始。我看到填充最多的空间是真阴性(261334)和真阳性(50633)，然而，有大量的假阴性(43547)。这类似于逻辑模型。关于假阳性，与逻辑模型相比，假阳性率略低:该模型在所有未逮捕中的假阳性率为 3.06%，而逻辑模型为 3.36%

![](img/807676907ae40629aacec0ff7d41f2c1.png)

这显示了决策树模型的混淆矩阵，显示了最多的 TP 和 TN，其次是 FN。

![](img/4709ce9e6985c49da37ef6e22389d8ad.png)

这显示了上述混淆矩阵背后的数字。

我最初做的决策树非常大，对于分析来说不现实。这消除了决策树能够看到分裂和节点的好处。

![](img/cfcc4f0c96654fd4b8ce7c23ff63967a.png)

这是原始的树，它太复杂、太大，可能会过度拟合。

> 第 4 部分:修剪树

为了更有效地利用这棵树，我需要把它修剪掉。为此，我决定改变最大深度。我尝试了不同的深度组合，发现最大深度 4 仍然具有很高的准确性(83.30%，略低于较大树的 85.76%)，但对于分析来说更现实。这让我相信它不是不合适的，因为精度仍然接近更大的树。

![](img/9c5901af857b312d6c2b6c58cd2c196a.png)

这显示了最大深度为 4 的决策树。最重要的特征是它是否是毒品犯罪，我怀疑这是因为我发现它是与逮捕最相关的变量。

> 第 5 部分:寻找最重要的特性

最后，我想看看这个模型最重要的特征是什么。我创建了一个特征及其重要性的数据框架，并打印了前 3 项。

![](img/c1e2469d9907f211eb5aaf5d7ee4d823.png)

这显示了按重要性排列的前三个特征:主要类型毒品、攻击和社区区域。

# 步骤 8:应用随机森林模型

> 第 1 部分:导入和拟合模型

首先，我导入了随机森林分类器，还导入了准确度分数、分类报告、召回分数、精确度分数和准确度分数。然后，我用 100 个随机估计值和 0 的随机状态来拟合我的模型。

> 第 2 部分:评估准确性

在此之后，我可以拟合我的模型并计算 predict_rf、recall_rf、precision_rf 和 accuracy_score。

我发现预测逮捕的准确率为 85.762%，略低于决策树的准确率，但高于逻辑模型。此外，它高于虚拟分类器模型的基线准确度(74.11%)。

![](img/a29e3289b2cd08fa70f2bd262403079e.png)

这表明随机森林模型的准确度约为 85.762%

> 第 3 部分:创建可视化

为了创建随机森林模型的可视化，我想创建一个混淆矩阵来显示它的准确性。它将最多的预测显示为真阴性(261254)，其次是真阳性(50709)，然后是大量的假阴性(43471)。这与以前用逻辑和决策树模型发现的结果相似。关于假阳性，该模型的数量(3.08%)略高于决策树，但低于 logistic。

![](img/ff4c704f7d6012aa56c462e53b2ff099.png)

这显示了随机森林模型的混淆矩阵，显示了最多的 TN，然后是 TP，然后是 FN。

![](img/476804c04de7abf1269e01efba56cd45.png)

这些是随机森林模型的上述混淆矩阵背后的数字。

> 第 4 部分:调整随机森林模型中的特征

最后，我想看看调整随机森林模型中要素数量的影响。

首先，我看了大量的 max 特性:10。有了这个，我得到了 85.762%的准确率。这只是略高于以前的森林模型。

![](img/f6c2b323d654e3559974c97b3650a51e.png)

这显示了 10 个最大特征的准确度分数

接下来，我查看了少量 max 特性，以了解精确度会如何变化。我把 max features 改成了 2。这并没有从 10 个特征中显著调整准确度，表明从 10 到 2 的特征数量对于该模型的准确度是不重要的。

# 步骤 9:选择最终模型

为了选择最终的模型，我想看两件主要的事情:准确率和假阳性的数量。由于假阳性可能意味着对假逮捕的偏见，所以我想选择假阳性率最低、准确性最高的模型。为了帮助我做到这一点，我想写一些总结信息。

> **模型 1:虚拟分类器模型**

> 准确率:74.11%
> 
> 假阳性率:0% *将所有人归类为未逮捕

> **模型二:逻辑回归模型**

> 准确率:85.66%
> 
> 假阳性率:3.36%

> **模型三:决策树模型**

> 准确率:85.763%
> 
> 假阳性率:3.06%

> **模型 4:随机森林模型**

> 准确率:85.762%
> 
> 假阳性率:3.08%

总的来说，我认为决策树模型是这种情况下的最佳模型。它以最高的准确性平衡了最低的假阳性率(除了虚拟分类器)。此外，我相信决策树模型赋予的基于特征重要性可视化节点分裂的能力对于用户理解逮捕的可能性是有用的。总的来说，我认为这个模型和它的可视化在这种情况下是最强大和最有用的，并推荐使用它。