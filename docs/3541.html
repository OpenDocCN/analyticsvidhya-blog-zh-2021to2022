<html>
<head>
<title>Determining Late Payments on Loan Application Using Machine Learning— an Amateur Approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习确定贷款申请的逾期付款——一种业余方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/determining-late-payments-on-loan-application-using-machine-learning-an-amateur-approach-1ae7501ed4f0?source=collection_archive---------5-----------------------#2021-07-08">https://medium.com/analytics-vidhya/determining-late-payments-on-loan-application-using-machine-learning-an-amateur-approach-1ae7501ed4f0?source=collection_archive---------5-----------------------#2021-07-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/47c3da7223a80d2b8b47dbb72532272d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*8sJlrqlRayQWgsjG.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">检索自<a class="ae ip" href="https://consumercreditcardrelief.com/wp-content/uploads/2019/09/debt-relief-program.png" rel="noopener ugc nofollow" target="_blank">https://consumercreditcardlife . com/WP-content/uploads/2019/09/debt-relief-program . png</a></figcaption></figure><p id="3c62" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">延迟付款一直是金融公司面临的主要风险之一。通常我们有信用分析师来决定一个申请是否有违约的倾向。确定哪些申请有违约倾向的目的是拒绝该贷款申请，或者设计一个处理它们的程序，从而使公司的损失最小化。感谢技术，我们现在能够使用机器学习来检测将自动默认的应用程序。借此机会，我想演示一下我们如何使用机器学习来预测违约贷款。</p><h1 id="3925" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">数据</h1><p id="2f15" class="pw-post-body-paragraph iq ir hh is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn ha bi translated">机器学习需要<strong class="is hi"> <em class="kr">数据</em> </strong>。关于机器学习如何处理数据，有一个很好的类比。当我们想学习如何做某事时，比如说摄影，我们会寻找照片看起来有多漂亮的“模式”。一张色彩鲜艳的照片应该归类为美丽还是一张对称的照片应该归类为美丽？机器学习也使用这种方法。它需要过去事件的数据来理解有违约倾向的客户的模式。</p><p id="e5c6" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我今天要用的数据来自我之前参加的一个金融公司职位的招聘测试。数据的原始来源是匿名的，但数据本身非常有趣。如果你们有兴趣自己尝试这些数据，我会在文章末尾留下完整数据集的链接。数据由4个不同的表组成，我说的表是指4个不同的表。csv文件。这4个不同的表格是:</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es ks"><img src="../Images/324a8e57c758fc23f0470d7d39f487a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*e5IcgJSNprzFwqEJNbn6lg.png"/></div></figure><p id="efd5" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我们将逐步评估数据，从installmentpayment.csv开始，因为首先看到行为数据符合我的个人利益。我们将使用以下脚本加载数据:</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="90ec" class="lc jp hh ky b fi ld le l lf lg">train = pd.read_csv('app_train.csv')<br/>train.drop('Unnamed: 0',axis = 1,inplace=True)<br/>test = pd.read_csv('app_test.csv')<br/>test.drop('Unnamed: 0',axis = 1,inplace=True)<br/>prev = pd.read_csv('prev_app.csv')<br/>prev.drop('Unnamed: 0',axis = 1,inplace=True)<br/>behavior = pd.read_csv('installment_payment.csv')<br/>behavior.drop('Unnamed: 0',axis = 1,inplace=True)</span></pre><p id="2be7" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">第一步(如何处理installmentpayment.csv) </strong></p><p id="9ec6" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">installmentpayment.csv数据集包含以下变量:</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es lh"><img src="../Images/0da471bd7bff897edcb74ce08a3fefbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*gXLROamACGBdBS1muYKd5Q.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">行为数据集的列</figcaption></figure><p id="efdf" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我们将删除LN_ID，因为我们将使用以前的贷款数据集连接该表，而不是直接使用train数据集。这个表格的有趣之处在于，我们实际上可以将INST天数、支付天数、AMT_INST和AMT_PAY缩减为两个新变量，它们讨论的是同一件事。我们可以创建一个变量PREV_LATENESS来描述每次付款的延迟时间。我们还可以创建另一个变量来描述处方金额和支付金额之间的差异。我们将这个变量命名为PREV_PAY_DEFICIT，以衡量客户完成支付的能力。</p><p id="ae8d" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">虽然我们实际上可以创建更多的变量，如针对客户延迟的安装数量，或者针对延迟和支付赤字的客户规定的信用到期日，但我们将仅使用PREV _ LATENESS和PREV _ PAY _ DEFICIT来描述客户在此机会中的行为。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="2179" class="lc jp hh ky b fi ld le l lf lg">behavior.drop('LN_ID',axis= 1,inplace=True)<br/>behavior.describe()<br/>behaviorengineered = behavior[:]<br/>behaviorengineered['PREV_PAY_DEFICIT']=behaviorengineered.AMT_INST - behaviorengineered.AMT_PAY<br/>behaviorengineered['PREV_LATENESS'] = behaviorengineered.INST_DAYS - behaviorengineered.PAY_DAYS<br/>behaviorengineered.drop(['INST_DAYS','PAY_DAYS','AMT_INST','AMT_PAY'],axis = 1, inplace=True)</span><span id="0508" class="lc jp hh ky b fi li le l lf lg">behaviorengineered.fillna(behaviorengineered.median(),inplace=True)</span></pre><p id="16df" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">第二步(将行为数据与以前的贷款数据结合)</strong></p><p id="f9b0" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我们将添加使用以前的贷款数据设计的行为数据，以描述以前客户的贷款申请数据及其行为。先前的贷款申请数据集包含每个唯一的SK_ID_PREV显示一行，这意味着每行仅描述先前申请的一个记录。同时，行为数据集具有多个具有相同SK_ID_PREV的行，这意味着它显示了形成相应SK_ID_PREV的行为的每个SK_ID_PREV的动作。因此，INST编号是一个SK_ID_PREV行为的“<em class="kr">n”</em>记录。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lj"><img src="../Images/14b9631780728f9f013afddab89ea1c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*vxmzj5phKLWgmT72KR0U2w.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">行为数据集中剩余的列</figcaption></figure><p id="0443" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi">— — — — — — — — — —-</p><p id="2230" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">在将客户的行为添加到之前的贷款申请数据集中之前，我们应该通过SK_ID_PREV对行为数据集中的客户行为进行分组。</p><p id="bff0" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi">— — — — — — — — — —-</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="caa2" class="lc jp hh ky b fi ld le l lf lg">prevpaydeficit = behaviorengineered.groupby('SK_ID_PREV')['PREV_PAY_DEFICIT'].agg(lambda x:x.median() if x.notnull().any() else np.nan)<br/>prevlateness = behaviorengineered.groupby('SK_ID_PREV')['PREV_LATENESS'].agg(lambda x:x.median() if x.notnull().any() else np.nan)</span><span id="d8da" class="lc jp hh ky b fi li le l lf lg">prev['PREV_PAY_DEFICIT'] = prev['SK_ID_PREV'].apply(lambda x: prevpaydeficit[x] if x in prevpaydeficit.index else np.nan)<br/>prev['PREV_LATENESS'] = prev['SK_ID_PREV'].apply(lambda x:prevlateness[x] if x in prevlateness.index else np.nan)</span></pre><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lo"><img src="../Images/6478342ad315e4e594782293bf09c197.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*nSoHG9YBuCOlb-ZqaMIdgQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">先前贷款数据集中缺失值的数量，包括行为列</figcaption></figure><p id="df8d" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">上图中显示的缺失值的数量表明，我们有许多关于以前贷款的缺失信息。此外，由于缺少行为列值的数量，我们的SK_ID_PREV行为几乎有一半没有被描述。我们不会在意现在出现的缺失值，因为这些数据只是作为训练数据集的附加数据。在我们将所有数据合并为一个训练数据集后，我们将注意显示的缺失值。此外，我们需要对之前贷款数据集中的分类变量进行编码。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="5a53" class="lc jp hh ky b fi ld le l lf lg">from sklearn.preprocessing import LabelEncoder, OrdinalEncoder</span><span id="7abf" class="lc jp hh ky b fi li le l lf lg">LEcontract_type = LabelEncoder()<br/>LEweekdays_apply = LabelEncoder()<br/>LEcontract_status = LabelEncoder()<br/>LEyield_group = LabelEncoder()</span><span id="cb3f" class="lc jp hh ky b fi li le l lf lg">prev['CONTRACT_TYPE'] = LEcontract_type.fit_transform(prev.CONTRACT_TYPE)<br/>prev.WEEKDAYS_APPLY = LEweekdays_apply.fit_transform(prev.WEEKDAYS_APPLY)<br/>prev.CONTRACT_STATUS = LEcontract_status.fit_transform(prev.CONTRACT_STATUS)<br/>prev.YIELD_GROUP = LEyield_group.fit_transform(prev.YIELD_GROUP)<br/>prev.describe()</span></pre><p id="4f5c" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">减少使用的变量</strong></p><p id="5545" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">在之前的贷款数据集中有20个变量或列，包括行为列。将所有20个变量添加到训练数据集中是可能的，但是这将是资源的浪费。为什么？因为可能存在彼此高度相关的变量，因此可以由其他变量来表示。减少数据最简单的方法是查看所有变量的相关矩阵，描述相关变量，并使用两个相关变量中的一个。这种方法通常被避免，因为它需要对解释的数据有深入的理解，但是为了数据准备的长度，我将使用它。我们将会看到相关性高于0.3或低于-0.3的变量。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="db6a" class="lc jp hh ky b fi ld le l lf lg">prev.drop(['SK_ID_PREV'],axis = 1, inplace = True)<br/>corrprevbhv = prev.corr()<br/>corr_triuprevbhv = corrprevbhv.where(~np.tril(np.ones(corrprevbhv.shape)).astype(np.bool))<br/>corr_triuprevbhv = corr_triuprevbhv.stack()<br/>corr_triuprevbhv.name = 'Pearson Correlation Coefficient'<br/>corr_triuprevbhv.index.names = ['First Var', 'Second Var']<br/>corr_triuprevbhv[(corr_triuprevbhv &gt; 0.3)|(corr_triuprevbhv &lt; -0.3)].to_frame()</span></pre><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lp"><img src="../Images/d3fd2e2c7be2ae844b027d83433cd8e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ETyVQdwnm3_XawMqQK4BEA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">所有列与各自临界值的相关性</figcaption></figure><p id="996f" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">从相关结果中，我选择了几个变量，在我看来，这些变量可以代表与之高度相关的其他变量。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="ac4d" class="lc jp hh ky b fi ld le l lf lg">prevbhvfinal = prev[['LN_ID','CONTRACT_TYPE','CONTRACT_STATUS','AMT_DOWN_PAYMENT','PRICE','WEEKDAYS_APPLY','HOUR_APPLY','DAYS_DECISION','PREV_PAY_DEFICIT','PREV_LATENESS','TERMINATION']]</span></pre><p id="5211" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">就像行为数据集一样，我们想检查这里的LN_ID是否是每行唯一的。我们检查它的值计数，并将其与行数进行比较。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/0c3979ed2883555e2569cd63ab9d1c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*fVUePqMqpwDyzyegngecBg.png"/></div></figure><p id="fcf1" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这表明在数据中有多次出现的LN _ IDs。在将之前的贷款数据集添加到训练和测试数据集之前，我们应该按照LN_ID对其进行分组。</p><p id="f62a" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">第三步(将以前的贷款数据分组并将其添加到培训测试数据中)</strong></p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="a80f" class="lc jp hh ky b fi ld le l lf lg">contract_type = prevbhvfinal.groupby(['LN_ID'])['CONTRACT_TYPE'].agg(lambda x: mode(x)[0][0] if x.notnull().any() else np.nan )<br/>contract_status = prevbhvfinal.groupby(['LN_ID'])['CONTRACT_STATUS'].agg(lambda x: mode(x)[0][0] if x.notnull().any() else np.nan)<br/>amt_down_payment = prevbhvfinal.groupby(['LN_ID'])['AMT_DOWN_PAYMENT'].agg(lambda x: x.median() if x.notnull().any() else np.nan )<br/>price = prevbhvfinal.groupby(['LN_ID'])['PRICE'].agg(lambda x: x.median() if x.notnull().any() else np.nan )<br/>weekdays_apply = prevbhvfinal.groupby(['LN_ID'])['WEEKDAYS_APPLY'].agg(lambda x: mode(x)[0][0] if x.notnull().any() else np.nan )<br/>hour_apply = prevbhvfinal.groupby(['LN_ID'])['HOUR_APPLY'].agg(lambda x: x.median() if x.notnull().any() else np.nan )<br/>days_decision = prevbhvfinal.groupby(['LN_ID'])['DAYS_DECISION'].agg(lambda x: x.median() if x.notnull().any() else np.nan )<br/>prev_pay_deficit = prevbhvfinal.groupby(['LN_ID'])['PREV_PAY_DEFICIT'].agg(lambda x: x.median() if x.notnull().any() else np.nan )<br/>prev_lateness = prevbhvfinal.groupby(['LN_ID'])['PREV_LATENESS'].agg(lambda x: x.median() if x.notnull().any() else np.nan )<br/>termination = prevbhvfinal.groupby(['LN_ID'])['TERMINATION'].agg(lambda x: x.median() if x.notnull().any() else np.nan )</span></pre><p id="c2d5" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">在前面贷款数据集的分组完成之后，我们可以开始将这些变量作为附加列添加到train-test数据集。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="7eb6" class="lc jp hh ky b fi ld le l lf lg">LEincometype = LabelEncoder()<br/>LEeducation = OrdinalEncoder(categories = [['Academic degree','Lower secondary','Secondary / secondary special','Incomplete higher','Higher education']])<br/>LEfamilystatus = LabelEncoder()<br/>LEhousingtypes = LabelEncoder()<br/>LEorganizationtype = LabelEncoder()</span><span id="0b2e" class="lc jp hh ky b fi li le l lf lg">train['PREV_CONTRACT_TYPE'] = train['LN_ID'].apply(lambda x: contract_type[x] if x in contract_type.index else np.nan)<br/>train['PREV_AMT_DOWN_PAYMENT'] = train['LN_ID'].apply(lambda x: amt_down_payment[x] if x in amt_down_payment.index else np.nan)<br/>train['PREV_PRICE'] = train['LN_ID'].apply(lambda x: price[x] if x in price.index else np.nan)<br/>train['PREV_WEEKDAYS_APPLY'] = train['LN_ID'].apply(lambda x: weekdays_apply[x] if x in weekdays_apply.index else np.nan)<br/>train['PREV_HOUR_APPLY'] = train['LN_ID'].apply(lambda x: hour_apply[x] if x in hour_apply.index else np.nan)<br/>train['PREV_DAYS_DECISION'] = train['LN_ID'].apply(lambda x: days_decision[x] if x in days_decision.index else np.nan)<br/>train['PREV_PAY_DEFICIT'] = train['LN_ID'].apply(lambda x: prev_pay_deficit[x] if x in prev_pay_deficit.index else np.nan)<br/>train['PREV_LATENESS'] = train['LN_ID'].apply(lambda x: prev_lateness[x] if x in prev_lateness.index else np.nan)<br/>train['PREV_TERMINATION'] = train['LN_ID'].apply(lambda x: termination[x] if x in termination.index else np.nan)<br/>train['PREV_CONTRACT_STATUS'] = train['LN_ID'].apply(lambda x:contract_status[x] if x in contract_status.index else np.nan)</span><span id="b2b1" class="lc jp hh ky b fi li le l lf lg">fortraingenddummy = pd.get_dummies(train.GENDER)<br/>train['GENDER_F'], train['GENDER_M'] = fortraingenddummy['F'],fortraingenddummy['M']<br/>train.drop('GENDER',axis = 1, inplace= True)<br/>train['INCOME_TYPE'] = LEincometype.fit_transform(train['INCOME_TYPE'])<br/>train['EDUCATION'] = LEeducation.fit_transform(train.loc[:,['EDUCATION']])<br/>train['FAMILY_STATUS'] = LEfamilystatus.fit_transform(train['FAMILY_STATUS'])<br/>train['HOUSING_TYPE'] = LEhousingtypes.fit_transform(train['HOUSING_TYPE'])<br/>train['ORGANIZATION_TYPE'] = LEorganizationtype.fit_transform(train['ORGANIZATION_TYPE'])</span><span id="2550" class="lc jp hh ky b fi li le l lf lg">train['CONTRACT_TYPE'] = LEcontract_type.transform(train['CONTRACT_TYPE'])<br/>train['WEEKDAYS_APPLY'] = LEweekdays_apply.transform(train['WEEKDAYS_APPLY'])</span></pre><p id="0241" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">对测试数据集也是如此。</p><p id="6be0" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">第四步(清理训练数据、训练模型和评估)</strong></p><p id="7dd9" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">关于之前遇到的缺失值，我们可以在训练数据集中开始评估。我们还将导入KNNImputer和StandardScaler来缩放数据并估算缺失值。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="e06e" class="lc jp hh ky b fi ld le l lf lg">from sklearn.impute import KNNImputer<br/>from sklearn.preprocessing import StandardScaler</span><span id="e88b" class="lc jp hh ky b fi li le l lf lg">train.isnull().sum()/len(train)<br/>test.isnull().sum()/len(test)</span></pre><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/7475901f659b738e5140433388e6240d.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*G4hbzB5uvO8gv58KSyQtZA.png"/></div></figure><p id="9d23" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">缺失值的百分比没有之前贷款数据集中缺失值的百分比大。这表明大部分具有缺失值的SK_ID_PREV不是列车数据中使用的LN_ID的SK_ID_PREV。EXT_SCORE_1包含50%的缺失值，这确实令人担忧。我们将删除此列，因为它包含大量缺失值，使得它很难包含在模型中。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="0878" class="lc jp hh ky b fi ld le l lf lg">train.drop('EXT_SCORE_1',axis = 1, inplace = True)<br/>test.drop('EXT_SCORE_1',axis = 1, inplace=True)</span></pre><p id="4762" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">在将训练数据输入模型之前，我们还需要检查类的不平衡。我们可以简单地通过对目标变量使用value_counts()来检查它。</p><p id="ee86" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><code class="du ls lt lu ky b">train.TARGET.value_counts()</code></p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es lv"><img src="../Images/0ed0c27cdf8643c396492e2e51bd192a.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*dzf561CpT0FNQSntKZYWlg.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">列车数据中的类别不平衡</figcaption></figure><p id="185e" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">在我们对数据进行缩放和估算之后，我们可以在以后处理等级不平衡的问题。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="b8ed" class="lc jp hh ky b fi ld le l lf lg">scale = StandardScaler()<br/>impute=KNNImputer()</span><span id="87e9" class="lc jp hh ky b fi li le l lf lg">xtrain = train.drop('TARGET',axis = 1, inplace = False)<br/>ytrain = train['TARGET']</span><span id="dfe3" class="lc jp hh ky b fi li le l lf lg">xtest = test.drop('TARGET',axis = 1, inplace = False)<br/>ytest = test['TARGET']</span><span id="fb27" class="lc jp hh ky b fi li le l lf lg">xtrainscaled = pd.DataFrame(scale.fit_transform(xtrain),columns = xtrain.columns)<br/>xtrainscaledimpute = pd.DataFrame(impute.fit_transform(xtrainscaled),columns = xtrainscaled.columns)</span><span id="9308" class="lc jp hh ky b fi li le l lf lg">xtestscaled = pd.DataFrame(scale.transform(xtest),columns=xtest.columns)</span><span id="b2f1" class="lc jp hh ky b fi li le l lf lg">xtestscaledimpute = pd.DataFrame(impute.fit_transform(xtestscaled),columns = xtestscaled.columns)</span></pre><p id="02e0" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">在处理类别不平衡之前，我们需要检查训练数据中的相关性。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="9236" class="lc jp hh ky b fi ld le l lf lg">corr = train.corr()<br/>corr_triu = corr.where(~np.tril(np.ones(corr.shape)).astype(np.bool))<br/>corr_triu = corr_triu.stack()<br/>corr_triu.name = 'Pearson Correlation Coefficient'<br/>corr_triu.index.names = ['First Var', 'Second Var']<br/>corr_triu[(corr_triu &gt; 0.3) | (corr_triu &lt; -0.3)].to_frame()</span></pre><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es lw"><img src="../Images/a95e5de67e5cab506feaa052d123983c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*0zZSSON97pZ1hSRKusJfeQ.png"/></div></figure><p id="c0ee" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我们可以降低价格和工作日，然后继续处理阶级不平衡。我们将使用SMOTE来处理职业不平衡。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="48d9" class="lc jp hh ky b fi ld le l lf lg">xtrainscaledimpute.drop(['PRICE','DAYS_WORK'],axis = 1 , inplace = True)<br/>xtestscaledimpute.drop(['PRICE','DAYS_WORK'], axis = 1, inplace = True)</span><span id="4785" class="lc jp hh ky b fi li le l lf lg">from imblearn.over_sampling import SMOTE<br/>sm = SMOTE(random_state = 42)<br/>xtrainfinal, ytrainfinal = sm.fit_resample(xtrainscaledimpute, ytrain)</span></pre><p id="96a5" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">造型</strong></p><p id="6bea" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我将利用这个机会使用逻辑回归，随机森林和感知器。我们将展示用GridSearchCV稍微调整的每个模型的结果。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="5d9f" class="lc jp hh ky b fi ld le l lf lg">from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.linear_model import Perceptron, LogisticRegression<br/>from sklearn.metrics import classification_report,accuracy_score,roc_auc_score,f1_score<br/>from sklearn.model_selection import GridSearchCV,RandomizedSearchCV</span><span id="5c68" class="lc jp hh ky b fi li le l lf lg"><strong class="ky hi">###logreg</strong><br/>logreg = GridSearchCV(LogisticRegression(max_iter=300),dict(solver = ['newton-cg', 'lbfgs', 'sag', 'saga']),<br/>                      scoring='roc_auc')<br/>logreg.fit(xtrainfinal,ytrainfinal)<br/>logreg.best_estimator_<br/>logregmodel = LogisticRegression(max_iter=300, solver='sag')<br/>logregmodel.fit(xtrainfinal,ytrainfinal)<br/>ypredlogreg = (logregmodel.predict_proba(xtestscaledimpute)[:,1]&gt;=0.5).astype(int)<br/>rocauclogreg = round(roc_auc_score(ytest,ypredlogreg),3)<br/>classreportlogreg = classification_report(ytest,ypredlogreg)<br/>print('Logistic Regression Classification Report\n'+classreportlogreg+"\nROC AUC Score: "+str(rocauclogreg)+"\nF1-Score: "+str(f1_score(ytest,ypredlogreg)))<br/>acclogreg = round(accuracy_score(ytest,ypredlogreg),3)</span><span id="7c37" class="lc jp hh ky b fi li le l lf lg"><strong class="ky hi">###Perceptron</strong><br/>Perceptron = GridSearchCV(Perceptron(random_state = 42),dict(penalty=['l2','l1','elasticnet','None'],<br/>                                                             class_weight = ['balanced','None']))<br/>Perceptron.fit(xtrainfinal,ytrainfinal)<br/>ypredPercept = Perceptron.predict(xtestscaledimpute)<br/>rocaucPercept = round(roc_auc_score(ytest,ypredPercept),3)<br/>classreportPercept = classification_report(ytest,ypredPercept)<br/>print('Perceptron Classification Report\n'+classreportPercept+"\nROC AUC Score: "+str(rocaucPercept)+"\nF1-Score: "+str(f1_score(ytest,ypredPercept)))<br/>accPercept= round(accuracy_score(ytest,ypredPercept),3)</span><span id="25a4" class="lc jp hh ky b fi li le l lf lg"><strong class="ky hi">###RandomForestClassifier</strong><br/>RF = RandomizedSearchCV(RandomForestClassifier(random_state=42),dict(n_estimators=[100,150,200],<br/>                                                criterion = ['gini','entropy'],<br/>                                                max_features = ['sqrt','log2']),<br/>                        random_state=42,scoring='roc_auc')<br/>RF.fit(xtrainfinal,ytrainfinal)<br/>RF.best_params_<br/>RFmodel = RandomForestClassifier(n_estimators= 200, max_features= 'log2', criterion= 'entropy',random_state=42)<br/>RFmodel.fit(xtrainfinal,ytrainfinal)<br/>ypredrf = (RFmodel.predict_proba(xtestscaledimpute)[:,1]&gt;=0.5).astype(int)<br/>rocaucrf = round(roc_auc_score(ytest,ypredrf),3)<br/>classreportrf = classification_report(ytest,ypredrf)<br/>print('Random Forest Classification Report \n'+classreportrf+"\nROC AUC Score: "+str(rocaucrf)+"\nF1-Score: "+str(f1_score(ytest,ypredrf)))<br/>accrf = round(accuracy_score(ytest,ypredrf),3)</span></pre><div class="kt ku kv kw fd ab cb"><figure class="lx ii ly lz ma mb mc paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/2e472d5f4c5b2f1b39ade06b0137c3e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*hyxuD7y5CEAfCVe5ihO51Q.png"/></div></figure><figure class="lx ii md lz ma mb mc paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/9c0b2202af4900473955060c9d263489.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*faA8M3oPll_jDydHRod2OQ.png"/></div></figure><figure class="lx ii me lz ma mb mc paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/cd5ec78b57f26956ff9de03bb9aee7e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*wZ8cVkz6i5qCJvv4g5A_Bw.png"/></div><figcaption class="il im et er es in io bd b be z dx mf di mg mh translated">使用GridSearchCV训练的模型(概率截止值不变)</figcaption></figure></div><p id="278d" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我们可以看到，在所有的模型中，感知器和逻辑回归似乎是最好的替代品。我们通过关注F1分数、ROC AUC分数和准确性分数来衡量模型的性能。</p><blockquote class="mi mj mk"><p id="8490" class="iq ir kr is b it iu iv iw ix iy iz ja ml jc jd je mm jg jh ji mn jk jl jm jn ha bi translated"><em class="hh">注意:在这个项目中，我选择使用F1得分、ROC AUC和准确性来衡量模型性能，因为目标是最大化对两类客户(倾向于延迟付款的客户和倾向于及时付款的客户)做出的预测。</em> <em class="hh">如果目标是</em> <strong class="is hi"> <em class="hh">减少假阴性的数量(客户迟到但我们没有察觉)</em> </strong> <em class="hh">，我们应该用</em><strong class="is hi"><em class="hh"/></strong><em class="hh">来衡量模型的表现。</em></p></blockquote><p id="3434" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">虽然感知器和逻辑回归可能是最好的选择，但我们没有试图改变逻辑回归和随机森林的预测概率的截止值。在我们改变了随机森林和逻辑回归的预测概率的截止值后，我们可以看到它们的新性能。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="5122" class="lc jp hh ky b fi ld le l lf lg"><strong class="ky hi">###logreg</strong><br/>logreg = GridSearchCV(LogisticRegression(max_iter=300),dict(solver = ['newton-cg', 'lbfgs', 'sag', 'saga']), scoring='roc_auc')<br/>logreg.fit(xtrainfinal,ytrainfinal)<br/>logreg.best_estimator_<br/>logregmodel = LogisticRegression(max_iter=300, solver='sag')<br/>logregmodel.fit(xtrainfinal,ytrainfinal)<br/>ypredlogreg = (logregmodel.predict_proba(xtestscaledimpute)[:,1]&gt;=0.62).astype(int)<br/>rocauclogreg = round(roc_auc_score(ytest,ypredlogreg),3)<br/>classreportlogreg = classification_report(ytest,ypredlogreg)<br/>print('Logistic Regression Classification Report\n'+classreportlogreg+"\nROC AUC Score: "+str(rocauclogreg)+"\nF1-Score: "+str(f1_score(ytest,ypredlogreg)))<br/>acclogreg = round(accuracy_score(ytest,ypredlogreg),3)</span><span id="92b5" class="lc jp hh ky b fi li le l lf lg"><strong class="ky hi">###RandomForestClassifier</strong><br/>RF = RandomizedSearchCV(RandomForestClassifier(random_state=42),dict(n_estimators=[100,150,200], criterion = ['gini','entropy'], max_features = ['sqrt','log2']),random_state=42,scoring='roc_auc')<br/>RF.fit(xtrainfinal,ytrainfinal)<br/>RF.best_params_<br/>RFmodel = RandomForestClassifier(n_estimators= 200, max_features= 'log2', criterion= 'entropy',random_state=42)<br/>RFmodel.fit(xtrainfinal,ytrainfinal)<br/>ypredrf = (RFmodel.predict_proba(xtestscaledimpute)[:,1]&gt;=0.318).astype(int)<br/>rocaucrf = round(roc_auc_score(ytest,ypredrf),3)<br/>classreportrf = classification_report(ytest,ypredrf)<br/>print('Random Forest Classification Report \n'+classreportrf+"\nROC AUC Score: "+str(rocaucrf)+"\nF1-Score: "+str(f1_score(ytest,ypredrf)))<br/>accrf = round(accuracy_score(ytest,ypredrf),3)</span></pre><div class="kt ku kv kw fd ab cb"><figure class="lx ii mo lz ma mb mc paragraph-image"><img src="../Images/446bab3dff287588c2f4e41f6150ad8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*myMl199TMoNXmLNWe3w9AQ.png"/></figure><figure class="lx ii mp lz ma mb mc paragraph-image"><img src="../Images/3b3dd8e187e3016fb889ebd896926872.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*QvMTabW_y8hP2jfgk4LSGw.png"/><figcaption class="il im et er es in io bd b be z dx mq di mr mh translated">逻辑回归和随机森林在改变截止值后的新表现</figcaption></figure></div><p id="d265" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">在不改变预测概率截止值的情况下，逻辑回归和随机森林比逻辑回归和随机森林表现得更好。通过改变临界值，随机森林的准确性从90%下降到80%，但其ROC AUC得分和F1得分显著增加。我们更关注ROC AUC得分和F1得分，因为这两个指标都描述了模型可以预测每个类别的程度(在本例中，是违约贷款还是正常贷款)。</p><h1 id="dc97" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结论</h1><p id="224e" class="pw-post-body-paragraph iq ir hh is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn ha bi translated">这个模型本身并不完美。我本可以尝试做但没有做的事情的ROC AUC得分&lt;0.7 is still considered poor. But the point of this writing is to described one of the approaches we can take to build a model that determines late payments of loan applications.</p><p id="e78b" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi"/></p><p id="8cfa" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">正如我之前提到的，这只是处理这些数据的众多方法之一。在这篇文章之外，我想尝试的几件事是:</p><ul class=""><li id="27e4" class="ms mt hh is b it iu ix iy jb mu jf mv jj mw jn mx my mz na bi translated">我们可以尝试在行为数据上更有创造性。除了迟到和支付赤字，我们可以创造更多的变量。</li><li id="023e" class="ms mt hh is b it nb ix nc jb nd jf ne jj nf jn mx my mz na bi translated">我们可以尝试使用聚类以前的贷款或行为数据。这可以让我们深入了解可能以某种方式表现的看不见的集群，从而可以用于总结以前的贷款或行为数据，同时有效地最小化添加到模型中的变量数量。</li><li id="aa6d" class="ms mt hh is b it nb ix nc jb nd jf ne jj nf jn mx my mz na bi translated">我们可以尝试PCA来减少给定的变量。这肯定是一个比只看所有变量的相关矩阵更好的方法。</li></ul><div class="ng nh ez fb ni nj"><a href="https://drive.google.com/drive/folders/1EfnzjMyjibi25HHyrlGTmEbxrZphnl-Q?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab dw"><div class="nl ab nm cl cj nn"><h2 class="bd hi fi z dy no ea eb np ed ef hg bi translated">贷款申请数据集- Google Drive</h2><div class="nq l"><h3 class="bd b fi z dy no ea eb np ed ef dx translated">链接到使用的数据集和列描述</h3></div><div class="nr l"><p class="bd b fp z dy no ea eb np ed ef dx translated">drive.google.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx ij nj"/></div></div></a></div><p id="2aad" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">感谢您的阅读！</p></div></div>    
</body>
</html>