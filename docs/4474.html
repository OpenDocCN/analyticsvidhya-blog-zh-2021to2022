<html>
<head>
<title>Regularization!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正规化！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/regularization-understanding-fb8cd7b7c0e2?source=collection_archive---------2-----------------------#2021-10-22">https://medium.com/analytics-vidhya/regularization-understanding-fb8cd7b7c0e2?source=collection_archive---------2-----------------------#2021-10-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="fb49" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">读者你好，</p><p id="570b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这篇博文将帮助你理解为什么正则化在训练机器学习模型中是重要的，以及为什么它是ML领域中谈论最多的话题。</p><h2 id="b384" class="jj jk hh bd jl jm jn jo jp jq jr js jt iw ju jv jw ja jx jy jz je ka kb kc kd bi translated"><strong class="ak">让我们开始</strong></h2><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ke"><img src="../Images/35194faadfd08518d03da17727b842a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v-6hImejBq-0z9ICCmYKkQ.png"/></div></div></figure><p id="5eb8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">那么，让我们看看这个图。我们从中破译了什么？<br/>在该图中，x轴是迭代所用的时间，y轴是训练和测试数据的损失。你能注意到这里有什么不对吗？</p><p id="e778" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">是的，损失值在训练数据上很好地趋于下降，但在测试数据的某个点上却迅速上升。这可不好。测试数据存在一定程度的过度拟合。我们如何解决这个问题？</p><p id="cfb4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们可以做一件事，我们可以提前停止迭代以避免过度拟合。这被称为<strong class="in hi">提前停止</strong>。<strong class="in hi">提前停止</strong>是一项功能，当选择的指标停止改善时，该功能可以自动停止训练。但是提前停止的一个问题是<strong class="in hi">模型没有利用所有可用的训练数据，因此使得训练数据非常有限。</strong></p><p id="ecb2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">那么现在应该怎么做呢？这就是正规化可以拯救的地方。<br/>正则化是将系数估计值最小化为零的技术。这意味着它可以防止模型学习数据的复杂性，并避免过度拟合。</p><p id="2c78" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">有两种类型的正则化技术:</p><ol class=""><li id="86b6" class="kq kr hh in b io ip is it iw ks ja kt je ku ji kv kw kx ky bi translated">里脊回归</li><li id="fec5" class="kq kr hh in b io kz is la iw lb ja lc je ld ji kv kw kx ky bi translated">套索回归</li></ol><h2 id="c647" class="jj jk hh bd jl jm jn jo jp jq jr js jt iw ju jv jw ja jx jy jz je ka kb kc kd bi translated">里脊回归</h2><p id="2bd0" class="pw-post-body-paragraph il im hh in b io le iq ir is lf iu iv iw lg iy iz ja lh jc jd je li jg jh ji ha bi translated">岭回归是一种正则化技术，其中我们引入了一个称为<strong class="in hi">岭回归损失</strong>的小偏差。这也被称为L-2正则化</p><p id="1276" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在简单回归中，这是拟合过程，损失函数是<strong class="in hi">残差平方和</strong>。在等式中，Y是具有特征和斜率的线。看下面的等式。<br/>如果我们有一个特征，那么它将是<strong class="in hi">Y =<em class="lj">βx</em>+c<br/>+T19】如果我们有两个特征，那么它将是<strong class="in hi">Y =<em class="lj">β1x 1</em>+<em class="lj">β2x 2</em>+c<br/></strong>并且对于多个特征来说，情况是这样的<strong class="in hi"><br/><em class="lj">Y≈β0+β1x 1+β2x 2+…+βpXp【t3x】</em></strong></strong></p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es lk"><img src="../Images/2897cc03b4451234767b6a9a81de8673.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/0*by2GJk6UnPonzJEl.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">残差平方和</figcaption></figure><p id="b0d8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">所以RSS是线性回归的成本函数，因此岭回归的成本函数变成<strong class="in hi"> RSS + λ(slope) </strong></p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es lp"><img src="../Images/175b36e978fb8f5fc50296fcf010edf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/0*G3BTpX7DaCuWNiE3.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">岭回归的成本函数</figcaption></figure><p id="e7f5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">因此，我们必须通过添加偏差值来惩罚拟合线，从而降低该值。这将减少过度拟合的机会。<strong class="in hi"> λ </strong>值介于0-1之间。</p><h2 id="2c49" class="jj jk hh bd jl jm jn jo jp jq jr js jt iw ju jv jw ja jx jy jz je ka kb kc kd bi translated">套索回归</h2><p id="763b" class="pw-post-body-paragraph il im hh in b io le iq ir is lf iu iv iw lg iy iz ja lh jc jd je li jg jh ji ha bi translated">这也称为L-1正则化。Lasso回归的工作方式与Ridge相似，但唯一的区别是惩罚项。在山脊中，我们取斜率的平方，而在套索中，我们取斜率的大小。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es lq"><img src="../Images/2aec7fafbf5496e90763c1ec834dea47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/0*Z_wTil0XQwYQwhkl.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">套索回归的成本函数</figcaption></figure><p id="e35d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们使用斜率大小，因为我们不仅避免了过度拟合，而且使用它进行特征选择。</p><p id="6c67" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在岭回归中，当我们增加罚项时，斜率将慢慢趋向于零，但在套索回归中，当我们增加罚项时，斜率将变为零。这就是我们使用lasso回归作为特征选择方法的原因。因为在增加罚项之后，一些特征值将变为零，因此我们得出结论，这些特征对于预测我们的最佳拟合线并不重要。</p><h2 id="9a49" class="jj jk hh bd jl jm jn jo jp jq jr js jt iw ju jv jw ja jx jy jz je ka kb kc kd bi translated">岭回归和套索回归的区别</h2><ul class=""><li id="4eef" class="kq kr hh in b io le is lf iw lr ja ls je lt ji lu kw kx ky bi translated">岭回归有助于避免过度拟合的问题，但它保留了模型中的所有特征。而套索回归不仅有助于解决过度拟合的问题，还可以从模型中移除低效的特征。</li><li id="b73b" class="kq kr hh in b io kz is la iw lb ja lc je ld ji lu kw kx ky bi translated">岭回归使系数值缓慢地趋向于零，但在拉索回归中，系数变为零。</li></ul><h1 id="536c" class="lv jk hh bd jl lw lx ly jp lz ma mb jt mc md me jw mf mg mh jz mi mj mk kc ml bi translated">终于！</h1><p id="0cde" class="pw-post-body-paragraph il im hh in b io le iq ir is lf iu iv iw lg iy iz ja lh jc jd je li jg jh ji ha bi translated"><em class="lj">感谢阅读！</em></p><p id="991b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果你喜欢这个，请投我一票，也建议一些更多的主题。</p><p id="3598" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">请随时在<a class="ae mm" href="https://www.linkedin.com/in/ajinkya-mishrikotkar-a6594a144/" rel="noopener ugc nofollow" target="_blank"> <strong class="in hi"> LinkedIn </strong> </a>上联系我</p><p id="29c1" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">欢迎在下面发表评论，并提出你认为我错过了什么。谢谢！</p></div></div>    
</body>
</html>