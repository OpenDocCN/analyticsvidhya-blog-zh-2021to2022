<html>
<head>
<title>Activation function for Artificial Neural Network.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工神经网络的激活函数。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-function-for-artificial-neural-network-8dc5b4854bdc?source=collection_archive---------18-----------------------#2021-01-07">https://medium.com/analytics-vidhya/activation-function-for-artificial-neural-network-8dc5b4854bdc?source=collection_archive---------18-----------------------#2021-01-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/78ca0905f8da33271453738f7fa37a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HiC4_wI6RrhzCc_i"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://unsplash.com/@possessedphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">附身摄影</a>在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><h1 id="044f" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">介绍</h1><p id="5d0e" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">激活函数是人工智能背后的主要计算核心，主要用于神经网络，今天我们将通过一个简短的介绍和一个常见用例的清晰示例来概述其中的一些功能。</p><h2 id="311f" class="kq iv hh bd iw kr ks kt ja ku kv kw je kd kx ky ji kh kz la jm kl lb lc jq ld bi translated">二元阶跃函数</h2><p id="05d2" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated"><em class="le">二进制阶跃函数</em>或“<strong class="ju hi">亥维赛阶跃函数”</strong>，是一个代表开启特定值或在特定时间后开启阈值的信号的函数。二进制阶跃函数主要用于单个感知器神经网络，并用于在两个类别之间进行线性分离。但是在神经网络上使用二进制阶跃函数的背后有一点需要注意，基于微积分，阶跃函数的梯度下降总是0，这表示在更新权重时没有等级变化。</p><p id="67f6" class="pw-post-body-paragraph js jt hh ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp ha bi translated">接下来，我们可以找到二进制阶跃函数的“python”实现。</p><figure class="lk ll lm ln fd ii"><div class="bz dy l di"><div class="lo lp l"/></div></figure><figure class="lk ll lm ln fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/d1eccad59c669426c92ad61e052c9b29.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*tfjoy2i1SDO2tlL-fULZgw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">二元阶跃函数图</figcaption></figure><h2 id="6cfd" class="kq iv hh bd iw kr ks kt ja ku kv kw je kd kx ky ji kh kz la jm kl lb lc jq ld bi translated">线性激活函数</h2><p id="7554" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">线性激活函数它接受来自范围[-inf，+inf]的输入并产生范围[-inf，+inf]，比在{0，1}之间堆叠的二进制阶跃函数好不了多少。相反，它可以像所有的<em class="le">线性函数</em>一样共享相同的问题，其中导数总是常数，使得反向传播在更新权重方面无用。线性激活的另一个问题是，它使得NN中的堆叠层没有效果，并且最后一层作为第一层仍然具有线性激活。</p><p id="08d1" class="pw-post-body-paragraph js jt hh ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp ha bi translated">下面是线性实现的代码片段和图表</p><figure class="lk ll lm ln fd ii"><div class="bz dy l di"><div class="lo lp l"/></div></figure><figure class="lk ll lm ln fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/b15f16ba1d60017133616eeee84e6fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*KHyTaYqNb1FqO2kYBmJwYA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">线性激活函数图</figcaption></figure><h2 id="2876" class="kq iv hh bd iw kr ks kt ja ku kv kw je kd kx ky ji kh kz la jm kl lb lc jq ld bi translated">Sigmoid激活函数</h2><p id="8e33" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">说到我们的第一个非线性激活函数，也是几种原因中最常用的一个，与Heaviside阶跃函数具有相同的形状，但其平滑性可以防止输出在0和1之间跳跃。它使sigmoid函数成为分类的最佳拟合函数，有助于进行清晰的预测。</p><figure class="lk ll lm ln fd ii"><div class="bz dy l di"><div class="lo lp l"/></div></figure><figure class="lk ll lm ln fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/34179f0f852147582193acfbfc86ce1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*DYjafA7ZVHVhY-DPWRRAtg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Sigmoid激活函数</figcaption></figure><h2 id="b38d" class="kq iv hh bd iw kr ks kt ja ku kv kw je kd kx ky ji kh kz la jm kl lb lc jq ld bi translated">Tanh激活函数</h2><p id="1964" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">双曲正切函数是一个三角函数，与sigmoid函数一样，具有梯度下降初等变换的所有优点。但是Tanh激活函数有其秘密武器来对抗最强的负值，因为零中心形状。</p><figure class="lk ll lm ln fd ii"><div class="bz dy l di"><div class="lo lp l"/></div></figure><figure class="lk ll lm ln fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/0903592a78f33c6c2ef67a3afbc42582.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*CJdJoTnJDVHrPCWhFNIcaw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Tanh激活函数</figcaption></figure><h2 id="80ac" class="kq iv hh bd iw kr ks kt ja ku kv kw je kd kx ky ji kh kz la jm kl lb lc jq ld bi translated">整流线性单元</h2><p id="c96c" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">shot的校正线性单元或RelU是用于比Sigmoid或Tanh更快地收敛神经网络的激活函数。尽管它看起来像一个线性函数，但它是为负范围设计的，这给了函数一个导数。但是RelU函数可能会死于零值或负值。</p><figure class="lk ll lm ln fd ii"><div class="bz dy l di"><div class="lo lp l"/></div></figure><figure class="lk ll lm ln fd ii er es paragraph-image"><div class="er es ls"><img src="../Images/7fc22a2f87c1960d13a95554b12f094d.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*TeMNTkPcmXf4dz2yPJU10w.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">ReLU激活功能</figcaption></figure><h2 id="3d3d" class="kq iv hh bd iw kr ks kt ja ku kv kw je kd kx ky ji kh kz la jm kl lb lc jq ld bi translated">Softmax激活功能</h2><p id="ac34" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">softmax激活函数最适合输出图层，因为它能够在多重分类中对输出类之间的一对一进行分类。</p><figure class="lk ll lm ln fd ii"><div class="bz dy l di"><div class="lo lp l"/></div></figure></div></div>    
</body>
</html>