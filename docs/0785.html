<html>
<head>
<title>Business Name Generator with TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TensorFlow的企业名称生成器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/business-name-generator-with-tensorflow-eba86c35d9cf?source=collection_archive---------6-----------------------#2021-01-31">https://medium.com/analytics-vidhya/business-name-generator-with-tensorflow-eba86c35d9cf?source=collection_archive---------6-----------------------#2021-01-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="623b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">再次成名</em></p><h2 id="6db5" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">动机</h2><p id="d575" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">前几天，我在想给我正在开发的应用取个名字。但是我想到的都已经存在了，他们的域名不可用，或者很贵。我的想法用完了，我开始需要一个工具来获得可供选择的想法。我找到了许多关于这方面的有用网页，但它们仍然对我不起作用。所以，我决定用Python构建自己的企业名称生成器，并与大家分享，这样也许你会发现它很有帮助，或者至少你学到了一些你不知道的东西。</p><p id="b389" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我认为一个网站、应用程序或企业的名称至关重要。即使在以前，我也不会因为名字而想去几个公司工作。创建一个简单、易记、易拼、可在互联网上使用的域名并不容易。这就是我创建这个项目的动机。而且，我认为如果你从递归神经网络和张量流开始，这是理想的。它还允许你将它扩展到更复杂的新内容，比如自动完成信息，就像Gmail在我们写电子邮件时所做的那样。</p><h2 id="9f7f" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">程序</h2><p id="78d2" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">要在给定空字符串、前缀或后缀的情况下创建名称生成器，我们必须用我们希望遵循的模式加载数据集。它可以是一本字典，许多不同语言的字典，一个公司名称，短语，混合的列表，等等。这取决于你，结果将取决于它；为了简单起见，我选择了字典。接下来，我们将处理RNN输入的数据。如果你不知道什么是RNNs，我推荐吴恩达的<a class="ae kd" href="https://www.deeplearning.ai/" rel="noopener ugc nofollow" target="_blank"> deeplearning.ai </a>课程。然后，我们将使用TensorFlow构建、编译和训练我们的模型，这将帮助我们进行预测。最后，我将向您展示我用来利用模型并生成新名称的算法。</p><h2 id="75f4" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">要求</h2><ul class=""><li id="f35a" class="ke kf hh ig b ih jy il jz ip kg it kh ix ki jb kj kk kl km bi translated">Python 3</li><li id="fc89" class="ke kf hh ig b ih kn il ko ip kp it kq ix kr jb kj kk kl km bi translated">张量流&gt; = 2.0</li></ul><h2 id="cd63" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">导入包</h2><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="f448" class="jd je hh kx b fi lb lc l ld le">from tensorflow.keras.callbacks import LambdaCallback<br/>from tensorflow.keras.optimizers import RMSprop<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense<br/>from tensorflow.keras.layers import LSTM<br/>import numpy as np<br/>import random<br/>import sys<br/>import io<br/>import os</span></pre><h2 id="156f" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">加载数据</h2><p id="9752" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">在接下来的几行中，您必须添加单词的路径以及您想要遵循的模式(或者句子，如果您想要开始玩自动完成消息的话)。在我的例子中，我将<a class="ae kd" href="https://github.com/dwyl/english-words/blob/master/words_alpha.txt" rel="noopener ugc nofollow" target="_blank">英语词典</a>添加到一个名为<code class="du lf lg lh kx b">data.txt</code>的空文件中。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="232d" class="jd je hh kx b fi lb lc l ld le">path = 'data.txt'<br/>with io.open(path, encoding='utf-8') as f:<br/>    text = f.read().lower()</span></pre><p id="def4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">检查它的长度。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="a9aa" class="jd je hh kx b fi lb lc l ld le">print('length:', len(text))</span><span id="a5a7" class="jd je hh kx b fi li lc l ld le">Output:<br/>length: 1908035</span></pre><h2 id="81c1" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">数据处理</h2><p id="3873" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">列出数据中所有不同的字符。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="aeb8" class="jd je hh kx b fi lb lc l ld le">chars = sorted(list(set(text)))<br/>print('total chars:', len(chars))<br/>print(chars)</span><span id="14e3" class="jd je hh kx b fi li lc l ld le">Output:<br/>total chars: 28<br/>['\n', "'", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']</span></pre><p id="7693" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">替换或删除我们不想在名字中出现的字符。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="e226" class="jd je hh kx b fi lb lc l ld le">text = text.replace("'", "")</span></pre><p id="93d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为每个字符设置一个索引，并创建两个字典。这将有助于重建模型的输出。</p><ul class=""><li id="56ed" class="ke kf hh ig b ih ii il im ip lj it lk ix ll jb kj kk kl km bi translated">从字符到索引(我们需要它来进行输入训练)</li><li id="bbc3" class="ke kf hh ig b ih kn il ko ip kp it kq ix kr jb kj kk kl km bi translated">从索引到字符(我们需要它来重建模型的输出)</li></ul><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="c741" class="jd je hh kx b fi lb lc l ld le">char_indices = dict((c, i) for i, c in enumerate(chars))<br/>indices_char = dict((i, c) for i, c in enumerate(chars))<br/><br/>print(char_indices)</span><span id="df96" class="jd je hh kx b fi li lc l ld le">Output:<br/>{'\n': 0, "'": 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}</span></pre><p id="22b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将文本分成几行并删除空行。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="bceb" class="jd je hh kx b fi lb lc l ld le">lines = text.split('\n')<br/>lines = [line for line in lines if len(line)!=0]<br/>print("number of lines:", len(lines))</span><span id="ec20" class="jd je hh kx b fi li lc l ld le">Output:<br/>number of lines: 194433</span></pre><p id="3e0c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">获取最长一行的长度(在我的例子中是字典中最长的单词)。我们也可以检查最小尺寸。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="fed8" class="jd je hh kx b fi lb lc l ld le">maxlen = len(max(lines, key=len))<br/>minlen = len(min(lines, key=len))<br/><br/>print("line with longest length: "+ str(maxlen))<br/>print("line with shorter length: "+ str(minlen))</span><span id="6d7c" class="jd je hh kx b fi li lc l ld le">Output:<br/>line with longest length: 31<br/>line with shorter length: 1</span></pre><p id="7eed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将把文本切割成半冗余的<code class="du lf lg lh kx b">maxlen</code>字符序列，添加零的预填充。例如，如果最大长度是20，对于单词，<code class="du lf lg lh kx b">hello</code>，我们将创建下一个序列:<code class="du lf lg lh kx b">0000000000000000hello</code>。</p><p id="bb00" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可能会问为什么我要固定所有的字的大小。事实证明，如果我们将固定大小传递给TensorFlow中的LSTM，训练的速度会更快，因为它会创建固定形状的张量，在单个矩阵操作中实现许多操作。</p><p id="c72a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可能会问的另一个问题是:为什么我添加了一个前填充而不是后填充的零？由于LSTM基于数据序列进行学习，并试图找到与过去的关系，因此在输入中添加白噪声，即LSTM预填充，似乎是最佳选择。我发现一篇论文[1]表明，如果对数据应用后置填充而不是前置填充，性能会更差。在我的方法中，我认为不清楚什么是好的性能，因为我们想要生成遵循某种模式的随机字符串。</p><p id="de52" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从代码开始，我们要完成两个列表。一个有序列的子序列，另一个有对应的下一个字符。在<code class="du lf lg lh kx b">hello</code>示例中，我们将在每个列表中获得五个字符串:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="76b9" class="jd je hh kx b fi lb lc l ld le">List of subsequence       | Next char<br/>| --- | --- |<br/>'000000000000000hello'    |  '\n'<br/>'0000000000000000hell'    |  'o'<br/>'00000000000000000hel'    |  'l'<br/>'000000000000000000he'    |  'l'<br/>'0000000000000000000h'    |  'e'</span></pre></div><div class="ab cl lm ln go lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ha hb hc hd he"><pre class="kw kx ky kz aw la bi"><span id="b78d" class="jd je hh kx b fi lt lu lv lw lx lc l ld le">steps = 1<br/>sequences = []<br/>next_chars = []<br/><br/>for line in lines:<br/>    # pre-padding with zeros<br/>    s = (maxlen - len(line))*'0' + line<br/>    sequences.append(s)<br/>    next_chars.append('\n')<br/>    for it,j in enumerate(line):<br/>        if (it &gt;= len(line)-1):<br/>            continue<br/>        s = (maxlen - len(line[:-1-it]))*'0' + line[:-1-it]<br/>        sequences.append(s)<br/>        next_chars.append(line[-1-it])</span></pre><p id="afec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了方便起见，我称<code class="du lf lg lh kx b">sequences</code>为子序列列表；我将使用这个列表作为模型的输入，使用<code class="du lf lg lh kx b">next_char</code>列表作为相同的输出。</p><p id="7036" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">检查我们有多少个序列，并查看我们创建的列表的几个示例。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="37be" class="jd je hh kx b fi lb lc l ld le">print('total sequences:', len(sequences))<br/>print(sequences[66], next_chars[66])<br/>print(sequences[67], next_chars[67])<br/>print(sequences[68], next_chars[68])</span><span id="5716" class="jd je hh kx b fi li lc l ld le">Output:<br/>total sequences: 1713551<br/>00000000000000000000000aaronica l<br/>000000000000000000000000aaronic a<br/>0000000000000000000000000aaroni c</span></pre><p id="68f5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">序列的数量大于单词的数量。如果可以的话，我们可以继续。</p><h2 id="8357" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">…向量化…</h2><p id="7d67" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">现在我们必须向量化列表(T6和T7)。列表中的每个元素都将是一个<code class="du lf lg lh kx b">(maxlen)x(len(chars))</code>的2D矩阵。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="cfea" class="jd je hh kx b fi lb lc l ld le">x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)<br/>y = np.zeros((len(sequences), len(chars)), dtype=np.bool)<br/>for i, seq in enumerate(sequences):<br/>    for t, char in enumerate(seq):<br/>        if char != '0':<br/>            x[i, t, char_indices[char]] = 1<br/>    y[i, char_indices[next_chars[i]]] = 1</span></pre><p id="e5e9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">查看矢量化的示例。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="3516" class="jd je hh kx b fi lb lc l ld le">print(x[70][-5:]*1)</span><span id="3a08" class="jd je hh kx b fi li lc l ld le">Output:<br/>[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]<br/> [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]<br/> [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]<br/> [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]<br/> [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]]</span></pre><p id="d3b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意到序列70是<code class="du lf lg lh kx b">000000000000000000000000000aaro</code>，它的矢量化在每最后四行中必须有一个不同于0的值。我们还可以看到<code class="du lf lg lh kx b">next_char</code>是<code class="du lf lg lh kx b">n</code>，所以它的矢量化必须在第16个位置有一个1(索引15):</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="fcf2" class="jd je hh kx b fi lb lc l ld le">print(y[70]*1)</span><span id="ec64" class="jd je hh kx b fi li lc l ld le">Output:<br/>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]</span></pre><h2 id="6c99" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">建立模型:单一的LSTM</h2><p id="3038" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">在建立模型之前，我将定义一些函数:<code class="du lf lg lh kx b">sample</code>和<code class="du lf lg lh kx b">generate_new_names</code>，稍后我将解释这些函数，但我需要它们作为火车的回调，以查看模型在每个时期的表现。大多数时候，人们将数据集分为训练集和测试集。但在这里，很难验证模型的性能。我们可以通过查看在每个epoch上生成的一些没有前缀或后缀集的随机名称来了解它的性能。我们还可以测量其中有多少是真正被发明的，或者只是我们数据集中的一个名字。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="2553" class="jd je hh kx b fi lb lc l ld le">prefix = ""<br/>max_names = 10<br/><br/>def sample(preds):<br/>    """ function that sample an index from a probability array """<br/>    preds = np.asarray(preds).astype('float64')<br/>    preds = preds / np.sum(preds)<br/>    probas = np.random.multinomial(1, preds, 1)<br/>    return np.random.choice(range(len(chars)), p = probas.ravel())<br/><br/>def print_name_generated(name):<br/>    print(name, flush=True)<br/>def print_list_generated(lst):<br/>    print(lst, flush=True)<br/>    <br/>    <br/>def generate_new_names(*args):<br/>    print("----------Generatinig names----------")<br/><br/>    # Add pre-padding of zeros in the input.<br/>    sequence = ('{0:0&gt;' + str(maxlen) + '}').format(prefix).lower()<br/><br/>    # tmp variables<br/>    tmp_generated = prefix<br/>    list_outputs = list()<br/><br/>    while (len(list_outputs) &lt; max_names):<br/><br/>        # Vectorize the input of the model.<br/>        x_pred = np.zeros((1, maxlen, len(chars)))<br/>        for t, char in enumerate(sequence):<br/>            if char != '0':<br/>                x_pred[0, t, char_indices[char]] = 1<br/><br/>        # Predict the probabilities of the next char.<br/>        preds = model.predict(x_pred, verbose=0)[0]<br/><br/>        # Chose one based on the distribution obtained in the output of the model.<br/>        next_index = sample(preds)<br/>        # Get the corresponding char.<br/>        next_char = indices_char[next_index]<br/><br/>        # If the char is a new line character or the name start to be bigger than the longest word, <br/>        # try to add it to the list and reset temp variables.<br/>        if next_char == '\n' or len(tmp_generated) &gt; maxlen:<br/>            <br/>            # If the name generated is not in the list, append it and print it.<br/>            if tmp_generated not in list_outputs:<br/>                list_outputs.append(tmp_generated)<br/>                print_name_generated(tmp_generated)<br/>            # Reset tmp variables<br/>            sequence = ('{0:0&gt;' + str(maxlen) + '}').format(prefix).lower()<br/>            tmp_generated = prefix<br/>        else:<br/>    <br/>            # Append the char to the sequence that we're generating.<br/>            tmp_generated += next_char<br/>            # Add pre-padding of zeros to the sequence generated and continue.<br/>            sequence = ('{0:0&gt;' + str(maxlen) + '}').format(tmp_generated).lower()<br/>            <br/>    # Show the intersection of the words generated and your dataset. . <br/>    print("Set of words already in the dataset:")<br/>    print_list_generated(set(lines).intersection(list_outputs))<br/>    <br/>    # Show the rate of how many repeated words you've created.<br/>    total_repited = len(set(lines).intersection(list_outputs))<br/>    total = len(list_outputs)<br/>    print("Rate of total invented words: " + "{:.2f}".format((total-total_repited)/total))<br/>    print("-----------------End-----------------")<br/>    <br/># Function invoked at the end of each epoch. Prints generated names.<br/>callback = LambdaCallback(on_epoch_end=generate_new_names)</span></pre><p id="17c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我创建了一个LSTM模型，用softmax作为它的激活函数，因为我们想抽样概率。我使用<code class="du lf lg lh kx b">categorical_crossentropy</code>作为损失函数，因为它主要用于这类有分类输出的问题。我没有强有力的论据说明为什么我选择128和RMSpror作为批量大小和优化器；我在网上看了一些例子，把那些论点。最后，我使用的纪元数是2，因为我开始看到生成了许多以相同字符开头的单词。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="6bbe" class="jd je hh kx b fi lb lc l ld le"># build and train the model<br/>model = Sequential()<br/>model.add(LSTM(64, input_shape=(maxlen, len(chars))))<br/>model.add(Dense(len(chars), activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))<br/>history = model.fit(x, y, batch_size=128, epochs=2, verbose=2, callbacks=[callback])</span><span id="e346" class="jd je hh kx b fi li lc l ld le">Output:<br/>Epoch 1/2<br/>----------Generatinig names----------<br/>joysteer<br/>jutted<br/>intrancies<br/>ylockress<br/>jowrs<br/>juity<br/>jatesuperers<br/>jascarfalc<br/>hibamentarias<br/>sheffing<br/>Set of words already in the dataset:<br/>{'jutted'}<br/>Rate of total invented words: 0.90<br/>-----------------End-----------------<br/> - 676s - loss: 1.9652<br/>Epoch 2/2<br/>----------Generatinig names----------<br/>jecked<br/>illofi<br/>japolers<br/>jurmanist<br/>quoagall<br/>hombiorns<br/>walskins<br/>jiallites<br/>licking<br/>revect<br/>Set of words already in the dataset:<br/>{'licking'}<br/>Rate of total invented words: 0.90<br/>-----------------End-----------------<br/> - 719s - loss: 1.8855</span></pre><h2 id="d978" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">生成新名称功能的说明</h2><p id="35ea" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">最后，我们准备创造新的企业名称！我画了一个图来解释我的算法是做什么的，你可以走到函数前面，跟着它的代码和他们的注释。</p><figure class="ks kt ku kv fd ly er es paragraph-image"><div class="ab fe cl lz"><img src="../Images/d745bf61490f4f9d8b00f72c259b41f0.png" data-original-src="https://miro.medium.com/v2/format:webp/1*SV_VMt1gig6pK2phqzYqJg.png"/></div></figure><h2 id="5d56" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">给定前缀的名称生成</h2><p id="fcf3" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">运行以下代码获取新名称。不要忘记填写您想要的<code class="du lf lg lh kx b">prefix</code>以及您想要在<code class="du lf lg lh kx b">max_names</code>中生成的总名称。每次运行代码，都会得到不同的结果，因为<code class="du lf lg lh kx b">sample</code>函数会根据概率任意选择下一个字符。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="f414" class="jd je hh kx b fi lb lc l ld le"># Insert a prefix of the name you'd like to generate <br/># (it could be empty)</span><span id="a9cd" class="jd je hh kx b fi li lc l ld le">prefix = "deep"</span><span id="af28" class="jd je hh kx b fi li lc l ld le"># Insert how many names you'd like to generate.</span><span id="b4c8" class="jd je hh kx b fi li lc l ld le">max_names = 1000<br/><br/>generate_new_names()</span><span id="5644" class="jd je hh kx b fi li lc l ld le">Output:<br/>----------Generatinig names----------<br/>deeper<br/>deept<br/>deepitic<br/>deependtia<br/>deepro<br/>deepinations<br/>deephisraric<br/>deepa<br/>deeprotized<br/>deeperatis<br/>...</span><span id="9ef4" class="jd je hh kx b fi li lc l ld le">Set of words already in the dataset:<br/>{'deepens', 'deeply', 'deeper', 'deepest', 'deepen', 'deeps', 'deep'}<br/>Rate of total invented words: 0.99<br/>-----------------End-----------------</span></pre><p id="ad82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我喜欢<em class="jc"> deepro </em>，今天它的域名在互联网上是可用的！</p><h2 id="a8cb" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">给定后缀的名称生成</h2><p id="8988" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">一种方法是反转字典的每一行，将其矢量化，然后再次训练模型。打印输出时，再次翻转。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="10f5" class="jd je hh kx b fi lb lc l ld le"># Reverse the text<br/>lines = [line[::-1] for line in lines]<br/><br/># Print the reversed names generated splited<br/>def print_name_generated(name):<br/>    print(name[::-1], flush=True)<br/>def print_list_generated(lst):<br/>    print([l[::-1] for l in lst], flush=True)</span></pre><p id="80e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下一段代码只是之前工作的副本，但是没有注释。我什么都没改变。让我们执行它。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="fa88" class="jd je hh kx b fi lb lc l ld le">steps = 1<br/>sequences = []<br/>next_chars = []<br/><br/>for line in lines:<br/>    # pre-padding with zeros<br/>    s = (maxlen - len(line))*'0' + line<br/>    sequences.append(s)<br/>    next_chars.append('\n')<br/>    for it,j in enumerate(line):<br/>        if (it &gt;= len(line)-1):<br/>            continue<br/>        s = (maxlen - len(line[:-1-it]))*'0' + line[:-1-it]<br/>        sequences.append(s)<br/>        next_chars.append(line[-1-it])<br/>        <br/>x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)<br/>y = np.zeros((len(sequences), len(chars)), dtype=np.bool)<br/>for i, seq in enumerate(sequences):<br/>    for t, char in enumerate(seq):<br/>        if char != '0':<br/>            x[i, t, char_indices[char]] = 1<br/>    y[i, char_indices[next_chars[i]]] = 1<br/><br/>prefix = ""<br/>max_names = 10<br/>    <br/>model = Sequential()<br/>model.add(LSTM(64, input_shape=(maxlen, len(chars))))<br/>model.add(Dense(len(chars), activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))<br/>history = model.fit(x, y, batch_size=128, epochs=2, verbose=2, callbacks=[callback])</span><span id="d684" class="jd je hh kx b fi li lc l ld le">Output:<br/>Epoch 1/2<br/>----------Generatinig names----------<br/>shedaw<br/>baj<br/>kej<br/>triperch<br/>soilitiast<br/>donlewdray<br/>emperadow<br/>kaj<br/>daw<br/>stask<br/>Set of words already in the dataset:<br/>['daw']<br/>Rate of total invented words: 0.90<br/>-----------------End-----------------<br/> - 716s - loss: 2.0443<br/>Epoch 2/2<br/>----------Generatinig names----------<br/>vergial<br/>uneroof<br/>flaj<br/>troomindax<br/>gonagux<br/>churbez<br/>berbshatz<br/>gornwook<br/>carinax<br/>genglaj<br/>Set of words already in the dataset:<br/>[]<br/>Rate of total invented words: 1.00<br/>-----------------End-----------------<br/> - 601s - loss: 1.9618</span></pre><p id="e5d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，在生成的名称中插入一个后缀，并执行下一段代码。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="46bd" class="jd je hh kx b fi lb lc l ld le"># Insert a suffix of your business name (could be empty):<br/>suffix = "it"</span><span id="f307" class="jd je hh kx b fi li lc l ld le"># Insert how many names you'd like to generate:<br/>max_names = 10<br/><br/># This reverse the prefix <br/>prefix = suffix[::-1]<br/>generate_new_names()<br/># later, it will be reversed again in the print function</span><span id="3bb0" class="jd je hh kx b fi li lc l ld le">Output:<br/>----------Generatinig names----------<br/>gallit<br/>lammit<br/>tosit<br/>precabimit<br/>unclelit<br/>terlouemit<br/>unfit<br/>imbagmit<br/>frocknanghit<br/>bramit<br/>Set of words already in the dataset:<br/>['unfit']<br/>Rate of total invented words: 0.90<br/>-----------------End-----------------</span></pre><h2 id="be3d" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">结论</h2><p id="19b1" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">我们已经使用TensorFlow基于数据集成功创建了新的企业名称模式。他们中的许多人在互联网上有自己的域名，而且也很经济。我们还修改了代码，用后缀代替前缀来创建名字，得到了令人满意的结果。</p><p id="5b33" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">查看结果，该模型学习如何组合单词或在旧术语中添加拼写错误，但保持单词的简单发音。此外，它有时添加，例如，副词功能，以非形容词性的条款，如" ly "在一个名词的结尾。</p><p id="f9dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，根据我们问题的性质，我们可以通过使用回调来成功地定义时期的数量，并在训练期间查看每个时期的模型输出，在我们的模型中定义了两个时期的情况下获得了良好的结果。</p><h2 id="a219" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">参考</h2><p id="1c77" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">[1]Dwarampudi，m .和Subba Reddy，N. V .，“填充对LSTMs和CNN的影响”，<em class="jc"> arXiv电子版</em>，2019年。</p></div><div class="ab cl lm ln go lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ha hb hc hd he"><p id="7e95" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我希望它是有帮助的，并且你和我一样喜欢它。如有疑问或建议，请评论。</p></div></div>    
</body>
</html>