<html>
<head>
<title>Cyclical Learning Rates</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">周期性学习率</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/cyclical-learning-rates-a922a60e8c04?source=collection_archive---------10-----------------------#2021-04-05">https://medium.com/analytics-vidhya/cyclical-learning-rates-a922a60e8c04?source=collection_archive---------10-----------------------#2021-04-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9bdd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以更少的迭代次数实现机器学习模型的更高精度。</p><p id="e5e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学习速率影响训练时间和模型效率。学习率取决于损失函数景观，损失函数景观取决于模型架构和数据集。为了更快地收敛模型，需要一个最优的学习速率。</p><h2 id="d2a6" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">最佳学习率是多少？</h2><p id="3317" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">导致损失急剧下降的学习率称为最佳学习率。损失减少得越快，学习率就越理想。</p><h2 id="0077" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">如何找到最优的学习率？</h2><p id="07fa" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">如果损失在慢慢减少，我们可以提高学习速度。如果损失在波动并增加，则学习率过高，因此需要降低它。损失函数图可以检查学习率是否足够好。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/bd5aa19c2e5cd60d63fba864fc56853e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IN4Km-G7l8SBUd2ny1DhJA.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">图片鸣谢:<a class="ae kt" href="https://cs231n.github.io/neural-networks-3/#annealing-the-learning-rate" rel="noopener ugc nofollow" target="_blank">https://cs 231n . github . io/neural-networks-3/# annealing-the-learning-rate</a></figcaption></figure><p id="c41a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从图中，我们可以看到，非常高的学习率会使损失函数发散，而高的学习率会卡在次优点。低学习率导致线性收敛，但需要更多的迭代，而高学习率导致更快的收敛。因此，找到一个好的学习率可以使我们的模型更快地收敛。</p><h2 id="a45f" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">设定学习率的不同方法</h2><ol class=""><li id="c144" class="ku kv hi ih b ii jy im jz iq kw iu kx iy ky jc kz la lb lc bi translated">恒定学习率——在这种情况下，我们将学习率设置为某个值，该值在整个培训过程中保持不变。</li><li id="ae35" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">自适应学习率-在这种情况下，模型使用各种优化算法来改变学习率，同时训练模型。训练以较大的学习率开始，学习率逐渐降低，并且在最终模型收敛时变得太小。要了解更多这方面的内容，你可以在这里阅读<a class="ae kt" href="https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6" rel="noopener" target="_blank"/>。</li><li id="f2c1" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">循环学习率——在这种情况下，学习率在学习率范围内波动。它从最小学习速率向最大学习速率增加，然后降低到最小学习速率。它被证明是更有效的，并导致一些模型更快的收敛。</li></ol><h1 id="4043" class="li je hi bd jf lj lk ll jj lm ln lo jn lp lq lr jq ls lt lu jt lv lw lx jw ly bi translated">循环学习率</h1><p id="70ac" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">类似于自适应学习率，学习率变化是循环的，总是返回到学习率的初始值。非常高的学习率会导致模型波动更大或偏离最小值，而较低的学习率会导致模型收敛非常慢或收敛到局部最小值。循环学习率(CLR)允许保持高和低的学习率，使得模型不会随着从局部最小值的跳跃而发散。</p><p id="e41c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在CLR中，学习率在基本学习率和最大学习率之间波动。学习率的振荡可以基于各种函数——三角函数(线性)、韦尔奇窗(抛物线)或汉恩窗(正弦)。三角窗口是改变学习率的一种更简单的方式。</p><h2 id="f935" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">CLR为什么管用？</h2><p id="f85e" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">根据<a class="ae kt" href="https://arxiv.org/pdf/1406.2572.pdf" rel="noopener ugc nofollow" target="_blank"> Dauphin 2014，</a>局部极小不是优化大型深度神经网络的主要障碍，<a class="ae kt" href="https://en.wikipedia.org/wiki/Saddle_point#:~:text=In%20mathematics%2C%20a%20saddle%20point,local%20extremum%20of%20the%20function." rel="noopener ugc nofollow" target="_blank">鞍点</a>是优化路径的关键点。</p><p id="9a68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">鞍点处的梯度太小，会减慢学习过程。然而，增加学习速率允许快速遍历鞍点平台。</p><h2 id="0e41" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">CLR的变化</h2><p id="7132" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">因为我们需要改变学习率值，所以变化是基于变化的逻辑。它可以是线性、指数或任何其他函数。</p><h2 id="7c5e" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">三角形窗户</h2><p id="604b" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">在这种情况下，学习率改变逻辑是线性的，即我们将以某个常数从最小学习率到最大学习率增加学习率，并且将以相同的常数从最大学习率到最小学习率降低学习率。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lz"><img src="../Images/84ea274545ba54e5089eee58318bfa7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*wBv9WvQUKCeu38I2Qad42g.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">三角窗|图片来源:【https://arxiv.org/pdf/1506.01186.pdf T4】</figcaption></figure><h2 id="ed71" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">抛物线窗口</h2><p id="5500" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">学习率使用抛物线函数增加或减少。</p><h2 id="f87a" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">正弦窗</h2><p id="da18" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">学习率使用正弦函数增加或减少。</p><p id="c2ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将会看到一个三角形的窗户。</p><h1 id="97f0" class="li je hi bd jf lj lk ll jj lm ln lo jn lp lq lr jq ls lt lu jt lv lw lx jw ly bi translated">LR更新逻辑</h1><p id="ff54" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated"><strong class="ih hj">选择。LR </strong> —学习率的下限</p><p id="3d0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> maxLR </strong> —最大学习率边界</p><p id="e7c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">迭代</strong> —在一个小批量上训练的网络</p><p id="0ba5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> epochCounter </strong> —完成的迭代次数</p><p id="4bc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">周期长度</strong> —学习率返回初始值之前的迭代次数。</p><p id="1731" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步长</strong> —从opt移动学习率所需的迭代次数。Lr至maxLR(周期长度的一半)。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ma"><img src="../Images/01e7b36e63c2e5747815034fb94e695d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*V6FhYP95VC9bGxmh6kQv0A.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">LR计算逻辑|图像信用:<a class="ae kt" href="https://arxiv.org/pdf/1506.01186.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.01186.pdf</a></figcaption></figure><p id="dc83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本地循环定义了当前正在进行的循环。该周期将总是恒定的，并且直到历元计数器是周期长度的倍数，该周期才改变，因此使用math.floor。</p><h2 id="4d26" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">估计周期长度的正确值</h2><p id="88c5" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">根据<a class="ae kt" href="https://arxiv.org/pdf/1506.01186.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>，步长应该等于一个历元中迭代次数的2-10倍。我们可以使用数据集大小和批量大小来计算一个时期中出现的迭代。如果数据集包含50，000个数据条目，并且批量大小为100，则一个时期中的迭代次数将为500 (50，000/100)。</p><h2 id="e3da" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">估计最小和最大学习率边界</h2><p id="3038" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">本文提到了确定学习率边界值的简单方法。这种方法被称为“LR范围测试”。在这种方法中，我们通过增加多个时期的学习率来运行模型。步长值保持等于您希望模型运行的所有时段中的迭代次数，以便学习率线性增加。绘制模型的图形映射精度和学习率值。当精度增加时，以及当精度下降或变得不均匀时，记录学习率值。</p><h2 id="924d" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">参考</h2><ol class=""><li id="7e93" class="ku kv hi ih b ii jy im jz iq kw iu kx iy ky jc kz la lb lc bi translated">关于CLR的论文—【https://arxiv.org/pdf/1506.01186.pdf T2】</li><li id="9299" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">【https://github.com/bckenstler/CLR T4】</li><li id="f17f" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><a class="ae kt" href="https://www.jeremyjordan.me/nn-learning-rate/" rel="noopener ugc nofollow" target="_blank">https://www.jeremyjordan.me/nn-learning-rate/</a></li></ol></div></div>    
</body>
</html>