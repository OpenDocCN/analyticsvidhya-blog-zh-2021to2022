<html>
<head>
<title>Predicting (at least trying) asset returns with Machine Learning techniques using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python通过机器学习技术预测(至少尝试)资产回报</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/predicting-at-least-trying-asset-returns-with-machine-learning-techniques-using-python-1b0946100caf?source=collection_archive---------2-----------------------#2021-04-23">https://medium.com/analytics-vidhya/predicting-at-least-trying-asset-returns-with-machine-learning-techniques-using-python-1b0946100caf?source=collection_archive---------2-----------------------#2021-04-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a10d" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">试图通过强制ML模型来超越纯粹的机会，以及为什么我们没有这样做</h2></div><p id="44d6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">统计学可以用来预测任何有预测因子的事物。然而，有效市场假说(EMH)指出，这不是资产回报的情况，因为市场价格将反映现有的信息，除了少数例外(法玛和马尔基尔，1970)。此外，鉴于只有少数选定的积极基金可以持续跑赢选定的基准指数(马尔基尔，2005)，有足够的理由相信，未来价格的可预测性几乎是不可能的，因为所有已知的和可用于预测价格和击败市场的东西都已经打折了。因此，我们回到了肤浅的预测，基于运气的胜利，并为此付费。</p><p id="e920" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，还有另一种相反的方式来理解有效市场。这是一个事实，假设所有相关信息都已经被贴现，如果我们想要尝试任何类型的预测，价格就是我们所需要的。例如，网上有几篇论文表明，应用一组机器学习技术可以输出超过80%准确度的方向预测(Patel，Shah，Thakkar，&amp; Koetcha，2014)。这种精确度与其他策略(如凯利标准)相结合，可以产生相关的结果。</p><p id="749d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我将展示三种不同的机器学习技术的代码，即:逻辑回归和贝叶斯分类器、决策树和支持向量机。要预测的资产有:JP摩根(JPM)、美国银行(BAC)和花旗银行(C)。可以通过拟合/预测2008年对这三家银行产生巨大影响的危机来进行进一步的研究，看看这些模型是否会发生什么酷的事情。至于我们的数据，我们将使用Quandl的随时间变化的股票价格数据库，我们将生成一个带有建议的制造指标的表格，即:收盘价、回报、回报符号、动量、10天简单移动平均线和10天指数移动平均线。</p><p id="790f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">设置我们的数据</strong></p><p id="8eb0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们开始干预我们的机器学习模型之前，我将创建三个数据框，每个资产一个。首先，我将从Quandl库获取从2014年1月1日到2018年1月1日的收盘价(鉴于我在Quandl中的免费帐户，不能更接近当前日期)。然后，我将通过计算D+0和D-1的价格比率的对数来为我们的对数回报创建一个列。然后，我将为我们的滞后回报创建5列，小心地删除无效值，并将它们放入D+0线。动量柱是通过减去D+0和D-1的价格创建的，而简单和移动平均线是为过去10天创建的。最终数据框被截断，因此数据将符合公式所需的形状。代码如下所示。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="0145" class="kc kd hi jy b fi ke kf l kg kh">import numpy as np<br/>import pandas as pd<br/>from numpy import inf<br/>from pandas_datareader import data as web</span><span id="873f" class="kc kd hi jy b fi ki kf l kg kh">#Get close data</span><span id="8595" class="kc kd hi jy b fi ki kf l kg kh">dataJPM = pd.DataFrame(web.DataReader('AXP', data_source='quandl', start="2014-01-01", end="2018-01-01", access_key="Hvta3RUqWUHaHFkzfyEA")['Close'])<br/>dataBAC = pd.DataFrame(web.DataReader('BAC', data_source='quandl', start="2014-01-01", end="2018-01-01", access_key="Hvta3RUqWUHaHFkzfyEA")['Close'])<br/>dataC = pd.DataFrame(web.DataReader('C', data_source='quandl', start="2014-01-01", end="2018-01-01", access_key="Hvta3RUqWUHaHFkzfyEA")['Close'])</span><span id="3811" class="kc kd hi jy b fi ki kf l kg kh">#Create RETURN feature</span><span id="a3b3" class="kc kd hi jy b fi ki kf l kg kh">dataJPM["Returns"] = np.log(dataJPM.Close/dataJPM.Close.shift(-1))<br/>dataBAC["Returns"] = np.log(dataBAC.Close/dataBAC.Close.shift(-1))<br/>dataC["Returns"] = np.log(dataC.Close/dataC.Close.shift(-1))</span><span id="ae41" class="kc kd hi jy b fi ki kf l kg kh">#Create 5 LAGGED DAILY RETURNS</span><span id="ef1e" class="kc kd hi jy b fi ki kf l kg kh">lags = 5<br/>banks = [dataJPM, dataBAC, dataC]<br/>col = []<br/>for bank in banks:<br/>    for lag in range(1,lags+1):<br/>        col = 'ret_%d' % lag<br/>        bank[col] = bank['Returns'].shift(-lag)</span><span id="b415" class="kc kd hi jy b fi ki kf l kg kh">#Remove NaN</span><span id="0890" class="kc kd hi jy b fi ki kf l kg kh">dataJPM.dropna(inplace=True)<br/>dataBAC.dropna(inplace=True)<br/>dataC.dropna(inplace=True)</span><span id="6dca" class="kc kd hi jy b fi ki kf l kg kh">#Create MOMENTUM feature</span><span id="602a" class="kc kd hi jy b fi ki kf l kg kh">dataJPM["Momentum"] = dataJPM.Close - dataJPM.Close.shift(-1)<br/>dataBAC["Momentum"] = dataBAC.Close - dataBAC.Close.shift(-1)<br/>dataC["Momentum"] = dataC.Close - dataC.Close.shift(-1)</span><span id="3369" class="kc kd hi jy b fi ki kf l kg kh">#Create SIMPLE MOVING AVERAGE feature</span><span id="deba" class="kc kd hi jy b fi ki kf l kg kh">dataJPM["SMA10"] = (dataJPM.Close + dataJPM.Close.shift(-1) + dataJPM.Close.shift(-2) + dataJPM.Close.shift(-3) + dataJPM.Close.shift(-4) + dataJPM.Close.shift(-5) + dataJPM.Close.shift(-6) + dataJPM.Close.shift(-7) + dataJPM.Close.shift(-8) + dataJPM.Close.shift(-9))/10<br/>dataBAC["SMA10"] = (dataBAC.Close + dataBAC.Close.shift(-1) + dataBAC.Close.shift(-2) + dataBAC.Close.shift(-3) + dataBAC.Close.shift(-4) + dataBAC.Close.shift(5) + dataBAC.Close.shift(-6) + dataBAC.Close.shift(-7) + dataBAC.Close.shift(-8) + dataBAC.Close.shift(-9))/10<br/>dataC["SMA10"] = (dataC.Close + dataC.Close.shift(-1) + dataC.Close.shift(-2) + dataC.Close.shift(-3) + dataC.Close.shift(-4) + dataC.Close.shift(-5) + dataC.Close.shift(-6) + dataC.Close.shift(-7) + dataC.Close.shift(-8) + dataC.Close.shift(-9))/10</span><span id="b71a" class="kc kd hi jy b fi ki kf l kg kh">#Create EXPONENTIAL MOVING AVERAGE feature</span><span id="23be" class="kc kd hi jy b fi ki kf l kg kh">dataJPM["EMA10"] = dataJPM["SMA10"]<br/>dataBAC["EMA10"] = dataBAC["SMA10"]<br/>dataC["EMA10"] = dataC["SMA10"]</span><span id="34e6" class="kc kd hi jy b fi ki kf l kg kh">alpha = 0.5<br/>dataJPM["EMA10"] = (dataJPM.Close-dataJPM.EMA10.shift(-1))*(alpha) + dataJPM.EMA10.shift(-1)<br/>dataBAC["EMA10"] = (dataBAC.Close-dataBAC.EMA10.shift(-1))*(alpha) + dataBAC.EMA10.shift(-1)<br/>dataC["EMA10"] = (dataC.Close-dataC.EMA10.shift(-1))*(alpha) + dataC.EMA10.shift(-1)</span><span id="f97f" class="kc kd hi jy b fi ki kf l kg kh">dataJPMfinal = dataJPM.truncate(after="2014-01-27")<br/>dataBACfinal = dataBAC.truncate(after="2014-01-27")<br/>dataCfinal = dataC.truncate(after="2014-01-27")</span></pre><p id="2437" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了给我们的模型编码，我们将利用Scikit Learn的库。此外，我们的数据将在需要时使用Scikit的标准定标器进行标准化，以避免分配不成比例的权重，这会严重影响我们模型的准确性。接下来，对于所有模型，我们将使用Scikit的KFold将我们的数据分成训练集和测试集，4/5用于训练，1/5用于测试。这种拆分将使我们能够更好地评估我们的模型在实时情况下会如何发展。</p><p id="c758" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">关于PNL回测的说明</strong></p><p id="5985" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了测试我们的策略，我们将为每个模型和股票绘制三条曲线。第一条曲线将代表任何给定股票的完全多头被动头寸，这可以作为基准:我们得到比这条曲线更高的结果，我们的主动管理正在产生alpha。我们将绘制的第二条线是对给定策略的完全投资头寸，即全押:我们将100%的资本放在我们模型中给定的策略上，并纳入100%的结果，无论它们是上涨还是下跌。这可以简单地通过累加所有回报的总和乘以我们的预测符号来实现:如果预测值为1，回报为正，我们就获利；如果predictor是-1，回报是负的，我们也有收益；不同的结果增加了负回报。</p><p id="b599" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的第三条曲线将考虑半凯利。凯利标准是赌注的大小，以百分比或总资本的形式给出，你应该在一系列的赌注中下注以最大化预期回报。这是通过假设我们的模型有一定的优势，即赢的概率p高于50%来计算的；然后，假设第二天收益的分布是正态的，通过泰勒展开式及其导数等于零，我们得到了最优分配。在我们的例子中，考虑1:1 (b=1)的偶数赌注，我们有:</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es kj"><img src="../Images/7ad1948dd0305f46fe946c2a02ba245e.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*n_WlebNrW3HBvEf4CyeIMA.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">凯利准则</figcaption></figure><p id="bfc9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通常，凯利会解决策略中资本分配的最佳百分比。然而，这种策略往往给出非常高的标准差，这在主动管理型基金中可能会吓跑投资者。因此，经理们可能会选择使用半凯利，因为我们的泰勒展开是抛物线(不是真的，但对我们的解释来说足够接近)，如果我们只分配我们标准的一半，我们将sigma减少一半，但回报率仅减少25%，这是一个很好的卖点，可以安慰我们的经理，而不会吓到投资者。因此，在对我们的策略进行回溯测试时，我们将考虑以下几点。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es kr"><img src="../Images/3b74f024eb8d5b7f2e014edfb6b1d7be.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*EQOckCUhEY8FLJe6B5mSuw.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">半凯利:心脏衰弱者</figcaption></figure><p id="1dfa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">贝叶斯分类器</strong></p><p id="4f65" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将通过朴素贝叶斯分类器进行预测回报的第一次尝试。这个分类器从一个简单的假设开始，即数据中的特征之间没有相关性，并使用贝叶斯定理进行分类。y是up (1)和down (-1)的二进制状态，x1，…，xn是我们的特征向量，我们得到如下。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es ks"><img src="../Images/3b6cd92e9d9feb2bc397d3309f72e391.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*Gl5KT_MXcum1C8lII30zFg.png"/></div></figure><p id="5800" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设不考虑变量之间的依赖性，分母向量的每个特征的概率保持不变，并且我们想要二进制结果，我们的等式变成如下。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es kt"><img src="../Images/aecbe4b4ae1d047b604f7587b237ca1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*r9uiqFrmjEnZAqF1buLXsw.png"/></div></figure><p id="f648" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该等式是给定y的特征概率的所有概率与y的概率的乘积。一系列概率的乘积可能导致下溢，这促使我们在实践中使用对数概率之和的指数。由此产生的另一个问题是，如果任何𝑃(𝑥𝑖 |𝑦碰巧为零，我们的方程将总是返回零，因此我们的模型将通过使用平滑变量来平滑我们的概率，这将使任何零概率稍微超过零。</p><p id="f059" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">不同的贝叶斯分类器方法对特征的概率分布做出不同的假设。在我们的例子中，我们将使用高斯变量，假设正态分布的概率。代码如下所示。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="7d3f" class="kc kd hi jy b fi ke kf l kg kh">from sklearn.naive_bayes import GaussianNB<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.model_selection import KFold<br/>gnb = GaussianNB()<br/>cols = ['Momentum', 'SMA10', 'EMA10', 'ret_1', 'ret_2', 'ret_3', 'ret_4', 'ret_5']</span><span id="563c" class="kc kd hi jy b fi ki kf l kg kh">scaler = StandardScaler()<br/>scaler.fit(dataBACfinal)</span><span id="1179" class="kc kd hi jy b fi ki kf l kg kh">dataBACfinalScaled = pd.DataFrame(scaler.transform(dataBACfinal))<br/>dataBACfinalScaled.columns = ['Close', 'Returns', 'ret_1', 'ret_2', 'ret_3', 'ret_4', 'ret_5', 'Momentum', 'SMA10', 'EMA10']</span><span id="ece4" class="kc kd hi jy b fi ki kf l kg kh">y = dataBACfinal["Returns"].values<br/>X = dataBACfinalScaled[cols].shift(-1).fillna(0).values<br/>kf = KFold(n_splits=5, shuffle=True)<br/>kf.get_n_splits(X)</span><span id="c891" class="kc kd hi jy b fi ki kf l kg kh">for train_index, test_index in kf.split(X):<br/>    X_train, X_test = X[train_index], X[test_index]<br/>    y_train, y_test = y[train_index], y[test_index]</span><span id="0285" class="kc kd hi jy b fi ki kf l kg kh">gnb.fit(pd.DataFrame(X_train), np.sign(y_train))</span><span id="2db3" class="kc kd hi jy b fi ki kf l kg kh">dfyGNB = pd.DataFrame(y_test)<br/>dfyGNB.columns = ["Returns"]<br/>dfyGNB['NB_pred'] = gnb.predict(pd.DataFrame(X_test))<br/>dfyGNB['log_p1'] = pd.DataFrame(gnb.predict_proba(pd.DataFrame(X_test)))[0]<br/>dfyGNB['log_p2'] = pd.DataFrame(gnb.predict_proba(pd.DataFrame(X_test)))[1]<br/>dfyGNB['log_p3'] = pd.DataFrame(gnb.predict_proba(pd.DataFrame(X_test)))[2]<br/>dfyGNB['p'] = dfyGNB[['log_p1', 'log_p2', 'log_p3']].max(axis=1)</span></pre><p id="3126" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> PNL回溯测试</strong></p><p id="4a27" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，为了测试我们的模型的准确性，我们绘制了以下内容</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="96e0" class="kc kd hi jy b fi ke kf l kg kh">dfyGNB['NB_returns'] = dfyGNB['Returns'] * dfyGNB['NB_pred']<br/>dfyGNB["Kelly_NB_returns"] = dfyGNB['Returns'] * dfyGNB['NB_pred'] * (((2*(dfyGNB['p']))-1)/2)<br/>dfyGNB[['Returns', 'NB_returns', 'Kelly_NB_returns']].cumsum().apply(np.exp).plot(figsize=(10, 6))</span></pre><p id="fefe" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就产生了下面这个不起眼的图形。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es ku"><img src="../Images/9bb8a2b072ed5a8575b0b6a133d623e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*OqCB0CCBNPUG5z-BTJJurw.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">NB退货— JPM</figcaption></figure><p id="f68c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过计算每一次我们添加一个正结果的次数，并除以播放次数，我们可以得到我们的模型在这次测试中的准确性。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="e9e5" class="kc kd hi jy b fi ke kf l kg kh">sum(1 for x in dfyGNB['NB_returns'] if x &gt; 0)/len(dfyGNB['NB_returns'])</span></pre><p id="21ed" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">48.48%的准确率，我们的模型基本上是在抛硬币。类似的结果适用于下面的BAC和C。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es kv"><img src="../Images/33d3a056c6e262faec97b28e4b705f18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*liq6hobE2dSeFBBhY1ShDw.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">NB退货— BAC</figcaption></figure><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es kw"><img src="../Images/b2552621a1f96cf569c1ad4ae36659bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*zw5MOGWUpdfZ-9WgImgK-w.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">NB返回— C</figcaption></figure><p id="7079" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">逻辑回归</strong></p><p id="6877" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">逻辑回归不同于线性回归，因为我们用下面的函数来拟合模型。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es kx"><img src="../Images/164d962f6c8a02f318083aa0ef3ded3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*5k_FKK7J7J_jPXHdWmmGsQ.png"/></div></figure><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es ky"><img src="../Images/890743f52630f41a06d9cd81f767d485.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*IB95_8s1aAUb_W67dcR5HQ.png"/></div></figure><p id="ccf7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这使得值的上部为0或1(在曲线的中间被切割)。在我们的模型中，我们还将应用KFold方法将数据分为训练和测试，调整我们的变量，并应用正则化。正则化是应用于拟合模型的惩罚，它具有通过减少由我们的回归给出的大系数来平衡过度拟合的效果。我们的模型使用L2范数来计算这些惩罚，这是平方和。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="037c" class="kc kd hi jy b fi ke kf l kg kh">from sklearn import linear_model<br/>from sklearn.model_selection import KFold<br/>from sklearn.preprocessing import StandardScaler</span><span id="9c82" class="kc kd hi jy b fi ki kf l kg kh">lm = linear_model.LogisticRegression(C = 1e5, dual=True, solver = 'liblinear', max_iter = 1000, multi_class = 'ovr')<br/>cols = ['Momentum', 'SMA10', 'EMA10', 'ret_1', 'ret_2', 'ret_3', 'ret_4', 'ret_5']</span><span id="8bf2" class="kc kd hi jy b fi ki kf l kg kh">scaler = StandardScaler()<br/>scaler.fit(dataBACfinal)</span><span id="f478" class="kc kd hi jy b fi ki kf l kg kh">dataBACfinalScaled = pd.DataFrame(scaler.transform(dataBACfinal))<br/>dataBACfinalScaled.columns = ['Close', 'Returns', 'ret_1', 'ret_2', 'ret_3', 'ret_4', 'ret_5', 'Momentum', 'SMA10', 'EMA10']</span><span id="f09c" class="kc kd hi jy b fi ki kf l kg kh">y = dataBACfinal["Returns"].values<br/>X = dataBACfinalScaled[cols].shift(-1).fillna(0).values<br/>kf = KFold(n_splits=5, shuffle=True)<br/>kf.get_n_splits(X)</span><span id="b330" class="kc kd hi jy b fi ki kf l kg kh">for train_index, test_index in kf.split(X):<br/>    X_train, X_test = X[train_index], X[test_index]<br/>    y_train, y_test = y[train_index], y[test_index]</span><span id="b028" class="kc kd hi jy b fi ki kf l kg kh">lm.fit(pd.DataFrame(X_train), np.sign(y_train))<br/>dfy = pd.DataFrame(y_test)<br/>dfy.columns = ["Returns"]<br/>dfy['SMA10'] = dataBACfinalScaled['SMA10']</span><span id="de79" class="kc kd hi jy b fi ki kf l kg kh">dfy['log_pred'] = lm.predict(pd.DataFrame(X_test))<br/>dfy['log_p1'] = pd.DataFrame(lm.predict_proba(pd.DataFrame(X_test)))[0]<br/>dfy['log_p2'] = pd.DataFrame(lm.predict_proba(pd.DataFrame(X_test)))[1]<br/>dfy['log_p3'] = pd.DataFrame(lm.predict_proba(pd.DataFrame(X_test)))[2]<br/>dfy['p'] = dfy[['log_p1', 'log_p2', 'log_p3']].max(axis=1)</span><span id="445c" class="kc kd hi jy b fi ki kf l kg kh">dfy['log_returns'] = dfy['Returns'] * dfy['log_pred']<br/>dfy["Kelly_log_returns"] = dfy['Returns'] * dfy ['log_pred'] * (((2*(dfy['p']))-1)/2)<br/>dfy[['Returns', 'log_returns', 'Kelly_log_returns']].cumsum().apply(np.exp).plot(figsize=(10, 6))</span><span id="084e" class="kc kd hi jy b fi ki kf l kg kh">sum(1 for x in dfy['log_returns'] if x &gt; 0)/len(dfy['log_returns'])</span></pre><p id="e7fa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> PNL回测</strong></p><p id="4a1f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这些结果如下图所示。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es kz"><img src="../Images/9fead1e227f95be715e13f000e822ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*Rk2flxeKG6goWr41VclXuw.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">逻辑回归— JPM</figcaption></figure><p id="ceb5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该模型显示了51.01%的准确性。然而，结果经常在48%和52%的准确度之间变化，因此它仍然是赌博，并且没有产生一致的alpha。BAC和C的类似结果如下。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es la"><img src="../Images/4c51c4a13b39cb65cf3e9f639f959387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*yphqHTWhKVsrh0CzKC0tQg.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">逻辑回归BAC</figcaption></figure><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es kv"><img src="../Images/ff59dc1cef4cd7a3c7a04e84f0cee4f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*b7rLRw4pKIJF4HMTFjlw9g.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">逻辑回归— C</figcaption></figure><p id="b816" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">决策树</strong></p><p id="64f3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">决策树是容易理解模型。他们的分类过程很简单，并且不像我们的其他模型那样受到黑箱效应的影响。为了创建一个决策树，我们将使用熵的定义，它来自信息论的概念，如下所示。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es lb"><img src="../Images/136c27eea2b9db797e5e46ff06f0bfa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*7ST8uJonaPgcNj-kJpFqpQ.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">熵公式</figcaption></figure><p id="8b3f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该等式将用于将数据分割成碎片较少的子集。我们的模型将使用的算法如下:</p><ol class=""><li id="14bc" class="lc ld hi iz b ja jb jd je jg le jk lf jo lg js lh li lj lk bi translated">如果所有数据都有相同的标签，则创建一个叶子并停止；</li><li id="087d" class="lc ld hi iz b ja ll jd lm jg ln jk lo jo lp js lh li lj lk bi translated">如果没有更多可用的特性分色，则创建一个具有最高可能值的叶子并停止；</li><li id="88b5" class="lc ld hi iz b ja ll jd lm jg ln jk lo jo lp js lh li lj lk bi translated">否则，为每个特征测试一个分区；</li><li id="d621" class="lc ld hi iz b ja ll jd lm jg ln jk lo jo lp js lh li lj lk bi translated">选择熵最小的一个；</li><li id="6431" class="lc ld hi iz b ja ll jd lm jg ln jk lo jo lp js lh li lj lk bi translated">基于上一次决策添加一个节点；</li><li id="03e8" class="lc ld hi iz b ja ll jd lm jg ln jk lo jo lp js lh li lj lk bi translated">对每个子集重复上述步骤。</li></ol><p id="dd68" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果数据是连续的，则设置拆分位置的标准基于最小化均方误差和平均绝对误差。使用Scikit学习库，我们编写了以下模型。同样，我们借用KFold方法来分割数据，这是一个很好的做法，因为这种方法倾向于严重过度拟合测试数据。这一次，没有理由缩放我们的数据，因为我们没有创建可能导致不平衡系数的单一回归。在这个模型中缩放只会阻碍我们对它的理解。</p><p id="f6a6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，我们并不试图进行回归，我们只是想预测向上或向下的运动。因此，我们创建了另一个带有回报符号的列，1表示负回报，0表示正好为零(发生了几次)，1表示正回报。这个专栏将用于拟合和测试我们的模型。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="42d1" class="kc kd hi jy b fi ke kf l kg kh">from sklearn import tree<br/>from sklearn.model_selection import KFold<br/>from subprocess import call<br/>dataBACfinal['Up_down'] = (np.sign(dataBACfinal['Returns']))<br/>cols = ['Momentum', 'SMA10', 'EMA10', 'ret_1', 'ret_2', 'ret_3', 'ret_4', 'ret_5']</span><span id="b3a4" class="kc kd hi jy b fi ki kf l kg kh">kf = KFold(n_splits=5, shuffle=True)</span><span id="d527" class="kc kd hi jy b fi ki kf l kg kh">X = dataBACfinal[cols].fillna(0).values<br/>y = dataBACfinal["Up_down"].shift(-1).fillna(0).values</span><span id="fcae" class="kc kd hi jy b fi ki kf l kg kh">for train_index, test_index in kf.split(X):<br/>    X_train, X_test = X[train_index], X[test_index]<br/>    y_train, y_test = y[train_index], y[test_index]</span><span id="26a9" class="kc kd hi jy b fi ki kf l kg kh">clf = tree.DecisionTreeClassifier()</span><span id="2d09" class="kc kd hi jy b fi ki kf l kg kh">clf.fit(X_train,y_train)</span><span id="ab39" class="kc kd hi jy b fi ki kf l kg kh">CTreeResult = pd.DataFrame(dataBACfinal['Returns'].values[test_index])<br/>CTreeResult.columns = ['Returns']<br/>CTreeResult['tree_prediction'] = pd.DataFrame(clf.predict(X_test))<br/>CTreeResult['log_p1'] = pd.DataFrame(clf.predict_proba(pd.DataFrame(X_test)))[0]<br/>CTreeResult['log_p2'] = pd.DataFrame(clf.predict_proba(pd.DataFrame(X_test)))[1]<br/>CTreeResult['log_p3'] = pd.DataFrame(clf.predict_proba(pd.DataFrame(X_test)))[2]<br/>CTreeResult['p'] = CTreeResult[['log_p1', 'log_p2', 'log_p3']].max(axis=1)</span><span id="2254" class="kc kd hi jy b fi ki kf l kg kh">CTreeResult['tree_returns'] = CTreeResult['Returns'] * CTreeResult['tree_prediction']<br/>CTreeResult["Kelly_tree_returns"] = CTreeResult['Returns'] * CTreeResult['tree_prediction'] * (((2*(CTreeResult['p']))-1)/2)</span><span id="b67f" class="kc kd hi jy b fi ki kf l kg kh">CTreeResult[['Returns', 'tree_returns', 'Kelly_tree_returns']].cumsum().apply(np.exp).plot(figsize=(10, 6))</span><span id="b771" class="kc kd hi jy b fi ki kf l kg kh">sum(1 for x in CTreeResult['tree_returns'] if x &gt; 0)/len(CTreeResult['tree_returns'])</span></pre><p id="2111" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> PNL回溯测试</strong></p><p id="a866" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这将产生以下结果。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es lq"><img src="../Images/c4c8a20ec679c5fb97f7799c3548a116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*PPa8--0b_jMZ-ZUW-ELmgQ.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">决策树— JPM</figcaption></figure><p id="e0e5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该模型的准确率为46.50%。</p><p id="8010" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过使用tree.export_graphviz(clf)命令，可以使用Graphviz可视化这个模型。这些树如下所示。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es lr"><img src="../Images/ef7b2a7c5615dab4b20fd7c0f511a18e.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*kzqNapKZeL4uDPCowApK7Q.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">JPM决策树可视化</figcaption></figure><p id="c7cc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BAC和C的结果如下。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es ls"><img src="../Images/c20a996f20d1cd11166bc4770ca57c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*2XguMJIzY2CTykMZdpMnpg.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">决策树— BAC</figcaption></figure><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es lt"><img src="../Images/2157acfa1c945337c04fc1065fa5a75a.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*GSDguYcbwT3rhnRUf7lE6g.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">BAC决策树可视化</figcaption></figure><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es lu"><img src="../Images/7bd6ff246a4f46862f2974ed27789342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*e-GZiX9NzhE5xt19KDllaw.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">决策树— C</figcaption></figure><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es lv"><img src="../Images/d68ab4c98788acca624eaf9795af81d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*66j0pDxNrsIi2nMuqwMc_Q.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">c决策树可视化</figcaption></figure><p id="4bef" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">支持向量机</strong></p><p id="ba2b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">支持向量机尝试使用向量来线性分离数据，这些向量在它尝试分类的数据集之间以最高的可能间隔绘制。鉴于它通常构建一个超平面来绘制这些向量，可视化可能会变得复杂。假设我们的矢量位置基于距离，我们将在拟合之前缩放矩阵，并使用KFold来分割数据。我们将使用Scikit学习库来计算我们的模型，如下所示。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="fe87" class="kc kd hi jy b fi ke kf l kg kh">from sklearn import svm<br/>from sklearn.model_selection import KFold<br/>from sklearn.preprocessing import StandardScaler</span><span id="690f" class="kc kd hi jy b fi ki kf l kg kh">dataBACfinal['Up_down'] = (np.sign(dataBACfinal['Returns']))<br/>cols = ['Momentum', 'SMA10', 'EMA10', 'ret_1', 'ret_2', 'ret_3', 'ret_4', 'ret_5']</span><span id="d294" class="kc kd hi jy b fi ki kf l kg kh">scaler = StandardScaler()<br/>scaler.fit(dataBACfinal)</span><span id="69b2" class="kc kd hi jy b fi ki kf l kg kh">dataBACfinalScaled = pd.DataFrame(scaler.transform(dataBACfinal))<br/>dataBACfinalScaled.columns = ['Close', 'Returns', 'ret_1', 'ret_2', 'ret_3', 'ret_4', 'ret_5', 'Momentum', 'SMA10', 'EMA10', 'Up_down']</span><span id="b559" class="kc kd hi jy b fi ki kf l kg kh">kf = KFold(n_splits=5, shuffle=True)</span><span id="4c56" class="kc kd hi jy b fi ki kf l kg kh">X = dataBACfinalScaled[cols].fillna(0).values<br/>y = dataBACfinal["Up_down"].shift(-1).fillna(0).values</span><span id="59bf" class="kc kd hi jy b fi ki kf l kg kh">for train_index, test_index in kf.split(X):<br/>    X_train, X_test = X[train_index], X[test_index]<br/>    y_train, y_test = y[train_index], y[test_index]</span><span id="f84c" class="kc kd hi jy b fi ki kf l kg kh">svmm = svm.SVC(gamma=0.001, probability = True)<br/>svmm.fit(X_train, y_train)</span><span id="209d" class="kc kd hi jy b fi ki kf l kg kh">CSVMResult = pd.DataFrame(dataBACfinal['Returns'].values[test_index])<br/>CSVMResult.columns = ['Returns']<br/>CSVMResult['log_p1'] = pd.DataFrame(svmm.predict_proba(pd.DataFrame(X_test)))[0]<br/>CSVMResult['log_p2'] = pd.DataFrame(svmm.predict_proba(pd.DataFrame(X_test)))[1]<br/>CSVMResult['log_p3'] = pd.DataFrame(svmm.predict_proba(pd.DataFrame(X_test)))[2]<br/>CSVMResult['p'] = CSVMResult[['log_p1', 'log_p2', 'log_p3']].max(axis=1)<br/>CSVMResult['SVM_prediction'] = pd.DataFrame(svmm.predict(X_test))<br/>CSVMResult['SVM_returns'] = CSVMResult['Returns'] * CSVMResult['SVM_prediction']<br/>CSVMResult["Kelly_SVM_returns"] = CSVMResult['Returns'] * CSVMResult['SVM_prediction'] * (((2*(CSVMResult['p']))-1)/2)<br/>CSVMResult[['Returns', 'SVM_returns', 'Kelly_SVM_returns']].cumsum().apply(np.exp).plot(figsize=(10, 6))</span></pre><p id="d426" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> PNL回测</strong></p><p id="b15c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这为我们提供了以下结果，准确率为47.97%</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es lw"><img src="../Images/0ae4eab33fadd5b59ecbcd2f980e2622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*TlyGaRyo58V3o_tLKSiS6Q.png"/></div></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">SVM — JPM</figcaption></figure><p id="acc4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们试图在2D图形中可视化我们的矢量时，大多数方法得到的是一个没有等高线的分布。这是因为我们使用了很多特性，用图形表示它们变得非常复杂。例如，我们可以通过选择SMA和EMA作为我们表示的特征来绘制我们的图形，如下所示。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es mb"><img src="../Images/6b01c9ec3e0344db52bf1a143bbbe8d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*U8ASWaZw8kzzOk480u0sDQ.png"/></div></figure><p id="1424" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的数据在那里，但轮廓超出了界限。BAC和C的结果相似，如下所示。</p><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es mc"><img src="../Images/72207696e156c4b0a514c12ddf6107e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*rj_ShlSDEGhuio7yAAOKcQ.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">SVM — BAC</figcaption></figure><figure class="jt ju jv jw fd kk er es paragraph-image"><div class="er es md"><img src="../Images/71c053b8edf6a37553569830e4a588a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*l6HXOOhFVOpwvDoOuwjLyA.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">SVM——C</figcaption></figure><p id="3ece" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">总结</strong></p><p id="2d6f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们有时可能会被诱惑去测试很酷的新ML模型来预测股票市场的未来回报，但是在这种情况下，我们才能真正感受到价格行为的随机性，以及这对有梦想的统计学家来说是多么不可逾越。用开箱即用的模型进行简单的预测很可能会让你一无所获(就像我在这里做的一样)，这太复杂了，大多数常客的方法甚至会在你有一个好的演示文稿向交易台的同事展示并要求开发预算之前崩溃。</p><p id="02e3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">自然，肯定会有赢家(上面的一些模型在回溯测试中显示了超过10%的alpha，但请记住，<strong class="iz hj">回溯测试不是科学[重复10次] </strong>)，但赢家自然会在随机环境中出现，幸存者偏差会在很大程度上说服即使是幸运的人，他们也只是那么好。</p><p id="9989" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">最后，鉴于金融市场固有的随机性，对任何表现出过度确定性并声称拥有不合理实力的人都要持怀疑态度，即使那个人就是你自己。</strong></p></div></div>    
</body>
</html>