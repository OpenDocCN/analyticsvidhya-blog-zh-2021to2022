<html>
<head>
<title>Evolution of sequence-to-sequence learning- a journey from “Feed Forward Neural Network” to “The Transformer”</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">序列到序列学习的进化——从“前馈神经网络”到“变压器”的旅程</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/evolution-of-sequence-to-sequence-learning-a-journey-from-feed-forward-neural-network-to-the-2f2d9344a5d3?source=collection_archive---------22-----------------------#2021-02-21">https://medium.com/analytics-vidhya/evolution-of-sequence-to-sequence-learning-a-journey-from-feed-forward-neural-network-to-the-2f2d9344a5d3?source=collection_archive---------22-----------------------#2021-02-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/2f1127a815fd830d94cf0c8eee67d5e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3t72jTaRQ_jp8gWT3hqK0A.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated"><a class="ae hu" href="https://miro.medium.com/max/1694/1*aj7Rs6UNxbTXuJmrTcj4Ew.jpeg" rel="noopener">https://miro . medium . com/max/1694/1 * aj7rs 6 unxbtxujmrtcj 4 ew . JPEG</a></figcaption></figure><div class=""/><p id="5d62" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这篇博客的主要目的是展示不同序列对序列(Seq2Seq)模型的局限性，以及如何在这一领域不断改进。</p><h2 id="f98a" class="js jt hx bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">Seq2Seq学习是什么？</h2><p id="cce6" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">Seq2Seq模型将单词序列(一个或多个句子)作为输入，并生成单词输出序列。利用Seq2Seq学习可以开发许多应用程序。一些用例是机器翻译、文档摘要、语音识别、阅读理解、实体提取、图像字幕等等。</p><h2 id="d6dc" class="js jt hx bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">应用:</h2><p id="f181" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">1-机器翻译</p><figure class="kt ku kv kw fd hj er es paragraph-image"><div class="er es ks"><img src="../Images/691d709e217db487120b347a9f19915c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*2r-rxYNmL1bQPduQ9JTACA.png"/></div></figure><p id="00b6" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2-文档摘要</p><figure class="kt ku kv kw fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es kx"><img src="../Images/0e14f7b215737566a63ea177606b816b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7L8WAPVPEfSQiTyBN6qu3A.png"/></div></div></figure><p id="8592" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">3-语音识别</p><figure class="kt ku kv kw fd hj er es paragraph-image"><div class="er es ky"><img src="../Images/41f30888637258a9335917e3ae4c33d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*vMduoQzfrjkC5II9UkBGgw.png"/></div></figure><h1 id="545f" class="kz jt hx bd ju la lb lc jy ld le lf kc lg lh li kf lj lk ll ki lm ln lo kl lp bi translated">前馈神经网络:</h1><p id="2cbf" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">最简单的神经网络架构- FFNN，其中信息仅单向流动，即从输入层到输出层。</p><figure class="kt ku kv kw fd hj er es paragraph-image"><div class="er es lq"><img src="../Images/9aa037ebff9b6e2e87703364ec1d7784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*DiUIKMblFHcYuzGttxXZyg.png"/></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">图片来源:<a class="ae hu" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank"> Udacity NLP纳米学位课程</a></figcaption></figure><h2 id="8271" class="js jt hx bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">FFNN的局限性:</h2><p id="cf23" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">在80年代中期，人们认识到FFNN模型不能捕捉时间相关性，即随时间变化的相关性。几乎所有的真实用例要么是顺序的，要么是时变的。换句话说，给定时间戳的输出不仅取决于当前输入，还取决于过去的输入。FFNN无法处理这些属性，因为缺少捕获过去输入状态的内部内存元素。然而，我们将在后面看到，FFNN如何与其他更复杂的算法相结合，以产生各种NLP任务中的艺术状态。</p><h1 id="0bb5" class="kz jt hx bd ju la lb lc jy ld le lf kc lg lh li kf lj lk ll ki lm ln lo kl lp bi translated">时间延迟神经网络(TDNN):</h1><p id="d60e" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">第一次尝试是在神经网络中引入记忆元素。网络能够查看当前输入以及过去的输入。</p><figure class="kt ku kv kw fd hj er es paragraph-image"><div class="er es lr"><img src="../Images/9cc203198cd32ee1436b7bb35c4acc08.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*llQfa80no3Ct-cQPHsYM7g.png"/></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">图片来源:<a class="ae hu" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank"> Udacity NLP纳米学位课程</a></figcaption></figure><h2 id="03d1" class="js jt hx bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">TDNN的局限性:</h2><p id="18a2" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">虽然TDNNs能够捕获时间相关性，但它仅限于所选的时间窗口。这意味着，网络无法看到过去的输入超过选定的时间戳。</p><h1 id="4fc8" class="kz jt hx bd ju la lb lc jy ld le lf kc lg lh li kf lj lk ll ki lm ln lo kl lp bi translated">递归神经网络(RNN):</h1><p id="1260" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">接下来是rnn，它们能够通过向网络引入反馈或记忆元素来捕捉时间依赖性。隐藏层的输出作为下一个训练步骤的输入，允许网络查看过去的输入。</p><figure class="kt ku kv kw fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ls"><img src="../Images/d836f2c3fb6d084227df999d7bad55d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0i2l09XbC3bt9YohRjujXA.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">图片鸣谢:<a class="ae hu" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank"> Udacity NLP纳米学位课程</a></figcaption></figure><h2 id="f4d6" class="js jt hx bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">rnn的限制:</h2><p id="979d" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">90年代初，人们发现所有的rnn都存在消失梯度问题。如果网络看得更深(时间戳大于8或10)，它就失去了捕捉过去输入的本质的能力。换句话说，RNNs无法学习“长期依赖”。</p><h1 id="07e5" class="kz jt hx bd ju la lb lc jy ld le lf kc lg lh li kf lj lk ll ki lm ln lo kl lp bi translated">LSTM:</h1><p id="27c4" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">LSTM是专门为避免长期依赖问题而设计的。它使用门来固定状态变量，并决定在将来适当的时候是否重新引入。因此，可以表示任意时间间隔，且可以捕捉时间相关性。</p><figure class="kt ku kv kw fd hj er es paragraph-image"><div class="er es lt"><img src="../Images/69cd852da1851a5a64a676f6e9679fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*9k0RGXQxV6hd8hBzB9s_hw.png"/></div></figure><p id="6e9f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">LSTM的另一个变种叫做GRU(门控循环单元),它被证明在捕获长期依赖方面非常有效。</p><p id="a3eb" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">序列到序列模型有两个组件-编码器和解码器。编码器接收所有输入，对其进行处理，并将捕获的信息(上下文向量)发送给解码器，以生成输出序列。编码器和解码器只不过是LSTM\GRU的rnn</p><figure class="kt ku kv kw fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lu"><img src="../Images/72496583facefe1c84152315afd59bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WAQfZLW549YjiNuwh5lxyA.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">图片鸣谢:<a class="ae hu" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank"> Udacity NLP纳米学位课程</a></figcaption></figure><h2 id="e757" class="js jt hx bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">限制:</h2><p id="bd77" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">如果你注意到，编码器只把最后一个隐藏状态传递给解码器，而不管输入大小的长度。因此，最后一个隐藏状态(也称为上下文向量)的大小是Seq2Seq模型中的一个限制因素。甚至，如果我们将上下文向量的大小固定得非常大，模型倾向于在短句上过度拟合。注意力解决了这个问题。</p><h1 id="4305" class="kz jt hx bd ju la lb lc jy ld le lf kc lg lh li kf lj lk ll ki lm ln lo kl lp bi translated">注意机制</h1><p id="553f" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">与以前的Seq2Seq模型相比，注意力不仅仅传递给最后一个隐藏状态，而是传递给解码器所有的隐藏状态。这提供了上下文向量大小的灵活性，更长的序列可以具有更长的上下文向量，其捕获输入序列的更好信息。现在，轮到解码器来识别输入序列的哪一部分需要更多的关注。解码器通过在输入隐藏状态的帮助下计算上下文向量来做到这一点。</p><figure class="kt ku kv kw fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lv"><img src="../Images/89c58b3d302c8d724ba928e5a9d15c21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LBah6kFCCGDl3LambeeD9A.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">图片鸣谢:<a class="ae hu" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">https://jalammar . github . io/visualizing-neural-machine-translation-mechanics-of-seq 2 seq-models-with-attention/</a></figcaption></figure><h2 id="6f30" class="js jt hx bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">注意机制的限制:</h2><p id="b350" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">在注意机制中使用RNNs增加了模型的复杂性，并且不允许模型对模型训练进行并行处理。题为“注意力是你所需要的全部”的论文提出了一种新的方法，其中编码器和解码器仅使用注意力，而不使用RNNs。他们称这个模型为变形金刚。</p><figure class="kt ku kv kw fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lw"><img src="../Images/73e6337c251dd8fc0c600bab55667603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VAKCcesV8pG9MyjNaeTN0A.png"/></div></div></figure><h1 id="1dad" class="kz jt hx bd ju la lb lc jy ld le lf kc lg lh li kf lj lk ll ki lm ln lo kl lp bi translated">变形金刚</h1><p id="bbf6" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">变压器是编码器和解码器的堆叠，本文提出了6层。编码器和解码器都使用了一个叫做自我关注的概念和一个前馈神经网络。事实证明，模型在质量上更胜一筹，训练时间也相对更短。</p><figure class="kt ku kv kw fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lx"><img src="../Images/af642169030771e0a3fbba32c555dbf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*70ZK3hGKpPNj-ZA56yqqiA.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">图片鸣谢:<a class="ae hu" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank"> Udacity NLP纳米学位课程</a>。</figcaption></figure><p id="0a7d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">基于Transformer的模型被证明是非常强大的，因为它们已经在许多NLP任务中实现了SOTA。迁移学习允许实践者在NLP的下游任务中使用预先训练的模型。研究人员和实践者开始把这想象成NLP和NLU任务中的图像网络时刻。</p><p id="4d69" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个领域仍然广泛开放，需要更多的研究，在未来我们将看到更多的进步。</p><h1 id="7ebf" class="kz jt hx bd ju la lb lc jy ld le lf kc lg lh li kf lj lk ll ki lm ln lo kl lp bi translated">参考资料:</h1><div class="hg hh ez fb hi ly"><a href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hy fi z dy md ea eb me ed ef hw bi translated">自然语言处理在线课程</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">新的！纳米学位课程预计时间为3个月，每周10-15小时，2021年2月24日前注册进入教室…</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">www.udacity.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ho ly"/></div></div></a></div><div class="hg hh ez fb hi ly"><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hy fi z dy md ea eb me ed ef hw bi translated">了解LSTM网络</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">2015年8月27日发布人类不是每秒钟都从零开始思考。当你读这篇文章时，你…</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">colah.github.io</p></div></div></div></a></div><div class="hg hh ez fb hi ly"><a href="https://jalammar.github.io/" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hy fi z dy md ea eb me ed ef hw bi translated">杰伊·阿拉玛</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">通过可视化模型层之间的隐藏状态，我们可以得到一些关于模型“思维过程”的线索…</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">jalammar.github.io</p></div></div><div class="mh l"><div class="mn l mj mk ml mh mm ho ly"/></div></div></a></div><h2 id="04aa" class="js jt hx bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">我的联系方式:</h2><p id="f4a2" class="pw-post-body-paragraph iu iv hx iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr ha bi translated">www.linkedin.com/in/dhirendra-srivastava-894b4b195<a class="ae hu" href="http://www.linkedin.com/in/dhirendra-srivastava-894b4b195" rel="noopener ugc nofollow" target="_blank"/></p><p id="7df0" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">【https://twitter.com/iamDhirendraS T4】</p></div></div>    
</body>
</html>