<html>
<head>
<title>K-means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k均值聚类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-means-clustering-3ce2456db7f1?source=collection_archive---------16-----------------------#2021-01-17">https://medium.com/analytics-vidhya/k-means-clustering-3ce2456db7f1?source=collection_archive---------16-----------------------#2021-01-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e9d0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从头开始解释和实现</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/0075dd92a8019a4c5b0e91defce71c2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/0*mQCGBdYhzZ8YMZPv.png"/></div></div></figure><p id="0df1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">K-means聚类是一种无监督的机器学习技术。它的目标是将n个观测值划分为k个T2簇。</p><blockquote class="jp jq jr"><p id="c4ec" class="ie if jo ig b ih ii ij ik il im in io js iq ir is jt iu iv iw ju iy iz ja jb ha bi translated"><strong class="ig hi">K均值</strong>算法是一种迭代算法，它试图将数据集划分为<em class="hh"> K </em>个不同的非重叠子组(聚类),其中每个数据点仅属于<strong class="ig hi">一个组</strong></p></blockquote><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jv"><img src="../Images/68e8b2771b55a98cab1478fdb9446e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/1*Nx6IyGfRAV1ly6uDGnVCxQ.gif"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">来源:维基百科</figcaption></figure><blockquote class="jp jq jr"><p id="5132" class="ie if jo ig b ih ii ij ik il im in io js iq ir is jt iu iv iw ju iy iz ja jb ha bi translated"><strong class="ig hi">目标→在k个簇中划分数据，其中k是我们已知的。</strong></p></blockquote><h1 id="7a94" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">kmeans算法流程</h1><ol class=""><li id="f9e9" class="ky kz hh ig b ih la il lb ip lc it ld ix le jb lf lg lh li bi translated">指定集群的数量<em class="jo"> K </em></li><li id="eab0" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">随机初始化K个中心(质心)。</li><li id="f421" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">对数据集进行迭代，直到达到最大迭代次数，或者在聚类的质心发生变化之前。</li><li id="b4c7" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">计算每个点与每个聚类中心的欧几里德距离。将每个点分配给一个距离最小的簇。这个步骤被称为<strong class="ig hi">电子步骤(期望步骤)。{ </strong> <em class="jo"> K-Means是期望算法</em> <strong class="ig hi"> } </strong>的特例</li><li id="9167" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">计算每个聚类的平均值，然后用该聚类的质心更新它。这一步被称为<strong class="ig hi"> M-step(最大化步骤)</strong></li><li id="ce78" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">重复这些，直到我们达到最大迭代或我们正确的质心。</li></ol><blockquote class="jp jq jr"><p id="d006" class="ie if jo ig b ih ii ij ik il im in io js iq ir is jt iu iv iw ju iy iz ja jb ha bi translated"><strong class="ig hi"> K-means可以表述为E-M(期望最大化)算法。e步填充点，而M步更新中心。</strong></p></blockquote><h2 id="4e48" class="lo kb hh bd kc lp lq lr kg ls lt lu kk ip lv lw ko it lx ly ks ix lz ma kw mb bi translated">K-means背后的数学</h2><p id="1ae5" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip mc ir is it md iv iw ix me iz ja jb ha bi translated">正如我们在其他机器学习算法中看到的那样，我们有一个损失函数。所以我们这里又把它称为目标函数或惯性。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mf"><img src="../Images/b217573ae063b1f8d43b013a0fca9918.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/0*uqLhZv-oQj_zGLJo.png"/></div></figure><p id="97cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在这里引入wik是因为我们要为一个点不属于其他聚类的聚类增加距离。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mg"><img src="../Images/3ba2bda6b5857117411a856a1b44507e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/0*kXkrYmGPZBfw8uGs.png"/></div></figure><p id="a5cb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们首先相对于wik最小化J，保持μk固定。然后我们最小化J . w . r . t .μk并处理wik固定。</p><p id="51bf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们首先区分J w.r.t. wik并更新集群的分配时，这是<strong class="ig hi">电子步骤</strong>。然后我们对J w.r.t. μk进行微分并更新质心，这被称为<strong class="ig hi"> M步</strong>。</p><p id="c153" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">电子步骤是</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mh"><img src="../Images/bffe95754c3160cd6d702bad11c0b50c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9gkJE13MlrwmmdrL.png"/></div></div></figure><p id="ecc1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">M-step是</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mi"><img src="../Images/182cfd7acf1b01d229a19d6e04487db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ts3Uz94NafkYhyqX.png"/></div></div></figure><p id="f524" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这与我们将平均值作为分母表示1和0的总和是第k个簇中的点数是一样的。而分子是该簇中那些点的值的总和。</p><p id="7e75" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种最小化误差并找到wik和μk的方法叫做坐标下降法，它有这样一个图</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mj"><img src="../Images/fae0e708a74e85885036528d04fb521c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*W-U9RnvFs25Kn8ng.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">来源:维基百科</figcaption></figure><p id="ce96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">K均值代码</p><pre class="jd je jf jg fd mk ml mm mn aw mo bi"><span id="9fad" class="lo kb hh ml b fi mp mq l mr ms"># Distance between two points Euclidian Distance<br/>def distance(a1,a2):<br/>    return np.sqrt(np.sum((a1-a2)**2))</span><span id="8aaa" class="lo kb hh ml b fi mt mq l mr ms">class <strong class="ml hi">KMeans</strong>():<br/>    def __init__(self,k=5,max_iter=100):<br/>        self.k= k<br/>        self.max_iter=max_iter<br/>        self.clusters = {}<br/>        self.label = []<br/>        <br/>    def <strong class="ml hi">initialization</strong>(self,X):<br/>        for i in range(self.k):<br/>            center=np.zeros((2,))<br/>            cx = np.random.uniform(low=38.8250, high=39.0000)<br/>            cy = np.random.uniform(low=-76.9, high=-77.15)<br/>            center[0] = cx<br/>            center[1] = cy<br/>            #center = 10*(2*np.random.random((X.shape[1],))-1)<br/>            points = []<br/>            cluster = {<br/>                'center':center,<br/>                'points':points,<br/>                'id'    :i<br/>            }<br/>            self.clusters[i]=cluster<br/>        self.label = np.zeros((X.shape[0],1))<br/>    <br/>    def <strong class="ml hi">assignPointTOClusters</strong>(self,X):<br/>        for i in range(X.shape[0]):<br/>            dist = []<br/>            curr_x = X[i]<br/>        <br/>            for ki in range(self.k):<br/>                d = distance(curr_x,self.clusters[ki]['center'])<br/>                dist.append(d)<br/>            <br/>            current_cluster = np.argmin(dist)<br/>            self.clusters[current_cluster]['points'].append(curr_x)<br/>            self.label[i]=(self.clusters[current_cluster]['id'])<br/>            <br/>    def check(self,old_c,new_c):<br/>        distances = [distance(old_c[i], new_c[i]) for i in range(self.k)]<br/>        return sum(distances) == 0<br/>        <br/>    def <strong class="ml hi">updateClusters</strong>(self):<br/>        for kx in range(self.k):<br/>            pts = np.array(self.clusters[kx]['points'])<br/>            <br/>            if pts.shape[0]&gt;0: # If cluster has some nonzero points<br/>                new_u = pts.mean(axis=0)<br/>                self.clusters[kx]['center'] = new_u<br/>                # Clear the list<br/>                self.clusters[kx]['points'] = []<br/>    <br/>    def <strong class="ml hi">plotClusters</strong>(self):<br/>        for kx in range(self.k):<br/>            print(len(self.clusters[kx]['points']))<br/>            pts = np.array(self.clusters[kx]['points'])<br/>            # plot points , cluster center<br/>            try:<br/>                plt.scatter(pts[:,0],pts[:,1])<br/>            except:<br/>                pass<br/>            uk = self.clusters[kx]['center']<br/>            plt.scatter(uk[0],uk[1],color='black',marker="*")<br/>        plt.show()<br/>            <br/>    def <strong class="ml hi">fit</strong>(self,X):<br/>        print(self.k)<br/>        self.initialization(X)<br/>        for i in range(self.max_iter):<br/>            print("i is ",i)<br/>            self.assignPointTOClusters(X)<br/>            self.plotClusters()<br/>            old_c = [self.clusters[i]['center'] for i in range(self.k)]<br/>            self.updateClusters()<br/>            new_c = [self.clusters[i]['center'] for i in range(self.k)]<br/>            if self.check(old_c,new_c):<br/>                break</span></pre><h1 id="80d4" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">K-Means++只是一个介绍</h1><ul class=""><li id="60c4" class="ky kz hh ig b ih la il lb ip lc it ld ix le jb mu lg lh li bi translated">K-Means对初始化非常敏感，如果初始化不好，我们的算法就不能得到期望的聚类数。</li><li id="6ed5" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb mu lg lh li bi translated">为了克服这个问题，我们使用称为K-Means++的技术(在论文<a class="ae mv" href="https://arxiv.org/abs/1202.1585" rel="noopener ugc nofollow" target="_blank">中描述了K-Means类型算法的鲁棒种子选择</a>)，该技术选择初始中心，使得它们统计上接近最终中心。</li></ul></div></div>    
</body>
</html>