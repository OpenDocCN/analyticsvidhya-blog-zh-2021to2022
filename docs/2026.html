<html>
<head>
<title>Building a sentiment analysis system with Naïve Bayes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用朴素贝叶斯构建情感分析系统</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-a-sentiment-analysis-system-with-na%C3%AFve-bayes-d35b723a6bdc?source=collection_archive---------9-----------------------#2021-04-01">https://medium.com/analytics-vidhya/building-a-sentiment-analysis-system-with-na%C3%AFve-bayes-d35b723a6bdc?source=collection_archive---------9-----------------------#2021-04-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/2d2ccf1d00a6372a735d6376e5da8ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0stw6zt4hWep_iAa.jpg"/></div></div></figure><p id="4d2b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这项研究中，我们建立了一个情感分析系统。我们的系统基于朴素贝叶斯分类器。首先，我们应用了两个文本预处理步骤。(1)去掉标点符号和停用词,( 2)提取词干。之后，我们实现了朴素贝叶斯算法，无需第三方库。我们通过自己编写的F1-score函数来评估我们的模型的性能。在最后一节中，我们给出了使用和不使用文本预处理技术的模型的结果。根据我们的结果，使用这两种技术可以提高F1分数。</p><h1 id="86b9" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">1-文本预处理</h1><p id="4bf0" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">为了利用朴素贝叶斯模型获得良好的情感分析性能，我们使用了一些文本预处理技术。在这项研究中，我们首先读取IMDB电影评论数据集，并将其转换为熊猫数据帧。然后我们删除了不使用的列。在我们删除“索引”和“文件”列后，我们有3列引用样本的“类型”、“审查”和“标签”。数据集的一些示例如下。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es kq"><img src="../Images/c78ce188e81d7b4c3ca819590810180a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*fXMZqMeCfUlGgj9t94qQeA.png"/></div></figure><p id="0ace" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在开始文本预处理之前，我们删除了标签为“unsup”的行。我们将数据分为训练集和测试集，并删除了它们各自的“类型”列。我们使用了两种文本预处理技术，第一种是删除标点符号和停用词，第二种是对句子中的单词进行词干处理。</p><p id="2ba6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们使用“nltk”库来删除标点符号和停用词。“WordPunctTokenizer”用于去掉标点符号。在我们对句子进行标记后，我们使用‘isal num()’函数将大写字母转换为小写字母，并且只接受字母数字单词。下面是去掉标点符号的例子。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/52cf87cbd378e27015bc3be3fce9751d.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*3Q9QU9bFj3SYJLhHIPT5uA.png"/></div></figure><p id="4d46" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">之后，我们下载了英语停用词，并将其导入。我们检查了所有标记化的单词，看它们是否是停用词。如果它们不是停用词，我们就把它们放到预处理过的句子中。没有停用词的句子举例如下。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kw"><img src="../Images/dc8c56246fb06532a592d8213b1c3137.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*-BMzvejv716Hc3wd3zf7Xg.png"/></div></div></figure><p id="7f09" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最近我们用“PorterStemmer”来寻找词干。带茎的样本在下面。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es kx"><img src="../Images/f924b5bf65985b46c6ee89356fc1c59a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*JcfWHt4Iae-TajnwlOm1-Q.png"/></div></figure><p id="765c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后，我们使用“数据和标签()”函数分离数据集的标签列。该函数返回“训练数据、训练标签、测试数据和测试标签”。在这个函数中，我们将正面评价标为1，负面评价标为0。</p><h1 id="6880" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">2-朴素贝叶斯实现及性能评估</h1><p id="ccfa" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">在实现步骤中，我们首先计算每个标签的评论数量。然后，我们通过将每一类划分为总评论数来计算每一类的日志先验。之后，我们在名为“单词计数”的字典中计算每个单词在每个类别中的频率，该字典保存每个单词在每个类别中的出现次数。</p><p id="a491" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在预测部分，我们首先取一个句子并应用预处理步骤。然后，我们对句子进行标记，并根据包含的单词计算肯定或否定的概率。</p><p id="ea0a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下面的朴素贝叶斯公式给出了根据它的文字成为一个积极评论的概率。这里p(pos)是正面评论的对数先验，p(wi|pos)是在正面评论中找到单词wi的概率。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es ky"><img src="../Images/3e4cd384ebe16f5e061d553b701da301.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*oKRC1GZHw59-Twzw4Z1Pww.png"/></div></figure><p id="7214" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">同样，我们可以像下面这样计算负的概率。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es kz"><img src="../Images/3fb45dbf616ea73ca47646e21c60f218.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*02MRgDBOe3twCP5IdpfSxw.png"/></div></figure><p id="3c23" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里两个概率的分母相同，在寻找最大概率时不会有影响。因此我们可以将概率改写如下:</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es la"><img src="../Images/b03ec9c163168199979da8da237a1aee.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*_utX2SRrB_IYgbhvTSMD-Q.png"/></div></figure><p id="9725" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这里，我们将许多可能性相互相乘。这可能会产生非常小的数字，并将计算结果四舍五入为零。为了避免这一点，我们采取双方的日志。因此对数概率如下所示。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es lb"><img src="../Images/9214f9bf036fa82db169af94c5e0c8f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*806dydGcFncN5GatkMeliw.png"/></div></figure><p id="790d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当我们计算成为正面或负面评论的概率时，存在一个挑战。如果我们遇到一个单词，它是“正”字典包括的，而“负”字典不包括的，那么对数概率将是0，而0的对数是未定义的。因此，我们应用拉普拉斯平滑。我们给分子加1，但也必须加上词汇量的大小来平衡分母。</p><p id="42d3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">基于每个词在正面和负面评论中的出现率，我们将概率较大的类别作为模型的预测。</p><p id="61ef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们用F1分数来评估性能。我们应该首先计算精确度和召回率来计算F1分数。首先我们给出一些定义如下:</p><p id="4c4d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">真阳性:</strong>模型预测阳性，样本实际阳性</p><p id="c99c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">真阴性:</strong>模型预测为阴性，样本实际为阴性</p><p id="f138" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">假阳性:</strong>模型预测为阳性，样本实际为阴性</p><p id="eeeb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">假阴性:</strong>模型预测为阴性，样本实际为阳性</p><p id="9feb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">精度显示了我们估计为正的值中有多少实际上是正的。</p><p id="41bd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">精度</strong> =真阳性/(真阳性+假阳性)</p><p id="22e7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">灵敏度(召回率)是一个指标，它显示实际上呈阳性的样本中有多少是我们估计为阳性的。</p><p id="0a62" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">回忆</strong> =真阳性/(真阳性+假阴性)</p><p id="faa8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">F1值显示了精确值和召回值的调和平均值。</p><p id="4475" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">F1-得分</strong>= 2 *精度*召回/(精度+召回)</p><h1 id="f994" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">3-各种因素的分析</h1><p id="fdf4" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">在本节中，我们将讨论添加一些提到的因素的影响。首先，我们在没有停用词移除和词干提取的情况下训练朴素贝叶斯模型。我们算了一下F1的分数:0.8306。在第二个实验中，我们添加了停用词移除。这个实验是在没有词干的情况下进行的，F1分数是0.8425。在第三个实验中，我们在预处理中增加了词干提取步骤，并计算出0.8371的F1值。根据我们的结果，添加两个预处理步骤对模型做出了积极的贡献。</p><p id="c87c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你可以在我的GitHub页面上找到实现笔记本。</p><p id="074f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae lc" href="https://github.com/melikenurm/nlp-naive-bayes" rel="noopener ugc nofollow" target="_blank">https://github.com/melikenurm/nlp-naive-bayes </a></p></div></div>    
</body>
</html>