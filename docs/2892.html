<html>
<head>
<title>Bert For Topic Modeling ( Bert vs LDA )</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于主题建模的Bert(Bert vs LDA)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bert-for-topic-modeling-bert-vs-lda-8076e72c602b?source=collection_archive---------1-----------------------#2021-05-23">https://medium.com/analytics-vidhya/bert-for-topic-modeling-bert-vs-lda-8076e72c602b?source=collection_archive---------1-----------------------#2021-05-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="39f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将使用LDA ( <strong class="ih hj">潜在的狄利克雷分配</strong>，它是为这个目的而设计的)和单词嵌入来进行主题建模。我将尝试对不同降维算法组合(TF-IDF、LDA和Bert)应用主题建模(PCA、TSNE、UMAP)。</p><p id="6d39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代码在github。(<a class="ae jd" href="https://github.com/mcelikkaya/medium_articles2/blob/main/bertlda_topic_modeling.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="69b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我要用的数据集是，sk learn“fetch _ 20 news groups”。我将下载所有组并用作数据。我做一些非常基本的清洁工作。我的最终数据集有11300个条目。详情请查看方法“<strong class="ih hj"> build_data </strong>”。</p><p id="2060" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">话题建模</strong>的本质是一种利用频率项矩阵的问题。如果一些单词在一些文档中出现得更多，这意味着它们有相似的主题。如果咖啡和茶在文档中出现得太多，我们可以推断它与饮料有关。</p><p id="f283" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将从我们的文档中创建嵌入。首先，我们将尝试简单的<strong class="ih hj"> TF-IDF </strong>，这意味着我们依赖于文档中的词频。然后，我们将尝试使用预训练方法(<strong class="ih hj"> Bert </strong>)生成嵌入是否会比上述方法更好。</p><p id="f1cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，我将尝试<strong class="ih hj"> LDA </strong>方法，这是一种使用概率方法的高级主题建模技术。<strong class="ih hj"> LDA </strong>简单假设，文档是主题的混合体，一些单词在一些主题中出现的概率比其他的更大。因此<strong class="ih hj"> LDA </strong>为属于一个主题的每个文档给出一个概率向量。</p><p id="2b60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用单词、句子嵌入时，<strong class="ih hj"> NLP </strong>问题遭遇高维。文档矩阵的宽度等于所有词汇中的字数。这个维度太高，无法应用大多数算法。因此，我们必须对嵌入向量进行降维才能使用它们。我将尝试使用其中的3个</p><p id="f9b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> PCA : </strong>线性降维<br/> <strong class="ih hj"> T-SNE : </strong>非线性降维，保留数据中的局部结构。<br/> <strong class="ih hj"> Umap : </strong>非线性降维，保留局部和大部分全局结构</p><p id="7cc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代码中的主要方法如下。事实上，所有的方法都需要嵌入。方法"<strong class="ih hj">predict _ topics _ with _ k means</strong>"将<strong class="ih hj"> Kmeans </strong>应用于嵌入和返回标签。<br/>方法“<strong class="ih hj"> reduce_umap </strong>”、“<strong class="ih hj"> reduce_tsne </strong>”和“<strong class="ih hj"> reduce_pca </strong>”简单地在一个缩减的空间内进行嵌入和返回嵌入。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><h1 id="d82e" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><strong class="ak"> TF-IDF </strong></h1><p id="bea9" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">使用<strong class="ih hj"> Tf-idf </strong>我们创建了一个非常高维度和稀疏的向量。为了应用聚类，我们最好缩减维度。我将尝试两种方法<strong class="ih hj"> T-Sne </strong>和<strong class="ih hj"> Umap </strong>。当我将<strong class="ih hj"> Tf-idf </strong>应用于项目时，我得到一个向量(11314，70990)</p><pre class="je jf jg jh fd ko kp kq kr aw ks bi"><span id="4261" class="kt jm hi kp b fi ku kv l kw kx">11300 : Number of items in dataset<br/>70990 : No of items in vocabulary</span></pre><p id="0bea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个维度很高。我用<strong class="ih hj"> T-sne </strong>和<strong class="ih hj"> Umap </strong>应用<strong class="ih hj"> Kmeans </strong>进行聚类。以下是两者的可视化。当维数减少时，<strong class="ih hj"> Tf-idf </strong>似乎提供了良好的嵌入。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ky"><img src="../Images/49145ead1cc6e77115b98922d658d657.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*eoLr8EV4Nt5JOlosz5L2DQ.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lb"><img src="../Images/1b5c204a114aae32c9082f2edbdd31cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*XEjqJoQ03gKD5YNxB2xLvQ.png"/></div></figure><p id="2b3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">两个情节似乎都不错。如果我们检查Silhoutte分数，我们可以看到Umap和T-sne有相似的结果。原始嵌入得分很低，因为嵌入维数很高。</p><p id="2656" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">*** <strong class="ih hj">剪影得分</strong>是一个对象与它自己的聚类相比与其他聚类相似程度的度量。<br/>最佳值为1，最差值为-1。接近0的值表示重叠的簇。<br/>负值通常表示样本被分配到错误的聚类，因为不同的聚类更相似。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lc"><img src="../Images/a7fbc267b68221ffc3f486b6d97721a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cRINRlC13Oh7F6x0R69DPw.png"/></div></div></figure><h1 id="5b9d" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><strong class="ak"> LDA </strong></h1><p id="785f" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">如何申请LDA 我就不赘述了。可以查看其他非常好的教程或者我的代码简单使用。在应用<strong class="ih hj"> LDA </strong>之后，我们得到[ <strong class="ih hj">数量_主题x概率</strong>的列表，该列表显示了文档所属的可能主题分数。例如，下面我们可以看到，对于10处的矢量嵌入，文档10属于的概率。概率总和为1。从下面我们可以说对于索引10处的文档，最可能的主题是7和8。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lb"><img src="../Images/50e68c84f7eebf57f4fe12400426eb0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*cO1qDcR0XbFD6_rkat9I8g.png"/></div></figure><p id="930b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们可以用它作为每个文档的向量，并应用之前的方法。下面你可以看到降维方法表现不佳，因为<strong class="ih hj"> LDA </strong>向量的维数很低，应用这些方法没有太大意义。事实上，没有什么作为<strong class="ih hj"> LDA </strong>向量，它只是给出属于一个主题的概率，我用它作为向量，正如所料，它没有给出任何好的结果。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lh"><img src="../Images/35b8e31538900f5464aec1d2f69d6c98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*Tf62SoLjNm8fkXiwxTkz-w.png"/></div></figure><h1 id="cd73" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">伯特</h1><p id="f6fe" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">我们将如何处置伯特很简单。从这些文档中创建一个嵌入，并将该嵌入用作其他聚类算法的源。下面你可以看到用<strong class="ih hj"> SentenceTransformer </strong>嵌入文档是多么容易。生成的向量维数为768。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es li"><img src="../Images/3708f11f069a374293ba255e5c0919ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*d3JrTjFl9yyGLJ0tQi6bpA.png"/></div></figure><p id="2bf6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们检查分数，我们可以看到乌玛普和T2的表现非常好。这意味着，<strong class="ih hj"> Bert </strong>做得很好，创建了丰富的嵌入，<strong class="ih hj"> T-sne </strong>和<strong class="ih hj"> Umap </strong>在减少这些方面做得很好。Raw <strong class="ih hj"> Bert </strong>嵌入由于维数过高而表现不佳。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lj"><img src="../Images/5f1912d71cd258991f6a0a18157ffcbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*AHVW5Le628jZ1aTrm3VB5A.png"/></div></figure><p id="3d4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你想看到所有的可视化效果，请查看github代码。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lk"><img src="../Images/681a216a33eb6d49d1779e1ac7f74559.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*t_xj-vXqEz7BnNg5SOfiyQ.png"/></div></figure><p id="7343" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我尝试了3种主题建模的方法。LDA是主题建模的默认方法。如果需要更简单而非黑盒的模型，可以使用<strong class="ih hj"> TF-IDF </strong>或那种风格的单词嵌入。如果你认为你的文档有上下文，你想利用句子嵌入，你可以尝试<strong class="ih hj"> Bert </strong>来获得更好的结果。</p></div></div>    
</body>
</html>