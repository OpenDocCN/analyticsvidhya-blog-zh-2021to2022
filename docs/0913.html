<html>
<head>
<title>What is a Neural Network?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是神经网络？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-networks-in-a-nutshell-with-java-b4a635a2c4af?source=collection_archive---------4-----------------------#2021-02-06">https://medium.com/analytics-vidhya/neural-networks-in-a-nutshell-with-java-b4a635a2c4af?source=collection_archive---------4-----------------------#2021-02-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/476712533127bf6d15c36ef5d7d31192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3hpcsXFvL7KTK18N8Y7RAQ.jpeg"/></div></div></figure><div class=""/><p id="29cf" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最强大和最广泛使用的人工智能方法之一叫做<strong class="ir ht">神经网络</strong>。但是，它们到底是什么呢？它们是如何工作的？让我用简单的英语解释一下。</p><h1 id="c70c" class="jn jo hs bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">什么是神经网络？</h1><p id="0801" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">神经网络是被称为<strong class="ir ht">神经元</strong>的连接节点的集合。</p><h1 id="f94d" class="jn jo hs bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">什么是神经元？</h1><p id="d977" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">神经元是一个节点，它有一个或多个<strong class="ir ht">输入</strong>和一个<strong class="ir ht">输出</strong>，如图1所示。神经元内部发生三种行为:</p><ul class=""><li id="5c86" class="kq kr hs ir b is it iw ix ja ks je kt ji ku jm kv kw kx ky bi translated">权重与每个<strong class="ir ht">输入</strong>相关联，以放大或去放大它；</li><li id="540d" class="kq kr hs ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">将所有加权输入相加；</li><li id="cd8d" class="kq kr hs ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">总和被用作确定最终输出的<strong class="ir ht">激活功能</strong>的输入。</li></ul><figure class="lf lg lh li fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es le"><img src="../Images/83e0b9939c591f537effc654b2d9c702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yYxpKD2bBD1YEjYmGXEXwQ.jpeg"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图一。神经元就像一个等式——它将权重与每个输入关联起来，然后将加权输入相加并应用激活函数来计算输出。</figcaption></figure><h1 id="8601" class="jn jo hs bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">什么是激活函数？</h1><p id="af2f" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">激活函数是一个数学方程，它为小输入输出一个小值，如果输入超过<strong class="ir ht">阈值则输出一个大值。</strong>常用激活函数的一个例子是图2所示的<strong class="ir ht"> sigmoid函数</strong>。</p><figure class="lf lg lh li fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ln"><img src="../Images/cba40b8bbbcfa92c522e843724b598f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EIKZ-rgVKviu70ghWxxauw.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图二。sigmoid函数通常用作神经元的激活函数。</figcaption></figure><p id="233d" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这个想法很简单:接近零的输入值将导致输出的显著变化，而太大或太小的输入值将导致最小的差异。</p><h1 id="9a81" class="jn jo hs bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">神经元是如何连接的？</h1><p id="8a4f" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">一个神经元的输出可以用作其他神经元的输入。通常，神经元聚集成<strong class="ir ht">层</strong>。层是一个通用术语，适用于在神经网络内特定深度一起工作的节点的集合。输出从第一层传播到最后一层。如图3所示，通常有一个<strong class="ir ht">输入层</strong>，一个或多个中间层(称为<strong class="ir ht">隐藏层</strong>，以及一个<strong class="ir ht">输出层</strong>。</p><ul class=""><li id="8140" class="kq kr hs ir b is it iw ix ja ks je kt ji ku jm kv kw kx ky bi translated">输入图层仅包含数据。那里没有正常工作的神经元。</li><li id="d7d1" class="kq kr hs ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">隐藏层是<em class="lo"> </em>学习发生的地方——稍后，我们将回顾如何发生。</li><li id="ee3f" class="kq kr hs ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">输出层包含计算最终输出的神经元。</li></ul><figure class="lf lg lh li fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lp"><img src="../Images/d306be4a08be054b8177764b8a994c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eHuad5nVXkvGjhCgQuUf4A.jpeg"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图3。一种神经网络，在输入层有两个输入，两个隐藏层，在输出层有一个神经元。</figcaption></figure><p id="ce68" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输入和输出神经元的数量取决于手头的问题。隐藏神经元的数量往往是输入和输出神经元的总和，但不是规律。</p><h1 id="56cc" class="jn jo hs bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">它们是如何工作的？</h1><p id="d9b1" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">神经网络帮助我们对信息进行分类。它们通过处理例子来训练(学习)，每个例子包含一个已知的<em class="lo">输入</em>和<em class="lo">输出</em>。<strong class="ir ht">训练过程</strong>的目标是计算与每个神经元中每个输入相关的权重值。一旦我们训练了神经网络，即我们计算了所有权重的权重，我们就可以使用神经网络将新的看不见的输入映射到输出。</p><h1 id="60cf" class="jn jo hs bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">例子</h1><p id="b497" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">神经网络的Hello-World示例通常是实现一个神经网络来识别XOR运算符。这个神经网络有</p><ul class=""><li id="f02c" class="kq kr hs ir b is it iw ix ja ks je kt ji ku jm kv kw kx ky bi translated">两个输入，</li><li id="a6b4" class="kq kr hs ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">一个输出，以及</li><li id="b4a3" class="kq kr hs ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">我们将使用一个具有三个神经元的隐藏层——正如推荐的，输入和输出神经元的总和。</li></ul><p id="f760" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们的神经网络如图4所示，我们将使用输入数据来训练网络和已知的输出。</p><figure class="lf lg lh li fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lq"><img src="../Images/8b5a3a4a3dce4ab3819eed963c457b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XF0X5QraAUsUQcShg-Ar3g.jpeg"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图4。我们的神经网络用于计算XOR运算符。</figcaption></figure><h2 id="2733" class="lr jo hs bd jp ls lt lu jt lv lw lx jx ja ly lz kb je ma mb kf ji mc md kj me bi translated">第一步。初始化权重和偏差</h2><p id="3fcf" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">神经网络的第一步是初始化权重。我们有什么选择？</p><ul class=""><li id="cbc8" class="kq kr hs ir b is it iw ix ja ks je kt ji ku jm kv kw kx ky bi translated">只用零初始化——这不是一个好策略😳。请记住，权重将乘以输入，因此当权重等于零时，输入不再起作用，神经网络无法正常学习。</li><li id="19a6" class="kq kr hs ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated"><strong class="ir ht">随机初始化权重</strong>–这有点天真，但除了在少数情况下，它经常工作得很好。让我们在例子中使用这种方法。</li><li id="aaa3" class="kq kr hs ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">高级策略可用。</li></ul><p id="c782" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，我们将用随机值初始化神经网络中的九个权重值。</p><h2 id="f52c" class="lr jo hs bd jp ls lt lu jt lv lw lx jx ja ly lz kb je ma mb kf ji mc md kj me bi translated">第二步。正向传播</h2><p id="b759" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">这是一个为网络提供一个输入并观察输出的花哨名称。我们从输入层开始，计算隐藏层的输出。结果被向前传递到下一层。然后，我们使用隐藏层的输出作为输入来计算输出层的输出。图5显示了数学。这只是线性代数。就是这样。</p><figure class="lf lg lh li fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mf"><img src="../Images/8d105f26d57efd4214fa789e54b5f821.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JvA0rHmqG2wGxuefRyEGiw.jpeg"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图5。计算每个神经元的输出，从输入层开始向前移动；使用一层神经元的输出作为下一层神经元的输入。</figcaption></figure><h2 id="6bc0" class="lr jo hs bd jp ls lt lu jt lv lw lx jx ja ly lz kb je ma mb kf ji mc md kj me bi translated"><strong class="ak">第三步。计算误差</strong></h2><p id="80a2" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated"><strong class="ir ht">误差</strong>计算为已知输出和计算出的<strong class="ir ht"> </strong>输出(本例中为输出₃)之间的差值。误差值通常是平方的，以消除负号，并对较大的差异给予更大的权重。除以2并不影响计算，而且有助于以后使求导更简单。</p><figure class="lf lg lh li fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mg"><img src="../Images/4eee58a28871a9be023d63e14f39a3aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DwGnup3dh3suAIGv0NU2Sg.jpeg"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图6。误差计算为已知输出和计算的<strong class="bd jp"> </strong>输出之间的差值。</figcaption></figure><p id="32f9" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果神经网络在输出层中有一个以上的节点，则误差计算为所有部分误差的总和。</p><h2 id="f6da" class="lr jo hs bd jp ls lt lu jt lv lw lx jx ja ly lz kb je ma mb kf ji mc md kj me bi translated"><strong class="ak">第四步。反向传播</strong></h2><p id="02fc" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">由于我们使用随机值作为权重，我们的输出可能会有很高的误差。我们需要减少误差。减少误差的唯一方法是改变计算值。并且，改变计算值的唯一方法是通过<strong class="ir ht">修改权重</strong>的值。权重的适当调整确保了随后的输出将更接近预期的输出。重复这个过程，直到我们对网络能够产生与已知输出足够接近的结果感到满意。</p><p id="9501" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如何修改权值以减少误差？</p><p id="8c56" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">简单回答:使用<strong class="ir ht">梯度下降算法</strong>。它是在1847年首次提出的。它应用多变量微积分，特别是<strong class="ir ht">偏导数</strong>。误差函数相对于每个权重的<strong class="ir ht">导数用于调整权重值。误差函数的导数可以乘以一个选定的数(称为<strong class="ir ht">学习率</strong>，以确保新的更新权重最小化误差函数。学习率是一个很小的正值，通常在0.0到1.0之间。</strong></p><p id="3f24" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了计算权重的偏导数，我们需要误差函数的<strong class="ir ht">导数和sigmoid函数的<strong class="ir ht">导数。</strong>图7显示了权重更新的一般方程，以及求解权重W₆方程的一个示例——输出层中神经元的第一个输入的权重。</strong></p><p id="fb7e" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">应用微积分链规则原理计算复合函数的导数。请注意，对于输出层中的神经元和隐藏层中的神经元，计算是相似的，但并不相同。</p><figure class="lf lg lh li fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mh"><img src="../Images/3fcfacae14183ec3354458041d9224fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AP9SS-6tnwx_h0NbKz4K0Q.jpeg"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图7。等式至:更新权重值(红色)、误差和误差导数(灰色)以及计算输出(sigmoid函数)导数。</figcaption></figure><p id="3a8d" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，我们从随机权重值开始，然后:</p><ul class=""><li id="2627" class="kq kr hs ir b is it iw ix ja ks je kt ji ku jm kv kw kx ky bi translated">我们使用图5中的数学计算所有神经元的输出(正向传播)以及计算输出和已知输出之间的差异(误差)。</li><li id="90fc" class="kq kr hs ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">如果差异大于我们的预期，我们计算新的权重值(反向传播)。</li></ul><p id="d136" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这两个活动重复进行，直到我们将误差降低到可接受的值。一个<strong class="ir ht"> <em class="lo">可接受的</em> </strong> <em class="lo"> </em> <strong class="ir ht"> <em class="lo">误差</em> </strong>可以是0到0.05之间的任何值。</p><h1 id="205e" class="jn jo hs bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">编写示例代码</h1><p id="ca08" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">让我们看看上面描述的四个步骤在代码中是怎样的。我们要用Java实现一个简单的神经网络。我不想重新发明轮子；只需展示具体细节，就能理解事情是如何运作的。</p><p id="977d" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先，属性:</p><ul class=""><li id="8665" class="kq kr hs ir b is it iw ix ja ks je kt ji ku jm kv kw kx ky bi translated">定义我们将使用的学习率的常数值；</li><li id="eb99" class="kq kr hs ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">三个变量来存储每层中的节点总数—我们将创建一个神经网络，其中两个节点在输入层，三个在隐藏层，一个在输出层。</li><li id="bc03" class="kq kr hs ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">存储权重值、偏差值和每个神经元输出的三个数组。</li></ul><p id="f408" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将创建一个具有六个节点的神经网络，我们将需要九个权重和四个偏置值用于隐藏和输出层节点。</p><figure class="lf lg lh li fd hj"><div class="bz dy l di"><div class="mi mj l"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图8。SimpleNeuralNetwork.java-类中的属性</figcaption></figure><h2 id="268e" class="lr jo hs bd jp ls lt lu jt lv lw lx jx ja ly lz kb je ma mb kf ji mc md kj me bi translated">第一步。初始化权重和偏差</h2><p id="0352" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">我们可以使用构造函数来初始化数组，并将初始值放入权重和偏差中。记住，最初，它们只是随机值。第11行和第13行进行初始化。</p><figure class="lf lg lh li fd hj"><div class="bz dy l di"><div class="mi mj l"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图9。SimpleNeuralNetwork.java——建造商</figcaption></figure><h2 id="0b9d" class="lr jo hs bd jp ls lt lu jt lv lw lx jx ja ly lz kb je ma mb kf ji mc md kj me bi translated">第二步。正向传播</h2><p id="92a5" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">我们需要求解图5所示的方程。因此，让我们为此创建一个方法。请注意，输入被作为节点处理(在输入层)，但它们不计算这些节点的输出值。我们计算隐藏层节点和输出层节点的输出。通过将权重值乘以输入值，将它们相加，然后应用激活函数来计算输出。我们使用sigmoid作为激活函数，我们创建一个sigmoid方法只是为了保持关注点的分离。注意这里的complex，基本上是图5中描述的线性代数的实现。我们将对每一组输入值运行此操作，因此，它将对{0，0}、{0，1}、{1，0}和{1，1}运行4次</p><figure class="lf lg lh li fd hj"><div class="bz dy l di"><div class="mi mj l"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图10。SimpleNeuralNetwork.java—前向传播，即计算隐层和输出层中每个神经元的输出</figcaption></figure><h2 id="fe56" class="lr jo hs bd jp ls lt lu jt lv lw lx jx ja ly lz kb je ma mb kf ji mc md kj me bi translated">第三步。计算误差</h2><p id="a839" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">在我们的例子中，输出层只有一个神经元，误差计算非常简单。但是，让我们通过创建一个可用于输出层中的一个或多个神经元的实现来概括我们代码中的思想。这个实现如图11所示。</p><figure class="lf lg lh li fd hj"><div class="bz dy l di"><div class="mi mj l"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图11。SimpleNeuralNetwork.java—误差计算</figcaption></figure><h2 id="7c22" class="lr jo hs bd jp ls lt lu jt lv lw lx jx ja ly lz kb je ma mb kf ji mc md kj me bi translated">第四步。反向传播</h2><p id="8e35" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">最后，让我们创建学习部分——一个实现负责更新权重值的数学的方法。多元微积分就住在那里。此方法针对每一组已知的输出值运行，因此，它将针对{0.0}、{1.0}、{1.0}和{0.0}运行4次。</p><figure class="lf lg lh li fd hj"><div class="bz dy l di"><div class="mi mj l"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图12。SimpleNeuralNetwork.java-反向传播，即计算所有神经元中权重和偏差值的新值</figcaption></figure><p id="3825" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们有所有的零件；是时候将它们放在一起并运行我们的实现了。看一看我们类的<em class="lo"> main() </em>方法，作为总结:</p><ul class=""><li id="ef40" class="kq kr hs ir b is it iw ix ja ks je kt ji ku jm kv kw kx ky bi translated">训练数据(输入和已知输出)用两个数组表示。</li><li id="c412" class="kq kr hs ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">创建的神经网络对象有两个输入，三个节点位于隐藏层，一个节点位于输出层。</li><li id="7806" class="kq kr hs ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">向前传播、误差计算和向后传播运行10，000次。</li></ul><figure class="lf lg lh li fd hj"><div class="bz dy l di"><div class="mi mj l"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图13。SimpleNeuralNetwork.java—主要方法</figcaption></figure><p id="a585" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后，让我们试试我们的神经网络。经过10，000次迭代后，我们的神经网络是活跃的，并以可接受的性能工作。图14显示了错误率是如何降低的。X轴代表迭代次数(0到10，000)，Y轴是在图13所示的<em class="lo"> main() </em>方法的第18行和第23行中计算的均方误差。</p><figure class="lf lg lh li fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mk"><img src="../Images/b9771d70883dc8a6cecdf5ccf66b4e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VFvWQKekeJCl58G_qDjCIw.jpeg"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">图14。每次迭代的错误。总共运行10，000次迭代。误差从0.4242下降到0.0116</figcaption></figure><p id="6e18" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于大约100行代码来说还不错(你可以从我的<a class="ae ml" href="https://github.com/javiergs/Medium/blob/main/NeuralNetwork/BasicNeuralNetwork.java" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>下载完整的源代码)。然而，我们可以使用库用大约10行代码完成同样的工作。其中一个这样的库是<a class="ae ml" href="https://deeplearning4j.org/" rel="noopener ugc nofollow" target="_blank"><strong class="ir ht">Eclipse deep learning 4j</strong></a><strong class="ir ht">，</strong>一个为Java编写的开源、分布式深度学习库。我们可以使用一个库来解决更复杂的问题，例如训练一个用于图像分类的神经网络。输入将增加，训练数据集将更加重要(比我们的四行XOR更重要)，并且我们将需要不止一个隐藏层。但那是另一个故事了。感谢阅读。请在下面留下您的反馈和评论。</p></div><div class="ab cl mm mn go mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ha hb hc hd he"><h1 id="7be1" class="jn jo hs bd jp jq mt js jt ju mu jw jx jy mv ka kb kc mw ke kf kg mx ki kj kk bi translated">参考</h1><p id="690b" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">你想了解更多细节吗？在此回顾sigmoid函数的<a class="ae ml" href="https://www.anotsorandomwalk.com/first-derivative-of-the-sigmoid-function/" rel="noopener ugc nofollow" target="_blank">导数；这里复习一下微积分中的</a><a class="ae ml" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank">链式法则</a>；在这里回顾一下<a class="ae ml" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">渐变后代</a>的定义；这里详细描述了反向传播背后的数学原理。</p></div></div>    
</body>
</html>