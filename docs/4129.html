<html>
<head>
<title>Next Word Prediction Using SwiftKey Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用SwiftKey数据预测下一个单词</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/next-word-prediction-using-swiftkey-data-f121f59bc7d?source=collection_archive---------3-----------------------#2021-08-30">https://medium.com/analytics-vidhya/next-word-prediction-using-swiftkey-data-f121f59bc7d?source=collection_archive---------3-----------------------#2021-08-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="0cef" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">下一个单词预测的详细实现</h2></div><p id="850a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这篇博客中，我将带你通过<strong class="iy hi">概率</strong>和<strong class="iy hi">分类</strong>方法来预测给定单词序列的下一个单词。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es js"><img src="../Images/d7519ed6402869a42a86ed17790f815b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*SwewJ4FPzQ6mxOnwBd4wNA.jpeg"/></div></figure><h1 id="bf37" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">目录</h1><ul class=""><li id="15f2" class="ks kt hh iy b iz ku jc kv jf kw jj kx jn ky jr kz la lb lc bi translated"><strong class="iy hi">商业问题</strong></li><li id="5410" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><strong class="iy hi">毫升配方</strong></li><li id="7504" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><strong class="iy hi">现有工作</strong></li><li id="cab4" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><strong class="iy hi">第一次切割方法</strong></li><li id="4818" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><strong class="iy hi">数据可视化</strong></li><li id="7f53" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><strong class="iy hi">概率方法</strong></li><li id="93c8" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><strong class="iy hi">分类方法</strong></li><li id="5b0a" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><strong class="iy hi">结果比较</strong></li><li id="d428" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><strong class="iy hi">例子</strong></li><li id="ffa7" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><strong class="iy hi">未来作品</strong></li><li id="8dc0" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><strong class="iy hi">参考文献</strong></li></ul><h1 id="6c41" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">商业问题</h1><p id="c9fe" class="pw-post-body-paragraph iw ix hh iy b iz ku ii jb jc kv il je jf li jh ji jj lj jl jm jn lk jp jq jr ha bi translated">这个问题属于自然语言处理(NLP)领域，文本挖掘是人工智能的一个分支，帮助计算机理解、解释和操纵人类语言。文本挖掘是一种人工智能技术，它使用NLP将文档中的非结构化文本数据转换为ML算法可以使用的结构化数据。下一个单词预测问题使用上述技术来完成结果。</p><h2 id="1d61" class="ll kb hh bd kc lm ln lo kg lp lq lr kk jf ls lt km jj lu lv ko jn lw lx kq ly bi translated">为什么下一个单词预测很重要？</h2><p id="be34" class="pw-post-body-paragraph iw ix hh iy b iz ku ii jb jc kv il je jf li jh ji jj lj jl jm jn lk jp jq jr ha bi translated">它有助于减少击键次数，从而节省打字时间，检查拼写错误，帮助回忆单词。仍在努力提高语言技能的学生可以接受它的帮助。患有阅读障碍的人也可以从中受益。</p><h1 id="3cd9" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">ML公式</h1><p id="1ace" class="pw-post-body-paragraph iw ix hh iy b iz ku ii jb jc kv il je jf li jh ji jj lj jl jm jn lk jp jq jr ha bi translated">在本节中，我将讨论如何将上述业务问题转换为机器学习问题。</p><h2 id="b39f" class="ll kb hh bd kc lm ln lo kg lp lq lr kk jf ls lt km jj lu lv ko jn lw lx kq ly bi translated">数据概述</h2><p id="8f40" class="pw-post-body-paragraph iw ix hh iy b iz ku ii jb jc kv il je jf li jh ji jj lj jl jm jn lk jp jq jr ha bi translated">为了解决这个问题，我们需要文本数据的语料库。该数据由<strong class="iy hi">快速键</strong>提供。</p><p id="20b6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">数据来源</strong>:<a class="ae lz" href="https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip" rel="noopener ugc nofollow" target="_blank">https://d 396 qusza 40 orc . cloudfront . net/dsscapstone/dataset/Coursera-swift key . zip</a></p><p id="e55d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个zip文件包含4个文件夹，每个文件夹包含4种不同语言的数据，即英语、俄语、芬兰语和德语。</p><p id="9d28" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我将使用英语文件夹中的数据。它包含以下文件。</p><ul class=""><li id="9fdd" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">en _ us . twitter . txt——这个文本文件包含来自Twitter的所有英语语言数据。</li><li id="ae0b" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">en_US.news.txt —此文本文件包含新闻网站和频道的所有英语语言数据。</li><li id="f581" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">en_US.blogs.txt —该文本文件包含所有博客网站的所有英语语言数据。</li></ul><h2 id="cdfa" class="ll kb hh bd kc lm ln lo kg lp lq lr kk jf ls lt km jj lu lv ko jn lw lx kq ly bi translated">将现实世界映射到ML问题</h2><p id="86d2" class="pw-post-body-paragraph iw ix hh iy b iz ku ii jb jc kv il je jf li jh ji jj lj jl jm jn lk jp jq jr ha bi translated">下一个单词预测包括预测下一个单词。因此，给定从语料库中生成的单词序列，我必须预测下一个出现概率最高的单词。<strong class="iy hi">因此，这是一个语言预测建模问题，也称为语言建模。这是概率模型。</strong></p><p id="ac41" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们还可以用另一种方法来处理这个问题。我们可以把下一个要预测的单词看作一个类。因此它可以被视为多类分类问题。</p><h2 id="75f6" class="ll kb hh bd kc lm ln lo kg lp lq lr kk jf ls lt km jj lu lv ko jn lw lx kq ly bi translated">业务目标和约束</h2><p id="908d" class="pw-post-body-paragraph iw ix hh iy b iz ku ii jb jc kv il je jf li jh ji jj lj jl jm jn lk jp jq jr ha bi translated"><strong class="iy hi">目标</strong></p><ul class=""><li id="f10c" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">从语料库中清除文本数据</li><li id="f4d6" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">从清理的数据中创建序列</li><li id="d15c" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">建立统计或神经语言模型来预测下一个单词</li></ul><p id="1cdc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">约束</strong></p><ul class=""><li id="84fc" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">内存限制-语料库大小可能过大，这可能会导致内存错误。</li><li id="680e" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">延迟限制—这是一个低延迟问题，因为整个问题旨在实现快速键入。</li><li id="ae84" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">OOV单词——注意词汇之外的单词是很重要的，因为所有的单词可能不会出现在语料库中，但是模型应该能够处理它。</li></ul><h2 id="59ad" class="ll kb hh bd kc lm ln lo kg lp lq lr kk jf ls lt km jj lu lv ko jn lw lx kq ly bi translated">绩效指标</h2><p id="093e" class="pw-post-body-paragraph iw ix hh iy b iz ku ii jb jc kv il je jf li jh ji jj lj jl jm jn lk jp jq jr ha bi translated"><strong class="iy hi">对于概率模型</strong></p><p id="0d31" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">困惑度:它被定义为由字数归一化的测试集的逆概率。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es md"><img src="../Images/eba4d5f11d40a6618a578bab54a4a768.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*E12o5jsKGvbvZcLZ-XcqCQ.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">困惑方程</figcaption></figure><p id="4cab" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">为分类模式</strong></p><p id="128d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">分类交叉熵:每个下一个要预测的单词被认为是一个类别，所以我将使用分类交叉熵。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es mi"><img src="../Images/c34901f6e62362277fe0dee7aafa3c52.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*zKp1vZDjZW0SUaBAKsWGdw.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">范畴交叉熵公式</figcaption></figure><h1 id="7e7a" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">现有工作</h1><ol class=""><li id="b7c0" class="ks kt hh iy b iz ku jc kv jf kw jj kx jn ky jr mj la lb lc bi translated">论文doi:<a class="ae lz" href="http://10.1109/icdse47409.2019.8971796" rel="noopener ugc nofollow" target="_blank"><strong class="iy hi">10.1109/icdse 47409 . 2019 . 8971796</strong></a></li></ol><p id="b8ca" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这篇研究论文是关于使用深度学习技术预测印地语中的下一个单词。我总结了用于印地语下一个单词预测的技术。</p><ul class=""><li id="1274" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">清理文本并生成唯一的单词</li><li id="147e" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">这些独特的单词存储在字典中，并映射到索引，因为神经网络与索引一起工作更好</li><li id="d463" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">设置序列长度，例如，如果序列长度=6，则句子被分成6:1的比例，即。前六个字在input_x中，其余的在input_y中</li><li id="960e" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">基于序列长度从input_x创建张量，并存储在data_x中</li><li id="38c0" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">对于data _x中的每个张量序列，作为input_y的后续单词序列作为张量存储在data_y中。</li><li id="9eef" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">LSTM和双LSTM用于基于下一个单词的最高概率的预测。</li><li id="d0b2" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">使用的激活函数是Softmax来分配下一个单词的概率。</li><li id="6c7c" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">由于该问题被视为多类分类问题，因此损失函数是分类交叉熵。</li><li id="7243" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">LSTM模型运行了197个时期，而双LSTM模型运行了121个时期。</li><li id="0360" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">对于所有的序列长度都重复了这一过程。</li></ul><p id="653b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2.论文doi:<a class="ae lz" href="https://doi.org/10.1007/s00521-020-05245-3(0123456789().,-volV)(01234567 89(). ,- volV)" rel="noopener ugc nofollow" target="_blank"><strong class="iy hi">https://doi . org/10.1007/s 00521-020-05245-3(0123456789()。，-volV)(01234567 89()。</strong>，-volV)</a></p><p id="201d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">本文为库尔德语开发了下一个单词预测模型。在这里，他们讨论了这个问题的N元语言模型。</p><p id="bfd1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> N元语言模型</strong></p><p id="ffa5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在语言模型中，要么概率被分配给一系列单词，要么概率被分配给给定一些前面单词的下一个单词。</p><p id="c968" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里讨论<strong class="iy hi"> 2场景</strong>:</p><p id="62b4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">第一种情况</strong> -在句子(w1 w2 w3 w4 w5)的情况下，句子的概率由P(w1)*P(w2)*P(w3)*P(w4)*P(w5)给出，其中P(wn)是wn的概率。</p><p id="2027" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">p(wn)=(wn在语料库中的出现次数)/(语料库中的总字数)</p><p id="7133" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">第二种情况</strong> —在句子(w1 w2 w3 w4 w5)的情况下，句子的概率由P(w1)* P(w2 | w1)* P(w3 | w1 w2)* P(w4 | w1 w2 w3)* P(w5 | w1 w2 w3 w4)给出，其中P(wn|wn-1)是wn的概率，给定前面的单词是wn-1。</p><p id="39d1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">P(wn|wn-1)= P(wn-1 wn)/P(wn-1)，即(wn-1 wn)在语料库中出现的概率除以wn-1出现的概率。</p><p id="875f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">n元模型可以是不同的类型。那些是一元，二元，三元等等。</p><p id="aaa5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> Unigram模型— </strong>在该模型中，句子中每个词出现的概率由P(wi) =文本语料库中wi的计数/总字数给出。</p><p id="3eb6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">二元模型— </strong>这类似于上面讨论的第二种情况。但是这里我们只考虑前一个词。</p><p id="f5a1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在句子(w1 w2 w3 w4 w5)的情况下，句子的概率由P(w1)* P(w2 | w1)* P(w3 | w2)* P(w4 | w3)* P(w5 | w4)给出，其中P(wn|wn-1)是给定前一个单词wn-1时wn的概率。</p><p id="f580" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">三元模型— </strong>这类似于二元模型。但是这里我们考虑了前面的两个词。</p><p id="ed01" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，n元模型的表达式为</p><p id="eba9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">P(wi|w1w2…wi-1)=P(wi | w(i-n+1)…。wi-1)。</p><p id="a7ff" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这也叫马尔可夫假设。</p><p id="404c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在处理0概率，使用<strong class="iy hi">愚蠢退避算法</strong>。</p><p id="6479" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果高阶n元语法的结果为0，那么我们退回到低阶n元语法。</p><p id="0f6b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">它由下面的公式给出。</p><p id="c429" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果f(w iik+1)&gt; 0，则s(wi | wi-k+1i-1)= f(w iik+1)/f(w i1k+1)</p><p id="8bc7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">αS(wi | w i1 ik+2)否则</p><p id="cf69" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如:让我们考虑一个单词序列“这是一个非常美丽的”。</p><p id="1de7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这里，我们必须找到“美丽”的概率给定“这是一个非常”。假设“美丽”从未出现在“这是一个非常”的上下文中，那么对于4-grams模型，“美丽”的概率为0。所以我们退回到3-gram模型，找到概率为α*P(“漂亮”|“是非常”)。</p><p id="7582" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> 3。关于平滑的文章</strong></p><p id="f892" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">链接:</strong><a class="ae lz" href="https://towardsdatascience.com/n-gram-language-models-af6085435eeb" rel="noopener" target="_blank"><strong class="iy hi">https://towardsdatascience . com/n-gram-language-models-af 6085435 eeb</strong></a></p><p id="bad2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">需要平滑来处理在未知上下文中出现在测试集中的单词。有不同的平滑技术。</p><ul class=""><li id="e810" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">拉普拉斯平滑:为了计算概率，我们简单地在分子上加1，在分母上加一个额外的V词汇。p(word)=(word count+1)/(total number of words(N)+V)</li><li id="14dd" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">P(new_word)=1/(N+V)</li><li id="235c" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">添加k- Smoothing:我们将在计数中添加一些分数(k ),而不是给单词的频率加1。</li><li id="0365" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">p(word)=(word count+k)/(total number of words(N)+kV)</li><li id="27e5" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">P(new_word)=k/(N+kV)</li><li id="4a46" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">退避和插值:退避与前面解释的愚蠢退避算法相同。插值是通过给每个n元模型赋予权重来组合不同的n元模型的方法，例如unigram、bigram、trigram模型。</li></ul><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es mk"><img src="../Images/166c9ff353868c906ada31df1c9f37cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*URWTrtP2eamHi7F9"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">后退和插值</figcaption></figure><h1 id="ded1" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">首次切割法</h1><p id="b12d" class="pw-post-body-paragraph iw ix hh iy b iz ku ii jb jc kv il je jf li jh ji jj lj jl jm jn lk jp jq jr ha bi translated">在这一节中，我讨论了我决定用来解决这个项目的第一种方法。</p><ul class=""><li id="f0d4" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">从3个不同的文本文件中读取数据，然后将它们一起添加到1。</li><li id="b08a" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">清理文本，包括删除标点符号，数字，特殊字符，多余的空格。</li><li id="5751" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">停用字词不会被删除，因为它们对下一个字词预测起着重要作用。</li><li id="c82e" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">将所有单词转换成小写。</li></ul><p id="f63d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">可视化</strong></p><ul class=""><li id="7e33" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">生成一元、二元、三元模型及其频率。</li><li id="17d6" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">可视化n个字母的单词云。</li></ul><p id="1a17" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">特化</strong></p><p id="2be2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> <em class="ml">对于马尔可夫链模型使用ngrams </em> </strong></p><ul class=""><li id="ef2e" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">我将创建一个有2个键的字典-第一个键将是不同单词序列的元组，第二个键将有下一个单词。</li><li id="1c38" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">该字典的值将是给定第一个关键字以及拉普拉斯平滑的第二个关键字出现的概率。</li><li id="dbbb" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">这将是二元和三元模型。</li></ul><p id="3472" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> <em class="ml">为神经网络模型</em> </strong></p><ul class=""><li id="59fe" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">将整个数据编码成索引。</li><li id="d3ba" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">我将从保持索引顺序的编码数据中生成长度为2、3和4的序列。</li><li id="73cd" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">然后，我将序列分为x和y，这样只有最右边的索引在y中，其余的在x中。</li></ul><p id="b40c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">造型</strong></p><p id="ff2d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> <em class="ml">马尔可夫链模型</em> </strong></p><ul class=""><li id="f7f4" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">我将开发一个使用全部数据的二元和三元概率的马尔可夫模型。</li></ul><p id="75fb" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> <em class="ml">神经网络模型</em> </strong></p><ul class=""><li id="10ae" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">对于每个序列长度，将x和y分成训练和测试数据集</li><li id="bd32" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">为不同的序列长度训练不同的LSTM模型。</li><li id="7e9c" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">不同的LSTM模型将用于不同的输入长度。</li></ul><h1 id="8c12" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">数据可视化</h1><p id="5ae0" class="pw-post-body-paragraph iw ix hh iy b iz ku ii jb jc kv il je jf li jh ji jj lj jl jm jn lk jp jq jr ha bi translated">为了对语料库有所了解，我在清理数据后进行了一些可视化处理。</p><h2 id="193b" class="ll kb hh bd kc lm ln lo kg lp lq lr kk jf ls lt km jj lu lv ko jn lw lx kq ly bi translated">数据清理</h2><p id="e9d2" class="pw-post-body-paragraph iw ix hh iy b iz ku ii jb jc kv il je jf li jh ji jj lj jl jm jn lk jp jq jr ha bi translated">为了清洁，我执行了以下步骤:</p><ul class=""><li id="f820" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated"><strong class="iy hi">去除多余的空格</strong>:单词之间所有多余的空格都被去除。</li><li id="080a" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><strong class="iy hi">删除特殊字符</strong>:删除任何形式的特殊字符，包括标点符号。</li><li id="bf17" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><strong class="iy hi">对文本数据进行分词</strong>:使用<strong class="iy hi"> <em class="ml"> nltk分词器</em> </strong>对新闻和博客数据进行分词。</li><li id="3a29" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><strong class="iy hi">标记Twitter数据</strong>:使用<strong class="iy hi"> <em class="ml"> nltk TweetTokenizer标记Twitter数据。</em> </strong></li></ul><p id="250a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">代码示例:</strong></p><figure class="jt ju jv jw fd jx"><div class="bz dy l di"><div class="mm mn l"/></div></figure><h2 id="d9ef" class="ll kb hh bd kc lm ln lo kg lp lq lr kk jf ls lt km jj lu lv ko jn lw lx kq ly bi translated">可视化1</h2><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="er es mo"><img src="../Images/37c8662778e9037618ea9796438850f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HbAn09Z_Ri5V45DTvwgxfQ.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx translated">基于频率的前50个单词</figcaption></figure><ul class=""><li id="2cc3" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">根据出现频率可视化前50个单词</li><li id="2392" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">大多数都是停用词，因为它们经常被使用。</li></ul><h2 id="0469" class="ll kb hh bd kc lm ln lo kg lp lq lr kk jf ls lt km jj lu lv ko jn lw lx kq ly bi translated">可视化2</h2><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="er es mt"><img src="../Images/51e99ff0c27253073bbd9c5dd8c65823.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8mjWiFBhrlRUrJaPcoO4Ag.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx translated">基于频率的最后50个单词</figcaption></figure><ul class=""><li id="d244" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">根据词频可视化最后50个单词</li><li id="82fc" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">它们在整个语料库中都只出现一次</li></ul><h2 id="9d46" class="ll kb hh bd kc lm ln lo kg lp lq lr kk jf ls lt km jj lu lv ko jn lw lx kq ly bi translated">可视化3</h2><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="er es mu"><img src="../Images/e76aeb009fcf645fd48ada1367bed07d.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*qNpAb7SFhEKTOxPWEM6ouw.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx translated">单字单词云</figcaption></figure><ul class=""><li id="1b04" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">用单字生成单词云。</li><li id="c2be" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">这里没有停用词，因为我试图将最常用的非停用词形象化。</li></ul><h2 id="9367" class="ll kb hh bd kc lm ln lo kg lp lq lr kk jf ls lt km jj lu lv ko jn lw lx kq ly bi translated">可视化4</h2><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es mv"><img src="../Images/6d86847708cff8a5dbdf1449ae879c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*J1yj9QdKHYnX1r3U1vTmUA.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">双字母单词云</figcaption></figure><ul class=""><li id="499d" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">这里我包括了停用词，否则短语的意思会改变。</li><li id="290b" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">从“云”这个词来看，有很多介词的用法</li></ul><h2 id="e578" class="ll kb hh bd kc lm ln lo kg lp lq lr kk jf ls lt km jj lu lv ko jn lw lx kq ly bi translated">可视化5</h2><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es mv"><img src="../Images/7376054eeb332871da09c2134e2be5c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*hY7Ne8TS5nOXXeNe_v887g.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">三元词云</figcaption></figure><p id="bb84" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">最常用的短语</strong></p><ul class=""><li id="b554" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">其中一个</li><li id="c9e3" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">“走出”</li><li id="40a2" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">“能够”</li><li id="01d2" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">一些</li><li id="b962" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">将会是</li></ul><h2 id="d54b" class="ll kb hh bd kc lm ln lo kg lp lq lr kk jf ls lt km jj lu lv ko jn lw lx kq ly bi translated">可视化6</h2><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es mv"><img src="../Images/bd1b36abd34837f1b78cbf2fb89b6ae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*3_ub15P1zP8qHMA7LnPTWw.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">四元单词云</figcaption></figure><p id="3a46" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">最常用的短语</strong></p><ul class=""><li id="9817" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr kz la lb lc bi translated">在中间</li><li id="90e4" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">最'之一'</li><li id="b4d1" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">其余的</li><li id="afce" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">在中间</li></ul><p id="13cf" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在大多数情况下，会大量使用停用词。</p><p id="a133" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">整个语料库中共有101098262个词。从视觉效果来看，很明显它们中的大多数都是停用词。所以在开发模型时不能排除它们。此外，我无法处理整个语料库，因为我没有足够的内存来支持对如此大数据的操作。</p><h1 id="ec32" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated"><strong class="ak">概率方法</strong></h1><p id="f35d" class="pw-post-body-paragraph iw ix hh iy b iz ku ii jb jc kv il je jf li jh ji jj lj jl jm jn lk jp jq jr ha bi translated">在概率方法中，在给定一组输入单词的情况下，基于其出现的概率来预测下一个单词。</p><p id="26e2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这一部分，我们有两个模型。</p><ol class=""><li id="8fe0" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr mj la lb lc bi translated"><strong class="iy hi">马尔可夫模型</strong>——详细解释见<a class="ae lz" rel="noopener" href="/@prerana1298/markov-chain-model-e642050e6c6f">此处</a>。</li><li id="4fa4" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr mj la lb lc bi translated"><strong class="iy hi"> GPT- 1型</strong> -详细说明见<a class="ae lz" rel="noopener" href="/@prerana1298/next-word-prediction-using-gpt-1-ae999acfe3de">此处</a>。</li></ol><h1 id="2a43" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">分类方法</h1><p id="07e7" class="pw-post-body-paragraph iw ix hh iy b iz ku ii jb jc kv il je jf li jh ji jj lj jl jm jn lk jp jq jr ha bi translated">在分类方法中，下一个单词被视为要预测的类别。因此，这个问题变成了一个多类分类问题。</p><p id="8caf" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这一部分，我们有两个模型。</p><ol class=""><li id="a008" class="ks kt hh iy b iz ja jc jd jf ma jj mb jn mc jr mj la lb lc bi translated"><strong class="iy hi">带和不带注意机构的堆叠LSTM</strong>-详细说明见<a class="ae lz" rel="noopener" href="/@prerana1298/next-word-prediction-using-lstms-98a1edec8594">此处</a>。</li><li id="b9ee" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr mj la lb lc bi translated"><strong class="iy hi">艾伯特模型</strong> -详细说明见<a class="ae lz" rel="noopener" href="/@prerana1298/next-word-prediction-using-albert-4e2bf5372132">此处</a>。</li></ol><h1 id="8dac" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">结果比较</h1><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es mw"><img src="../Images/5c20e9d60366c54261943b9aac10e26d.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*A8-9WNQ0W6tP2Jb9f43FNA.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">不同型号的性能</figcaption></figure><p id="445a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">尽管Al-BERT的测试损失是最低的，但正如我们前面看到的，它产生了非常糟糕的预测。所以我决定将GPT作为最终的预测模型，因为它是第二好的。</p><h1 id="b17a" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">下一个单词预测</h1><p id="d0ca" class="pw-post-body-paragraph iw ix hh iy b iz ku ii jb jc kv il je jf li jh ji jj lj jl jm jn lk jp jq jr ha bi translated">以下是GPT模型预测下一个单词的几个例子:</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es mx"><img src="../Images/e1edda1c86b5dbddb8c69987318bec3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*va3strthIJsjjDE2PBaWEg.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">预测示例</figcaption></figure><p id="b466" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> Github链接:</strong><a class="ae lz" href="https://github.com/kurchi1205/Next-word-Prediction-using-Swiftkey-Data" rel="noopener ugc nofollow" target="_blank">https://Github . com/kurchi 1205/Next-word-Prediction-using-swift key-Data</a></p><h1 id="66d6" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">未来作品</h1><ul class=""><li id="90d8" class="ks kt hh iy b iz ku jc kv jf kw jj kx jn ky jr kz la lb lc bi translated">由于内存问题，我只处理了全部数据的1%,如果我们处理全部数据，结果会好得多。</li><li id="40e1" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">最佳模型是通过微调GPT -1获得的。如果我们微调GPT-2和GPT-3，我们可以得到更好的结果，因为它们有更多的参数。</li><li id="1949" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated">Swiftkey数据还包含许多不同语言的语料库，我们可以对它们进行处理。</li></ul><h1 id="c23e" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">参考</h1><ul class=""><li id="62a2" class="ks kt hh iy b iz ku jc kv jf kw jj kx jn ky jr kz la lb lc bi translated"><a class="ae lz" href="https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2019/08/综合-指南-语言-模型-nlp-python-code/ </a></li><li id="ab4f" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><a class="ae lz" href="https://thecleverprogrammer.com/2020/07/20/next-word-prediction-model/" rel="noopener ugc nofollow" target="_blank">https://thecleverprogrammer . com/2020/07/20/next-word-prediction-model/</a></li><li id="b945" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><a class="ae lz" href="https://www.youtube.com/watch?v=BAN3NB_SNHY" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=BAN3NB_SNHY</a></li><li id="dd82" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><a class="ae lz" href="https://thinkinfi.com/fasttext-word-embeddings-python-implementation/" rel="noopener ugc nofollow" target="_blank">https://thinkinfo . com/fast text-word-embeddings-python-implementation/</a></li><li id="c875" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><a class="ae lz" href="https://www.youtube.com/watch?v=gHC9tRyVSNE" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=gHC9tRyVSNE</a></li><li id="f99b" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><a class="ae lz" rel="noopener" href="/swlh/language-modelling-with-nltk-20eac7e70853">https://medium . com/swlh/language-modeling-with-nltk-20 EAC 7 e 70853</a></li><li id="7170" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><a class="ae lz" href="https://stackoverflow.com/questions/54978443/predicting-missing-words-in-a-sentence-natural-language-processing-model" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/54978443/predicting-missing-words-in-a-sentence-natural-language-processing-model</a></li><li id="6c0e" class="ks kt hh iy b iz ld jc le jf lf jj lg jn lh jr kz la lb lc bi translated"><a class="ae lz" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/blog/how-to-generate</a></li></ul></div></div>    
</body>
</html>