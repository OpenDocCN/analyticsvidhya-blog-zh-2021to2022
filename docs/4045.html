<html>
<head>
<title>Review: MCnet or Motion Content Network for video prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:MCnet或运动内容网络用于视频预测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/review-mcnet-or-motion-content-network-for-video-prediction-6cbd626e5747?source=collection_archive---------7-----------------------#2021-08-20">https://medium.com/analytics-vidhya/review-mcnet-or-motion-content-network-for-video-prediction-6cbd626e5747?source=collection_archive---------7-----------------------#2021-08-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="0243" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated"><a class="ae iw" href="https://arxiv.org/abs/1706.08033v2" rel="noopener ugc nofollow" target="_blank">自然视频序列预测的运动和内容分解，Villegas等人</a></h2></div><p id="f9ea" class="pw-post-body-paragraph ix iy hh iz b ja jb ii jc jd je il jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">在我之前的文章中，我描述了<strong class="iz hi"> FutureGAN </strong> ( <a class="ae iw" rel="noopener" href="/@ad892/review-of-futuregan-predict-future-video-frames-using-generative-adversarial-networks-gans-3120d90d54e0">链接到文章</a>)，这是一个基于GAN的未来帧预测框架。FutureGAN的关键之处在于，它没有使用任何形式的RNNs来模拟视频的时间动态。相反，它使用<em class="jt">时空3d卷积</em>来模拟视频中的内容和运动。然而，我们确实注意到它无法胜过<strong class="iz hi"> MCnet </strong>模型。因此，在本文中，我们将探讨用于视频预测的MCnet模型。</p><p id="a16c" class="pw-post-body-paragraph ix iy hh iz b ja jb ii jc jd je il jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">视频预测具有挑战性，因为与静态图像不同，视频在时域中包含复杂的变换和运动模式。为了解决这个问题，MCnet使用了一种创造性的方法，将视频预测任务分成两半。</p><h1 id="e5af" class="ju jv hh bd jw jx jy jz ka kb kc kd ke in kf io kg iq kh ir ki it kj iu kk kl bi translated">MCnet关键理念:</h1><ol class=""><li id="2895" class="km kn hh iz b ja ko jd kp jg kq jk kr jo ks js kt ku kv kw bi translated">MCnet采用了一种复杂的方法来分解运动和内容，这是产生视频动态的两个关键组成部分。</li><li id="ed25" class="km kn hh iz b ja kx jd ky jg kz jk la jo lb js kt ku kv kw bi translated">这是一个聪明的技巧，因为我们可以将视频预测视为两个独立的任务。首先，预测视频的内容或视频中图像帧的空间布局。第二，预测视频中的运动或时间动态。</li><li id="9b9d" class="km kn hh iz b ja kx jd ky jg kz jk la jo lb js kt ku kv kw bi translated">通过独立地对运动和内容建模，帧预测简化为组合内容和运动特征，这简化了下一帧预测的复杂任务。</li></ol><h1 id="09a7" class="ju jv hh bd jw jx jy jz ka kb kc kd ke in kf io kg iq kh ir ki it kj iu kk kl bi translated">MCnet架构:</h1><ol class=""><li id="2e06" class="km kn hh iz b ja ko jd kp jg kq jk kr jo ks js kt ku kv kw bi translated">MCnet生成器模型建立在编码器-解码器卷积神经网络和卷积LSTM的基础上。</li><li id="1ef9" class="km kn hh iz b ja kx jd ky jg kz jk la jo lb js kt ku kv kw bi translated">编码器-解码器卷积神经网络用于对内容建模。</li><li id="aa80" class="km kn hh iz b ja kx jd ky jg kz jk la jo lb js kt ku kv kw bi translated">卷积LSTM用于模拟运动或时间动态。</li><li id="d5a1" class="km kn hh iz b ja kx jd ky jg kz jk la jo lb js kt ku kv kw bi translated">鉴别器被优化以执行二进制分类，即识别假的和真实的视频帧。</li></ol><p id="c7c6" class="pw-post-body-paragraph ix iy hh iz b ja jb ii jc jd je il jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated"><em class="jt">为了模拟空间和时间动态，MCnet生成器模型c </em>包括以下网络的<em class="jt">:</em></p><ol class=""><li id="8c77" class="km kn hh iz b ja jb jd je jg lc jk ld jo le js kt ku kv kw bi translated">运动编码器</li><li id="def7" class="km kn hh iz b ja kx jd ky jg kz jk la jo lb js kt ku kv kw bi translated">内容编码器</li><li id="17ed" class="km kn hh iz b ja kx jd ky jg kz jk la jo lb js kt ku kv kw bi translated">多尺度运动内容残差</li><li id="04d2" class="km kn hh iz b ja kx jd ky jg kz jk la jo lb js kt ku kv kw bi translated">组合层和解码器</li></ol><h2 id="2864" class="lf jv hh bd jw lg lh li ka lj lk ll ke jg lm ln kg jk lo lp ki jo lq lr kk ls bi translated">运动编码器:</h2><ol class=""><li id="bb33" class="km kn hh iz b ja ko jd kp jg kq jk kr jo ks js kt ku kv kw bi translated">运动编码器捕捉场景组件的时间动态，并用于模拟视频中的运动。</li><li id="7769" class="km kn hh iz b ja kx jd ky jg kz jk la jo lb js kt ku kv kw bi translated">CNN与ConvLSTM一起用于编码器</li></ol><figure class="lu lv lw lx fd ly er es paragraph-image"><div class="er es lt"><img src="../Images/48f16b86b61ba688a8cd38f9b2557451.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*92XMvcC01UaqWdPs08-wWg.jpeg"/></div><figcaption class="mb mc et er es md me bd b be z dx translated">MCnet运动编码器</figcaption></figure><h2 id="4dfa" class="lf jv hh bd jw lg lh li ka lj lk ll ke jg lm ln kg jk lo lp ki jo lq lr kk ls bi translated">内容编码器:</h2><ol class=""><li id="ae65" class="km kn hh iz b ja ko jd kp jg kq jk kr jo ks js kt ku kv kw bi translated">内容编码器从单个帧中提取重要的空间特征，例如场景的空间布局和视频中的显著对象。</li><li id="1793" class="km kn hh iz b ja kx jd ky jg kz jk la jo lb js kt ku kv kw bi translated">它是由专门从单帧中提取特征的卷积神经网络(CNN)实现的。</li></ol><figure class="lu lv lw lx fd ly er es paragraph-image"><div class="er es mf"><img src="../Images/5647250ced90fa4e14ab10416a1b3c53.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*SvEmdNJ0_J6xoqcVsZOJfg.jpeg"/></div><figcaption class="mb mc et er es md me bd b be z dx translated">MCnet内容编码器</figcaption></figure><h2 id="fc15" class="lf jv hh bd jw lg lh li ka lj lk ll ke jg lm ln kg jk lo lp ki jo lq lr kk ls bi translated">多尺度运动内容残差；</h2><ol class=""><li id="3994" class="km kn hh iz b ja ko jd kp jg kq jk kr jo ks js kt ku kv kw bi translated">为了防止在运动和内容编码器中的汇集操作之后信息丢失，使用了剩余连接。</li><li id="fc32" class="km kn hh iz b ja kx jd ky jg kz jk la jo lb js kt ku kv kw bi translated">在去轮询操作之后，网络中的剩余连接将每个尺度的运动内容特征传送到解码器层中。</li></ol><h2 id="9fe7" class="lf jv hh bd jw lg lh li ka lj lk ll ke jg lm ln kg jk lo lp ki jo lq lr kk ls bi translated">组合层和解码器:</h2><ol class=""><li id="dc34" class="km kn hh iz b ja ko jd kp jg kq jk kr jo ks js kt ku kv kw bi translated">来自两个编码器路径的输出分别编码运动和内容的高级表示。给定这些表示，解码器的目标是生成下一帧的像素级预测。</li><li id="d0d8" class="km kn hh iz b ja kx jd ky jg kz jk la jo lb js kt ku kv kw bi translated">为此，它首先通过在深度维度上连接特征向量，将运动和内容组合成统一的表示。</li></ol><figure class="lu lv lw lx fd ly er es paragraph-image"><div class="er es mg"><img src="../Images/244ca24c3a39fd463d66a191d08830c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*cr0Yc8wrW_7y9i1tsU2tng.jpeg"/></div><figcaption class="mb mc et er es md me bd b be z dx translated">MCnet架构</figcaption></figure><h1 id="f9ce" class="ju jv hh bd jw jx jy jz ka kb kc kd ke in kf io kg iq kh ir ki it kj iu kk kl bi translated">MCnet算法概述:</h1><figure class="lu lv lw lx fd ly er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mh"><img src="../Images/3a02d8ad75890ca4feec3bd868a2c8b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aXMmTP5YZ7M2w-eyPXnrHA.png"/></div></div></figure><h1 id="2f23" class="ju jv hh bd jw jx jy jz ka kb kc kd ke in kf io kg iq kh ir ki it kj iu kk kl bi translated">模型培训:</h1><figure class="lu lv lw lx fd ly er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mm"><img src="../Images/beefe802e88afb31ce54ecc9b3ba8f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lAhpvBWBvF-_0S_KGCOOlg.png"/></div></div></figure><h1 id="3fc0" class="ju jv hh bd jw jx jy jz ka kb kc kd ke in kf io kg iq kh ir ki it kj iu kk kl bi translated">结果:</h1><figure class="lu lv lw lx fd ly er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mn"><img src="../Images/861803e9bba52f013550d6da3d438087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-yiiOugd1JY32MmptOCQ1A.png"/></div></div><figcaption class="mb mc et er es md me bd b be z dx translated">MCnet结果比较:给定10个输入帧，模型逐个递归预测20个帧。</figcaption></figure><p id="58cf" class="pw-post-body-paragraph ix iy hh iz b ja jb ii jc jd je il jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">在上图中，我们可以看到，对于KTH和魏茨曼数据集，MCnet + RES模型(残差连接)在所有其他模型中表现最佳。值得注意的一件有趣的事情是，没有RES的MCnet在KTH数据集上的表现比Conv LSTM + RES模型更差。这证明了在MCnet模型中使用RES连接的必要性，因为它通过传递运动内容特征来避免消失梯度问题，从而防止信息丢失。</p><p id="d8a9" class="pw-post-body-paragraph ix iy hh iz b ja jb ii jc jd je il jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">随着时间的推移，由于预测空间和时间动态的不确定性增加，SSIM和PSNR值急剧下降。下图也对此进行了描述。</p><figure class="lu lv lw lx fd ly er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mo"><img src="../Images/9c25eb1f7eb333955d21593c85c221ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XrMvtDcEN0N3l29jLf7Jxw.png"/></div></div><figcaption class="mb mc et er es md me bd b be z dx translated">MCnet结果</figcaption></figure><h1 id="6a94" class="ju jv hh bd jw jx jy jz ka kb kc kd ke in kf io kg iq kh ir ki it kj iu kk kl bi translated">参考:</h1><p id="34d8" class="pw-post-body-paragraph ix iy hh iz b ja ko ii jc jd kp il jf jg mp ji jj jk mq jm jn jo mr jq jr js ha bi translated"><a class="ae iw" href="https://arxiv.org/abs/1706.08033" rel="noopener ugc nofollow" target="_blank">自然视频序列预测的运动和内容分解，Villegas等人</a></p></div></div>    
</body>
</html>