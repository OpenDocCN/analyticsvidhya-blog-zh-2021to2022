# 数据预处理的三大支柱

> 原文：<https://medium.com/analytics-vidhya/three-pillars-of-data-preprocessing-715c032019d8?source=collection_archive---------16----------------------->

![](img/d28eb73be825aebb72895324ffa98771.png)

你们中的许多人可能都熟悉“维数灾难”，这个术语描述了由大量属性引发的问题，这些属性转化为大量的维度。一般来说，维数应该减少到它们所能达到的最有效的最小值，以便计算成本保持在控制之下，同时提取的信息足以解决手头的问题。在阐述前面的论点时，人们应该考虑到，两个特征可能单独提供足够的信息，但由于高度的相互关联性，当结合在一起时，就不再提供任何有意义的信息，因此，在仔细选择哪些属性应该被检查以及以何种相关方式进行检查的背后的推理，从而不会产生不必要的复杂性。

> 训练样本的数量相对于自由参数(例如突触权重)的因子越大，分类器的泛化能力越好。

具有许多特征的数据集会产生大量的自由参数，当样本数量很小时，这很容易成为一个问题。因此，在这种特殊情况下，保持少量的特征是明智的，这样才能满足前面提到的说法。

![](img/4085e298a6b50a55c73d1df364dd5a81.png)

把分母保持在低水平！

随后，在特征之间寻找最佳候选是正常的，这就是*特征约简* ( *或特征选择*)发挥作用的地方。在我们继续深入研究三个主要程序之前，我想给这个主题增加另一个层面，无意双关。坦率地说，正确阐述预先接受这一确切命题的一个好方法是:

> 主要目的是选择那些属性，使得**类间距离**大，而**类内方差**小。

# 离群点移除

离群点被认为是离其相应随机变量的平均值太远的任何数据点。一般来说，这些分数会产生糟糕的结果，并且对培训过程没有任何显著的价值，因此需要采取某些措施来避免这种情况:

> **如果**他们的数量少，我们可以除掉他们。
> 
> **否则，**工程师应选择不受异常值影响的成本函数。

例如，最小二乘误差并非真正不受影响，因为对异常值求平方将导致更大的误差，因此它们在成本函数中占据主导地位。

# 数据规范化

数据集中的可变性等同于不同的要素比例，因为其中一个或多个要素比例可能遵循不同的比例范围，与带宽较小的要素比例相比，这种概念很快转化为成本函数中较大值的优势，如前所述。因此，最正常的步骤是将所有值的总范围拉平，使它们显示为一个共同的参考点。相应地，这意味着建立一个正态分布，其中数据点的平均值为 0，标准偏差等于 1。

![](img/2fdb80faa8ff4f1d0cd2f492fe1a7cb2.png)

平均

![](img/82facee227d9e85cddbaa39dd1941cdf.png)

标准偏差

![](img/135034ffc94b7f8f85ade2de25301893.png)

标准化数据点

值得注意的是，这个过程是其他线性过程中的一个，就像[-1，1]缩放一样。随后，可以利用非线性缩放函数，如 **Softmax** 。

# 缺失数据

因为我们在讨论真实世界的解决方案，所以预计总数据的一部分会丢失是正常的，这种现象在社会科学或预后医学数据集中观察到。那我们该怎么办？本质上，该解决方案被命名为**插补**，并可分为三种可能的候选方案:

> 用**零**替换缺失值，
> 
> 用**条件** **平均值** {E(缺失|观察)}，或
> 
> 用 a 替换缺失值**非条件** **表示**(通过可用观测值计算)。

当然，处理这种情况的简单方法是简单地去除它们，但是当数据集不够大以至于不能进行这种激烈的措施时，这可能会导致问题，从而导致提取的信息减少。

# 结论

数据为王，并将继续呈指数级增长，在使用它来训练我们的机器之前，它通常需要法规和适当的检查。因此，我们应该了解我们的工具，并充分利用它，无论是预处理、收集，还是简单地…观察。