<html>
<head>
<title>Probabilistic Justification for using specific Loss function in different types of Machine Learning Algorithms (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在不同类型的机器学习算法中使用特定损失函数的概率证明(1)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/probabilistic-justification-for-using-specific-loss-function-in-different-types-of-machine-e60fda8146b2?source=collection_archive---------16-----------------------#2021-03-04">https://medium.com/analytics-vidhya/probabilistic-justification-for-using-specific-loss-function-in-different-types-of-machine-e60fda8146b2?source=collection_archive---------16-----------------------#2021-03-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="6142" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过概率在不同类型的监督机器学习算法中使用特定种类的损失函数的数学证明</p><blockquote class="jc jd je"><p id="03d3" class="ie if jf ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">本质上，所有的模型都是错的，但有些是有用的。</p></blockquote><p id="3b90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jj" href="http://en.wikiquote.org/wiki/George_E._P._Box" rel="noopener ugc nofollow" target="_blank">乔治E.P. Box </a></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es jk"><img src="../Images/9f7225719e226bd15a535b12d4c0310e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*erOfakppGYZ7UwUAg28iCQ.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">由<a class="ae jj" href="https://igraph.org/" rel="noopener ugc nofollow" target="_blank"> igraph </a>的联合创作者<a class="ae jj" href="http://hal.elte.hu/~nepusz/" rel="noopener ugc nofollow" target="_blank">塔玛斯·内普斯</a></figcaption></figure><p id="cf2a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我开始研究机器学习时，最麻烦但没有多大意义的事情之一是为不同的机器学习算法选择损失函数，为不同种类的ML算法使用特定损失函数的理由大多数时候是基于直觉，而不是任何正式的数学证明</p><p id="eb09" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢概率论，今天我们将通过概率解释，以最数学合理、一致和完整的方式回答我们难以捉摸的问题。</p><p id="3bd4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我将使用来自斯坦福大学的<a class="ae jj" href="http://cs229.stanford.edu/" rel="noopener ugc nofollow" target="_blank"> CS229:机器学习</a>中的材料(与斯坦福大学的<a class="ae jj" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习课程(吴恩达)相比，本课程更高级，需要更深入的数学理解</a>本课程的先决条件包括熟悉多变量微积分、概率论和线性代数)</p><p id="2fd8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但首先，有一点背景</p><h1 id="1945" class="jw jx hh bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">背景</h1><p id="6ecf" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">对于几乎所有的初学者来说，你在ML旅程开始时体验到的机器学习的第一种味道是回归问题，这是绝对有意义的，因为掌握回归的概念只需要生疏的高中线性代数和微积分，你需要理解的只是跟随花哨的术语</p><p id="ceaf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">假设</strong></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es kz"><img src="../Images/9e32710ddd33b1d53aa53dd18aa1c5c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Axkz3P8lIL9wluZQtIwlqA.jpeg"/></div></div></figure><p id="8a59" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参数</strong></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es le"><img src="../Images/9c727fa6c9c24fdedf08d079df71f0a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*wmwN9a5udYUKP1sfkNKhdQ.jpeg"/></div></figure><p id="80df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">成本函数</strong></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lf"><img src="../Images/6e29f324c58d37215bbb4be8afe0b276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rXhpV1NMRbOQMXyKqcFPLw.jpeg"/></div></div></figure><p id="8ec0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">优化算法</strong></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lg"><img src="../Images/3d2acfe5466df6c15eeb2757749b1363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_yWIBLDKCZZVOQRR94UM2Q.jpeg"/></div></div></figure><p id="b750" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这一点上，我假设你熟悉回归和优化算法的机制，或者至少熟悉梯度下降或梯度上升</p><p id="5932" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意一件事，术语<strong class="ig hi">成本函数</strong>通常更通用，它是训练集上的<strong class="ig hi">损失函数</strong>加上一些模型复杂度惩罚(正则化)的总和。</p><blockquote class="jc jd je"><p id="37fa" class="ie if jf ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">成本函数J通常用于评估模型的性能，并由损失函数L定义如下:</p></blockquote><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lh"><img src="../Images/cf6ae30c2f258e4e1d5349042ba04c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*sQU13ApL1kMGqGUVN6xA0g.jpeg"/></div></div></figure><p id="9142" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果损失是凸形，意味着你的成本函数也是凸的，同样，如果损失是凹形，意味着你的成本函数也是凹的</p><p id="2212" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在梯度下降的情况下，当函数是凸的(下图)时，我们在寻找全局最小值，而在梯度上升的情况下，当函数是凹的时，我们在寻找全局最大值。全局最大值和全局最小值都是成本函数的临界点。</p><p id="aeaf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">具有全局最小值或最大值的成本函数是机器学习算法的期望属性，因为我们可以使用分析和数值优化方法计算出参数值，该参数值给出了成本函数的最大值/最小值。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es li"><img src="../Images/f92aa8271411ada94af42b11b0725e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*wsBakfF2Geh1zgY4HJbwFQ.gif"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jj" href="https://alykhantejani.github.io/images/gradient_descent_line_graph.gif" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><figure class="jl jm jn jo fd jp"><div class="bz dy l di"><div class="lj lk l"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">这里快速回顾一下python代码在成本函数MSE上的梯度下降，以便从头开始回归</figcaption></figure><h1 id="601a" class="jw jx hh bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">为什么用MSE作为回归的损失函数？</h1><p id="ea94" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">现在在回归问题的情况下(多元/单变量线性或多项式回归)，你可能已经注意到我们几乎总是使用均方差(MSE)作为成本函数</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ll"><img src="../Images/6ad199ef6ee8646141b92c3528458f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*50dwPCSNYNdOsdAJ3WmS8A.jpeg"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">由<a class="ae jj" rel="noopener" href="/@rohanhirekerur">罗汉</a>上<a class="ae jj" rel="noopener" href="/analytics-vidhya/a-comprehensive-guide-to-loss-functions-part-1-regression-ff8b847675d6">中</a></figcaption></figure><p id="bb8a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">问题仍然是，当你有太多的其他凸函数(如上图)可以用作回归的成本函数时，我们为什么要使用均方差(MSE)作为成本函数。此外，与平均绝对误差(MAE)相比，我们的均方误差(MSE)对异常值也更敏感，我们知道绝大多数的<strong class="ig hi">异常值都是噪声</strong>，因此MAE对异常值的鲁棒性是我们应该关注的，对吗？</p><p id="f9f2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通常，您会看到从微积分角度给出的选择均方误差(MSE)而不是平均绝对误差(MAE)作为成本函数的基本原理，</p><blockquote class="jc jd je"><p id="c1be" class="ie if jf ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">函数在急转弯处的导数是未定义的，这意味着导数的图形在急转弯处是不连续的。</p></blockquote><p id="2ba2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">平均绝对误差(MAE)的形状显示在左侧，我们的均方误差形状显示在右侧</p><div class="jl jm jn jo fd ab cb"><figure class="lm jp ln lo lp lq lr paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><img src="../Images/c161aa3f772804a9c7126d1a0e2a905a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*aWKoWV3EgWoN4Pe7gIkXbw.gif"/></div></figure><figure class="lm jp ln lo lp lq lr paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><img src="../Images/4c5867f51fbfd730ed522d226f545032.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*ZGwHMhK1CxwTJRKpUsEqpw.gif"/></div><figcaption class="js jt et er es ju jv bd b be z dx ls di lt lu translated">我们在一个连续体上有x的梯度，其形状与上图(右)中的均方误差(MSE)相同，我们在一个连续体上有|x|的梯度，其形状与上图(左)中的平均绝对误差(MAE)相同<a class="ae jj" href="https://math.stackexchange.com/users/132791/alice-ryhl" rel="noopener ugc nofollow" target="_blank">切线动画来源:Alice Ryhl </a> l</figcaption></figure></div><p id="ee8c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们有x在连续统上的梯度，它的形状与均方误差(MSE)相同(右),我们有|x|在连续统上的梯度，它的形状与平均绝对误差(MAE)相同(左)</p><p id="ae96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">很明显，在RHS图中，平均绝对误差(MAE)有一个急转弯，这使得它在寻找全局极值时效率很低，但还有一些其他情况请注意，平均绝对误差(MAE)的梯度如何始终保持不变，直到它达到x=0，并在x=0时从LHS和RHS(右图)突然变化。现在将它与接近x=0时平滑变化的均方误差(MSE)的梯度进行比较，而不是从一条线到另一条线的突然变化(左图)嗯，好吧，但它与我们的问题有什么关系？</p><p id="542b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你记得梯度下降，我们有<strong class="ig hi">超参数</strong> α，也称为<strong class="ig hi">学习率</strong>，我们只根据它的值分配一次，我们向全局极值迈出每一步，如果α的值小，我们采取小的步骤，我们几乎总是向极值收敛，如果α的值大，我们可能会发散，所以通常我们满足于α的平衡值，但是在像平均绝对误差(MAE)这样的成本函数的情况下，我们必须根据我们对它的梯度的观察，在每一步上改变这个值</p><p id="ab3c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">但是但是</strong> …如果我们使用Huber损失来解决所有与右转相关的问题呢？此外，那不是数学证明，我们需要一些数学证明！这正是我接下来要展示给你们的</p><h1 id="cee9" class="jw jx hh bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">回归的概率解释</h1><p id="9144" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">让我们假设在y(响应变量)和x(独立变量)之间存在某种关系，其中ϵ是一个随机误差项，它独立于x，并且具有一个平均值0。请记住，这个ϵ是一个不可约误差，它捕捉未建模的效应或随机的，y是真值</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lv"><img src="../Images/15fdccb1bde8e5b5a1b19f987a92f9ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*7OWKiP4vnLWzvCU1MUkePA.jpeg"/></div></figure><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lw"><img src="../Images/11fcbacf73711dfb8f4d24691c3513ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*DKdoAR4Y6vkhQKsYM7CIdA.jpeg"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">对于单个例子(x(i)，y(i))</figcaption></figure><p id="8986" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，我们可以看到，误差项是y(i)，x(i)的函数，并由θ参数化</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lx"><img src="../Images/d42aab1baa1d82d90851bb2a2bff3d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*NAVz0U6kl6X4pkzuEZhMgw.jpeg"/></div></figure><p id="32cd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我希望大家大胆假设这些ϵ(i误差项按照均值为零、方差为σ的高斯分布(正态分布)分布</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ly"><img src="../Images/8e08f08ca577d51a9e7aeda3a46334f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*IWkqlddHuFffJMldVmcCVw.jpeg"/></div></figure><p id="ff66" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，ϵ(i的<strong class="ig hi">概率密度函数</strong> ( <strong class="ig hi"> PDF </strong>)由下式给出</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lz"><img src="../Images/2ecede91ac425a5b80d9eb375c8fb534.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*8eP7tCOZeoaQBCUdkYvXVg.jpeg"/></div></figure><p id="a188" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将上式中的值代入e(i)</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ma"><img src="../Images/0bdb013d378f1551ad5ed6681fd33840.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZlynzEcye3C23cUeFXYlw.jpeg"/></div></div></figure><p id="f6be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">单个例子的概率由p(y(i)∣x(i给出)；θ))那么数据的概率由p(y，x；θ).你读p(y(i)，x(i)的方式；θ)是给定x(i)并由θ参数化的y(i)的概率</p><blockquote class="jc jd je"><p id="cbe2" class="ie if jf ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">这个量p(y，x；θ)通常被视为y(或许还有X)的函数，<br/>为θ的固定值。当我们希望明确地将其视为θ的函数时，我们将称之为<strong class="ig hi">似然函数</strong></p></blockquote><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es mb"><img src="../Images/bd6208804612b74a52d0f6cf1bdda3f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*hA4h0d8Ka9NM9V-hcZVTWg.jpeg"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><strong class="bd jy">似然函数</strong></figcaption></figure><p id="ba25" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个L(θ)是我们的数据的概率，这意味着它是从y(i)到y(m)的所有y值的概率(其中m是数据实例的数量)，给定从x(i)到x(m)的所有x，参数化的θ应该等于这个</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es mc"><img src="../Images/cc356cb275c629f5c07a8ce7ff11257f.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*59h1h5zx7vkDJKne4bjYGw.jpeg"/></div></figure><p id="c782" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个L(θ)等于y(i)的概率的乘积，因为我们假设ϵ(i是独立同分布的，我们知道当概率独立时</p><blockquote class="jc jd je"><p id="1a29" class="ie if jf ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">如果事件是独立的，那么它们同时发生的概率是它们各自发生的概率的乘积。</p></blockquote><p id="3bf2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">并代入p(y(i)∣x(i)；θ))从上面我们得到这个</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es md"><img src="../Images/0a47dd3e4c447aab9d0360e1ebafdfd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*njZfKsFqgY-Ixv1JOvNd_w.jpeg"/></div></figure><p id="6935" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我将取我们的<strong class="ig hi">似然函数L(θ) </strong>的对数，并将其称为<strong class="ig hi">对数似然𝔏(θ) </strong></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es me"><img src="../Images/5802ff7b79d12740dd94d06023e452fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*s0hAM0e3bvKVPZvS1YrxYQ.jpeg"/></div></figure><p id="d711" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，给定这个关于y(i)和x(i)的概率模型，选择参数<strong class="ig hi"> θ的最佳猜测的合理方式是什么？</strong></p><p id="61b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">最大似然</strong>原理说，我们应该选择使得<strong class="ig hi"> θ </strong>使数据尽可能高的概率。即，我们应该选择<strong class="ig hi"> θ </strong>来最大化<strong class="ig hi">对数似然𝔏(θ) </strong></p><p id="0c17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，最大化<strong class="ig hi">对数似然𝔏(θ) </strong>忽略所有常数项<strong class="ig hi"> </strong>给我们这个</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mf"><img src="../Images/8523b200afa5f83e39a98769694a8179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pAFNAddqUdWPEM3ICn9L9g.jpeg"/></div></div></figure><p id="a2e0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">“眼熟”？是的，这是我们熟悉的回归损失函数的均方误差(MSE ),因此这个证明表明，选择<strong class="ig hi"> θ </strong>的值来最小化均方误差(MSE)与在我们所做的一组假设下找到参数<strong class="ig hi"> θ </strong>的<strong class="ig hi">最大似然估计</strong>是一回事</p><p id="e67d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">万岁！现在我们有了数学上合理、一致和清晰的证据，证明为什么我们在回归问题中使用均方差(MSE)作为成本函数</p><h1 id="a3c8" class="jw jx hh bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">结论</h1><p id="5acc" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">今天，我们已经看到了通过概率解释使用均方误差(MSE)作为回归成本函数的数学理由。这个博客是三个博客之一，下一个博客将是关于交叉熵损失函数作为逻辑回归的成本函数，最后一个博客将是关于支持向量机(SVM)及其成本函数</p></div></div>    
</body>
</html>