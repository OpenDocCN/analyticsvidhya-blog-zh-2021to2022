<html>
<head>
<title>Vision Transformer: “Attention in Images”</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">视觉转换器:“图像中的注意力”</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/vision-transformer-attention-in-images-a86d0a8adc1b?source=collection_archive---------7-----------------------#2021-07-18">https://medium.com/analytics-vidhya/vision-transformer-attention-in-images-a86d0a8adc1b?source=collection_archive---------7-----------------------#2021-07-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="dd50" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">虽然NLP的大部分成功可以归功于Transformer架构“注意力是你所需要的”，但它的应用仍然局限于图像分析领域。就计算机视觉而言，CNN架构仍然是最受欢迎的方法之一。在本文中，我将重点介绍使用图像块作为输入的视觉转换器，以及用于图像分类的转换器架构的编码器部分。最后，我将展示如何使用拥抱脸接口轻松实现这个架构。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jj"><img src="../Images/15153dc922126df3a191d883cd9dcbce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1TYyyqCx4IX_pqvSJoXoNA.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">一幅图像相当于16x16个字:大规模图像识别的变形金刚。arXiv预印本arXiv:2010.11929  (2020)。</figcaption></figure></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><h1 id="f005" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">内容:</h1><ul class=""><li id="8134" class="ky kz hh in b io la is lb iw lc ja ld je le ji lf lg lh li bi translated">变压器架构</li><li id="91d6" class="ky kz hh in b io lj is lk iw ll ja lm je ln ji lf lg lh li bi translated">视觉变压器</li><li id="0027" class="ky kz hh in b io lj is lk iw ll ja lm je ln ji lf lg lh li bi translated">Python实现</li><li id="292b" class="ky kz hh in b io lj is lk iw ll ja lm je ln ji lf lg lh li bi translated">对比——CNN与ViT</li><li id="f635" class="ky kz hh in b io lj is lk iw ll ja lm je ln ji lf lg lh li bi translated">前进的道路</li></ul></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><h1 id="8d0c" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">变压器架构</h1><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es lo"><img src="../Images/c74e8068d68a21f7915893d54b408197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_joIeRrpQMElcMVof-FgAQ.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">你所需要的只是关注。<em class="jz">神经信息处理系统的进展</em>。2017.</figcaption></figure><p id="99a8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在深入研究Vision Transformer之前，我们需要了解Transformer架构的基本概念及其工作原理。我不会深入探讨，但会提到几个对理解视觉转换器很重要的方面。</p><p id="0520" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">变压器架构由两部分组成——编码器和解码器。在本文中，我将重点关注转换器的编码器部分，因为这部分将在图像分类任务的视觉转换器中使用。</p><p id="1833" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">编码器将嵌入的文本作为输入，将其通过6个相同的层，每个层由2个子层组成。第一子层是多头自关注机制，第二子层是全连接前馈网络。注意机制可以被视为3个加权向量(k，q，v)的列表，每个向量基于输入文本中单词的重要性被分配不同的权重。要了解更多细节，我建议浏览这篇文章:</p><div class="lp lq ez fb lr ls"><a href="https://becominghuman.ai/attention-is-all-you-need-16bf481d8b5c" rel="noopener  ugc nofollow" target="_blank"><div class="lt ab dw"><div class="lu ab lv cl cj lw"><h2 class="bd hi fi z dy lx ea eb ly ed ef hg bi translated">你需要的只是关注</h2><div class="lz l"><h3 class="bd b fi z dy lx ea eb ly ed ef dx translated">关于变压器的解释</h3></div><div class="ma l"><p class="bd b fp z dy lx ea eb ly ed ef dx translated">becominghuman.ai</p></div></div><div class="mb l"><div class="mc l md me mf mb mg jt ls"/></div></div></a></div><h1 id="601c" class="ka kb hh bd kc kd mh kf kg kh mi kj kk kl mj kn ko kp mk kr ks kt ml kv kw kx bi translated">视觉转换器(ViT)</h1><p id="527c" class="pw-post-body-paragraph il im hh in b io la iq ir is lb iu iv iw mm iy iz ja mn jc jd je mo jg jh ji ha bi translated">现在，我们已经了解了标准transformer架构是如何工作的，让我们看看如何将它用于图像分类任务。ViT不是采用1D序列的记号嵌入，而是采用位置嵌入和面片嵌入的和，它们是展平的2D面片的线性投影的D维序列。视觉变换器学习在位置嵌入的相似性中对图像内的距离进行编码，即，更近的片趋向于具有更相似的位置嵌入。自我关注层帮助模型学习信息，甚至从图像的最低层学习信息。最后，编码的输入通过MLP(多层感知)层，该层预测K个类别中的1个。</p><p id="5e4c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">模型训练包括2个步骤:在第一步中，模型在大型数据集上进行预训练，然后为了在较小的下游任务上进行微调，预训练的预测头被移除并由D x K前馈层代替。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><h1 id="0b20" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">Python实现</h1><p id="8eda" class="pw-post-body-paragraph il im hh in b io la iq ir is lb iu iv iw mm iy iz ja mn jc jd je mo jg jh ji ha bi translated">现在我们已经了解了视觉转换器的工作原理，让我们使用流行的拥抱脸接口来实现一个用于图像分类的基本预训练ViT模型。</p><pre class="jk jl jm jn fd mp mq mr ms aw mt bi"><span id="ce78" class="mu kb hh mq b fi mv mw l mx my">from transformers import ViTFeatureExtractor, ViTForImageClassification<br/>from PIL import Image<br/>import requests</span><span id="1fca" class="mu kb hh mq b fi mz mw l mx my">url = '<a class="ae na" href="https://images.app.goo.gl/9TBrJ5JtD3qYNmQJ9" rel="noopener ugc nofollow" target="_blank">https://images.app.goo.gl/9TBrJ5JtD3qYNmQJ9</a>'<br/>image = Image.open(requests.get(url, stream=True).raw)</span><span id="98d7" class="mu kb hh mq b fi mz mw l mx my">feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')<br/>model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')</span><span id="89d0" class="mu kb hh mq b fi mz mw l mx my">inputs = feature_extractor(images=image, return_tensors="pt")<br/>outputs = model(**inputs)<br/>logits = outputs.logits<br/><br/>predicted_class_idx = logits.argmax(-1).item()<br/>print("Predicted class:", model.config.id2label[predicted_class_idx])</span></pre><p id="be31" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">上述代码也可以从拥抱脸网站轻松找到:</p><div class="lp lq ez fb lr ls"><a href="https://huggingface.co/google/vit-base-patch16-224" rel="noopener  ugc nofollow" target="_blank"><div class="lt ab dw"><div class="lu ab lv cl cj lw"><h2 class="bd hi fi z dy lx ea eb ly ed ef hg bi translated">google/vit-base-patch16-224拥抱脸</h2><div class="lz l"><h3 class="bd b fi z dy lx ea eb ly ed ef dx translated">Vision Transformer (ViT)模型在ImageNet-21k(1400万张图像，21，843个类别)上预先训练，分辨率为224x224…</h3></div><div class="ma l"><p class="bd b fp z dy lx ea eb ly ed ef dx translated">huggingface.co</p></div></div><div class="mb l"><div class="nb l md me mf mb mg jt ls"/></div></div></a></div><p id="1de9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">上面的代码加载了ViT特征提取器和ViT图像分类函数，以使用Google开源的预训练模型。ViT特征提取器将图像作为输入，并通过位置嵌入将其转换为16x16的面片，以使其适合于输入到模型中。模型“Google/vit-base-patch 16–224”在图像分辨率为224x224的ImageNet数据集上进行了预训练，用于预测1000个ImageNet类中的一个。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><h1 id="0f4b" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">对比——CNN与ViT</h1><p id="87ca" class="pw-post-body-paragraph il im hh in b io la iq ir is lb iu iv iw mm iy iz ja mn jc jd je mo jg jh ji ha bi translated">尽管Vision transformer的表现令人难以置信，但它的大部分成功都要归功于大规模的预训练数据集。作者声称，在小数据集上，CNN的表现仍然比ViT好得多，但随着训练数据样本的增加，ViT优于所有其他状态的art模型。</p><h1 id="94c7" class="ka kb hh bd kc kd mh kf kg kh mi kj kk kl mj kn ko kp mk kr ks kt ml kv kw kx bi translated">前进的道路</h1><p id="2f92" class="pw-post-body-paragraph il im hh in b io la iq ir is lb iu iv iw mm iy iz ja mn jc jd je mo jg jh ji ha bi translated">现在，由于图像分类变换器模型的存在，可以做更多的探索来改进自监督训练方法。在序列到序列变换器模型中，也可以开发用于对象检测和图像分割任务的基于变换器的方法。最后，ViT的进一步扩展可以减少微调时间并提高性能。</p></div></div>    
</body>
</html>