<html>
<head>
<title>Azure ML — ParallelRunStep — Run Batch Training in scale</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Azure ML — ParallelRunStep —大规模运行批量培训</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/azure-ml-parallelrunstep-run-batch-training-in-scale-1b7c0a62e19a?source=collection_archive---------8-----------------------#2021-09-17">https://medium.com/analytics-vidhya/azure-ml-parallelrunstep-run-batch-training-in-scale-1b7c0a62e19a?source=collection_archive---------8-----------------------#2021-09-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="b0ff" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">如何大规模并行运行批量培训</h1><h1 id="d344" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">用例</h1><ul class=""><li id="c0f8" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">并行运行模型进行训练</li><li id="3f0f" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">基于数据大小进行扩展</li></ul><h1 id="47d1" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">密码</h1><ul class=""><li id="abf0" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">首先创建一个存储帐户</li><li id="ef01" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">创建一个名为泰坦尼克号的目录</li><li id="7f46" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">从数据文件夹上传至少2个Tiantic.csv。对于第二个文件，复制并重命名为Titanic1.csv</li><li id="b244" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">现在创建一个笔记本</li></ul><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="e67f" class="ki if hh ke b fi kj kk l kl km">from azureml.core import Workspace<br/>ws = Workspace.from_config()</span><span id="437f" class="ki if hh ke b fi kn kk l kl km">from azureml.core import Workspace, Dataset</span><span id="08e3" class="ki if hh ke b fi kn kk l kl km">subscription_id = 'xxxxxxxxxxxxxxxxxxxxx'<br/>resource_group = 'xxxxxxxxxxxxxxx'<br/>workspace_name = 'xxxxxxxxxxxxxxxx'</span><span id="b0f1" class="ki if hh ke b fi kn kk l kl km">workspace = Workspace(subscription_id, resource_group, workspace_name)</span><span id="f1c3" class="ki if hh ke b fi kn kk l kl km">dataset = Dataset.get_by_name(workspace, name='TitanicTraining')<br/>data = dataset.to_pandas_dataframe()</span></pre><ul class=""><li id="029c" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated">现在创建一个数据存储</li><li id="9e53" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">这将访问blob存储</li><li id="33c3" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">创建一个数据存储“titanicstore”</li></ul><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="90c2" class="ki if hh ke b fi kj kk l kl km">from azureml.core.datastore import Datastore</span><span id="c938" class="ki if hh ke b fi kn kk l kl km">account_name = "storageacctname"<br/>datastore_name="titanicstore"<br/>container_name="titanic"</span><span id="8740" class="ki if hh ke b fi kn kk l kl km">titanic_data = Datastore.register_azure_blob_container(ws, <br/>                      datastore_name=datastore_name, <br/>                      container_name= container_name, <br/>                      account_name=account_name, <br/>                      overwrite=True)</span></pre><ul class=""><li id="db3b" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated">从上述数据存储创建数据集</li></ul><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="ccbb" class="ki if hh ke b fi kj kk l kl km">from azureml.core.dataset import Dataset</span><span id="8fc4" class="ki if hh ke b fi kn kk l kl km">titanic_ds_name = 'titanic_data'</span><span id="f7f7" class="ki if hh ke b fi kn kk l kl km">path_on_datastore = titanic_data.path('/')<br/>input_titanic_ds = Dataset.Tabular.from_delimited_files(path=path_on_datastore, validate=False)<br/>named_titanic_ds = input_titanic_ds.as_named_input(titanic_ds_name)</span></pre><ul class=""><li id="f1bc" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated">设置输出文件夹配置</li></ul><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="fdf2" class="ki if hh ke b fi kj kk l kl km">from azureml.core.dataset import Dataset<br/>from azureml.data import OutputFileDatasetConfig</span><span id="8cc6" class="ki if hh ke b fi kn kk l kl km">output_dir = OutputFileDatasetConfig(name="scores")</span></pre><ul class=""><li id="a754" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated">创建要运行的计算群集</li></ul><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="47b5" class="ki if hh ke b fi kj kk l kl km">from azureml.core.compute import AmlCompute, ComputeTarget<br/>from azureml.exceptions import ComputeTargetException<br/>compute_name = "cpu-cluster"</span><span id="ed4e" class="ki if hh ke b fi kn kk l kl km"># checks to see if compute target already exists in workspace, else create it<br/>try:<br/>    compute_target = ComputeTarget(workspace=ws, name=compute_name)<br/>except ComputeTargetException:<br/>    config = AmlCompute.provisioning_configuration(vm_size="Standard_F16s_v2",<br/>                                                   vm_priority="dedicated", <br/>                                                   min_nodes=0, <br/>                                                   max_nodes=4)</span><span id="9d5d" class="ki if hh ke b fi kn kk l kl km">    compute_target = ComputeTarget.create(workspace=ws, name=compute_name, provisioning_configuration=config)<br/>    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)</span></pre><ul class=""><li id="d678" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated">编写批量训练代码</li></ul><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="844b" class="ki if hh ke b fi kj kk l kl km">%%writefile batch_train.py<br/># azureml-core of version 1.0.72 or higher is required<br/># azureml-dataprep[pandas] of version 1.1.34 or higher is required<br/>import io<br/>import pickle<br/>import argparse<br/>import numpy as np<br/>import pandas as pd</span><span id="f758" class="ki if hh ke b fi kn kk l kl km">import joblib<br/>import os<br/>import urllib<br/>import shutil<br/>import azureml</span><span id="8e2a" class="ki if hh ke b fi kn kk l kl km">from azureml.core.model import Model<br/>from azureml.core import Workspace, Dataset<br/>from sklearn.model_selection import train_test_split</span><span id="5a69" class="ki if hh ke b fi kn kk l kl km">#model_path = "model_08102021.pkl"</span><span id="bc3b" class="ki if hh ke b fi kn kk l kl km">#model = joblib.load(model_path)</span><span id="bd8f" class="ki if hh ke b fi kn kk l kl km">#data = dataset.to_pandas_dataframe().drop(columns="Survived")</span><span id="fc65" class="ki if hh ke b fi kn kk l kl km">#result = model.predict(data)<br/>def init():<br/>    global test_model</span><span id="2e04" class="ki if hh ke b fi kn kk l kl km">    model_path = Model.get_model_path("titanic")</span><span id="a92c" class="ki if hh ke b fi kn kk l kl km">    #model_path = Model.get_model_path(args.model_name)<br/>    #with open(model_path, 'rb') as model_file:<br/>    #    test_model = joblib.load(model_file)</span><span id="e22e" class="ki if hh ke b fi kn kk l kl km">def run(mini_batch):<br/>    result = []<br/>    # Load the dataset<br/>    subscription_id = 'xxxxxxxxxxxxxxxxxxxxxx'<br/>    resource_group = 'xxxx'<br/>    workspace_name = 'xxxx'</span><span id="de80" class="ki if hh ke b fi kn kk l kl km">    workspace = Workspace(subscription_id, resource_group, workspace_name)</span><span id="8efd" class="ki if hh ke b fi kn kk l kl km">    dataset = Dataset.get_by_name(workspace, name='TitanicTraining')<br/>    df = dataset.to_pandas_dataframe()<br/>    df1 = pd.get_dummies(df)<br/>    df2 = df1.dropna()<br/>    <br/>    y = df2["Survived"]<br/>    X = df2.drop(columns="Survived")<br/>    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)<br/>    from sklearn.ensemble import RandomForestClassifier<br/>    # define dataset<br/>    # define the model<br/>    model = RandomForestClassifier()<br/>    # fit the model<br/>    model.fit(X_train, y_train)<br/>    y_pred = model.predict(X_test)<br/>    from sklearn.metrics import classification_report<br/>    from sklearn.metrics import roc_auc_score<br/>    from sklearn.metrics import f1_score<br/>    print(classification_report(y_test, y_pred))<br/>    from sklearn.metrics import accuracy_score<br/>    accuracy_score(y_test, y_pred)<br/>    print("roc_auc_score: ", roc_auc_score(y_test, y_pred))<br/>    print("f1 score: ", f1_score(y_test, y_pred))<br/>    # get importance<br/>    importance = model.feature_importances_<br/>    # summarize feature importance<br/>    for i,v in enumerate(importance):<br/>        print('Feature: %0d, Score: %.5f' % (i,v))<br/>    <br/>    pd.options.mode.chained_assignment = None<br/>    result = X_test<br/>    result['predictoutput'] = y_pred<br/>    #result = y_pred</span><span id="2780" class="ki if hh ke b fi kn kk l kl km">    return result</span></pre><ul class=""><li id="a450" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated">环境</li></ul><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="4413" class="ki if hh ke b fi kj kk l kl km">from azureml.core import Environment<br/>from azureml.core.conda_dependencies import CondaDependencies<br/>from azureml.core.runconfig import DEFAULT_CPU_IMAGE</span><span id="9d61" class="ki if hh ke b fi kn kk l kl km">cd = CondaDependencies.create(pip_packages=["azureml-train-automl-runtime==1.32.0","inference-schema","azureml-interpret==1.32.0","azureml-defaults==1.32.0", "numpy&gt;=1.16.0,&lt;1.19.0",<br/>"pandas==0.25.1","scikit-learn==0.22.1", "fbprophet==0.5","holidays==0.9.11","psutil&gt;=5.2.2,&lt;6.0.0", "xgboost&lt;=0.90"])</span><span id="97df" class="ki if hh ke b fi kn kk l kl km">env = Environment(name="parallelenv")<br/>env.python.conda_dependencies = cd<br/>env.docker.base_image = DEFAULT_CPU_IMAGE</span></pre><ul class=""><li id="628d" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated">并行配置的配置</li></ul><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="91aa" class="ki if hh ke b fi kj kk l kl km">from azureml.pipeline.steps import ParallelRunConfig</span><span id="87cd" class="ki if hh ke b fi kn kk l kl km">parallel_run_config = ParallelRunConfig(<br/>    environment=env,<br/>    entry_script="batch_train.py",<br/>    #source_directory=".",<br/>    output_action="summary_only",<br/>    mini_batch_size="1MB",<br/>    error_threshold=-1,<br/>    allowed_failed_count=1,<br/>    compute_target=compute_target,<br/>    process_count_per_node=16,<br/>    node_count=4<br/>)</span><span id="4321" class="ki if hh ke b fi kn kk l kl km">from azureml.pipeline.steps import ParallelRunStep<br/>from datetime import datetime</span><span id="22ed" class="ki if hh ke b fi kn kk l kl km">parallel_step_name = "batchtraining-" + datetime.now().strftime("%Y%m%d%H%M")</span><span id="51c3" class="ki if hh ke b fi kn kk l kl km">label_config = Dataset.get_by_name(workspace, name='TitanicTraining')</span><span id="beee" class="ki if hh ke b fi kn kk l kl km">batch_score_step = ParallelRunStep(<br/>    name=parallel_step_name,<br/>    inputs=[named_titanic_ds],<br/>    #inputs=[label_ds],<br/>    output=output_dir,<br/>    #arguments=["--model_name", "inception",<br/>    #           "--labels_dir", label_config],<br/>    #side_inputs=[label_config],<br/>    parallel_run_config=parallel_run_config,<br/>    allow_reuse=False<br/>)</span><span id="bcae" class="ki if hh ke b fi kn kk l kl km">from azureml.core import Experiment<br/>from azureml.pipeline.core import Pipeline</span><span id="be59" class="ki if hh ke b fi kn kk l kl km">pipeline = Pipeline(workspace=ws, steps=[batch_score_step])<br/>pipeline_run = Experiment(ws, 'Tutorial-Batch-Training').submit(pipeline)<br/>pipeline_run.wait_for_completion(show_output=True)</span></pre><ul class=""><li id="c0e5" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated">下载结果并显示</li></ul><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="1220" class="ki if hh ke b fi kj kk l kl km">import pandas as pd</span><span id="f6c1" class="ki if hh ke b fi kn kk l kl km">batch_run = next(pipeline_run.get_children())<br/>batch_output = batch_run.get_output_data("scores")<br/>batch_output.download(local_path="inception_results")</span><span id="e648" class="ki if hh ke b fi kn kk l kl km">for root, dirs, files in os.walk("inception_results"):<br/>    for file in files:<br/>        if file.endswith("parallel_run_step.txt"):<br/>            result_file = os.path.join(root, file)</span><span id="198a" class="ki if hh ke b fi kn kk l kl km">df = pd.read_csv(result_file, delimiter=":", header=None)<br/>#df.columns = ["Filename", "Prediction"]<br/>print("Prediction has ", df.shape[0], " rows")<br/>df.head(10)</span></pre></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><p id="42c6" class="pw-post-body-paragraph la lb hh je b jf ko lc ld jh kp le lf jj lg lh li jl lj lk ll jn lm ln lo jp ha bi translated"><em class="lp">最初发表于</em><a class="ae lq" href="https://github.com/balakreshnan/Samples2021/blob/main/DataScience/ParallelTabularTraining.md" rel="noopener ugc nofollow" target="_blank"><em class="lp">【https://github.com】</em></a><em class="lp">。</em></p></div></div>    
</body>
</html>