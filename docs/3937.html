<html>
<head>
<title>RGB-D Salient Object Detection Using the Siamese Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用暹罗网络的RGB-D显著目标检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/rgb-d-salient-object-detection-using-the-siamese-network-3ff20ea04955?source=collection_archive---------8-----------------------#2021-08-10">https://medium.com/analytics-vidhya/rgb-d-salient-object-detection-using-the-siamese-network-3ff20ea04955?source=collection_archive---------8-----------------------#2021-08-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/e2ceab0ded898b27d0b33ff5ddc55f3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*yfV3GrGuCuaGwBNkDd3XMQ.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">RGB _图像，地面_真实，模型_预测</figcaption></figure><h1 id="5dfe" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">简介:</h1><p id="fcdb" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">显著目标检测(SOD)只不过是检测或识别图像中的目标，当人类看到该图像时，他/她通常关注该图像。已经开发了许多模型来使用RGB图像和深度图像进行SOD，其中一些也是最先进的模型。这些现有的模型分别处理RGB图像和深度图像，以提取特征并融合这些特征用于最终预测。这些特征融合可以通过三种方式完成。早期融合2。晚期融合和3。中间融合如图。由于这些模型独立地从RGB和深度图像中提取特征，因此它们生成大量的参数，并且它们需要大量的数据来训练。</p><figure class="km kn ko kp fd ii er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es kl"><img src="../Images/dc32bb0a3687ab4df7e7a611d528cf48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*FkR_QtTjlaXNkfGKME5iiQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">图片来自研究论文</figcaption></figure><p id="86e2" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">在这里，我们正在实现一种新颖的联合学习和密集协作融合(JL-DCF)架构，这在<a class="ae kz" href="https://arxiv.org/pdf/2008.12134.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hi">这篇</strong> </a>论文中有所解释。与现有模型不同的是，该方法通过一个共享主干网络同时从RGB图像和深度图像中提取特征。它使用中间融合方法来融合特征。由于该方法使用单个CNN类型的网络来从两个输入中提取特征，因此它具有更少的参数和存储器，在计算方面比现有模型更好。模型的架构如下所示。</p><figure class="km kn ko kp fd ii er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es la"><img src="../Images/86e344c2e6b3db070783d9957abb86b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w2LDO9xb-kp0Qp90W_BeSQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">图片来自研究论文</figcaption></figure><p id="b865" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">该框架由两个模块组成:如上所示的联合学习和密集合作融合。联合学习组件将RGB图像和深度图像像320 x 320 x 3 x 2一样作为一批，通过暹罗网络(CNN)同时提取特征。这些特征然后通过CP(压缩模块)被馈送到DCF组件，该CP模块通过侧路径接收来自暹罗网络的输入并压缩信道，然后在与FA模块的输出级联之前传递到交叉模式融合模块(CM)。</p><p id="3a9e" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">我们可以使用不同的网络，如ResNet-101、ResNet-50、VGG-16作为JL组件的主干。这里，在这个项目中，我们使用Resnet101和VGG16作为主干网络来训练不同的模型，并在NJU2K数据集上执行SOD任务。数据取自<a class="ae kz" href="https://drive.google.com/file/d/1R1O2dWr6HqpTOiDn6hZxUWTesOSJteQo/view" rel="noopener ugc nofollow" target="_blank">这里的<strong class="jp hi">这里的</strong> </a>。</p><h1 id="72a1" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">模型架构:</h1><p id="e740" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">让我们更深入地理解模型架构，我们可以将整个框架分为两部分，即联合学习(JL)和密集协作融合(DCF)</p><h2 id="ecd9" class="lb iq hh bd ir lc ld le iv lf lg lh iz jy li lj jd kc lk ll jh kg lm ln jl lo bi translated">联合学习:</h2><p id="6725" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">在联合学习组件中，我们可以使用CNN类型的网络作为共享主干。如上图所示，我们将RGB图像和它在第四维(即像320x320x3x2这样的批处理维)中对应的深度图像组合起来，然后将其作为CNN网络的输入。如图所示，来自共享CNN骨干网的分级特征然后通过副路径以副输出的方式被利用。这些旁路包含不同的过滤器尺寸和通道号。这些旁路的输出具有不同的信道号，我们将它们传递到压缩模块(CP)(图3中的CP1∞CP6，实际上由卷积层加上ReLU非线性实现)。在这个CP模块中，我们将特征压缩到标准的k个信道。这些CP模块的输出仍然是批量的，我们将这些输出馈送到DCF组件的跨模态融合(CM)模块。我们将最后一个压缩模块(即CP6)的输出分成两部分，然后通过1×1，1卷积层，然后将这些输出与下采样的地面真实值进行比较，并计算损耗。这种损失被称为全局损失Lg，骨干网从这种损失中学习，并通过反向传播得到更新。</p><h2 id="dfcf" class="lb iq hh bd ir lc ld le iv lf lg lh iz jy li lj jd kc lk ll jh kg lm ln jl lo bi translated">密集合作融合(DCF):</h2><p id="9e39" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">CP模块的输出包含成批的RGB和深度信息。我们通过跨模态融合模块(CM)将这些输出馈送到DCF组件。在这个CM模块中，我们首先分割批次，然后执行元素相加和元素相乘来进行特征融合，我们称之为合作融合，如图所示。数学上，让批特征由{Xrgb，Xd}表示，其中Xrgb，Xd表示rgb和深度特征张量，每个张量分别具有k个通道。CM模块以如下方式进行融合:</p><p id="a75a" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">CM({Xrgb，Xd}) = Xrgb ⊕ Xd ⊕ (Xrgb ⊗ Xd)</p><figure class="km kn ko kp fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/9dff851eafa43cd79cd380edad83d4b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*fNZvkNhkdMRt_Zix-TW85w.png"/></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd ir">来自纸张的CM模块图像</strong></figcaption></figure><p id="c4f6" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">其中“⊕”和“⊗”表示逐元素的加法和乘法。因为我们做了元素加法和乘法，所以混合输出仍然包含k个通道。</p><p id="9bec" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">然后，我们将这些来自CM模块的输出提供给特征聚合(FA)模块。如上图所示，这些FA模块通过密集连接进行连接。在这些FA模块中，我们使用不同的卷积层和max-pooling层执行非线性聚合和转换，如图所示。最终的FA模块FA1具有最好的特征，我们将这些特征馈送到1x1，1卷积层以获得最终的输出。如图所示，该最终预测由训练期间调整大小的地面实况(GT)图监控。我们将这一阶段产生的损耗称为最终损耗，记为Lf。</p><figure class="km kn ko kp fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/a99b10fad163dd72c2bb9ff4c6473429.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*hrIOXoKuDsTxdZFcJB3Kbg.png"/></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd ir">来自纸张的FA模块图像</strong></figcaption></figure><h1 id="82af" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">损失函数:</h1><p id="720a" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">我们模型的总损失函数由总制导损失Lg和最终损失Lf组成。假设G表示来自地面实况的监督，Sc rgb和Sc d表示包含在模块CP6之后的批次中的粗略预测图，Sf是模块FA1之后的最终预测。总损失函数定义为:</p><p id="f0c9" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">Ltotal = Lf (S f，G) + λ X x∈{rgb，d} Lg(S c x，G)，(2)</p><p id="442e" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">在本项目中，λ平衡了全局指导的重点，我们给出了相等的权重，即λ=1，我们采用广泛使用的Lg和Lf的二进制交叉熵损失为:</p><p id="229c" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">L(S，G)= Xi[Gi log(Si)+(1 Gi)log(1 Si)]，(3)</p><p id="129a" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">其中I表示像素索引，S ∈ {Scrgb，Scd，Sf }。</p><h1 id="bf27" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">绩效指标:</h1><p id="e715" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">出于评估目的，我们使用了五个指标，所有指标都是基于FA1模块的最终输出计算的。这些指标是:</p><ol class=""><li id="351a" class="lr ls hh jp b jq ku ju kv jy lt kc lu kg lv kk lw lx ly lz bi translated"><strong class="jp hi">精度:</strong>精度可以定义为真阳性(TP)与真阳性(TP)和假阳性(FP)之和的比值。我们可以通过比较来自FA1模块的二进制输出和调整大小的二进制基础真值来计算精度。</li><li id="982a" class="lr ls hh jp b jq ma ju mb jy mc kc md kg me kk lw lx ly lz bi translated"><strong class="jp hi">召回:</strong>召回可以定义为真阳性(TP)与真阳性(TP)和假阴性(FN)之和的比值。我们可以通过比较来自FA1模块的二进制输出和调整大小的二进制基础事实来计算召回率。</li><li id="28bd" class="lr ls hh jp b jq ma ju mb jy mc kc md kg me kk lw lx ly lz bi translated"><strong class="jp hi">F-β测度:</strong>F-测度可以定义为</li></ol><figure class="km kn ko kp fd ii er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es mf"><img src="../Images/ccbdd0caff1a1bab63fff8d497075a55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/0*AkILChjJTiKB7wdD.png"/></div></div></figure><p id="5349" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">其中β是精确度和召回率之间的权重。我们按照论文中的建议设定β^2 = 0.3。</p><p id="f320" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">4.<strong class="jp hi"> E-Score: </strong>在我们的任务中使用了增强对齐度量(E-measure)来评估模型的性能。<a class="ae kz" href="https://www.ijcai.org/proceedings/2018/0097.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hi">这篇</strong> </a>论文帮助我们更好地理解了度量。该度量同时考虑了像素级和图像级统计。它将按如下方式计算:</p><p id="5f03" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">首先计算偏置矩阵ϕ，它是二进制(预测显著图)FM的每个像素值与其全局平均值之间的距离。</p><figure class="km kn ko kp fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/8976aa6787143b70a67f7d71d8cd4f0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*yjGrp3JLbqYjP1xKqa-iFQ.png"/></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd ir">来自纸张的图像</strong></figcaption></figure><p id="b2fd" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">其中A是所有元素的值都为1的矩阵。计算前景图(FM)和地面实况(GT)的偏差矩阵，如ϕgt. ϕfm然后，计算对齐矩阵ξFM，它是ϕFM和ϕGT.之间的相关矩阵</p><figure class="km kn ko kp fd ii er es paragraph-image"><div class="er es mh"><img src="../Images/b7b4db900d09607eee1d32b0b5cb90af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*_YIEjfv2OadmH7823UUeGA.png"/></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd ir">图片来自纸张</strong></figcaption></figure><p id="b287" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">其中，o表示Hadamard乘积，它只不过是元素态乘积。现在，计算如下所示的增强对齐矩阵:</p><p id="352f" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">φFM = f(ξFM)</p><p id="6c40" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">其中，f是二次型函数f(x) = 1/ 4 (1 + x) 2</p><p id="1801" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">最后，现在计算增强对齐度量(E-measure)</p><figure class="km kn ko kp fd ii er es paragraph-image"><div class="er es mi"><img src="../Images/3b118e03fd04e8a522fa9c23e5da8638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*nPG5q4pRzRPjZms0El__Og.png"/></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd ir">图片来自纸张</strong></figcaption></figure><p id="13a0" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">其中，w和h是前景图即预测图的宽度和高度。</p><p id="cf6e" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">5.<strong class="jp hi">平均绝对误差:</strong>平均绝对误差可定义为</p><figure class="km kn ko kp fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/fd75ab6c8356bf564833a202574f5de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*PR0u94ykz4r3WaHpXS_cYQ.png"/></div></figure><p id="3040" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">其中Smap(x，y)和G(x，y)对应于像素位置(x，y)处的显著值和地面真实值。</p><h1 id="80cd" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">模型实现:</h1><p id="18d7" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">让我们一步一步地实现模型。分别构建模型的子部分，然后我们将使用一个功能API来组合所有这些部分。这里，我们使用VGG16主干构建模型。整个模型在Tensorflow中构建和执行。</p><ol class=""><li id="7e69" class="lr ls hh jp b jq ku ju kv jy lt kc lu kg lv kk lw lx ly lz bi translated"><strong class="jp hi"> VGG16主干:</strong>我们使用函数式API构建了没有顶部三个密集层的VGG16。为了计算速度，我们将输入形状，即图像尺寸减少到160×160×3。VGG16的构造代码如下:</li></ol><figure class="km kn ko kp fd ii"><div class="bz dy l di"><div class="mj mk l"/></div><figcaption class="il im et er es in io bd b be z dx translated">这里是<a class="ae kz" href="https://gist.github.com/sureshreddy129/a8cde790fb987b24a8d0e77b5a88f0fb" rel="noopener ugc nofollow" target="_blank">的<strong class="ak">要诀</strong>的</a></figcaption></figure><figure class="km kn ko kp fd ii"><div class="bz dy l di"><div class="mj mk l"/></div><figcaption class="il im et er es in io bd b be z dx translated">下面是<a class="ae kz" href="https://gist.github.com/sureshreddy129/f38474390c53143451fab82d97daa791" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">要诀</strong> </a></figcaption></figure><p id="0a10" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">2.<strong class="jp hi">次要路径:</strong>主干之后，我们必须构建次要路径。这些侧路径只不过是如图所示的具有不同参数的卷积层。下面括号中的参数从左到右依次是:内核大小、通道号、步幅、膨胀率和填充</p><figure class="km kn ko kp fd ii er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es ml"><img src="../Images/6e5843dbf6ab601c1335614a67765577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SgMFjkNwqVgbw3V1h1V1QQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd ir">来自纸张的图像</strong></figcaption></figure><p id="5437" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">这些是本文中推荐的旁道参数，我们据此建立了旁道。我们在类对象中构建了这些侧路径，代码如下:</p><figure class="km kn ko kp fd ii"><div class="bz dy l di"><div class="mj mk l"/></div><figcaption class="il im et er es in io bd b be z dx translated">这里是<a class="ae kz" href="https://gist.github.com/sureshreddy129/6e3239fd5f336fc0cf568380d7c6a13d" rel="noopener ugc nofollow" target="_blank">的<strong class="ak">要诀</strong>的</a></figcaption></figure><p id="43f5" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">3.<strong class="jp hi">压缩模块(CP): </strong> CP模块是简单的卷积层，其核大小为3×3，滤波器数量为64，如本文所建议的。</p><p id="678e" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">4.<strong class="jp hi">跨模态融合模块(CM):</strong>CM模块的构建如上图所示。这里，我们必须首先分离CP模块的输出，然后进行元素加法和乘法。代码如下所示:</p><figure class="km kn ko kp fd ii"><div class="bz dy l di"><div class="mj mk l"/></div><figcaption class="il im et er es in io bd b be z dx translated">这里是<a class="ae kz" href="https://gist.github.com/sureshreddy129/910344be0e737c36aa483b40a7a334c1" rel="noopener ugc nofollow" target="_blank">的<strong class="ak">要诀</strong>的</a></figcaption></figure><p id="98e4" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">5.<strong class="jp hi">特征聚合模块(FA):</strong>FA模块按照上面给出的FA图中的说明进行构建。代码如下:</p><figure class="km kn ko kp fd ii"><div class="bz dy l di"><div class="mj mk l"/></div><figcaption class="il im et er es in io bd b be z dx translated">这里是<a class="ae kz" href="https://gist.github.com/sureshreddy129/455e26caa23779367d7bb1a16e731d5f" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">的要诀</strong> </a></figcaption></figure><p id="f868" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">到目前为止，我们已经构建了模型所需的所有子模块。现在，让我们定义性能指标。我们使用model.fit中的回调函数计算指标。回调函数的代码如下:</p><p id="0d54" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated"><strong class="jp hi">电子分数:</strong></p><figure class="km kn ko kp fd ii"><div class="bz dy l di"><div class="mj mk l"/></div><figcaption class="il im et er es in io bd b be z dx translated">这里是<a class="ae kz" href="https://gist.github.com/sureshreddy129/1c8302329f667df9f7697f964c0e1bf5" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">的要诀</strong> </a></figcaption></figure><p id="ce5a" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">所有其他指标:</p><figure class="km kn ko kp fd ii"><div class="bz dy l di"><div class="mj mk l"/></div><figcaption class="il im et er es in io bd b be z dx translated">这里是<strong class="ak"> </strong> <a class="ae kz" href="https://gist.github.com/sureshreddy129/b34accf72bce046e0ed601c873a44074" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">要诀</strong> </a></figcaption></figure><p id="9632" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">我们创建了所需的所有子模块和性能指标。现在，让我们建立最终的模型。</p><h2 id="d0ac" class="lb iq hh bd ir lc ld le iv lf lg lh iz jy li lj jd kc lk ll jh kg lm ln jl lo bi translated">最终型号:</h2><p id="4bb6" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">在这里，我们连接所有子模块并构建最终模型。首先，我们需要提供RGB图像和深度图像作为输入，需要形成一批2个图像，需要获得三个输出，两个来自联合学习组件，一个是最终输出。为此，我们使用tensorflow的多输入多输出模型。输入层的代码如下所示。</p><figure class="km kn ko kp fd ii"><div class="bz dy l di"><div class="mj mk l"/></div><figcaption class="il im et er es in io bd b be z dx translated">这里是<a class="ae kz" href="https://gist.github.com/sureshreddy129/e0ef95d583c27170d90112c5101e0509" rel="noopener ugc nofollow" target="_blank">的<strong class="ak">要诀</strong>的</a></figcaption></figure><p id="8ad8" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">现在，我们将旁路与VGG16连接起来，如下所示:</p><p id="dea8" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">side_path1至conv1_2</p><p id="c7f8" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">side_path2至conv2_2</p><p id="dac4" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">side_path3至conv3_3</p><p id="4be0" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">side_path4至conv4_3</p><p id="34c9" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">side_path5至conv5_3</p><p id="2e0e" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">VGG16网络的side_path6到pool5。</p><p id="25ad" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">我们将这些旁路连接到CP模块，并将这些CP模块的输出馈送到CM模块。为了连接CM模块的输出和FA模块的输出，我们必须以不同的速率对FA模块的输出进行上采样，以匹配CM模块输出的空间维度。最终模型的代码如下所示:</p><figure class="km kn ko kp fd ii"><div class="bz dy l di"><div class="mj mk l"/></div><figcaption class="il im et er es in io bd b be z dx translated">这里是<a class="ae kz" href="https://gist.github.com/sureshreddy129/f58509cd07a8a7f722f3c3e810cbe5e2" rel="noopener ugc nofollow" target="_blank">的<strong class="ak">要诀</strong>的</a></figcaption></figure><p id="ea79" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">现在，我们用0.0001的learning_rate用Adam优化器编译这个模型。尝试了不同的学习速率，但是0.0001给出了更好的收敛性。</p><p id="a67b" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">像这样，我们建立了4个不同的模型，它们是:</p><ol class=""><li id="74f3" class="lr ls hh jp b jq ku ju kv jy lt kc lu kg lv kk lw lx ly lz bi translated">带VGG16和基本参数的JL_DCF。这里是<a class="ae kz" href="https://github.com/sureshreddy129/Salien-Object-Detection/blob/main/Case_Study2_basic_modelling_.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hi">代码</strong>。</a></li><li id="8702" class="lr ls hh jp b jq ma ju mb jy mc kc md kg me kk lw lx ly lz bi translated">带有VGG16和附加conv层的JL_DCF。这里是<a class="ae kz" href="https://github.com/sureshreddy129/Salien-Object-Detection/blob/main/Case_study2_modified_.modelling.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hi">代码。</strong>T25】</a></li><li id="6902" class="lr ls hh jp b jq ma ju mb jy mc kc md kg me kk lw lx ly lz bi translated">带有ResNet101和基本参数的JL_DCF。这里是<a class="ae kz" href="https://github.com/sureshreddy129/Salien-Object-Detection/blob/main/Case_Study2_Resnet_basic_modelling_.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hi">代码。</strong>T29】</a></li><li id="11cf" class="lr ls hh jp b jq ma ju mb jy mc kc md kg me kk lw lx ly lz bi translated">带有ResNet101和附加Conv图层的JL_DCF。这里是<a class="ae kz" href="https://github.com/sureshreddy129/Salien-Object-Detection/blob/main/Case_Study2_Resnet_modified_modelling_.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hi">代码。</strong>T3】</a></li></ol><p id="2323" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">对于带有额外Conv层的模型，除了FA模块中的现有层以及CP6和FA1的输出之外，我们还添加了额外的Conv层。</p><p id="3567" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">对于配有Resnet101网络的型号，我们只需更改主干网络，其余所有设置保持不变。由于ResNet101的第一个卷积层的步幅已经为2，因此在我们的示例中，最浅级别的要素的空间大小为80x80。为了获得完整尺寸(160x160)的特征而不进行琐碎的上采样，我们借用VGG-16的conv1_1和conv1_2层进行特征提取。侧path 1∞path 6分别连接到ResNet101的conv1_2、conv1、res2c、res3b3、res4b22和res5c。我们还将res5a块的步距从2更改为1。以上给出了这些模型的完整代码。</p><h1 id="0dcb" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">培训后分析:</h1><p id="c5bc" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">在对四个模型进行了40个时期的训练后，我们对所有四个模型进行了训练后分析，以挑选最佳模型。对于这项任务，我们使用预测的图像和这些图像的E分数作为参考。基于这些预测和E分数，我们选择了JL DCF，VGG16作为主干网络，其他Conv层作为我们的最佳网络。<strong class="jp hi"> </strong> <a class="ae kz" href="https://github.com/sureshreddy129/Salien-Object-Detection/blob/main/Post_training_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hi">这里的</strong> </a>是岗位培训分析笔记本的全部代码。</p><p id="bf74" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated">在训练后分析中，我们还检查了模型对哪种类型的图像工作最差，我们将E分数小于0.50的图像区分为最差。在397个测试样本中，模型预测14个样本最差，占总样本的3.52%。我们的结论是，在具有多个对象、低光照、模糊的图像以及具有薄且深的对象的图像上，该模型表现很差。对于这些图像，模型检测另一个对象，而不是在地面真实图像中被遮蔽的对象，如下所示:</p><figure class="km kn ko kp fd ii er es paragraph-image"><div class="er es ie"><img src="../Images/0c5a7a912be694bee5a642a12f0e819d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*apSY3AHzHbuOz5t68yNeOA.jpeg"/></div></figure><figure class="km kn ko kp fd ii er es paragraph-image"><div class="er es ie"><img src="../Images/5259adb46f1ddd47edd9a7520235eb62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*j8vHoQnkULKBDYHiPKz1DQ.jpeg"/></div></figure><figure class="km kn ko kp fd ii er es paragraph-image"><div class="er es ie"><img src="../Images/0005eee1974a9d3c6e2aa84c9d713a5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*NxbwlX3ntAuryEyzN77MoA.jpeg"/></div></figure><figure class="km kn ko kp fd ii er es paragraph-image"><div class="er es ie"><img src="../Images/49bc2f87287212226dd4b3f2703d9753.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*tXgSWfp2KVARcphDW-EO1w.jpeg"/></div></figure><figure class="km kn ko kp fd ii er es paragraph-image"><div class="er es ie"><img src="../Images/f83bf8e870633aeb6015e8ed809a4f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*j32gD87PhJ2Rfi0jWhsBMg.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd ir">来自模型预测的图像</strong></figcaption></figure><h1 id="3d0b" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">构建最终检测系统:</h1><p id="60f8" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">到目前为止，我们用不同的参数建立了不同的模型，并在NJU2K数据集上对它们进行了训练。基于它们的性能，我们选择了具有VGG16和附加conv层的JL DCF作为我们的最佳模型。这里，我们正在构建最终的检测系统，其中模型以RGB_image和深度图像作为输入，并检测图像中的显著对象。<a class="ae kz" href="https://github.com/sureshreddy129/Salien-Object-Detection/blob/main/Final_notebook_cs2.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hi">这里的</strong> </a>是最终的笔记本代码，我们部署了这个代码。下面给出了部署的演示视频。</p><div class="mm mn ez fb mo mp"><a href="https://drive.google.com/file/d/1fsF6P8M2NiZzxADug6FdhQBJZNemQJyQ/view?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hi fi z dy mu ea eb mv ed ef hg bi translated">模型的部署</h2></div><div class="mw l"><div class="mx l my mz na mw nb ij mp"/></div></div></a></div><h1 id="3d46" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">未来工作:</h1><p id="573d" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">我们仅在包含1985幅图像的NJU2K数据集上训练所有模型，如果我们在除NJU2K之外的其他RGB-D数据集上训练，模型的性能可能会提高。我们还可以通过锐化、旋转等数据增强来提高模型的性能。，我们也可以尝试JL DCF模型与其他骨干像VGG19，ResNet152和DenseNet等。和额外的Conv层来提高性能。</p><h1 id="7ebc" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">Github:</h1><p id="830b" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">你可以在这里看到我的代码</p><div class="mm mn ez fb mo mp"><a href="https://github.com/sureshreddy129/Salien-Object-Detection" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hi fi z dy mu ea eb mv ed ef hg bi translated">GitHub-sureshreddy 129/Salien-Object-Detection:使用暹罗的RGB-D显著对象检测…</h2><div class="nc l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">以暹罗网络为骨干的RGB-D显著目标检测-GitHub-sureshreddy 129/Salien-Object-Detection…</h3></div><div class="nd l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">github.com</p></div></div><div class="mw l"><div class="ne l my mz na mw nb ij mp"/></div></div></a></div><h1 id="b660" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">Linkedin个人资料:</h1><p id="6796" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">【linkedin.com/in/suresh-kumarreddy-573b25a1 T4】</p><h1 id="e004" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">参考资料:</h1><p id="c7b6" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated"><a class="ae kz" href="https://www.appliedaicourse.com" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com</a></p><p id="27b6" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2008.12134.pdf" rel="noopener ugc nofollow" target="_blank">用于RGB-D显著物体检测的连体网络及其他</a></p><p id="51fd" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated"><a class="ae kz" href="http://.https://openaccess.thecvf.com/content_ICCV_2017/papers/Fan_Structure-Measure_ A_New_ICCV_2017_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://open access . the CVF . com/content _ ICCV _ 2017/papers/Fan _ Structure-Measure _ A _ New _ ICCV _ 2017 _ paper . pdf</a></p><p id="13db" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated"><a class="ae kz" href="http://dpfan.net/wp-content/uploads/IJCV2021_Smeasure-Minor.pdf" rel="noopener ugc nofollow" target="_blank">http://dpfan . net/WP-content/uploads/ijcv 2021 _ s measure-minor . pdf</a></p><p id="e74f" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated"><a class="ae kz" href="https://www.ijcai.org/proceedings/2018/0097.pdf" rel="noopener ugc nofollow" target="_blank">https://www.ijcai.org/proceedings/2018/0097.pdf</a></p><p id="4997" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated"><a class="ae kz" href="https://onlinehelp.explorance.com/blueml/Content/articles/getstarted/mlcalculations. htm?TocPath=Get%20started%7C_____3" rel="noopener ugc nofollow" target="_blank">https://online help . explorance . com/blueml/Content/articles/get started/ml calculations。htm？TocPath = Get % 20 started % 7C _ _ _ _ _ 3</a></p><div class="mm mn ez fb mo mp"><a href="https://www.tensorflow.org/guide/keras/functional#models_with_multiple_inputs_and_outputs" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hi fi z dy mu ea eb mv ed ef hg bi translated">功能API | TensorFlow核心</h2><div class="nc l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">函数式API将模型视为层的Dag。对于大多数深度学习架构来说是这样，但并不是所有的…</h3></div><div class="nd l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">www.tensorflow.org</p></div></div><div class="mw l"><div class="nf l my mz na mw nb ij mp"/></div></div></a></div><p id="51a5" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated"><a class="ae kz" href="https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-ker as-17f34e75bb3d" rel="noopener" target="_blank">https://towards data science . com/one-shot-learning-with-siamese-networks-using-ker as-17 f 34 e 75 bb 3d</a></p><p id="28b6" class="pw-post-body-paragraph jn jo hh jp b jq ku js jt ju kv jw jx jy kw ka kb kc kx ke kf kg ky ki kj kk ha bi translated"><a class="ae kz" href="https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942" rel="noopener" target="_blank">https://towards data science . com/a-friendly-introduction-to-siamese-networks-85ab 17522942</a></p></div></div>    
</body>
</html>