<html>
<head>
<title>Mathematics and Statistics behind Machine Learning — PART 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习背后的数学和统计学——第三部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mathematics-and-statistics-behind-machine-learning-part-3-e5dffb9129f2?source=collection_archive---------7-----------------------#2021-01-06">https://medium.com/analytics-vidhya/mathematics-and-statistics-behind-machine-learning-part-3-e5dffb9129f2?source=collection_archive---------7-----------------------#2021-01-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="03d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是博客的第三部分，也是最后一部分，因为我将涵盖机器学习背后的数学和统计学的最后主题。</p><p id="515a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你没有看到我之前的博客，请一定要看看。这是它的链接。</p><p id="9fb5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第一部分— </strong></p><div class="jc jd ez fb je jf"><a href="https://shubhangagrawal1999.medium.com/mathematics-and-statistics-behind-machine-learning-part-1-eede0e152d57" rel="noopener follow" target="_blank"><div class="jg ab dw"><div class="jh ab ji cl cj jj"><h2 class="bd hi fi z dy jk ea eb jl ed ef hg bi translated">机器学习背后的数学和统计学——第一部分</h2><div class="jm l"><h3 class="bd b fi z dy jk ea eb jl ed ef dx translated">线性代数</h3></div><div class="jn l"><p class="bd b fp z dy jk ea eb jl ed ef dx translated">shubhangagrawal1999.medium.com</p></div></div><div class="jo l"><div class="jp l jq jr js jo jt ju jf"/></div></div></a></div><p id="dc81" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第二部分— </strong></p><div class="jc jd ez fb je jf"><a href="https://shubhangagrawal1999.medium.com/mathematics-and-statistics-behind-machine-learning-part-2-9aa2f068ecf9" rel="noopener follow" target="_blank"><div class="jg ab dw"><div class="jh ab ji cl cj jj"><h2 class="bd hi fi z dy jk ea eb jl ed ef hg bi translated">机器学习背后的数学和统计学——第二部分</h2><div class="jm l"><h3 class="bd b fi z dy jk ea eb jl ed ef dx translated">机器学习的概率与统计</h3></div><div class="jn l"><p class="bd b fp z dy jk ea eb jl ed ef dx translated">shubhangagrawal1999.medium.com</p></div></div><div class="jo l"><div class="jv l jq jr js jo jt ju jf"/></div></div></a></div><p id="92bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，不再拖延，让我们开始这个系列的最后一部分。</p><figure class="jx jy jz ka fd kb er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es jw"><img src="../Images/dd48858cd203256b0cf3a0d4db444b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z1hcwwjaUArH64I3sdbtVA.png"/></div></div></figure><p id="514e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">在最后一部分，我们剩下两个主题多元微积分|算法和复杂性。所以在这篇博客中，我们将讨论这些话题。</strong></p></div><div class="ab cl kh ki go kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="ha hb hc hd he"><h1 id="30c2" class="ko kp hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated"><strong class="ak">多元微积分</strong></h1><p id="3255" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">在我们讨论微积分的定义、应用和所有东西之前，我想提一下，我写这篇博客是希望你有微积分的一般知识，比如微分和积分，因为这可能是理解这个主题所需要的。</p><p id="8444" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">多元意味着多个变量。因此，多元微积分是一个涉及多个变量的微积分领域。多元微积分也称为偏导数，用于给定函数的数学优化(大多数是凸的，因为凸函数往往有最小值)。使用多元微积分，我们可以很容易地优化我们的凸函数算法到最小值或最低点，有一些方法会陷入局部最小值，但也有一些方法来避免它们。</p><figure class="jx jy jz ka fd kb er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es jw"><img src="../Images/ec995e8ed28dd260ffaf00c1f8756db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z8nl0eQewP2U_V99ZABvdA.png"/></div></div></figure><h1 id="42ad" class="ko kp hh bd kq kr lr kt ku kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll bi translated">梯度下降</h1><p id="ac69" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">梯度下降是一种机器学习算法，它迭代地寻找其参数的最佳值。它考虑了用户定义的学习率和初始参数值。</p><h2 id="e673" class="lw kp hh bd kq lx ly lz ku ma mb mc ky ip md me lc it mf mg lg ix mh mi lk mj bi translated">公式:</h2><figure class="jx jy jz ka fd kb er es paragraph-image"><div class="er es mk"><img src="../Images/032b415d5880d318839dcfffd0df5f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*cHrV0EFeG-9_4dv0zYychw.png"/></div></figure><h2 id="50cf" class="lw kp hh bd kq lx ly lz ku ma mb mc ky ip md me lc it mf mg lg ix mh mi lk mj bi translated">我们为什么需要它？</h2><p id="b21d" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">一般来说，我们要做的是，找到给出参数最优值的公式。但是在这个算法里，它是自己找值的！</p><h2 id="c038" class="lw kp hh bd kq lx ly lz ku ma mb mc ky ip md me lc it mf mg lg ix mh mi lk mj bi translated">推导的一些基本规则:</h2><p id="8fb8" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">(一)标量倍数规则:</p><figure class="jx jy jz ka fd kb er es paragraph-image"><div class="er es ml"><img src="../Images/80b367f15c904cf97ee7779ab9ecdd93.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/0*znZjmbnp6QXwEqeK.png"/></div></figure><p id="3880" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">(B)总和规则:</p><figure class="jx jy jz ka fd kb er es paragraph-image"><div class="er es mm"><img src="../Images/2cac71b33c296765c2cab41ea1ce402d.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/0*TQuDIRFf-1w9U6SY.png"/></div></figure><p id="d443" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">权力规则:</p><figure class="jx jy jz ka fd kb er es paragraph-image"><div class="er es mn"><img src="../Images/10a53482626baab8bcdf1225ade293bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/0*ZyckaYM2hSn-0-vQ.png"/></div></figure><p id="3dd3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">(D)连锁规则:</p><figure class="jx jy jz ka fd kb er es paragraph-image"><div class="er es mo"><img src="../Images/3568345c706ebf45ea4dee67e8cba3c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/0*IRrMCiPMQLA5oa7X.png"/></div></figure><p id="1c13" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">让我们举个例子来了解更多关于梯度下降和多元微积分的知识。</strong></p><h1 id="97dd" class="ko kp hh bd kq kr lr kt ku kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll bi translated"><strong class="ak">线性回归中的梯度下降:</strong></h1><p id="f217" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">在线性回归中，模型的目标是根据给定的输入值(x)获得预测y值的最佳回归直线。为模型定型时，模型会计算成本函数，该函数测量预测值(pred)和真实值(y)之间的均方根误差。该模型以最小化成本函数为目标。<br/>为了使成本函数最小化，模型需要具有的最佳值？1和？2.最初型号选择？1和？然后迭代地更新这些值，以便最小化成本函数，直到它达到最小值。当模型达到最小成本函数时，它将具有最好的？1和？2价值观。使用这些最终更新的值。1和？2在线性方程的假设方程中，模型尽可能地预测y的值。</p><p id="97de" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">线性回归成本函数:</strong></p><figure class="jx jy jz ka fd kb er es paragraph-image"><div class="er es mp"><img src="../Images/8f7232571e6f4744b684506cf853039a.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/0*41I_XAhSo1GgkGEo.jpg"/></div></figure><figure class="jx jy jz ka fd kb er es paragraph-image"><div class="er es mq"><img src="../Images/4966bdde9417e63fdaf3cf467624989a.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/0*KfVoImqNmHJO3PsM.jpg"/></div></figure><p id="e50d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">线性回归的梯度下降算法:</strong></p><figure class="jx jy jz ka fd kb er es paragraph-image"><div class="er es mr"><img src="../Images/7f67e3db614a0f77b01b74ecf4e3c068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*rwrpJa9DpH3uZvYN3of0Fg.jpeg"/></div></figure><figure class="jx jy jz ka fd kb er es paragraph-image"><div class="er es ms"><img src="../Images/5ce3ac477b6c2262c49919c736e2db59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*8EWipOEVJBfkFd4d4ShvdQ.jpeg"/></div></figure><pre class="jx jy jz ka fd mt mu mv mw aw mx bi"><span id="e441" class="lw kp hh mu b fi my mz l na nb">-&gt; <strong class="mu hi">?j     :</strong> Weights of the hypothesis.<br/>-&gt; <strong class="mu hi">h?(xi) :</strong> predicted y value for ith input.<br/>-&gt; <strong class="mu hi">j     : </strong>Feature index number (can be 0, 1, 2, ......, n).<br/>-&gt; <strong class="mu hi">?     :</strong> Learning Rate of Gradient Descent.</span><span id="a17c" class="lw kp hh mu b fi nc mz l na nb"><strong class="mu hi">Note: Here '?' represents Theta</strong></span></pre></div><div class="ab cl kh ki go kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="ha hb hc hd he"><h1 id="9418" class="ko kp hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated"><strong class="ak">算法和复杂度</strong></h1><h2 id="55f4" class="lw kp hh bd kq lx ly lz ku ma mb mc ky ip md me lc it mf mg lg ix mh mi lk mj bi translated">机器学习算法综述</h2><p id="01e9" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">当处理数据以模拟业务决策时，您通常会使用监督和非监督学习方法。</p><p id="4438" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">目前的一个热门话题是图像分类等领域的半监督学习方法，在这些领域中，有大量的数据集，但只有很少的标记样本。</p><p id="5807" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里我要提一下按相似度分组的各种机器学习算法。</p><figure class="jx jy jz ka fd kb er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es nd"><img src="../Images/47017a4e150499e0d17d6b40875ae9e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6GFNBjxJrLM-xtLEIkfdHg.png"/></div></div></figure><h2 id="6a9c" class="lw kp hh bd kq lx ly lz ku ma mb mc ky ip md me lc it mf mg lg ix mh mi lk mj bi translated">回归算法</h2><p id="95bf" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">最流行的回归算法有:</p><ul class=""><li id="cbbc" class="ne nf hh ig b ih ii il im ip ng it nh ix ni jb nj nk nl nm bi translated">普通最小二乘回归(OLSR)</li><li id="6d3e" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">线性回归</li><li id="6845" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">逻辑回归</li><li id="37ef" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">逐步回归</li><li id="31c3" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">多元自适应回归样条(MARS)</li><li id="622b" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">局部估计的散点图平滑(黄土)</li></ul><h2 id="ca92" class="lw kp hh bd kq lx ly lz ku ma mb mc ky ip md me lc it mf mg lg ix mh mi lk mj bi translated">基于实例的算法</h2><p id="e30a" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">最流行的基于实例的算法有:</p><ul class=""><li id="18dd" class="ne nf hh ig b ih ii il im ip ng it nh ix ni jb nj nk nl nm bi translated">k-最近邻</li><li id="ce16" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">学习矢量量化(LVQ)</li><li id="2001" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">自组织映射(SOM)</li><li id="2f42" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">局部加权学习(LWL)</li><li id="13cf" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">支持向量机(SVM)</li></ul><h2 id="dc2f" class="lw kp hh bd kq lx ly lz ku ma mb mc ky ip md me lc it mf mg lg ix mh mi lk mj bi translated">正则化算法</h2><p id="f7d8" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">最流行的正则化算法有:</p><ul class=""><li id="0f63" class="ne nf hh ig b ih ii il im ip ng it nh ix ni jb nj nk nl nm bi translated">里脊回归</li><li id="466a" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">最小绝对收缩和选择算子(LASSO)</li><li id="e0d7" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">弹性网</li><li id="5ed8" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">最小角度回归</li></ul><h2 id="5296" class="lw kp hh bd kq lx ly lz ku ma mb mc ky ip md me lc it mf mg lg ix mh mi lk mj bi translated">决策树算法</h2><p id="969e" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">最流行的决策树算法有:</p><ul class=""><li id="1820" class="ne nf hh ig b ih ii il im ip ng it nh ix ni jb nj nk nl nm bi translated">分类和回归树</li><li id="7ded" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">迭代二分法3 (ID3)</li><li id="a173" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">C4.5和C5.0(强大方法的不同版本)</li><li id="8b4f" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">卡方自动交互检测(CHAID)</li><li id="3533" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">决策树桩</li><li id="5262" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">M5</li><li id="ab44" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">条件决策树</li></ul><h2 id="589d" class="lw kp hh bd kq lx ly lz ku ma mb mc ky ip md me lc it mf mg lg ix mh mi lk mj bi translated">聚类算法</h2><p id="3cb9" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">最流行的聚类算法有:</p><ul class=""><li id="4a8f" class="ne nf hh ig b ih ii il im ip ng it nh ix ni jb nj nk nl nm bi translated">k均值</li><li id="a685" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">k-中间值</li><li id="3981" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">期望最大化</li><li id="5d46" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">分层聚类</li></ul><h2 id="d1d4" class="lw kp hh bd kq lx ly lz ku ma mb mc ky ip md me lc it mf mg lg ix mh mi lk mj bi translated">人工神经网络算法</h2><p id="1cf3" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">最流行的人工神经网络算法有:</p><ul class=""><li id="5018" class="ne nf hh ig b ih ii il im ip ng it nh ix ni jb nj nk nl nm bi translated">感知器</li><li id="d8b0" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">多层感知器(MLP)</li><li id="84d8" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">反向传播</li><li id="a42e" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">随机梯度下降</li><li id="e4de" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">Hopfield网络</li><li id="a3a2" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">径向基函数网络(RBFN)</li></ul><h2 id="a23e" class="lw kp hh bd kq lx ly lz ku ma mb mc ky ip md me lc it mf mg lg ix mh mi lk mj bi translated">深度学习算法</h2><p id="9345" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">最流行的深度学习算法有:</p><ul class=""><li id="86f6" class="ne nf hh ig b ih ii il im ip ng it nh ix ni jb nj nk nl nm bi translated">卷积神经网络(CNN)</li><li id="4e87" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">递归神经网络</li><li id="f2de" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">长短期记忆网络</li><li id="26f6" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">堆叠式自动编码器</li><li id="25d4" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">深层玻尔兹曼机器(DBM)</li><li id="a781" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">深度信仰网络(DBN)</li></ul><h2 id="2a85" class="lw kp hh bd kq lx ly lz ku ma mb mc ky ip md me lc it mf mg lg ix mh mi lk mj bi translated">降维算法</h2><p id="b2b5" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">最流行的降维算法有:</p><ul class=""><li id="d339" class="ne nf hh ig b ih ii il im ip ng it nh ix ni jb nj nk nl nm bi translated">主成分分析</li><li id="3c47" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">主成分回归</li><li id="5a8a" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">偏最小二乘回归(PLSR)</li><li id="262b" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">Sammon映射</li><li id="6950" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">多维标度(MDS)</li><li id="8a0d" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">投影寻踪</li><li id="b75d" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">线性判别分析(LDA)</li><li id="0197" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">混合判别分析</li><li id="c3a3" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">二次判别分析(QDA)</li><li id="bcdb" class="ne nf hh ig b ih nn il no ip np it nq ix nr jb nj nk nl nm bi translated">灵活判别分析(FDA)</li></ul><p id="8d3a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要了解更多此类算法，请访问下面的链接-</p><div class="jc jd ez fb je jf"><a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/" rel="noopener  ugc nofollow" target="_blank"><div class="jg ab dw"><div class="jh ab ji cl cj jj"><h2 class="bd hi fi z dy jk ea eb jl ed ef hg bi translated">机器学习算法之旅-机器学习掌握</h2><div class="jm l"><h3 class="bd b fi z dy jk ea eb jl ed ef dx translated">在这篇文章中，我们将浏览最流行的机器学习算法。游览主要景点是很有用的。</h3></div><div class="jn l"><p class="bd b fp z dy jk ea eb jl ed ef dx translated">machinelearningmastery.com</p></div></div><div class="jo l"><div class="ns l jq jr js jo jt ju jf"/></div></div></a></div><h1 id="4e71" class="ko kp hh bd kq kr lr kt ku kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll bi translated">ML模型的计算复杂性</h1><figure class="jx jy jz ka fd kb er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es nt"><img src="../Images/3970415852825d07facb598913c60287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wimMzbXGsyjYSpgaaAROBA.png"/></div></div></figure><p id="5e4c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">时间复杂度</strong>可以被视为对输入大小的算法执行快慢的度量。时间复杂度总是相对于某个输入大小(比如n)给出的。<br/> <strong class="ig hi">空间复杂度</strong>可以看做是你执行算法所需要的额外内存量。像时间复杂度一样，它也是相对于某个输入大小(n)给出的。</p><p id="6714" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">算法/模型的复杂性通常用<strong class="ig hi">大O符号表示，</strong>定义了一个算法的上限，它只从上面限制一个函数。<br/>下图可视化了算法复杂性的不同情况。</p><figure class="jx jy jz ka fd kb er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es nu"><img src="../Images/0849b9a4ac04ce4a9acccee501e42179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oWPs30hJ5tnvYZUV.png"/></div></div><figcaption class="nv nw et er es nx ny bd b be z dx translated"><a class="ae nz" href="http://bigocheatsheet.com/" rel="noopener ugc nofollow" target="_blank">http://bigocheatsheet.com/</a></figcaption></figure><p id="95c9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了写出计算复杂度，我们假设，<br/> n=训练样本的数量，d=数据的维数，<br/> k=邻居的数量</p><blockquote class="oa ob oc"><p id="205c" class="ie if od ig b ih ii ij ik il im in io oe iq ir is of iu iv iw og iy iz ja jb ha bi translated"><strong class="ig hi">K最近邻的复杂度寻找K最近邻</strong></p></blockquote><p id="851f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="od">训练时间复杂度</em> </strong> <em class="od"> = O(knd) </em> <br/>循环通过每个训练观测，并计算训练集观测和新观测之间的距离<em class="od"> d </em>。</p><p id="2045" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">时间与实例数量(n)和维度(d)成线性关系。</p><p id="a458" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="od">空间复杂度</em></strong><em class="od">= O(nd)</em><br/>K近邻存储数据。测试需要更长的时间，因为你必须将每个测试实例与整个训练数据进行比较。</p><blockquote class="oa ob oc"><p id="10fc" class="ie if od ig b ih ii ij ik il im in io oe iq ir is of iu iv iw og iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="hh">逻辑回归的复杂性</em> </strong></p></blockquote><p id="e099" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">训练时间复杂度意味着在逻辑回归中，它意味着求解最优化问题。<br/> <strong class="ig hi"> <em class="od">列车时间复杂度= </em> </strong> <em class="od"> O(nd) </em></p><p id="36a6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="od">空间复杂度</em></strong><em class="od">= O(d)</em><br/><strong class="ig hi">注:</strong>逻辑回归对于低延迟应用非常好。</p><blockquote class="oa ob oc"><p id="000a" class="ie if od ig b ih ii ij ik il im in io oe iq ir is of iu iv iw og iy iz ja jb ha bi translated"><strong class="ig hi"><em class="hh">SVM的复杂性</em> </strong></p></blockquote><p id="08bc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="od">训练时间复杂度</em> </strong> =O(n ) <br/>注意:如果n较大，避免使用SVM。</p><p id="7beb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"><em class="od"/></strong>= O(K * d)<br/>K =支持向量的数量，d =数据的维数</p><blockquote class="oa ob oc"><p id="89b6" class="ie if od ig b ih ii ij ik il im in io oe iq ir is of iu iv iw og iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="hh">决策树的复杂性</em> </strong></p></blockquote><p id="19b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="od">训练时间复杂度</em> </strong> = O(n*log(n)*d) <br/> n=训练集中的点数<br/>d =数据的维数</p><p id="ebb4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="od">运行时复杂度</em> </strong> = O(树的最大深度)<br/> <strong class="ig hi">注:</strong>当我们有大量低维数据时，我们使用决策树。</p><blockquote class="oa ob oc"><p id="ed18" class="ie if od ig b ih ii ij ik il im in io oe iq ir is of iu iv iw og iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="hh">随机森林的复杂性</em> </strong></p></blockquote><p id="f673" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="od">训练时间复杂度</em></strong>= O(n * log(n)* d * k)<br/>k =决策树数量<br/>注:当我们有大量特征合理的数据时。然后我们可以使用多核来并行化我们的模型，以训练不同的决策树。</p><p id="28c7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="od">运行时复杂度</em> </strong> = O(树的深度* k) <br/> <strong class="ig hi"> <em class="od">空间复杂度</em> </strong> = O(树的深度*k) <br/>注意:随机森林相对其他算法要快一些。</p><blockquote class="oa ob oc"><p id="058a" class="ie if od ig b ih ii ij ik il im in io oe iq ir is of iu iv iw og iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="hh">朴素贝叶斯的复杂性</em> </strong></p></blockquote><p id="6a68" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="od">训练时间复杂度</em> </strong> = O(n*d) <br/> <strong class="ig hi"> <em class="od">运行时间复杂度</em></strong><em class="od">=</em>O(c * d)<em class="od"><br/>我们必须为每个类检索特征‘c’</em></p></div><div class="ab cl kh ki go kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="ha hb hc hd he"><p id="d606" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这最后一部分，我试图提供关于多元微积分|算法和复杂性的重要信息。希望你能在这里找到有用的东西。谢谢你一直读到最后。如果你喜欢我的博客，请点击下面的按钮。让我知道我的博客是否真的有用。此外，如果你没有检查我的同一主题的其他部分，下面是他们的链接。</p><p id="69bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第一部分— </strong></p><div class="jc jd ez fb je jf"><a href="https://shubhangagrawal1999.medium.com/mathematics-and-statistics-behind-machine-learning-part-1-eede0e152d57" rel="noopener follow" target="_blank"><div class="jg ab dw"><div class="jh ab ji cl cj jj"><h2 class="bd hi fi z dy jk ea eb jl ed ef hg bi translated">机器学习背后的数学和统计学——第一部分</h2><div class="jm l"><h3 class="bd b fi z dy jk ea eb jl ed ef dx translated">线性代数</h3></div><div class="jn l"><p class="bd b fp z dy jk ea eb jl ed ef dx translated">shubhangagrawal1999.medium.com</p></div></div><div class="jo l"><div class="jp l jq jr js jo jt ju jf"/></div></div></a></div><p id="2184" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第二部分— </strong></p><div class="jc jd ez fb je jf"><a href="https://shubhangagrawal1999.medium.com/mathematics-and-statistics-behind-machine-learning-part-2-9aa2f068ecf9" rel="noopener follow" target="_blank"><div class="jg ab dw"><div class="jh ab ji cl cj jj"><h2 class="bd hi fi z dy jk ea eb jl ed ef hg bi translated">机器学习背后的数学和统计学——第二部分</h2><div class="jm l"><h3 class="bd b fi z dy jk ea eb jl ed ef dx translated">机器学习的概率与统计</h3></div><div class="jn l"><p class="bd b fp z dy jk ea eb jl ed ef dx translated">shubhangagrawal1999.medium.com</p></div></div><div class="jo l"><div class="jv l jq jr js jo jt ju jf"/></div></div></a></div></div><div class="ab cl kh ki go kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="ha hb hc hd he"><h1 id="6a31" class="ko kp hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated"><strong class="ak">参考文献— </strong></h1><div class="jc jd ez fb je jf"><a href="https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/" rel="noopener  ugc nofollow" target="_blank"><div class="jg ab dw"><div class="jh ab ji cl cj jj"><h2 class="bd hi fi z dy jk ea eb jl ed ef hg bi translated">线性回归中的梯度下降</h2><div class="jm l"><h3 class="bd b fi z dy jk ea eb jl ed ef dx translated">在线性回归中，该模型的目标是获得最佳拟合的回归线，以根据预测值来预测y值</h3></div><div class="jn l"><p class="bd b fp z dy jk ea eb jl ed ef dx translated">www.geeksforgeeks.org</p></div></div><div class="jo l"><div class="oh l jq jr js jo jt ju jf"/></div></div></a></div><div class="jc jd ez fb je jf"><a rel="noopener follow" target="_blank" href="/towards-artificial-intelligence/gradient-descent-algorithm-explained-2fe9da0de9a2"><div class="jg ab dw"><div class="jh ab ji cl cj jj"><h2 class="bd hi fi z dy jk ea eb jl ed ef hg bi translated">梯度下降算法讲解</h2><div class="jm l"><h3 class="bd b fi z dy jk ea eb jl ed ef dx translated">一步一步的数学推导</h3></div><div class="jn l"><p class="bd b fp z dy jk ea eb jl ed ef dx translated">medium.com</p></div></div><div class="jo l"><div class="oi l jq jr js jo jt ju jf"/></div></div></a></div></div></div>    
</body>
</html>