<html>
<head>
<title>Weight Pruning with Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Keras进行重量修剪</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/weight-pruning-with-keras-86f742dbdb58?source=collection_archive---------3-----------------------#2021-03-28">https://medium.com/analytics-vidhya/weight-pruning-with-keras-86f742dbdb58?source=collection_archive---------3-----------------------#2021-03-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/afce27156df319ad67344be046d9809e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AXE3Tcxdebf71Ltcxajp6g.png"/></div></div></figure><p id="2731" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这篇博客中，我们将通过Keras了解权重修剪的概念。基本上，权重剪枝是一种模型优化技术。在权重剪枝中，它在训练过程中逐渐将模型权重归零，以实现模型稀疏。</p><p id="cb7a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这种技术通过模型压缩带来了改进。这种技术被广泛用于减少模型的延迟。</p><p id="f547" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我将在时尚MNIST数据集中实现权重修剪，在这里我对正常方法和修剪方法进行了比较。</p><p id="4681" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我将实施的例子将需要Tensorflow版本2.4以及</p><p id="fbf7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Tensorflow-model-optimization我们需要安装这个包。</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="cc2d" class="jx jy hi jt b fi jz ka l kb kc">pip install -q tensorflow-model-optimization</span></pre><h1 id="5310" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">导入必要的依赖关系</strong></h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="a00e" class="jx jy hi jt b fi jz ka l kb kc">import os<br/>import time<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import tempfile<br/>from sklearn.metrics import accuracy_score<br/>from sys import getsizeof</span><span id="c7ee" class="jx jy hi jt b fi la ka l kb kc">import tensorflow as tf<br/>import tensorflow_model_optimization as tfmot<br/>from tensorflow import keras<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, GlobalAvgPool2D, Dropout</span><span id="8a86" class="jx jy hi jt b fi la ka l kb kc">%load_ext tensorboard</span></pre><p id="9926" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们定义一些辅助函数来决定我们将要生成的模型的文件大小</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="fd43" class="jx jy hi jt b fi jz ka l kb kc"><strong class="jt hj">def</strong> get_file_size(file_path):<br/>    size = os.path.getsize(file_path)<br/>    <strong class="jt hj">return</strong> size<br/><br/><strong class="jt hj">def</strong> convert_bytes(size, unit=<strong class="jt hj">None</strong>):<br/>    <strong class="jt hj">if</strong> unit == "KB":<br/>        <strong class="jt hj">return</strong> print('File Size: ' + str(round(size/1024, 3)) + 'Kilobytes')<br/>    <strong class="jt hj">elif</strong> unit == 'MB':<br/>        <strong class="jt hj">return</strong> print('File Size: ' + str(round(size/(1024*1024), 3)) + 'Megabytes')<br/>    <strong class="jt hj">else</strong>:<br/>        <strong class="jt hj">return</strong> print('File Size: ' + str(size) + 'bytes')</span></pre><h1 id="e0b8" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">加载时尚MNIST数据集</h1><p id="d50d" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated"><a class="ae lg" href="https://github.com/zalandoresearch/fashion-mnist" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">时尚MNIST </strong> </a>数据集包含10个类别的70000张灰度图像。这些图像以低分辨率(28 x 28像素)显示了单件衣服，如下所示:</p><figure class="jo jp jq jr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/6d69f990be0234a75d9805f76ef8672e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GNLB2jtcfb_xTqgQd9ntJA.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated">时尚MNIST数据集</figcaption></figure><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="4b08" class="jx jy hi jt b fi jz ka l kb kc">fashion_mnist = tf.keras.datasets.fashion_mnist<br/>(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()<br/><br/><em class="lm">#Storing test labels</em><br/>test_labels = y_test<br/><br/>x_train = x_train.astype("float32") / 255.0<br/>x_train = np.reshape(x_train, (-1, 28, 28, 1))<br/>y_train = tf.one_hot(y_train, 10)<br/><br/>x_test = x_test.astype("float32") / 255.0<br/>x_test = np.reshape(x_test, (-1, 28, 28, 1))<br/>y_test = tf.one_hot(y_test, 10)</span></pre><h1 id="8f5c" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">定义标签</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="d286" class="jx jy hi jt b fi jz ka l kb kc">class_name = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']</span></pre><h2 id="da5b" class="jx jy hi bd ke ln lo lp ki lq lr ls km jb lt lu kq jf lv lw ku jj lx ly ky lz bi translated">显示训练的形状以及测试图像和标签</h2><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="0ff2" class="jx jy hi jt b fi jz ka l kb kc">print("Training Image Shape: ",x_train.shape) <br/>print("Training Label Shape", y_train.shape) <br/>print("Testing Image Shape: ",x_test.shape) <br/>print("Testing Label Shape", y_test.shape)</span><span id="cf00" class="jx jy hi jt b fi la ka l kb kc">Training Image Shape:  (60000, 28, 28, 1) <br/>Training Label Shape (60000, 10) <br/>Testing Image Shape:  (10000, 28, 28, 1) <br/>Testing Label Shape (10000, 10)</span></pre><h1 id="d9ba" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">定义超参数</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="de12" class="jx jy hi jt b fi jz ka l kb kc">AUTO = tf.data.AUTOTUNE<br/>BATCH_SIZE = 64<br/>EPOCHS = 10<br/>NUM_CLASSES=10</span></pre><h1 id="0617" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">创建数据管道</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="82ba" class="jx jy hi jt b fi jz ka l kb kc">train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))<br/><br/>train_ds = (<br/>    train_ds<br/>    .shuffle(BATCH_SIZE * 100)<br/>    .batch(BATCH_SIZE)<br/>)<br/><br/>test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))<br/><br/>test_ds = (<br/>    test_ds<br/>    .batch(BATCH_SIZE)<br/>)</span></pre><p id="4719" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="lm">流水线</em> </strong>准备好了！</p><h1 id="e682" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">可视化训练图像</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="9682" class="jx jy hi jt b fi jz ka l kb kc">sample_images, sample_labels = next(iter(train_ds))<br/>plt.figure(figsize=(10, 10))<br/><strong class="jt hj">for</strong> i, (image, label) <strong class="jt hj">in</strong> enumerate(zip(sample_images[:9], sample_labels[:9])):<br/>    ax = plt.subplot(3, 3, i + 1)<br/>    plt.imshow(image.numpy().squeeze())<br/>    plt.title(class_name[np.argmax(label.numpy().tolist())])<br/>    plt.axis("off")</span></pre><figure class="jo jp jq jr fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/51500f17d431ca74f05ee7d65ff6da50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*Fj0LlJ_qElcV56Du_cZemw.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">训练图像</figcaption></figure><h1 id="10c8" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">定义模型</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="eda0" class="jx jy hi jt b fi jz ka l kb kc">def training_model():<br/>    model = tf.keras.Sequential(<br/>        [<br/>            Conv2D(16, (5, 5), activation="relu", input_shape=(28, 28, 1)),<br/>            MaxPooling2D(pool_size=(2, 2)),<br/>            Conv2D(32, (5, 5), activation="relu"),<br/>            MaxPooling2D(pool_size=(2, 2)),<br/>            Dropout(0.2),<br/>            GlobalAvgPool2D(),<br/>            Flatten(),<br/>            Dense(128, activation="relu"),<br/>            Dense(NUM_CLASSES, activation="softmax"),<br/>        ]<br/>    )<br/>    return model</span></pre><p id="49a1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了再现性，我们将浅网络的初始随机权重序列化。</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="8cfb" class="jx jy hi jt b fi jz ka l kb kc">initial_model = training_model()<br/>initial_model.save_weights("initial_weights.h5")</span></pre><h1 id="e7ec" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">让我们编译和训练我们的模型</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="8d1e" class="jx jy hi jt b fi jz ka l kb kc">model.load_weights("initial_weights.h5")</span><span id="5d92" class="jx jy hi jt b fi la ka l kb kc">model.summary()</span><span id="bf39" class="jx jy hi jt b fi la ka l kb kc">model.compile(optimizer='adam',<br/>              loss="categorical_crossentropy",<br/>              metrics=['accuracy'])</span><span id="03f2" class="jx jy hi jt b fi la ka l kb kc">model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS)</span><span id="417d" class="jx jy hi jt b fi la ka l kb kc">test_loss, test_acc = model.evaluate(test_ds)<br/>print("Baseline Test accuracy: {:.2f}%".format(test_acc * 100))</span></pre><h1 id="d3ce" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">模型摘要</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="50b4" class="jx jy hi jt b fi jz ka l kb kc">Model: "sequential_1"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>conv2d_2 (Conv2D)            (None, 24, 24, 16)        416       <br/>_________________________________________________________________<br/>max_pooling2d_2 (MaxPooling2 (None, 12, 12, 16)        0         <br/>_________________________________________________________________<br/>conv2d_3 (Conv2D)            (None, 8, 8, 32)          12832     <br/>_________________________________________________________________<br/>max_pooling2d_3 (MaxPooling2 (None, 4, 4, 32)          0         <br/>_________________________________________________________________<br/>dropout_1 (Dropout)          (None, 4, 4, 32)          0         <br/>_________________________________________________________________<br/>global_average_pooling2d_1 ( (None, 32)                0         <br/>_________________________________________________________________<br/>flatten_1 (Flatten)          (None, 32)                0         <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 128)               4224      <br/>_________________________________________________________________<br/>dense_3 (Dense)              (None, 10)                1290      <br/>=================================================================<br/>Total params: 18,762<br/>Trainable params: 18,762<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="64fd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在训练我们的模型后，我们得到了86.97%的基线准确率。</p><h1 id="652f" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">保存基线模型</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="d996" class="jx jy hi jt b fi jz ka l kb kc">_, keras_file = tempfile.mkstemp('.h5')<br/>tf.keras.models.save_model(model, keras_file, include_optimizer=<strong class="jt hj">False</strong>)<br/><br/>print('Saved Baseline Model to:', keras_file)</span></pre><h1 id="f65f" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">带有修剪的微调模型</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="91d3" class="jx jy hi jt b fi jz ka l kb kc">prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude</span></pre><h1 id="3e2f" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">定义超参数</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="9915" class="jx jy hi jt b fi jz ka l kb kc">VALIDATION_SPLIT = 0.1 <!-- --># 10% of training set will be used for validation set.<br/>EPOCHS=6</span></pre><p id="dfa2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意:我们采用了更少的纪元，在我们的基线模型中大约是10</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="ea7d" class="jx jy hi jt b fi jz ka l kb kc">images, labels = next(iter(train_ds))<br/><br/>num_images = images.shape[0] * (1 - VALIDATION_SPLIT)<br/>end_step = np.ceil(num_images / BATCH_SIZE).astype(np.int32) * EPOCHS</span></pre><h1 id="0ac8" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">定义修剪模型</h1><p id="cd73" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">在本例中，模型以50%的稀疏度开始(50%的权重为零)，以80%的稀疏度结束。</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="a8e0" class="jx jy hi jt b fi jz ka l kb kc"># Define model for pruning<br/>pruning_params = {<br/>      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,                                                              final_sparsity=0.80,                                                             begin_step=0,                                                               end_step=end_step)<br/>}</span><span id="b972" class="jx jy hi jt b fi la ka l kb kc">model = training_model()<br/>model.load_weights("initial_weights.h5")<br/><br/>model_for_pruning = prune_low_magnitude(model, **pruning_params)</span></pre><h1 id="8731" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">需要重新编译</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="7b5b" class="jx jy hi jt b fi jz ka l kb kc">model_for_pruning.compile(optimizer='adam',<br/>              loss="categorical_crossentropy",<br/>              metrics=['accuracy'])<br/><br/>model_for_pruning.summary()</span></pre><h1 id="c9ad" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">摘要</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="6160" class="jx jy hi jt b fi jz ka l kb kc">Model: "sequential_2"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>prune_low_magnitude_conv2d_4 (None, 24, 24, 16)        818       <br/>_________________________________________________________________<br/>prune_low_magnitude_max_pool (None, 12, 12, 16)        1         <br/>_________________________________________________________________<br/>prune_low_magnitude_conv2d_5 (None, 8, 8, 32)          25634     <br/>_________________________________________________________________<br/>prune_low_magnitude_max_pool (None, 4, 4, 32)          1         <br/>_________________________________________________________________<br/>prune_low_magnitude_dropout_ (None, 4, 4, 32)          1         <br/>_________________________________________________________________<br/>prune_low_magnitude_global_a (None, 32)                1         <br/>_________________________________________________________________<br/>prune_low_magnitude_flatten_ (None, 32)                1         <br/>_________________________________________________________________<br/>prune_low_magnitude_dense_4  (None, 128)               8322      <br/>_________________________________________________________________<br/>prune_low_magnitude_dense_5  (None, 10)                2572      <br/>=================================================================<br/>Total params: 37,351<br/>Trainable params: 18,762<br/>Non-trainable params: 18,589<br/>_________________________________________________________________</span></pre><h1 id="39cb" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">训练你的修剪模型</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="0d71" class="jx jy hi jt b fi jz ka l kb kc">logdir = tempfile.mkdtemp()</span><span id="5bfc" class="jx jy hi jt b fi la ka l kb kc">callbacks = [<br/>  tfmot.sparsity.keras.UpdatePruningStep(),<br/>  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),<br/>]</span><span id="2a05" class="jx jy hi jt b fi la ka l kb kc">model_for_pruning.fit(train_ds, validation_data=test_ds, epochs=EPOCHS, callbacks=callbacks)<br/>_, model_for_pruning_accuracy = model_for_pruning.evaluate(test_ds)<br/>print("Pruned test accuracy: {:.2f}%".format(model_for_pruning_accuracy * 100))</span></pre><p id="6aee" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">修剪后的模型准确率为82.90%</p><h1 id="82a3" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">保存修剪模型</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="f5a6" class="jx jy hi jt b fi jz ka l kb kc">model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)<br/><br/>_, pruned_keras_file = tempfile.mkstemp('.h5')<br/>tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=<strong class="jt hj">False</strong>)<br/>print('Saved pruned Keras model to:', pruned_keras_file)</span></pre><p id="0d07" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在保存了相应的模型之后，我将保存的模型转换为TF-Lite模型。将模型转换成TF-Lite后，我决定对TF-Lite模型进行推理。</p><p id="d14d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意:我是在测试图像上做出的推断。</p><h1 id="919f" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">结果</h1><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="051a" class="jx jy hi jt b fi jz ka l kb kc">Test accuracy TFLITE Baseline Model : 0.829<br/>Test accuracy TFLITE Pruned Model : 0.829</span></pre><p id="a542" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以清楚地看到，在TF-Lite模型上进行推理时，两者的精度保持不变。</p><figure class="jo jp jq jr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/4de4c00a16f3bc9ce42e3ec0e2c7a493.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dvEUf35yQ_w7xTKU_Pss9A.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated">桌子</figcaption></figure><p id="1809" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从这个表中，我们可以得出结论，剪枝模型比基线模型更好</p><p id="4d6a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">笔记本链接:</strong></p><div class="mf mg ez fb mh mi"><a href="https://github.com/sayannath/Tensorflow-Notebooks/blob/main/Weight_Pruning_in_Keras_with_Fashion_MNIST.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hj fi z dy mn ea eb mo ed ef hh bi translated">sayan Nath/tensor flow-笔记本</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">我的Tensorflow笔记本。在这些笔记本中，我实现了各种模型优化技术。…</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">github.com</p></div></div></div></a></div><p id="f9b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Github简介</p><div class="mf mg ez fb mh mi"><a href="https://github.com/sayannath" rel="noopener  ugc nofollow" target="_blank"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hj fi z dy mn ea eb mo ed ef hh bi translated">sayannath -概述</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">我是萨彦纳特。我是KIIT大学本科二年级的学生。我是……的主要贡献者之一</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">github.com</p></div></div><div class="mr l"><div class="ms l mt mu mv mr mw io mi"/></div></div></a></div><h1 id="2927" class="kd jy hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">社交把手</h1><p id="a7d9" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated"><em class="lm">insta gram:</em><a class="ae lg" href="https://www.instagram.com/sayannath235/" rel="noopener ugc nofollow" target="_blank"><em class="lm">https://www.instagram.com/sayannath235</em></a><em class="lm">/</em></p><p id="1f4e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lm">领英:</em><a class="ae lg" href="https://www.linkedin.com/in/sayannath235/" rel="noopener ugc nofollow" target="_blank"><em class="lm">https://www.linkedin.com/in/sayannath235</em></a><em class="lm">/</em></p><p id="1fdd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lm">邮件:sayannath235@gmail.com</em></p></div></div>    
</body>
</html>