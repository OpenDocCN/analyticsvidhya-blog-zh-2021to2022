<html>
<head>
<title>Anomaly Detection in Images — AUTOENCODERS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图像中的异常检测—自动编码器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/anomaly-detection-in-images-autoencoders-b780abf88f51?source=collection_archive---------2-----------------------#2021-06-13">https://medium.com/analytics-vidhya/anomaly-detection-in-images-autoencoders-b780abf88f51?source=collection_archive---------2-----------------------#2021-06-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/3e3059c6ba5b4645b55adca97dfa7c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*khJZhEVg8ssaoao1KYNSbg.png"/></div></figure><h1 id="453a" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">简介:</h1><p id="110d" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><em class="ki"/><strong class="jm hj"><em class="ki">自动编码</em> </strong> <em class="ki">“是一种数据压缩算法，其中压缩和解压缩函数为:</em></p><ol class=""><li id="774f" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh kq kr ks kt bi translated"><strong class="jm hj"> <em class="ki">数据特定:</em> </strong> <em class="ki">这意味着他们只能压缩与他们所受训练相似的数据。</em></li><li id="21d0" class="kj kk hi jm b jn ku jr kv jv kw jz kx kd ky kh kq kr ks kt bi translated"><strong class="jm hj"> <em class="ki">有损:</em> </strong> <em class="ki">这意味着解压缩后的输出与原始输入相比会降低质量</em></li><li id="22f3" class="kj kk hi jm b jn ku jr kv jv kw jz kx kd ky kh kq kr ks kt bi translated"><strong class="jm hj"> <em class="ki">从例子中自动学习，而不是由人设计:</em> </strong> <em class="ki">这意味着很容易训练算法的特定实例，这些实例将在特定类型的输入上表现良好。它不需要任何新的工程，只需要适当的训练数据。</em></li></ol><p id="e1e9" class="pw-post-body-paragraph jk jl hi jm b jn kl jp jq jr km jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated"><em class="ki">它是一种</em> <strong class="jm hj"> <em class="ki">无监督</em> </strong> <em class="ki">学习技术，其中我们利用神经网络来完成</em> <strong class="jm hj"> <em class="ki">表征学习</em> </strong> <em class="ki">的任务。</em></p><ul class=""><li id="08ca" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">自动编码器</em> </strong> <em class="ki">由一个</em> <strong class="jm hj"> <em class="ki">编码器</em> </strong> <em class="ki">网络和一个</em> <strong class="jm hj"> <em class="ki">解码器</em> </strong> <em class="ki">网络组成。编码器将高维输入编码成低维潜在表示，也称为</em> <strong class="jm hj"> <em class="ki">【瓶颈】</em> </strong> <em class="ki"> </em> <strong class="jm hj"> <em class="ki">层</em> </strong> <em class="ki">。解码器采用这种低维潜在表示，并重构原始输入。</em></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/6c4b4de0d02c1fd042ebe4f771d4cb23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*xBRaaGowkNtPSrLCNfogVQ.png"/></div></figure><ul class=""><li id="8993" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh lc kr ks kt bi translated"><em class="ki">如上所述，我们可以将一个未标记的数据集构建为一个监督学习问题，任务是输出</em> <strong class="jm hj"> <em class="ki"> x̂ </em> </strong> <em class="ki">，原始输入</em><em class="ki"/><strong class="jm hj"><em class="ki">x</em></strong><em class="ki">的</em><em class="ki">重构。这个网络可以通过</em> <strong class="jm hj"> <em class="ki">训练，最小化</em> </strong> <em class="ki"> </em> <strong class="jm hj"> <em class="ki">重构</em></strong><em class="ki"/><strong class="jm hj"><em class="ki">误差</em></strong><em class="ki"/><strong class="jm hj"><em class="ki">【l(x,x̂】</em></strong><em class="ki">，从而度量出</em> <strong class="jm hj"> <em class="ki">之间的差异</em>T117】</strong></li></ul><h1 id="26e1" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">应用:</h1><p id="a360" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><strong class="jm hj">自动编码器</strong>有趣的实际应用有:</p><ol class=""><li id="7818" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh kq kr ks kt bi translated"><strong class="jm hj"> <em class="ki">数据去噪</em> </strong>、</li><li id="bcf1" class="kj kk hi jm b jn ku jr kv jv kw jz kx kd ky kh kq kr ks kt bi translated"><strong class="jm hj"> <em class="ki">降维进行数据可视化:</em> </strong> <em class="ki">通过适当的维度和稀疏约束，</em> <strong class="jm hj"> <em class="ki">自动编码器</em> </strong> <em class="ki">可以学习比</em><strong class="jm hj"><em class="ki">PCA</em></strong><em class="ki">或其他基本技术更有趣的数据投影。因为神经网络能够学习非线性关系，这可以被认为是PCA </em>  <em class="ki">的</em> <strong class="jm hj"> <em class="ki">更强大的(非线性)一般化。</em><strong class="jm hj"><em class="ki">PCA试图发现描述原始数据的低维超平面，而自动编码器能够学习非线性流形。</em> </strong></strong></li><li id="461f" class="kj kk hi jm b jn ku jr kv jv kw jz kx kd ky kh kq kr ks kt bi translated"><strong class="jm hj"> <em class="ki">图像识别、异常检测和语义分割。</em> </strong></li><li id="a0d2" class="kj kk hi jm b jn ku jr kv jv kw jz kx kd ky kh kq kr ks kt bi translated"><strong class="jm hj">推荐引擎。</strong></li></ol><h2 id="79d3" class="li in hi bd io lj lk ll is lm ln lo iw jv lp lq ja jz lr ls je kd lt lu ji lv bi translated">结构相似指数(SSIM)损失函数:</h2><ul class=""><li id="cbd5" class="kj kk hi jm b jn jo jr js jv lw jz lx kd ly kh lc kr ks kt bi translated"><strong class="jm hj"> SSIM </strong> <em class="ki">用作度量标准来衡量两个给定图像之间的相似性。</em></li><li id="535f" class="kj kk hi jm b jn ku jr kv jv kw jz kx kd ky kh lc kr ks kt bi translated"><strong class="jm hj"><em class="ki">【SSIM】</em></strong><em class="ki">结构相似度从一幅图像中提取3个关键特征:</em> <strong class="jm hj"> <em class="ki">亮度、对比度和结构。</em>T47】</strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/54d0643e47e76539bc99b8c2097f9d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*YFO0w4E7OV5aY-GLP6UC4w.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">来源:<a class="ae md" href="https://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf" rel="noopener ugc nofollow" target="_blank">https://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf</a></figcaption></figure><ul class=""><li id="9b93" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">两个给定图像之间的结构相似性指数</em> </strong> <em class="ki">是介于-1和+1之间的值。值+1表示两个给定图像</em> <strong class="jm hj"> <em class="ki">非常相似或相同</em> </strong> <em class="ki">，而值-1表示两个给定图像</em> <strong class="jm hj"> <em class="ki">非常不同。</em> </strong></li><li id="2b6a" class="kj kk hi jm b jn ku jr kv jv kw jz kx kd ky kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">对于相似的图像，SSIM损失函数会更小，而对于异常的图像，SSIM损失函数会更大。</em> </strong></li></ul><h1 id="0668" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">问题陈述:</h1><ul class=""><li id="9b95" class="kj kk hi jm b jn jo jr js jv lw jz lx kd ly kh lc kr ks kt bi translated"><em class="ki">给出了</em><strong class="jm hj"><em class="ki">【n】幅时尚图片——MNIST数据</em> </strong> <em class="ki">，其中包含了几幅</em><strong class="jm hj"><em class="ki">【m】幅图片来自MNIST手写数据</em> </strong> <em class="ki">。我们需要</em> <strong class="jm hj"> <em class="ki">在不使用任何迁移学习技术的情况下过滤掉异常。</em>T87】</strong></li></ul><h1 id="12d5" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">让我们看看如何处理这个问题陈述:</h1><ul class=""><li id="b623" class="kj kk hi jm b jn jo jr js jv lw jz lx kd ly kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">导入所需库</em> </strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es me"><img src="../Images/9a04464230a3b8501606f8f5716b375e.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*YBiqcR9QZVVM5xX-H1rneA.png"/></div></figure><ul class=""><li id="cefa" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">加载时尚MNIST训练和测试数据集，对其进行归一化和整形。</em>T95】</strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/83f42cb2a62e00930c2751b2375be687.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*UdBCpmfpa_Zlsg1vNseK0A.png"/></div></figure><h2 id="0688" class="li in hi bd io lj lk ll is lm ln lo iw jv lp lq ja jz lr ls je kd lt lu ji lv bi translated">构建自动编码器架构:</h2><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mg"><img src="../Images/c8a5a95e393346dc7fb28ae54e7cc79d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IDibF77bpApUyD6tzbei2Q.png"/></div></div></figure><ul class=""><li id="1286" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">构建编码器部分</em> </strong> <em class="ki"> </em> : <em class="ki">编码器将高维输入编码成低维潜在表示，也称为瓶颈层。</em></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es ml"><img src="../Images/bc6a1d7cc01bcaeb6ea665fcaf8a64ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2u-Ni_UrmBagVSbUwBF-tA.png"/></div></div></figure><ul class=""><li id="9fb9" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">构建解码器部分</em> </strong> : <em class="ki">解码器将对潜在表示进行解压缩，以重建输入数据。</em></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mm"><img src="../Images/cd21d8e3d163cf9b680ab9b8836f8389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RrLrKhOOE__oEHKBWuIu2Q.png"/></div></div></figure><ul class=""><li id="4dcb" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh lc kr ks kt bi translated"><em class="ki">输出层使用一个sigmoid激活函数，因为它将输出展平到范围[0，1]内。</em></li><li id="850f" class="kj kk hi jm b jn ku jr kv jv kw jz kx kd ky kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">定义结构相似指数(SSIM)损失函数:</em> </strong> <em class="ki">对于相似的图像，</em><strong class="jm hj"><em class="ki"/></strong><em class="ki">损失函数会比较小，对于异常的图像，SSIM损失函数会比较大。</em></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/e22d1dc9a46d0701b3a3bbe31d4db8b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*OOeApw5XHTZYUJTH7yvMAQ.png"/></div></figure><ul class=""><li id="744c" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">定义自动编码器:</em> </strong> <em class="ki">乐观者:亚当，损失:SSIM损失</em></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/d56b70aa55c5d054ddebe91d819e1a44.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*vjl5xY4jBVQVn4wzlFHbCQ.png"/></div></figure><ul class=""><li id="db3c" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">让我们来看看我们的自动编码器神经网络</em> </strong>的架构</li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/29e1602293ac567b0cad0522b3786458.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*uwQTYvinnxiesRJqBwEbPA.png"/></div></figure><ul class=""><li id="2f4f" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">设置TENSORBOARD为回调，用于测井损耗度量和训练:</em> </strong> <em class="ki">训练10个历元，批量为128。</em></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mq"><img src="../Images/1f4613bb6a97d483b76e5f84e931b51b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fppMcm5Eb1EhyvBI7-XiOA.png"/></div></div></figure><ul class=""><li id="b9c7" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">我们来查看一下TENSORBOARD中的训练和测试损耗:</em> </strong> <em class="ki">多运行几个历元的实验可以得到更好的结果。</em></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/4189dde3e266a583c4040d2d492642f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*x9o75FknBvrwAxGMNcJHMQ.png"/></div></figure><ul class=""><li id="5bf9" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">为测试数据重建时尚MNIST图像并可视化:</em> </strong> <em class="ki">将测试数据集传递给</em> <strong class="jm hj"> <em class="ki">自动编码器</em> </strong> <em class="ki">并预测重建的数据。</em> <strong class="jm hj"> <em class="ki">使原始图像和重建图像可视化。</em> </strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es ms"><img src="../Images/272124cba7ddb786835f32b7a9cedc98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*haHtnHlwfMwXJhXwR599ng.png"/></div></div></figure><h2 id="ee07" class="li in hi bd io lj lk ll is lm ln lo iw jv lp lq ja jz lr ls je kd lt lu ji lv bi translated">现在，我们的自动编码器已经被训练来从时尚MNIST数据中重建图像。 </h2><ul class=""><li id="1063" class="kj kk hi jm b jn jo jr js jv lw jz lx kd ly kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">现在，让我们介绍MNIST手写图像数据，我们的自动编码器模型将使用SSIM损失将其视为异常。</em>T73】</strong></li><li id="756d" class="kj kk hi jm b jn ku jr kv jv kw jz kx kd ky kh lc kr ks kt bi translated"><strong class="jm hj"> <em class="ki">加载MNIST手写的训练和测试数据，归一化并整形。</em>T77】</strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/9ac03b64450dc0a24de8e097bcd6e4ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*QHrN3Cvpdf2m6ctyX05C1g.png"/></div></figure><ul class=""><li id="b5ba" class="kj kk hi jm b jn kl jr km jv kn jz ko kd kp kh lc kr ks kt bi translated">现在，使用我们在时尚-MNIST数据集上训练的自动编码器来预测时尚-MNIST数据和MNIST手写数据，并检查那里的SSIM损失。T81】</li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mv"><img src="../Images/35bda07a2e2ac77d0ae9ee79fedbb53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7zjWK5PrDi9LbBHws8mv6Q.png"/></div></div></figure><h1 id="e5df" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">观察:</h1><ul class=""><li id="622e" class="kj kk hi jm b jn jo jr js jv lw jz lx kd ly kh lc kr ks kt bi translated"><em class="ki">从上图可以看出，对于训练数据集(时尚MNIST数据集)的重建而言，</em> <strong class="jm hj"> <em class="ki"> SSIM损失最小，然而，对于没有训练自动编码器的数据集(即MNIST手写)而言，SSIM损失较高。</em>T87】</strong></li></ul><h1 id="c993" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">结论:</h1><ul class=""><li id="4794" class="kj kk hi jm b jn jo jr js jv lw jz lx kd ly kh lc kr ks kt bi translated"><em class="ki">自动编码器适用于</em> <strong class="jm hj"> <em class="ki">识别异常</em> </strong> <em class="ki">。因为</em> <strong class="jm hj"> <em class="ki">自动编码器</em> </strong> <em class="ki">学习如何基于属性压缩数据(</em> <strong class="jm hj"> <em class="ki"> ie。输入特征向量</em> </strong> <em class="ki">)之间的相关性在</em> <strong class="jm hj"> <em class="ki">训练</em> </strong> <em class="ki">期间从数据中发现，这些模型通常只能重建与模型在训练期间观察到的观察值类别相似的数据。</em></li></ul><p id="9d9a" class="pw-post-body-paragraph jk jl hi jm b jn kl jp jq jr km jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">感谢你阅读这篇博客。如果你喜欢它，请鼓掌，关注并分享。T29】</p><h1 id="b85d" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">在哪里可以找到我的代码？</h1><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mw"><img src="../Images/30450deae1fba470c5149bcbb72ce53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CWJz0e0UfUw7CKfm.jpeg"/></div></div></figure><p id="cb00" class="pw-post-body-paragraph jk jl hi jm b jn kl jp jq jr km jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated"><strong class="jm hj">Github</strong>:<a class="ae md" href="https://github.com/SubhamIO/AnomalyDetection---Deep-AUTOENCODERS" rel="noopener ugc nofollow" target="_blank">https://github.com/SubhamIO/AnomalyDetection深度自动编码器</a></p><h1 id="8275" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">参考资料:</h1><ul class=""><li id="94d2" class="kj kk hi jm b jn jo jr js jv lw jz lx kd ly kh lc kr ks kt bi translated"><a class="ae md" href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/</a></li><li id="e41f" class="kj kk hi jm b jn ku jr kv jv kw jz kx kd ky kh lc kr ks kt bi translated"><a class="ae md" href="https://www.jeremyjordan.me/autoencoders/" rel="noopener ugc nofollow" target="_blank">https://www.jeremyjordan.me/autoencoders/</a></li><li id="9892" class="kj kk hi jm b jn ku jr kv jv kw jz kx kd ky kh lc kr ks kt bi translated"><a class="ae md" href="https://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf" rel="noopener ugc nofollow" target="_blank">https://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf</a></li><li id="960d" class="kj kk hi jm b jn ku jr kv jv kw jz kx kd ky kh lc kr ks kt bi translated"><a class="ae md" rel="noopener" href="/srm-mic/all-about-structural-similarity-index-ssim-theory-code-in-pytorch-6551b455541e">https://medium . com/SRM-mic/all-about-structural-similarity-index-ssim-theory-code-in-py torch-6551 b 455541 e</a></li></ul></div></div>    
</body>
</html>