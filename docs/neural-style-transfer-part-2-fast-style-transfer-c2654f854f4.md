# ç¥ç»é£æ ¼è½¬ç§»ç¬¬ 2 éƒ¨åˆ†:å¿«é€Ÿé£æ ¼è½¬ç§»

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/neural-style-transfer-part-2-fast-style-transfer-c2654f854f4?source=collection_archive---------16----------------------->

![](img/20eb45346ab02d2f8a5d3eb29258473d.png)

è¿™æ˜¯é£æ ¼è½¬æ¢ç³»åˆ—çš„ç¬¬ 2 éƒ¨åˆ†ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ç”¨ TensorFlow çš„ç†è®ºå’Œå®ç°æ¥ä»‹ç»ä½¿ç”¨è‡ªåŠ¨ç¼–ç å™¨çš„å¿«é€Ÿé£æ ¼è½¬æ¢ã€‚

è¿™æ˜¯ç¥ç»ç±»å‹è½¬æ¢çš„ç¬¬äºŒéƒ¨åˆ†ï¼Œåœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦ä¸€ç§ç±»å‹è½¬æ¢çš„æŠ€æœ¯ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºå¿«é€Ÿç±»å‹è½¬æ¢ã€‚è¿™æ˜¯å¯¹[ä¸Šä¸€ç¯‡æ–‡ç« ](https://tarunbisht11.medium.com/neural-style-transfer-part-1-introduction-dc17a3eb86d2)çš„è·Ÿè¿›ï¼Œå¦‚æœä½ æ­£åœ¨ç›´æ¥é˜…è¯»å®ƒçš„ç¬¬äºŒéƒ¨åˆ†ï¼Œé‚£ä¹ˆæˆ‘å»ºè®®ä½ å…ˆé˜…è¯»[ä¸Šä¸€éƒ¨åˆ†](https://tarunbisht11.medium.com/neural-style-transfer-part-1-introduction-dc17a3eb86d2)ï¼Œå› ä¸ºè®¸å¤šè¯é¢˜éƒ½æ˜¯ä»é‚£ç¯‡æ–‡ç« è·Ÿè¿›çš„ã€‚

åœ¨ gatys é£æ ¼è½¬ç§»ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰è®­ç»ƒä»»ä½•ç½‘ç»œï¼Œæˆ‘ä»¬åªæ˜¯æ ¹æ®æŸå¤±å‡½æ•°(style_loss + content_loss)ä¼˜åŒ–è¾“å‡ºå›¾åƒï¼Œå¹¶ä¸”ä¼˜åŒ–éœ€è¦ä¸€äº›è½®æ¬¡ï¼Œå› æ­¤ç”Ÿæˆé£æ ¼åŒ–å›¾åƒæ˜¯ä¸€ä¸ªéå¸¸ç¼“æ…¢çš„è¿‡ç¨‹ã€‚å°†è¿™ç§æŠ€æœ¯ç”¨äºå®æ—¶è§†é¢‘ğŸ˜­åˆ«æäº†ã€‚

è¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦ç”Ÿæˆå¤šä¸ªç›¸åŒæ ·å¼çš„å›¾åƒï¼Œå› ä¸ºæˆ‘ä»¬æ¯æ¬¡éƒ½åœ¨ä¸ºç›¸åŒæ ·å¼çš„å›¾åƒä¼˜åŒ–è¾“å‡ºå›¾åƒã€‚å¦‚æœæœ‰ä¸€ç§æ–¹æ³•å¯ä»¥å­¦ä¹ æ ·å¼å›¾åƒçš„è¾“å…¥-è¾“å‡ºæ˜ å°„ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ä¸€æ¬¡æ€§ç”Ÿæˆè¯¥æ ·å¼çš„å›¾åƒã€‚ğŸ¤”æ˜¯çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è‡ªåŠ¨ç¼–ç å™¨æ¥å­¦ä¹ è¾“å…¥å›¾åƒå’Œæ ·å¼åŒ–è¾“å‡ºå›¾åƒä¹‹é—´çš„æ˜ å°„ï¼Œæ–¹æ³•æ˜¯ä½¿ç”¨ä¹‹å‰å®šä¹‰çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒå®ƒã€‚

å¿«é€Ÿé£æ ¼è½¬ç§»è®©æˆ‘ä»¬è®­ç»ƒä¸€æ¬¡ï¼Œå¹¶ç”Ÿæˆæ— é™çš„å›¾åƒï¼Œæ˜¯çš„ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥è®¾è®¡è§†é¢‘ï¼Œç”šè‡³æ˜¯å®æ—¶ç½‘ç»œè§†é¢‘ã€‚

# è¦ç‚¹

å¿«é€Ÿçš„é£æ ¼è½¬æ¢è®©æˆ‘ä»¬ä¸€æ¬¡è®­ç»ƒå°±èƒ½ç”Ÿæˆæ— é™çš„å›¾åƒã€‚æˆ‘ä»¬è®¨è®ºçš„å…³äºæŸå¤±å‡½æ•°ç†è®ºçš„å¤§éƒ¨åˆ†è¦ç‚¹æ˜¯ç›¸åŒçš„ï¼Œè¿™é‡Œçš„ä¸»è¦åŒºåˆ«æ˜¯æˆ‘ä»¬å°†æ›´å¤šåœ°å…³æ³¨ä½¿ç”¨æŸå¤±å‡½æ•°çš„è®­ç»ƒæ¨¡å‹å’Œå­¦ä¹ æ˜ å°„ã€‚

åœ¨é˜…è¯»è¿™ç¯‡æ–‡ç« ä¹‹å‰ï¼Œå…ˆå¤ä¹ ä¸€ä¸‹å…³äºè‡ªåŠ¨ç¼–ç å™¨çš„çŸ¥è¯†ï¼Œç‰¹åˆ«æ˜¯å·ç§¯è‡ªåŠ¨ç¼–ç å™¨å’Œæ·±åº¦å­¦ä¹ ä¸­çš„æ®‹å·®å±‚(è·³è¿‡è¿æ¥),å› ä¸ºæˆ‘ä¸ä¼šè§£é‡Šå®ƒä»¬ï¼Œä½†æˆ‘ä»¬å°†åœ¨è¿™é‡Œå®ç°å®ƒä»¬ï¼Œæ‰€ä»¥å…ˆå¤ä¹ ä¸€äº›å…³äºå·ç§¯è‡ªåŠ¨ç¼–ç å™¨å’Œæ®‹å·®å±‚çš„åŸºæœ¬çŸ¥è¯†ï¼Œè¿™å°†æœ‰åŠ©äºè½»æ¾ç†è§£å®ç°

1.æˆ‘ä»¬è®­ç»ƒä¸€ä¸ªå‰é¦ˆç½‘ç»œï¼Œä½¿ç”¨ [Gatys ç­‰äºº](https://arxiv.org/abs/1508.06576)è®ºæ–‡ä¸­å®šä¹‰çš„æŸå¤±å‡½æ•°å°†è‰ºæœ¯é£æ ¼åº”ç”¨äºå›¾åƒï¼Œæ›´å¤šè§£é‡Šå‚è§[ä¹‹å‰çš„æ–‡ç« ](https://tarunbisht11.medium.com/neural-style-transfer-part-1-introduction-dc17a3eb86d2)ã€‚

2.æˆ‘ä»¬å°†ä½¿ç”¨çš„å‰é¦ˆç½‘ç»œæ˜¯ä¸€ä¸ªæ®‹å·®è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œï¼Œå®ƒå°†å†…å®¹å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªé£æ ¼åŒ–çš„å›¾åƒã€‚è¿™ä¸åœ¨[åŸå§‹å®ç°](https://arxiv.org/abs/1603.08155)ä¸­ä½¿ç”¨çš„ç½‘ç»œç›¸åŒ

3.Model è¿˜ä½¿ç”¨å®ä¾‹è§„èŒƒåŒ–ï¼Œè€Œä¸æ˜¯åŸºäºçº¸å¼ å®ä¾‹è§„èŒƒåŒ–çš„æ‰¹å¤„ç†è§„èŒƒåŒ–:å¿«é€Ÿæ ·å¼åŒ–ç¼ºå°‘çš„è¦ç´ ï¼Œå› ä¸ºè¿™æä¾›äº†æ›´å¥½çš„ç»“æœã€‚

4.æˆ‘ä»¬å°†ä½¿ç”¨ vgg19 æ¥è®¡ç®—æ„ŸçŸ¥æŸå¤±æ›´å¤šçš„å·¥ä½œæè¿°åœ¨çº¸ä¸Šã€‚

å¦‚æœç°åœ¨æœ‰äººæƒ³å°è¯•è§†é¢‘å’Œå›¾åƒçš„é£æ ¼è½¬æ¢ï¼Œæˆ‘å·²ç»ä¸ºåŒæ ·çš„ç›®çš„åˆ›å»ºäº†ä¸€ä¸ª [GitHub åº“](https://github.com/tarun-bisht/fast-style-transfer)ï¼Œå¹¶é™„æœ‰è¯´æ˜ã€‚

# å¯¼å…¥å¿…è¦çš„æ¨¡å—

è®©æˆ‘ä»¬ä»å¯¼å…¥æ‰€æœ‰å¿…éœ€çš„æ¨¡å—å¼€å§‹:

*   numpy:ç”¨äºæ•°ç»„æ“ä½œ
*   å¼ é‡æµ:ç”¨äºå¼ é‡è¿ç®—
*   tensor flow . keras:tensor flow çš„é«˜çº§ç¥ç»ç½‘ç»œåº“ï¼Œç”¨äºåˆ›å»ºç¥ç»ç½‘ç»œ
*   pillow:ç”¨äºå°†å›¾åƒè½¬æ¢ä¸º numpy æ•°ç»„ï¼Œå°† numpy æ•°ç»„è½¬æ¢ä¸ºå›¾åƒï¼Œä¿å­˜è¾“å‡ºå›¾åƒã€‚
*   æ—¶é—´:ç”¨äºè®¡ç®—æ¯æ¬¡è¿­ä»£çš„æ—¶é—´
*   matplotlib:ç”¨äºæ˜¾ç¤ºç¬”è®°æœ¬ä¸­çš„å›¾åƒå’Œå›¾å½¢
*   è¯·æ±‚ï¼Œbase64ï¼Œio:ç”¨äºä» URL ä¸‹è½½å’ŒåŠ è½½å›¾åƒ
*   os:æ“ä½œç³»ç»Ÿçº§å‘½ä»¤

```
**import** numpy **as** np
**import** tensorflow **as** tf
**from** tensorflow.keras.applications **import** vgg19
**from** tensorflow.keras.models **import** load_model,Model
**from** PIL **import** Image
**import** time
**import** matplotlib.pyplot **as** plt
**import** matplotlib
**import** requests
**import** base64
**import** os
**from** pathlib **import** Path
**from** io **import** BytesIO
matplotlib.rcParams['figure.figsize'] = (12,12)
matplotlib.rcParams['axes.grid'] = **False**
```

# å®šä¹‰æ•ˆç”¨å‡½æ•°

```
**def** **load_image**(image_path, dim=None, resize=False):
    img= Image.open(image_path)
    **if** dim:
        **if** resize:
            img=img.resize(dim)
        **else**:
            img.thumbnail(dim)
    img= img.convert("RGB")
    **return** np.array(img)
```

ä¸Šé¢çš„å‡½æ•°ç”¨äºä»æŒ‡å®šçš„è·¯å¾„åŠ è½½å›¾åƒå¹¶å°†å…¶è½¬æ¢æˆ numpy æ•°ç»„

```
**def** **load_url_image**(url,dim=None,resize=False):
    img_request=requests.get(url)
    img= Image.open(BytesIO(img_request.content))
    **if** dim:
        **if** resize:
            img=img.resize(dim)
        **else**:
            img.thumbnail(dim)
    img= img.convert("RGB")
    **return** np.array(img)
```

è¿™ä¸ªå‡½æ•°ä» URL åŠ è½½å›¾åƒï¼Œå¹¶å°†å…¶è½¬æ¢æˆ numpy æ•°ç»„

```
**def** **array_to_img**(array):
    array=np.array(array,dtype=np.uint8)
    **if** np.ndim(array)&gt;3:
        **assert** array.shape[0]==1
        array=array[0]
    **return** Image.fromarray(array)**def** **show_image**(image,title=None):
    **if** len(image.shape)&gt;3:
        image=tf.squeeze(image,axis=0)
    plt.imshow(image)
    **if** title:
        plt.title=title**def** **plot_images_grid**(images,num_rows=1):
    n=len(images)
    **if** n &gt; 1:
        num_cols=np.ceil(n/num_rows)
        fig,axes=plt.subplots(ncols=int(num_cols),nrows=int(num_rows))
        axes=axes.flatten()
        fig.set_size_inches((15,15))
        **for** i,image **in** enumerate(images):
            axes[i].imshow(image)
    **else**:
        plt.figure(figsize=(10,10))
        plt.imshow(images[0])
```

ä»¥ä¸Šä¸‰ä¸ªå‡½æ•°ç”¨äºè½¬æ¢å’Œç»˜åˆ¶å›¾åƒ:

*   å°†ä¸€ä¸ªæ•°ç»„è½¬æ¢æˆå›¾åƒ
*   show_image:ç»˜åˆ¶å•ä¸ªå›¾åƒ
*   plot_images_grid:åœ¨ç½‘æ ¼ä¸­ç»˜åˆ¶æ‰¹é‡å›¾åƒ

# å¿«é€Ÿé£æ ¼è½¬æ¢çš„æ­¥éª¤

è®­ç»ƒæ¨¡å‹æ˜¯å…·æœ‰æ®‹å·®å±‚çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚è¾“å…¥å›¾åƒè¢«ä¼ é€’åˆ°ç¼–ç å™¨éƒ¨åˆ†ï¼Œå¹¶ä¼ æ’­åˆ°è§£ç å™¨éƒ¨åˆ†ã€‚è¾“å‡ºä¸è¾“å…¥å¤§å°ç›¸åŒï¼Œå¹¶æ˜¾ç¤ºç”Ÿæˆçš„å›¾åƒã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯æ ¹æ®ä¸€ç§ç§°ä¸ºæ„ŸçŸ¥æŸå¤±çš„æŸå¤±è®­ç»ƒçš„ï¼Œè¿™ç§æŸå¤±çš„è®¡ç®—æ–¹æ³•ä¸æˆ‘ä»¬åœ¨ gatys é£æ ¼è½¬ç§»ä¸­è®¡ç®—çš„æ–¹æ³•ç›¸åŒã€‚ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä»å®šä¹‰çš„æ ·å¼å’Œå†…å®¹å±‚æå–ç‰¹å¾å›¾ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ¥è®¡ç®—æ ·å¼æŸå¤±å’Œå†…å®¹æŸå¤±ã€‚(æ›´å¤šç»†èŠ‚è¯·é˜…è¯»[ä¹‹å‰çš„å¸–å­](https://tarunbisht11.medium.com/neural-style-transfer-part-1-introduction-dc17a3eb86d2)ï¼Œé‚£é‡Œæœ‰è§£é‡Š)

ä½œä¸ºè®­ç»ƒæ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬éœ€è¦è®­ç»ƒæ•°æ®ï¼Œå¯¹äºè®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸åŒå›¾åƒçš„æ•°æ®é›†(å¯ä»¥æ˜¯ä»»ä½•åƒäººã€ç‹—ã€æ±½è½¦ç­‰)..)æ•£è£…ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯[å¯å¯æ•°æ®é›†](http://images.cocodataset.org/zips/train2014.zip)ï¼Œå®ƒæœ‰å¾ˆå¤šå›¾ç‰‡ã€‚æˆ‘ä¹Ÿä½¿ç”¨è¿‡ [Kaggle æŒ‘æˆ˜æ•°æ®é›†](https://www.kaggle.com/c/gan-getting-started)ï¼Œå®ƒæœ‰ä¸åŒé£æ™¯çš„å›¾åƒï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œæ£€æŸ¥ä»£ç å†…æ ¸[ã€‚æˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªæ ·å¼å›¾åƒï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ autoencoder å­¦ä¹ å®ƒçš„æ ·å¼ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•ç»˜ç”»æˆ–ç´ æ(ä»äº’è”ç½‘ä¸Šé€‰æ‹©ä¸€ä¸ª)](https://www.kaggle.com/tarunbisht11/generate-art-using-fast-style-transfer-in-a-second)

å¯¹äºè®­ç»ƒï¼Œè¯¥æ¨¡å‹æˆ‘ä»¬å°†ä¸€æ‰¹å„ç§å†…å®¹çš„è¾“å…¥è®­ç»ƒå›¾åƒå‘é€åˆ° autoencoderï¼Œauto encoder ä¸ºæˆ‘ä»¬æä¾›è¾“å‡ºã€‚è¯¥è¾“å‡ºå¿…é¡»æ˜¯æˆ‘ä»¬çš„é£æ ¼å›¾åƒï¼Œè€Œè®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬å°†è¿™äº›è¾“å‡ºå›¾åƒåˆ†æ‰¹ä¼ é€’åˆ°æˆ‘ä»¬çš„æŸå¤±æ¨¡å‹(vgg19 ),å¹¶æå–ä¸åŒå±‚(å†…å®¹å±‚å’Œé£æ ¼å±‚)çš„ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾ç„¶åç”¨äºè®¡ç®—é£æ ¼æŸå¤±å’Œå†…å®¹æŸå¤±ï¼Œå…¶åŠ æƒå’Œäº§ç”Ÿè®­ç»ƒç½‘ç»œçš„æ„ŸçŸ¥æŸå¤±ã€‚ä¸‹é¢è¿™å¼ æ¥è‡ªè®ºæ–‡çš„å›¾ç‰‡å¾ˆå¥½åœ°æè¿°äº†å®ƒã€‚

![](img/8d597d6d43c6576d28c7327ead5f3c5e.png)

ç»è¿‡è®­ç»ƒåï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯¥ç½‘ç»œä¸€æ¬¡æ€§è®¾è®¡ä»»ä½•å›¾åƒï¼Œè€Œæ— éœ€ä¼˜åŒ–

è¯¥ç½‘ç»œçš„ä¸»è¦äº®ç‚¹:

*   æ®‹ç•™å±‚
*   ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹
*   è§£ç å™¨çš„è¾“å‡ºè¢«ä¼ é€’åˆ°æŸè€—æ¨¡å‹(VGG)ä»¥è®¡ç®—æŸè€—
*   è®­ç»ƒéœ€è¦è®¡ç®—ï¼Œå› ä¸ºæˆ‘ä»¬æ¯ä¸€æ­¥éƒ½è¦å°†è¿™äº›å›¾åƒä¼ é€’ç»™ä¸¤ä¸ªç½‘ç»œ

# å®šä¹‰æŸå¤±

ä¸ºäº†è®¡ç®—é£æ ¼æŸå¤±å’Œå†…å®¹æŸå¤±ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ vgg19ï¼ŒåŸå§‹å®ç°ä½¿ç”¨ vgg16ã€‚

```
vgg=vgg19.VGG19(weights='imagenet',include_top=**False**)
vgg.summary()Model: "vgg19"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(**None**, **None**, **None**, 3)]   0         
_________________________________________________________________
block1_conv1 (Conv2D)        (**None**, **None**, **None**, 64)    1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (**None**, **None**, **None**, 64)    36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (**None**, **None**, **None**, 64)    0         
_________________________________________________________________
block2_conv1 (Conv2D)        (**None**, **None**, **None**, 128)   73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (**None**, **None**, **None**, 128)   147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (**None**, **None**, **None**, 128)   0         
_________________________________________________________________
block3_conv1 (Conv2D)        (**None**, **None**, **None**, 256)   295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (**None**, **None**, **None**, 256)   590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (**None**, **None**, **None**, 256)   590080    
_________________________________________________________________
block3_conv4 (Conv2D)        (**None**, **None**, **None**, 256)   590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (**None**, **None**, **None**, 256)   0         
_________________________________________________________________
block4_conv1 (Conv2D)        (**None**, **None**, **None**, 512)   1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (**None**, **None**, **None**, 512)   2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (**None**, **None**, **None**, 512)   2359808   
_________________________________________________________________
block4_conv4 (Conv2D)        (**None**, **None**, **None**, 512)   2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (**None**, **None**, **None**, 512)   0         
_________________________________________________________________
block5_conv1 (Conv2D)        (**None**, **None**, **None**, 512)   2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (**None**, **None**, **None**, 512)   2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (**None**, **None**, **None**, 512)   2359808   
_________________________________________________________________
block5_conv4 (Conv2D)        (**None**, **None**, **None**, 512)   2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (**None**, **None**, **None**, 512)   0         
=================================================================
Total params: 20,024,384
Trainable params: 20,024,384
Non-trainable params: 0
_________________________________________________________________
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å®šä¹‰å°†ç”¨äºè®¡ç®—æŸå¤±çš„å±‚ã€‚

```
content_layers=['block4_conv2']style_layers=['block1_conv1',
            'block2_conv1',
            'block3_conv1',
            'block4_conv1',
            'block5_conv1']
```

è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªç±»ï¼Œè¯¥ç±»ä½¿ç”¨ä¸€äº›é¢å¤–çš„æ–¹æ³•æ¥åˆ›å»ºæŸå¤±æ¨¡å‹ï¼Œä»¥ä¾¿ä»ç½‘ç»œè®¿é—®è¦ç´ åœ°å›¾ã€‚æˆ‘ä»¬åœ¨[ä¹‹å‰çš„æ–‡ç« ](https://tarunbisht11.medium.com/neural-style-transfer-part-1-introduction-dc17a3eb86d2)ä¸­ä¹Ÿä½¿ç”¨è¿‡è¿™äº›å‡½æ•°ï¼Œè¿™é‡Œæˆ‘ä»¬åªæ˜¯å°†å®ƒä»¬å°è£…åœ¨ä¸€ä¸ªç±»ä¸­ã€‚

```
**class** **LossModel**:
    **def** **__init__**(self,pretrained_model,content_layers,style_layers):
        self.model=pretrained_model
        self.content_layers=content_layers
        self.style_layers=style_layers
        self.loss_model=self.get_model() **def** **get_model**(self):
        self.model.trainable=**False**
        layer_names=self.style_layers + self.content_layers
        outputs=[self.model.get_layer(name).output **for** name **in** layer_names]
        new_model=Model(inputs=self.model.input,outputs=outputs)
        **return** new_model

    **def** **get_activations**(self,inputs):
        inputs=inputs*255.0
        style_length=len(self.style_layers)
        outputs=self.loss_model(vgg19.preprocess_input(inputs))
        style_output,content_output=outputs[:style_length],outputs[style_length:]
        content_dict={name:value **for** name,value **in** zip(self.content_layers,content_output)}
        style_dict={name:value **for** name,value **in** zip(self.style_layers,style_output)}
        **return** {'content':content_dict,'style':style_dict}
```

ç°åœ¨æˆ‘ä»¬ä½¿ç”¨ä¸Šé¢çš„ç±»åˆ›å»ºæˆ‘ä»¬çš„æŸå¤±æ¨¡å‹

```
loss_model = LossModel(vgg, content_layers, style_layers)
```

è®©æˆ‘ä»¬å®šä¹‰æŸå¤±å‡½æ•°æ¥è®¡ç®—å†…å®¹å’Œé£æ ¼æŸå¤±ï¼Œä¸‹é¢æ–¹æ³•`content_loss`å’Œ`style _loss`åˆ†åˆ«è®¡ç®—å†…å®¹å’Œé£æ ¼æŸå¤±ã€‚é€šè¿‡å¯¹è¿™äº›æŸå¤±è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œæˆ‘ä»¬å¾—åˆ°äº†å®šä¹‰åœ¨`preceptual_loss`å‡½æ•°ä¸­çš„æ„ŸçŸ¥æŸå¤±ã€‚è¿™äº›æŸå¤±å‡½æ•°çš„ç»†èŠ‚åŒ…å«åœ¨[å‰ä¸€ç¯‡æ–‡ç« ](https://tarunbisht11.medium.com/neural-style-transfer-part-1-introduction-dc17a3eb86d2)ä¸­ã€‚

```
**def** **content_loss**(placeholder,content,weight):
    **assert** placeholder.shape == content.shape
    **return** weight*tf.reduce_mean(tf.square(placeholder-content))**def** **gram_matrix**(x):
    gram=tf.linalg.einsum('bijc,bijd-&gt;bcd', x, x)
    **return** gram/tf.cast(x.shape[1]*x.shape[2]*x.shape[3],tf.float32)**def** **style_loss**(placeholder,style, weight):
    **assert** placeholder.shape == style.shape
    s=gram_matrix(style)
    p=gram_matrix(placeholder)
    **return** weight*tf.reduce_mean(tf.square(s-p))**def** **preceptual_loss**(predicted_activations,content_activations,
                    style_activations,content_weight,style_weight,
                    content_layers_weights,style_layer_weights):
    pred_content = predicted_activations["content"]
    pred_style = predicted_activations["style"]
    c_loss = tf.add_n([content_loss(pred_content[name],content_activations[name],
                                  content_layers_weights[i]) **for** i,name **in** enumerate(pred_content.keys())])
    c_loss = c_loss*content_weight
    s_loss = tf.add_n([style_loss(pred_style[name],style_activations[name],
                                style_layer_weights[i]) **for** i,name **in** enumerate(pred_style.keys())])
    s_loss = s_loss*style_weight
    **return** c_loss+s_loss
```

# åˆ›å»ºè‡ªåŠ¨ç¼–ç å™¨

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é¦–å…ˆä¸ºæˆ‘ä»¬çš„ç½‘ç»œå®šä¹‰äº†æ‰€æœ‰å¿…è¦çš„å±‚:

*   ReflectionPadding2D:ç”¨äºå°†åå°„å¡«å……åº”ç”¨åˆ° conv ç½‘ä¸­çš„å›¾åƒ
*   å®ä¾‹è§„èŒƒåŒ–:æˆ‘ä»¬ä½¿ç”¨å®ä¾‹è§„èŒƒåŒ–è€Œä¸æ˜¯æ‰¹å¤„ç†è§„èŒƒåŒ–ï¼Œå› ä¸ºå®ƒèƒ½ç»™å‡ºæ›´å¥½çš„ç»“æœã€‚å®ƒå°†é€šé“ä¸Šçš„è¾“å…¥æ ‡å‡†åŒ–ã€‚
*   conv layer:conv å›¾å±‚çš„å—ï¼Œå…·æœ‰å¡«å……-> conv _ å›¾å±‚->å®ä¾‹ _ è§„èŒƒåŒ–ç»„åˆ
*   å‰©ä½™å±‚:å…·æœ‰ä¸¤ä¸ªè½¬åŒ–å±‚å—çš„å‰©ä½™å±‚
*   UpsampleLayer:å¯¹ autoencoder ä¸­çš„ç“¶é¢ˆè¡¨ç¤ºè¿›è¡Œä¸Šé‡‡æ ·(å¦‚æœä½ è¯»è¿‡å…³äº auto encoder çš„æ–‡ç« ï¼Œä½ å°±ä¼šæ˜ç™½æˆ‘çš„æ„æ€)ã€‚å®ƒå¯ä»¥è¢«è®¤ä¸ºæ˜¯åå·ç§¯å±‚ã€‚

```
**class** **ReflectionPadding2D**(tf.keras.layers.Layer):
    **def** **__init__**(self, padding=(1, 1), **kwargs):
        super(ReflectionPadding2D, self).__init__(**kwargs)
        self.padding = tuple(padding)
    **def** **call**(self, input_tensor):
        padding_width, padding_height = self.padding
        **return** tf.pad(input_tensor, [[0,0], [padding_height, padding_height], 
                                     [padding_width, padding_width], [0,0] ], 'REFLECT')**class** **InstanceNormalization**(tf.keras.layers.Layer):
    **def** **__init__**(self,**kwargs):
        super(InstanceNormalization, self).__init__(**kwargs)
    **def** **call**(self,inputs):
        batch, rows, cols, channels = [i **for** i **in** inputs.get_shape()]
        mu, var = tf.nn.moments(inputs, [1,2], keepdims=**True**)
        shift = tf.Variable(tf.zeros([channels]))
        scale = tf.Variable(tf.ones([channels]))
        epsilon = 1e-3
        normalized = (inputs-mu)/tf.sqrt(var + epsilon)
        **return** scale * normalized + shift**class** **ConvLayer**(tf.keras.layers.Layer):
    **def** **__init__**(self,filters,kernel_size,strides=1,**kwargs):
        super(ConvLayer,self).__init__(**kwargs)
        self.padding=ReflectionPadding2D([k//2 **for** k **in** kernel_size])
        self.conv2d=tf.keras.layers.Conv2D(filters,kernel_size,strides)
        self.bn=InstanceNormalization()
    **def** **call**(self,inputs):
        x=self.padding(inputs)
        x=self.conv2d(x)
        x=self.bn(x)
        **return** x**class** **ResidualLayer**(tf.keras.layers.Layer):
    **def** **__init__**(self,filters,kernel_size,**kwargs):
        super(ResidualLayer,self).__init__(**kwargs)
        self.conv2d_1=ConvLayer(filters,kernel_size)
        self.conv2d_2=ConvLayer(filters,kernel_size)
        self.relu=tf.keras.layers.ReLU()
        self.add=tf.keras.layers.Add()
    **def** **call**(self,inputs):
        residual=inputs
        x=self.conv2d_1(inputs)
        x=self.relu(x)
        x=self.conv2d_2(x)
        x=self.add([x,residual])
        **return** x**class** **UpsampleLayer**(tf.keras.layers.Layer):
    **def** **__init__**(self,filters,kernel_size,strides=1,upsample=2,**kwargs):
        super(UpsampleLayer,self).__init__(**kwargs)
        self.upsample=tf.keras.layers.UpSampling2D(size=upsample)
        self.padding=ReflectionPadding2D([k//2 **for** k **in** kernel_size])
        self.conv2d=tf.keras.layers.Conv2D(filters,kernel_size,strides)
        self.bn=InstanceNormalization()
    **def** **call**(self,inputs):
        x=self.upsample(inputs)
        x=self.padding(x)
        x=self.conv2d(x)
        **return** self.bn(x)
```

ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„è¿™äº›å±‚ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå·ç§¯è‡ªåŠ¨ç¼–ç å™¨ã€‚

å»ºç­‘:

*   3 ä¸ª ConvLayer
*   5 ä¸ªæ®‹æ¸£å±‚
*   3 ä¸ªä¸Šé‡‡æ ·å±‚

```
**class** **StyleTransferModel**(tf.keras.Model):
    **def** **__init__**(self,**kwargs):
        super(StyleTransferModel, self).__init__(name='StyleTransferModel',**kwargs)
        self.conv2d_1= ConvLayer(filters=32,kernel_size=(9,9),strides=1,name="conv2d_1_32")
        self.conv2d_2= ConvLayer(filters=64,kernel_size=(3,3),strides=2,name="conv2d_2_64")
        self.conv2d_3= ConvLayer(filters=128,kernel_size=(3,3),strides=2,name="conv2d_3_128")
        self.res_1=ResidualLayer(filters=128,kernel_size=(3,3),name="res_1_128")
        self.res_2=ResidualLayer(filters=128,kernel_size=(3,3),name="res_2_128")
        self.res_3=ResidualLayer(filters=128,kernel_size=(3,3),name="res_3_128")
        self.res_4=ResidualLayer(filters=128,kernel_size=(3,3),name="res_4_128")
        self.res_5=ResidualLayer(filters=128,kernel_size=(3,3),name="res_5_128")
        self.deconv2d_1= UpsampleLayer(filters=64,kernel_size=(3,3),name="deconv2d_1_64")
        self.deconv2d_2= UpsampleLayer(filters=32,kernel_size=(3,3),name="deconv2d_2_32")
        self.deconv2d_3= ConvLayer(filters=3,kernel_size=(9,9),strides=1,name="deconv2d_3_3")
        self.relu=tf.keras.layers.ReLU()
    **def** **call**(self, inputs):
        x=self.conv2d_1(inputs)
        x=self.relu(x)
        x=self.conv2d_2(x)
        x=self.relu(x)
        x=self.conv2d_3(x)
        x=self.relu(x)
        x=self.res_1(x)
        x=self.res_2(x)
        x=self.res_3(x)
        x=self.res_4(x)
        x=self.res_5(x)
        x=self.deconv2d_1(x)
        x=self.relu(x)
        x=self.deconv2d_2(x)
        x=self.relu(x)
        x=self.deconv2d_3(x)
        x = (tf.nn.tanh(x) + 1) * (255.0 / 2)
        **return** x

    ## used to print shapes of each layer to check if input shape == output shape
    ## I don't know any better solution to this right now
    **def** **print_shape**(self,inputs):
        print(inputs.shape)
        x=self.conv2d_1(inputs)
        print(x.shape)
        x=self.relu(x)
        x=self.conv2d_2(x)
        print(x.shape)
        x=self.relu(x)
        x=self.conv2d_3(x)
        print(x.shape)
        x=self.relu(x)
        x=self.res_1(x)
        print(x.shape)
        x=self.res_2(x)
        print(x.shape)
        x=self.res_3(x)
        print(x.shape)
        x=self.res_4(x)
        print(x.shape)
        x=self.res_5(x)
        print(x.shape)
        x=self.deconv2d_1(x)
        print(x.shape)
        x=self.relu(x)
        x=self.deconv2d_2(x)
        print(x.shape)
        x=self.relu(x)
        x=self.deconv2d_3(x)
        print(x.shape)
```

åœ¨æ­¤å®šä¹‰è¾“å…¥å½¢çŠ¶å’Œæ‰¹é‡å¤§å°

```
input_shape=(256,256,3)
batch_size=4
```

ä½¿ç”¨`StyleTransferModel`ç±»åˆ›å»ºæ ·å¼æ¨¡å‹

```
style_model = StyleTransferModel()
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ£€æŸ¥æ‰€æœ‰å±‚çš„å½¢çŠ¶ï¼Œå¹¶éªŒè¯è¾“å…¥å½¢çŠ¶å’Œè¾“å‡ºå½¢çŠ¶

```
style_model.print_shape(tf.zeros(shape=(1,*input_shape)))(1, 256, 256, 3)
(1, 256, 256, 32)
(1, 128, 128, 64)
(1, 64, 64, 128)
(1, 64, 64, 128)
(1, 64, 64, 128)
(1, 64, 64, 128)
(1, 64, 64, 128)
(1, 64, 64, 128)
(1, 128, 128, 64)
(1, 256, 256, 32)
(1, 256, 256, 3)
```

# åŸ¹è®­æ¨¡å¼

è¿™é‡Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªç”¨äºè®­ç»ƒçš„ä¼˜åŒ–å™¨ï¼Œæˆ‘ä»¬ä½¿ç”¨å­¦ä¹ ç‡ä¸º 1e-3 çš„ Adam ä¼˜åŒ–å™¨

```
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)**def** **train_step**(dataset,style_activations,steps_per_epoch,style_model,loss_model,optimizer,
               checkpoint_path="./",content_weight=1e4, style_weight=1e-2,
               total_variation_weight=0.004):
    batch_losses=[]
    steps=1
    save_path=os.path.join(checkpoint_path,f"model_checkpoint.ckpt")
    print("Model Checkpoint Path: ",save_path)
    **for** input_image_batch **in** dataset:
        **if** steps-1 &gt;= steps_per_epoch:
            **break**
        **with** tf.GradientTape() **as** tape:
            outputs=style_model(input_image_batch)
            outputs=tf.clip_by_value(outputs, 0, 255)
            pred_activations=loss_model.get_activations(outputs/255.0)
            content_activations=loss_model.get_activations(input_image_batch)["content"] 
            curr_loss=preceptual_loss(pred_activations,content_activations,style_activations,content_weight,
                                      style_weight,content_layers_weights,style_layers_weights)
            curr_loss += total_variation_weight*tf.image.total_variation(outputs)
        batch_losses.append(curr_loss)
        grad = tape.gradient(curr_loss,style_model.trainable_variables)
        optimizer.apply_gradients(zip(grad,style_model.trainable_variables))
        **if** steps % 1000==0:
            print("checkpoint saved ",end=" ")
            style_model.save_weights(save_path)
            print(f"Loss: {tf.reduce_mean(batch_losses).numpy()}")
        steps+=1
    **return** tf.reduce_mean(batch_losses)
```

åœ¨ä¸Šé¢çš„å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå•ç‹¬çš„è®­ç»ƒæ­¥éª¤ã€‚åœ¨å‡½æ•°å†…éƒ¨:

*   é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºæ¨¡å‹æ£€æŸ¥ç‚¹å®šä¹‰äº† save_path
*   å¯¹äºæ¯ä¸ªæ—¶æœŸçš„æ­¥æ•°ï¼Œæˆ‘ä»¬è¿è¡Œä¸€ä¸ªè®­ç»ƒå¾ªç¯
*   å¯¹äºæ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬å‘å‰ä¼ é€’ä¸€æ‰¹å›¾åƒå°†å…¶ä¼ é€’ç»™æˆ‘ä»¬çš„æŸå¤±æ¨¡å‹
*   è·å–æ‰¹é‡å›¾åƒçš„å†…å®¹å±‚æ¿€æ´»
*   è¿åŒæ¥è‡ªæ ·å¼å›¾åƒçš„æ ·å¼æ¿€æ´»å’Œå†…å®¹æ¿€æ´»ï¼Œæˆ‘ä»¬è®¡ç®—æ„ŸçŸ¥æŸå¤±
*   ä¸ºäº†å¹³æ»‘ï¼Œæˆ‘ä»¬ç»™å›¾åƒå¢åŠ äº†ä¸€äº›æ€»å˜å·®æŸå¤±
*   è®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºæ¨¡å‹å¯è®­ç»ƒå‚æ•°çš„æ¢¯åº¦
*   æœ€ååå‘ä¼ æ’­ä¼˜åŒ–
*   æ¯ 1000 æ­¥ä¿å­˜ä¸€ä¸ªæ£€æŸ¥ç‚¹

# ä¸ºè®­ç»ƒé…ç½®æ•°æ®é›†

ä¸‹è½½ coco æ•°æ®é›†ç”¨äºè®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•å…¶ä»–æ‰¹é‡å›¾åƒçš„å›¾åƒæ•°æ®é›†ã€‚ä½¿ç”¨ wget ä»¥ zip æ ¼å¼ä¸‹è½½ coco æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œå°†ä¸‹è½½ zip æ–‡ä»¶è§£å‹ç¼©ã€‚

```
wget [http://images.cocodataset.org/zips/train2014.zip](http://images.cocodataset.org/zips/train2014.zip)--2020-07-12 08:14:59--  http://images.cocodataset.org/zips/train2014.zip
Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.224.88
Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.224.88|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 13510573713 (13G) [application/zip]
Saving to: â€˜train2014.zipâ€™train2014.zip       100%[===================&gt;]  12.58G  25.0MB/s    **in** 6m 56s 2020-07-12 08:21:55 (31.0 MB/s) - â€˜train2014.zipâ€™ saved [13510573713/13510573713]mkdir coco
unzip -qq train2014.zip -d coco
```

å¯¹äºè®­ç»ƒï¼Œè¯¥æ¨¡å‹å…è®¸åˆ›å»º tensorflow æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä»æŒ‡å®šçš„è·¯å¾„åŠ è½½æ‰€æœ‰å›¾åƒï¼Œè°ƒæ•´å®ƒä»¬çš„å¤§å°ä½¿å…¶å…·æœ‰ç›¸åŒçš„å¤§å°ï¼Œä»¥ä¾¿è¿›è¡Œæœ‰æ•ˆçš„æ‰¹é‡è®­ç»ƒï¼Œå¹¶å®ç°æ‰¹å¤„ç†å’Œé¢„å–ã€‚ä¸‹é¢çš„ç±»ä¸ºè®­ç»ƒåˆ›å»º tfdatasetã€‚

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨å›ºå®šå¤§å°çš„å›¾åƒè®­ç»ƒæ¨¡å‹ï¼Œä½†æˆ‘ä»¬å¯ä»¥ç”Ÿæˆä»»ä½•å¤§å°çš„å›¾åƒï¼Œå› ä¸ºæ¨¡å‹ä¸­çš„æ‰€æœ‰å±‚éƒ½æ˜¯å·ç§¯å±‚ã€‚

```
**class** **TensorflowDatasetLoader**:
    **def** **__init__**(self,dataset_path,batch_size=4, image_size=(256, 256),num_images=None):
        images_paths = [str(path) **for** path **in** Path(dataset_path).glob("*.jpg")]
        self.length=len(images_paths)
        **if** num_images **is** **not** **None**:
            images_paths = images_paths[0:num_images]
        dataset = tf.data.Dataset.from_tensor_slices(images_paths).map(
            **lambda** path: self.load_tf_image(path, dim=image_size),
            num_parallel_calls=tf.data.experimental.AUTOTUNE,
        )
        dataset = dataset.batch(batch_size,drop_remainder=**True**)
        dataset = dataset.repeat()
        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
        self.dataset=dataset
    **def** **__len__**(self):
        **return** self.length
    **def** **load_tf_image**(self,image_path,dim):
        image = tf.io.read_file(image_path)
        image = tf.image.decode_jpeg(image, channels=3)
        image= tf.image.resize(image,dim)
        image= image/255.0
        image = tf.image.convert_image_dtype(image, tf.float32)
        **return** image
```

ä½¿ç”¨ä¸Šé¢çš„ç±»ï¼Œè®©æˆ‘ä»¬ä» coco æ•°æ®é›†å›¾åƒåˆ›å»º tfdatasetã€‚æˆ‘ä»¬æŒ‡å®šå›¾åƒæ–‡ä»¶å¤¹çš„è·¯å¾„(æ‰€æœ‰å›¾åƒéƒ½åœ¨è¿™é‡Œ)å’Œæ‰¹é‡å¤§å°

```
loader=TensorflowDatasetLoader("coco/train2014/",batch_size=4)loader.dataset.element_specTensorSpec(shape=(4, 256, 256, 3), dtype=tf.float32, name=**None**)
```

ç»˜åˆ¶ä¸€äº›å›¾åƒä»¥æŸ¥çœ‹æ•°æ®é›†ä¸­çš„å›¾åƒ

```
plot_images_grid(next(iter(loader.dataset.take(1))))
```

![](img/d68216635928afc810ea1ead0fd04f53.png)

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨`load_url_image`ä» URL åŠ è½½æ ·å¼å›¾åƒå¹¶ç»˜åˆ¶å®ƒã€‚

```
# setting up style image
url="https://www.edvardmunch.org/images/paintings/the-scream.jpg"
style_image=load_url_image(url,dim=(input_shape[0],input_shape[1]),resize=**True**)
style_image=style_image/255.0show_image(style_image)
```

æ¥ä¸‹æ¥ï¼Œåˆ©ç”¨æŸå¤±æ¨¡å‹æå–é£æ ¼å›¾åƒçš„é£æ ¼å±‚ç‰¹å¾å›¾

```
style_image=style_image.astype(np.float32)
style_image_batch=np.repeat([style_image],batch_size,axis=0)
style_activations=loss_model.get_activations(style_image_batch)["style"]
```

# åŸ¹è®­æ¨¡å¼

å®šä¹‰å†…å®¹æƒé‡ã€æ ·å¼æƒé‡å’Œæ€»å˜åŒ–æƒé‡è¿™äº›æ˜¯æˆ‘ä»¬å¯ä»¥è°ƒæ•´ä»¥æ”¹å˜è¾“å‡ºå›¾åƒä¸­æ ·å¼å’Œå†…å®¹çš„å¼ºåº¦çš„è¶…å‚æ•°

```
content_weight=1e1
style_weight=1e2
total_variation_weight=0.004
```

ç°åœ¨å®šä¹‰è¦è®­ç»ƒçš„æ—¶æœŸæ•°ã€æ¯ä¸ªæ—¶æœŸçš„æ­¥æ•°å’Œæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„

```
epochs=2num_images=len(loader)
steps_per_epochs=num_images//batch_size
print(steps_per_epochs)20695save_path = "./scream"os.makedirs(save_path, exist_ok=**True**)
```

å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒé€šè¿‡ä»¥åŠç²¾åº¦æ ¼å¼æ‰§è¡Œè¿ç®—ï¼Œå®ƒæä¾›äº†æ˜¾è‘—çš„è®¡ç®—åŠ é€Ÿã€‚

```
**try**:
    policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')
    tf.keras.mixed_precision.experimental.set_policy(policy) 
**except**:
    **pass**
```

å¦‚æœå…ˆå‰çš„æ£€æŸ¥ç‚¹å­˜åœ¨äºè¯¥è·¯å¾„ï¼Œåˆ™åŠ è½½è¯¥æ£€æŸ¥ç‚¹å¹¶ç»§ç»­è¿›ä¸€æ­¥è®­ç»ƒï¼Œå¦åˆ™æˆ‘ä»¬ä»å¤´å¼€å§‹è®­ç»ƒ

```
**if** os.path.isfile(os.path.join(save_path,"model_checkpoint.ckpt.index")):
    style_model.load_weights(os.path.join(save_path,"model_checkpoint.ckpt"))
    print("resuming training ...")
**else**:
    print("training scratch ...")training scratch ...
```

æœ€åï¼Œæˆ‘ä»¬å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚åœ¨æ¯ä¸ªæ—¶æœŸï¼Œæˆ‘ä»¬è°ƒç”¨`train_step`å‡½æ•°ï¼Œè¯¥å‡½æ•°è¿è¡Œåˆ°æ¯ä¸ªæ—¶æœŸå®šä¹‰çš„æ­¥éª¤æ•°ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªæ—¶æœŸä¹‹åä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œç”¨äºè¿›ä¸€æ­¥çš„æ¨ç†å’Œè®­ç»ƒã€‚

```
epoch_losses=[]
**for** epoch **in** range(1,epochs+1):
    print(f"epoch: {epoch}")
    batch_loss=train_step(loader.dataset,style_activations,steps_per_epochs,style_model,loss_model,optimizer,
                          save_path,
                          content_weight,style_weight,total_variation_weight,
                          content_layers_weights,style_layers_weights)
    style_model.save_weights(os.path.join(save_path,"model_checkpoint.ckpt"))
    print("Model Checkpointed at: ",os.path.join(save_path,"model_checkpoint.ckpt"))
    print(f"loss: {batch_loss.numpy()}")
    epoch_losses.append(batch_loss)epoch: 1
Model Checkpoint Path:  ./scream/model_checkpoint.ckpt
checkpoint saved  Loss: 6567731.5
checkpoint saved  Loss: 6464426.5
checkpoint saved  Loss: 6402768.0
checkpoint saved  Loss: 6336974.5
checkpoint saved  Loss: 6281922.5
checkpoint saved  Loss: 6232056.0
checkpoint saved  Loss: 6191586.5
checkpoint saved  Loss: 6155332.0
checkpoint saved  Loss: 6119712.5
checkpoint saved  Loss: 6085571.5
checkpoint saved  Loss: 6062698.0
checkpoint saved  Loss: 6036787.0
checkpoint saved  Loss: 6011265.5
checkpoint saved  Loss: 5988809.5
checkpoint saved  Loss: 5969908.0
checkpoint saved  Loss: 5950925.0
checkpoint saved  Loss: 5931179.5
checkpoint saved  Loss: 5912791.5
checkpoint saved  Loss: 5894602.0
checkpoint saved  Loss: 5880713.0
Model Checkpointed at:  ./scream
loss: 5869695.5
epoch: 2
Model Checkpoint Path:  ./scream/model_checkpoint.ckpt
checkpoint saved  Loss: 5520494.5
checkpoint saved  Loss: 5532450.5
checkpoint saved  Loss: 5529669.0
checkpoint saved  Loss: 5524684.0
checkpoint saved  Loss: 5518524.5
checkpoint saved  Loss: 5508913.5
checkpoint saved  Loss: 5503493.5
checkpoint saved  Loss: 5501864.0
checkpoint saved  Loss: 5497016.0
checkpoint saved  Loss: 5491713.0
checkpoint saved  Loss: 5491244.5
checkpoint saved  Loss: 5484620.0
checkpoint saved  Loss: 5482881.0
checkpoint saved  Loss: 5476766.5
checkpoint saved  Loss: 5472491.0
checkpoint saved  Loss: 5466294.5
checkpoint saved  Loss: 5459984.0
checkpoint saved  Loss: 5454912.5
checkpoint saved  Loss: 5449535.5
checkpoint saved  Loss: 5446370.0
Model Checkpointed at:  ./scream
loss: 5442546.5
```

åœ¨è®­ç»ƒæ¨¡å‹ä¹‹åï¼Œè®©ç»˜åˆ¶ä¸æ—¶æœŸç›¸å…³çš„æŸå¤±ï¼Œå¹¶æ£€æŸ¥æŸå¤±æ‘˜è¦

```
plt.plot(epoch_losses)
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Process")
plt.show()
```

![](img/a29608337530d1fd411dc7bc3e02c982.png)

ç°åœ¨æ˜¯æ—¶å€™ç”Ÿæˆä¸€äº›æ ·å¼å›¾åƒäº†ã€‚æˆ‘ä»¬é¦–å…ˆå°†ä¿å­˜çš„æ¨¡å‹æ£€æŸ¥ç‚¹åŠ è½½åˆ° autoencoder ä¸­ã€‚

```
**if** os.path.isfile(os.path.join(save_path,"model_checkpoint.ckpt.index")):
    style_model.load_weights(os.path.join(save_path,"model_checkpoint.ckpt"))
    print("loading weights ...")
**else**:
    print("no weights found ...")loading weights ...
```

åŠ è½½å›¾åƒè¿›è¡Œæ ·å¼åŒ–ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæµ®ç‚¹å‹ã€‚

```
test_image_url="https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/chicago-skyline-on-a-clear-day-royalty-free-image-115891582-1557159569.jpg"test_image=load_url_image(test_image_url,dim=(640,480))
test_image=np.expand_dims(test_image,axis=0)test_image=test_image.astype(np.float32)
```

åœ¨æ¨¡å‹çš„ä¸€æ¬¡å‘å‰ä¼ é€’ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°ç”Ÿæˆçš„æ ·å¼å›¾åƒ

```
predicted_image=style_model(test_image)
```

å°†ç”Ÿæˆçš„å›¾åƒåƒç´ ç®ä½åœ¨ 0 åˆ° 255 ä¹‹é—´ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º uint8ã€‚æˆ‘ä»¬å¾—åˆ°äº†æˆ‘ä»¬ç”Ÿæˆçš„é£æ ¼å›¾åƒï¼Œç»˜åˆ¶å®ƒå¹¶æ£€æŸ¥å®ƒçš„å¤–è§‚ï¼Œè¿˜ä¿å­˜å®ƒå¹¶ä¸æœ‹å‹åˆ†äº«

```
predicted_image=np.clip(predicted_image,0,255)
predicted_image=predicted_image.astype(np.uint8)test_output=test_image.astype(np.uint8)
test_output=tf.squeeze(test_output).numpy()
predicted_output=tf.squeeze(predicted_image).numpy()plot_images_grid([test_output,predicted_output])
```

![](img/7fae5e2b8c87ece1f8ec80fa4fb523ce.png)

å¦‚æœä½ æ²¡æœ‰è¶³å¤Ÿçš„è®¡ç®—èƒ½åŠ›ï¼Œä½¿ç”¨ colab æˆ– kaggle å†…æ ¸ï¼Œä»–ä»¬æä¾›å…è´¹çš„ GPU ç”šè‡³ TPU æ¥è®­ç»ƒè¿™äº›æ¨¡å‹ï¼Œä¸€æ—¦è®­ç»ƒå®Œæ¯•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ£€æŸ¥ç‚¹åœ¨ä»»ä½•æœ‰ GPU æˆ– CPU çš„ç³»ç»Ÿä¸­è¿›è¡Œé£æ ¼è½¬æ¢ã€‚

ä½¿ç”¨`opencv`,æˆ‘ä»¬ä¹Ÿå¯ä»¥è½»æ¾åˆ›å»ºé£æ ¼è§†é¢‘ã€‚

# ç»“æœ

ä¸€äº›å›¾åƒç»“æœ

![](img/9d984165b4867ca837d678d1a2db0bed.png)![](img/32196514533bbba2ad1699a058dbd11c.png)![](img/ef22dea7bce9e50dad92a2e787c1a46e.png)

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰å®æ—¶è§†é¢‘é£æ ¼åŒ–çš„è¡ŒåŠ¨

ä¸‹é¢æ˜¯è§†é¢‘é£æ ¼åŒ–çš„è¡ŒåŠ¨

ç°åœ¨ç”Ÿæˆä¸åŒçš„å›¾åƒå’Œè§†é¢‘ï¼Œç©å®ƒï¼Œåˆ†äº«ä»¤äººå…´å¥‹çš„ç»“æœã€‚

å¦‚æœç°åœ¨æœ‰äººæƒ³å°è¯•è§†é¢‘å’Œå›¾åƒçš„é£æ ¼è½¬æ¢ï¼Œæˆ‘å·²ç»ä¸ºåŒæ ·çš„ç›®çš„åˆ›å»ºäº†ä¸€ä¸ª [github åº“](https://github.com/tarun-bisht/fast-style-transfer)ï¼Œå¹¶é™„æœ‰è¯´æ˜ã€‚

æ„Ÿè°¢é˜…è¯»ã€‚âœŒâœŒâœŒ

# å‚è€ƒ

[å®ä¾‹è§„èŒƒåŒ–:å¿«é€Ÿé£æ ¼åŒ–ç¼ºå°‘çš„è¦ç´ ](https://arxiv.org/abs/1607.08022)

[å®æ—¶é£æ ¼è½¬æ¢å’Œè¶…åˆ†è¾¨ç‡çš„æ„ŸçŸ¥æŸå¤±](https://arxiv.org/abs/1603.08155)

[è‰ºæœ¯é£æ ¼çš„ç¥ç»ç®—æ³•](https://arxiv.org/abs/1508.06576)

# é‡è¦é“¾æ¥

[Github åº“](https://github.com/tarun-bisht/fast-style-transfer)

[è°·æ­Œ Colab ç¬”è®°æœ¬](https://github.com/tarun-bisht/blogs-notebooks/blob/master/style-transfer/Neural%20Style%20Transfer%20Part%202.ipynb)

[Youtube è§†é¢‘](https://www.youtube.com/watch?v=GrS4rWifdko)