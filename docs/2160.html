<html>
<head>
<title>3D Reconstruction News — WACV 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">3D重建新闻— WACV 2021</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/3d-reconstruction-news-wacv-2021-4a8e504aea19?source=collection_archive---------11-----------------------#2021-04-08">https://medium.com/analytics-vidhya/3d-reconstruction-news-wacv-2021-4a8e504aea19?source=collection_archive---------11-----------------------#2021-04-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="fcdd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">【免责声明:3D重建新闻是一系列博客文章，我在其中回顾了在某次会议上对深度估计、MVS、SfM、VO、VSLAM和其他与从图像进行大规模户外3D重建相关的领域的最佳贡献】</em></p><p id="0e96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">就这样，今年的首届计算机视觉大会(<a class="ae jd" href="http://wacv2021.thecvf.com/home" rel="noopener ugc nofollow" target="_blank"> WACV 2021 </a>)结束了。我们是否更擅长仅从图像中找出世界的三维结构？算是吧。以下是我的三点建议:</p><ul class=""><li id="0adf" class="je jf hh ig b ih ii il im ip jg it jh ix ji jb jj jk jl jm bi translated">美国宇航局利用3D重建寻找火星上的安全着陆点。</li><li id="9abf" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">注意机制满足多视角立体(再次)。</li><li id="b4c3" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">单目深度可以通过全景分割图或连续适应来改善。</li></ul></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h2 id="d879" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">计算约束UAS的单目图像序列稠密三维重建</h2><p id="5432" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">美国宇航局的新型<a class="ae jd" href="https://mars.nasa.gov/technology/helicopter/" rel="noopener ugc nofollow" target="_blank">火星直升机</a>需要在未知地形中探测安全着陆点，仅使用一个向下的摄像头和机载处理能力【1】。单目选择的动机是飞行中的直升机和地形之间的距离比任何潜在的立体基线都要高得多，从而导致深度误差。另一方面，单目设置不能预先校准，需要精确的相机姿态估计。同时，3D重建应该足够密集以允许着陆点检测，并且足够高效以满足计算约束。</p><p id="564d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所有这些需求之间的权衡通过以下轻量级管道来实现:</p><ul class=""><li id="dbc3" class="je jf hh ig b ih ii il im ip jg it jh ix ji jb jj jk jl jm bi translated">视觉惯性里程计前端通过跟踪快速特征来计算姿态先验(是的，<a class="ae jd" href="https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test" rel="noopener ugc nofollow" target="_blank">快速</a>是算法的名称)。</li><li id="6c1c" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">关键帧从输入流中选择，并与附加的相机姿态一起收集在滑动窗口缓冲区中，这些相机姿态通过窗口束调整进行局部优化。</li><li id="5d2a" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">选择并校正一对关键帧，然后使用传统的立体算法来计算深度。</li></ul><p id="93b9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有趣的奖励:在<a class="ae jd" href="http://ceres-solver.org/" rel="noopener ugc nofollow" target="_blank"> Ceres </a>、<a class="ae jd" href="https://github.com/RainerKuemmerle/g2o" rel="noopener ugc nofollow" target="_blank"> g2o </a>和<a class="ae jd" href="https://gtsam.org/" rel="noopener ugc nofollow" target="_blank"> GTSAM </a>之间有一个很好的比较，显示Ceres对于这种小问题明显更快！</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h2 id="6f3c" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">多视角立体视觉的远程注意网络</h2><p id="2d20" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">注意机制在计算机视觉中变得无处不在，多视图立体视觉(MVS)也不例外(例如参见[2])。在这项名为LANet的新工作中，作者利用注意力来捕捉像素之间的每视图和多视图空间相关性，这与之前计算像素成本量的MVS网络形成对比[3]。第一个贡献是远程注意力模块(LAM)的设计，该模块产生一组注意力特征图来馈送给网络。参考特征图中像素之间的相互依赖性被编码在全局描述符中，然后该全局描述符与来自每个源图像的注意力向量相结合，以计算多视图依赖性。然后，类似于其他基线方法，对这些特征进行扭曲、融合和处理。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es kz"><img src="../Images/71036cfde6711dfcb89d9d669a7a6c7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UOEyZ6oey0sYZm7P2SuMkw.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">远程注意模块(LAM)。图片来自[3]。</figcaption></figure><p id="9f09" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第二个贡献是一个新的中间损失，明确监督概率体积，将其约束为高斯分布。MVS网络将深度估计为概率体积上的期望，但是先前的工作忽略了不同分布可能共享相同期望的事实。理想情况下，分布应该近似为高斯分布，并以真实深度为中心。为此，他们生成地面真实概率体，并用预测的概率体计算交叉熵损失，以惩罚非高斯分布。下图显示了基线方法[4]和LANet之间的比较。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lp"><img src="../Images/76a701eb8de0a83cd7edaffb743d70d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KpWdU16y3HpSFt0l7KVRvw.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">左:CasMVSNet预测一个分散的深度分布，右:LANet预测近似高斯。图片来自[3]。</figcaption></figure><p id="3fe5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个个人的，非请求的意见:尽管有有趣的贡献，我认为多视图立体应该远离像这样的监督方法，拥抱自我监督。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h2 id="9ef0" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">使用全景分割图提升单目深度</h2><p id="3e62" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">最近，已经做了很多工作来将语义信息包括到单目深度估计框架中。这篇新论文[5]表明全景分割[6]是一条正确的道路，因为深度不连续性往往出现在每个特定对象的边界，而不是像传统的语义分割图那样出现在通用类的边界。下图为不熟悉的人展示了语义分割、实例分割和全景分割的区别。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lq"><img src="../Images/738834ab7bbbc660430774fb1a9b790e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3v6ZKnXTjDuI7re6ZKD8xQ.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">语义分割、实例分割和全景分割的区别。图片来自[6]。</figcaption></figure><p id="2591" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个想法是通过引入一个新的全景增强损失而发展起来的，它包括三个方面:</p><ul class=""><li id="4bee" class="je jf hh ig b ih ii il im ip jg it jh ix ji jb jj jk jl jm bi translated">全景左右一致性项惩罚左全景地图和通过将左全景地图扭曲到右视图，然后再回到左视图而获得的地图之间的差异。这应该可以捕捉到当前预测深度的累积误差。</li><li id="de37" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">全景引导对准术语在全景不连续的地方加强深度不连续(见下图)。</li><li id="3aea" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">全景引导平滑项旨在移除通常出现在RGB图像中而不是深度图中的冗余边缘。</li></ul><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lr"><img src="../Images/f6616be3019aae9dd058cb4f7770403f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5fnIeZqlpI6OMi1gq7hjgw.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">左:输入图像，中心:全景分割图的边缘，右:深度图的边缘。图片来自[5]。</figcaption></figure><p id="ebca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种新的损失，需要地面真实全景图或预先计算的全景图的监督，与标准的几何无监督损失相结合，用于训练非常类似于具有混合监督的Monodepth 2 [7]的架构。</p><p id="a8be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个很酷的实用技巧:如果对语义图进行缩减采样，会出现奇怪的伪像。因此，低分辨率比例的损失是通过对深度进行上采样来计算的，而不是对全景图进行下采样。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h2 id="7c75" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">CoMoDA:使用过去经验的连续单目深度适应</h2><p id="9bdb" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">你有没有想过训练完一个模特之后会发生什么？常见的假设是，网络参数将保持固定，因此浪费了许多从新数据中学习的潜在信息。Kuznietzov和他的同事[8]通过应用持续学习策略，在新的测试视频上即时调整预训练模型(即Monodepth 2，再次[7])，结合了最近和遥远过去的经验，解决了这个问题。</p><p id="3f68" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">提议的框架建立在一个基线上，其工作方式如下。给定一个预训练的模型和一个新的视频，估计没有自适应的前两帧的深度，然后从第三帧开始估计每一帧的深度:</p><ul class=""><li id="e317" class="je jf hh ig b ih ii il im ip jg it jh ix ji jb jj jk jl jm bi translated">估计前一帧与其两个相邻帧之间的摄像机运动。</li><li id="2983" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">如果不满足最小运动约束，请转到下一步。否则，使用姿态估计和先前推断的深度从其邻居重建先前的帧，然后计算损失并更新网络参数。</li><li id="25c1" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">估计当前帧的深度图并继续。</li></ul><p id="0b9f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，这种原始基线太不稳定，无法实现任何性能改进，因此本文的主要贡献是引入了存储一批样本(即三个连续图像)的重放缓冲区:一个来自测试视频，几个随机取自过去的经验，无论是最近的还是遥远的。这允许避免过度拟合并稳定适应，产生令人印象深刻的结果。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="9a39" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参考文献</strong></p><p id="3f64" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[1]多姆尼克等人，<em class="jc">计算约束UAS单目图像序列的密集三维重建，</em> WACV 2021</p><p id="f339" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2]罗等，【注意感知多视点立体视觉】，2020</p><p id="ee0c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3]张等，<em class="jc">多视点立体视觉远程注意网络</em>，WACV 2021</p><p id="a072" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4]晓东等，<em class="jc">高分辨率多视点立体与立体匹配的级联代价体积</em>，CVPR 2020</p><p id="4d9d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[5] Saeedan等人，<em class="jc">使用全景分割图增强单目深度</em>，WACV 2021</p><p id="48a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[6]基里洛夫等，<em class="jc">全景分割</em>，CVPR 2019</p><p id="fd36" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[7] Godard等人，<em class="jc">挖掘自监督单目深度预测</em>，ICCV 2019</p><p id="ec74" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[8] Kuznietzov等人，<em class="jc"> CoMoDA:使用过去经验的连续单目深度适应</em>，WACV 2021</p></div></div>    
</body>
</html>