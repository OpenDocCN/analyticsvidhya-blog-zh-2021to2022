<html>
<head>
<title>Azure Synapse Workspace End to End Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Azure Synapse Workspace端到端机器学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/azure-synapse-workspace-end-to-end-machine-learning-1b52d9492e4e?source=collection_archive---------18-----------------------#2021-06-23">https://medium.com/analytics-vidhya/azure-synapse-workspace-end-to-end-machine-learning-1b52d9492e4e?source=collection_archive---------18-----------------------#2021-06-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="f076" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">这是一个端到端的示例演示，展示了spark、专用sql池和机器学习</h1><h1 id="d4da" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">用例</h1><ul class=""><li id="d6f9" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">使用SQL专用池的数据工程和ETL</li><li id="55c0" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">使用Synapse Spark的数据工程和ETL</li><li id="fd1a" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">展示如何调用管道的案例</li><li id="a9ba" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">端到端的编排显示</li></ul><h1 id="cc58" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">步伐</h1><ul class=""><li id="a53f" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">端到端架构</li><li id="86b3" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">展示我们如何在规模上不匹配spark、SQL和机器学习的案例</li></ul><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es jz"><img src="../Images/89b9db90bf6faf1fd0d945d0d13fe0e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ubJXxXMjYrTfjuZue0Bu2w.jpeg"/></div></div></figure><ul class=""><li id="bdcc" class="jc jd hh je b jf kl jh km jj kn jl ko jn kp jp jq jr js jt bi translated">端到端解释</li><li id="456c" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">恢复管道以启动专用池</li><li id="7d93" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">使用HTTP活动</li><li id="98d4" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">使用方法:贴</li></ul><pre class="ka kb kc kd fd kq kr ks kt aw ku bi"><span id="3020" class="kv if hh kr b fi kw kx l ky kz"><a class="ae la" href="https://management.azure.com/subscriptions/subscriptionid/resourceGroups/resourcegroup/providers/Microsoft.Synapse/workspaces/workspacename/sqlPools/dedicatedpoolname/resume?api-version=2019-06-01-preview" rel="noopener ugc nofollow" target="_blank">https://management.azure.com/subscriptions/subscriptionid/resourceGroups/resourcegroup/providers/Microsoft.Synapse/workspaces/workspacename/sqlPools/dedicatedpoolname/resume?api-version=2019-06-01-preview</a></span></pre><ul class=""><li id="6e67" class="jc jd hh je b jf kl jh km jj kn jl ko jn kp jp jq jr js jt bi translated">现在清除聚合表以加载新的表</li><li id="b4ba" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">我使用存储过程来清理数据</li></ul><pre class="ka kb kc kd fd kq kr ks kt aw ku bi"><span id="c22f" class="kv if hh kr b fi kw kx l ky kz">DROP PROCEDURE [dbo].[dropdailyaggr]<br/>GO<br/><br/>SET ANSI_NULLS ON<br/>GO<br/>SET QUOTED_IDENTIFIER ON<br/>GO<br/><br/>CREATE PROC [dbo].[dropdailyaggr] AS<br/>Drop Table [wwi].[dailyaggr]<br/>GO</span></pre><ul class=""><li id="4215" class="jc jd hh je b jf kl jh km jj kn jl ko jn kp jp jq jr js jt bi translated">使用pyspark的NYC yellow ETL/数据工程管道示例</li><li id="12df" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">使用pyspark</li></ul><pre class="ka kb kc kd fd kq kr ks kt aw ku bi"><span id="56e4" class="kv if hh kr b fi kw kx l ky kz">from azureml.opendatasets import NycTlcYellow<br/><br/>data = NycTlcYellow()<br/>data_df = data.to_spark_dataframe()<br/># Display 10 rows<br/>display(data_df.limit(10))</span><span id="e119" class="kv if hh kr b fi lb kx l ky kz">from pyspark.sql.functions import * <br/>from pyspark.sql import *</span><span id="1301" class="kv if hh kr b fi lb kx l ky kz">df1 = data_df.withColumn("Date", (col("tpepPickupDateTime").cast("date"))) <br/>display(df1)</span><span id="4765" class="kv if hh kr b fi lb kx l ky kz">df2 = df1.withColumn("year", year(col("date"))) .withColumn("month", month(col("date"))) .withColumn("day", dayofmonth(col("date"))) .withColumn("hour", hour(col("date")))</span><span id="d4bb" class="kv if hh kr b fi lb kx l ky kz">dfgrouped = df2.groupBy("year","month").agg(sum("fareAmount").alias("Total"),count("vendorID").alias("Count")).sort(asc("year"), asc("month"))</span><span id="adff" class="kv if hh kr b fi lb kx l ky kz">dfgrouped.repartition(1).write.mode('overwrite').parquet("/dailyaggr/parquet/dailyaggr.parquet")</span><span id="6c34" class="kv if hh kr b fi lb kx l ky kz">dfgrouped.repartition(1).write.mode('overwrite').option("header","true").csv("/dailyaggrcsv/csv/dailyaggr.csv")</span><span id="2063" class="kv if hh kr b fi lb kx l ky kz">df2.createOrReplaceTempView("nycyellow")</span><span id="a4dd" class="kv if hh kr b fi lb kx l ky kz">%%sql <br/>select * from nycyellow limit 100</span><span id="f8fb" class="kv if hh kr b fi lb kx l ky kz">%%sql<br/>select  year(cast(tpepPickupDateTime  as timestamp)) as tsYear,<br/>        month(cast(tpepPickupDateTime  as timestamp)) as tsmonth,<br/>        day(cast(tpepPickupDateTime  as timestamp)) as tsDay, <br/>        hour(cast(tpepPickupDateTime  as timestamp)) as tsHour,<br/>        avg(totalAmount) as avgTotal, avg(fareAmount) as avgFare<br/>from nycyellow<br/>group by  tsYear, tsmonth,tsDay, tsHour<br/>order by  tsYear, tsmonth,tsDay, tsHour</span><span id="788b" class="kv if hh kr b fi lb kx l ky kz">%%sql <br/>DROP TABLE dailyaggr</span><span id="b0e5" class="kv if hh kr b fi lb kx l ky kz">%%sql<br/>CREATE TABLE dailyaggr<br/>  COMMENT 'This table is created with existing data'<br/>  AS select  year(cast(tpepPickupDateTime  as timestamp)) as tsYear,<br/>        month(cast(tpepPickupDateTime  as timestamp)) as tsmonth,<br/>        day(cast(tpepPickupDateTime  as timestamp)) as tsDay, <br/>        hour(cast(tpepPickupDateTime  as timestamp)) as tsHour,<br/>        avg(totalAmount) as avgTotal, avg(fareAmount) as avgFare<br/>from nycyellow<br/>group by  tsYear, tsmonth,tsDay, tsHour<br/>order by  tsYear, tsmonth,tsDay, tsHour</span><span id="c4de" class="kv if hh kr b fi lb kx l ky kz">dailyaggr = spark.sql("SELECT tsYear, tsMonth, tsDay, tsHour, avgTotal FROM dailyaggr")<br/>display(dailyaggr)</span><span id="7a15" class="kv if hh kr b fi lb kx l ky kz">%%spark<br/>import com.microsoft.spark.sqlanalytics.utils.Constants<br/>import org.apache.spark.sql.SqlAnalyticsConnector._</span><span id="3731" class="kv if hh kr b fi lb kx l ky kz">from pyspark.ml.regression import LinearRegression</span><span id="a699" class="kv if hh kr b fi lb kx l ky kz">%%pyspark <br/>import pyspark <br/>print(print(pyspark.__version__))</span><span id="0034" class="kv if hh kr b fi lb kx l ky kz">%%spark<br/>import org.apache.spark.ml.feature.VectorAssembler <br/>import org.apache.spark.ml.linalg.Vectors <br/>val dailyaggr = spark.sql("SELECT tsYear, tsMonth, tsDay, tsHour, avgTotal FROM dailyaggr")<br/>val featureCols=Array("tsYear","tsMonth","tsDay","tsHour") <br/>val assembler: org.apache.spark.ml.feature.VectorAssembler= new VectorAssembler().setInputCols(featureCols).setOutputCol("features") <br/>val assembledDF = assembler.setHandleInvalid("skip").transform(dailyaggr) <br/>val assembledFinalDF = assembledDF.select("avgTotal","features")</span><span id="d274" class="kv if hh kr b fi lb kx l ky kz">%%spark<br/>import com.microsoft.spark.sqlanalytics.utils.Constants<br/>import org.apache.spark.sql.SqlAnalyticsConnector._</span><span id="1b87" class="kv if hh kr b fi lb kx l ky kz">%%spark dailyaggr.repartition(2).write.synapsesql("accsynapsepools.wwi.dailyaggr", Constants.INTERNAL)</span><span id="04ce" class="kv if hh kr b fi lb kx l ky kz">%%spark<br/>import org.apache.spark.ml.feature.Normalizer <br/>val normalizedDF = new Normalizer().setInputCol("features").setOutputCol("normalizedFeatures").transform(assembledFinalDF)</span><span id="e187" class="kv if hh kr b fi lb kx l ky kz">%%spark val normalizedDF1 = normalizedDF.na.drop()</span><span id="24e2" class="kv if hh kr b fi lb kx l ky kz">%%spark <br/>val Array(trainingDS, testDS) = normalizedDF1.randomSplit(Array(0.7, 0.3))</span><span id="bd3b" class="kv if hh kr b fi lb kx l ky kz">%%spark<br/>import org.apache.spark.ml.regression.LinearRegression<br/>// Create a LinearRegression instance. This instance is an Estimator. <br/>val lr = new LinearRegression().setLabelCol("avgTotal").setMaxIter(100)<br/>// Print out the parameters, documentation, and any default values. println(s"Linear Regression parameters:\n ${lr.explainParams()}\n") <br/>// Learn a Linear Regression model. This uses the parameters stored in lr.<br/>val lrModel = lr.fit(trainingDS)<br/>// Make predictions on test data using the Transformer.transform() method.<br/>// LinearRegression.transform will only use the 'features' column. <br/>val lrPredictions = lrModel.transform(testDS)</span><span id="e206" class="kv if hh kr b fi lb kx l ky kz">%%spark<br/>import org.apache.spark.sql.functions._ <br/>import org.apache.spark.sql.types._ <br/>println("\nPredictions : " ) <br/>lrPredictions.select($"avgTotal".cast(IntegerType),$"prediction".cast(IntegerType)).orderBy(abs($"prediction"-$"avgTotal")).distinct.show(15)</span><span id="0a62" class="kv if hh kr b fi lb kx l ky kz">%%spark<br/>import org.apache.spark.ml.evaluation.RegressionEvaluator <br/><br/>val evaluator_r2 = new RegressionEvaluator().setPredictionCol("prediction").setLabelCol("avgTotal").setMetricName("r2") <br/>//As the name implies, isLargerBetter returns if a larger value is better or smaller for evaluation. <br/>val isLargerBetter : Boolean = evaluator_r2.isLargerBetter <br/>println("Coefficient of determination = " + evaluator_r2.evaluate(lrPredictions))</span><span id="66d2" class="kv if hh kr b fi lb kx l ky kz">%%spark<br/>//Evaluate the results. Calculate Root Mean Square Error <br/>val evaluator_rmse = new RegressionEvaluator().setPredictionCol("prediction").setLabelCol("avgTotal").setMetricName("rmse") <br/>//As the name implies, isLargerBetter returns if a larger value is better for evaluation. <br/>val isLargerBetter1 : Boolean = evaluator_rmse.isLargerBetter <br/>println("Root Mean Square Error = " + evaluator_rmse.evaluate(lrPredictions))</span><span id="a567" class="kv if hh kr b fi lb kx l ky kz">%%spark <br/>val dailyaggrdf = spark.read.synapsesql("accsynapsepools.wwi.dailyaggr")</span><span id="4b5e" class="kv if hh kr b fi lb kx l ky kz">%%spark <br/>display(dailyaggrdf)</span><span id="d2f7" class="kv if hh kr b fi lb kx l ky kz">%%spark <br/>dailyaggrdf.count()</span></pre><ul class=""><li id="ae94" class="jc jd hh je b jf kl jh km jj kn jl ko jn kp jp jq jr js jt bi translated">scala代码中的纽约假日</li></ul><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es lc"><img src="../Images/863ddd0d04aeaeeebacffc03c5e2d738.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A3w9mShlVVlty8VCPXIp3g.jpeg"/></div></div></figure><pre class="ka kb kc kd fd kq kr ks kt aw ku bi"><span id="5e17" class="kv if hh kr b fi kw kx l ky kz">// Load nyc green taxi trip records from azure open dataset<br/>val blob_account_name = "azureopendatastorage"<br/><br/>val nyc_blob_container_name = "nyctlc"<br/>val nyc_blob_relative_path = "green"<br/>val nyc_blob_sas_token = ""<br/><br/>val nyc_wasbs_path = f"wasbs://$nyc_blob_container_name@$blob_account_name.blob.core.windows.net/$nyc_blob_relative_path"<br/>spark.conf.set(f"fs.azure.sas.$nyc_blob_container_name.$blob_account_name.blob.core.windows.net",nyc_blob_sas_token)<br/><br/>val nyc_tlc = spark.read.parquet(nyc_wasbs_path)</span><span id="d624" class="kv if hh kr b fi lb kx l ky kz">// Filter data by time range<br/>import java.sql.Timestamp<br/>import org.joda.time.DateTime<br/><br/>val end_date = new Timestamp(DateTime.parse("2018-06-06").getMillis)<br/>val start_date = new Timestamp(DateTime.parse("2018-05-01").getMillis)<br/><br/>val nyc_tlc_df = nyc_tlc.filter((nyc_tlc("lpepPickupDatetime") &gt;= start_date) &amp;&amp; (nyc_tlc("lpepPickupDatetime") &lt;= end_date)) <br/>nyc_tlc_df.show(5, truncate = false)</span><span id="1649" class="kv if hh kr b fi lb kx l ky kz">// Extract month, day of month, and day of week from pickup datetime and add a static column for the country code to join holiday data. <br/>import org.apache.spark.sql.functions._<br/><br/>val nyc_tlc_df_expand = (<br/>                        nyc_tlc_df.withColumn("datetime", to_date(col("lpepPickupDatetime")))<br/>                                  .withColumn("month_num",month(col("lpepPickupDatetime")))<br/>                                  .withColumn("day_of_month",dayofmonth(col("lpepPickupDatetime")))<br/>                                  .withColumn("day_of_week",dayofweek(col("lpepPickupDatetime")))<br/>                                  .withColumn("hour_of_day",hour(col("lpepPickupDatetime")))<br/>                                  .withColumn("country_code",lit("US"))<br/>                        )</span><span id="e3f2" class="kv if hh kr b fi lb kx l ky kz">// Remove unused columns from nyc green taxi data<br/>val nyc_tlc_df_clean = nyc_tlc_df_expand.drop(<br/>                    "lpepDropoffDatetime", "puLocationId", "doLocationId", "pickupLongitude", <br/>                     "pickupLatitude", "dropoffLongitude","dropoffLatitude" ,"rateCodeID", <br/>                     "storeAndFwdFlag","paymentType", "fareAmount", "extra", "mtaTax",<br/>                     "improvementSurcharge", "tollsAmount", "ehailFee", "tripType" )</span><span id="01d7" class="kv if hh kr b fi lb kx l ky kz">// Display 5 rows<br/>nyc_tlc_df_clean.show(5, truncate = false)</span><span id="b5a6" class="kv if hh kr b fi lb kx l ky kz">// Load public holidays data from azure open dataset<br/>val hol_blob_container_name = "holidaydatacontainer"<br/>val hol_blob_relative_path = "Processed"<br/>val hol_blob_sas_token = ""<br/><br/>val hol_wasbs_path = f"wasbs://$hol_blob_container_name@$blob_account_name.blob.core.windows.net/$hol_blob_relative_path"<br/>spark.conf.set(f"fs.azure.sas.$hol_blob_container_name.$blob_account_name.blob.core.windows.net",hol_blob_sas_token)<br/><br/>val hol_raw = spark.read.parquet(hol_wasbs_path)<br/><br/>// Filter data by time range<br/>val hol_df = hol_raw.filter((hol_raw("date") &gt;= start_date) &amp;&amp; (hol_raw("date") &lt;= end_date))<br/><br/>// Display 5 rows<br/>// hol_df.show(5, truncate = false)</span><span id="4e53" class="kv if hh kr b fi lb kx l ky kz">val hol_df_clean = (<br/>                hol_df.withColumnRenamed("countryRegionCode","country_code")<br/>                .withColumn("datetime",to_date(col("date")))<br/>                )<br/><br/>hol_df_clean.show(5, truncate = false)</span><span id="7b78" class="kv if hh kr b fi lb kx l ky kz">// enrich taxi data with holiday data<br/>val nyc_taxi_holiday_df = nyc_tlc_df_clean.join(hol_df_clean, Seq("datetime", "country_code") , "left")<br/><br/>nyc_taxi_holiday_df.show(5,truncate = false)</span><span id="5ff4" class="kv if hh kr b fi lb kx l ky kz">// Create a temp table and filter out non empty holiday rows<br/><br/>nyc_taxi_holiday_df.createOrReplaceTempView("nyc_taxi_holiday_df")<br/>val result = spark.sql("SELECT * from nyc_taxi_holiday_df WHERE holidayName is NOT NULL ")<br/>result.show(5, truncate = false)</span><span id="97b4" class="kv if hh kr b fi lb kx l ky kz">// Load weather data from azure open dataset<br/>val weather_blob_container_name = "isdweatherdatacontainer"<br/>val weather_blob_relative_path = "ISDWeather/"<br/>val weather_blob_sas_token = ""<br/><br/>val weather_wasbs_path = f"wasbs://$weather_blob_container_name@$blob_account_name.blob.core.windows.net/$weather_blob_relative_path"<br/>spark.conf.set(f"fs.azure.sas.$weather_blob_container_name.$blob_account_name.blob.core.windows.net",hol_blob_sas_token)<br/><br/>val isd = spark.read.parquet(weather_wasbs_path)<br/><br/>// Display 5 rows<br/>// isd.show(5, truncate = false)</span><span id="164a" class="kv if hh kr b fi lb kx l ky kz">// Filter data by time range<br/>val isd_df = isd.filter((isd("datetime") &gt;= start_date) &amp;&amp; (isd("datetime") &lt;= end_date))<br/><br/>// Display 5 rows<br/>isd_df.show(5, truncate = false)</span><span id="4b25" class="kv if hh kr b fi lb kx l ky kz">al weather_df = (<br/>                isd_df.filter(isd_df("latitude") &gt;= "40.53")<br/>                        .filter(isd_df("latitude") &lt;= "40.88")<br/>                        .filter(isd_df("longitude") &gt;= "-74.09")<br/>                        .filter(isd_df("longitude") &lt;= "-73.72")<br/>                        .filter(isd_df("temperature").isNotNull)<br/>                        .withColumnRenamed("datetime","datetime_full")<br/>                        )</span><span id="4928" class="kv if hh kr b fi lb kx l ky kz">// Remove unused columns<br/>val weather_df_clean = weather_df.drop("usaf", "wban", "longitude", "latitude").withColumn("datetime", to_date(col("datetime_full")))<br/><br/>//weather_df_clean.show(5, truncate = false)</span><span id="3e38" class="kv if hh kr b fi lb kx l ky kz">val weather_df_grouped = (<br/>                        weather_df_clean.groupBy('datetime).<br/>                        agg(<br/>                            mean('snowDepth) as "avg_snowDepth",<br/>                            max('precipTime) as "max_precipTime",<br/>                            mean('temperature) as "avg_temperature",<br/>                            max('precipDepth) as "max_precipDepth"<br/>                            )<br/>                        )<br/><br/>weather_df_grouped.show(5, truncate = false)</span><span id="fb7f" class="kv if hh kr b fi lb kx l ky kz">// Enrich taxi data with weather<br/>val nyc_taxi_holiday_weather_df = nyc_taxi_holiday_df.join(weather_df_grouped, Seq("datetime") ,"left")<br/>nyc_taxi_holiday_weather_df.cache()</span><span id="942d" class="kv if hh kr b fi lb kx l ky kz">nyc_taxi_holiday_weather_df.show(5,truncate = false)</span><span id="2b9f" class="kv if hh kr b fi lb kx l ky kz">// Run the describe() function on the new dataframe to see summary statistics for each field.<br/>display(nyc_taxi_holiday_weather_df.describe())</span><span id="b042" class="kv if hh kr b fi lb kx l ky kz">nyc_taxi_holiday_weather_df.count</span><span id="0a0b" class="kv if hh kr b fi lb kx l ky kz">// Remove invalid rows with less than 0 taxi fare or tip<br/>val final_df = (<br/>            nyc_taxi_holiday_weather_df.<br/>            filter(nyc_taxi_holiday_weather_df("tipAmount") &gt; 0).<br/>            filter(nyc_taxi_holiday_weather_df("totalAmount") &gt; 0)<br/>            )</span><span id="2efb" class="kv if hh kr b fi lb kx l ky kz">spark.sql("DROP TABLE IF EXISTS NYCTaxi.nyc_taxi_holiday_weather");</span><span id="54e4" class="kv if hh kr b fi lb kx l ky kz">spark.sql("DROP DATABASE IF EXISTS NYCTaxi"); <br/>spark.sql("CREATE DATABASE NYCTaxi"); <br/>spark.sql("USE NYCTaxi");</span><span id="9c25" class="kv if hh kr b fi lb kx l ky kz">final_df.write.saveAsTable("nyc_taxi_holiday_weather");<br/>val final_results = spark.sql("SELECT COUNT(*) FROM nyc_taxi_holiday_weather");<br/>final_results.show(5, truncate = false)</span></pre><ul class=""><li id="b6eb" class="jc jd hh je b jf kl jh km jj kn jl ko jn kp jp jq jr js jt bi translated">暂停管道以暂停专用池</li><li id="a2ae" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">使用HTTP活动</li><li id="40a5" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">使用方法:贴</li></ul><pre class="ka kb kc kd fd kq kr ks kt aw ku bi"><span id="a68b" class="kv if hh kr b fi kw kx l ky kz"><a class="ae la" href="https://management.azure.com/subscriptions/subscriptionid/resourceGroups/resourcegroup/providers/Microsoft.Synapse/workspaces/workdpspacename/sqlPools/dedicatedpoolname/pause?api-version=2019-06-01-preview" rel="noopener ugc nofollow" target="_blank">https://management.azure.com/subscriptions/subscriptionid/resourceGroups/resourcegroup/providers/Microsoft.Synapse/workspaces/workdpspacename/sqlPools/dedicatedpoolname/pause?api-version=2019-06-01-preview</a></span></pre></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><pre class="kq kr ks kt aw ku bi"><span id="ce7f" class="kv if hh kr b fi lk ll lm ln lo kx l ky kz">// Load nyc green taxi trip records from azure open dataset<br/>val blob_account_name = "azureopendatastorage"<br/><br/>val nyc_blob_container_name = "nyctlc"<br/>val nyc_blob_relative_path = "green"<br/>val nyc_blob_sas_token = ""<br/><br/>val nyc_wasbs_path = f"wasbs://$nyc_blob_container_name@$blob_account_name.blob.core.windows.net/$nyc_blob_relative_path"<br/>spark.conf.set(f"fs.azure.sas.$nyc_blob_container_name.$blob_account_name.blob.core.windows.net",nyc_blob_sas_token)<br/><br/>val nyc_tlc = spark.read.parquet(nyc_wasbs_path)</span><span id="b2f8" class="kv if hh kr b fi lb kx l ky kz">// Filter data by time range<br/>import java.sql.Timestamp<br/>import org.joda.time.DateTime<br/><br/>val end_date = new Timestamp(DateTime.parse("2018-06-06").getMillis)<br/>val start_date = new Timestamp(DateTime.parse("2018-05-01").getMillis)<br/><br/>val nyc_tlc_df = nyc_tlc.filter((nyc_tlc("lpepPickupDatetime") &gt;= start_date) &amp;&amp; (nyc_tlc("lpepPickupDatetime") &lt;= end_date)) <br/>nyc_tlc_df.show(5, truncate = false)</span><span id="779e" class="kv if hh kr b fi lb kx l ky kz">// Extract month, day of month, and day of week from pickup datetime and add a static column for the country code to join holiday data. <br/>import org.apache.spark.sql.functions._<br/><br/>val nyc_tlc_df_expand = (<br/>                        nyc_tlc_df.withColumn("datetime", to_date(col("lpepPickupDatetime")))<br/>                                  .withColumn("month_num",month(col("lpepPickupDatetime")))<br/>                                  .withColumn("day_of_month",dayofmonth(col("lpepPickupDatetime")))<br/>                                  .withColumn("day_of_week",dayofweek(col("lpepPickupDatetime")))<br/>                                  .withColumn("hour_of_day",hour(col("lpepPickupDatetime")))<br/>                                  .withColumn("country_code",lit("US"))<br/>                        )</span><span id="cd3c" class="kv if hh kr b fi lb kx l ky kz">// Remove unused columns from nyc green taxi data<br/>val nyc_tlc_df_clean = nyc_tlc_df_expand.drop(<br/>                    "lpepDropoffDatetime", "puLocationId", "doLocationId", "pickupLongitude", <br/>                     "pickupLatitude", "dropoffLongitude","dropoffLatitude" ,"rateCodeID", <br/>                     "storeAndFwdFlag","paymentType", "fareAmount", "extra", "mtaTax",<br/>                     "improvementSurcharge", "tollsAmount", "ehailFee", "tripType" )</span><span id="107b" class="kv if hh kr b fi lb kx l ky kz">// Display 5 rows<br/>nyc_tlc_df_clean.show(5, truncate = false)</span><span id="8979" class="kv if hh kr b fi lb kx l ky kz">// Load public holidays data from azure open dataset<br/>val hol_blob_container_name = "holidaydatacontainer"<br/>val hol_blob_relative_path = "Processed"<br/>val hol_blob_sas_token = ""<br/><br/>val hol_wasbs_path = f"wasbs://$hol_blob_container_name@$blob_account_name.blob.core.windows.net/$hol_blob_relative_path"<br/>spark.conf.set(f"fs.azure.sas.$hol_blob_container_name.$blob_account_name.blob.core.windows.net",hol_blob_sas_token)<br/><br/>val hol_raw = spark.read.parquet(hol_wasbs_path)<br/><br/>// Filter data by time range<br/>val hol_df = hol_raw.filter((hol_raw("date") &gt;= start_date) &amp;&amp; (hol_raw("date") &lt;= end_date))<br/><br/>// Display 5 rows<br/>// hol_df.show(5, truncate = false)</span><span id="b42d" class="kv if hh kr b fi lb kx l ky kz">val hol_df_clean = (<br/>                hol_df.withColumnRenamed("countryRegionCode","country_code")<br/>                .withColumn("datetime",to_date(col("date")))<br/>                )<br/><br/>hol_df_clean.show(5, truncate = false)</span><span id="d1d1" class="kv if hh kr b fi lb kx l ky kz">// enrich taxi data with holiday data<br/>val nyc_taxi_holiday_df = nyc_tlc_df_clean.join(hol_df_clean, Seq("datetime", "country_code") , "left")<br/><br/>nyc_taxi_holiday_df.show(5,truncate = false)</span><span id="ddc8" class="kv if hh kr b fi lb kx l ky kz">// Create a temp table and filter out non empty holiday rows<br/><br/>nyc_taxi_holiday_df.createOrReplaceTempView("nyc_taxi_holiday_df")<br/>val result = spark.sql("SELECT * from nyc_taxi_holiday_df WHERE holidayName is NOT NULL ")<br/>result.show(5, truncate = false)</span><span id="de4f" class="kv if hh kr b fi lb kx l ky kz">// Load weather data from azure open dataset<br/>val weather_blob_container_name = "isdweatherdatacontainer"<br/>val weather_blob_relative_path = "ISDWeather/"<br/>val weather_blob_sas_token = ""<br/><br/>val weather_wasbs_path = f"wasbs://$weather_blob_container_name@$blob_account_name.blob.core.windows.net/$weather_blob_relative_path"<br/>spark.conf.set(f"fs.azure.sas.$weather_blob_container_name.$blob_account_name.blob.core.windows.net",hol_blob_sas_token)<br/><br/>val isd = spark.read.parquet(weather_wasbs_path)<br/><br/>// Display 5 rows<br/>// isd.show(5, truncate = false)</span><span id="879f" class="kv if hh kr b fi lb kx l ky kz">// Filter data by time range<br/>val isd_df = isd.filter((isd("datetime") &gt;= start_date) &amp;&amp; (isd("datetime") &lt;= end_date))<br/><br/>// Display 5 rows<br/>isd_df.show(5, truncate = false)</span><span id="fee6" class="kv if hh kr b fi lb kx l ky kz">al weather_df = (<br/>                isd_df.filter(isd_df("latitude") &gt;= "40.53")<br/>                        .filter(isd_df("latitude") &lt;= "40.88")<br/>                        .filter(isd_df("longitude") &gt;= "-74.09")<br/>                        .filter(isd_df("longitude") &lt;= "-73.72")<br/>                        .filter(isd_df("temperature").isNotNull)<br/>                        .withColumnRenamed("datetime","datetime_full")<br/>                        )</span><span id="69ce" class="kv if hh kr b fi lb kx l ky kz">// Remove unused columns<br/>val weather_df_clean = weather_df.drop("usaf", "wban", "longitude", "latitude").withColumn("datetime", to_date(col("datetime_full")))<br/><br/>//weather_df_clean.show(5, truncate = false)</span><span id="48ac" class="kv if hh kr b fi lb kx l ky kz">val weather_df_grouped = (<br/>                        weather_df_clean.groupBy('datetime).<br/>                        agg(<br/>                            mean('snowDepth) as "avg_snowDepth",<br/>                            max('precipTime) as "max_precipTime",<br/>                            mean('temperature) as "avg_temperature",<br/>                            max('precipDepth) as "max_precipDepth"<br/>                            )<br/>                        )<br/><br/>weather_df_grouped.show(5, truncate = false)</span><span id="7623" class="kv if hh kr b fi lb kx l ky kz">// Enrich taxi data with weather<br/>val nyc_taxi_holiday_weather_df = nyc_taxi_holiday_df.join(weather_df_grouped, Seq("datetime") ,"left")<br/>nyc_taxi_holiday_weather_df.cache()</span><span id="dfa9" class="kv if hh kr b fi lb kx l ky kz">nyc_taxi_holiday_weather_df.show(5,truncate = false)</span><span id="4cd2" class="kv if hh kr b fi lb kx l ky kz">// Run the describe() function on the new dataframe to see summary statistics for each field.<br/>display(nyc_taxi_holiday_weather_df.describe())</span><span id="cd25" class="kv if hh kr b fi lb kx l ky kz">nyc_taxi_holiday_weather_df.count</span><span id="10af" class="kv if hh kr b fi lb kx l ky kz">// Remove invalid rows with less than 0 taxi fare or tip<br/>val final_df = (<br/>            nyc_taxi_holiday_weather_df.<br/>            filter(nyc_taxi_holiday_weather_df("tipAmount") &gt; 0).<br/>            filter(nyc_taxi_holiday_weather_df("totalAmount") &gt; 0)<br/>            )</span><span id="0636" class="kv if hh kr b fi lb kx l ky kz">spark.sql("DROP TABLE IF EXISTS NYCTaxi.nyc_taxi_holiday_weather");</span><span id="9d6f" class="kv if hh kr b fi lb kx l ky kz">spark.sql("DROP DATABASE IF EXISTS NYCTaxi"); <br/>spark.sql("CREATE DATABASE NYCTaxi"); <br/>spark.sql("USE NYCTaxi");</span><span id="b55c" class="kv if hh kr b fi lb kx l ky kz">final_df.write.saveAsTable("nyc_taxi_holiday_weather");<br/>val final_results = spark.sql("SELECT COUNT(*) FROM nyc_taxi_holiday_weather");<br/>final_results.show(5, truncate = false)</span><span id="cfdc" class="kv if hh kr b fi lb kx l ky kz">// Load nyc green taxi trip records from azure open dataset<br/>val blob_account_name = "azureopendatastorage"<br/><br/>val nyc_blob_container_name = "nyctlc"<br/>val nyc_blob_relative_path = "green"<br/>val nyc_blob_sas_token = ""<br/><br/>val nyc_wasbs_path = f"wasbs://$nyc_blob_container_name@$blob_account_name.blob.core.windows.net/$nyc_blob_relative_path"<br/>spark.conf.set(f"fs.azure.sas.$nyc_blob_container_name.$blob_account_name.blob.core.windows.net",nyc_blob_sas_token)<br/><br/>val nyc_tlc = spark.read.parquet(nyc_wasbs_path)</span><span id="83dc" class="kv if hh kr b fi lb kx l ky kz">// Filter data by time range<br/>import java.sql.Timestamp<br/>import org.joda.time.DateTime<br/><br/>val end_date = new Timestamp(DateTime.parse("2018-06-06").getMillis)<br/>val start_date = new Timestamp(DateTime.parse("2018-05-01").getMillis)<br/><br/>val nyc_tlc_df = nyc_tlc.filter((nyc_tlc("lpepPickupDatetime") &gt;= start_date) &amp;&amp; (nyc_tlc("lpepPickupDatetime") &lt;= end_date)) <br/>nyc_tlc_df.show(5, truncate = false)</span><span id="ce0f" class="kv if hh kr b fi lb kx l ky kz">// Extract month, day of month, and day of week from pickup datetime and add a static column for the country code to join holiday data. <br/>import org.apache.spark.sql.functions._<br/><br/>val nyc_tlc_df_expand = (<br/>                        nyc_tlc_df.withColumn("datetime", to_date(col("lpepPickupDatetime")))<br/>                                  .withColumn("month_num",month(col("lpepPickupDatetime")))<br/>                                  .withColumn("day_of_month",dayofmonth(col("lpepPickupDatetime")))<br/>                                  .withColumn("day_of_week",dayofweek(col("lpepPickupDatetime")))<br/>                                  .withColumn("hour_of_day",hour(col("lpepPickupDatetime")))<br/>                                  .withColumn("country_code",lit("US"))<br/>                        )</span><span id="7a78" class="kv if hh kr b fi lb kx l ky kz">// Remove unused columns from nyc green taxi data<br/>val nyc_tlc_df_clean = nyc_tlc_df_expand.drop(<br/>                    "lpepDropoffDatetime", "puLocationId", "doLocationId", "pickupLongitude", <br/>                     "pickupLatitude", "dropoffLongitude","dropoffLatitude" ,"rateCodeID", <br/>                     "storeAndFwdFlag","paymentType", "fareAmount", "extra", "mtaTax",<br/>                     "improvementSurcharge", "tollsAmount", "ehailFee", "tripType" )</span><span id="3f17" class="kv if hh kr b fi lb kx l ky kz">// Display 5 rows<br/>nyc_tlc_df_clean.show(5, truncate = false)</span><span id="0d09" class="kv if hh kr b fi lb kx l ky kz">// Load public holidays data from azure open dataset<br/>val hol_blob_container_name = "holidaydatacontainer"<br/>val hol_blob_relative_path = "Processed"<br/>val hol_blob_sas_token = ""<br/><br/>val hol_wasbs_path = f"wasbs://$hol_blob_container_name@$blob_account_name.blob.core.windows.net/$hol_blob_relative_path"<br/>spark.conf.set(f"fs.azure.sas.$hol_blob_container_name.$blob_account_name.blob.core.windows.net",hol_blob_sas_token)<br/><br/>val hol_raw = spark.read.parquet(hol_wasbs_path)<br/><br/>// Filter data by time range<br/>val hol_df = hol_raw.filter((hol_raw("date") &gt;= start_date) &amp;&amp; (hol_raw("date") &lt;= end_date))<br/><br/>// Display 5 rows<br/>// hol_df.show(5, truncate = false)</span><span id="0970" class="kv if hh kr b fi lb kx l ky kz">val hol_df_clean = (<br/>                hol_df.withColumnRenamed("countryRegionCode","country_code")<br/>                .withColumn("datetime",to_date(col("date")))<br/>                )<br/><br/>hol_df_clean.show(5, truncate = false)</span><span id="4119" class="kv if hh kr b fi lb kx l ky kz">// enrich taxi data with holiday data<br/>val nyc_taxi_holiday_df = nyc_tlc_df_clean.join(hol_df_clean, Seq("datetime", "country_code") , "left")<br/><br/>nyc_taxi_holiday_df.show(5,truncate = false)</span><span id="945c" class="kv if hh kr b fi lb kx l ky kz">// Create a temp table and filter out non empty holiday rows<br/><br/>nyc_taxi_holiday_df.createOrReplaceTempView("nyc_taxi_holiday_df")<br/>val result = spark.sql("SELECT * from nyc_taxi_holiday_df WHERE holidayName is NOT NULL ")<br/>result.show(5, truncate = false)</span><span id="27f9" class="kv if hh kr b fi lb kx l ky kz">// Load weather data from azure open dataset<br/>val weather_blob_container_name = "isdweatherdatacontainer"<br/>val weather_blob_relative_path = "ISDWeather/"<br/>val weather_blob_sas_token = ""<br/><br/>val weather_wasbs_path = f"wasbs://$weather_blob_container_name@$blob_account_name.blob.core.windows.net/$weather_blob_relative_path"<br/>spark.conf.set(f"fs.azure.sas.$weather_blob_container_name.$blob_account_name.blob.core.windows.net",hol_blob_sas_token)<br/><br/>val isd = spark.read.parquet(weather_wasbs_path)<br/><br/>// Display 5 rows<br/>// isd.show(5, truncate = false)</span><span id="941e" class="kv if hh kr b fi lb kx l ky kz">// Filter data by time range<br/>val isd_df = isd.filter((isd("datetime") &gt;= start_date) &amp;&amp; (isd("datetime") &lt;= end_date))<br/><br/>// Display 5 rows<br/>isd_df.show(5, truncate = false)</span><span id="9537" class="kv if hh kr b fi lb kx l ky kz">al weather_df = (<br/>                isd_df.filter(isd_df("latitude") &gt;= "40.53")<br/>                        .filter(isd_df("latitude") &lt;= "40.88")<br/>                        .filter(isd_df("longitude") &gt;= "-74.09")<br/>                        .filter(isd_df("longitude") &lt;= "-73.72")<br/>                        .filter(isd_df("temperature").isNotNull)<br/>                        .withColumnRenamed("datetime","datetime_full")<br/>                        )</span><span id="91f1" class="kv if hh kr b fi lb kx l ky kz">// Remove unused columns<br/>val weather_df_clean = weather_df.drop("usaf", "wban", "longitude", "latitude").withColumn("datetime", to_date(col("datetime_full")))<br/><br/>//weather_df_clean.show(5, truncate = false)</span><span id="3c3a" class="kv if hh kr b fi lb kx l ky kz">val weather_df_grouped = (<br/>                        weather_df_clean.groupBy('datetime).<br/>                        agg(<br/>                            mean('snowDepth) as "avg_snowDepth",<br/>                            max('precipTime) as "max_precipTime",<br/>                            mean('temperature) as "avg_temperature",<br/>                            max('precipDepth) as "max_precipDepth"<br/>                            )<br/>                        )<br/><br/>weather_df_grouped.show(5, truncate = false)</span><span id="f13e" class="kv if hh kr b fi lb kx l ky kz">// Enrich taxi data with weather<br/>val nyc_taxi_holiday_weather_df = nyc_taxi_holiday_df.join(weather_df_grouped, Seq("datetime") ,"left")<br/>nyc_taxi_holiday_weather_df.cache()</span><span id="3cdb" class="kv if hh kr b fi lb kx l ky kz">nyc_taxi_holiday_weather_df.show(5,truncate = false)</span><span id="0af9" class="kv if hh kr b fi lb kx l ky kz">// Run the describe() function on the new dataframe to see summary statistics for each field.<br/>display(nyc_taxi_holiday_weather_df.describe())</span><span id="a225" class="kv if hh kr b fi lb kx l ky kz">nyc_taxi_holiday_weather_df.count</span><span id="a3d4" class="kv if hh kr b fi lb kx l ky kz">// Remove invalid rows with less than 0 taxi fare or tip<br/>val final_df = (<br/>            nyc_taxi_holiday_weather_df.<br/>            filter(nyc_taxi_holiday_weather_df("tipAmount") &gt; 0).<br/>            filter(nyc_taxi_holiday_weather_df("totalAmount") &gt; 0)<br/>            )</span><span id="e5df" class="kv if hh kr b fi lb kx l ky kz">spark.sql("DROP TABLE IF EXISTS NYCTaxi.nyc_taxi_holiday_weather");</span><span id="de72" class="kv if hh kr b fi lb kx l ky kz">spark.sql("DROP DATABASE IF EXISTS NYCTaxi"); <br/>spark.sql("CREATE DATABASE NYCTaxi"); <br/>spark.sql("USE NYCTaxi");</span><span id="e9bb" class="kv if hh kr b fi lb kx l ky kz">final_df.write.saveAsTable("nyc_taxi_holiday_weather");<br/>val final_results = spark.sql("SELECT COUNT(*) FROM nyc_taxi_holiday_weather");<br/>final_results.show(5, truncate = false)</span></pre><ul class=""><li id="01eb" class="jc jd hh je b jf kl jh km jj kn jl ko jn kp jp jq jr js jt bi translated">天蓝色ML火花</li></ul><pre class="ka kb kc kd fd kq kr ks kt aw ku bi"><span id="764d" class="kv if hh kr b fi kw kx l ky kz">import matplotlib.pyplot as plt<br/>from datetime import datetime<br/>from dateutil import parser<br/>from pyspark.sql.functions import unix_timestamp, date_format, col, when<br/>from pyspark.ml import Pipeline<br/>from pyspark.ml import PipelineModel<br/>from pyspark.ml.feature import RFormula<br/>from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer<br/>from pyspark.ml.classification import LogisticRegression<br/>from pyspark.mllib.evaluation import BinaryClassificationMetrics<br/>from pyspark.ml.evaluation import BinaryClassificationEvaluator</span><span id="0f6a" class="kv if hh kr b fi lb kx l ky kz">from azureml.opendatasets import NycTlcYellow<br/><br/>end_date = parser.parse('2018-06-06')<br/>start_date = parser.parse('2018-05-01')<br/>nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)<br/>filtered_df = nyc_tlc.to_spark_dataframe()</span><span id="c22b" class="kv if hh kr b fi lb kx l ky kz"># To make development easier, faster and less expensive down sample for now<br/>sampled_taxi_df = filtered_df.sample(True, 0.001, seed=1234)</span><span id="ad96" class="kv if hh kr b fi lb kx l ky kz">#sampled_taxi_df.show(5)<br/>display(sampled_taxi_df)</span><span id="4645" class="kv if hh kr b fi lb kx l ky kz">sampled_taxi_df.createOrReplaceTempView("nytaxi")</span><span id="6551" class="kv if hh kr b fi lb kx l ky kz">taxi_df = sampled_taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'rateCodeId', 'passengerCount'\<br/>                                , 'tripDistance', 'tpepPickupDateTime', 'tpepDropoffDateTime'\<br/>                                , date_format('tpepPickupDateTime', 'hh').alias('pickupHour')\<br/>                                , date_format('tpepPickupDateTime', 'EEEE').alias('weekdayString')\<br/>                                , (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime'))).alias('tripTimeSecs')\<br/>                                , (when(col('tipAmount') &gt; 0, 1).otherwise(0)).alias('tipped')<br/>                                )\<br/>                        .filter((sampled_taxi_df.passengerCount &gt; 0) &amp; (sampled_taxi_df.passengerCount &lt; 8)\<br/>                                &amp; (sampled_taxi_df.tipAmount &gt;= 0) &amp; (sampled_taxi_df.tipAmount &lt;= 25)\<br/>                                &amp; (sampled_taxi_df.fareAmount &gt;= 1) &amp; (sampled_taxi_df.fareAmount &lt;= 250)\<br/>                                &amp; (sampled_taxi_df.tipAmount &lt; sampled_taxi_df.fareAmount)\<br/>                                &amp; (sampled_taxi_df.tripDistance &gt; 0) &amp; (sampled_taxi_df.tripDistance &lt;= 100)\<br/>                                &amp; (sampled_taxi_df.rateCodeId &lt;= 5)<br/>                                &amp; (sampled_taxi_df.paymentType.isin({"1", "2"}))<br/>                                )</span><span id="b167" class="kv if hh kr b fi lb kx l ky kz">taxi_featurised_df = taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'passengerCount'\<br/>                                                , 'tripDistance', 'weekdayString', 'pickupHour','tripTimeSecs','tipped'\<br/>                                                , when((taxi_df.pickupHour &lt;= 6) | (taxi_df.pickupHour &gt;= 20),"Night")\<br/>                                                .when((taxi_df.pickupHour &gt;= 7) &amp; (taxi_df.pickupHour &lt;= 10), "AMRush")\<br/>                                                .when((taxi_df.pickupHour &gt;= 11) &amp; (taxi_df.pickupHour &lt;= 15), "Afternoon")\<br/>                                                .when((taxi_df.pickupHour &gt;= 16) &amp; (taxi_df.pickupHour &lt;= 19), "PMRush")\<br/>                                                .otherwise(0).alias('trafficTimeBins')<br/>                                              )\<br/>                                       .filter((taxi_df.tripTimeSecs &gt;= 30) &amp; (taxi_df.tripTimeSecs &lt;= 7200))</span><span id="f300" class="kv if hh kr b fi lb kx l ky kz"># Since the sample uses an algorithm that only works with numeric features, convert them so they can be consumed<br/>sI1 = StringIndexer(inputCol="trafficTimeBins", outputCol="trafficTimeBinsIndex")<br/>en1 = OneHotEncoder(dropLast=False, inputCol="trafficTimeBinsIndex", outputCol="trafficTimeBinsVec")<br/>sI2 = StringIndexer(inputCol="weekdayString", outputCol="weekdayIndex")<br/>en2 = OneHotEncoder(dropLast=False, inputCol="weekdayIndex", outputCol="weekdayVec")<br/><br/># Create a new dataframe that has had the encodings applied<br/>encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(taxi_featurised_df).transform(taxi_featurised_df)</span><span id="39fb" class="kv if hh kr b fi lb kx l ky kz">#Decide on the split between training and testing data from the dataframe<br/>trainingFraction = 0.7<br/>testingFraction = (1-trainingFraction)<br/>seed = 1234<br/><br/># Split the dataframe into test and training dataframes<br/>train_data_df, test_data_df = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)</span><span id="f115" class="kv if hh kr b fi lb kx l ky kz">## Create a new LR object for the model<br/>logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'tipped')<br/><br/>## The formula for the model<br/>classFormula = RFormula(formula="tipped ~ pickupHour + weekdayVec + passengerCount + tripTimeSecs + tripDistance + fareAmount + paymentType+ trafficTimeBinsVec")<br/><br/>## Undertake training and create an LR model<br/>lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)<br/><br/>## Saving the model is optional but its another form of inter session cache<br/>datestamp = datetime.now().strftime('%m-%d-%Y-%s')<br/>fileName = "lrModel_" + datestamp<br/>logRegDirfilename = fileName<br/>lrModel.save(logRegDirfilename)<br/><br/>## Predict tip 1/0 (yes/no) on the test dataset, evaluation using AUROC<br/>predictions = lrModel.transform(test_data_df)<br/>predictionAndLabels = predictions.select("label","prediction").rdd<br/>metrics = BinaryClassificationMetrics(predictionAndLabels)<br/>print("Area under ROC = %s" % metrics.areaUnderROC)</span><span id="9df4" class="kv if hh kr b fi lb kx l ky kz">## Plot the ROC curve, no need for pandas as this uses the modelSummary object<br/>modelSummary = lrModel.stages[-1].summary<br/><br/>plt.plot([0, 1], [0, 1], 'r--')<br/>plt.plot(modelSummary.roc.select('FPR').collect(),<br/>         modelSummary.roc.select('TPR').collect())<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>plt.show()</span></pre><ul class=""><li id="2626" class="jc jd hh je b jf kl jh km jj kn jl ko jn kp jp jq jr js jt bi translated">火花MLLib建模</li></ul><pre class="ka kb kc kd fd kq kr ks kt aw ku bi"><span id="c639" class="kv if hh kr b fi kw kx l ky kz">import matplotlib.pyplot as plt<br/>from datetime import datetime<br/>from dateutil import parser<br/>from pyspark.sql.functions import unix_timestamp, date_format, col, when<br/>from pyspark.ml import Pipeline<br/>from pyspark.ml import PipelineModel<br/>from pyspark.ml.feature import RFormula<br/>from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer<br/>from pyspark.ml.classification import LogisticRegression<br/>from pyspark.mllib.evaluation import BinaryClassificationMetrics<br/>from pyspark.ml.evaluation import BinaryClassificationEvaluator</span><span id="4bd6" class="kv if hh kr b fi lb kx l ky kz">from azureml.opendatasets import NycTlcYellow<br/><br/>end_date = parser.parse('2018-06-06')<br/>start_date = parser.parse('2018-05-01')<br/>nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)<br/>filtered_df = nyc_tlc.to_spark_dataframe()</span><span id="a88b" class="kv if hh kr b fi lb kx l ky kz"># To make development easier, faster and less expensive down sample for now<br/>sampled_taxi_df = filtered_df.sample(True, 0.001, seed=1234)</span><span id="9d48" class="kv if hh kr b fi lb kx l ky kz">#sampled_taxi_df.show(5)<br/>display(sampled_taxi_df)</span><span id="598a" class="kv if hh kr b fi lb kx l ky kz">sampled_taxi_df.createOrReplaceTempView("nytaxi")</span><span id="8345" class="kv if hh kr b fi lb kx l ky kz">taxi_df = sampled_taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'rateCodeId', 'passengerCount'\<br/>                                , 'tripDistance', 'tpepPickupDateTime', 'tpepDropoffDateTime'\<br/>                                , date_format('tpepPickupDateTime', 'hh').alias('pickupHour')\<br/>                                , date_format('tpepPickupDateTime', 'EEEE').alias('weekdayString')\<br/>                                , (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime'))).alias('tripTimeSecs')\<br/>                                , (when(col('tipAmount') &gt; 0, 1).otherwise(0)).alias('tipped')<br/>                                )\<br/>                        .filter((sampled_taxi_df.passengerCount &gt; 0) &amp; (sampled_taxi_df.passengerCount &lt; 8)\<br/>                                &amp; (sampled_taxi_df.tipAmount &gt;= 0) &amp; (sampled_taxi_df.tipAmount &lt;= 25)\<br/>                                &amp; (sampled_taxi_df.fareAmount &gt;= 1) &amp; (sampled_taxi_df.fareAmount &lt;= 250)\<br/>                                &amp; (sampled_taxi_df.tipAmount &lt; sampled_taxi_df.fareAmount)\<br/>                                &amp; (sampled_taxi_df.tripDistance &gt; 0) &amp; (sampled_taxi_df.tripDistance &lt;= 100)\<br/>                                &amp; (sampled_taxi_df.rateCodeId &lt;= 5)<br/>                                &amp; (sampled_taxi_df.paymentType.isin({"1", "2"}))<br/>                                )</span><span id="3040" class="kv if hh kr b fi lb kx l ky kz">taxi_featurised_df = taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'passengerCount'\<br/>                                                , 'tripDistance', 'weekdayString', 'pickupHour','tripTimeSecs','tipped'\<br/>                                                , when((taxi_df.pickupHour &lt;= 6) | (taxi_df.pickupHour &gt;= 20),"Night")\<br/>                                                .when((taxi_df.pickupHour &gt;= 7) &amp; (taxi_df.pickupHour &lt;= 10), "AMRush")\<br/>                                                .when((taxi_df.pickupHour &gt;= 11) &amp; (taxi_df.pickupHour &lt;= 15), "Afternoon")\<br/>                                                .when((taxi_df.pickupHour &gt;= 16) &amp; (taxi_df.pickupHour &lt;= 19), "PMRush")\<br/>                                                .otherwise(0).alias('trafficTimeBins')<br/>                                              )\<br/>                                       .filter((taxi_df.tripTimeSecs &gt;= 30) &amp; (taxi_df.tripTimeSecs &lt;= 7200))</span><span id="5047" class="kv if hh kr b fi lb kx l ky kz"># Since the sample uses an algorithm that only works with numeric features, convert them so they can be consumed<br/>sI1 = StringIndexer(inputCol="trafficTimeBins", outputCol="trafficTimeBinsIndex")<br/>en1 = OneHotEncoder(dropLast=False, inputCol="trafficTimeBinsIndex", outputCol="trafficTimeBinsVec")<br/>sI2 = StringIndexer(inputCol="weekdayString", outputCol="weekdayIndex")<br/>en2 = OneHotEncoder(dropLast=False, inputCol="weekdayIndex", outputCol="weekdayVec")<br/><br/># Create a new dataframe that has had the encodings applied<br/>encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(taxi_featurised_df).transform(taxi_featurised_df)</span><span id="13de" class="kv if hh kr b fi lb kx l ky kz">#Decide on the split between training and testing data from the dataframe<br/>trainingFraction = 0.7<br/>testingFraction = (1-trainingFraction)<br/>seed = 1234<br/><br/># Split the dataframe into test and training dataframes<br/>train_data_df, test_data_df = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)</span><span id="a8ed" class="kv if hh kr b fi lb kx l ky kz">## Create a new LR object for the model<br/>logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'tipped')<br/><br/>## The formula for the model<br/>classFormula = RFormula(formula="tipped ~ pickupHour + weekdayVec + passengerCount + tripTimeSecs + tripDistance + fareAmount + paymentType+ trafficTimeBinsVec")<br/><br/>## Undertake training and create an LR model<br/>lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)<br/><br/>## Saving the model is optional but its another form of inter session cache<br/>datestamp = datetime.now().strftime('%m-%d-%Y-%s')<br/>fileName = "lrModel_" + datestamp<br/>logRegDirfilename = fileName<br/>lrModel.save(logRegDirfilename)<br/><br/>## Predict tip 1/0 (yes/no) on the test dataset, evaluation using AUROC<br/>predictions = lrModel.transform(test_data_df)<br/>predictionAndLabels = predictions.select("label","prediction").rdd<br/>metrics = BinaryClassificationMetrics(predictionAndLabels)<br/>print("Area under ROC = %s" % metrics.areaUnderROC)</span><span id="6b48" class="kv if hh kr b fi lb kx l ky kz">## Plot the ROC curve, no need for pandas as this uses the modelSummary object<br/>modelSummary = lrModel.stages[-1].summary<br/><br/>plt.plot([0, 1], [0, 1], 'r--')<br/>plt.plot(modelSummary.roc.select('FPR').collect(),<br/>         modelSummary.roc.select('TPR').collect())<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>plt.show()</span></pre></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><p id="9c14" class="pw-post-body-paragraph lp lq hh je b jf kl lr ls jh km lt lu jj lv lw lx jl ly lz ma jn mb mc md jp ha bi translated"><em class="me">最初发表于</em><a class="ae la" href="https://github.com/balakreshnan/Samples2021/blob/main/Synapseworkspace/e3esample.md" rel="noopener ugc nofollow" target="_blank"><em class="me">【https://github.com】</em></a><em class="me">。</em></p></div></div>    
</body>
</html>