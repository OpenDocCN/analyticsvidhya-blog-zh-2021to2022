<html>
<head>
<title>Transformer Implementation (Attention all you Need)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器实施(注意所有你需要的)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bert-implementation-multi-head-attention-4a10142636fe?source=collection_archive---------10-----------------------#2021-02-04">https://medium.com/analytics-vidhya/bert-implementation-multi-head-attention-4a10142636fe?source=collection_archive---------10-----------------------#2021-02-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c251" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个变压器的架构纯粹是基于自我关注，不管RNN序列模型。自我注意的目标是通过关联序列的不同位置来捕捉每个序列的表征。使用这个基础，BERT打破了早期文本分析和表示模型中固有的熟悉的从左到右的灌输。</p><p id="90cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我将解释本文中提到的转换器的编码部分(<a class="ae jd" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意，你所需要的一切</a>)，并且也尝试涵盖本文的一些重要理论方面。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/5a4ae330cf0820fa7fb0a38b1c7fba7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8puLmZD6l_p5AvYXoZkh6A.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">伯特建筑</figcaption></figure><p id="2d4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上是变形金刚的架构图。基本上，它包含多层编码器和解码器。我敢肯定，几乎我们所有人都多次见过这种架构。所以让我们把它分成多个部分，并试着理解它的每一部分。</p><p id="7e64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">模型建筑</strong></p><ul class=""><li id="a625" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">嵌入</li><li id="89f0" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">位置编码</li><li id="c401" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">编码器</li><li id="ca85" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">解码器</li><li id="ae2e" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">多头注意力</li><li id="703b" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">位置式前馈网络</li><li id="e59b" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">标准化图层</li><li id="7b37" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">Softmax</li></ul><p id="05bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">嵌入</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ki"><img src="../Images/9cba206b247361e8f063341538f611ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*ISje_Ewi40qy4c_tLgbtSg.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">变压器嵌入</figcaption></figure><p id="e309" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在transformers中，输入嵌入在发送进行进一步处理之前与位置编码连接在一起。</p><p id="ee48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">什么是位置编码？</p><p id="e582" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于变压器不使用任何顺序网络，因此不存在递归，所有输入令牌都一次性传递给模型。因此，在这种情况下，transformer需要一些关于序列顺序的信息。因此，为了获得关于输入标记排序的信息，作者使用了位置编码概念。</p><p id="efa2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">位置编码与嵌入具有相同的维数d_model，因此两者可以相加。</p><p id="681e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这项工作中，作者使用不同频率的正弦和余弦函数:</p><p id="0f04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PE(pos，2i)=sin(pos/10000**2i/dmodel) →偶数位置</p><p id="7e83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PE(pos，2i+1)= cos(pos/10000 * * 2i+1/d model)→对于奇数位置</p><pre class="jf jg jh ji fd kj kk kl km aw kn bi"><span id="5302" class="ko kp hi kk b fi kq kr l ks kt"><strong class="kk hj">class</strong> <strong class="kk hj">Embeddings</strong>(nn.Module):<br/>    <em class="ku">"""</em><br/><em class="ku">    Implements embeddings of the words and adds their positional encodings. </em><br/><em class="ku">    """</em><br/>    <strong class="kk hj">def</strong> __init__(self, vocab_size, d_model, max_len = 50):<br/>        super(Embeddings, self).__init__()<br/>        self.d_model = d_model<br/>        self.dropout = nn.Dropout(0.1)<br/>        self.embed = nn.Embedding(vocab_size, d_model)<br/>        self.pe = self.create_positinal_encoding(max_len, self.d_model)<br/>        self.dropout = nn.Dropout(0.1)<br/>        <br/>    <strong class="kk hj">def</strong> create_positinal_encoding(self, max_len, d_model):<br/>        pe = torch.zeros(max_len, d_model).to(device)<br/>        <strong class="kk hj">for</strong> pos <strong class="kk hj">in</strong> range(max_len):   <em class="ku"># for each position of the word</em><br/>            <strong class="kk hj">for</strong> i <strong class="kk hj">in</strong> range(0, d_model, 2):   <em class="ku"># for each dimension of the each position</em><br/>                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))<br/>                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))<br/>        pe = pe.unsqueeze(0)   <em class="ku"># include the batch size</em><br/>        <strong class="kk hj">return</strong> pe<br/>        <br/>    <strong class="kk hj">def</strong> forward(self, encoded_words):<br/>        <strong class="kk hj">embedding = self.embed(encoded_words) * math.sqrt(self.d_model)</strong><br/>        <strong class="kk hj">embedding += self.pe[:, :embedding.size(1)]   <em class="ku"># pe will automatically be expanded with the same batch size as encoded_words</em><br/>        </strong>embedding = self.dropout(embedding)<br/>        return embedding</span></pre><p id="45bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">reference→<a class="ae jd" href="https://github.com/fawazsammani/chatbot-transformer/blob/master/transformer%20chatbot.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/fawazsammani/chatbot-transformer/blob/master/transformer % 20 chatbot . ipynb</a></p><p id="5e19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里d_model是模型维度。在create_positional _encoding函数中，我们初始化了位置编码值。而在forward函数中，会在每次迭代后动态地将嵌入与位置编码连接起来。</p><p id="3e93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请参考下面的文章，以获得位置编码背后的深刻见解。</p><div class="kv kw ez fb kx ky"><a rel="noopener follow" target="_blank" href="/swlh/elegant-intuitions-behind-positional-encodings-dc48b4a4a5d1"><div class="kz ab dw"><div class="la ab lb cl cj lc"><h2 class="bd hj fi z dy ld ea eb le ed ef hh bi translated">位置编码背后优雅的直觉</h2><div class="lf l"><h3 class="bd b fi z dy ld ea eb le ed ef dx translated">我们如何捕捉位置信息？</h3></div><div class="lg l"><p class="bd b fp z dy ld ea eb le ed ef dx translated">medium.com</p></div></div><div class="lh l"><div class="li l lj lk ll lh lm jo ky"/></div></div></a></div><p id="2f31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">编码器和编码器层</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ln"><img src="../Images/c9fda18756a10b2d3034e625647b48bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*Jm9QptCZRXB04c1XEEsi8w.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">编码器变压器</figcaption></figure><p id="91a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上图所示，在发送到解码器部分之前，有3个不同的任务正在进行。这里Nx表示编号id编码层。</p><ol class=""><li id="8d1e" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc lo ka kb kc bi translated"><strong class="ih hj">多头关注</strong></li></ol><p id="b9f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意力功能可以描述为将查询和一组键-值对映射到输出，其中查询、键、值和输出都是向量。输出被计算为值的加权和，其中分配给每个值的权重由查询与相应键的兼容性函数来计算。我们称这种特殊的注意力为“成比例的点积注意力”。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lp"><img src="../Images/d24c5ad67df8e3d8c885bb57c89164ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ymf80QOrZ916vytOq8WipA.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><a class="ae jd" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">“标量点积注意”</a></figcaption></figure><p id="8a7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">而在变形金刚中的多注意力或者我们称之为自我注意力，输入令牌被分成多个块(默认为12个)。现在，自我关注独立地作用于所有这些分离的标记。</p><p id="7e8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是多头注意力层的实现。n_heads表示我们在处理时需要的头数。在我们的例子中，它是8。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lq"><img src="../Images/c66e71ef47908a80620334c40fb6c4a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l2WbXNC4vhxbFCKujEGBlg.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">多头注意力</figcaption></figure><p id="27de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2 <strong class="ih hj">位置前馈层</strong></p><p id="c624" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了注意子层之外，编码器和解码器中的每一层都包含一个完全连接的前馈网络，该网络被单独且相同地应用于每个位置。这由两个线性转换组成，中间有一个ReLU激活。</p><p id="9210" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然不同位置的线性变换是相同的，但是它们在层与层之间使用不同的参数。另一种描述方式是两个内核大小为1的卷积。为了从数据中获得更多的见解，首先，他们对较高数量神经元的数据进行卷积，然后回到线性层中相似数量的神经元</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lr"><img src="../Images/501ec047ac7253ce36f4db10aea78bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*6G3F5J94gdwdyxV6zQTGxQ.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">位置式前馈网络</figcaption></figure><p id="8985" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3 <strong class="ih hj">编码器堆栈层</strong></p><p id="52f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在变压器中，输入令牌通过多个编码器层，以获得自我关注层的最大好处。默认情况下，作者使用6个编码器和解码器层。</p><p id="55d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里值得注意的一点是，在将输入传递到下一层之前，我们也将原始输入标记连接到它。使得下一层将利用上一个编码器层的输出以及从原始源输入中获得洞察力。这类似于“重新发送跳过连接”。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ls"><img src="../Images/31c33db6f19ba962243cad8e2e370fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HQxnyHvsMcyID_09o0k5Bg.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">编码器层</figcaption></figure><p id="da2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们需要通过我们的源输入令牌从我们的编码器类调用所有这些功能。</p><p id="47ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ModuleList →在列表中保存子模块。</p><p id="64a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lt lu lv kk b"><a class="ae jd" href="https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList" rel="noopener ugc nofollow" target="_blank">ModuleList</a></code>可以像普通的Python列表一样被索引，但是它包含的模块被正确注册，并且对所有的<code class="du lt lu lv kk b"><a class="ae jd" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" rel="noopener ugc nofollow" target="_blank">Module</a></code>方法可见。</p><p id="719a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ModuleList类似于顺序列表，但略有不同。(<a class="ae jd" href="https://discuss.pytorch.org/t/when-should-i-use-nn-modulelist-and-when-should-i-use-nn-sequential/5463" rel="noopener ugc nofollow" target="_blank">https://discuse . py torch . org/t/when-should-I-use-nn-module list-and-when-should-I-use-nn-sequential/5463</a>)</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lw"><img src="../Images/185c32d3fea7ac32949ecb7b291f6704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UiuJd623CpJSWM7y8dH2Ng.png"/></div></div></figure><p id="6dd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解码器架构</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lx"><img src="../Images/2620b8aae1ccb57e154ee83601a512b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*NyJVewwI4d_IrRnJQRZbZw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">解码器架构</figcaption></figure><p id="d0b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解码器的功能与编码器几乎相同。有一些细微的变化，比如屏蔽输出。</p><p id="b28c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为什么要掩蔽？</p><p id="7e95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">需要屏蔽来防止解码器查看下一个令牌。以便模型仅在先前标记的帮助下预测下一个。为了执行三角遮罩，我们使用了torch API的"<strong class="ih hj"> torch.tril，torch.trio </strong>"</p><p id="c7b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与编码器层类似，解码器层也使用多头注意力和位置式前馈网络。</p><p id="6c7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解码层</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ly"><img src="../Images/bda5b5d825390afe47c862b41b4cef18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ZFUEDP6QpUHXvIfOpBCYg.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">解码器层</figcaption></figure><p id="5a76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解码器</strong></p><p id="cdeb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">出于可视化的目的，我们将注意力作为一种输出。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lz"><img src="../Images/4a8dd15fd8bc6d0053120a792cc20513.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ReyEP3t5qGChycWf98QeQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">解码器</figcaption></figure><p id="c9e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们需要从另一个类调用解码器和编码器。下面是我的Github链接的完整代码。</p><p id="de88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://github.com/deepak121993/END/blob/main/Transformer_from_Scratch.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Deepak 121993/END/blob/main/Transformer _ from _ scratch . ipynb</a></p><p id="0741" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一篇文章将讨论Bert的实现及其上下文嵌入对句子相似性的使用。</p><p id="89b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">特别感谢Rohan和团队关于深度学习NLP的精彩讲座。</p><p id="c627" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">##继续学习</p><p id="b912" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><div class="kv kw ez fb kx ky"><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener  ugc nofollow" target="_blank"><div class="kz ab dw"><div class="la ab lb cl cj lc"><h2 class="bd hj fi z dy ld ea eb le ed ef hh bi translated">带注释的变压器</h2><div class="lf l"><h3 class="bd b fi z dy ld ea eb le ed ef dx translated">“你所需要的只是关注”的转变在过去的一年里一直萦绕在许多人的脑海中。此外…</h3></div><div class="lg l"><p class="bd b fp z dy ld ea eb le ed ef dx translated">nlp.seas.harvard.edu</p></div></div><div class="lh l"><div class="ma l lj lk ll lh lm jo ky"/></div></div></a></div><p id="6c99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://github.com/fawazsammani/chatbot-transformer/" rel="noopener ugc nofollow" target="_blank">https://github.com/fawazsammani/chatbot-transformer/</a></p></div></div>    
</body>
</html>