<html>
<head>
<title>Parallel DynamoDB loading with Lambda</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Lambda并行DynamoDB加载</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/parallel-dynamodb-loading-with-lambda-7737deba673f?source=collection_archive---------12-----------------------#2021-01-17">https://medium.com/analytics-vidhya/parallel-dynamodb-loading-with-lambda-7737deba673f?source=collection_archive---------12-----------------------#2021-01-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/9b011a57559c2f5ef57bd5db24c2e441.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5BzZIrEL-9BT4Fw-DnrdxQ.png"/></div></div></figure><p id="2257" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我最近读了一篇非常有趣的博客文章(链接如下),其中谈到了一个将大量数据快速加载到DynamoDB的解决方案。受此启发，我想“我要建造它”，看看我们能做些什么。</p><div class="jn jo ez fb jp jq"><a href="https://towardsdatascience.com/dynamo-exports-may-get-your-data-out-but-this-is-still-the-fastest-way-to-move-data-in-5bcd9748cc00" rel="noopener follow" target="_blank"><div class="jr ab dw"><div class="js ab jt cl cj ju"><h2 class="bd hi fi z dy jv ea eb jw ed ef hg bi translated">在30分钟内将1亿多条记录导入DynamoDB！</h2><div class="jx l"><h3 class="bd b fi z dy jv ea eb jw ed ef dx translated">AWS上周发布了一个新功能，只需点击几下就可以导出一个完整的发电机表，但也值得了解如何…</h3></div><div class="jy l"><p class="bd b fp z dy jv ea eb jw ed ef dx translated">towardsdatascience.com</p></div></div><div class="jz l"><div class="ka l kb kc kd jz ke in jq"/></div></div></a></div><h1 id="1e8c" class="kf kg hh bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">设置</strong></h1><p id="7914" class="pw-post-body-paragraph ip iq hh ir b is ld iu iv iw le iy iz ja lf jc jd je lg jg jh ji lh jk jl jm ha bi translated">你可以在我的GitHub上找到所有的源代码(包括设置基础设施的Terraform)。</p><div class="jn jo ez fb jp jq"><a href="https://github.com/CloudySnake/parallel-dynamo-loads" rel="noopener  ugc nofollow" target="_blank"><div class="jr ab dw"><div class="js ab jt cl cj ju"><h2 class="bd hi fi z dy jv ea eb jw ed ef hg bi translated">云计算/并行发电机负载</h2><div class="jx l"><h3 class="bd b fi z dy jv ea eb jw ed ef dx translated">通过在GitHub上创建一个帐户，为CloudySnake/parallel-dynamo-loads开发做出贡献。</h3></div><div class="jy l"><p class="bd b fp z dy jv ea eb jw ed ef dx translated">github.com</p></div></div><div class="jz l"><div class="li l kb kc kd jz ke in jq"/></div></div></a></div><p id="90f5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在我们进行的过程中，我将复制一些代码片段，但对于整个代码库，请查看顶部的GitHub链接。</p><p id="9b25" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我的设置非常简单，我将创建一个简单的DynamoDB表，并创建一百万行测试数据，我希望将这些数据放入is中。在这个过程中，我将看一下将数据加载到Dynamo的几个选项，这样我们就可以感受一下它们的运行速度。我的测试数据假装我有一个想要加载的客户记录负载，它由customer_id (uuid)、客户获取日期、记录创建时间戳和最后更新时间戳组成。</p><p id="d787" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你会看到我正在使用PynamoDB与Dynamo对话，我是这个库的忠实粉丝，因为它使对Dynamo的读写比直接使用boto3容易得多。当您进行更复杂的更新或查询时尤其如此，尽管在本文中我们不会进行复杂的查询。</p><h1 id="38d6" class="kf kg hh bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">本地加载数据</h1><p id="22d1" class="pw-post-body-paragraph ip iq hh ir b is ld iu iv iw le iy iz ja lf jc jd je lg jg jh ji lh jk jl jm ha bi translated">我们首先创建少量的记录，然后使用Pynamo来写记录。有两种方法可以做到这一点，单记录写入和批量写入。</p><p id="6039" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">单记写道</strong></p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="1192" class="ls kg hh lo b fi lt lu l lv lw"><strong class="lo hi">def write_to_datastore</strong>(customer_id: <strong class="lo hi">str</strong>, acquisition_date: <strong class="lo hi">str</strong>) -&gt; <strong class="lo hi">None</strong>:<br/>    current_time = datetime.now().strftime("%Y-%m-%d %H:%M")<br/><br/>    customer_record = Datastore(<br/>        hash_key=f"CUST#<strong class="lo hi">{</strong>customer_id<strong class="lo hi">}</strong>",<br/>        range_key=<strong class="lo hi">None</strong>,<br/>        customer_acquisition_date=acquisition_date,<br/>        created_timestamp=current_time,<br/>        last_updated_timestamp=current_time,<br/>    )<br/>    customer_record.save()<br/><br/><br/><strong class="lo hi">@timeit<br/>def load_records</strong>():<br/>    <strong class="lo hi">"""<br/>    10,000 records - 256s (4.2m)<br/>    """<br/>    with open</strong>("src/testing_data/outputs/load_file.txt", "r") <strong class="lo hi">as </strong>f:<br/>        csv_reader = csv.reader(f, delimiter=",")<br/>        <strong class="lo hi">for </strong>row <strong class="lo hi">in </strong>csv_reader:<br/>            write_to_datastore(customer_id=row[<strong class="lo hi">0</strong>], acquisition_date=row[<strong class="lo hi">1</strong>])<br/><br/><br/><strong class="lo hi">if </strong>__name__ == "__main__":<br/>    load_records()</span></pre><p id="f976" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这段代码将遍历测试文件，一次一条记录，并将数据写入Dynamo。您将从我的docstring注释中看到，这样做大约需要。一万张唱片四分钟。在这种速度下，我们的一百万行可能需要7个多小时来加载。</p><p id="8e52" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">批记录写着</strong></p><p id="9987" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Dynamo提供批量写入功能，可以一次写入25条记录。Pynamo通过batch_write上下文管理器使这变得特别容易。</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="8534" class="ls kg hh lo b fi lt lu l lv lw"><strong class="lo hi">def create_record</strong>(customer_id: <strong class="lo hi">str</strong>, acquisition_date: <strong class="lo hi">str</strong>) -&gt; Datastore:<br/>    current_time = datetime.now().strftime("%Y-%m-%d %H:%M")<br/><br/>    customer_record = Datastore(<br/>        hash_key=f"CUST#<strong class="lo hi">{</strong>customer_id<strong class="lo hi">}</strong>",<br/>        range_key=<strong class="lo hi">None</strong>,<br/>        customer_acquisition_date=acquisition_date,<br/>        created_timestamp=current_time,<br/>        last_updated_timestamp=current_time,<br/>    )<br/>    <strong class="lo hi">return </strong>customer_record<br/><br/><br/><strong class="lo hi">@timeit<br/>def load_records</strong>():<br/>    <strong class="lo hi">"""<br/>    10,000 records - 13.61s<br/>    50,000 records - 65.39s<br/>    """<br/>    with open</strong>("src/testing_data/outputs/load_file.txt", "r") <strong class="lo hi">as </strong>f:<br/>        csv_reader = csv.reader(f, delimiter=",")<br/>        <strong class="lo hi">with </strong>Datastore.batch_write() <strong class="lo hi">as </strong>batch:<br/>            <strong class="lo hi">for </strong>row <strong class="lo hi">in </strong>csv_reader:<br/>                customer_record = create_record(row[<strong class="lo hi">0</strong>], row[<strong class="lo hi">1</strong>])<br/>                batch.save(customer_record)<br/><br/><br/><strong class="lo hi">if </strong>__name__ == "__main__":<br/>    load_records()</span></pre><p id="228c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在速度加快了，我们可以在13秒内载入一万张唱片。那还不错，但是一百万张唱片仍然需要大约二十分钟。我想做得更好。</p></div><div class="ab cl lx ly go lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ha hb hc hd he"><h1 id="3c36" class="kf kg hh bd kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky mi la lb lc bi translated">带有Lambda的并联负载</h1><p id="f904" class="pw-post-body-paragraph ip iq hh ir b is ld iu iv iw le iy iz ja lf jc jd je lg jg jh ji lh jk jl jm ha bi translated">理论很简单，如果我可以在大约一分钟内加载50，000条记录，那么Lambda也可以——如果我可以同时运行20条Lambda，我就可以在一分钟内加载一百万条记录。</p><p id="81e6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Lambda处理器</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="2669" class="ls kg hh lo b fi lt lu l lv lw"><strong class="lo hi">def handler</strong>(message, context):<br/>    s3 = boto3.resource('s3', region_name=os.environ["AWS_REGION"])<br/><br/>    body = json.loads(message["Records"][<strong class="lo hi">0</strong>]["body"])<br/>    bucket = body["s3BucketName"]<br/>    key = body["s3Key"]<br/><br/>    content = get_key_content(s3, bucket, key)<br/><br/>    <strong class="lo hi">try</strong>:<br/>        <strong class="lo hi">with </strong>Datastore.batch_write() <strong class="lo hi">as </strong>batch:<br/>            <strong class="lo hi">for </strong>record <strong class="lo hi">in </strong>content:<br/>                customer_record = create_record(<br/>                    record["customer_id"], record["acquisition_date"]<br/>                )<br/>                batch.save(customer_record)<br/>        remove_s3_data(s3, bucket, key)<br/>    <strong class="lo hi">except Exception as </strong>e:<br/>        <strong class="lo hi">print</strong>(e)<br/>        <strong class="lo hi">raise</strong></span></pre><p id="9a83" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有几点需要注意:</p><ul class=""><li id="da0b" class="mj mk hh ir b is it iw ix ja ml je mm ji mn jm mo mp mq mr bi translated">我最初用Cloudwatch事件尝试过，但是一个cloudwatch事件的大小被限制在256KB。这是一个很大的数据，但对我们的目的来说还不够。</li><li id="6c5a" class="mj mk hh ir b is ms iw mt ja mu je mv ji mw jm mo mp mq mr bi translated">相反，我把东西推到sqs队列中，并且我正在利用SQS扩展库(<a class="ae mx" href="https://github.com/timothymugayi/boto3-sqs-extended-client-lib" rel="noopener ugc nofollow" target="_blank">https://github . com/timothymugayi/boto 3-SQS-extended-client-lib</a>)，它通过把东西推到S3，在256KB的SQS限制附近工作。</li></ul><p id="6a09" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我肯定还有其他方法可以做到这一点(例如，我可以让一个Lambda读取整个文件，并向下游Lambda发出指令，指示要处理文件的哪些部分)。但这是一个合理的开端。</p><p id="072b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">约束</strong></p><p id="6dde" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我使用了一个新的DynamoDB表，可以按需计费。首次创建dynamo表时，其写入容量限制为每秒4，000次写入。如果我们尝试将表推得比这更快，我们会看到写错误。</p><p id="861f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">粗略地说，这不是一个问题，任何失败都将重试三次，如果不起作用，一旦可见性超时，消息将返回到SQS队列进行重新处理。就我们的目的而言，我没意见。值得指出的是，如果一个Pynamo批处理写入失败，那么我们将强制重新处理整个20k批处理，我确信我们可以更高效，并且只重新提交失败的批处理(25个记录)进行处理，这将加快重新处理的速度，但是因为我们的目标是不出现错误，所以我不太担心这一点。</p><p id="938a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因为我们使用的是按需分配表，AWS将在后台增加表的容量。这意味着，随着我们的表负载增加，我们可以执行的写入次数将会增加，错误也会越来越少。如果您在已经满负荷的现有表上执行此操作，那么您应该很少看到错误。</p><p id="0ec6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">加载尝试</strong></p><p id="5b0d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">每批运行1–20，000条记录</strong></p><p id="39a4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">运行1有许多发电机故障(正如所料)。当Lambda重试完成时，我能够加载大约75万条记录，其余的放回SQS队列。总的来说，这花了大约25分钟。我仍然认为一旦桌子变暖，这个速度会加快。这仍然是从本地机器发送数据的一个改进，因为SQS &amp;λ重试应该确保所有的数据都被处理，即使我们必须等待。这比我们的第一种方法更“一劳永逸”。</p><p id="2fdb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是运行1的发电机指标</p><figure class="lj lk ll lm fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es my"><img src="../Images/5555997234a63da0b015370a5a85c44a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dQdli-aKTD7NCg2HZIzvIw.png"/></div></div></figure><p id="2fd2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们看到最初的写入上限为4k，但是在第二次运行时(为了重试),我们看到每秒7k的新上限。</p><p id="21ea" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">每批运行2–20，000条记录</strong></p><p id="8396" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有了一个稍微热一点的表，我们再启动一百万行，看看会发生什么。</p><figure class="lj lk ll lm fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mz"><img src="../Images/bcf433bc92eac363a505b4d389d96127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2cG_idvMHRoH9tH_Opf9qA.png"/></div></div></figure><p id="c7ef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们在谈话。每秒13，122次写入的巨大峰值在大约两分钟内加载了整个一百万记录数据集(基于cloudwatch日志)。在这一点上，瓶颈不是dynamo db——而是我在本地遍历文件并将SQS消息发送到队列的速度。</p></div><div class="ab cl lx ly go lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ha hb hc hd he"><p id="1ee7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">成本</strong></p><p id="57e2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">DynamoDB的按需计费模式意味着你要为你消耗的资源付费，所以花一个小时加载一百万条记录或在一分钟内完成将花费DynamoDB成本的节省。当然，这里有λ和SQS的“成本”,但这是如此之小，我甚至不会考虑它。</p><p id="3926" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于我这个非常简单的例子，加载一个uid和一些日期意味着我正在编写一个150字节的对象，每行消耗1个WCU。在爱尔兰，一百万个写请求单元的成本是1.4135美元，或者换句话说“非常便宜”。</p><p id="7419" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">结论</strong></p><p id="8655" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">通过使用SQS和Lambda并行地将数据加载到DynamoDB中，您可以显著地加快速度，一旦您的表达到最大容量，您就可以在几分钟内加载数百万行数据。</p></div></div>    
</body>
</html>