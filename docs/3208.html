<html>
<head>
<title>Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-9fd219098405?source=collection_archive---------11-----------------------#2021-06-18">https://medium.com/analytics-vidhya/linear-regression-9fd219098405?source=collection_archive---------11-----------------------#2021-06-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/97fe790c5f6b5b91b0d6a6da1c59051f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HV3Jco2p0StN2WDSq_-UMA.jpeg"/></div></div></figure><p id="25b9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">目录</strong></p><ol class=""><li id="e2ef" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">线性回归简介</li><li id="055c" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">普通最小二乘法</li><li id="e71c" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">梯度下降</li><li id="dd51" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">线性回归的假设</li><li id="34d1" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">无需sklearn的多元线性回归的Python实现</li><li id="d0fb" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">使用R平方值评估模型</li></ol><h2 id="ece4" class="kb kc hh bd kd ke kf kg kh ki kj kk kl ja km kn ko je kp kq kr ji ks kt ku kv bi translated">什么是线性回归</h2><p id="392a" class="pw-post-body-paragraph ip iq hh ir b is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm ha bi translated">线性回归是最简单和最流行的机器学习算法之一。这是一种用于预测建模的统计方法。</p><p id="ab83" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是一个<strong class="ir hi">线性模型</strong>，即假设因变量(目标)和自变量之间呈线性关系的模型。当输入只包含一个变量时，称为<strong class="ir hi">简单线性回归</strong>，当使用多个输入变量来预测因变量的值时，称为<strong class="ir hi">多元线性回归</strong>。</p><p id="f272" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">回归线</strong>是一条显示因变量和自变量之间关系的线。线性回归旨在找到<strong class="ir hi">最佳拟合线</strong>，即通过观察值散点图的线，该线最好地表达了这些变量之间的关系。简单回归线方程由下式给出:</p><p id="a100" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">y = B₀ + B₁*x</p><p id="4f31" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在哪里，</p><p id="8160" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">y =因变量</p><p id="5384" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">x =独立变量</p><p id="5371" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">B₁ =比例因子/输入变量系数</p><p id="e3e7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">B₀ =截距/偏差系数</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lb"><img src="../Images/a6f43de73de82bce6fb834718af62ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*FXZBifq2SsUF2xs_63kAaw.gif"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">最佳线性回归线</figcaption></figure><h1 id="f476" class="lk kc hh bd kd ll lm ln kh lo lp lq kl lr ls lt ko lu lv lw kr lx ly lz ku ma bi translated"><strong class="ak">普通最小二乘法</strong></h1><p id="48a7" class="pw-post-body-paragraph ip iq hh ir b is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm ha bi translated">不同的系数值给出不同的回归线。为了找到最佳拟合线，最常用的方法是<strong class="ir hi">普通最小二乘法(OLS) </strong>。OLS试图最小化残差平方和，以获得最佳系数值集。<strong class="ir hi">残差</strong>定义为实际值和预测值之间的差值。成本函数由下式给出:</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mb"><img src="../Images/175a9d69d0cb59e12757fae76cb19f6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*u7Q7K5myXrExb59dfFFM6g.png"/></div></figure><p id="e4fb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，</p><p id="85e5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">y =实际值</p><p id="ec4c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">ŷ =预测值</p><p id="660c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一旦系数的最佳值已知，我们可以将这些值与自变量一起放入线性回归方程中，并找出因变量的值。</p><p id="e85b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">线性回归模型的复杂度</strong>以模型中使用的系数个数的形式计算。当系数变为0时，该变量对预测没有影响，并且模型的复杂性降低。例如:</p><p id="006b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">y = C + M₁X₁ + M₂X₂ + M₃X₃</p><p id="b724" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果M₂ = 0，y = C + M₁X₁ + M₃X₃</p><h1 id="80ed" class="lk kc hh bd kd ll lm ln kh lo lp lq kl lr ls lt ko lu lv lw kr lx ly lz ku ma bi translated">梯度下降</h1><p id="cb29" class="pw-post-body-paragraph ip iq hh ir b is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm ha bi translated">梯度下降是最小化成本函数的优化算法。首先，我们将系数和截距初始化为一些随机值，然后梯度下降通过迭代更新它们的值来找到对应于成本函数的全局或局部最小值的系数值集。</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mc"><img src="../Images/d4c865affdf7467c9554826befd0a5af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*yDX6YXpltV2QdZuSweHVrA.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">价值函数</figcaption></figure><p id="f664" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们绘制了成本函数(J)对因变量(y)的曲线，并获得了下图:</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es md"><img src="../Images/4f761cb3a41af2b3489bd645cc4c1b62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*c_oiuv5FMo2YQMcfhp2DAA.png"/></div></figure><p id="aa55" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">梯度下降旨在达到成本函数的全局最小值。移动的方向由偏导数的斜率给出，如下式所示。</p><figure class="lc ld le lf fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es me"><img src="../Images/711dff2230340e06f81037f3dc090275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kY8lGMUqlsU8LDq00oQEpg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">使用此公式计算系数和截距的新值</figcaption></figure><p id="e90e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">求解上述方程，我们得到用于更新系数和截距的最终方程:</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/68cdc7f245aa67454a0f4c4268ad7beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*nydbf0fKMwSOvf3ftTk56g.png"/></div></figure><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/e7390f1e8af4a4267bb8d5fabf1f4bc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*W6TIMaBQ5DJHDtgS6eY7Vg.png"/></div></figure><p id="b75c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于每次迭代，误差不断减小，所以我们将继续迭代，直到误差不可能再减小。</p><p id="b790" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这里，alpha被称为<strong class="ir hi">学习速率</strong>，它控制着我们向最小值前进的幅度。如果迈出的步伐太大，我们可能会跳过最小值。但是，如果步长太小，则需要太长时间才能达到最小值。</p><h1 id="238f" class="lk kc hh bd kd ll lm ln kh lo lp lq kl lr ls lt ko lu lv lw kr lx ly lz ku ma bi translated">线性回归的假设</h1><p id="2273" class="pw-post-body-paragraph ip iq hh ir b is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm ha bi translated">为了得到最好的结果，线性回归做了一些假设。</p><ol class=""><li id="b333" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">它假设因变量和自变量之间呈线性关系。</li><li id="772d" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">多重共线性(独立要素之间的相关性)很小或没有。由于多重共线性，很难区分哪个要素影响了目标变量，哪个没有。此外，多重共线性会导致过度拟合。</li><li id="e24b" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">从属和独立特征中没有噪声。</li><li id="1faf" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">数据点形成高斯分布。</li><li id="39c8" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">输入变量必须缩放。</li></ol><p id="d258" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了获得更好的预测，请确保满足上述假设。这可以通过变换数据以使其具有线性关系、移除异常值、使用标准化和规范化来缩放数据、移除相关特征以及使用对数变换或BoxCox变换将分布转换为正态分布来完成。</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mh"><img src="../Images/b59f08ceb1ed7e0c9e8af988297985bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*7VUKHiY_oOPbyaIH-olkEQ.jpeg"/></div></figure><p id="cb65" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们用Python实现线性回归</p><h2 id="aa33" class="kb kc hh bd kd ke kf kg kh ki kj kk kl ja km kn ko je kp kq kr ji ks kt ku kv bi translated">无sklearn的多元线性回归</h2><p id="48ee" class="pw-post-body-paragraph ip iq hh ir b is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm ha bi translated">要遵循给定的Python实现，您需要安装以下库:</p><ol class=""><li id="e6f2" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">numpy</li><li id="a139" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">熊猫</li><li id="bdc1" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">matplotlib</li><li id="14da" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">海生的</li></ol><p id="5744" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Python的实现和数据集也可以从<a class="ae mi" href="https://github.com/tanvipenumudy/ML-Stream-Interns-Summer-21/tree/main/Khushi%20Jain" rel="noopener ugc nofollow" target="_blank">的<strong class="ir hi">这里的</strong>进入</a>。</p><pre class="lc ld le lf fd mj mk ml mm aw mn bi"><span id="8883" class="kb kc hh mk b fi mo mp l mq mr">import numpy as np</span><span id="6098" class="kb kc hh mk b fi ms mp l mq mr">import pandas as pd</span><span id="0c65" class="kb kc hh mk b fi ms mp l mq mr">import matplotlib.pyplot as plt</span><span id="66b8" class="kb kc hh mk b fi ms mp l mq mr">import seaborn as sns</span><span id="662a" class="kb kc hh mk b fi ms mp l mq mr">data = pd.read_csv('dataset.txt', names=['size', 'bedroom', 'price'])</span><span id="9ec9" class="kb kc hh mk b fi ms mp l mq mr">data.head()</span></pre><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/60cf63457f62f9e2d38f9eb614114111.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*W2NKpmRWtd-9hBwUlEi_pw.png"/></div></figure><p id="274f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">两个独立特征的尺度不同，因此尺寸特征将主导卧室特征。为了防止这种情况，我们应用了标准化。</p><pre class="lc ld le lf fd mj mk ml mm aw mn bi"><span id="d0db" class="kb kc hh mk b fi mo mp l mq mr">for column in data.columns:</span><span id="a305" class="kb kc hh mk b fi ms mp l mq mr">    data[column] = (data[column]-data[column].mean()) / data[column].std()<br/>data.head()</span></pre><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mu"><img src="../Images/1d91bd845ce59f26cd272855b2db2529.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*WHX8dtiGXOM5f234f-WLWg.png"/></div></figure><pre class="lc ld le lf fd mj mk ml mm aw mn bi"><span id="f8c8" class="kb kc hh mk b fi mo mp l mq mr">X = data.iloc[:, :2].values<br/>Y = data.iloc[:, 2:].values<br/>x0_ones = np.ones((data.shape[0], 1))<br/>X = np.concatenate( (X, x0_ones), axis=1 )<br/>theta = np.zeros((1, 3))<br/>alpha = 0.01<br/>iterations = 1000<br/>n = X.shape[0]</span><span id="ef99" class="kb kc hh mk b fi ms mp l mq mr"># Cost function<br/>def compute_cost(X, Y, theta):<br/>residual_sq = (( X @ theta.T ) - Y )**2<br/>sum_residual_sq = np.sum(residual_sq)<br/>return (sum_residual_sq/(2*n))</span><span id="3f74" class="kb kc hh mk b fi ms mp l mq mr"># Gradient Descent<br/>def gradient_descent(X, Y, theta, iterations, alpha):<br/>  cost = np.zeros(iterations)<br/>  for i in range(iterations):<br/>    theta = theta - (alpha/n) * np.sum( (X* ((X @ theta.T) - Y) ), axis = 0)<br/>    cost[i] = compute_cost(X, Y, theta)<br/>  return (theta, cost)</span><span id="77a4" class="kb kc hh mk b fi ms mp l mq mr">coeff, cost = gradient_descent(X, Y, theta, iterations, alpha)</span><span id="73bb" class="kb kc hh mk b fi ms mp l mq mr">print(coeff)<br/>print(cost[999])</span></pre><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mv"><img src="../Images/8af616649dadd2ba2826f6f38b7b978b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*zvVfd5oh3pqpt3yzlMiCKA.png"/></div></figure><p id="a888" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">预测目标变量值:</strong></p><pre class="lc ld le lf fd mj mk ml mm aw mn bi"><span id="e584" class="kb kc hh mk b fi mo mp l mq mr">y_pred = np.sum(coeff * X, axis=1)</span></pre><h1 id="369b" class="lk kc hh bd kd ll lm ln kh lo lp lq kl lr ls lt ko lu lv lw kr lx ly lz ku ma bi translated">评估模型</h1><p id="acf8" class="pw-post-body-paragraph ip iq hh ir b is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm ha bi translated"><strong class="ir hi"> R平方方法</strong></p><p id="2f8e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">r平方方法确定拟合优度。它衡量回归线与观察值的吻合程度。它说明因变量中有多少%的变化是由独立特征解释的。</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mw"><img src="../Images/0c266c7070454d62955efc37755ac4a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*DekNesLdTFJ3Ah5KiQa4XA.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">Img src-黑客地球</figcaption></figure><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lb"><img src="../Images/47519c952e5d6c715ac6fbd463ac0112.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*XpArHnOxyN-P6eYqG7q3vg.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">决定系数/ R平方</figcaption></figure><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mx"><img src="../Images/4ae63bb4ad5068b9ff497544008c8ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*0yMtsNvFLQ0JPkEEKewDHw.png"/></div></figure><p id="7a92" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">R平方值越高，模型越好</strong>，因为这意味着更多的变化已经由独立特征解释。</p><pre class="lc ld le lf fd mj mk ml mm aw mn bi"><span id="f8d9" class="kb kc hh mk b fi mo mp l mq mr">nume = 0<br/>deno = 0<br/>for i in range(n):<br/>  nume += (y_pred[i]-Y.mean())**2<br/>  deno += (Y[i]-Y.mean())**2<br/>r2 = nume/deno<br/>r2</span></pre><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es my"><img src="../Images/b6b11e0fa738b7d2b48140a5b07d1ce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*3rDl0mGG78F6Vehbyn4sZw.png"/></div></figure><p id="3002" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这意味着72.78%的目标变量的变化是由于大小和卧室。</p><p id="eb63" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是<strong class="ir hi">高R平方值并不总是好的</strong>。例如:让我们考虑一个以摄氏度为单位预测温度的情况，其中一个输入特征是以华氏度为单位的温度。这肯定会给出高R平方值，但这种模型是无用的。</p><p id="4f35" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">小的R平方值并不总是坏的</strong>。试图预测人类行为的研究将具有较低的R平方值，因为人类行为是高度不可预测的。</p><p id="6b14" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">R平方的缺点</strong></p><p id="abee" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">每次向模型中添加独立变量时，r平方都会增加。它永远不会减少，即使只是变量之间的偶然相关(多重共线性)。因此，我们添加的点越多，回归似乎就越符合我们的数据。我们添加的一些点可能无关紧要(不符合模型)。r平方不关心这样的点。我们加得越多，决定系数要么保持不变，要么变得更高。而是使用调整后的R平方值。</p><p id="3fba" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">调整后的R平方</strong></p><p id="62fc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">调整后的R平方考虑了模型中使用的独立要素的数量。</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mz"><img src="../Images/deb1228cc10a82b0ed35e6eaf7f8e771.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*G_raIZAOhYXp3gooMBcksg.png"/></div></figure><p id="018f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> n </strong> =观察次数</p><p id="bbb3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> k </strong> =独立特征的数量</p><p id="2a39" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> R </strong> =由模型确定的R平方值。</p><p id="6158" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，如果R平方在添加新要素时没有显著增加，则调整后的R平方的值将会减少。</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es na"><img src="../Images/5184ff85ebb95f4745d47816d136c2e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*po1NIUwfoT2netqj71dl-A.png"/></div></figure><p id="6e55" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然而，如果在添加新特征时，R平方值显著增加，则调整后的R平方值也将增加。</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es nb"><img src="../Images/dbd4a9804f4282be770c76284507b387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*nvPNSH2JKWMQcpRwrugWsA.png"/></div></figure><p id="ca8f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这样，我们完成了第一个机器学习算法。希望你觉得有用。</p><p id="6eec" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你喜欢我的博客，那么看看我以前的一些博客:</p><ol class=""><li id="1a78" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><a class="ae mi" href="https://khushijain2810.medium.com/introduction-to-opencv-586e38d536fd" rel="noopener"> OpenCV </a></li><li id="4205" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><a class="ae mi" href="https://khushijain2810.medium.com/seaborn-data-visualization-library-142ac64d5560" rel="noopener"> Seaborn </a></li><li id="0a94" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><a class="ae mi" href="https://khushijain2810.medium.com/pandas-python-data-analysis-library-1d061c982fc8" rel="noopener">熊猫</a></li><li id="e0ea" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">Numpy </li></ol><p id="d32b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">快乐学习！</p></div></div>    
</body>
</html>