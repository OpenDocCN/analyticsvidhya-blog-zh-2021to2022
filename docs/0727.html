<html>
<head>
<title>Random Forest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/random-forest-4a2981aab4f7?source=collection_archive---------11-----------------------#2021-01-28">https://medium.com/analytics-vidhya/random-forest-4a2981aab4f7?source=collection_archive---------11-----------------------#2021-01-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/295b6d5f5966479d9e441c6ea2e9299e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_Lih0DIVBeReDbWy.png"/></div></div></figure><p id="f073" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">随机森林</strong>是一种流行的机器学习算法，属于监督学习技术。它可用于ML中的分类和回归问题。它基于集成学习的概念，集成学习是一个组合多个分类器来解决复杂问题并提高模型性能的过程。</p><p id="bde3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">顾名思义，“<strong class="is hj"> <em class="jo">随机森林是一种分类器，它包含了给定数据集的各个子集上的若干决策树，并取平均值以提高该数据集的预测精度。</em> </strong>“随机森林不是依赖于一棵决策树，而是从每棵树中提取预测，并基于预测的多数票，它预测最终的输出。</p><p id="a158" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">森林中的树的数量越多，精度越高，并且防止了过度拟合的问题。</p><p id="d5ba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下图解释了随机森林算法的工作原理:</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es jp"><img src="../Images/d96740d589e481de2c8c1772a7615e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*-BEY1zk7aJtSyAgS.png"/></div></figure><p id="5a13" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">例子</strong></p><p id="bd71" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设有一个包含多个水果图像的数据集。因此，这个数据集被提供给随机森林分类器。数据集被分成子集，并提供给每个决策树。在训练阶段，每个决策树产生一个预测结果，当一个新的数据点出现时，随机森林分类器根据大多数结果预测最终的决策。考虑下图:</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es jp"><img src="../Images/0dd8a07b5ba43cb5963c582e94aa8c7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*IKdUuVG563BFeCGT.png"/></div></figure><p id="19bc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">随机森林的假设</strong></p><p id="8f3c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于随机森林组合了多个树来预测数据集的类别，因此一些决策树可能会预测正确的输出，而另一些则可能不会。但是所有的树一起预测正确的输出。因此，下面是对更好的随机森林分类器的两个假设:</p><ul class=""><li id="d5cb" class="ju jv hi is b it iu ix iy jb jw jf jx jj jy jn jz ka kb kc bi translated">数据集的特征变量中应该有一些实际值，以便分类器可以预测准确的结果，而不是猜测的结果。</li><li id="c0cf" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">来自每棵树的预测必须具有非常低的相关性。</li></ul><p id="dc5f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Python实现</strong></p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="0834" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">from</strong> sklearn.ensemble <strong class="kj hj">import</strong> RandomForestClassifier</span><span id="5944" class="kn ko hi kj b fi kt kq l kr ks">rndmFrst = RandomForestClassifier(criterion= 'entropy',<br/> <strong class="kj hj">max_depth</strong> = 12,<br/> <strong class="kj hj">max_features</strong> = 'log2',<br/> <strong class="kj hj">min_samples_leaf</strong> = 1,<br/> <strong class="kj hj">min_samples_split</strong>= 5,<br/> <strong class="kj hj">n_estimators</strong> = 90,random_state=6)</span></pre><p id="767d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">随机森林分类器的超参数</strong></p><blockquote class="ku kv kw"><p id="26a6" class="iq ir jo is b it iu iv iw ix iy iz ja kx jc jd je ky jg jh ji kz jk jl jm jn hb bi translated">RandomForestClassifier(n _ estimators = 100，*，criterion='gini '，max_depth=None，min_samples_split=2，min_samples_leaf=1，min_weight_fraction_leaf=0.0，max_features='auto '，max_leaf_nodes=None，min _ infinity _ decrease = 0.0，min _ infinity _ split = None，bootstrap=True，oob_score=False，n_jobs=None，random_state=None，verbose=0，warm_start=False，class_weight=None</p></blockquote><ul class=""><li id="4a0a" class="ju jv hi is b it iu ix iy jb jw jf jx jj jy jn jz ka kb kc bi translated"><strong class="is hj"> n_estimators: int，default=100 </strong> —森林中树木的数量。</li><li id="7e70" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated"><strong class="is hj">标准:{"gini "，" entropy"}，default="gini" </strong> —支持的标准是基尼杂质的" gini "和信息增益的" entropy "。</li><li id="24b3" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated"><strong class="is hj"> max_depth : int，default=None — </strong>树的最大深度。</li><li id="8448" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated"><strong class="is hj"> min_samples_split : int或float，default=2 — </strong>拆分内部节点所需的最小样本数</li><li id="8449" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated"><strong class="is hj"> min_samples_leaf : int或float，default=1 — </strong>叶节点所需的最小样本数。</li><li id="151e" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated"><strong class="is hj">min _ weight _ fraction _ leaf:float，default=0.0 — </strong>(所有输入样本的)权重总和在叶节点上所需的最小加权分数。</li><li id="dd62" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated"><strong class="is hj"> max_leaf_nodes : int，default=None — </strong>用<code class="du la lb lc kj b">max_leaf_nodes</code>以最佳优先方式生长树。最佳节点被定义为杂质的相对减少。如果没有，则无限数量的叶节点。</li><li id="416e" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated"><strong class="is hj">min _ infinity _ decrease:float，default=0.0 — </strong>如果分割导致杂质减少大于或等于该值，则将分割节点。</li><li id="3874" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated"><strong class="is hj">min _ infinity _ split:float，default=None — </strong>提前停止树生长的阈值。如果一个节点的杂质高于阈值，它就会分裂，否则它就是一片叶子。</li><li id="44b0" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated"><strong class="is hj"> bootstrap : bool，default=True — </strong>构建树时是否使用bootstrap样本。如果为False，则使用整个数据集来构建每棵树。</li><li id="1fd2" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated"><strong class="is hj"> n_jobs : int，default=None — </strong>并行运行的作业数。<code class="du la lb lc kj b">-1</code>表示使用所有处理器</li><li id="8bed" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated"><strong class="is hj"> random_state: int，RandomState instance or None，default=None — </strong>控制构建树时使用的样本自举的随机性(如果<code class="du la lb lc kj b">bootstrap=True</code>)和在每个节点寻找最佳分割时考虑的特征采样(如果<code class="du la lb lc kj b">max_features &lt; n_features</code>)</li><li id="0ad8" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated"><strong class="is hj"> max_samples : int或float，default=None — </strong>如果bootstrap为真，则从X中抽取的用于训练每个基本估计量的样本数。</li></ul><p id="6500" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">尝试使用wine数据集实现</p><p id="b7d0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">导入库</strong></p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="11a3" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">import</strong> pandas as pd<br/>from sklearn.tree <strong class="kj hj">import</strong> DecisionTreeClassifier, export_graphviz<br/>from sklearn.ensemble <strong class="kj hj">import</strong> RandomForestClassifier<br/>from sklearn <strong class="kj hj">import</strong> tree<br/>from sklearn.model_selection <strong class="kj hj">import</strong> train_test_split,GridSearchCV<br/>from sklearn.preprocessing <strong class="kj hj">import</strong> StandardScaler<br/>from sklearn.metrics <strong class="kj hj">import</strong> accuracy_score, confusion_matrix, roc_curve, roc_auc_score<br/>from sklearn.externals.six <strong class="kj hj">import</strong> StringIO  <br/>from IPython.display <strong class="kj hj">import</strong> Image  <br/>from sklearn.tree <strong class="kj hj">import</strong> export_graphviz<br/><strong class="kj hj">import</strong> pydotplus</span></pre><p id="f746" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">读取数据</strong></p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="96a1" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">data</strong> = pd.read_csv("winequality_red.csv")<br/><strong class="kj hj">data</strong></span></pre><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/00bf588984eef2397f865190825462a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TGdFdh7-WmMDqspn.png"/></div></div></figure><p id="12aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">葡萄酒数据集</p><p id="39d0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">指定因变量&amp;自变量</strong></p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="8e8d" class="kn ko hi kj b fi kp kq l kr ks">X = data.drop(columns = 'quality')<br/>y = data['quality']</span></pre><p id="f0fd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">将数据分割成训练并测试</strong></p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="940c" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">x_train</strong>,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.30, random_state= 355)</span></pre><p id="f041" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">初始化随机森林分类器模型</strong></p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="7beb" class="kn ko hi kj b fi kp kq l kr ks">rand_clf = RandomForestClassifier(random_state=6)</span><span id="e07b" class="kn ko hi kj b fi kt kq l kr ks">rand_clf.fit(x_train,y_train)</span><span id="af76" class="kn ko hi kj b fi kt kq l kr ks">rand_clf.score(x_test,y_test)</span></pre><p id="5203" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi">0.64375</p><p id="68e6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">模型的准确率64%。我们需要通过调整超参数来调整该模型，以获得该模型的最佳拟合参数。让我们来看看细节。</p><p id="38ab" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">应用如下的超参数，</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="365e" class="kn ko hi kj b fi kp kq l kr ks">grid_param = {<br/>    "n_estimators" : [90,100,115,130],<br/>    'criterion': ['gini', 'entropy'],<br/>    'max_depth' : <strong class="kj hj">range</strong>(2,20,1),<br/>    'min_samples_leaf' : <strong class="kj hj">range</strong>(1,10,1),<br/>    'min_samples_split': <strong class="kj hj">range</strong>(2,10,1),<br/>    'max_features' : ['auto','log2']<br/>}</span></pre><p id="c6b1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用GridsearchCV的下一步是执行超参数调整的过程，以便确定给定模型的最佳值。GridSearchCV是Scikit-learn(或SK-learn)的model_selection包中的一个函数</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="b0e5" class="kn ko hi kj b fi kp kq l kr ks">sklearn.model_selection.GridSearchCV(estimator, param_grid,scoring=None,<br/>          n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, <br/>          pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)</span></pre><p id="a59c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.estimator:传递您想要检查超参数的模型实例。</p><p id="0e5f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.params_grid:保存想要尝试的超参数的字典对象</p><p id="da45" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.评分:您想要使用的评估指标，您可以简单地传递一个有效的字符串/评估指标的对象</p><p id="5e2b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.cv:对于每一组选定的超参数，您必须尝试的交叉验证的次数</p><p id="bc73" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5.verbose:在将数据放入GridSearchCV时，可以将它设置为1以获得详细的打印结果</p><p id="f2a9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">6.n_jobs:您希望为该任务并行运行的进程数，如果为-1，将使用所有可用的处理器。</p><p id="4ac9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们尝试将随机森林分类器应用到GridSearchCV中。</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="e3dc" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">grid_search</strong> = GridSearchCV(estimator=rand_clf,param_grid=grid_param,cv=5,n_jobs =-1,verbose = 3)</span><span id="74ab" class="kn ko hi kj b fi kt kq l kr ks">grid_search.fit(x_train,y_train)</span><span id="7064" class="kn ko hi kj b fi kt kq l kr ks"><strong class="kj hj">grid_search</strong>.best_params_</span><span id="a429" class="kn ko hi kj b fi kt kq l kr ks"><strong class="kj hj">{'criterion': 'entropy',<br/> 'max_depth': 12,<br/> 'max_features': 'log2',<br/> 'min_samples_leaf': 1,<br/> 'min_samples_split': 5,<br/> 'n_estimators': 90}</strong></span></pre><p id="0a22" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上述参数是该模型的最佳参数。我们需要将这些参数应用于随机森林分类器，并验证其准确性。</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="e17e" class="kn ko hi kj b fi kp kq l kr ks">rand_clf = RandomForestClassifier(criterion= 'entropy',<br/> max_depth = 12,<br/> max_features = 'log2',<br/> min_samples_leaf = 1,<br/> min_samples_split= 5,<br/> n_estimators = 90,random_state=6)</span><span id="2e50" class="kn ko hi kj b fi kt kq l kr ks">rand_clf.fit(x_train,y_train)</span><span id="c8bb" class="kn ko hi kj b fi kt kq l kr ks">rand_clf.score(x_test,y_test)</span></pre><p id="9ecc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi">0.6604166666666667</p><p id="3770" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">太好了！在使用GridsearchCV的最佳参数后，我们的准确度提高了2%。</p><p id="0b84" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">集成学习</strong></p><p id="51f5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">集成学习通过组合几个模型来帮助改善机器学习结果。与单一模型相比，这种方法可以产生更好的预测性能。基本思想是学习一组分类器(专家)，并允许他们投票。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/2eca3b9a250d1de6cfab72076f4e1ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OPVMkF0AkchdNYq2.png"/></div></div></figure><p id="4808" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们用一个例子来理解集成学习的概念。</p><blockquote class="ku kv kw"><p id="5ad9" class="iq ir jo is b it iu iv iw ix iy iz ja kx jc jd je ky jg jh ji kz jk jl jm jn hb bi translated">假设你是一名电影导演，你创作了一部关于一个非常重要和有趣的主题的短片。现在，您希望在电影公开之前获得初步反馈(评级)。你能做那件事的可能方法是什么？</p><p id="5f7c" class="iq ir jo is b it iu iv iw ix iy iz ja kx jc jd je ky jg jh ji kz jk jl jm jn hb bi translated"><strong class="is hj">答:</strong>你可以让你的一个朋友为你的电影评分。<br/>现在完全有可能你选择的那个人非常爱你，不想让你心碎，所以给你创作的糟糕作品打了1星。</p><p id="ed59" class="iq ir jo is b it iu iv iw ix iy iz ja kx jc jd je ky jg jh ji kz jk jl jm jn hb bi translated">另一种方法是请你的5位同事给电影评分。这应该能让你对这部电影有更好的了解。这种方法可以为您的电影提供真实的评级。但是一个问题仍然存在。这5个人可能不是你电影主题的“主题专家”。当然，他们可能理解电影摄影、镜头或音频，但同时可能不是黑色幽默的最佳评判者。</p><p id="18a4" class="iq ir jo is b it iu iv iw ix iy iz ja kx jc jd je ky jg jh ji kz jk jl jm jn hb bi translated">让50个人给电影打分怎么样？有些可能是你的朋友，有些可能是你的同事，有些甚至可能是完全陌生的人。</p><p id="e869" class="iq ir jo is b it iu iv iw ix iy iz ja kx jc jd je ky jg jh ji kz jk jl jm jn hb bi translated">在这种情况下，回答会更加一般化和多样化，因为现在你有了拥有不同技能的人。事实证明，这是一种比我们之前看到的案例更好的获得诚实评级的方法。</p><p id="1dbd" class="iq ir jo is b it iu iv iw ix iy iz ja kx jc jd je ky jg jh ji kz jk jl jm jn hb bi translated">通过这些例子，你可以推断出，与个人相比，一个多样化的群体可能会做出更好的决定。与单一模型相比，多样化的模型也是如此。机器学习中的这种多样化是通过一种称为集成学习的技术实现的。</p></blockquote><p id="431c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">集成技术</strong></p><p id="b768" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">装袋</strong></p><p id="d0f9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Bagging是一种集成技术，其中对训练数据的不同子集使用单个训练算法，其中子集采样通过替换(引导)来完成。一旦在所有子集上训练了该算法，bagging就通过聚合该算法在不同子集上做出的所有预测来做出预测。在回归的情况下，bagging预测只是所有预测的平均值，而在分类器的情况下，bagging预测是所有预测中最频繁的预测(多数投票)。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/61e204c94d34ce4266c96b6371b5c79c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Xq88JYx5HL1pSC07.png"/></div></div></figure><p id="fb49" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">增压</strong></p><p id="352c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Boosting是一种集成建模技术，它试图从多个弱分类器中构建一个强分类器。通过使用串联的弱模型来建立模型。首先，根据训练数据建立模型。然后建立第二个模型，试图纠正第一个模型中存在的错误。继续这个过程并添加模型，直到正确预测了完整的训练数据集或者添加了最大数量的模型。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/1c1f88ce5f84475abc07b6b463dcd660.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/0*QiRCpbWCcMjNvwUK.png"/></div></figure><p id="69cb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们试着按部就班地理解它:</p><ul class=""><li id="801b" class="ju jv hi is b it iu ix iy jb jw jf jx jj jy jn jz ka kb kc bi translated">B1由10个数据点组成，这10个数据点由两种类型组成，即加号(+)和减号(-)，其中5个数据点为加号(+)，另外5个数据点为减号(-)，每个点最初都被赋予相同的权重。第一个模型试图对数据点进行分类，并生成一条垂直分隔线，但它错误地将3加号(+)分类为减号(-)。</li><li id="8104" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">B2由来自先前模型的10个数据点组成，其中3个错误分类的加号(+)的权重更大，使得当前模型更努力地正确分类这些加号(+)。该模型生成一条垂直分隔线，该分隔线正确分类了先前错误分类的加号(+)，但在这次尝试中，它错误分类了三个减号(-)。</li><li id="7772" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">B3由来自先前模型的10个数据点组成，其中3个错误分类的负号(-)的权重更大，使得当前模型更努力地正确分类这些负号(-)。该模型生成一条水平分隔线，该分隔线可以正确分类之前错误分类的减号(-)。</li><li id="0930" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">B4将B1、B2和B3结合在一起，以建立一个强有力的预测模型，该模型比使用的任何单个模型都好得多。</li></ul><p id="46a4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">堆叠</strong></p><p id="d3e8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">叠加是一种集成技术，它将两个或多个模型(也称为基础模型)的预测进行组合，并将该组合用作新模型(元模型)的输入，即新模型基于基础模型的预测进行训练。</p><p id="b69e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设你有一个分类问题，你可以使用几个模型，如逻辑回归，SVM，KNN，随机森林等。这个想法是使用一些模型，如KNN，SVM作为基础模型，并使用这些模型进行预测。现在，这些模型做出的预测被用作随机森林的输入特征，以进行训练并给出预测。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/001c4a61d73ffbfd1747fdf1503aa0d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8YeYcYKMf6pDHsGH.png"/></div></div></figure><p id="2225" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">叠加技术的实现</strong></p><p id="ce0e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">导入所需的库</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="7a0c" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">Copy</strong><strong class="kj hj">import</strong> pandas <strong class="kj hj">as</strong> pd <br/><strong class="kj hj">import</strong> matplotlib.pyplot <strong class="kj hj">as</strong> plt <br/><strong class="kj hj">from</strong> mlxtend.plotting <strong class="kj hj">import</strong> plot_confusion_matrix <br/><strong class="kj hj">from</strong> mlxtend.classifier <strong class="kj hj">import</strong> StackingClassifier <br/><strong class="kj hj">from</strong> sklearn.model_selection <strong class="kj hj">import</strong> train_test_split <br/><strong class="kj hj">from</strong> sklearn.preprocessing <strong class="kj hj">import</strong> StandardScaler <br/><strong class="kj hj">from</strong> sklearn.linear_model <strong class="kj hj">import</strong> LogisticRegression <br/><strong class="kj hj">from</strong> sklearn.neighbors <strong class="kj hj">import</strong> KNeighborsClassifier <br/><strong class="kj hj">from</strong> sklearn.naive_bayes <strong class="kj hj">import</strong> GaussianNB <br/><strong class="kj hj">from</strong> sklearn.metrics <strong class="kj hj">import</strong> confusion_matrix <br/><strong class="kj hj">from</strong> sklearn.metrics <strong class="kj hj">import</strong> accuracy_score</span></pre><p id="e360" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正在加载数据集</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="81bc" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">df</strong> = pd.read_csv('heart.csv') # loading the dataset <br/>df.head()					 </span></pre><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es li"><img src="../Images/be561e09228b9e3bc74f9d5a9cb8623e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/0*06kGuY7P__OPX_CC.png"/></div></figure><p id="4d4f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">将数据分为训练和测试</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="8ba0" class="kn ko hi kj b fi kp kq l kr ks"># Creating X and y for training <br/><strong class="kj hj">X</strong> = df.drop('target', axis = 1) <br/><strong class="kj hj">y</strong> = df['target']</span><span id="6606" class="kn ko hi kj b fi kt kq l kr ks"># 20 % training dataset is considered for testing <br/><strong class="kj hj">X_train</strong>, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)</span></pre><p id="0cf8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">标准化数据</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="1ed7" class="kn ko hi kj b fi kp kq l kr ks"># initializing sc object <br/>sc = StandardScaler() </span><span id="6e63" class="kn ko hi kj b fi kt kq l kr ks"># variables that needed to be transformed <br/>var_transform = ['thalach', 'age', 'trestbps', 'oldpeak', 'chol'] <br/>X_train[var_transform] = sc.fit_transform(X_train[var_transform])</span><span id="8ca5" class="kn ko hi kj b fi kt kq l kr ks"># standardising training data <br/>X_test[var_transform] = sc.transform(X_test[var_transform])</span><span id="3c6a" class="kn ko hi kj b fi kt kq l kr ks"># standardising test data <br/>print(X_train.head())</span></pre><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/af874927bd7dba551801fe2cd8c71677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/0*m37BQLBh7PPm4BOE.png"/></div></figure><p id="a930" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">构建第一层估算器</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="37f5" class="kn ko hi kj b fi kp kq l kr ks">KNC = KNeighborsClassifier() # initialising KNeighbors Classifier <br/>NB = GaussianNB()			 # initialising Naive Bayes</span></pre><p id="7f84" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们用第一层估计量进行训练和评估，以观察堆叠模型和一般模型的性能差异</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="aef1" class="kn ko hi kj b fi kp kq l kr ks">model_kNeighborsClassifier = KNC.fit(X_train, y_train) # fitting Training Set <br/>pred_knc = model_kNeighborsClassifier.predict(X_test) # Predicting on test dataset</span></pre><p id="359a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">估价</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="0a06" class="kn ko hi kj b fi kp kq l kr ks">acc_knc = accuracy_score(y_test, pred_knc) # evaluating accuracy score <br/>print('accuracy score of KNeighbors Classifier is:', acc_knc * 100)</span></pre><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/8ab6298348144ba16b8d5d1bf652c3be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/0*Ca4ljo1NDl5jFR4m.png"/></div></figure><p id="4cda" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">朴素贝叶斯分类器</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="51ba" class="kn ko hi kj b fi kp kq l kr ks">model_NaiveBayes = NB.fit(X_train, y_train) <br/>pred_nb = model_NaiveBayes.predict(X_test)</span><span id="f660" class="kn ko hi kj b fi kt kq l kr ks">acc_nb = accuracy_score(y_test, pred_nb) <br/>print('Accuracy of Naive Bayes Classifier:', acc_nb * 100)</span></pre><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/f38097cd0635b1d5a09939de17339d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/0*u2sPHuinKzL-AveY.png"/></div></figure><p id="cef7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">实现堆叠分类器</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="2522" class="kn ko hi kj b fi kp kq l kr ks">lr = LogisticRegression() # defining meta-classifier <br/>clf_stack = StackingClassifier(classifiers =[KNC, NB], meta_classifier = lr, use_probas = True, use_features_in_secondary = True)</span></pre><p id="44fc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">培训和评估</p><pre class="jq jr js jt fd ki kj kk kl aw km bi"><span id="7536" class="kn ko hi kj b fi kp kq l kr ks"># training of stacked model<br/>model_stack = clf_stack.fit(X_train, y_train) </span><span id="de25" class="kn ko hi kj b fi kt kq l kr ks"># predictions on test data using stacked model<br/>pred_stack = model_stack.predict(X_test)</span><span id="6dde" class="kn ko hi kj b fi kt kq l kr ks">acc_stack = accuracy_score(y_test, pred_stack) <br/># evaluating accuracy <br/>print('accuray score of Stacked model:', acc_stack * 100)</span></pre><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/c32882df47234aff2ace87da10cbe2a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*SEm_u1phjKI0qV_2.png"/></div></figure><p id="4c8f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的两个独立模型的准确率接近80%，我们的堆叠模型的准确率接近84%。通过组合两个独立的模型，我们获得了显著的性能提升。</p><p id="c8dd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们会赶上另一篇有趣的文章。</p><p id="da03" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">快乐学习:)</p></div></div>    
</body>
</html>