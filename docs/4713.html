<html>
<head>
<title>Sentiment Analysis of Movie Reviews pt.4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">电影评论的情感分析第四部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sentiment-analysis-of-movie-reviews-pt-4-575ccddfdbc3?source=collection_archive---------1-----------------------#2022-01-15">https://medium.com/analytics-vidhya/sentiment-analysis-of-movie-reviews-pt-4-575ccddfdbc3?source=collection_archive---------1-----------------------#2022-01-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="420b" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">第四部分伯特</h2></div><p id="74b5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">更多代码链接到我的Github:<a class="ae js" href="https://github.com/charliezcr/Sentiment-Analysis-of-Movie-Reviews/blob/main/sa_p4.ipynb" rel="noopener ugc nofollow" target="_blank">https://Github . com/charliezcr/情操分析-电影评论/blob/main/sa_p4.ipynb </a></p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><p id="8c62" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在IMDb影评情感分析研究的第<a class="ae js" rel="noopener" href="/analytics-vidhya/sentiment-analysis-of-movie-reviews-pt-1-1a52daa90cdc"> 1 </a>、<a class="ae js" rel="noopener" href="/analytics-vidhya/sentiment-analysis-of-movie-reviews-pt-2-45045225a263"> 2 </a>、<a class="ae js" rel="noopener" href="/analytics-vidhya/sentiment-analysis-of-movie-reviews-b5241ca736b7"> 3 </a>部分，我使用了经典的监督学习方法。在这一部分，我将结合模型BERT使用迁移学习。</p><h1 id="e8f2" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">迁移学习</h1><p id="8c17" class="pw-post-body-paragraph iw ix hh iy b iz ks ii jb jc kt il je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">与我自己训练的模型第1-3部分不同，迁移学习是一种使用现有的预训练模型来处理特定任务的方法。这次的模型BERT是谷歌研究人员发明的用于自然语言理解领域多种用途的最先进的深度学习模型。通过对IMDb电影评论的数据集进行微调，我不需要训练自己的模型，而是使用转移的模型来完成预测电影评论情绪的特定任务。打个比喻来说，训练一个有监督的学习模型就像自己造车一样；微调一个转移学习模型就像调车一样，买车，改装。</p><h1 id="cda0" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">Pytorch和CUDA</h1><p id="2b48" class="pw-post-body-paragraph iw ix hh iy b iz ks ii jb jc kt il je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">看完所有数据，我会用Pytorch，一个深度学习的包。Pytorch中的CUDA是一个允许使用GPU进行并行计算的工具包(但实际上我的本地计算机上没有GPU，所以我仍将使用CPU)。用比喻的方式来解释。当你交付东西的时候，使用CPU就像开快车移动少量货物；使用GPU就像同时驾驶多辆皮卡移动大量货物。因为在深度学习中，计算是大量的矩阵乘法，使用GPU会加快进程。</p><p id="45fe" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对句子进行分词时，最大长度将设置为128。因此，超过126个标记的句子(另外两个标记是“cls”和“sep”，意思是句子的开始和结束)将被截断。少于126的句子末尾没有填充。</p><h1 id="4358" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">标记化</h1><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es kx"><img src="../Images/9a7354c5561e7cff5a9d0d699101ca45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YRUXUVRvneeixp6t.jpg"/></div></div></figure><p id="6392" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在<a class="ae js" rel="noopener" href="/analytics-vidhya/sentiment-analysis-of-movie-reviews-pt-1-1a52daa90cdc">第1部分</a>中，我使用TF-IDF将一个句子转换成向量，将句子转换成矩阵。在这一部分中，我将直接使用BERT tokenizer将句子转换为编码，由输入id和注意掩码组成。输入id是从句子中的单词转换而来的BERT的标记。注意力屏蔽表明每个标记在句子中需要多少注意力。</p><p id="ba7b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如，在句子<em class="lj">中有两只鸟。</em>如果“那里”得到最多的关注，我们就知道鸟在那里，而不是在这里。如果“二”得到的关注最多，我们知道不止1只鸟，不到3只鸟。如果“鸟”，我们知道那里的众生是鸟，而不是其他东西。</p><p id="a19c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">BERT中的注意机制使这个编码器在情感分析中很高效，因为它会注意到包含情感的单词和短语，如“喜欢”和“不喜欢”，是电影评论中最需要注意的。此外，BERT tokenizer读取句子是双向的，即从左到右和从右到左，因此它也考虑单词在句子中的位置。</p><p id="e088" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对句子进行分词时，最大长度将设置为128。因此，超过126个标记的句子(另外两个标记是“cls”和“sep”，意思是句子的开始和结束)将被截断。少于126的句子末尾没有填充。</p><pre class="ky kz la lb fd lk ll lm ln aw lo bi"><span id="43c5" class="lp kb hh ll b fi lq lr l ls lt"><strong class="ll hi">from</strong> transformers <strong class="ll hi">import</strong> BertTokenizer</span><span id="b5be" class="lp kb hh ll b fi lu lr l ls lt"><em class="lj"># load bert tokenizer</em></span><span id="2bc8" class="lp kb hh ll b fi lu lr l ls lt">tokenizer <strong class="ll hi">=</strong> BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case<strong class="ll hi">=True</strong>)</span><span id="78d9" class="lp kb hh ll b fi lu lr l ls lt"><em class="lj"># setting the maximum length of encodings to 128, so that encodings will not be too long</em></span><span id="8106" class="lp kb hh ll b fi lu lr l ls lt">encodings <strong class="ll hi">=</strong> tokenizer(reviews, truncation<strong class="ll hi">=True</strong>, padding<strong class="ll hi">=</strong>'max_length', return_tensors<strong class="ll hi">=</strong>'pt', max_length<strong class="ll hi">=</strong>128)</span><span id="b979" class="lp kb hh ll b fi lu lr l ls lt">input_ids <strong class="ll hi">=</strong> encodings['input_ids']</span><span id="ade2" class="lp kb hh ll b fi lu lr l ls lt">attention_masks <strong class="ll hi">=</strong> encodings['attention_mask']</span></pre><h1 id="844f" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">数据集和数据加载器</h1><p id="7675" class="pw-post-body-paragraph iw ix hh iy b iz ks ii jb jc kt il je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">对句子进行编码后，输入的id、注意屏蔽和标签将以张量形式存储在TensorDataset中，并分成训练集和测试集。然后，数据加载器在数据集上包装可重复项，并支持自动批处理、采样、混排和多进程数据加载。</p><p id="ec42" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">批处理意味着数据加载器将发送一批编码到模型中，用于训练过程中的小批量随机梯度下降。在这项研究中，我将批量大小设置为16。每次，dataloader都会将训练集中的所有数据进行重排，以生成新的一批数据。</p><pre class="ky kz la lb fd lk ll lm ln aw lo bi"><span id="bb27" class="lp kb hh ll b fi lq lr l ls lt"><strong class="ll hi">from</strong> torch.utils.data <strong class="ll hi">import</strong> TensorDataset, random_split, DataLoader</span><span id="d31d" class="lp kb hh ll b fi lu lr l ls lt"><em class="lj"># splitting traing and validation dataset</em></span><span id="3128" class="lp kb hh ll b fi lu lr l ls lt">dataset <strong class="ll hi">=</strong> TensorDataset(input_ids, attention_masks, torch.tensor(labels))</span><span id="ff50" class="lp kb hh ll b fi lu lr l ls lt">train_size <strong class="ll hi">=</strong> int(df.shape[0] <strong class="ll hi">*</strong> 0.8)</span><span id="235e" class="lp kb hh ll b fi lu lr l ls lt">val_size <strong class="ll hi">=</strong> df.shape[0] <strong class="ll hi">-</strong> train_size</span><span id="9af6" class="lp kb hh ll b fi lu lr l ls lt">train_dataset, val_dataset <strong class="ll hi">=</strong> random_split(dataset, [train_size, val_size])</span><span id="f9d8" class="lp kb hh ll b fi lu lr l ls lt"><em class="lj"># load datasets into dataloaders</em></span><span id="1ce8" class="lp kb hh ll b fi lu lr l ls lt">train_dataloader <strong class="ll hi">=</strong> DataLoader(train_dataset, shuffle<strong class="ll hi">=True</strong>, batch_size<strong class="ll hi">=</strong>16)</span><span id="2a03" class="lp kb hh ll b fi lu lr l ls lt">eval_dataloader <strong class="ll hi">=</strong> DataLoader(val_dataset, batch_size<strong class="ll hi">=</strong>16)</span></pre><h1 id="3ffc" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">培养</h1><p id="9338" class="pw-post-body-paragraph iw ix hh iy b iz ks ii jb jc kt il je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">Epochs表示在训练过程中训练集将被发送到模型的次数。对于培训，我将把时期的数量设置为3。通常推荐的历元数是2-4。我将使用ADAM作为梯度下降的优化器。</p><pre class="ky kz la lb fd lk ll lm ln aw lo bi"><span id="456b" class="lp kb hh ll b fi lq lr l ls lt"><strong class="ll hi">from</strong> transformers <strong class="ll hi">import</strong> BertForSequenceClassification, AdamW, get_scheduler</span><span id="6c55" class="lp kb hh ll b fi lu lr l ls lt"><em class="lj"># setting the model</em></span><span id="5c1b" class="lp kb hh ll b fi lu lr l ls lt">model <strong class="ll hi">=</strong> BertForSequenceClassification.from_pretrained(</span><span id="5e5b" class="lp kb hh ll b fi lu lr l ls lt">"bert-base-uncased",  <em class="lj"># Use the 12-layer BERT model, with an uncased vocab.</em></span><span id="c5ee" class="lp kb hh ll b fi lu lr l ls lt">num_labels<strong class="ll hi">=</strong>2,  <em class="lj"># The number of output labels--2 for binary classification.</em></span><span id="4e52" class="lp kb hh ll b fi lu lr l ls lt">output_attentions<strong class="ll hi">=False</strong>,  <em class="lj"># Whether the model returns attentions weights.</em></span><span id="a6d4" class="lp kb hh ll b fi lu lr l ls lt">output_hidden_states<strong class="ll hi">=False</strong>,  <em class="lj"># Whether the model returns all hidden-states.</em>)<br/></span><span id="a586" class="lp kb hh ll b fi lu lr l ls lt"><em class="lj"># setting the epochs</em></span><span id="eab3" class="lp kb hh ll b fi lu lr l ls lt">num_epochs <strong class="ll hi">=</strong> 3</span><span id="6dc8" class="lp kb hh ll b fi lu lr l ls lt"><em class="lj"># setting for gradient descent</em></span><span id="6520" class="lp kb hh ll b fi lu lr l ls lt">optimizer <strong class="ll hi">=</strong> AdamW(model.parameters(), lr<strong class="ll hi">=</strong>5e-5)</span><span id="7a0d" class="lp kb hh ll b fi lu lr l ls lt">num_training_steps <strong class="ll hi">=</strong> num_epochs <strong class="ll hi">*</strong> len(train_dataloader)</span><span id="131c" class="lp kb hh ll b fi lu lr l ls lt">lr_scheduler <strong class="ll hi">=</strong> get_scheduler("linear", optimizer<strong class="ll hi">=</strong>optimizer, num_warmup_steps<strong class="ll hi">=</strong>0, num_training_steps<strong class="ll hi">=</strong>num_training_steps)</span><span id="95bb" class="lp kb hh ll b fi lu lr l ls lt">model.train()</span><span id="c250" class="lp kb hh ll b fi lu lr l ls lt"><strong class="ll hi">for</strong> epoch <strong class="ll hi">in</strong> range(num_epochs):</span><span id="424e" class="lp kb hh ll b fi lu lr l ls lt"><strong class="ll hi">for</strong> batch <strong class="ll hi">in</strong> train_dataloader:</span><span id="8b75" class="lp kb hh ll b fi lu lr l ls lt"><em class="lj"># send batches to device (cpu or gpu)</em></span><span id="b4cc" class="lp kb hh ll b fi lu lr l ls lt">b_input_ids <strong class="ll hi">=</strong> batch[0].to(device)</span><span id="17e7" class="lp kb hh ll b fi lu lr l ls lt">b_input_mask <strong class="ll hi">=</strong> batch[1].to(device)</span><span id="0dea" class="lp kb hh ll b fi lu lr l ls lt">b_labels <strong class="ll hi">=</strong> batch[2].to(device)</span><span id="40a1" class="lp kb hh ll b fi lu lr l ls lt">outputs <strong class="ll hi">=</strong> model(b_input_ids, token_type_ids<strong class="ll hi">=None</strong>, attention_mask<strong class="ll hi">=</strong>b_input_mask, labels<strong class="ll hi">=</strong>b_labels, return_dict<strong class="ll hi">=True</strong>)</span><span id="f716" class="lp kb hh ll b fi lu lr l ls lt">loss <strong class="ll hi">=</strong> outputs.loss</span><span id="7793" class="lp kb hh ll b fi lu lr l ls lt">loss.backward()</span><span id="fa53" class="lp kb hh ll b fi lu lr l ls lt">optimizer.step()</span><span id="1459" class="lp kb hh ll b fi lu lr l ls lt">lr_scheduler.step()</span><span id="80bd" class="lp kb hh ll b fi lu lr l ls lt">optimizer.zero_grad()</span></pre><h1 id="a273" class="ka kb hh bd kc kd ke kf kg kh ki kj kk in kl io km iq kn ir ko it kp iu kq kr bi translated">估价</h1><p id="9a8f" class="pw-post-body-paragraph iw ix hh iy b iz ks ii jb jc kt il je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">在[30]中:</p><pre class="ky kz la lb fd lk ll lm ln aw lo bi"><span id="f17c" class="lp kb hh ll b fi lq lr l ls lt"><strong class="ll hi">from</strong> sklearn.metrics <strong class="ll hi">import</strong> accuracy_score</span><span id="ec4a" class="lp kb hh ll b fi lu lr l ls lt">y_pred <strong class="ll hi">=</strong> []</span><span id="eab4" class="lp kb hh ll b fi lu lr l ls lt">y_true <strong class="ll hi">=</strong> []</span><span id="db61" class="lp kb hh ll b fi lu lr l ls lt">model.eval()</span><span id="1d0e" class="lp kb hh ll b fi lu lr l ls lt"><strong class="ll hi">for</strong> batch <strong class="ll hi">in</strong> eval_dataloader:</span><span id="e71c" class="lp kb hh ll b fi lu lr l ls lt">b_input_ids <strong class="ll hi">=</strong> batch[0].to(device)</span><span id="8433" class="lp kb hh ll b fi lu lr l ls lt">b_input_mask <strong class="ll hi">=</strong> batch[1].to(device)</span><span id="d704" class="lp kb hh ll b fi lu lr l ls lt">b_labels <strong class="ll hi">=</strong> batch[2].to(device)</span><span id="c606" class="lp kb hh ll b fi lu lr l ls lt"><em class="lj"># set gradient to zero at the start of every batch</em></span><span id="bcae" class="lp kb hh ll b fi lu lr l ls lt"><strong class="ll hi">with</strong> torch.no_grad():</span><span id="dd54" class="lp kb hh ll b fi lu lr l ls lt">outputs <strong class="ll hi">=</strong> model(b_input_ids, token_type_ids<strong class="ll hi">=None</strong>, attention_mask<strong class="ll hi">=</strong>b_input_mask, labels<strong class="ll hi">=</strong>b_labels, return_dict<strong class="ll hi">=True</strong>)</span><span id="5268" class="lp kb hh ll b fi lu lr l ls lt">logits <strong class="ll hi">=</strong> outputs.logits</span><span id="8043" class="lp kb hh ll b fi lu lr l ls lt">predictions <strong class="ll hi">=</strong> torch.argmax(logits, dim<strong class="ll hi">=-</strong>1)</span><span id="44a6" class="lp kb hh ll b fi lu lr l ls lt">y_pred.extend(predictions.tolist())</span><span id="5f96" class="lp kb hh ll b fi lu lr l ls lt">y_true.extend(b_labels.tolist())</span><span id="0d6f" class="lp kb hh ll b fi lu lr l ls lt">print(f'Accuracy: {accuracy_score(y_pred, y_true)}')</span></pre><blockquote class="lv lw lx"><p id="c219" class="iw ix lj iy b iz ja ii jb jc jd il je ly jg jh ji lz jk jl jm ma jo jp jq jr ha bi translated">精确度:0.905</p></blockquote><p id="0a5b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以看到，准确率高于90%，也优于第1-3部分中的经典监督训练，证明BERT是一个更高级的情感分析模型。</p></div></div>    
</body>
</html>