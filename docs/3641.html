<html>
<head>
<title>How to break the “Curse of Dimensionality”?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何打破「维度诅咒」？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-break-the-curse-of-dimensionality-b366d5d23c86?source=collection_archive---------8-----------------------#2021-07-15">https://medium.com/analytics-vidhya/how-to-break-the-curse-of-dimensionality-b366d5d23c86?source=collection_archive---------8-----------------------#2021-07-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="0192" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">少即是多| ML中的特性选择</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/670449ab974f6bcbfae394fb552ff2dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CLwToYSfNgeMxz19zpIYIw.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:——【dataaspirant.com】</figcaption></figure><p id="ad00" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据越多，分析就越好。或者，那是神话？一次又一次，我们不断听到这个术语——“维数灾难”。为什么拥有更多维度(数据集中更多要素)是一种诅咒？让我们来深潜一下！</p><p id="de8c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设，我们有25个观察值，只有一个特征，也就是说，只有一个属性定义这些观察值。我们将这25个观察值绘制成平面上的点。因为只有一个特征，所以只有一个维度。从下图中，我们可以看到1D空间中的点是紧密排列的。</p><p id="8154" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">保持观察值的数量不变，如果我们将特征的数量增加到两个，那么这些点在2D空间中变得稀疏。稀疏性随着第三特征的增加而增加。你可以想象当有100或更多的维度时会发生什么。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ju"><img src="../Images/1046c86cc8087b907698e14e4813eb04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g-8AujbKOOdHI6IDT6mo_g.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:——【aiaspirant.com T4】</figcaption></figure><h2 id="991a" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated"><strong class="ak"> <em class="kq">空间的指数增长如何成为诅咒？</em>T9】</strong></h2><p id="1aaf" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">数据通常分为训练数据和测试数据。模型建立在训练数据上，并根据测试数据进行评估，也就是说，模型在未知数据上的性能被测量。</p><p id="72e9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我们正在建立一个员工流失模型，唯一的特征是性别。我们在训练数据中至少需要两个样本<em class="jc">(每个类别一个——女性和男性)</em>，以便模型可以准确地预测在测试数据中何时遇到相同的类别。现在，我们添加另一个特性，年龄组<em class="jc">(有两个类别，25-40岁，40-60岁)</em>。如果我们需要每个组合<em class="jc">(男性&amp;25-40岁，男性&amp;40-60岁，女性&amp;25-40岁，女性&amp;40-60岁)</em>的最少一个样本，我们在训练数据中将需要至少四个样本。对于第三个特征，比如具有三个类别的工作级别，我们将需要训练数据中的12个<em class="jc"> (2*2*3) </em>样本。</p><p id="631b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，样本大小必须随着特征数量的增加而增加，以便模型相当准确。如果有100+个特征，应该有足够的样本来解释每个特征。在这种意义上，如果模型仅在选择的特征组合上被训练，则当新的/不太频繁的特征组合被馈送到模型时，模型可能不能准确地预测结果。这是典型的<strong class="ig hi"> <em class="jc">过度拟合</em> </strong>的场景。</p><p id="fd1d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基于距离的算法，如kNN、聚类等。会认为每个记录是等距的，或者每个样本将是特征的唯一组合(由于代表性不足)。</p><blockquote class="kw kx ky"><p id="c20a" class="ie if jc ig b ih ii ij ik il im in io kz iq ir is la iu iv iw lb iy iz ja jb ha bi translated">我们拥有的特征越多，我们需要的样本数量就越多，才能在样本中很好地代表所有特征值的组合。</p></blockquote><blockquote class="lc"><p id="2e5f" class="ld le hh bd lf lg lh li lj lk ll jb dx translated">特征多于样本数量的场景(假设每个样本解释一个特征)被称为<em class="kq">维数灾难</em>。</p></blockquote><h2 id="47b6" class="jv jw hh bd jx jy lm ka kb kc ln ke kf ip lo kh ki it lp kk kl ix lq kn ko kp bi translated">治愈方法是什么？</h2><p id="598e" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">好吧，对于这个问题，显而易见且直截了当的答案是<strong class="ig hi"> <em class="jc">增加</em> </strong>的样本数量。如果维度如此之大，以至于增加更多的样本会使模型变得复杂怎么办？下一个最好的选择是<strong class="ig hi"> <em class="jc">来减少尺寸</em> </strong>。</p><p id="3ab7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然大的维度会对模型性能造成威胁(过度拟合)，但非常低的维度无助于模型学习数据中的基本模式(欠拟合)。<a class="ae jt" rel="noopener" href="/analytics-vidhya/whats-the-trade-off-between-bias-and-variance-2e25dd08716b"> <em class="jc">从这里了解更多关于过拟合&amp;欠拟合(偏差方差权衡)</em> </a>。</p><div class="lr ls ez fb lt lu"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/whats-the-trade-off-between-bias-and-variance-2e25dd08716b"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hi fi z dy lz ea eb ma ed ef hg bi translated">偏倚和方差之间的权衡是什么？</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">"近似正确比完全错误要好。"沃伦·巴菲特</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">medium.com</p></div></div><div class="md l"><div class="me l mf mg mh md mi jn lu"/></div></div></a></div><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mj"><img src="../Images/e2b5da6b7361be51d0a6ff7e93fd40d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*EygqHlRUP-c7Z-gDTisquA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:——<a class="ae jt" href="https://towardsdatascience.com" rel="noopener" target="_blank">towardsdatascience.com</a></figcaption></figure><p id="5e68" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">模型性能随着特征数量的增加而提高。它通过最佳数量的特征达到最高精度。此后性能下降。这被称为<strong class="ig hi">休斯现象</strong>。</p><p id="20ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是降维的两种方法。</p><p id="9406" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"><em class="jc"># 1——特征提取</em></strong>:——这是一种降低维数的技术，其中新的特征集是作为原始特征的组合而创建的，这样，<strong class="ig hi"> <em class="jc">新特征数&lt;原始特征数</em> </strong>。<strong class="ig hi"> <em class="jc"> </em> </strong>主成分分析&amp;因子分析都是特征提取的例子。查看我的博客中关于PCA的详细解释。 </p><div class="lr ls ez fb lt lu"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/multi-dimensional-data-boon-or-bane-3de73c9650bb"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hi fi z dy lz ea eb ma ed ef hg bi translated">多维数据——好事还是坏事？</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">PCA(主成分分析)的“为什么、什么和如何”</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">medium.com</p></div></div><div class="md l"><div class="mk l mf mg mh md mi jn lu"/></div></div></a></div><p id="deb6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"><em class="jc"># 2——特征选择</em></strong>:——该技术涉及从原始特征空间中选择相关/有用特征的子集。有几种方法可以选择这些特征。这篇博客讨论了一些特征选择的常用方法。</p></div><div class="ab cl ml mm go mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ha hb hc hd he"><h1 id="cb3c" class="ms jw hh bd jx mt mu mv kb mw mx my kf mz na nb ki nc nd ne kl nf ng nh ko ni bi translated"><strong class="ak">特征选择方法</strong></h1><h2 id="4ee7" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated"><strong class="ak"> #1 —过滤方法:- </strong></h2><p id="a831" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">过滤方法是预处理步骤的一部分，并且独立于所使用的ML算法。这些方法可以进一步分为无监督的和有监督的，这取决于目标变量是否被考虑用于特征选择。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nj"><img src="../Images/c3fa79a18268c555d123b076ed5f9ebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZuIMCSSqQzLF8wqw2Wsm1A.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:——<a class="ae jt" href="https://www.analyticsvidhya.com/" rel="noopener ugc nofollow" target="_blank">analyticsvidhya.com</a></figcaption></figure><h2 id="b9a5" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">→无监督过滤方法:-</h2><p id="6620" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">这些方法不考虑目标变量。这些方法在去除冗余/无用变量方面是有用的。</p><ul class=""><li id="9a6b" class="nk nl hh ig b ih ii il im ip nm it nn ix no jb np nq nr ns bi translated"><strong class="ig hi"> <em class="jc">缺失值比率</em> </strong> :-具有多个缺失值的特征，即如果缺失值的比率大于阈值，则这些特征不传达任何信息，因此可以在建模之前被丢弃。<em class="jc">可根据数据和域选择阈值。</em></li><li id="13b6" class="nk nl hh ig b ih nt il nu ip nv it nw ix nx jb np nq nr ns bi translated"><strong class="ig hi"> <em class="jc">低方差</em> </strong> :-方差很小或可以忽略的特征可以认为是<em class="jc">常量</em>。没有太多的变化，这些特征携带很少或没有信息。可以选择适当的阈值，基于该阈值，可以丢弃低方差特征。对值进行归一化可能是理想的，因为方差取决于要素的范围，并且预计每个要素的方差都不同。</li><li id="73a1" class="nk nl hh ig b ih nt il nu ip nv it nw ix nx jb np nq nr ns bi translated"><strong class="ig hi"> <em class="jc">高相关性</em> </strong> :-彼此具有高相关性(线性关系)的特征通常传达相似的信息。这些特征变得多余。例如，随着年龄的增长，工作级别增加，工资也增加。这些功能中的一个或多个可能会被删除。</li></ul><h2 id="3d41" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">→监督过滤方法:-</h2><p id="e46d" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">这些方法会考虑目标变量。通过测量独立特征和目标变量之间的关系，这些方法在去除无关变量方面是有用的。</p><blockquote class="kw kx ky"><p id="ece7" class="ie if jc ig b ih ii ij ik il im in io kz iq ir is la iu iv iw lb iy iz ja jb ha bi translated">互信息是通过独立特征获得的关于目标变量的信息量。具有最大互信息的特征被选为重要/相关特征。</p></blockquote><ul class=""><li id="56f5" class="nk nl hh ig b ih ii il im ip nm it nn ix no jb np nq nr ns bi translated">使用互信息进行特征选择的函数—<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"><em class="jc">mutual _ info _ class if</em></strong></a><strong class="ig hi"><em class="jc">，</em></strong><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"><em class="jc">mutual _ info _ regression</em></strong></a></li></ul><blockquote class="kw kx ky"><p id="85b6" class="ie if jc ig b ih ii ij ik il im in io kz iq ir is la iu iv iw lb iy iz ja jb ha bi translated">也可以计算独立特征的p值，以确定它们的显著性。p值越高，显著性越低。可以丢弃最不重要的特征。</p></blockquote><ul class=""><li id="4389" class="nk nl hh ig b ih ii il im ip nm it nn ix no jb np nq nr ns bi translated">使用p值的特征选择技术— <a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="jc"> ANOVA </em> </strong>(方差分析)</a>、<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="jc">卡方</em> </strong> </a>、<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="jc">皮尔逊系数</em> </strong> </a></li></ul><h2 id="0700" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated"><strong class="ak"> <em class="kq"> #2 —包装方法</em> </strong> :-</h2><p id="af68" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">特征的子集用于训练模型。基于样本外数据的模型性能，添加或删除特征并构建后续模型。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ny"><img src="../Images/9b410e94d927c4072d2cb9a2605516f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wib1bV5Ntwx41svnVAELxw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:——<a class="ae jt" href="https://www.analyticsvidhya.com/" rel="noopener ugc nofollow" target="_blank">analyticsvidhya.com</a></figcaption></figure><p id="bd38" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是一些常用的包装方法。</p><ul class=""><li id="80fa" class="nk nl hh ig b ih ii il im ip nm it nn ix no jb np nq nr ns bi translated"><strong class="ig hi"> <em class="jc">正向选择</em> :- </strong>从初始模型中的一个特征开始的建模迭代过程。在再次构建模型之前，添加一个最能改进模型的附加特征。这一过程一直持续到新特征的添加不能提高模型性能为止。</li><li id="aab1" class="nk nl hh ig b ih nt il nu ip nv it nw ix nx jb np nq nr ns bi translated"><strong class="ig hi"> <em class="jc">后向选择</em> </strong> :-这与前向选择类似，除了初始模型是用所有特征构建的。在每个迭代步骤中，最不重要的特征被移除，直到模型性能没有改善。</li><li id="1d3c" class="nk nl hh ig b ih nt il nu ip nv it nw ix nx jb np nq nr ns bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="jc">递归特征消除</em> </strong> </a> :-这使用外部估计器，如线性回归或随机森林，取决于监督学习的类型。初始模型具有所有功能。基于系数值或特征重要性，确定最不重要的特征。在移除最不重要的特征之后，构建下一个模型。从这些模型中只选择最好的模型。该最佳模型用于与在连续步骤中建立的模型进行比较。由于，它着眼于最佳模型，<strong class="ig hi">对于给定的步长只有</strong>，这种算法也被称为<strong class="ig hi"> <em class="jc">贪婪优化算法</em> </strong>。<em class="jc">特征子空间的大小(要选择的最佳特征数量)是一个超参数，可以进行调整以提高模型性能。每一步要移除的特征数量也是一个超参数。</em>默认情况下，仅删除最不重要的特征。这个迭代过程一直持续到达到最佳特征数量<em class="jc">(超参数)</em>。</li></ul><h2 id="3d87" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated"><strong class="ak"> <em class="kq"> #3 —嵌入方法</em> </strong> :-</h2><p id="b307" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">这些方法结合了过滤器和包装器方法的特点。当一个无关紧要的特性被删除时<em class="jc">(在包装方法中)</em>，这主要意味着该特性的系数变为零。嵌入方法不是删除特征，而是保留无关紧要的变量，然而，对它们的系数增加了惩罚，从而降低了它的重要性。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nz"><img src="../Images/92b1fff498cea6963972d8cd704fdd0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I0E26ss8sM5vqFEoKNGB3A.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:——<a class="ae jt" href="https://www.analyticsvidhya.com/" rel="noopener ugc nofollow" target="_blank">analyticsvidhya.com</a></figcaption></figure><p id="a737" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">岭回归</em> </strong> <em class="jc">(惩罚系数的平方和)</em><strong class="ig hi"><em class="jc">套索回归</em> </strong> <em class="jc">(惩罚系数的绝对值和)</em>都是嵌入式方法的流行例子。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es oa"><img src="../Images/4b073a756f71611155dd6132a7e2cfb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-xqfzhD7F7uxik-TSvy1A.png"/></div></div></figure></div><div class="ab cl ml mm go mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ha hb hc hd he"><h2 id="4d52" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">外卖:-</h2><p id="2d9d" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">为什么我们需要降维(在高维数据集中)？</p><p id="3ad5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">较少(最佳)数量的功能将…</p><ol class=""><li id="30eb" class="nk nl hh ig b ih ii il im ip nm it nn ix no jb ob nq nr ns bi translated">训练时间较少。</li><li id="eee5" class="nk nl hh ig b ih nt il nu ip nv it nw ix nx jb ob nq nr ns bi translated">降低模型复杂性，从而提高可解释性。</li><li id="f54f" class="nk nl hh ig b ih nt il nu ip nv it nw ix nx jb ob nq nr ns bi translated">降低过度拟合的可能性。</li><li id="6667" class="nk nl hh ig b ih nt il nu ip nv it nw ix nx jb ob nq nr ns bi translated">提高准确性。</li></ol><p id="41bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">嗯…</p><h1 id="6e9c" class="ms jw hh bd jx mt oc mv kb mw od my kf mz oe nb ki nc of ne kl nf og nh ko ni bi translated">少即是多！</h1></div></div>    
</body>
</html>