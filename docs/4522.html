<html>
<head>
<title>Policy and Value Functions in RL: REINFORCE AND SARSA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RL中的政策和价值功能:强化和SARSA</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/policy-and-value-functions-in-rl-reinforce-and-sarsa-f4418dbfc193?source=collection_archive---------3-----------------------#2021-11-07">https://medium.com/analytics-vidhya/policy-and-value-functions-in-rl-reinforce-and-sarsa-f4418dbfc193?source=collection_archive---------3-----------------------#2021-11-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/4560c3a05b0bfe121d0f5a856181d351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SmKXxUuECrCAu_U4"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated"><a class="ae hu" href="https://unsplash.com/@aideal?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">艾迪尔·华</a>在<a class="ae hu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/><div class=""><h2 id="ea18" class="pw-subtitle-paragraph iu hw hx bd b iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl dx translated">介绍</h2></div><p id="057c" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">在强化学习(RL)中，有两种重要的功能类型决定了主体在其环境中的行为。可以使用策略函数或值函数来控制代理的动作。基于策略的RL系统使用状态/动作概率来规划其对环境的动作，而基于奖励的RL系统使用奖励函数来确定其在环境中的动作。我们将从概念上讨论这两种算法类型，并通过一些简单的数学方法来计算这些算法——即分别为REINFORCE和SARSA。注意:这些概念可以在Laura Graesser和Wah Loon Keng的<em class="ki">“Python中深度强化学习理论和实践的基础”</em>中进行全面的回顾。</p><h2 id="0a36" class="kj kk hx bd kl km kn ko kp kq kr ks kt jv ku kv kw jz kx ky kz kd la lb lc ld bi translated">强化算法</h2><p id="eeeb" class="pw-post-body-paragraph jm jn hx jo b jp le iy jr js lf jb ju jv lg jx jy jz lh kb kc kd li kf kg kh ha bi translated">在强化学习(RL)中，代理通过一个<em class="ki">策略函数与其环境交互。</em><em class="ki">策略功能</em>指示代理将在其环境中做什么。在我们的例子中，策略(piθ)将状态映射到动作概率。在理解策略函数时要引入的第一个元素是目标函数。一个目标函数告诉我们RL系统的<strong class="jo hy"> <em class="ki">目标</em> </strong>，比如获得可能的最高分或者赢得比赛。在这个RL系统中，目标函数是:</p><figure class="lk ll lm ln fd hj er es paragraph-image"><div class="er es lj"><img src="../Images/1a157a11810663f779193cf6dc453889.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*WS9fgYbKw2PoUHhcnyZRLg.png"/></div></figure><p id="debe" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">本质上，使用梯度上升的目标函数的最大化为我们提供了如下的政策梯度:</p><figure class="lk ll lm ln fd hj er es paragraph-image"><div class="er es lo"><img src="../Images/5b0614927b0f877f9f6257d51a4b36d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*GOtWRHku2EUUI1ZsD49_Dg.png"/></div></figure><p id="dfbf" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">政策梯度是一种机制，通过这种机制，梯度产生的行动概率被改变。尽管数学很复杂，但要理解方程相对于θ的微分(微积分)将为我们提供<em class="ki">代理</em>到达其<strong class="jo hy"> <em class="ki">目标</em> </strong>的“路径”。</p><h2 id="572a" class="kj kk hx bd kl km kn ko kp kq kr ks kt jv ku kv kw jz kx ky kz kd la lb lc ld bi translated">SARSA算法</h2><p id="a959" class="pw-post-body-paragraph jm jn hx jo b jp le iy jr js lf jb ju jv lg jx jy jz lh kb kc kd li kf kg kh ha bi translated">接下来，我们将讨论RL中的值函数，它可以写成:</p><figure class="lk ll lm ln fd hj er es paragraph-image"><div class="er es lp"><img src="../Images/c63a9e717d1a5f9a82491fa50ca3376a.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*6YJmpu-nnOrtYIRr-jP1XQ.png"/></div></figure><p id="3fe2" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">价值函数试图(使用梯度上升)最大化一个RL系统的<em class="ki">回报</em>，以便代理基于回报选择其行动。RL中处理操纵值函数或Q函数的分支称为Q学习。</p><p id="c35a" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">有几个概念是Q函数的结果。首先，𝛾(伽玛)项决定了系统学习的速度。第二，<em class="ki">探索与开发</em>权衡以及𝜺(<em class="ki">ε)-贪婪</em>算法。</p><h2 id="a245" class="kj kk hx bd kl km kn ko kp kq kr ks kt jv ku kv kw jz kx ky kz kd la lb lc ld bi translated">伽马项</h2><p id="cee2" class="pw-post-body-paragraph jm jn hx jo b jp le iy jr js lf jb ju jv lg jx jy jz lh kb kc kd li kf kg kh ha bi translated">请注意，在q函数中有一个𝛾项，也称为贴现因子。它是报酬的乘数。如果𝛾很小，那么RL系统只关心不久的将来的步骤，而一个大的𝛾会使RL系统关心最终目标或未来的更多步骤。小𝛾导致更快的学习。对于许多应用程序来说，0.99𝛾是一个很好的默认值。</p><h2 id="65d9" class="kj kk hx bd kl km kn ko kp kq kr ks kt jv ku kv kw jz kx ky kz kd la lb lc ld bi translated">探索与开发的权衡&amp;𝜺(ε)贪婪算法</h2><p id="1aee" class="pw-post-body-paragraph jm jn hx jo b jp le iy jr js lf jb ju jv lg jx jy jz lh kb kc kd li kf kg kh ha bi translated">因为我们最大化了决定RL系统行为的Q函数，所以我们说这个算法是<em class="ki">“贪婪的”</em>𝜺(epsilon)是决定<em class="ki">函数有多贪婪的一个因素。奖励函数由(1 - 𝜺)确定，因此如果𝜺高(值=1.0)，函数变得不太确定，因为奖励都等于0，RL系统随机选择，使其“探索”低𝜺奖励函数在决策中高度敏感，导致“剥削”</em></p><h2 id="5379" class="kj kk hx bd kl km kn ko kp kq kr ks kt jv ku kv kw jz kx ky kz kd la lb lc ld bi translated">结论</h2><p id="601d" class="pw-post-body-paragraph jm jn hx jo b jp le iy jr js lf jb ju jv lg jx jy jz lh kb kc kd li kf kg kh ha bi translated">可以使用策略(pi)或基于值的算法(分别为加强和SARSA)来控制RL系统。策略算法利用它们的<strong class="jo hy">目标</strong>函数来决定行为，而基于值的算法利用<strong class="jo hy">奖励</strong>函数来决定行为。Q-learning已经应用于需要优化配置的计算机系统。它可以应用于新闻推荐系统，其中有一个动态的方法来满足用户的口味(它采用实时用户偏好，并使用Q-learning进行更改)。Q-learning可用于最大化交通灯控制，以学习交通拥堵时间并相应地确定灯。在更复杂的任务中，基于策略的算法往往工作得更好，因为它学习随机策略，而Q-learning涉及更连续的样本空间。随机策略如何胜过Q学习的一个例子是在石头/布/剪刀中。如果一个玩家每次都玩石头，Q学习方法就不起作用，因为它的回报会告诉系统每次输出相同的结果，而随机策略可以改变输出。</p><p id="1e19" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">可以使用这些政策的组合，研究似乎指向那个方向。我希望这个RL算法的简单介绍能对这个主题有所启发。祝你今天开心！玩的开心！</p></div></div>    
</body>
</html>