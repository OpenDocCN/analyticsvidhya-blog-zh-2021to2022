<html>
<head>
<title>Author Identification with Naive Bayes Algorithm — Model Building</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于朴素贝叶斯算法的作者识别——建模</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/author-identification-with-naive-bayes-algorithm-2-8b43854c1429?source=collection_archive---------16-----------------------#2021-01-02">https://medium.com/analytics-vidhya/author-identification-with-naive-bayes-algorithm-2-8b43854c1429?source=collection_archive---------16-----------------------#2021-01-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="2db1" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">托马斯·贝叶斯能区分史蒂夫·罗杰斯和托尼·斯塔克吗？</h2></div><p id="7e12" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">本文是三篇系列文章中的第二篇，旨在以端到端的方式构建朴素贝叶斯分类算法。在<a class="ae js" rel="noopener" href="/swlh/author-identification-using-naive-bayes-algorithm-1-abeeb88eb862">之前的帖子</a>中，我们查看了这个问题的数据:我们分别从《美国队长:第一复仇者联盟》、《钢铁侠》和《复仇者联盟4：终局之战》的剧本中搜集了《美国队长》和《钢铁侠》的对话。然后，我们对它们进行处理，删除所有动作/场景描述线索，只保留我们两个角色说的话，并创建了一个数据帧，并将它们保存为csv文件。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/d0909179b36da26d8f8adf544b0c7a8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j3tuPnOkDRrt39L7gOcwzg.jpeg"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">图片来自Pixabay演职员表:<a class="ae js" href="https://pixabay.com/users/vinsky2002-1151065/" rel="noopener ugc nofollow" target="_blank"> vinsky2002 </a></figcaption></figure><p id="07cb" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你可以参考<a class="ae js" rel="noopener" href="/swlh/author-identification-using-naive-bayes-algorithm-1-abeeb88eb862">这篇文章</a>来了解上面解释的所有操作。我们现在将集中于构建一个朴素贝叶斯分类器，并从头开始为之工作。在这篇文章中，我们将关注</p><ol class=""><li id="60b2" class="kj kk hh iy b iz ja jc jd jf kl jj km jn kn jr ko kp kq kr bi translated">理解条件概率</li><li id="6540" class="kj kk hh iy b iz ks jc kt jf ku jj kv jn kw jr ko kp kq kr bi translated">理解贝叶斯定理</li><li id="2b40" class="kj kk hh iy b iz ks jc kt jf ku jj kv jn kw jr ko kp kq kr bi translated">用于作者识别的朴素贝叶斯</li><li id="37ad" class="kj kk hh iy b iz ks jc kt jf ku jj kv jn kw jr ko kp kq kr bi translated">评估分类模型</li></ol><p id="15a9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">事不宜迟，让我们开始吧！</p></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="7496" class="le lf hh bd lg lh li lj lk ll lm ln lo in lp io lq iq lr ir ls it lt iu lu lv bi translated">理解条件概率</h1><p id="e3e4" class="pw-post-body-paragraph iw ix hh iy b iz lw ii jb jc lx il je jf ly jh ji jj lz jl jm jn ma jp jq jr ha bi translated">我们在之前的帖子里看到了对话的分布，其中大约40%是美国队长说的，60%是钢铁侠说的。我们可以用以下方式想象同样的情况:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mb"><img src="../Images/518bdf9f1e213060448d0fff2422ec6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W69oZs_adLQO5S3ucVcVRg.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">图片由Vinayak提供</figcaption></figure><p id="3939" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了简单起见，我们假设有25个对话，其中15个来自钢铁侠，用红色显示，10个来自美国队长，用蓝色显示；这是为了使图形代表我们的语料库。现在，我们可以从这张图中看出:</p><p id="cee4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">p(美国队长)= 10 / 25 = 2 / 5</p><p id="536d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">p(钢铁侠)= 15 / 25 = 3 / 5</p><p id="c4f8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，让我们看看对话，我们发现贾维斯这个词在钢铁侠的对话中出现了7次，而在美国队长的对话中只出现了两次。它可以在下图中显示</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mc"><img src="../Images/09f64b1283081f16d774559912446aa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O0wT-e1E4RunP4u2k4GZ8A.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">图片由Vinayak提供</figcaption></figure><p id="ee01" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里黄色代表Jarvis这个词出现在对话中。我们也可以在维恩图中看到同样的情况。</p><p id="efce" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果我只告诉你任何给定的对话有40%的几率是美国队长说的，有60%的几率是钢铁侠说的，然后问你一个给定的句子是钢铁侠说的还是美国队长说的，你很可能会说钢铁侠比美国队长多1.5倍。</p><p id="5ce6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">但是如果我告诉你这个句子包含单词Jarvis，现在你就有了一条额外的信息。你从上面的句子分布知道，贾维斯在钢铁侠的对话中出现的次数比在美国队长的对话中多。更具体地说，我们看到所有包含贾维斯(7)的对话中，有5个是钢铁侠说的，只有2个是美国队长说的。这意味着你很容易猜出钢铁侠的次数是美国队长的2.5倍，你猜对了。</p><p id="7aa3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">刚才发生的事情是条件概率在起作用的结果。仅仅通过提到一个句子包含Jarvis的事实，我们就将概率计算的语料库从25个减少到7个。其中，我们然后看到哪个角色说了多少次这个词，并计算了概率。简而言之，我们的条件是句子中有单词Jarvis。</p><p id="7ae7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">p(美国队长|贾维斯)= 2 / 7</p><p id="a8fe" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">p(钢铁侠|贾维斯)= 5 / 7</p><p id="8326" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，简而言之，我们可以从文氏图中看到，以特定证据为条件减少了我们的样本空间/语料库空间，我们可以更好地估计事件的概率。我们可以正式声明</p><p id="4e0c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">P(A|B) = P(A ∩ B) / P(B)</p><p id="292f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这在我们的例子中转化为以下两个表达式</p><p id="c66a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">P(美国队长|贾维斯)= P(美国队长∩贾维斯)/ P(贾维斯)</p><p id="68bd" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">P(钢铁侠|贾维斯)= P(钢铁侠∩贾维斯)/ P(贾维斯)</p></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="20d9" class="le lf hh bd lg lh li lj lk ll lm ln lo in lp io lq iq lr ir ls it lt iu lu lv bi translated">理解贝叶斯定理</h1><p id="400c" class="pw-post-body-paragraph iw ix hh iy b iz lw ii jb jc lx il je jf ly jh ji jj lz jl jm jn ma jp jq jr ha bi translated">使用上述条件概率的定义，我们可以做一些代数运算来得出一个方程，这就是俗称的贝叶斯规则或贝叶斯定理。</p><p id="5361" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">P(A|B) = P(A ∩ B) / P(B)</p><p id="1228" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">P(B|A) = P(B ∩ A) / P(A)</p><p id="6fb3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们知道P(B ∩ A) = P(A ∩ B)。</p><p id="8a27" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所以，我们可以把方程分开，最后得到</p><p id="03de" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">P(A | B) / P(B | A) = P(A) / P(B)</p><p id="27e1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">或者重组后</p><blockquote class="me"><p id="43c5" class="mf mg hh bd mh mi mj mk ml mm mn jr dx translated">P(A | B) = P(B | A) × P(A) / P(B)</p></blockquote><p id="5912" class="pw-post-body-paragraph iw ix hh iy b iz mo ii jb jc mp il je jf mq jh ji jj mr jl jm jn ms jp jq jr ha bi translated">在上面的等式中，每个术语都有特定的含义:</p><blockquote class="mt mu mv"><p id="07d8" class="iw ix md iy b iz ja ii jb jc jd il je mw jg jh ji mx jk jl jm my jo jp jq jr ha bi translated">p(A)—A类的先验概率</p><p id="92a2" class="iw ix md iy b iz ja ii jb jc jd il je mw jg jh ji mx jk jl jm my jo jp jq jr ha bi translated">P(B) —预测值B的先验概率</p><p id="a7c3" class="iw ix md iy b iz ja ii jb jc jd il je mw jg jh ji mx jk jl jm my jo jp jq jr ha bi translated">p(B | A)-似然性，即给定类别的预测值的概率</p><p id="e917" class="iw ix md iy b iz ja ii jb jc jd il je mw jg jh ji mx jk jl jm my jo jp jq jr ha bi translated">P(A | B) —给定预测值类别的后验概率</p></blockquote><p id="ce28" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当涉及到条件概率时，贝叶斯规则是一个非常有用的构造，因为在大多数情况下，我们知道条件概率的一种方式，但不知道另一种方式(出于许多实际原因)，这个规则帮助我们数字/现实地估计反向条件概率。</p><p id="e2a2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如，在疾病诊断实验中，我们可以通过对实际上正在遭受痛苦的人进行测试来估计P(阳性|疾病),但发现P(阳性|疾病)实际上是无法计算的，因为测试整个人群的成本太高，而且是徒劳的；尤其是如果这种疾病不太流行的话。</p><p id="2f32" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在我们的情况下，规则将是</p><p id="aa90" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">P(美国队长|贾维斯)= P(贾维斯|美国队长)× P(美国队长)/ P(贾维斯)</p><p id="3c03" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">RHS条件概率可以通过使用可用的文档语料库来估计，然后这可以在测试/推断时间期间用于找出哪个对话更可能是由哪个超级英雄说出的。</p></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="e07d" class="le lf hh bd lg lh li lj lk ll lm ln lo in lp io lq iq lr ir ls it lt iu lu lv bi translated">但是什么是朴素贝叶斯算法，它有什么幼稚之处？</h1><p id="8926" class="pw-post-body-paragraph iw ix hh iy b iz lw ii jb jc lx il je jf ly jh ji jj lz jl jm jn ma jp jq jr ha bi translated">我们已经在上面看到了贝叶斯规则或贝叶斯定理，即</p><p id="5320" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">P(A | B) = P(B | A) × P(A) / P(B)</p><p id="4d1d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在许多情况下，B实际上不是单个事件，而是一起发生的一系列事件。例如，在我们的例子中，我们可以将一个句子分解成单个的元素/标记/量子，在英语中是单词(在其他一些语言中可能是短语甚至字符)。该公式如下所示</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mz"><img src="../Images/2f1f722bca3ca863b2f6bb93698cc272.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*PTXRctol29g9VycBQ35HnQ.png"/></div></figure><p id="e2e9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果我们假设事件B1，B2，B3，… Bn是独立的，我们可以简化上面的等式并写成如下</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es na"><img src="../Images/92616f579ca7012f22bb436f083e8ee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*ordU-_07T8a-HcGH0FeP8Q.png"/></div></div></figure><p id="cf7b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所以，我们可以看到</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es nb"><img src="../Images/c3b1dc7a6013d3fab84be1a3704eaac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*Xjp5mC4YyyisIaERIXJxcA.png"/></div></figure><p id="0552" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上面的比例可以被用于在给定一组预测值的情况下识别最可能的类别。注意独立性假设是如何将这个复杂的问题变成非常模块化和相对简单的问题的。</p><p id="2e2d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在我们的例子中，我们可以取一个比率，得出这样的结果</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es nc"><img src="../Images/6ec9b03096451f5919cca239fc85e401.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*D8T1JC5iVR4Cy0uH0cYpPw.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">图片由Vinayak提供</figcaption></figure><p id="a42c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果比率大于1，我们可以说叙述者比Cap更可能是Stark，反之亦然。上述表示可以被简化并表示为</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es nd"><img src="../Images/61e0887d1cb3a18d8d9916cfe1de9155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*qkJOLf9gI-RzuYNExpXEpA.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">图片由Vinayak提供</figcaption></figure><p id="24c4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">其中Bi是句子B中的单个标记</p><p id="c04e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">句子中记号/单词的独立性是朴素贝叶斯算法中的一个朴素假设；因此得名。一般来说，在任何语言中，不同的单词和它们在句子中出现的顺序都是相互依赖的。在英语中，冠词总是与名词联系在一起，这意味着如果你有一个冠词，那么接下来的单词成为名词的可能性比动词或介词的可能性更大。</p><p id="4d67" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">尽管有天真的假设，但该算法为不同的任务提供了一个强大的基线，并且它的时间和空间复杂度远低于其他算法。这就是为什么在大多数分类问题中，数据科学家首先使用这种方法来获得基线，然后通过使用逐步复杂的方法来改进基线，如逻辑回归、随机森林、多层感知器等…</p></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="482b" class="le lf hh bd lg lh li lj lk ll lm ln lo in lp io lq iq lr ir ls it lt iu lu lv bi translated">用于作者识别的朴素贝叶斯</h1><p id="b424" class="pw-post-body-paragraph iw ix hh iy b iz lw ii jb jc lx il je jf ly jh ji jj lz jl jm jn ma jp jq jr ha bi translated">在我们之前工作的基础上，我们现在将看看如何使用朴素贝叶斯建立分类模型来识别对话的叙述者。如果你还没有，读读这篇前一篇文章会帮助你跟上我们正在做的事情。</p><p id="6388" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在阅读训练数据后，我们可以看到它看起来像这样:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ne"><img src="../Images/ed79310fd0be2e145763e51e88b8e651.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*qqWRGy7SA1DTZU_vY1YHGA.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">图片由Vinayak提供</figcaption></figure><p id="880c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由于一个句子是不同单词的某种排列，我们需要首先找到一种方法来解析这些单词。为此，我们使用了所谓的标记化。让我们来看看给定句子标记的一种方法。</p><h2 id="6cc7" class="nf lf hh bd lg ng nh ni lk nj nk nl lo jf nm nn lq jj no np ls jn nq nr lu ns bi translated">标记化</h2><p id="7f4d" class="pw-post-body-paragraph iw ix hh iy b iz lw ii jb jc lx il je jf ly jh ji jj lz jl jm jn ma jp jq jr ha bi translated">要将一个句子/文档用于任何任务，我们首先需要将其分解成各个组成部分，然后将其数字化，因为计算机只能处理数字。我们决定在特定任务中有意义的最小量，然后将句子分解成这些量。在英语中，对于我们的应用程序来说，在单词级别上分解句子是最有意义的(在其他语言中，您也可以根据相应语言的工作情况将其分解为短语/字符级别)。</p><figure class="ju jv jw jx fd jy"><div class="bz dy l di"><div class="nt nu l"/></div></figure><p id="faa7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们使用正则表达式来执行标记化过程。正如我们在上面看到的，我们首先使用了<code class="du nv nw nx ny b">compile</code>方法来定义我们的表达式，然后使用这些方法来<code class="du nv nw nx ny b">sub</code>(简称)那些带有各自标记的表达式。最终，我们将所有这些小写，并在空格上进行拆分，以获得我们的令牌。</p><p id="86aa" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是一个非常基本的方式来标记我们的句子；为了从一句话中提取出最多的内容，我们可以做得更好。例如，在英语中，如果你想表达当你打出相应的单词/句子时你在大喊大叫，那么用全大写字母写东西已经成为一种口语化的趋势。因此，所有大写的单词都可以在前面加上一个特殊的标记，比如MAJ，来表示这个意思，尽管句子中的每个单词都是小写的。类似地，所有专有名词都是标题大小写的，这可以通过在标题大小写的名词前添加一个特殊的标记来捕获。瑞秋·托马斯在<a class="ae js" href="https://www.youtube.com/playlist?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9" rel="noopener ugc nofollow" target="_blank">的这组视频</a>中对此做了很好的解释。你也可以参考<a class="ae js" href="https://github.com/fastai/course-nlp/blob/master/3-logreg-nb-imdb.ipynb" rel="noopener ugc nofollow" target="_blank">这本笔记本</a>以及特别的代币部分来了解更多。</p><p id="95a5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，我们将求助于上面定义的简单方法来进行标记化。我们正在捕捉标点符号，所有的单词和一些特殊的字符组合，如' nt，' ll等。表示否定、占有等。(即使它们在技术上不是单词，它们也有不同的含义，因此将它们分开并像那样使用是有意义的。)</p><h2 id="b036" class="nf lf hh bd lg ng nh ni lk nj nk nl lo jf nm nn lq jj no np ls jn nq nr lu ns bi translated">条件概率的计算</h2><p id="e806" class="pw-post-body-paragraph iw ix hh iy b iz lw ii jb jc lx il je jf ly jh ji jj lz jl jm jn ma jp jq jr ha bi translated">一旦我们有了代币，我们需要找出两个主要类别(美国队长和钢铁侠)的先验概率以及从他们的对话中提取的代币的条件概率。我们可以这样做</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es nz"><img src="../Images/e16461eecec90002a9fd719131f7d27e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f62TqqQs-5Dft5OBmtkJYQ.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">图片由Vinayak提供</figcaption></figure><p id="6943" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对句子进行标记后，我们找出两个类别(即钢铁侠和美国队长)中的单个标记的数量，并填充如上所示的表格。接下来，我们计算每个标记的条件概率，方法是将特定类别的标记数除以该类别的标记总数。对所有的类重复这个过程。</p><p id="f0b3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果我们使用上面讨论的修正的比率公式，我们可能会遇到两个潜在的问题</p><h2 id="208d" class="nf lf hh bd lg ng nh ni lk nj nk nl lo jf nm nn lq jj no np ls jn nq nr lu ns bi translated">被零除</h2><p id="67e5" class="pw-post-body-paragraph iw ix hh iy b iz lw ii jb jc lx il je jf ly jh ji jj lz jl jm jn ma jp jq jr ha bi translated">在包含P(word|class)等项的分母中，有可能一些概率项的值为零，就像上面的例子中我们看到的P(need | Iron Man) = 0。</p><p id="6a72" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了不遇到这样的问题，我们可以使用拉普拉斯平滑法，它将条件概率定义修改如下:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es oa"><img src="../Images/5be2b9430d6fe74828c55108a73bc22e.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*iTi3KG7jJyMd-VigcSPCzQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">图片由Vinayak提供</figcaption></figure><p id="6e8a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们在分子和术语vocab_size上加了1，vocab _ size指的是我们语料库中的单词总数。这将确保所有概率都是正的非零数量。</p><h2 id="3440" class="nf lf hh bd lg ng nh ni lk nj nk nl lo jf nm nn lq jj no np ls jn nq nr lu ns bi translated">乘法下溢</h2><p id="9bf3" class="pw-post-body-paragraph iw ix hh iy b iz lw ii jb jc lx il je jf ly jh ji jj lz jl jm jn ma jp jq jr ha bi translated">当绝对值小于1的数相乘时，它们的结果在数量上继续递减。如果我们有一个有很多单词的长句，这可能会发生；原则上来说，如果我们用手计算东西，这没什么大不了的，但是计算机不是这样工作的。</p><p id="0388" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">与数学不同，计算机是离散的和有限的；将许多小数字相乘将导致下溢，计算机无法准确计算结果。</p><p id="b6b9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们知道乘积的对数是对数的和，这就是我们用来把概率乘积转换成概率对数和的方法。这有助于规避这个问题。如果我们对上面讨论的修正方程取一个对数，我们最终得到</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ob"><img src="../Images/fde30ebc594fa585e2df2b85e81ae68e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*0cFBghpoMpd_i6msbTXQ-A.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">图片由Vinayak提供</figcaption></figure><p id="33d8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果RHS评估为正数，这意味着LHS上的比率大于1，这意味着对话更有可能来自Stark的颈静脉。</p><p id="f17c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">另一方面，如果RHS是负的，这意味着LHS上的比率小于1，并且更有可能是cap叙述了句子b</p><p id="efd3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在我们已经清楚了所有的术语和理论，是时候看看朴素贝叶斯算法了。</p><figure class="ju jv jw jx fd jy"><div class="bz dy l di"><div class="nt nu l"/></div></figure><p id="b1b2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们遵循上述概念中讨论的相同步骤，并将条目存储在字典的字典中，如下所示:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es oc"><img src="../Images/63f4fa0b5974d561a2e224f3e7932fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*devVC79H2QttJwi3oczYfA.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">图片由Vinayak提供</figcaption></figure><p id="7a91" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">该词汇词典包含作为关键字的每个单词/词汇项，并且它映射到如上所示的所考虑的类别的条件概率和对数似然性的词典。一旦我们有了这本字典，我们就可以训练模型了。先验概率的可能性还有一个组成部分，是这本字典的一部分。它只是映射到一个表示log(P(A) / P(B))的浮点值</p><p id="c086" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在朴素贝叶斯中，我们只需要这个字典映射就可以对任何给定的句子或文本片段进行推理。下面的函数描述了我们如何使用上面的概率进行预测。</p><figure class="ju jv jw jx fd jy"><div class="bz dy l di"><div class="nt nu l"/></div></figure><ul class=""><li id="f531" class="kj kk hh iy b iz ja jc jd jf kl jj km jn kn jr od kp kq kr bi translated">我们首先对句子进行标记，把它分解成独立的成分。</li><li id="dde3" class="kj kk hh iy b iz ks jc kt jf ku jj kv jn kw jr od kp kq kr bi translated">我们查找句子中每个标记的对数可能性；如果令牌不存在，我们使用一个特殊字段xxunk，并从概率字典中找到它的对数似然性。</li><li id="3faf" class="kj kk hh iy b iz ks jc kt jf ku jj kv jn kw jr od kp kq kr bi translated">我们将所有这些概率相加得出最终得分，并加上先验概率的对数似然。</li><li id="b3c6" class="kj kk hh iy b iz ks jc kt jf ku jj kv jn kw jr od kp kq kr bi translated">如果结果得分为正，我们将该对话归类为属于托尼·斯塔克，否则我们说它是由史蒂夫·罗杰斯叙述的。</li></ul><p id="2b97" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一旦我们写了这个函数，我们将在相同的数据上评估它的性能，然后在复仇者联盟结局的脚本中评估它的性能。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es oe"><img src="../Images/e929dd2c903801f1297b8879dbc57423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sNhhHeAKQt-dsrHv3m-PSQ.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">图片由Vinayak提供</figcaption></figure><p id="0732" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">训练和测试数据的准确率分别为88%和57%，两个数据集的混淆矩阵如上所示。我们可以看到算法对钢铁侠对话非常敏感，善于正确识别。这是意料之中的，因为我们有一个不平衡的数据集，更多的例子来自美国队长的对话，而较少来自钢铁侠的对话。</p><p id="bf89" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，我们现在有了朴素贝叶斯算法的工作实现。我们将存储上面显示的词汇表，并在下一篇文章中使用它进行推理，在下一篇文章中，我们将构建一个API端点并对其进行dockerize以进行部署。</p><p id="2508" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我希望你能像我写这篇文章时一样喜欢阅读上面的文章。请继续关注下一篇关于使用Flask、Flasgger和Dockers部署该模型的文章。</p></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="bc99" class="le lf hh bd lg lh li lj lk ll lm ln lo in lp io lq iq lr ir ls it lt iu lu lv bi translated">参考</h1><ol class=""><li id="19a9" class="kj kk hh iy b iz lw jc lx jf of jj og jn oh jr ko kp kq kr bi translated"><a class="ae js" href="https://www.coursera.org/learn/classification-vector-spaces-in-nlp" rel="noopener ugc nofollow" target="_blank">带分类的NLP&amp;向量空间— Deeplearning.ai </a></li><li id="7354" class="kj kk hh iy b iz ks jc kt jf ku jj kv jn kw jr ko kp kq kr bi translated"><a class="ae js" href="https://github.com/ElisonSherton/rogers-stark-classification" rel="noopener ugc nofollow" target="_blank">上述帖子的Github代码</a></li><li id="bfa1" class="kj kk hh iy b iz ks jc kt jf ku jj kv jn kw jr ko kp kq kr bi translated"><a class="ae js" href="https://www.youtube.com/watch?v=Q1zLqfnEXdw" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯&amp;瑞切尔·托马斯的正则表达式</a></li><li id="b5f4" class="kj kk hh iy b iz ks jc kt jf ku jj kv jn kw jr ko kp kq kr bi translated"><a class="ae js" rel="noopener" href="/swlh/author-identification-using-naive-bayes-algorithm-1-abeeb88eb862">上一篇关于使用朴素贝叶斯进行作者识别的数据收集的文章</a></li></ol></div></div>    
</body>
</html>