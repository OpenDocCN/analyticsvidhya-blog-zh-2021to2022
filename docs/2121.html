<html>
<head>
<title>A Beginners Guide For Machine Learning and Deep learning.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习和深度学习初学者指南。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-beginners-guide-for-machine-learning-and-deep-learning-1619762dab14?source=collection_archive---------18-----------------------#2021-04-05">https://medium.com/analytics-vidhya/a-beginners-guide-for-machine-learning-and-deep-learning-1619762dab14?source=collection_archive---------18-----------------------#2021-04-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/d1007acad4ccd587a598ac5e17584bc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NvH9vyEWpEqZsHkS"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">约翰·莫塞斯·鲍恩在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><blockquote class="iu iv iw"><p id="baef" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">理解数据表示</p></blockquote><p id="f936" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">在继续之前，让我们查找一个名为mnist的样本数据集。这是许多机器学习和深度学习实践中最常用的数据集之一。你可以从http://yann.lecun.com/exdb/mnist/的<a class="ae it" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">下载</a></p><p id="842e" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">可从该页面获得的MNIST手写数字数据库具有60，000个样本的训练集和10，000个样本的测试集。这是从NIST可获得的更大集合的子集。数字已经过大小标准化，并在固定大小的图像中居中。</p><p id="7899" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">对于那些想在真实世界的数据上尝试学习技术和模式识别方法，同时花费最少的精力进行预处理和格式化的人来说，这是一个很好的数据库。</p><p id="4eac" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">不幸的是，我们不打算覆盖这个数据集太多，它只是用来参考理解进一步的主题提到。</p><p id="356b" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi">涵盖的主题</strong></p><ul class=""><li id="f44a" class="jz ka hh ja b jb jc jf jg jw kb jx kc jy kd jv ke kf kg kh bi translated">关于张量及其类型的简单描述。</li><li id="33b6" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">关键属性(轴数、数据类型、形状)</li><li id="8810" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">数据张量的真实世界示例。</li><li id="8ddc" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">张量运算(整形、点积、广播)</li><li id="4822" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">神经网络引擎(基于梯度的优化，导数)</li></ul><blockquote class="iu iv iw"><p id="8057" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">张量</p></blockquote><p id="ec94" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">张量是存储在多维数组中的数据(数字)的容器。一般来说，所有当前的机器学习系统都使用张量作为它们的基本数据结构。2D张量是矩阵到任意维数的推广。在张量的情况下，一个维度通常称为轴。</p><blockquote class="iu iv iw"><p id="4264" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">张量/张量表示的类型</p></blockquote><ul class=""><li id="22ea" class="jz ka hh ja b jb jc jf jg jw kb jx kc jy kd jv ke kf kg kh bi translated"><strong class="ja hi"><em class="iz">Scaler/0D Tensors</em></strong><em class="iz"/>—只包含一个eg的数字。</li></ul><p id="4692" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">数组(32)</p><ul class=""><li id="0324" class="jz ka hh ja b jb jc jf jg jw kb jx kc jy kd jv ke kf kg kh bi translated"><strong class="ja hi"> <em class="iz">向量/ 1D张量</em></strong><em class="iz">——一个数字数组称为向量或1D张量，例如</em></li></ul><p id="4865" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">np.array([1，2，3，4，5])</p><p id="115c" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">上述向量有5个条目，并被表示为5维向量，请不要将5D向量视为5D张量。<strong class="ja hi"> <em class="iz">一个5D向量只有一个轴和5个沿轴的维度，而一个5D张量有5个轴。</em> </strong></p><p id="4566" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">维数可以表示为沿特定轴的条目数或张量中的轴数。是的，同意它令人困惑。</p><ul class=""><li id="f3b4" class="jz ka hh ja b jb jc jf jg jw kb jx kc jy kd jv ke kf kg kh bi translated"><strong class="ja hi"> <em class="iz">矩阵/ 2D张量— </em> </strong> <em class="iz">一个</em> n数组的向量是一个矩阵，一个矩阵有2个轴常指行和列。你可以直观地看到。</li></ul><p id="bcb9" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">np.array([[1，2，3，4，5]，[6，7，8，9，10]，[11，12，13，14，15]])</p><ul class=""><li id="4a5c" class="jz ka hh ja b jb jc jf jg jw kb jx kc jy kd jv ke kf kg kh bi translated"><strong class="ja hi"> <em class="iz"> 3D张量和高维张量— </em> </strong>如果你把这样的矩阵打包成一个新的数组，你会得到一个3D张量，通过打包一个3D张量，你会得到一个4D张量，等等。</li></ul><blockquote class="iu iv iw"><p id="eb6d" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">关键性控制程序</p></blockquote><ul class=""><li id="47cc" class="jz ka hh ja b jb jc jf jg jw kb jx kc jy kd jv ke kf kg kh bi translated"><strong class="ja hi"> <em class="iz">轴的数量</em> </strong> <em class="iz"> —您可以使用名为ndim的简单numpy函数查看一个张量轴。</em></li><li id="93db" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated"><strong class="ja hi"> <em class="iz"> Shape </em> </strong> —您可以使用numpy中名为Shape的函数查看张量的形状，该函数返回一个整数元组，该元组描述了张量沿每个轴有多少维。</li><li id="8f39" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated"><strong class="ja hi"> <em class="iz">数据类型或dtype </em> </strong> —您可以使用名为dtype的numpy函数查看您的张量数据类型，数据类型如“float32”、“float 64”、“unit8”等。</li></ul><blockquote class="iu iv iw"><p id="668a" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">数据张量的真实世界示例</p></blockquote><p id="6b56" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">让我们用几个与我们以后会遇到的例子相似的例子来使张量更具体。我们将要操作的数据几乎属于上述类别之一</p><ul class=""><li id="c610" class="jz ka hh ja b jb jc jf jg jw kb jx kc jy kd jv ke kf kg kh bi translated">矢量数据/ 2D张量形状</li><li id="3801" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">时间序列/序列/ 3D张量形状</li><li id="746c" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">图像/ 4D张量形状</li><li id="5517" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">视频数据/ 5D张量形状</li></ul><blockquote class="kn"><p id="77be" class="ko kp hh bd kq kr ks kt ku kv kw jv dx translated">矢量数据/ 2D张量数据</p></blockquote><p id="1112" class="pw-post-body-paragraph ix iy hh ja b jb kx jd je jf ky jh ji jw kz jl jm jx la jp jq jy lb jt ju jv ha bi translated">这是最常见的情况，其中每个数据点被编码为向量，因此一批数据将被编码为2D张量(向量阵列)，其中第一轴是样本轴，第二轴是特征轴。</p><p id="5ba2" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi"> <em class="iz">为eg </em>。</strong>文本文档的数据集，表示每个单词在其中出现的次数(假设一个字典有10，000个单词)。每个文档可以被编码为10，000个值的向量，因此600个单词的条目可以被存储到(600，10000)的张量中。</p><blockquote class="kn"><p id="02fa" class="ko kp hh bd kq kr lc ld le lf lg jv dx translated">时间序列/序列/ 3D张量形状</p></blockquote><p id="6af8" class="pw-post-body-paragraph ix iy hh ja b jb kx jd je jf ky jh ji jw kz jl jm jx la jp jq jy lb jt ju jv ha bi translated">在具有明确时间轴的3D张量中存储顺序或时间序列数据总是有意义的。每个样本可以被编码为一个矢量(2D张量)，因此一批数据将被编码为一个三维张量。</p><p id="c9fc" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi"> <em class="iz">为eg </em>。</strong>一个推文数据集，其中我们将每条推文编码为128个独特字符的字母表中的280个字符的序列，因此每条推文可以编码为形状的2D张量(280，128)，10，000条推文的数据集将得到(10000，280，128)。</p><blockquote class="kn"><p id="2c3f" class="ko kp hh bd kq kr lc ld le lf lg jv dx translated">图像/ 4D张量形状</p></blockquote><p id="dd16" class="pw-post-body-paragraph ix iy hh ja b jb kx jd je jf ky jh ji jw kz jl jm jx la jp jq jy lb jt ju jv ha bi translated">图像基本上有三维高度，宽度和颜色深度。尽管灰度数据(特别是像mnist数据)具有单一的颜色通道，因此可以存储在2D张量中，但是按照惯例，图像张量对于灰度图像总是具有一维颜色通道的3D。因此，一批大小为256 × 256的128幅灰度图像可以存储在形状张量(128，256，256，1)中，一批128幅彩色图像可以存储在形状张量(128，256，256，3)中。</p><blockquote class="kn"><p id="8068" class="ko kp hh bd kq kr lc ld le lf lg jv dx translated">视频数据/ 5D张量形状</p></blockquote><p id="b135" class="pw-post-body-paragraph ix iy hh ja b jb kx jd je jf ky jh ji jw kz jl jm jx la jp jq jy lb jt ju jv ha bi translated">视频数据是少数几种需要5D张量的真实世界数据之一。视频可以理解为一系列帧，每一帧都是一幅彩色图像。因为每个帧可以存储在3D张量(高度，宽度，颜色深度)中，所以帧序列可以存储在4D张量(帧，高度，宽度，颜色深度)中，因此一批不同的视频可以存储在形状的5D张量(样本，帧，高度，宽度，颜色深度)中。例如，以每秒4帧的速度采样的60秒、144 × 256的YouTube视频剪辑将有240帧。一批四个这样的视频剪辑将被存储在形状张量(4，240，144，256，3)中。总共有106，168，320个值！如果张量的数据类型是float32，那么每个值将存储在32位中，因此张量将表示405 MB。沉重！你在现实生活中遇到的视频要轻得多，因为它们不是存储在float32中的，它们通常是以较大的系数压缩的(如MPEG格式)。</p><blockquote class="iu iv iw"><p id="5d8b" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">神经网络或张量运算的齿轮</p></blockquote><ul class=""><li id="d9fc" class="jz ka hh ja b jb jc jf jg jw kb jx kc jy kd jv ke kf kg kh bi translated"><strong class="ja hi"> <em class="iz">元素式操作</em> </strong> —考虑独立应用于张量中每个条目的操作。</li><li id="7a06" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated"><strong class="ja hi"> <em class="iz">广播— </em> </strong>通过广播，您可以轻松应用2个张量元素的方式操作。</li><li id="96c2" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated"><strong class="ja hi"> <em class="iz">张量点— </em> </strong>点运算也称为乘积运算，是最常见、最有用的张量运算。与逐元素运算相反，它合并输入张量中的元素。</li><li id="cf72" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated"><strong class="ja hi"> <em class="iz">张量整形— </em> </strong>张量整形是指重新排列其行和列，以匹配目标形状。自然地，整形后的张量具有与初始<br/>张量相同的系数总数。</li></ul><blockquote class="iu iv iw"><p id="d996" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">神经网络的引擎</p></blockquote><ul class=""><li id="43df" class="jz ka hh ja b jb jc jf jg jw kb jx kc jy kd jv ke kf kg kh bi translated"><strong class="ja hi"> <em class="iz">基于梯度的优化— </em> </strong>神经网络层通常将其输入数据转换为eg。</li></ul><p id="b2cc" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">outputlayer=relu(dot(W，input)+b)</p><p id="8fe5" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">在该表达式中，W和b是张量，它们是层的属性，被称为层的<strong class="ja hi"> <em class="iz">【权重】</em> </strong>和<strong class="ja hi"> <em class="iz">可训练参数</em> </strong>。这些权重包含网络从训练数据的暴露中学习到的信息。最初，这些权重矩阵用小的随机值填充(这一步称为随机初始化)。当然，当W和b是随机的时，没有理由期望relu(dot(W，input) + b)会产生任何有用的表示。由此产生的表示毫无意义——但它们是一个起点。接下来是根据反馈信号逐渐调整这些权重。这种逐步调整，也叫<br/>训练。</p><p id="33a4" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">这发生在所谓的训练循环中，其工作方式如下。根据需要，循环重复这些步骤:</p><ul class=""><li id="160d" class="jz ka hh ja b jb jc jf jg jw kb jx kc jy kd jv ke kf kg kh bi translated">画出一批训练样本x和对应的目标y。</li><li id="a163" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">在x上运行网络(称为正向传递的步骤)以获得预测y_pred。</li><li id="d4fb" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">计算批次上的网络损失，这是y_pred和y之间不匹配<br/>的度量。</li><li id="f376" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">以略微减少该批次损失的方式更新网络的所有权重。</li></ul><p id="6332" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">我们最终得到一个训练数据损失非常低的网络:预测y_pred和预期目标y之间的不匹配很低。网络已经“学会”将其输入映射到正确的目标。从远处看，它可能看起来像魔术，但当我们把它简化为基本步骤时，它就变得简单了。</p><p id="a969" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi"> <em class="iz">张量运算的导数:梯度— </em> </strong>梯度是张量运算的导数。它是导数概念<br/>对多维输入函数的推广:即以张量为输入的函数。单个系数的函数f(x)的导数可以解释为f的曲线的斜率。同样，梯度(f)(W0)可以解释为描述f(W)在W0周围的曲率的张量。</p><ul class=""><li id="4eb3" class="jz ka hh ja b jb jc jf jg jw kb jx kc jy kd jv ke kf kg kh bi translated"><strong class="ja hi">应用于神经网络，这意味着<strong class="ja hi"> <em class="iz">解析地找到产生最小可能损失函数</em> </strong>的权重值的组合。这可以通过求解等式gradient(f)(W) = 0来实现。</strong></li><li id="0b6a" class="jz ka hh ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated"><strong class="ja hi"> <em class="iz">链式导数:反向传播算法— </em> </strong>在前面的算法中，我们很随意地假设，因为一个函数是可微的，所以我们可以显式地计算它的导数。实际上，一个神经网络函数由许多连锁在一起的张量运算组成，其中每一个都有一个简单的已知导数。例如，这是由三个张量运算a、b和c组成的网络f，具有权重矩阵W1、W2和W3:</li></ul><p id="b4ab" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">f(W1，W2，W3) = a(W1，b(W2，c(W3)))</p><p id="021b" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">微积分告诉我们，这样的函数链可以用下面的恒等式导出，叫做链法则:f(g(x)) = f'(g(x)) * g'(x)。将链规则应用于神经网络梯度值的计算产生了一种称为反向传播(有时也称为反向模式微分)的算法。反向传播从最终损失值开始，从顶层到底层反向工作，应用链规则来计算每个参数在损失值中的贡献。</p><p id="4dfc" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi"> <em class="iz">谢谢。</em> </strong></p></div></div>    
</body>
</html>