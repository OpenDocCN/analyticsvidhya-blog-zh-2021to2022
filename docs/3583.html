<html>
<head>
<title>Evolution Of Natural Language Processing(NLP)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理的发展</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/evolution-of-natural-language-processing-nlp-ac941b6523e9?source=collection_archive---------5-----------------------#2021-07-11">https://medium.com/analytics-vidhya/evolution-of-natural-language-processing-nlp-ac941b6523e9?source=collection_archive---------5-----------------------#2021-07-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex ie if ig ih er es paragraph-image"><div role="button" tabindex="0" class="ii ij di ik bf il"><div class="er es ca"><img src="../Images/376b76ae45eb8d7954cc096aaf3646ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gThM0OaZtn6DfKDCwnE_Bw.jpeg"/></div></div></figure><p id="07ef" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">在这篇文章中，我想分享一下过去十年中文本分析算法的发展。</p><p id="295c" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">自然语言(NLP)已经存在很长时间了，事实上，一个非常简单的单词袋模型是在20世纪50年代提出的。</p><p id="1b61" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">但是在这篇文章中，我想把重点放在NLP最近的发展上。</p><p id="e285" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">自2013年以来，由于机器学习算法的发展和进步，以及计算和内存成本的降低，该领域取得了巨大进展。</p><figure class="jn jo jp jq fd ih er es paragraph-image"><div role="button" tabindex="0" class="ii ij di ik bf il"><div class="er es jm"><img src="../Images/fd596dfa2050e5683b8d2e0f8f277d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gRh_JBMPMukfDV_yhQ7L_g.jpeg"/></div></div></figure><h1 id="0550" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated"><strong class="ak"> Word2Vec </strong></h1><p id="9c81" class="pw-post-body-paragraph io ip hh iq b ir kp it iu iv kq ix iy iz kr jb jc jd ks jf jg jh kt jj jk jl ha bi translated">2013年，由谷歌的托马斯·迈克尔·奥夫(Thomas Michael off)领导的研究团队推出了Word2Vec算法。</p><p id="1539" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">Word2Vec将文本转换成向量，也称为嵌入。每个向量由300个值组成，所以简称为300维向量。</p><p id="1716" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">因为它代表300维向量空间。然后，您可以使用这些矢量表示作为机器学习的输入。</p><p id="1917" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">使用这些向量，我们可以应用像K最近邻分类或聚类算法这样的算法。</p><p id="93a1" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">Word2Vec以两种不同的模型架构而闻名，</p><ul class=""><li id="43de" class="ku kv hh iq b ir is iv iw iz kw jd kx jh ky jl kz la lb lc bi translated">连续背单词</li><li id="a31f" class="ku kv hh iq b ir ld iv le iz lf jd lg jh lh jl kz la lb lc bi translated">连续跳跃图</li></ul><figure class="jn jo jp jq fd ih er es paragraph-image"><div role="button" tabindex="0" class="ii ij di ik bf il"><div class="er es li"><img src="../Images/439c1984b44871591e543a9bb670d16b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bBETsVNLyjnaFJgM9avkeQ.png"/></div></div></figure><p id="e180" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">该体系结构基于浅层两层神经网络。</p><p id="1b06" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated"><em class="lj"> CBOW从周围上下文单词的窗口中预测当前单词，而continuous skip gram使用当前单词来预测周围上下文单词的窗口。</em></p><p id="6aa2" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">Word2Vec面临的一个挑战是，它往往会遇到所谓的词汇问题，因为它的词汇只有300万个单词。建模体系结构给该单词分配零，这基本上是丢弃该单词。</p><p id="6758" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated"><a class="ae lk" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank">更多关于Word2Vec </a></p><h1 id="dde1" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated"><strong class="ak">手套</strong></h1><p id="9e7c" class="pw-post-body-paragraph io ip hh iq b ir kp it iu iv kq ix iy iz kr jb jc jd ks jf jg jh kt jj jk jl ha bi translated">2014年，由斯坦福大学的Jeffrey Pennington领导的研究团队引入了手套或全局向量。</p><p id="c74a" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">手套新方法使用回归模型通过无监督学习来学习单词表示。</p><p id="a567" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">该模型的主要直觉是简单的观察，即单词-单词共现概率的比率有可能编码某种形式的意义。</p><p id="4de8" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">GloVe还是有超词汇的问题。</p><p id="4467" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated"><a class="ae lk" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">关于手套的更多信息</a></p><h1 id="ac09" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">快速文本</h1><p id="afa7" class="pw-post-body-paragraph io ip hh iq b ir kp it iu iv kq ix iy iz kr jb jc jd ks jf jg jh kt jj jk jl ha bi translated">2016年，脸书的人工智能研究(FAIR)实验室在FastText上发表了他们的工作。</p><blockquote class="ll lm ln"><p id="0d10" class="io ip lj iq b ir is it iu iv iw ix iy lo ja jb jc lp je jf jg lq ji jj jk jl ha bi translated">他们说，这是高效文本分类和表示学习的图书馆</p></blockquote><p id="2f7a" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">FastText建立在Word2Vec的基础上，但是它将每个单词视为一组称为字符n-grams的子单词。这有助于解决Word2Vec和Glove中的词汇问题。</p><p id="7b2b" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">例如，单词“farming”被分成n个字母组，如</p><p id="270f" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">"农耕" =&gt; "f "，" fa "，" far "，" farm "，" farmi "，" farmin "，"农耕"</p><p id="788c" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">即使单词“farming”不在词汇表中，也有可能是“farm”。FastText学习的单词的嵌入是每个n-gram的嵌入的聚合，FastText使用相同的CBOW和skip-gram模型，FastText将Word2Vec的词汇效果增加到三百万个单词之外。</p><p id="5db0" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated"><a class="ae lk" href="https://fasttext.cc/" rel="noopener ugc nofollow" target="_blank">关于FastText的更多信息</a></p><h1 id="d001" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">变压器</h1><p id="086a" class="pw-post-body-paragraph io ip hh iq b ir kp it iu iv kq ix iy iz kr jb jc jd ks jf jg jh kt jj jk jl ha bi translated">文本分析演变的另一个大型里程碑是2017年在一篇名为“<a class="ae lk" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <em class="lj">注意力是你所需要的一切</em></a><em class="lj"/>的论文中引入了Transformer架构。</p><p id="5621" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">介绍了一种基于自注意机制的新型神经网络架构(隐藏状态的加权和作为上下文向量传递到未来时间步长，通常传递到序列到序列RNN的解码器部分)。</p><p id="df9d" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">注意力的概念之前已经针对不同的模型架构进行了研究，并且通常是指一个模型组件捕获输入和输出之间的相关性。在NLP术语中，注意力将把模型输出的每个单词映射到输入序列中的单词，根据它们对预测单词的重要性为它们分配权重。这种新的transformer架构中的自我关注机制专注于捕获输入序列中所有单词之间的关系，从而显著提高自然语言理解任务(如机器翻译)的准确性。虽然transformer架构标志着NLP的一个非常重要的里程碑，但其他研究团队仍在不断发展，将此作为替代架构的基础。</p><p id="e520" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated"><a class="ae lk" rel="noopener" href="/inside-machine-learning/what-is-a-transformer-d07dd1fbec04">关于变压器的更多信息</a></p><h1 id="1e54" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">BlazingText</h1><p id="d258" class="pw-post-body-paragraph io ip hh iq b ir kp it iu iv kq ix iy iz kr jb jc jd ks jf jg jh kt jj jk jl ha bi translated">2017年，AWS推出了BlazingText，他们说，BlazingText基于<a class="ae lk" href="https://aws.amazon.com/" rel="noopener ugc nofollow" target="_blank"> AWS </a>，提供了Word2Vec和文本分类算法的高度优化的实现。BlazingText使用多个CPU或GPU进行训练来扩展和加速Word2Vec。类似地，文本分类算法的BlazingText实现扩展了FastText，以使用带有自定义CUDA内核的GPU加速。CUDA或compute unified device architecture是由Nvidia开发的并行计算平台和编程模型。使用炽热的文本，您可以使用多核CPU或GPU在几分钟内训练一个超过10亿个单词的模型。BlazingText使用连续的单词包和skip gram训练体系结构创建字符n-gram和嵌入，BlazingText还允许您在事件基础上停止训练您的模型训练，比如说当验证准确性停止增加时。BlazingText还为存储在亚马逊简单存储服务或亚马逊S3中的数据集优化了IO。</p><p id="910c" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated"><a class="ae lk" href="https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html" rel="noopener ugc nofollow" target="_blank">关于BlazingText的更多信息</a></p><h1 id="287a" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">工程与后勤管理局</h1><p id="1942" class="pw-post-body-paragraph io ip hh iq b ir kp it iu iv kq ix iy iz kr jb jc jd ks jf jg jh kt jj jk jl ha bi translated">来自语言模型的嵌入(ElMo)，AllenNLP开发的NLP框架，2018。</p><blockquote class="ll lm ln"><p id="02d2" class="io ip lj iq b ir is it iu iv iw ix iy lo ja jb jc lp je jf jg lq ji jj jk jl ha bi translated"><em class="hh">他们喜欢这种深层次的语境化的词语表述。</em></p></blockquote><p id="4cbe" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">ELMo是一种在向量或嵌入中表示单词的新方法。这些单词向量是深度双向语言模型(biLM)的内部状态的学习函数，该模型是在大型文本语料库上预先训练的。</p><p id="16d6" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">在ELMO，单词向量是通过深度双向语言模型学习的。ELMO结合了前向和后向语言模型，因此能够更好地捕捉不同语言环境中的句法和语义。</p><p id="bd24" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated"><a class="ae lk" href="https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/" rel="noopener ugc nofollow" target="_blank">关于ELMo的更多信息</a></p><h1 id="62cc" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">GPT北部</h1><p id="8b8b" class="pw-post-body-paragraph io ip hh iq b ir kp it iu iv kq ix iy iz kr jb jc jd ks jf jg jh kt jj jk jl ha bi translated">OpenAI的生殖预训练(GPT)，2018年。就是半监督学习模型系列。</p><p id="3469" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">GPT基于变压器架构，但是执行两个训练步骤</p><ol class=""><li id="c0f3" class="ku kv hh iq b ir is iv iw iz kw jd kx jh ky jl lr la lb lc bi translated">GPT从大型未标记文本语料库中学习语言模型。</li><li id="8542" class="ku kv hh iq b ir ld iv le iz lf jd lg jh lh jl lr la lb lc bi translated">GPT使用标记数据执行监督学习步骤，以学习特定的NLP任务，如文本分类。</li></ol><p id="ded8" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">GPT是单向的。</p><p id="8aa1" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated"><a class="ae lk" rel="noopener" href="/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2">关于GPT车型的更多信息</a></p><h1 id="19ef" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">伯特</h1><p id="bfc9" class="pw-post-body-paragraph io ip hh iq b ir kp it iu iv kq ix iy iz kr jb jc jd ks jf jg jh kt jj jk jl ha bi translated">来自变形金刚(BERT)的双向编码器表示，谷歌人工智能语言，2018。</p><p id="edbf" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">自然语言处理的最新语言模型。</p><p id="e3bf" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">它起源于预训练上下文表征，包括<a class="ae lk" href="https://en.wikipedia.org/w/index.php?title=Semi-supervised_Sequence_Learning&amp;action=edit&amp;redlink=1" rel="noopener ugc nofollow" target="_blank">半监督序列学习</a>、<a class="ae lk" href="https://en.wikipedia.org/w/index.php?title=Generative_Pre-Training&amp;action=edit&amp;redlink=1" rel="noopener ugc nofollow" target="_blank">生成式预训练</a>、<a class="ae lk" href="https://en.wikipedia.org/wiki/ELMo" rel="noopener ugc nofollow" target="_blank"> ELMo </a>等。</p><p id="fbd6" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">BERT考虑给定单词的每次出现的<strong class="iq hi">上下文</strong>，</p><p id="dd54" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">举个例子，</p><p id="615c" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">考虑两句话，</p><p id="98d1" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">“他在经营一家公司”和“他在跑马拉松”</p><p id="59df" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">对于单词“running ”, BERT将提供一个上下文化的嵌入，根据句子的不同而不同。</p><p id="0dec" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">伯特是真正双向的，</p><p id="3844" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">在无监督训练步骤中，BERT从无标签文本、从左到右以及从右到左的上下文中学习表示。</p><p id="a5dd" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">最初的英语语言BERT有两种模型:</p><p id="23d6" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">(BERT _ BASE:具有12个双向自关注头的12个编码器，以及</p><p id="7e0d" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">(2)BERT _ LARGE:24个编码器，16个双向自关注头。</p><p id="0400" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">这两个模型都是根据从拥有8亿单词的<a class="ae lk" href="https://en.wikipedia.org/w/index.php?title=BooksCorpus&amp;action=edit&amp;redlink=1" rel="noopener ugc nofollow" target="_blank">图书语料库</a>和拥有2500万单词的英语维基百科中提取的未标记数据进行预训练的。</p><p id="677f" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated">这种新颖的方法引起了整个行业对BERT的兴趣，并导致了许多BERT模型的变化。其中一些是特定于语言、特定于领域的，而BERT模型是这类模型中最流行的。</p><p id="a716" class="pw-post-body-paragraph io ip hh iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl ha bi translated"><a class="ae lk" href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" rel="noopener" target="_blank">阅读更多关于伯特的信息</a></p></div></div>    
</body>
</html>