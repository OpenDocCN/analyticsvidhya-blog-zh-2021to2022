<html>
<head>
<title>The Less The Loss, The Better… But How?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">损失越少越好……但是怎么做呢？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-less-the-loss-the-better-but-how-66fe233c243c?source=collection_archive---------10-----------------------#2021-01-23">https://medium.com/analytics-vidhya/the-less-the-loss-the-better-but-how-66fe233c243c?source=collection_archive---------10-----------------------#2021-01-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="7cc9" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">你可以在我的Github上找到这篇文章。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/874159bfce3ef9ff68b2fc1363471d07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PqLjSI-rKJlf6ROl"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae iw" href="https://unsplash.com/@possessedphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">附身摄影</a>在<a class="ae iw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><p id="186c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">深度学习近年来取得了巨大的成功。随着数据越来越大，模型越来越深，我们必须使用优化算法来减少模型的损失。</p><p id="0fdf" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">无论是机器学习还是深度学习，我们总想把自己模型的损失降到最低。实际值和预测值之间的微小差异增加了我们未来预测的可靠性。</p><blockquote class="kj"><p id="c9d9" class="kk kl hh bd km kn ko kp kq kr ks ki dx translated">优化的目标是减少训练错误，通过这样做，我们应该小心过度拟合。</p></blockquote><p id="8023" class="pw-post-body-paragraph jn jo hh jp b jq kt ii js jt ku il jv jw kv jy jz ka kw kc kd ke kx kg kh ki ha bi translated">损失表明模型在准确性方面的表现。通过改变损失，我们可以改变我们训练的模型的性能。为此，我们通过最小化电流损耗来提高新模型的性能。就像这篇文章的名字一样，<strong class="jp hi"> <em class="ky">损失越少越好</em> </strong>。</p><blockquote class="kj"><p id="aa14" class="kk kl hh bd km kn ko kp kq kr ks ki dx translated"><em class="kz">将任何数学表达式最小化(或最大化)的过程称为优化。</em></p></blockquote><p id="2c98" class="pw-post-body-paragraph jn jo hh jp b jq kt ii js jt ku il jv jw kv jy jz ka kw kc kd ke kx kg kh ki ha bi translated">优化器是用于改变诸如权重、学习率等属性以减少损失的算法。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es la"><img src="../Images/eea61a134411bef819607fd4963c01b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*nM0iXoX5cWIROpbgWi4PQA.jpeg"/></div></figure><p id="93c7" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">学习率决定了每次迭代的步长。如果台阶很大，我们可能会错过最小值。如果很小，找到最小值需要时间。<strong class="jp hi">你可以阅读</strong> <a class="ae iw" rel="noopener" href="/analytics-vidhya/linear-regression-e5eecc7f26f1"> <strong class="jp hi">这篇</strong> </a> <strong class="jp hi">的文章，了解更多关于这个话题的详细信息。</strong></p><p id="03e4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我将在本文中提到六种优化算法。</p><ol class=""><li id="b153" class="lb lc hh jp b jq jr jt ju jw ld ka le ke lf ki lg lh li lj bi translated">随机梯度下降</li><li id="c883" class="lb lc hh jp b jq lk jt ll jw lm ka ln ke lo ki lg lh li lj bi translated">小批量随机梯度下降</li><li id="f6c1" class="lb lc hh jp b jq lk jt ll jw lm ka ln ke lo ki lg lh li lj bi translated">动力</li><li id="d5be" class="lb lc hh jp b jq lk jt ll jw lm ka ln ke lo ki lg lh li lj bi translated">阿达格拉德</li><li id="af71" class="lb lc hh jp b jq lk jt ll jw lm ka ln ke lo ki lg lh li lj bi translated">阿达德尔塔</li><li id="83b4" class="lb lc hh jp b jq lk jt ll jw lm ka ln ke lo ki lg lh li lj bi translated">RMSprop</li><li id="fe42" class="lb lc hh jp b jq lk jt ll jw lm ka ln ke lo ki lg lh li lj bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</li><li id="7fcd" class="lb lc hh jp b jq lk jt ll jw lm ka ln ke lo ki lg lh li lj bi translated">梯度下降</li></ol><p id="86bc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> <em class="ky">对于第8部分，我已经写过了。我会把</em> </strong> <a class="ae iw" rel="noopener" href="/analytics-vidhya/linear-regression-e5eecc7f26f1"> <strong class="jp hi"> <em class="ky">链接</em> </strong> </a> <strong class="jp hi"> <em class="ky">留给你去读。</em> </strong></p><p id="e8eb" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> <em class="ky">那我们开始吧！</em> </strong></p><h1 id="59b7" class="lp lq hh bd lr ls lt lu lv lw lx ly lz in ma io mb iq mc ir md it me iu mf mg bi translated">1.随机梯度下降</h1><p id="9f8c" class="pw-post-body-paragraph jn jo hh jp b jq mh ii js jt mi il jv jw mj jy jz ka mk kc kd ke ml kg kh ki ha bi translated">与梯度下降不同，它使用从原始数据集中随机选择的数据，而不是在每次迭代中使用整个数据集。最好是使用整个数据集来达到最小值，而不是随机地达到最小值，但是现在这个过程非常昂贵，因为数据开始增长。</p><p id="f6ec" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">SGD算法如下:</p><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="0c06" class="mr lq hh mn b fi ms mt l mu mv">θ = θ − α⋅∂(J(θ;x(i),y(i)))/∂θ</span></pre><p id="5a18" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">因为它是通过在每次迭代中从数据集中抽取随机样本来进行的，所以到达最小值的路径会有更多的噪声。但是只要我们达到最小值，这并不重要。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mw"><img src="../Images/48cd97ef573260aba5f722fff3cd7dd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YzXcNoFqoqO4dar1djgGhw.png"/></div></div></figure><blockquote class="kj"><p id="05c8" class="kk kl hh bd km kn mx my mz na nb ki dx translated">尽管与典型的梯度下降相比，它需要更多的迭代次数来达到最小值，但它在计算上仍然比典型的梯度下降便宜得多。</p></blockquote><h2 id="67d4" class="mr lq hh bd lr nc nd ne lv nf ng nh lz jw ni nj mb ka nk nl md ke nm nn mf no bi translated">SGD的优势</h2><ol class=""><li id="0317" class="lb lc hh jp b jq mh jt mi jw np ka nq ke nr ki lg lh li lj bi translated">内存需求小于梯度下降。</li><li id="fcf6" class="lb lc hh jp b jq lk jt ll jw lm ka ln ke lo ki lg lh li lj bi translated">通过每步仅关注训练集的一部分来减少过度拟合。</li></ol><h2 id="c4bc" class="mr lq hh bd lr nc ns ne lv nf nt nh lz jw nu nj mb ka nv nl md ke nw nn mf no bi translated">SGD的缺点</h2><ol class=""><li id="f461" class="lb lc hh jp b jq mh jt mi jw np ka nq ke nr ki lg lh li lj bi translated">可能会陷入局部最小值。</li><li id="d3d3" class="lb lc hh jp b jq lk jt ll jw lm ka ln ke lo ki lg lh li lj bi translated">完成一个纪元可能需要很长时间。</li></ol><h1 id="d351" class="lp lq hh bd lr ls lt lu lv lw lx ly lz in ma io mb iq mc ir md it me iu mf mg bi translated">2.小批量随机梯度下降</h1><p id="0e10" class="pw-post-body-paragraph jn jo hh jp b jq mh ii js jt mi il jv jw mj jy jz ka mk kc kd ke ml kg kh ki ha bi translated">我们知道梯度下降和随机梯度下降。让我们再复习一遍。然后我会提到批量梯度下降，我想让你知道。</p><p id="b061" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">梯度下降</strong>基本上是通过检查已经完成训练的数据集上的错误来执行改进过程。</p><p id="2ecc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">随机梯度下降</strong>，它不是在每个过程步骤中使用整个数据集，而是用它从数据中接收到的随机数据来执行优化过程。</p><p id="925a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">批量梯度下降</strong>是梯度下降算法的一种变体，它为训练数据集中的每个示例计算误差，但仅在评估了所有训练示例后更新模型。<a class="ae iw" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/" rel="noopener ugc nofollow" target="_blank">【1】</a></p><p id="7d31" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">小批量梯度下降是梯度下降算法的一种变体，该算法将训练数据集分成小批量，用于计算模型误差和更新模型系数。<a class="ae iw" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/" rel="noopener ugc nofollow" target="_blank">【2】</a></p><p id="4798" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">MB-SGD算法是SGD算法的扩展，它克服了SGD算法时间复杂度大的问题。MB-SGD算法从数据集中取出一批点或点的子集来计算导数。</p><p id="8c83" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">可以观察到，MB-SGD的损失函数的导数与GD的损失函数的导数在一些迭代次数之后几乎相同。但是与GD相比，MB-SGD实现最小值的迭代次数很大，并且计算成本也很高。<a class="ae iw" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/" rel="noopener ugc nofollow" target="_blank">【3】</a></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nx"><img src="../Images/269d4c6dbb4cb244d9dd34f3575f00df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WCmZ3BCijB3BgSaLY6ngBg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae iw" href="https://datascience-enthusiast.com/figures/kiank_minibatch.png" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="aaff" class="mr lq hh bd lr nc ns ne lv nf nt nh lz jw nu nj mb ka nv nl md ke nw nn mf no bi translated">MB-SGD的优势</h2><p id="f232" class="pw-post-body-paragraph jn jo hh jp b jq mh ii js jt mi il jv jw mj jy jz ka mk kc kd ke ml kg kh ki ha bi translated">1.与SGD算法相比，达到最小值需要更少的时间。</p><p id="7809" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">2.在计算上比SGD更高效。</p><h2 id="bbc3" class="mr lq hh bd lr nc ns ne lv nf nt nh lz jw nu nj mb ka nv nl md ke nw nn mf no bi translated">MB-SGD的缺点</h2><ol class=""><li id="c91c" class="lb lc hh jp b jq mh jt mi jw np ka nq ke nr ki lg lh li lj bi translated">可能会陷入局部最小值。</li><li id="86a1" class="lb lc hh jp b jq lk jt ll jw lm ka ln ke lo ki lg lh li lj bi translated">小批量需要为学习算法配置一个额外的“小批量”超参数。<a class="ae iw" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/" rel="noopener ugc nofollow" target="_blank">【4】</a></li></ol><h1 id="7fa4" class="lp lq hh bd lr ls lt lu lv lw lx ly lz in ma io mb iq mc ir md it me iu mf mg bi translated">3.动力</h1><p id="dc4d" class="pw-post-body-paragraph jn jo hh jp b jq mh ii js jt mi il jv jw mj jy jz ka mk kc kd ke ml kg kh ki ha bi translated">由于MB-SGD将在每次迭代时更新参数，因此它将采用的路径将是振荡的。动量也考虑先前的梯度来更新参数。</p><p id="e1ea" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">MB-SGD算法的一个主要缺点是权重的更新非常嘈杂。具有动量的SGD通过对梯度去噪克服了这个缺点。权重的更新依赖于有噪声的导数，并且如果我们以某种方式对导数去噪，那么收敛时间将会减少。想法是使用指数加权平均来对导数进行去噪，即与先前更新相比，给予最近更新更多的权重。它加速了向相关方向的收敛，减少了向无关方向的波动。该方法中还使用了一个超参数，称为动量，用“γ”表示。<a class="ae iw" href="https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html" rel="noopener ugc nofollow" target="_blank"/></p><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="4181" class="mr lq hh mn b fi ms mt l mu mv">V(t) = γ.V(t−1) + α.∂(J(θ))/∂θ</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ny"><img src="../Images/1af20ab56cd271792396d3ea4c5987f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q8Rw-I-jT3gNdC6Tcrb8Vg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae iw" href="https://miro.medium.com/max/1684/0*TKxSMrG2xPLtcRVy.png" rel="noopener">来源</a></figcaption></figure><blockquote class="nz oa ob"><p id="8122" class="jn jo ky jp b jq jr ii js jt ju il jv oc jx jy jz od kb kc kd oe kf kg kh ki ha bi translated"><strong class="jp hi">动量项γ通常设定为0.9或类似值。使用所有先前的更新来计算时间“t”处的动量，与先前的更新相比，给予最近的更新更大的权重。这导致收敛速度加快。</strong></p></blockquote><p id="9ba1" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">你可以简单的把它想成一个球从山上掉下来。它的速度会根据山坡的方向而变化。</p><h1 id="a40d" class="lp lq hh bd lr ls lt lu lv lw lx ly lz in ma io mb iq mc ir md it me iu mf mg bi translated">4.阿达格拉德</h1><p id="d61e" class="pw-post-body-paragraph jn jo hh jp b jq mh ii js jt mi il jv jw mj jy jz ka mk kc kd ke ml kg kh ki ha bi translated">其他优化算法保持学习速率不变，而AdaGrad根据权重分配自适应学习速率。它对与频繁出现的特征相关联的参数执行较小的更新，而对与不频繁出现的特征相关联的参数执行较大的更新。因此，该算法非常适合稀疏数据。AdaGrad在谷歌用于训练大规模神经网络。此外，AdaGrad用于训练手套单词嵌入，因为不常用的单词比常用的单词需要更多的更新。<a class="ae iw" href="https://ruder.io/optimizing-gradient-descent/index.html#adagrad" rel="noopener ugc nofollow" target="_blank">【6】</a></p><p id="d23a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我不会过多地探究这个问题的数学原理。如果您想了解关于该主题的更多信息，您可以阅读源代码6。</p><h2 id="89fe" class="mr lq hh bd lr nc ns ne lv nf nt nh lz jw nu nj mb ka nv nl md ke nw nn mf no bi translated">阿达格拉德的优势</h2><p id="4ba5" class="pw-post-body-paragraph jn jo hh jp b jq mh ii js jt mi il jv jw mj jy jz ka mk kc kd ke ml kg kh ki ha bi translated">1.无需更新学习率，因为它是手动改变的。</p><h2 id="dc61" class="mr lq hh bd lr nc ns ne lv nf nt nh lz jw nu nj mb ka nv nl md ke nw nn mf no bi translated">阿达格勒的缺点</h2><ol class=""><li id="b866" class="lb lc hh jp b jq mh jt mi jw np ka nq ke nr ki lg lh li lj bi translated">当迭代次数变得非常大时，学习率降低到非常小的数值，这导致收敛缓慢。</li></ol><h1 id="e4fb" class="lp lq hh bd lr ls lt lu lv lw lx ly lz in ma io mb iq mc ir md it me iu mf mg bi translated">5.阿达德尔塔</h1><p id="d768" class="pw-post-body-paragraph jn jo hh jp b jq mh ii js jt mi il jv jw mj jy jz ka mk kc kd ke ml kg kh ki ha bi translated">对于AdaGrad，随着迭代次数的增加，学习率变得非常小，因此收敛到最小值需要太长时间。AdaDelta基于梯度更新的移动窗口来调整学习速率，而不是累积所有过去的梯度。这样，即使已经进行了多次更新，AdaDelta也能继续学习。相比Adagrad，在AdaDelta的原始版本中，你不用设置初始学习速率。</p><p id="da75" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">使用Adadelta，我们甚至不需要设置默认学习率，因为它已经从更新规则中删除了。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es of"><img src="../Images/d199fbcf3230ea8026dc5c5b2f1ef7f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UMFXwMh6uohI5U8mQ_9X7A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae iw" href="https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="e577" class="lp lq hh bd lr ls lt lu lv lw lx ly lz in ma io mb iq mc ir md it me iu mf mg bi translated">6.RMSprop</h1><p id="5632" class="pw-post-body-paragraph jn jo hh jp b jq mh ii js jt mi il jv jw mj jy jz ka mk kc kd ke ml kg kh ki ha bi translated">RMSprop是Geoff Hinton在他的Coursera课程的第6e讲中提出的一种未发表的自适应学习率方法。RMSprop将学习率除以梯度平方的指数衰减平均值。Hinton建议将γ设置为0.9，而学习率η的一个好的默认值是0.001。RMSprop是为了解决Adagrad学习率急剧下降的问题而开发的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es og"><img src="../Images/e1213e204d35a58fd104fe287431245e.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*8CUqm5l6ejRMUhgWKp5aCQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae iw" href="https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="87b6" class="lp lq hh bd lr ls lt lu lv lw lx ly lz in ma io mb iq mc ir md it me iu mf mg bi translated">7.亚当</h1><p id="70ca" class="pw-post-body-paragraph jn jo hh jp b jq mh ii js jt mi il jv jw mj jy jz ka mk kc kd ke ml kg kh ki ha bi translated">Adam可以看作是RMSprop和带动量的随机梯度下降的组合。Adam计算每个参数的自适应学习率。除了存储像Adadelta和RMSprop这样的过去平方梯度vt的指数衰减平均值之外，Adam还保持过去梯度mt的指数衰减平均值，类似于动量。动量可以被视为一个沿斜坡向下运行的球，而亚当的行为就像一个有摩擦力的重球，因此更喜欢误差面上平坦的极小值。<a class="ae iw" href="https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html" rel="noopener ugc nofollow" target="_blank">【7】</a></p><h1 id="db3b" class="lp lq hh bd lr ls lt lu lv lw lx ly lz in ma io mb iq mc ir md it me iu mf mg bi translated">实施时间！</h1><p id="c022" class="pw-post-body-paragraph jn jo hh jp b jq mh ii js jt mi il jv jw mj jy jz ka mk kc kd ke ml kg kh ki ha bi translated">我用谷歌Colab做这项工作。可以通过这个<a class="ae iw" href="https://colab.research.google.com/notebooks/intro.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>免费使用Colab GPU！</p><p id="5286" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在，是实施的时候了。我们很快就会看到，并理解优化算法之间的差异。</p><p id="5611" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这部分我们将使用MNIST数据集。</p><blockquote class="nz oa ob"><p id="0916" class="jn jo ky jp b jq jr ii js jt ju il jv oc jx jy jz od kb kc kd oe kf kg kh ki ha bi translated"><strong class="jp hi"> <em class="hh">此作品均取自</em> </strong> <a class="ae iw" href="https://github.com/ayyucekizrak/Keras_ile_Derin_Ogrenmeye_Giris/blob/master/B%C3%B6l%C3%BCm4/Optimizasyon_Y%C3%B6ntemlerinin_Kar%C5%9F%C4%B1la%C5%9Ft%C4%B1rmas%C4%B1.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hi"> <em class="hh">此处</em> </strong> </a> <strong class="jp hi"> <em class="hh">。</em> </strong></p></blockquote><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="636f" class="mr lq hh mn b fi ms mt l mu mv"># importing libraries</span><span id="0d88" class="mr lq hh mn b fi oh mt l mu mv">import keras<br/>from keras.datasets import mnist<br/>from keras.models import load_model<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Flatten<br/>from keras. layers import Conv2D, MaxPooling2D<br/>from keras import backend as K<br/>from keras import optimizers<br/>from keras.callbacks import ReduceLROnPlateau<br/>import tensorflow  as tf<br/>from keras.layers import *<br/>import matplotlib.pyplot as plt</span><span id="3e89" class="mr lq hh mn b fi oh mt l mu mv"># download dataset</span><span id="bea6" class="mr lq hh mn b fi oh mt l mu mv">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><span id="8db5" class="mr lq hh mn b fi oh mt l mu mv">reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)</span><span id="ccc1" class="mr lq hh mn b fi oh mt l mu mv"># setting necessary parameters</span><span id="5cb5" class="mr lq hh mn b fi oh mt l mu mv">batch_size = 128<br/>num_classes = 10<br/>epochs = 20<br/>w_l2 = 1e-5</span></pre><p id="14d3" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">训练和测试形状；</p><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="e033" class="mr lq hh mn b fi ms mt l mu mv">img_rows, img_cols = 28, 28</span><span id="ed76" class="mr lq hh mn b fi oh mt l mu mv">if K.image_data_format() == 'channels_first' :<br/>         x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols)<br/>         x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols)<br/>         input_shape = (1, img_rows, img_cols)</span><span id="2a8f" class="mr lq hh mn b fi oh mt l mu mv">else:<br/>         x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)<br/>         x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)<br/>         input_shape = (img_rows, img_cols, 1)</span><span id="9fe5" class="mr lq hh mn b fi oh mt l mu mv">x_train = x_train.astype('float32')<br/>x_test = x_test.astype('float32')<br/>x_train /= 255<br/>x_test /= 255<br/>print(f"x_train shape : {x_train.shape}")<br/>print(x_train.shape[0], 'train samples')<br/>print(x_test.shape[0], 'test samples')</span><span id="4c3b" class="mr lq hh mn b fi oh mt l mu mv"># converting class vectors to binary class matrices</span><span id="a5bb" class="mr lq hh mn b fi oh mt l mu mv">y_train = keras.utils.to_categorical(y_train, num_classes)<br/>y_test = keras.utils.to_categorical(y_test, num_classes)</span></pre><p id="87e2" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es oi"><img src="../Images/f58b56b4c8e7be656a74399ded5cdbf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W4ntBykjDFZWVRQLjXOmRA.png"/></div></div></figure><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="ef63" class="mr lq hh mn b fi ms mt l mu mv"># building conventional neural networks</span><span id="9084" class="mr lq hh mn b fi oh mt l mu mv">from keras import regularizers<br/>model = Sequential()<br/>model.add(Conv2D(32, kernel_size=(3, 3), kernel_regularizer=regularizers.l2(w_l2),</span><span id="313a" class="mr lq hh mn b fi oh mt l mu mv">input_shape=input_shape))</span><span id="85ed" class="mr lq hh mn b fi oh mt l mu mv">model.add(BatchNormalization())</span><span id="fe4f" class="mr lq hh mn b fi oh mt l mu mv">model.add(Activation('relu'))</span><span id="e154" class="mr lq hh mn b fi oh mt l mu mv">model.add(Conv2D(64, (3, 3),  kernel_regularizer=regularizers.l2(w_l2)))</span><span id="564c" class="mr lq hh mn b fi oh mt l mu mv">model.add(BatchNormalization())</span><span id="66be" class="mr lq hh mn b fi oh mt l mu mv">model.add(Activation('relu'))</span><span id="057e" class="mr lq hh mn b fi oh mt l mu mv">model.add(MaxPooling2D(pool_size=(2, 2)))</span><span id="dbb3" class="mr lq hh mn b fi oh mt l mu mv">model.add(Dropout(0.25))</span><span id="7372" class="mr lq hh mn b fi oh mt l mu mv">model.add(Flatten())</span><span id="0671" class="mr lq hh mn b fi oh mt l mu mv">model.add(Dense(128, kernel_regularizer=regularizers.l2(w_l2)))</span><span id="1eb2" class="mr lq hh mn b fi oh mt l mu mv">model.add(BatchNormalization())</span><span id="eab2" class="mr lq hh mn b fi oh mt l mu mv">model.add(Activation('relu'))</span><span id="6681" class="mr lq hh mn b fi oh mt l mu mv">model.add(Dropout(0.5))</span><span id="6ec2" class="mr lq hh mn b fi oh mt l mu mv">model.add(Dense(num_classes, activation='softmax'))</span></pre><h2 id="dc56" class="mr lq hh bd lr nc ns ne lv nf nt nh lz jw nu nj mb ka nv nl md ke nw nn mf no bi translated">随机梯度下降；</h2><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="3dc9" class="mr lq hh mn b fi ms mt l mu mv">model.compile(loss = keras.losses.categorical_crossentropy, optimizer = keras.optimizers.SGD(), metrics = ['accuracy'])</span><span id="f7db" class="mr lq hh mn b fi oh mt l mu mv">model.summary()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es oj"><img src="../Images/2726b6e04ef6e6ea5cafa657d044cbce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vbu89TJeJB6Ppla4dqHVNQ.png"/></div></div></figure><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="06a9" class="mr lq hh mn b fi ms mt l mu mv"># training the model</span><span id="22b8" class="mr lq hh mn b fi oh mt l mu mv">hist_SGD = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, verbose = 1,</span><span id="dd20" class="mr lq hh mn b fi oh mt l mu mv">validation_data = (x_test, y_test), callbacks = [reduce_lr])</span><span id="b5e2" class="mr lq hh mn b fi oh mt l mu mv">score = model.evaluate(x_test, y_test, verbose = 0)</span><span id="f1a2" class="mr lq hh mn b fi oh mt l mu mv">print('Test loss: ', score[0])</span><span id="b48e" class="mr lq hh mn b fi oh mt l mu mv">print('Test accuracy: ', score[1])</span></pre><p id="26e6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">经过培训，我的价值观是:</p><p id="7255" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">测试损失:0。56860 . 68868686861</p><p id="6557" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">测试精度:0.98986868686</p><h2 id="05cc" class="mr lq hh bd lr nc ns ne lv nf nt nh lz jw nu nj mb ka nv nl md ke nw nn mf no bi translated">亚当</h2><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="45c8" class="mr lq hh mn b fi ms mt l mu mv">model.compile(loss = keras.losses.categorical_crossentropy, optimizer = keras.optimizers.Adam(), metrics = ['accuracy'])</span><span id="5eca" class="mr lq hh mn b fi oh mt l mu mv">model.summary()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ok"><img src="../Images/5e1694c3f6eaaebfc7b8ae4534665770.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*F4Nh3xBlReXluiTvPZlu1w.png"/></div></figure><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="f48f" class="mr lq hh mn b fi ms mt l mu mv"># training the model</span><span id="0995" class="mr lq hh mn b fi oh mt l mu mv">hist_adam = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, verbose = 1, validation_data = (x_test, y_test), callbacks = [reduce_lr])<br/>score = model.evaluate(x_test, y_test, verbose = 0)</span><span id="cb85" class="mr lq hh mn b fi oh mt l mu mv">print('Test loss:', score[0])<br/>print('Test accuracy:', score[1])</span></pre><p id="b8f2" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">训练完模型后:</p><p id="0ebd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">测试损失:0。56860 . 88888886861</p><p id="d7e6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">测试精度:0.99986868686</p><h2 id="9a5b" class="mr lq hh bd lr nc ns ne lv nf nt nh lz jw nu nj mb ka nv nl md ke nw nn mf no bi translated">RMSprop</h2><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="2a36" class="mr lq hh mn b fi ms mt l mu mv">model.compile(loss = keras.losses.categorical_crossentropy, optimizer = keras.optimizers.RMSprop(), metrics = ['accuracy'])</span><span id="d83e" class="mr lq hh mn b fi oh mt l mu mv">model.summary()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ol"><img src="../Images/fa7735ff162948426f74ad2101867c7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*vhf2_HIyBW_rw5fA8ZPoSw.png"/></div></figure><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="5545" class="mr lq hh mn b fi ms mt l mu mv">hist_RMSprob=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=[reduce_lr])</span><span id="a219" class="mr lq hh mn b fi oh mt l mu mv">score = model.evaluate(x_test, y_test, verbose=0)</span><span id="64d4" class="mr lq hh mn b fi oh mt l mu mv">print('Test loss:', score[0])</span><span id="b0ad" class="mr lq hh mn b fi oh mt l mu mv">print('Test accuracy:', score[1])</span></pre><p id="421f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">培训后:</p><p id="a0d6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">测试损失:0。56860 . 68868686861</p><p id="f4bc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">测试精度:0.9932000041007996</p><h2 id="34c5" class="mr lq hh bd lr nc ns ne lv nf nt nh lz jw nu nj mb ka nv nl md ke nw nn mf no bi translated">阿达格拉德</h2><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="5990" class="mr lq hh mn b fi ms mt l mu mv">model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adagrad(), metrics=['accuracy'])</span><span id="da48" class="mr lq hh mn b fi oh mt l mu mv">model.summary()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es om"><img src="../Images/6c4ac8446ef67571454284f87c705f85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*i5P9YP_aJgCMOC7NdDqvyg.png"/></div></div></figure><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="a9b4" class="mr lq hh mn b fi ms mt l mu mv">hist_adagrad=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=[reduce_lr])</span><span id="91e7" class="mr lq hh mn b fi oh mt l mu mv">score = model.evaluate(x_test, y_test, verbose=0)</span><span id="602e" class="mr lq hh mn b fi oh mt l mu mv">print('Test loss:', score[0])<br/>print('Test accuracy:', score[1])</span></pre><p id="a34c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">培训后:</p><p id="d88c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">测试损失:0。58660 . 68868686861</p><p id="0b88" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">测试精度:0.9932000041007996</p><h2 id="c3c2" class="mr lq hh bd lr nc ns ne lv nf nt nh lz jw nu nj mb ka nv nl md ke nw nn mf no bi translated">阿达德尔塔</h2><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="1463" class="mr lq hh mn b fi ms mt l mu mv">model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])</span><span id="2368" class="mr lq hh mn b fi oh mt l mu mv">model.summary()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es on"><img src="../Images/28213b80cb999eb5734b48a60bd23008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7lJbUt8hgW00QDCxQHAS0g.png"/></div></div></figure><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="0b4b" class="mr lq hh mn b fi ms mt l mu mv">hist_adadelta = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=[reduce_lr])</span><span id="d43e" class="mr lq hh mn b fi oh mt l mu mv">score = model.evaluate(x_test, y_test, verbose=0)</span><span id="eff8" class="mr lq hh mn b fi oh mt l mu mv">print('Test loss:', score[0])<br/>print('Test accuracy:', score[1])</span></pre><p id="b958" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">培训后:</p><p id="76da" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">测试损失:0。56860 . 88886888686</p><p id="f938" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">测试精度:0.9932000041007996</p><h2 id="22e3" class="mr lq hh bd lr nc ns ne lv nf nt nh lz jw nu nj mb ka nv nl md ke nw nn mf no bi translated">绘制结果</h2><blockquote class="nz oa ob"><p id="a59f" class="jn jo ky jp b jq jr ii js jt ju il jv oc jx jy jz od kb kc kd oe kf kg kh ki ha bi translated">我对下面的代码有一些问题。对于tensor flow 2+‘ACC’变成了‘accuracy’。这就是为什么原始代码对我不起作用。我把acc改成了accuracy。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es oo"><img src="../Images/0c0d5b6a6bcc21282b5660b5e6690707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WuTgVjzLQRfqFg3AONFIAQ.png"/></div></div></figure><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="9293" class="mr lq hh mn b fi ms mt l mu mv">hists = [hist_adam, hist_SGD, hist_RMSprob, hist_adadelta, hist_adagrad]</span><span id="e1b6" class="mr lq hh mn b fi oh mt l mu mv">plot_history(hists, attribute='accuracy', axis=(-1,21,0.965,1.0), loc='lower right')</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es op"><img src="../Images/be0b40b14fc9b5d3102c45b712249f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aYKxFVctNi63dTr-jYQFGA.png"/></div></div></figure><pre class="iy iz ja jb fd mm mn mo mp aw mq bi"><span id="d633" class="mr lq hh mn b fi ms mt l mu mv">plot_history(hists, attribute='loss', axis=(-1,21,0.009, 0.09), loc='lower right')</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es oq"><img src="../Images/02d4d1f12ab55283514f848428b512ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GIeodOteG5QfXmpb59gs2A.png"/></div></div></figure><p id="ac9c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">从上面的图中可以看出，ADAM优化比其他优化稍有不同。</p></div><div class="ab cl or os go ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="ha hb hc hd he"><h2 id="0fec" class="mr lq hh bd lr nc ns ne lv nf nt nh lz jw nu nj mb ka nv nl md ke nw nn mf no bi translated">你可以在我的<a class="ae iw" href="https://github.com/GuldenizBektas/Optimization-Algorithms" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到这篇文章的笔记本。</h2><h2 id="7733" class="mr lq hh bd lr nc ns ne lv nf nt nh lz jw nu nj mb ka nv nl md ke nw nn mf no bi translated">参考</h2><p id="de54" class="pw-post-body-paragraph jn jo hh jp b jq mh ii js jt mi il jv jw mj jy jz ka mk kc kd ke ml kg kh ki ha bi translated"><a class="ae iw" href="https://d2l.ai/chapter_optimization/optimization-intro.html#optimization-and-estimation" rel="noopener ugc nofollow" target="_blank">信号源1 </a></p><p id="1153" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae iw" href="https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html" rel="noopener ugc nofollow" target="_blank">信号源2 </a></p><p id="c78d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae iw" href="https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/" rel="noopener ugc nofollow" target="_blank">信号源3 </a></p><p id="cd01" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae iw" href="https://datascience-enthusiast.com/DL/Optimization_methods.html" rel="noopener ugc nofollow" target="_blank">信号源4 </a></p><p id="d365" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae iw" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/" rel="noopener ugc nofollow" target="_blank">信号源5 </a></p><p id="ae5a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae iw" href="https://ruder.io/optimizing-gradient-descent/index.html#adagrad" rel="noopener ugc nofollow" target="_blank">信号源6 </a></p><p id="a816" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae iw" href="https://github.com/ayyucekizrak/Keras_ile_Derin_Ogrenmeye_Giris/blob/master/B%C3%B6l%C3%BCm4/Optimizasyon_Y%C3%B6ntemlerinin_Kar%C5%9F%C4%B1la%C5%9Ft%C4%B1rmas%C4%B1.ipynb" rel="noopener ugc nofollow" target="_blank">来源7 </a></p></div></div>    
</body>
</html>