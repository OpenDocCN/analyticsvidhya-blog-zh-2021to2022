<html>
<head>
<title>Perceptron learning, from discrete to continuous — 02</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">感知器学习，从离散到连续— 02</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/perceptron-learning-from-discrete-to-continuous-02-b16ddf9e5ab6?source=collection_archive---------10-----------------------#2021-03-07">https://medium.com/analytics-vidhya/perceptron-learning-from-discrete-to-continuous-02-b16ddf9e5ab6?source=collection_archive---------10-----------------------#2021-03-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><h1 id="94cc" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">概述</h1><p id="62b1" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在<a class="ae ki" href="https://visual360.medium.com/perceptrons-as-linear-classifiers-01-fe60bebadcf0" rel="noopener">的上一篇文章</a>中，我们介绍了感知器的概念以及如何用它来建模线性分类器。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es kj"><img src="../Images/3d5eeaee93ac366b090016b8a404a5bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ANlT6ACKONa0i1J8.png"/></div></div></figure><p id="966f" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">一个感知器接受<em class="la"> n </em>个输入特征、<em class="la"> x、</em>并分别乘以相应的权重、<em class="la"> w、</em>加上一个偏差项，最后对结果应用一个激活函数并输出一个数字。之前，我们在试图找到一个模型的示例中使用阶跃函数作为激活，该模型将根据植物在阳光下度过的时间和给予的水量来分类植物是否会生长。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es lb"><img src="../Images/7acdecefd2f4421a05cca84a92f7bac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z66Vqbyx7yOBtsJNgj9aHg.png"/></div></div></figure><p id="c0c8" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">然而，仍有一个重大问题没有得到解答。</p><p id="31f1" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated"><em class="la">我们如何为我们的感知机找到最佳的权重和偏差？</em></p></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><h1 id="66fa" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">离散与连续</h1><p id="45a1" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">为了找到我们模型的最佳参数集，我们首先需要某种方法来量化以下哪个模型在分类数据点方面做得更好！</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es lc"><img src="../Images/9302336becd96c9d41566147f7c93a6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qZjbbQ7XpMXA4i4JLZ2iuA.png"/></div></div></figure><p id="8f8e" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">通过眼睛，我们可以明显地看出它是右边的模型，但是计算机怎么知道呢？这个游戏的名字是模型优化，我们首先找到一些方法来测量误差，然后将其最小化。</p><h2 id="8ceb" class="ld in hi bd io le lf lg is lh li lj iw jv lk ll ja jz lm ln je kd lo lp ji lq bi translated">朴素方法:感知器学习算法</h2><p id="1793" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">一个天真的想法是将误差设置为错误分类点的数量，然后尝试开发一种算法来最小化它。确实存在这样的算法，看起来是这样的。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es lr"><img src="../Images/27806a32fc84abd968543131622aba56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/0*KzdMgxcCqdQHfltJ.gif"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">来源:辉煌— <a class="ae ki" href="https://brilliant.org/practice/perceptron-learning-algorithm/?p=5" rel="noopener ugc nofollow" target="_blank">感知器学习算法pg5 </a></figcaption></figure><p id="48c4" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">下面是对它所做工作的高层次解释。</p><ul class=""><li id="b554" class="lw lx hi jm b jn kv jr kw jv ly jz lz kd ma kh mb mc md me bi translated">随机初始化权重和偏差(通过数据画一条随机线)。</li><li id="a78d" class="lw lx hi jm b jn mf jr mg jv mh jz mi kd mj kh mb mc md me bi translated">遍历所有数据点，并测试当前的参数集是否正确地对当前点进行了分类。</li><li id="e96d" class="lw lx hi jm b jn mf jr mg jv mh jz mi kd mj kh mb mc md me bi translated">如果点在当前模型下被错误分类，请按如下方式更新模型的权重和偏差:</li></ul><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es mk"><img src="../Images/7f43fc0537934686406a2386a6ae6298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rZvCtX3Z4wpcVYO4pM8hsw.png"/></div></div></figure><p id="e9fd" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">这里<em class="la"> w(k) </em>和<em class="la"> b(k) </em>分别代表<em class="la"> k </em>错误后的权重向量和偏差，<em class="la"> x(i) </em>和<em class="la"> y(i) </em>分别代表<em class="la">第I个</em>数据点(作为输入特征的向量)和标签(1或0 -记住阶跃函数激活)。</p><p id="7b5b" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">通过以这种方式更新权重和偏差，所发生的是我们增加了该点与决策边界的<strong class="jm hj"><em class="la"/></strong>有符号距离。其思想是，如果一个点与判定边界的距离为正，则它位于边界的右侧，因此我们希望为每个错误分类的点增加这个量。</p><p id="5fb6" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">我不会继续讨论这个算法，因为它是解决我们问题的一个相当幼稚的方法。对于那些想要更详细地了解如上所述更新权重和偏差到底如何等同于增加每个点与决策边界的有符号距离的人，请查看本文<a class="ae ki" href="https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975" rel="noopener" target="_blank">。</a></p><h2 id="6ce3" class="ld in hi bd io le lf lg is lh li lj iw jv lk ll ja jz lm ln je kd lo lp ji lq bi translated">离散优化的问题</h2><p id="8ea2" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">让我们继续，上面的方法有一些主要的问题。</p><p id="62f7" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">首先，这种方法特定于分类任务。假设我们有一个回归问题，我们想通过一些数据找到最佳拟合线，没有办法修改这个算法来返回最佳拟合线而不是决策边界。我们希望我们寻找最佳权重和偏差集的过程可以推广到任何最终目标是找到线性模型的任务。</p><p id="1d38" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">即使我们只关心分类任务，这个过程仍然不是理想的，因为它实际上没有找到最佳模型，它只是停留在它找到的第一个模型上，该模型将正确地分类所有点。因此，给定权重和偏差的随机初始化，理论上，该算法可以最终给出下面案例中的任何一条粉红色线作为最终模型。实际上，我们会说“最佳”答案看起来就像中间的绿线。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ml"><img src="../Images/85ed72b2456c42da9fd5390d03667beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4lsB_GWkNd7Anx1SSlx9Bw.png"/></div></div></figure><p id="b198" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">但是为什么<em class="la">确切地说</em>绿色的比粉色的好呢？毕竟，上面3行中的任何一行都会正确地对所有数据点进行分类。</p><p id="0892" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">为了理解为什么，我们需要从感知器的离散模型(其中每个点都被正确或错误地分类)转移到感知器的连续模型，该模型预测每个点落入特定类别的概率。</p><p id="25e1" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">为了进一步激发从离散模型向连续模型转变的需求，让我们考虑以下2个数据点示例:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es mm"><img src="../Images/90e6c39be204dabea69d846d92201fc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xD7kM9MnhbuNtVlHOPcAjg.png"/></div></div></figure><p id="0845" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">如果我们只是通过计算错误分类点的数量，上面的行是完全没问题的。</p><p id="a0c9" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">然而，从<em class="la">概率</em>的角度来考虑更为自然，其中远离决策边界线的点<em class="la">比靠近决策边界线的点</em>更有可能被正确分类。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es mn"><img src="../Images/d55c3df286f99cea1018cf289a04d6c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/0*WyJAiFXKyooWFH0r"/></div></figure><p id="f23b" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">在上面的图中，<em class="la"> y hat </em>表示成为某个特定类别的概率，我们用蓝色表示。我们可以看到，我们的决策边界是这样一条线，在这条线上有一半的点是蓝色的，现在我们可以更好地了解为什么这不好。看起来红色和蓝色的点几乎有相同的概率是蓝色的(它们或多或少都在决策边界上)！</p><p id="2b8f" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">如果我们现在尝试将我们的任务从尝试最大化正确分类点的数量改变为尝试最大化分类的<em class="la">确定性</em>，我们将得到一个更实际的模型。这是从离散误差函数到连续误差函数的转变。</p><p id="8825" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">如果我们有一个离散误差，就像计算正确分类的点数一样，一旦我们达到2(使用上面的例子)，我们就完成了。如果我们使用一个连续的误差，如概率，我们继续前进，即使这条线正确地分开了所有的点，直到我们找到一个决策边界，这是最确定的预测。</p><h2 id="a561" class="ld in hi bd io le lf lg is lh li lj iw jv lk ll ja jz lm ln je kd lo lp ji lq bi translated">连续感知器</h2><p id="a2b3" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">那么，我们需要对我们的感知器做什么改变才能得到概率输出，而不是二进制的0或1呢？我们需要应用的唯一改变是我们的激活函数。正是在这一点上，我们通过选择使用阶跃函数将离散化引入到我们的模型中，该函数只能取值0或1。</p><p id="2a8b" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">我们现在需要一个函数，它将给出一组连续的输出，但仍然限制在0和1之间(我们希望概率作为输出)。因此，我们将用sigmoid函数替换阶跃函数:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es mo"><img src="../Images/eacc87ac1ba6b3dfa093874a60529e6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UTPr-LEwFlSl5V79rXFWHA.png"/></div></div></figure><p id="130d" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">如您所见，sigmoid函数(顶部的激活函数)接受∞之间的任何值，并将其映射到0和1之间的值。这意味着我们的最终输出可以有效地代表概率。回到我们的植物例子，这意味着我们现在预测一个给定植物生长的概率，而不仅仅是分类它是否会生长！</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es mp"><img src="../Images/2c777d90a86b856c82e5325a6b4edd8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*ui-9irdqZC55tZKm5x1y3Q.png"/></div></figure><h1 id="1706" class="im in hi bd io ip mq ir is it mr iv iw ix ms iz ja jb mt jd je jf mu jh ji jj bi translated">测量概率误差</h1><p id="8883" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">好了，我们已经把激活函数改成了sigmoid，让我们的感知器输出一个连续的量(概率)。我们这样做是因为与使用离散感知器学习算法找到的模型相比，试图最大化确定性似乎会给出更可靠和直观的模型(也更好地概括)。</p><p id="bc40" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">也就是说，我们实际上仍然不知道如何为我们的模型找到最佳的权重和偏差。本质上，我们知道“最优”模型将通过最小化一些误差量来找到。我们现在添加的唯一信息是，我们想要测量某种概率误差。</p><p id="51d7" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">那么我们如何测量概率误差呢？让我们以两个数据点为例，尝试一些不同的想法。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es mv"><img src="../Images/14d7e6f5de18c7ae5b1644f238fe5b48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kCcR0S3vN0F-mSvU"/></div></div></figure><p id="c104" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">这里<em class="la"> y_hat </em>是一个数据点在<strong class="jm hj">蓝色</strong>类中的概率。因此，我们可以测量误差的一种方法是，将每个数据点与正确概率的“偏离”程度相乘:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es mw"><img src="../Images/afaa87d8fe6859ce149562a3139ebaa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/1*NxC18ZTn8tb06QO3M768mg.gif"/></div></figure><p id="c63e" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">这里，我们知道<strong class="jm hj">红色</strong>数据点为<strong class="jm hj">蓝色</strong>的概率应该是0，蓝色数据点为蓝色的概率应该是1。因此每个输出为0.8“关”。将它们相乘，我们会得到一个连续的量，随着我们更好地预测我们的数据点属于哪一类(颜色),这个量会减少。</p><p id="6fcd" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">听起来很完美，对吧？只有一个问题，那就是每个数据点的误差都在0和1之间。更具体地说，这与这样一个事实有关，我们拥有的数据点越多，0和1之间的数字就越多，我们必须相乘才能得到最终的误差。这可能<em class="la">非常</em>迅速导致我们得到绝对<em class="la">微小</em>的数字作为我们的最终误差。理论上，这不应该是一个问题，但是在计算上，处理微小的数字会导致不稳定的行为。因此，我们肯定希望避免以这种方式测量误差。</p><p id="ce99" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">另一种选择是将误差相加，而不是相乘。即:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es mx"><img src="../Images/6fbab5325a2512bc46ff418ec0a67d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/1*5qhGDdolqZvan4fPUnldgg.gif"/></div></figure><p id="c553" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">这避免了必须处理微小数字的问题，然而，由于另一个原因，这仍然不是理想的。也就是说，它没有因为错误分类而更严重地惩罚模型，而是因为增加了预测正确分类的数据点的信心而奖励了模型。</p><p id="1d61" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">下面是上述含义的一个示例，考虑一个三个数据点的示例，其中我们的模型预测如下:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es my"><img src="../Images/2799e27296052a5fa3d3bd608b541a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/0*SGbRh5ZftxLHKbvD"/></div></figure><p id="1115" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">这里的错误是:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es mz"><img src="../Images/84cc11ffb61723609e5fab85444fb99b.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/1*qVUS6mfQAJ3Z_UMusyRYCA.gif"/></div></figure><p id="3611" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">看看当我们的模型变得<em class="la">甚至</em> <em class="la">更</em>确定它做了正确的事情时会发生什么。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es my"><img src="../Images/ed35502eaf8a779bc9b6af56b001daf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/0*N0vr41H57zTSRgL8"/></div></figure><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es mz"><img src="../Images/99163ba67d05e321137ad2295ed50607.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/1*KQGUtH2bQmWd5WaMvBwLXQ.gif"/></div></figure><p id="23bb" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">误差下降了，即使模型仍然对一个点分类错误？！这个模型可以继续对其不正确的预测变得越来越有信心，仅仅是因为误差仍然会下降。这就是为什么简单地测量每个数据点的误差作为预测概率和真实标签之间的差异在实践中不起作用。我们需要更强有力地惩罚错误分类的东西，所以模型不能逃脱上述行为。</p><h2 id="5ad5" class="ld in hi bd io le lf lg is lh li lj iw jv lk ll ja jz lm ln je kd lo lp ji lq bi translated">交叉熵——概率误差的度量</h2><p id="5832" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">对于大多数测量概率误差的分类任务，使用二进制交叉熵作为误差函数。这是因为它具有加法属性，这意味着我们可以避免处理微小的数字，并且它还通过使用对数来更严重地惩罚错误分类，这使得它成为用于分类任务的理想错误。二元交叉熵的公式是这样的:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es na"><img src="../Images/5c4e5c963b8f2afc5e4b920838daf5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*kYDrj1kpaj1N-5YXpah4fA.png"/></div></figure><p id="ec8c" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">其中<em class="la"> N </em>是数据点的总数，<em class="la"> y_i </em>是每个数据点的标签(1或0)<em class="la">p(y _ I)</em>是该数据点在所选类别中的概率。</p><p id="2b15" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">让我们将这个错误应用到我们的3个数据点例子中，看看它是如何工作的。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es my"><img src="../Images/2799e27296052a5fa3d3bd608b541a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/0*SGbRh5ZftxLHKbvD"/></div></figure><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es nb"><img src="../Images/017eb0b91a554fb8a6c639f27eeb028f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/1*j9uYKkt5DZyZFtHwYhiQ2g.gif"/></div></figure><p id="a76b" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">现在让我们看看，如果我们的模型试图增加它对这是最佳决策边界的信心，会发生什么:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es my"><img src="../Images/29e8434067ff80ebb6c6bb02246b1c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/0*_uZjontPeFWxAJfk"/></div></figure><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es nb"><img src="../Images/f15a38e53cadb4c11706221cda37045a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/1*dsn_xxG7Y3lr7qQ1jBSUpg.gif"/></div></figure><p id="a847" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">错误上升，这是应该的！我们不希望模型变得更加确定它的分类，这种测量误差的方式确保我们因为错误分类而严重惩罚它。</p><p id="3d06" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">为了更直观地理解这是如何工作的，我推荐看一下这个视频为什么我们需要交叉熵损失。</p><h1 id="7a0b" class="im in hi bd io ip mq ir is it mr iv iw ix ms iz ja jb mt jd je jf mu jh ji jj bi translated">误差优化-梯度下降</h1><p id="cf86" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">好了，我们差不多要搞清楚如何找到我们的最佳权重和偏好了。现在我们已经有了误差的衡量标准，我们实际上该如何将误差最小化呢？</p><p id="7914" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">就像我们必须选择测量误差的方式一样，我们也需要选择优化误差的方式。在实践中，有几个优化器可供选择，但是为了更好地了解它们是如何工作的，我们将使用随机梯度下降这个“优化器的始祖”。一旦你理解了SGD(随机梯度下降)的工作原理，剩下的就容易学多了。我将从较高的层面概述它是如何工作的，并将细节留给以后的文章。</p><h2 id="7ebe" class="ld in hi bd io le lf lg is lh li lj iw jv lk ll ja jz lm ln je kd lo lp ji lq bi translated">向前传球</h2><p id="1e2b" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">让我们默认回到我们的2个数据点示例，但这次使用一些实际坐标(或输入要素的值)。我们初始化我们的权重和偏差，<em class="la">w0，w1</em>和<em class="la"> b，</em>为值1</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es my"><img src="../Images/b0af710f2fa649048de415b41c2181d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/0*LjmAmiGZoMA6rdcc"/></div></figure><p id="7d40" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">让我们获取第一个数据点，并在当前模型中运行它。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es nc"><img src="../Images/1d0b760f74240c65465715964ef94aae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mSX3-2LPg9Xv694V"/></div></div></figure><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es nd"><img src="../Images/b4819c08cd7dc65547218565289c5593.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/1*fbFNVYaq04kE2C6pFbBNPA.gif"/></div></figure><p id="2e73" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">所以对于我们的红色数据点，我们得到蓝色的概率是0.832。将蓝色类的标签设置为0，我们可以通过我们的误差/成本公式(用<em class="la"> C </em>表示)来传递它:(注意，这只是应用于一个数据点的二元交叉熵损失公式，而不是一整批):</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es ne"><img src="../Images/78d87ee81322f8aa21ad86c17d732508.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/1*ggfXtchIRgvudAvWx2koag.gif"/></div></figure><h2 id="46c4" class="ld in hi bd io le lf lg is lh li lj iw jv lk ll ja jz lm ln je kd lo lp ji lq bi translated">向后传球</h2><p id="fe43" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">我们现在计算以下梯度:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es nf"><img src="../Images/b306858cdd28cb29972903a436115ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/1*hCp3dQZVoN50K5qoqOFtYg.gif"/></div></figure><p id="bae6" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">这些梯度告诉我们在哪个方向轻推我们的权重和偏差，这样它们将降低特定数据点的误差(注意负号)。</p><p id="52eb" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">然后，我们向前传递第二个数据点，并计算这些梯度。</p><p id="e07e" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">最后，我们计算每个参数的梯度平均值，并按如下方式更新它们:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es ng"><img src="../Images/bd73b473ced7e54914c220af35d88fbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/1*tCIPy50B4fMdXn1-WY1HFg.gif"/></div></figure><p id="1e37" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">那个看起来有点像<em class="la"> n </em>的符号被称为<em class="la"> eta </em>，它只是一个常量，我们设置它是为了避免在迭代之间改变我们的参数太多，这确保了更平滑的训练过程。它被称为超参数，是我们将在项目系列中稍后讨论的内容。</p><p id="1933" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">更新参数后，我们再次通过模型向前运行数据，计算误差，然后平均梯度，最后再次更新参数。我们继续这样做，直到我们发现每次迭代后误差似乎没有减少多少。一旦我们到达这个阶段，我们就说这个模型已经被‘训练’过了，然后继续测试它。如果模型是好的，我们保留它，否则我们回到训练并改变超参数值，看看我们是否能得到更好的结果。</p><p id="7179" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">这就是我们如何找到最佳的权重和偏好。</p><p id="4791" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">请注意，根据我们对错误的定义，这个过程是完全可定制的。对于回归任务，我们可以将误差从二元交叉熵损失更改为均方误差损失，然后运行完全相同的过程。因为我们依赖于计算误差函数相对于参数的梯度，为了使这种方法有效，需要误差连续。为了得到连续的误差，我们需要感知器的连续输出。这就是为什么我们必须从离散到连续的变化。</p></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><h1 id="e3c7" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">摘要</h1><p id="34ed" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在这篇文章中，我们接触了很多不同的材料。如果你已经做到了以上几点，那么:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es nh"><img src="../Images/dab04d2dae86df5d1c2ec8387fb3d8d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/1*HYcroiM7cnxC8B1GMqE_4A.gif"/></div></figure><p id="da95" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">快速回顾一下，我们从回答这个问题开始，</p><p id="a8b5" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated"><em class="la">我们如何找到参数、权重和偏差的最佳值集？</em></p><p id="f3cb" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">然后我们了解到，我们首先需要找到一些方法来测量我们模型的误差，然后不得不担心找到一种方法来最小化它。</p><p id="3c6f" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">我们尝试了一种简单的方法，将误差设置为误分类点的数量，并使用感知器学习算法作为最小化该误差的方法。我们看到这种方法的缺点(不可靠，不太直观，不能很好地推广到其他任务)，并决定我们需要转向更连续的感知机模型。</p><p id="616a" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">然后，我们研究了如何修改感知器，让它输出概率。我们通过交换阶跃函数，并使用一个sigmoid作为我们的激活函数来实现这一点。</p><p id="d7b7" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">接下来，在确定二元交叉熵度量之前，我们概述了一些度量概率误差的方法。我们选择它是因为它的加法性质，允许我们避免处理微小的数字，它对对数的使用意味着我们对错误分类的严重惩罚。</p><p id="9355" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated">最后，我们看了梯度下降的高级概述，权重和偏差实际上是如何更新的，以减少给定数据集上的误差。</p><h1 id="ffbb" class="im in hi bd io ip mq ir is it mr iv iw ix ms iz ja jb mt jd je jf mu jh ji jj bi translated">后续步骤</h1><p id="87ac" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">接下来我们要处理的是<em class="la">非线性</em>决策边界。到目前为止，在我们处理的所有例子中，所有的数据都可以通过在数据集上画一条线来完全分离。当我们不能整齐地分离数据时会发生什么？这是将这些感知器组合起来形成神经网络的主要动机，这也是我们接下来要讨论的内容。</p><h2 id="0086" class="ld in hi bd io le lf lg is lh li lj iw jv lk ll ja jz lm ln je kd lo lp ji lq bi translated">问题</h2><p id="c0f3" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><em class="la">有哪些实际表达:</em></p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es nf"><img src="../Images/b306858cdd28cb29972903a436115ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/1*hCp3dQZVoN50K5qoqOFtYg.gif"/></div></figure><p id="0cf3" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated"><em class="la">(奖金，你能找出离散感知器学习算法中更新权重的方式与我们使用二进制交叉熵损失的梯度下降方式之间的相似性吗？</em></p><p id="5ed0" class="pw-post-body-paragraph jk jl hi jm b jn kv jp jq jr kw jt ju jv kx jx jy jz ky kb kc kd kz kf kg kh hb bi translated"><em class="la">为什么梯度下降需要连续误差函数？如果当错误仅仅是错误分类的点的数量时，我们试图使用它会发生什么？</em></p></div></div>    
</body>
</html>