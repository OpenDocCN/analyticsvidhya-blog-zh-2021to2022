<html>
<head>
<title>The journey of Gradient Descent — From Local to Global</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降之旅——从局部到全球</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/journey-of-gradient-descent-from-local-to-global-c851eba3d367?source=collection_archive---------2-----------------------#2021-02-25">https://medium.com/analytics-vidhya/journey-of-gradient-descent-from-local-to-global-c851eba3d367?source=collection_archive---------2-----------------------#2021-02-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/ae385a6fe9c287b1191b2330c7dedcc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZC9qItK9wI0F6BwSVYMQGg.png"/></div></div></figure><p id="9039" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在我的上一篇文章中，我介绍了梯度下降的直觉，以及它背后的数学实现，以达到成本函数的最小值。这看起来就像走下山一样简单(是的，这就是梯度下降)。</p><p id="89c5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后一篇文章以这样一句话结束:<em class="jn">有时，可能会发生这样的情况，成本函数的值不是达到全局最小值，而是停留在局部最小值或鞍点。</em></p><blockquote class="jo jp jq"><p id="2aad" class="ip iq jn ir b is it iu iv iw ix iy iz jr jb jc jd js jf jg jh jt jj jk jl jm ha bi translated">我还想给你看一些可能会破坏你热情的东西。</p></blockquote><p id="4804" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在深度学习中，有不同类型的成本函数，它们具有除此之外的不同形状。因此，在处理不同的成本函数时，这种简单形式的成本函数并不存在。</p><figure class="jv jw jx jy fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ju"><img src="../Images/abd54414a35ff8f8d97f04f4c6791a81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XrNHta2Pxb77WT7MdPdySw.png"/></div></div></figure><p id="6a85" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">相反，它大多有许多凸面，因此看起来有点像这样:</p><figure class="jv jw jx jy fd ii er es paragraph-image"><div class="er es jz"><img src="../Images/66717a24a9a5db4b04205536284fd2e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*VrIZ2ZutOIj0srw3Fm7onw.png"/></div></figure><p id="5e52" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如图所示，成本函数实际上有很多局部最小值，我们需要达到全局最小值。</p><h1 id="9a0a" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">让我们开始吧。</h1><p id="35b3" class="pw-post-body-paragraph ip iq hh ir b is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji lc jk jl jm ha bi translated">在训练时，成本函数达到最小值是很重要的，该最小值是给出接近精确结果的全局最小值(成本函数的最低值)。但是，我们可能会陷入局部最小值(所有邻近值中的最低值)或鞍点。由于不是真正的最小值，结果可能达不到预期。</p><figure class="jv jw jx jy fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ld"><img src="../Images/a9f01ca6c18d5f55a338f1ed8dfb621f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v0qCXQWSPID6q1O3K8cLrQ.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">局部最小值和全局最小值</figcaption></figure><figure class="jv jw jx jy fd ii er es paragraph-image"><div class="er es li"><img src="../Images/b8e97820fcdf75ec721fb247162ef794.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*nmrpFUAfhnBbf3xkaVo4lg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">鞍点</figcaption></figure><p id="c9c3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们不能完全避免陷入局部最小值或鞍点，但我们仍然可以使用一些技术来帮助缓解这个问题。</p><h1 id="c61e" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">随机梯度下降(SGD)和小批量SGD</h1><p id="aaea" class="pw-post-body-paragraph ip iq hh ir b is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji lc jk jl jm ha bi translated">SGD和小批量SGD是梯度下降(GD)的变形版本。考虑一个具有<em class="jn"> n </em>个示例(<em class="jn"> n &gt; 10 </em>个⁶).的数据集在GD中，我们考虑所有的<em class="jn"> n </em>个点来降低代价函数，这使得该算法使用高计算能力。而在SGD中，在每个历元中仅使用单个(<em class="jn"> n=1) </em>数据点，并且相应地更新参数。在小批量SGD中，我们从样本中取出<em class="jn"> k </em>个数据点(<em class="jn"> k &lt; n </em>)并更新我们的参数。这些选择样本<em class="jn">小于n </em>的方法使得算法计算友好。</p><p id="3675" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在梯度下降中，代价函数的值逐渐减小，这增加了遇到局部极小值的机会，并且使得不可能走出那个点。而在SGD中，变化是突然的。因此，它降低了陷入局部最小值的概率，即使它陷入了，由于它的颠簸运动，它也有可能肯定会出来。</p><figure class="jv jw jx jy fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lj"><img src="../Images/9b746f2d64059a50d09860f33036daf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a8bX5jq-VVy1nDtIHY5Row.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">SGD与梯度下降</figcaption></figure><h1 id="0e8c" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">正规化</h1><p id="a6ed" class="pw-post-body-paragraph ip iq hh ir b is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji lc jk jl jm ha bi translated">当成本函数达到局部最小值时，更新参数的过程试图得出结论，但是我们需要防止这种情况。在这种情况下，我们惩罚大的重量。在局部最小值处，即使导数为零，由于添加了正则项，权重也保持更新。有两种类型的正则L1(套索)和L2(山脊)。</p><h2 id="d10d" class="lk kb hh bd kc ll lm ln kg lo lp lq kk ja lr ls ko je lt lu ks ji lv lw kw lx bi translated">L2正则化(岭)</h2><p id="93c9" class="pw-post-body-paragraph ip iq hh ir b is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji lc jk jl jm ha bi translated">L2正则化中的成本函数的等式为:</p><figure class="jv jw jx jy fd ii er es paragraph-image"><div class="er es ly"><img src="../Images/3d3e0e93020c4081da3c45689c0116e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/1*em7dl6hbXjJnBaOXNFmnmg.gif"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">L2正则化(岭)</figcaption></figure><h2 id="ba55" class="lk kb hh bd kc ll lm ln kg lo lp lq kk ja lr ls ko je lt lu ks ji lv lw kw lx bi translated">L1正则化</h2><p id="1f53" class="pw-post-body-paragraph ip iq hh ir b is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji lc jk jl jm ha bi translated">L1正则化中的成本函数的等式是:</p><figure class="jv jw jx jy fd ii er es paragraph-image"><div class="er es lz"><img src="../Images/056359862707fe4458008243ebd1eec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/1*UHfwmCohFSiANoGFxIbUgg.gif"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">L1正则化</figcaption></figure><p id="3e7c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，当J(Y’，Y)的导数在局部最小值处为零时，正则项将惩罚成本函数，因此即使参数在局部最小值处也会被更新，而在全局最小值处，更新不会发生，因此我们可以使用<em class="jn">正则化</em>避免陷入局部最小值。</p><h1 id="d254" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">动力</h1><p id="2ba5" class="pw-post-body-paragraph ip iq hh ir b is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji lc jk jl jm ha bi translated">动量只是将先前更新的权重的一部分添加到当前权重。</p><p id="3a71" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们可以将权重更新等式写成如下:</p><figure class="jv jw jx jy fd ii er es paragraph-image"><div class="er es ma"><img src="../Images/f8ff3401d2a9079639a9ad1bcf3c57b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/1*LVOrHmrrG569qqbDBm-8dQ.gif"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">权重更新方程</figcaption></figure><p id="b55f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里β是动量因子，它<em class="jn">的范围在0和1之间(不包括0和1)</em>它也是一个可以调节的超参数。</p><p id="d42a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我在YouTube上发现了一个很好的势头可视化，由<a class="ae mb" href="https://www.youtube.com/user/diegodci" rel="noopener ugc nofollow" target="_blank"> <em class="jn">迭戈·伊纳西奥</em> </a> <a class="ae mb" href="https://www.youtube.com/channel/UChO9ZxMm1tCBqk8mrxPq-nA" rel="noopener ugc nofollow" target="_blank"> <em class="jn">，</em> </a> <em class="jn">做观看。</em></p><figure class="jv jw jx jy fd ii"><div class="bz dy l di"><div class="mc md l"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">动量的可视化</figcaption></figure><h1 id="56d8" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">改变学习率— <em class="me"> α </em></h1><p id="5c5e" class="pw-post-body-paragraph ip iq hh ir b is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji lc jk jl jm ha bi translated">我们可以使用<em class="jn">学习率</em>随着时间不断变化的值，而不是在整个训练过程中使用恒定的学习率。</p><p id="5454" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">改变学习率的方程式可以用简单的形式表示</p><figure class="jv jw jx jy fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/ddb896e2043a16d8a3a7b31bb51ef8c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/1*BUEvENIR2d7Y_uKt157wtQ.gif"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">学习速率退火方程</figcaption></figure><p id="dfcf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里T和<em class="jn"> α_o </em>是可以调整的超参数。这里<em class="jn"> t </em>可以从<em class="jn"> 0到T </em>变化，因此<em class="jn"> α </em>与<em class="jn"> t成反比关系。这里</em>的<em class="jn"> α </em>变化，直到<em class="jn"> t </em>碰到<em class="jn"> T </em>，在此期间，模型被称为处于<em class="jn">搜索阶段</em>，因此，学习速率随着时间而降低，因此可以使沿着误差表面的移动更加平滑。</p><h1 id="6504" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">清盘</h1><p id="e838" class="pw-post-body-paragraph ip iq hh ir b is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji lc jk jl jm ha bi translated">在这里，我们看到标准梯度下降可以在某种程度上进行优化，以便我们可以使用各种技术解决陷入局部最小值的问题，每种技术都增加了一个超参数供我们调整。然而，有许多算法，如<em class="jn"> Adam、Adagrad和Adadelta </em>，它们往往工作得更快，但也更难实现。</p><p id="d0ca" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是不同算法收敛速度的一个例子。</p><figure class="jv jw jx jy fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/5cf7d945005d761b4375572d0ca39e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*fIf-_BwwGB-k0doHrdf5oQ.gif"/></div></figure><p id="bede" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们到此结束。</p><p id="8808" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下一个故事再见。</p><p id="4251" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">谢了。</p></div></div>    
</body>
</html>