<html>
<head>
<title>Apache Beam- an easy guide.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">阿帕奇光束-一个简单的指南。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/apache-beam-an-easy-guide-172304900014?source=collection_archive---------12-----------------------#2021-03-10">https://medium.com/analytics-vidhya/apache-beam-an-easy-guide-172304900014?source=collection_archive---------12-----------------------#2021-03-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="a150" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如今，大数据框架风靡一时。理由很充分:需要处理和分析的数据量是巨大的。以至于我们经常无法在个人电脑上处理一些数据集。</p><p id="34e9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是大数据的用武之地。这个想法是使用多台计算机来处理给定的数据集。这通常需要在AWS或Azure这样的云服务中运行大量虚拟机，然后使用Spark或Dask这样的特定框架来解决数据问题。通过这种方式，我们可以利用几十台，有时甚至几百或几千台单独的计算机(或“节点”)来处理我们的数据。</p><p id="245f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么，如何以这种方式使用多台机器呢？正如我提到的，通常这需要使用特定的框架。其中最著名的是MapReduce和Spark。这些框架不同于使用传统python库进行分析或ML。它们几乎是不同的语言，或者至少是我们通常使用的语言的扩展。它们的工作方式也不同，我们必须适应它们，让它们发挥作用。然后，是将我们的代码部署到实际集群的不那么容易的步骤。</p><p id="c822" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最近，在我实习的时候，我试图分析一个相对较大的新闻文章数据集。我发现试图在这些上运行简单的矢量化会使我的colab实例崩溃。我意识到是时候升级游戏，进入大数据世界了。</p><p id="db37" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先我用Spark做了实验。后来我听说了一个叫Dask的东西。最后有人推荐Apche Beam。Apache Beam的优势在于，当与Google的数据流结合使用时，它几乎完全消除了集群设置方面的问题。几乎完全。我们仍然需要传递一些配置，但是我们不需要弄乱Docker映像和手动设置worker节点等。</p><p id="b0b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不幸的是，关于Beam的文档很少，而且很难找到存在的文档。因此，在让Beam为自己工作后，我想我应该写一篇帖子来帮助那些可能发现自己和我走在同一条路上的人。</p><p id="cb4e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">光束有两个主要部分:p集合和p转换。Pcollections是Beam使用的数据结构，PTransforms是应用于这些PCollections的转换。在一个典型的Beam脚本中，我们将数据读入一个PCollection，对该数据进行转换，然后将该应用程序的输出写入另一个PCollection。</p><p id="b16b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看一些示例代码:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/8dd931fb121a4180d6a32ab335499786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6LVE_kMhczjHsuiwgf5TQA.png"/></div></div></figure><p id="9f5d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们从导入光束模块开始。然后，我们从该模块实例化一个管道对象(参见以“with beam”开头的行。管道”)。这个管道对象是我们构建希望在集群上运行的实际管道的地方。您可能会看到，管道中的主要操作符是“|”操作符。这个操作符基本上是告诉Beam将左边的输入输入到右边的流程中。这个过程的结果存储在变量中。</p><p id="23b3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，在上面的代码中，在管道的第一行(即在beam之后……我们看到行data = p | beam . io . parquetio . ReadFromParquet。这里，我们获取pipline对象本身并将其输入ReadFromParquet转换。这个转换读取一个parquest文件，并将其转换成一个PCollection。pipline的第一步通常包括以这种方式将pipline对象输入到转换中。此转换生成的Pcollection存储为“数据”。</p><p id="09a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下一行中，我们将这个“数据”变量通过管道传递给另一个转换，特别是ParDo转换。这是光束独有的一种特殊类型的变换。尽管beam有许多固有的变换，如sum、grou等，ParDo变换是一种灵活的方式，可以对数据执行自定义变换。ParDo会将指定的函数应用到Pcollection的每一行。</p><p id="6128" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意，我们向ParDo传递了一个名为WordExtractingDFn的东西。这是我们为Beam构建的自定义函数。实际上它是一个物体。Beam中自定义函数的工作方式是，我们必须扩展DoFn类，并在这个扩展中定义一个名为“process”的方法，这就是我们函数逻辑的编写位置。这里有一个完整的例子:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jo"><img src="../Images/9dffd0ad11b4fdab55b8cfc013e95c34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tUdxDCOywrythmOYvgs57g.png"/></div></div></figure><p id="a99e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这里，您可以看到实践中的方法。我们定义了一个名为WordExtractingDoFn的新类，它扩展了beam。DoFn级，内置于Beam中。然后，我们定义我们的“process”方法，该方法具有我们希望应用于当前行的逻辑，该逻辑作为“element”传递到方法中。在这种情况下，我们使用tokenize函数(在别处定义——只要它在process方法中被调用，就没问题。只有这样，它才会被光束实际应用)。我们生成令牌，然后从这些令牌生成各种ngrams，然后我们做一些自定义的术语计数和评分。我们向行中添加新的“列”来保存我们的输出(参见“element['hates_score']等)。然后我们返回新行(作为元素),然后将该行添加到输出Pcollection中。</p><p id="ac89" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在Beam中要考虑的一个重要概念是“模式”。模式基本上是我们的Pcollection中的列名。通常，我们必须手动指定这些模式。这里的情况不是这样，因为当您读取一个Parquet文件时，Beam会推断出一个模式。然而，CSV文件不是这种情况，如果希望引用传递到DoFn方法的行中的特定字段(即列),则必须指定模式。以下是如何创建模式的示例:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jp"><img src="../Images/6b53450a19316663d8acc0acd6847415.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*gSFioBYdf9vpyyuO0okRgA.png"/></div></figure><p id="cce6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意，模式是使用pyarrow库的schema方法构建的。</p><p id="0f67" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是如何开始使用Apache Beam的快速入门。请继续关注下一篇文章，我将带您完成将管道部署到云的过程！</p></div></div>    
</body>
</html>