<html>
<head>
<title>How to Train Ms-Pacman with Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用强化学习训练Ms-Pacman</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-train-ms-pacman-with-reinforcement-learning-dea714a2365e?source=collection_archive---------7-----------------------#2021-05-27">https://medium.com/analytics-vidhya/how-to-train-ms-pacman-with-reinforcement-learning-dea714a2365e?source=collection_archive---------7-----------------------#2021-05-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="eb29" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">创建一个DQN代理来玩雅达利游戏</h2></div><p id="b009" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">作者:耶稣·埃内斯托·佩斯凯拉·瓦斯奎兹，马科斯·亚伯拉罕·桑切斯·加林多，何塞·阿尔贝托·莱瓦·孔特雷拉斯。</p><p id="4028" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae jt" href="https://cc.unison.mx/" rel="noopener ugc nofollow" target="_blank">索诺拉大学，计算机科学学士</a>，埃莫西约索诺拉。</p><h1 id="00fe" class="ju jv hh bd jw jx jy jz ka kb kc kd ke in kf io kg iq kh ir ki it kj iu kk kl bi translated">摘要</h1><p id="6f96" class="pw-post-body-paragraph iw ix hh iy b iz km ii jb jc kn il je jf ko jh ji jj kp jl jm jn kq jp jq jr ha bi translated">Pacman女士是Atari 2600的标志性游戏。随着OpenAI的Gym的创建，一个用于强化学习算法的工具包提供了为许多游戏创建代理的能力。在这个项目中，我们为Pacman女士创建了一个环境，并使用DQN网络对其进行了训练。</p><p id="2839" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> Github资源库</strong></p><div class="kr ks ez fb kt ku"><a href="https://github.com/StarVeteran/Ms-Pacman-DQN" rel="noopener  ugc nofollow" target="_blank"><div class="kv ab dw"><div class="kw ab kx cl cj ky"><h2 class="bd hi fi z dy kz ea eb la ed ef hg bi translated">星际老兵/Ms-Pacman-DQN</h2><div class="lb l"><h3 class="bd b fi z dy kz ea eb la ed ef dx translated">深度Q学习工具。为难民做准备。DQN原始仓库(…</h3></div><div class="lc l"><p class="bd b fp z dy kz ea eb la ed ef dx translated">github.com</p></div></div><div class="ld l"><div class="le l lf lg lh ld li lj ku"/></div></div></a></div><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lk"><img src="../Images/8f29846aab4698135c9e6d46e4a443e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kNLQwwJkIt6c_JEG"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">照片由<a class="ae jt" href="https://unsplash.com/@sharkovski?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Kirill Sharkovski </a>在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h2 id="dfb2" class="lz jv hh bd jw ma mb mc ka md me mf ke jf mg mh kg jj mi mj ki jn mk ml kk mm bi translated">介绍</h2><p id="4b80" class="pw-post-body-paragraph iw ix hh iy b iz km ii jb jc kn il je jf ko jh ji jj kp jl jm jn kq jp jq jr ha bi translated">Atari 2600是一款于1977年推出的视频游戏机，它是第一款凭借大量标志性游戏在游戏市场取得巨大成功的游戏机。Pacman女士于1981年推出，是一款街机游戏，是原始Pacman游戏的“未经授权”的续集。随着巨大的成功，Namco将其授权为官方名称。游戏本身和吃豆人游戏很像。不同的是，在这种情况下，主角是一个女人，它还开发了游戏和新的迷宫设计。</p><p id="7b1e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">《吃豆人女士》中的玩法和原版吃豆人游戏类似。玩家通过吃小球和躲避鬼魂获得点数(如果吃豆小姐碰到了鬼魂，它将失去一条命)。迷宫里还有更大的球，那就是<em class="js">助推器</em>(动力球)，让鬼魂蓝一段时间，让它们被吃掉以获得加分。</p><p id="46f3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2013年，DeepMind发布了第一个版本的Deep Q-Network (DQN)，这是一个计算机程序，能够在Atari 2600的许多经典游戏上拥有人类水平(甚至更高)的性能。在复杂的强化学习环境中，使用卷积神经网络可用于从原始视频数据中学习成功的控制策略。与人类学习一样，该算法也基于屏幕视觉。从零开始，你会发现让你达到(在很多情况下，超过)人类基准的游戏策略。</p><h2 id="015f" class="lz jv hh bd jw ma mb mc ka md me mf ke jf mg mh kg jj mi mj ki jn mk ml kk mm bi translated">开放环境Ms-Pacman-v0</h2><p id="d638" class="pw-post-body-paragraph iw ix hh iy b iz km ii jb jc kn il je jf ko jh ji jj kp jl jm jn kq jp jq jr ha bi translated">OpenAI MsPacman环境为RL研究人员提供了一个简单的界面，以模拟经典的MsPacman ATARI游戏来训练RL代理人。MsPacman环境使用暴露开发者的210 x 160三通道RGB图像来表示游戏的状态。它还提供了MsPacman在任何给定帧中剩余的生命数以及MsPacman在游戏过程中积累的游戏内奖励金额的信息。在给定的帧中，MsPacman有机会执行9种可能的操作之一，包括向左移动、向右移动、向上移动、向下移动、对角移动或停留在前一帧中的位置。。每个动作在k帧的持续时间内重复执行，其中k可以从2到4。</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es mn"><img src="../Images/27af48071378e57a180a863fb60d677f.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*MQs3G2uOjetks4E9ptOldw.jpeg"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">由<a class="ae jt" href="https://www.flickr.com/" rel="noopener ugc nofollow" target="_blank"> Flickr上的<a class="ae jt" href="https://www.flickr.com/photos/158561476@N06" rel="noopener ugc nofollow" target="_blank"> SteamXO </a>拍摄的照片</a></figcaption></figure><h2 id="d4df" class="lz jv hh bd jw ma mb mc ka md me mf ke jf mg mh kg jj mi mj ki jn mk ml kk mm bi translated">卷积神经网络(CNN)如何工作</h2><p id="ba73" class="pw-post-body-paragraph iw ix hh iy b iz km ii jb jc kn il je jf ko jh ji jj kp jl jm jn kq jp jq jr ha bi translated">卷积网络用于图像识别，通常使用三维张量作为输入。这些图像的高度和宽度是二维的，第三维属于RGB颜色。</p><p id="6412" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">基本上，你看到的每一种颜色都是由红、绿、蓝三色组合而成的。这些输入被称为通道。</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es mo"><img src="../Images/d8fca6307c081206829c59ed83b7ae11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*9JFEX-vG4tBktAzAbkbsdA.png"/></div></figure><p id="56f0" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">图像通过一个内核进行变换，这个内核是一个滤波器。如果您有一个RGB图像，那么每个滤镜都应用于图像中的每个颜色通道。内核遍历通道并计算相应值的点积。这是对所有像素进行的，其中一些像素同时移动。</p><p id="a1b4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一旦输入被处理，它们通常伴随着最大池层。它也有一个核大小，但它不是取核点积，而是返回核窗口的最大值，从而用几个扫描子集的最大值创建另一个新矩阵。</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es mo"><img src="../Images/bb9e96069df0e1b0b7b5ae8d438d5904.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*vsiWtGO-ooHZBqq4qplkFA.png"/></div></figure><h2 id="38dd" class="lz jv hh bd jw ma mb mc ka md me mf ke jf mg mh kg jj mi mj ki jn mk ml kk mm bi translated">深度Q网络</h2><p id="9449" class="pw-post-body-paragraph iw ix hh iy b iz km ii jb jc kn il je jf ko jh ji jj kp jl jm jn kq jp jq jr ha bi translated">DQN，或深度Q网络，用神经网络逼近Q学习框架中的状态值函数。在Atari游戏中，它将游戏的多帧作为输入状态值，将动作作为输出状态值。</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mp"><img src="../Images/b0e0b11af065b75da192dc22042fde61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zz2ANs8ZBKYBpfhw"/></div></div></figure><p id="f7df" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一般来说，它使用一个<em class="js">体验回放</em>将每集的步骤存储在内存中以供学习，其中许多记忆样本是从随机重复中抽取的。它存储代理观察到的转换，允许我们以后重用这些数据。Q-网络针对目标网络进行优化，该目标网络针对每<em class="js"> k </em>步使用最新权重进行周期性更新。通过避免移动目标的短期振荡，这使得训练更加稳定。第一个解决了在线学习中可能出现的自相关问题，重复记忆使这个问题更像是一个监督学习问题。</p><p id="d95d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当代理观察到环境的当前状态并选择一个动作时，环境转换到一个新的状态并返回一个指示该动作结果的奖励。</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es mq"><img src="../Images/ff5c80b0363edb098e758ca419043a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/0*bki-mg_nEIBZznIy"/></div></figure><p id="63ab" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上图展示了DQN的基本架构。正如你所看到的，首先一个代表游戏画面的原始像素向量被输入到一个前馈卷积神经网络。这个CNN用来从图像中提取抽象但重要的特征，提供一个更细致入微的游戏状态画面。在馈入一系列卷积和分组层之后，得到的张量馈入一系列完全连接的层，这些层连接到我们的最终输出层，在这种情况下，输出层包含9个单元，每个单元的值表示给定输入状态和网络的当前权重时9个动作中每个动作的Q值。Q值可以通过网络简单的向前一步来计算。</p><h2 id="6275" class="lz jv hh bd jw ma mb mc ka md me mf ke jf mg mh kg jj mi mj ki jn mk ml kk mm bi translated">DQN算法</h2><p id="a63b" class="pw-post-body-paragraph iw ix hh iy b iz km ii jb jc kn il je jf ko jh ji jj kp jl jm jn kq jp jq jr ha bi translated">Q-Learning的主要思想是如果我们有一个函数</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es mr"><img src="../Images/655b21f7a6b600ca944d72db0362d48e.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*aWwsTabkem9QnCsfS8m0Kg.png"/></div></figure><p id="ba0f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这可以告诉我们我们的回报是什么，如果我们在一个给定的州采取任何行动，那么我们可以很容易地建立一个政策，使我们的回报最大化:</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es ms"><img src="../Images/9de227ea8ea72827bd6506d1ebd1a89b.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*cksJaEuO7LaJ5RvKtI0yDg.png"/></div></figure><p id="8bbd" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">但是我们从零开始，所以我们还没有访问Q*的权限。然而，神经网络可以逼近值，所以我们可以简单地创建一个并训练它类似于Q*。</p><p id="fe8b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于训练更新规则，每个Q函数都服从贝尔曼方程:</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es mt"><img src="../Images/ad8a34a96cb7c2d5cf0f0e7522e295e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*z8iQDb5FMqvjSRFMoEW4jQ.png"/></div></figure><p id="98bb" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，Q-网络是一种神经网络，其权重由θ表示，θ作为非线性函数逼近器来逼近Q#。通过调整θ来训练Q网络，以便最小化在反向传播的每次迭代中改变的一系列损失函数。</p><h2 id="ff18" class="lz jv hh bd jw ma mb mc ka md me mf ke jf mg mh kg jj mi mj ki jn mk ml kk mm bi translated">DQN代理人看到了什么？</h2><p id="cea6" class="pw-post-body-paragraph iw ix hh iy b iz km ii jb jc kn il je jf ko jh ji jj kp jl jm jn kq jp jq jr ha bi translated">玩Atari的dqn在卷积神经网络(CNN)的帮助下利用屏幕，这是一系列卷积层，它们学会从像素值的混乱中检测相关的游戏信息。我们的模型考虑了当前和先前屏幕补丁之间的差异。在给定当前输入的情况下，网络试图预测采取每个行动的<em class="js">预期回报</em>。</p><h1 id="c58b" class="ju jv hh bd jw jx jy jz ka kb kc kd ke in kf io kg iq kh ir ki it kj iu kk kl bi translated">我们在DQN的代理人Pacman女士的结果</h1><figure class="ll lm ln lo fd lp"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="14f7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们在谷歌实验室培训我们的DQN代理。培训进行了50集，以了解我们的Pacman女士如何发挥，结果并不像我们最初想象的那样，在前25集，它没有给出我们最期望的分数，它收集了大约600分。大约在第35-40集时，分数显著增加，在作为示例显示的视频中，分数为1350是最好的结果，虽然不多，但这是一个令人满意的进步，50集的训练时间约为3小时。</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mw"><img src="../Images/723a7d8e9bdb3483c3a54a872692f70b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9mxvwDCfH3BUJik6"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">DQN代理与文献中最佳强化学习方法的比较。</figcaption></figure><p id="5da3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">看看这个用具有卷积神经网络的DQN训练的Atari游戏的对比图，可以看出Pac-man女士在训练时的可玩性水平方面是表现最差的。我们很可能会达到超人的游戏水平，我们将不得不训练我们的模型更长的时间，这将需要更多的计算能力或更多的时间来观察我们的Pacman女士如何清除迷宫的明显改善。</p><h1 id="fcb6" class="ju jv hh bd jw jx jy jz ka kb kc kd ke in kf io kg iq kh ir ki it kj iu kk kl bi translated">结论</h1><p id="0ad5" class="pw-post-body-paragraph iw ix hh iy b iz km ii jb jc kn il je jf ko jh ji jj kp jl jm jn kq jp jq jr ha bi translated">尽管我们的神经网络训练没有获得最佳结果，但我们已经了解了这种类型的算法和CNN是如何工作的，我们不仅看到了网络的结构是如何发现的，DQN算法是如何工作的，而且还看到了它是如何工作的。在行动中看到接受训练的代理在使用该算法进行训练时的行为</p><h1 id="6780" class="ju jv hh bd jw jx jy jz ka kb kc kd ke in kf io kg iq kh ir ki it kj iu kk kl bi translated">参考</h1><div class="kr ks ez fb kt ku"><a href="https://www.datahubbs.com/deepmind-dqn/" rel="noopener  ugc nofollow" target="_blank"><div class="kv ab dw"><div class="kw ab kx cl cj ky"><h2 class="bd hi fi z dy kz ea eb la ed ef hg bi translated">DQN与CNN:重现谷歌深度思维网络</h2><div class="lb l"><h3 class="bd b fi z dy kz ea eb la ed ef dx translated">从颜色中学习Q值通常，我在这个博客上不怎么从像素或图像中学习RL，因为1)我的研究…</h3></div><div class="lc l"><p class="bd b fp z dy kz ea eb la ed ef dx translated">www.datahubbs.com</p></div></div><div class="ld l"><div class="mx l lf lg lh ld li lj ku"/></div></div></a></div><div class="kr ks ez fb kt ku"><a href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html" rel="noopener  ugc nofollow" target="_blank"><div class="kv ab dw"><div class="kw ab kx cl cj ky"><h2 class="bd hi fi z dy kz ea eb la ed ef hg bi translated">强化学习(DQN)教程- PyTorch教程1.8.1+cu102文档</h2><div class="lb l"><h3 class="bd b fi z dy kz ea eb la ed ef dx translated">作者:Adam Paszke这个教程展示了如何使用PyTorch来训练一个深度Q学习(DQN)代理</h3></div><div class="lc l"><p class="bd b fp z dy kz ea eb la ed ef dx translated">pytorch.org</p></div></div><div class="ld l"><div class="my l lf lg lh ld li lj ku"/></div></div></a></div><div class="kr ks ez fb kt ku"><a href="https://towardsdatascience.com/advanced-dqns-playing-pac-man-with-deep-reinforcement-learning-3ffbd99e0814" rel="noopener follow" target="_blank"><div class="kv ab dw"><div class="kw ab kx cl cj ky"><h2 class="bd hi fi z dy kz ea eb la ed ef hg bi translated">高级DQNs:用深度强化学习玩吃豆人</h2><div class="lb l"><h3 class="bd b fi z dy kz ea eb la ed ef dx translated">2013年，DeepMind发布了其深度Q网络(DQN)的第一个版本，这是一个能够达到人类水平的计算机程序</h3></div><div class="lc l"><p class="bd b fp z dy kz ea eb la ed ef dx translated">towardsdatascience.com</p></div></div><div class="ld l"><div class="mz l lf lg lh ld li lj ku"/></div></div></a></div><p id="bb52" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个项目是为索诺拉大学的神经网络作业而开发的。感谢Julio Waissman博士使这个项目成为可能。</p></div></div>    
</body>
</html>