<html>
<head>
<title>Image Captioning using Encoder-Attention-Decoder technologies of Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ä½¿ç”¨æ·±åº¦å­¦ä¹ çš„ç¼–ç å™¨-æ³¨æ„åŠ›-è§£ç å™¨æŠ€æœ¯çš„å›¾åƒå­—å¹•</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/analytics-vidhya/image-captioning-using-encoder-attention-decoder-technologies-of-deep-learning-c3443505eb85?source=collection_archive---------8-----------------------#2021-04-25">https://medium.com/analytics-vidhya/image-captioning-using-encoder-attention-decoder-technologies-of-deep-learning-c3443505eb85?source=collection_archive---------8-----------------------#2021-04-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="98be" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">ä»Šå¤©æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•è§£å†³è‘—åçš„ä¸ºå›¾åƒæ·»åŠ å­—å¹•çš„æ·±åº¦å­¦ä¹ é—®é¢˜ã€‚åœ¨è®¨è®ºè§£å†³æ–¹æ¡ˆä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ ä»¬å¯¹<strong class="ik hi"> <em class="hh">ç¼–ç å™¨-è§£ç å™¨æ¶æ„</em> </strong>å’Œä»€ä¹ˆæ˜¯<strong class="ik hi">æ³¨æ„æœ‰åŸºæœ¬çš„äº†è§£ã€‚</strong> PS:å¦‚æœä½ æ²¡æœ‰è¿™äº›æ–¹é¢çš„çŸ¥è¯†ï¼Œéšæ—¶å¯ä»¥ä»å„ç§ç½‘ä¸Šèµ„æºä¸­ä¸€çª¥ç«¯å€ªã€‚</p></blockquote><p id="d5c6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">è®©æˆ‘ä»¬å…ˆé€šè¿‡æƒ³è±¡æˆ‘å¾—åˆ°çš„ä¸€äº›æœ€ç»ˆç»“æœæ¥å¼•èµ·ä½ çš„å…´è¶£ã€‚</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es jj"><img src="../Images/6a806600fb0b4b8909a63f447917104b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*YWdNFYEuZoOVQt8wwflv5A.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">é¢„æµ‹æè¿°:ä¸€åªæ£•è‰²çš„ç‹—åœ¨æµ…æ°´åŒº<end/></figcaption></figure><p id="6d9c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œåœ¨æ’å…¥è¿™å¼ å›¾ç‰‡åï¼Œåª’ä½“é—®æˆ‘å…³äºæ ‡é¢˜çš„é—®é¢˜ï¼Œæˆ‘åªæ˜¯å¤åˆ¶ç²˜è´´æ ‡é¢˜ğŸ˜Œæˆ‘çš„æ¨¡å‹é¢„æµ‹çš„ã€‚è¿™é‡Œå­—å¹•ä»¥ä¸€ä¸ª<end>æ ‡è®°ç»“æŸï¼Œè¿™æ˜¯è§£ç å™¨åœæ­¢åœ¨è¿™ä¸ª<end>æ ‡è®°åç”Ÿæˆæ–‡æœ¬æ‰€å¿…éœ€çš„ã€‚</end></end></p><blockquote class="ie if ig"><p id="84ed" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">æˆ‘è¦æå‡ºçš„æ¶æ„çš„åŠ¨æœºï¼Œæ‘˜è‡ª<a class="ae jv" href="https://arxiv.org/pdf/1502.03044.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ik hi"> <em class="hh">å±•ç¤ºã€å‡ºå¸­ã€è®²è¿°:è§†è§‰æ³¨æ„çš„ç¥ç»å›¾åƒå­—å¹•ç”Ÿæˆ</em> </strong> </a> <strong class="ik hi"> <em class="hh">ã€‚</em> </strong>æˆ‘å°†ä½¿ç”¨<em class="hh">è½¯å…³æ³¨</em>ä¸<em class="hh"> VGG-16ç¼–ç å™¨</em>å’Œ<em class="hh"> GRU (RNNå˜ç§)</em>å±‚è¿›è¡Œè§£ç ã€‚</p></blockquote></div><div class="ab cl jw jx go jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="ha hb hc hd he"><p id="24f4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">è®©æˆ‘ä»¬æ·±å…¥åˆ°ç¼–ç éƒ¨åˆ†ï¼›</p><blockquote class="ie if ig"><p id="cabf" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">å¯¼å…¥åº“</p></blockquote><pre class="jk jl jm jn fd kd ke kf kg aw kh bi"><span id="d32e" class="ki kj hh ke b fi kk kl l km kn">!pip install nltk==3.5<br/>from nltk.translate.meteor_score import meteor_score<br/>from nltk.translate.bleu_score import sentence_bleu<br/>import random<br/>from sklearn.model_selection import train_test_split<br/>import datetime<br/>import time<br/>from PIL import Image<br/>import collections<br/>import random<br/>from keras.models import load_model<br/>import os<br/>import cv2<br/>import matplotlib.pyplot as plt<br/>import joblib<br/>import pandas as pd<br/>import tensorflow as tf<br/>from tensorflow import keras<br/>from keras.applications.vgg16 import VGG16<br/>from keras.applications.vgg19 import VGG19<br/>from keras.models import Model<br/>import numpy as np<br/>import nltk<br/>nltk.download('stopwords')<br/>nltk.download('wordnet')<br/>nltk.download('words')<br/>from nltk.corpus import words<br/>from nltk.corpus import stopwords<br/>from nltk.stem.porter import PorterStemmer<br/>from nltk.stem import WordNetLemmatizer<br/>from keras.preprocessing.text import Tokenizer<br/>from keras.preprocessing.sequence import pad_sequences</span></pre><blockquote class="ie if ig"><p id="23c7" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">é¢„å¤„ç†</p></blockquote><p id="36ee" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">é¢„å¤„ç†éƒ¨åˆ†å®Œå…¨å–å†³äºä½ æ‰€æ‹¥æœ‰çš„æ•°æ®ï¼Œä¸€èˆ¬æ¥è¯´å¯¹äºé¢„å¤„ç†ï¼Œè¦æ³¨æ„ä»¥ä¸‹å‡ ä¸ªå…³é”®ç‚¹:</p><ol class=""><li id="40fe" class="ko kp hh ik b il im ip iq jg kq jh kr ji ks jf kt ku kv kw bi translated">å°†æ‚¨æ‰€æœ‰çš„åŸ¹è®­ã€éªŒè¯å’Œæµ‹è¯•æ•°æ®å­˜å‚¨åœ¨ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ã€‚æ­¤å¤–ï¼Œåœ¨è¿™ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œæ‚¨å¯ä»¥æœ‰ä¸åŒçš„æ–‡ä»¶å¤¹ç”¨äºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•å›¾åƒã€‚</li><li id="ae4d" class="ko kp hh ik b il kx ip ky jg kz jh la ji lb jf kt ku kv kw bi translated">æ‚¨å¿…é¡»æœ‰è®­ç»ƒå›¾åƒæ ‡é¢˜ã€‚</li><li id="5422" class="ko kp hh ik b il kx ip ky jg kz jh la ji lb jf kt ku kv kw bi translated">é¢„å¤„ç†ä¹‹åï¼Œå¿…é¡»æœ‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰å›¾åƒè·¯å¾„çš„å­—å…¸ä½œä¸ºé”®ï¼Œè¿™äº›å›¾åƒçš„æ ‡é¢˜ä½œä¸ºå€¼ã€‚</li></ol><p id="e012" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">æ³¨æ„:å¦‚æœå•ä¸ªå›¾åƒæœ‰å¤šä¸ªæ ‡é¢˜ï¼Œè¯·å°†å…¶å­˜å‚¨åœ¨å…·æœ‰ç›¸åº”å›¾åƒè·¯å¾„çš„åˆ—è¡¨ä¸­ã€‚</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lc"><img src="../Images/c4e6c5eb1488a25b4deaac23aa312412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tqkt9BiMIR-uj7XROnINuQ.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">å­—å…¸ç¤ºä¾‹</figcaption></figure><blockquote class="ie if ig"><p id="011c" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">æœ€åï¼Œä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘è¿˜ä¼šæä¾›githubçš„é“¾æ¥ã€‚</p></blockquote><p id="ce6d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">ç»è¿‡é¢„å¤„ç†åï¼Œæˆ‘ä»¬å¿…é¡»å°†æ•°æ®åŠ è½½åˆ°å¼ é‡ä¸­è¿›è¡Œè®­ç»ƒã€‚é¢„å¤„ç†å’Œæ•°æ®åŠ è½½æ˜¯ç‰¹å®šçš„ï¼Œå°†æ•°æ®è½¬æ¢ä¸ºæ•°æ®ã€‚æ— è®ºå¦‚ä½•ï¼Œæˆ‘ä¼šåœ¨Github repoä¸­æä¾›æˆ‘çš„æ•°æ®æ–‡ä»¶å¤¹ã€‚</p><h1 id="c099" class="lh kj hh bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">ç¼–ç å™¨-æ³¨æ„-è§£ç å™¨(æ˜¾ç¤ºå‡ºå¸­å‘Šè¯‰)</h1><pre class="jk jl jm jn fd kd ke kf kg aw kh bi"><span id="6cd5" class="ki kj hh ke b fi kk kl l km kn"># Setting hyper-parameters for models</span><span id="ca35" class="ki kj hh ke b fi me kl l km kn">BATCH_SIZE = 64<br/>BUFFER_SIZE = 1000<br/>embedding_dim = 256<br/>units = 512<br/>vocab_size = top_k + 1<br/>num_steps = len(img_name_train) // BATCH_SIZE<br/>features_shape = 2048<br/>attention_features_shape = 49</span></pre><p id="743e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">ç°åœ¨è®©æˆ‘ä»¬åˆ›å»ºç¼–ç å™¨ã€è§£ç å™¨å’Œæ³¨æ„åŠ›çš„ç±»åˆ«ã€‚</p><blockquote class="ie if ig"><p id="b01d" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">ç¼–ç å™¨ç±»åˆ«</p></blockquote><pre class="jk jl jm jn fd kd ke kf kg aw kh bi"><span id="c01c" class="ki kj hh ke b fi kk kl l km kn">class CNN_Encoder(tf.keras.Model):</span><span id="302f" class="ki kj hh ke b fi me kl l km kn">   def __init__(self, embedding_dim):<br/>      super(CNN_Encoder, self).__init__()<br/>      self.fc = tf.keras.layers.Dense(embedding_dim)<br/>   <br/>   def call(self, x): <br/>      x = self.fc(x)<br/>      x = tf.nn.relu(x) <br/>      return x</span></pre><blockquote class="ie if ig"><p id="10ff" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">æ³¨æ„ç±»</p></blockquote><pre class="jk jl jm jn fd kd ke kf kg aw kh bi"><span id="f283" class="ki kj hh ke b fi kk kl l km kn">class BahdanauAttention(tf.keras.Model): #Soft Attention<br/>  def __init__(self, units):<br/>    super(BahdanauAttention, self).__init__()<br/>    self.W1 = tf.keras.layers.Dense(units)<br/>    self.W2 = tf.keras.layers.Dense(units)<br/>    self.V = tf.keras.layers.Dense(1)</span><span id="b486" class="ki kj hh ke b fi me kl l km kn">  def call(self, features, hidden):<br/>    hidden_with_time_axis = tf.expand_dims(hidden, 1)</span><span id="d622" class="ki kj hh ke b fi me kl l km kn">    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +         <br/>                              self.W2(hidden_with_time_axis)))</span><span id="20a8" class="ki kj hh ke b fi me kl l km kn">    score = self.V(attention_hidden_layer)<br/>    attention_weights = tf.nn.softmax(score, axis=1)</span><span id="e5c0" class="ki kj hh ke b fi me kl l km kn">    context_vector = attention_weights * features<br/>    context_vector = tf.reduce_sum(context_vector, axis=1)</span><span id="e39e" class="ki kj hh ke b fi me kl l km kn">    return context_vector, attention_weights</span></pre><blockquote class="ie if ig"><p id="cff7" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">è§£ç å™¨ç±»åˆ«</p></blockquote><pre class="jk jl jm jn fd kd ke kf kg aw kh bi"><span id="2f85" class="ki kj hh ke b fi kk kl l km kn">class RNN_Decoder(tf.keras.Model):<br/>  def __init__(self, embedding_dim, units, vocab_size):<br/>    super(RNN_Decoder, self).__init__()<br/>    self.units = units<br/>    self.embedding = tf.keras.layers.Embedding(vocab_size,    embedding_dim)<br/>    <br/>    self.gru = tf.keras.layers.GRU(self.units,<br/>                                   return_sequences=True,<br/>                                   return_state=True,                       recurrent_initializer=â€™glorot_uniformâ€™)<br/>    <br/>    self.fc1 = tf.keras.layers.Dense(self.units)<br/>    self.fc2 = tf.keras.layers.Dense(vocab_size)<br/>    self.attention = BahdanauAttention(self.units)</span><span id="53b7" class="ki kj hh ke b fi me kl l km kn">  def call(self, x, features, hidden):<br/>    context_vector, attention_weights = self.attention(features, hidden)</span><span id="b063" class="ki kj hh ke b fi me kl l km kn">    x = self.embedding(x)<br/>    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)<br/>    output, state = self.gru(x)<br/>    x = self.fc1(output)<br/>    x = tf.reshape(x, (-1, x.shape[2])) <br/>    x = self.fc2(x)</span><span id="d19a" class="ki kj hh ke b fi me kl l km kn">    return x, state, attention_weights</span><span id="fda9" class="ki kj hh ke b fi me kl l km kn">  def reset_state(self, batch_size):<br/>    return tf.zeros((batch_size, self.units))</span></pre><blockquote class="ie if ig"><p id="4efb" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">ä¸ºäº†ä½¿ç”¨æ‰€æœ‰è¿™äº›ä½“ç³»ç»“æ„ï¼Œæˆ‘ä»¬å¿…é¡»ç¼–å†™ä¸€ä¸ªè®­ç»ƒå‡½æ•°æ¥æœ‰æ•ˆåœ°è®­ç»ƒæ¨¡å‹:</p></blockquote><pre class="jk jl jm jn fd kd ke kf kg aw kh bi"><span id="bc73" class="ki kj hh ke b fi kk kl l km kn"># defining loss function, optimizer, </span><span id="37f6" class="ki kj hh ke b fi me kl l km kn">encoder = CNN_Encoder(embedding_dim)<br/>decoder = RNN_Decoder(embedding_dim, units, vocab_size)</span><span id="639e" class="ki kj hh ke b fi me kl l km kn">optimizer = tf.keras.optimizers.Adam()<br/>loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=â€™noneâ€™)</span><span id="5857" class="ki kj hh ke b fi me kl l km kn">def loss_function(real, pred):</span><span id="9002" class="ki kj hh ke b fi me kl l km kn">  mask = tf.math.logical_not(tf.math.equal(real, 0))<br/>  loss_ = loss_object(real, pred)</span><span id="5567" class="ki kj hh ke b fi me kl l km kn">  mask = tf.cast(mask, dtype=loss_.dtype)<br/>  loss_ *= mask</span><span id="fcd3" class="ki kj hh ke b fi me kl l km kn">  return tf.reduce_mean(loss_)</span></pre><blockquote class="ie if ig"><p id="e3f2" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">åŸ¹è®­åŠŸèƒ½</p></blockquote><pre class="jk jl jm jn fd kd ke kf kg aw kh bi"><span id="185f" class="ki kj hh ke b fi kk kl l km kn">@tf.function<br/>def train_step(img_tensor, target):</span><span id="968f" class="ki kj hh ke b fi me kl l km kn">  loss = 0<br/>  hidden = decoder.reset_state(batch_size=target.shape[0])<br/>  dec_input = tf.expand_dims([tokenizer.word_index[â€˜&lt;start&gt;â€™]] * target.shape[0], 1)</span><span id="5c14" class="ki kj hh ke b fi me kl l km kn">  with tf.GradientTape() as tape:</span><span id="983b" class="ki kj hh ke b fi me kl l km kn">    features = encoder(img_tensor)<br/>    for i in range(1, target.shape[1]):</span><span id="c872" class="ki kj hh ke b fi me kl l km kn">      predictions, hidden, _ = decoder(dec_input, features, hidden)<br/>      loss += loss_function(target[:, i], predictions)</span><span id="baf0" class="ki kj hh ke b fi me kl l km kn">      dec_input = tf.expand_dims(target[:, i], 1)</span><span id="bafe" class="ki kj hh ke b fi me kl l km kn">  total_loss = (loss / int(target.shape[1]))</span><span id="8706" class="ki kj hh ke b fi me kl l km kn">  trainable_variables = encoder.trainable_variables +<br/>                        decoder.trainable_variables</span><span id="3490" class="ki kj hh ke b fi me kl l km kn">  gradients = tape.gradient(loss, trainable_variables)</span><span id="5145" class="ki kj hh ke b fi me kl l km kn">  optimizer.apply_gradients(zip(gradients, trainable_variables))</span><span id="963e" class="ki kj hh ke b fi me kl l km kn">  return loss, total_loss</span></pre><blockquote class="ie if ig"><p id="cbbd" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">å‘¼å«åˆ—è½¦åŠŸèƒ½</p></blockquote><pre class="jk jl jm jn fd kd ke kf kg aw kh bi"><span id="61ff" class="ki kj hh ke b fi kk kl l km kn">def train_data(dataset, EPOCHS=10):</span><span id="4a1e" class="ki kj hh ke b fi me kl l km kn">  loss_plot = []<br/>  <br/>  for epoch in range(EPOCHS):<br/>    start_time = time.time()<br/>    total_loss = 0</span><span id="e7f9" class="ki kj hh ke b fi me kl l km kn">    for (batch, (img_tensor, target)) in enumerate(dataset):</span><span id="b423" class="ki kj hh ke b fi me kl l km kn">      batch_loss, t_loss = train_step(img_tensor, target)<br/>      total_loss += t_loss</span><span id="b89b" class="ki kj hh ke b fi me kl l km kn">      if batch % 100 == 0:<br/>        average_batch_loss = batch_loss.numpy()/int(target.shape[1])</span><span id="b789" class="ki kj hh ke b fi me kl l km kn">    loss_plot.append(total_loss / num_steps)<br/>    end_time = time.time()<br/>    print(â€˜Epoch {0:d}/{1:d}â€™.format(epoch+1, EPOCHS), â€œ: {0:.3f}secâ€.format((end_time â€” start_time)))<br/>    print(â€˜===============&gt; train-loss=%.3fâ€™ % (total_loss/num_steps))<br/>  <br/>  return loss_plot</span></pre><p id="93a1" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">å¯¹äºè®­ç»ƒï¼Œåªéœ€è°ƒç”¨train_data()ï¼Œå¦‚ä¸‹å›¾:</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es mf"><img src="../Images/237d5ae2e262e37a77fa0a1465030018.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*OrFChhyIkfm019S_o0PQjQ.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">å‘¼å«åˆ—è½¦åŠŸèƒ½</figcaption></figure><p id="8334" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">å¦‚æœä½ æ´»åˆ°ç°åœ¨â€¦..ä¼Ÿå¤§çš„ğŸ˜</p><p id="abec" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">æ‚¨çš„ç¼–ç å™¨å’Œè§£ç å™¨æ¨¡å‹å·²ç»åˆ›å»ºï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸€äº›è¯„ä¼°ã€‚</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es mg"><img src="../Images/44db5a53994845380016650e733ff345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*hWkAwID3g8r9_M2I6bS9cQ.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">è¯„ä¼°åŠŸèƒ½</figcaption></figure><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es mg"><img src="../Images/f7f54b34c065c4d23dc9127b130ef66a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*2ahtSnMlWicI4e_OzazFug.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">ç»˜åˆ¶æ³¨æ„åŠ›æƒé‡</figcaption></figure><p id="d4b8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">è¿™äº›å‡½æ•°å°†åœ¨è¾“å…¥å›¾åƒå’Œæ ‡é¢˜æ–‡æœ¬ä¸Šç»˜åˆ¶æ³¨æ„åŠ›æƒé‡ã€‚</p><blockquote class="ie if ig"><p id="2f53" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">ä¸Šé¢ç»™å‡ºäº†ä¸€ä¸ªé¢„æµ‹å­—å¹•çš„ä¾‹å­ï¼Œè®©æˆ‘ä»¬å¯¹åŒä¸€å¹…å›¾åƒçš„æ³¨æ„åŠ›æƒé‡è¿›è¡Œå¯è§†åŒ–</p></blockquote><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es mh"><img src="../Images/158f92ddf52ead0f54ab4f380aad275d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*2WcbTMvXuOefCY11q25yBA.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">ç™½è‰²æ–¹æ¡†è¡¨ç¤ºæ³¨æ„åŠ›æƒé‡ã€‚è¯¥æ¨¡å‹åˆ†é…ç»™è¯¥å›¾åƒç”¨äºé¢„æµ‹å­—å¹•ã€‚</figcaption></figure><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es mi"><img src="../Images/3acfb638776c48b9effb6d6e7f7d821e.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*yLtGgVTky4JgC4ldssA3Ag.png"/></div></figure><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es mj"><img src="../Images/692c9a002acb3fa86d81ba0d9a144277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q5LX6-6-kGRjGxOIf_wr0Q.png"/></div></div></figure><p id="d93f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">å°±åˆ°æ­¤ä¸ºæ­¢å§ï¼äººä»¬ä¹Ÿå¯ä»¥æ£€æŸ¥BLEUåˆ†æ•°æˆ–æµæ˜Ÿï¼Œä½†é‚£æ˜¯ä¸å¤ªæ„Ÿå…´è¶£çš„ï¼Œå¦‚æœä½ ä¹Ÿæƒ³å­¦ä¹ ç¼–ç å®ƒä»¬ï¼Œåªéœ€å…³æ³¨æˆ‘çš„<strong class="ik hi"> <em class="ij"> GitHubå›è´­é“¾æ¥æ˜¯</em> </strong> <a class="ae jv" href="https://github.com/deepankarkansal/Image-Captioning" rel="noopener ugc nofollow" target="_blank"> <strong class="ik hi"> <em class="ij">è¿™é‡Œ</em> </strong> </a> <strong class="ik hi"> <em class="ij">ã€‚</em>T13ã€‘</strong></p><blockquote class="ie if ig"><p id="36fb" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">ä»£ç å®ç°:<a class="ae jv" href="https://www.linkedin.com/in/palak-2810-tiwari/" rel="noopener ugc nofollow" target="_blank"> <strong class="ik hi"> <em class="hh">å¸•æ‹‰å…‹æç“¦é‡Œ</em> </strong> </a>ï¼Œ<a class="ae jv" href="https://www.linkedin.com/in/deepankar-kansal-255911152/" rel="noopener ugc nofollow" target="_blank"> <strong class="ik hi"> <em class="hh">è¿ªæ½˜å¡å°”åè¨å°”</em> </strong> </a></p></blockquote><h1 id="5261" class="lh kj hh bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak"> <em class="mk">å‚è€ƒæ–‡çŒ®</em> </strong></h1><ol class=""><li id="84b0" class="ko kp hh ik b il ml ip mm jg mn jh mo ji mp jf kt ku kv kw bi translated"><a class="ae jv" href="https://arxiv.org/pdf/1502.03044.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ik hi"> <em class="ij">å±•ç¤ºã€å‡ºå¸­ã€è®²è¿°:è§†è§‰æ³¨æ„çš„ç¥ç»å›¾åƒå­—å¹•ç”Ÿæˆ</em> </strong> </a></li><li id="4f04" class="ko kp hh ik b il kx ip ky jg kz jh la ji lb jf kt ku kv kw bi translated"><a class="ae jv" href="https://arxiv.org/pdf/1808.02401.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ik hi"> <em class="ij">ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ„å»ºç¼–è§£ç å™¨:èµ°å‘ç°å®</em> </strong> </a></li></ol></div></div>    
</body>
</html>