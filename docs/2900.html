<html>
<head>
<title>Linear Regression — Using Numpy And Python — ML For Lazy 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归—使用Numpy和Python —适用于Lazy 2021的ML</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-using-numpy-and-python-ml-for-lazy-2021-d702915b1eeb?source=collection_archive---------9-----------------------#2021-05-23">https://medium.com/analytics-vidhya/linear-regression-using-numpy-and-python-ml-for-lazy-2021-d702915b1eeb?source=collection_archive---------9-----------------------#2021-05-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="bc8c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在文章<a class="ae jc" href="https://mlforlazy.in/linear-regression/" rel="noopener ugc nofollow" target="_blank">线性回归——理论和代码</a>中，我们学习了线性回归以及线性回归是如何工作的。</p><p id="70d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们看到了使用sci-kit learn库的概念的简单实现，并且能够通过具有一个输入特征和一个输出特征的数据点找到最佳拟合线。</p><p id="f145" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用sci-kit learn，我们学习了上层概念，当我们实现时，我们不知道幕后发生了什么。现在我们已经看到了LR的运行，并做了一些预测，是时候了解我们编写的代码的背后细节，并在不使用sci-kit库的情况下编写相同的代码了。</p><p id="faf8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">许多人知道上层的概念，很少有人知道幕后的概念。<strong class="ig hi">要了解树，不要只吃果实，还要了解树的根。</strong></p><p id="73f2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我们将使用python、NumPy和pandas编写代码来加载数据。我们可以使用纯python读取CSV文件，但此时，我们将使用pandas并使用pandas编写这些文件访问。</p><h1 id="24a5" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">最简单的回归方程。</h1><p id="b713" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">我们能得到的最简单的数据是有一个输入变量和一个输出变量的数据。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kg"><img src="../Images/a0e52b066eec68694a5e3020a0e45c7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/0*1FLe0jWfRHagWs4Z.jpg"/></div></figure><p id="5a22" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于这种数据，我们可以写出的最简单的方程是直线与截距和斜率的方程。</p><p id="d9f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">即:-</p><p id="306b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y = mx + c</p><p id="68b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中m =斜率，c =截距</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ko"><img src="../Images/bcbbcbc5518d771c6902a328bd370a87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cfn59uu7J_bCACE1.png"/></div></div></figure><p id="faab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">现在，当我们谈论线性回归时，整个目标是找到穿过数据的直线，以便该直线非常有效地逼近所有数据点，这意味着找到最佳斜率和最佳截距值。</strong></p><h2 id="86bb" class="kt je hh bd jf ku kv kw jj kx ky kz jn ip la lb jr it lc ld jv ix le lf jz lg bi translated">标量和向量的概念。</h2><p id="6a00" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">在上面的等式中，c和m都是标量。</p><p id="52de" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">标量是正常的数字，如2，3等。</p><p id="d27f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里我们用小写字母X，y表示定标器，用大写字母X，y表示向量。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ko"><img src="../Images/dfc23eb4e045a8600d978bbbf13aff1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VI7h_IioMrS72Sa0.png"/></div></div></figure><p id="0a64" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在机器学习中，我们把上面写的等式写成如下—[方式]</p><p id="56a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y =θ_ 0+θ_ 1 . x</p><p id="268a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">θnot和θ与截距和斜率相同，在这种情况下，它们是标量。但是在这里，我们说θnot是偏差，θ1是权重。简单来说，θnot是任意常数值，θ1是变量x1，x2等的常数。</p><p id="90f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为我们讨论的是简单的线性方程，所以我们得到的方程是一个变量，可以是这种形式[举例]即y = 2 + 5x</p><p id="0a3f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上式中，y =输出变量，2 =θnot[某个常数]，5是x的常数，根据上式，它等于θ。</p><p id="3b47" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我们有2个数据点，那么我们将有2个等式，每个等式一个。同样的，对于n个数据点，我们会有<strong class="ig hi"> n </strong>个不同θnot和θ值的方程。在2个数据点的例子中，我们可以有-</p><p id="2ae9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y_1 = 2 + 3x_1</p><p id="0244" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y2 = 1+6x 2</p><p id="8e38" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，y1是第一输入数据点的输出，y2是第二数据点的输出。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ko"><img src="../Images/7b59da54308da6fc00809668d2b115d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DE7dW-MvIzawwUHJ.png"/></div></div></figure><p id="cade" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们在寻找一条直线的方程时，我们在逼近数据点。权重和偏差不断变化，直到我们得到最佳拟合线。所以机器学习中的学习就是学习这些偏差和权重，得到它们的最优值。</p><p id="65cb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，学习=学习最佳权重和偏差。</p><p id="f774" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，由于真实数据不仅仅包含一两个数据点，而是包含大量的数据点，我们无法将所有这些数据点表示为单独的方程。在这种情况下，我们使用向量符号来简化我们的任务。直截了当地说，我们使用矩阵。而一维矩阵被称为向量。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ko"><img src="../Images/9cd28ba89d53ddf50e36a665d658c043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WdmQ3oULD9OX-HfD.png"/></div></div></figure><p id="6a15" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以从第<a class="ae jc" href="https://en.wikipedia.org/wiki/Matrix_(mathematics)" rel="noopener ugc nofollow" target="_blank">页</a>中了解更多关于标量、向量和矩阵的信息。</p><h1 id="3bc0" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">含有两个或更多变量的方程</h1><p id="6b73" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">真实世界的数据看起来像这样-</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ko"><img src="../Images/1845272e936c5cee24bdb750e837f3fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SmJBYcoJw7R54oTI.png"/></div></div></figure><p id="6fee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上述数据只有一个输入数据列，但实际数据有许多列，即许多影响输出的输入要素</p><p id="a341" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y _ 1 =θ_ 0+θ_ 1x _ 1+θ_ 2x _ 2…</p><p id="653c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些<strong class="ig hi">θ</strong>和<strong class="ig hi">θ</strong>可以用与上面相同的方式表示，即向量符号。唯一的区别是，X现在是一个m*n维矩阵，而不是一个列向量。</p><p id="3dbe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">x可以表示为-</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ko"><img src="../Images/8103067b8120f8ea91a0496acbfffd8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kB4lNhPPT84dinAx.png"/></div></div></figure><p id="1b61" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中X1，X2是单独的列向量</p><h1 id="d369" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">数据如何影响输出。</h1><p id="6b89" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">简单方程和多重方程是一样的，不同的是</p><p id="a399" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在简单的线性回归中，只有一个因素影响输出</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ko"><img src="../Images/b7691e99d674934945300b09eebdbf57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JWgDWe55GFK2xDlY.png"/></div></div></figure><p id="8fa5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在线性回归中，多个因素负责输出。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ko"><img src="../Images/f17f97ffbae7a71ac52a4c8fcad4d1cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8I8gf-TvxX4RFt6U.png"/></div></div></figure><h1 id="069a" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">方程的向量形式</h1><p id="09a0" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">这种形式的数据是相当庞大的，我们无法一一对其进行操作。所以对于这一点，向量运算来拯救，我们，在机器学习中，使用向量运算。</p><p id="a718" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，上述方程的矢量形式可以写成:</p><p id="f73f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y =θ。(点)X</p><p id="1f35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用NumPy点积。NumPy为我们提供了许多执行数学矩阵运算的函数。</p><p id="4cf7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了得到解决方案，我们必须转置θ，即，用列替换行，然后用特征矩阵点积。这样做是因为矩阵的乘法性质，即第一矩阵的行数应该等于第二矩阵的列数。</p><h1 id="8e23" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">解决方案</h1><p id="2e4f" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">当我们谈论线性回归时，我们试图找到简单直线方程的解。因为我们希望预测尽可能完美，所以我们学习了影响输出的两个常数。</p><p id="d166" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">线的方程是— y = mx + c，所以我们试着去学习它们。我们的目标是找到这些常数的值，这样它们就给出了最佳拟合线。</p><p id="5533" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于m和c，我们有以下等式</p><p id="aa45" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">m =(y . dot(X)-y . mean()* X . sum())/(X . dot(X)-X . mean()* X . sum())和</p><p id="65f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">c =(y . mean()*(X . dot(X))—X . mean()* y . dot(X))/(X . dot(X)-X . mean()* X . sum())</p><p id="08b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们需要做的，就是将这些方程用于我们的数据，并找到它们的值。下面的代码做到了这一点。</p><p id="4f1a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，我们需要导入库并读取数据。</p><pre class="kh ki kj kk fd lh li lj lk aw ll bi"><span id="a0c2" class="kt je hh li b fi lm ln l lo lp">import pandas as pd import numpy as np dataset = pd.read_csv("datapath") dataset.head()</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lq"><img src="../Images/5bdd365642f89841491f4cab0126d72a.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/0*VcOHrwe16jyf7Cfa.png"/></div></figure><p id="8296" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将数据分为输入和输出。X =输入列，y =输出列。通过使用“值”，我们将它们转换成NumPy数组。</p><pre class="kh ki kj kk fd lh li lj lk aw ll bi"><span id="70fe" class="kt je hh li b fi lm ln l lo lp">X = dataset[0].values y = dataset[1].values</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es lr"><img src="../Images/cb4f245a09f3151bc958be901447e603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AWu5Rn10bc_IRjPu.png"/></div></div></figure><p id="a5cb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们使用matplotlib可视化这些数据。</p><pre class="kh ki kj kk fd lh li lj lk aw ll bi"><span id="e740" class="kt je hh li b fi lm ln l lo lp">import matplotlib.pyplot as plt plt.scatter(X,y)</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ls"><img src="../Images/90370351aa0c1b1bfb61429aacfcdf8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/0*1yHFKX98GNUfW8My.png"/></div></figure><p id="848c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">应用我们上面针对m和c描述的等式</p><pre class="kh ki kj kk fd lh li lj lk aw ll bi"><span id="55fd" class="kt je hh li b fi lm ln l lo lp"># applying the equations we have for A, B denominator = X.dot(X)-X.mean() * X.sum()</span><span id="5491" class="kt je hh li b fi lt ln l lo lp">m = (y.dot(X) - y.mean() * X.sum())/ denominator print(a)</span><span id="9a83" class="kt je hh li b fi lt ln l lo lp">c = (y.mean() * (X.dot(X)) - X.mean() * y.dot(X)) / denominator print(b)</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lu"><img src="../Images/32936bb7d3529d81249a2e9379e292a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/0*Hnm60Hw-oXOHa9MM.png"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lv"><img src="../Images/a28255d16e004dc2abe508dba75881c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/0*nGcNZFeBEUdWnfgd.png"/></div></figure><p id="8bbc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，将m和c的值放入方程line-y = MX+c</p><pre class="kh ki kj kk fd lh li lj lk aw ll bi"><span id="2f67" class="kt je hh li b fi lm ln l lo lp">yHat = m*X + c print(yHat) # yhat is the predicted value</span></pre><p id="70a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看看，线是如何找到的。</p><pre class="kh ki kj kk fd lh li lj lk aw ll bi"><span id="5ea9" class="kt je hh li b fi lm ln l lo lp">plt.scatter(X, y) plt.plot(X, yHat, 'r')</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lw"><img src="../Images/52fe962895339afc27614598eee69c30.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/0*sMBdIP_OCjZ8gVx0.png"/></div></figure><p id="45b2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">误差计算</p><pre class="kh ki kj kk fd lh li lj lk aw ll bi"><span id="9e32" class="kt je hh li b fi lm ln l lo lp"># calculate the R sqr # yhat - y squared, then sum, then sqt, then divide by difference = yHat-y</span></pre><p id="0004" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里我们发现均方根误差。</p><pre class="kh ki kj kk fd lh li lj lk aw ll bi"><span id="ecd0" class="kt je hh li b fi lm ln l lo lp">R = np.square(difference) rms = np.sqrt(R.sum())/len(R) np.sqrt(difference.dot(difference))/len(difference) print(rms)</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es lx"><img src="../Images/292d3aeecce0319a7d86d81e3638c2e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/0*sZ88BlfOZvYGEod8.png"/></div></div></figure><p id="44e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">均方根越低，拟合越好。</p><p id="61b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此我们得到的直线方程是-</p><h1 id="8551" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">结论</h1><p id="ffcc" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">希望你了解了简单线性回归的背景，以及它在numpy和python中的实现。现在，起来喝杯咖啡吧。梳洗一番，自己动手弄脏手。</p><p id="febf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">看之前的帖子。</p><p id="0a99" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你喜欢这篇文章，请与朋友分享🙂</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ly"><img src="../Images/7cef6c82cbcd4fb27e20f203ed0b11cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:192/0*iscjfdkcdzFkGYIj"/></div></figure><p id="53df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我是一名来自克什米尔的计算机科学研究生。在这些日子里，我转向传播关于机器学习的信息，这是我的激情和未来的研究。目的是让人们了解和理解机器和深度学习本身的基本概念，这些概念对该领域的进一步成功至关重要。</p></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="6a93" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="mg">原载于2021年5月23日</em><a class="ae jc" href="https://mlforlazy.in/linear-regression-using-numpy-and-python/" rel="noopener ugc nofollow" target="_blank"><em class="mg">https://mlforlazy . in</em></a><em class="mg">。</em></p></div></div>    
</body>
</html>