<html>
<head>
<title>Multiclass Text Classification Using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习的多类文本分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multiclass-text-classification-using-deep-learning-f25b4b1010e5?source=collection_archive---------0-----------------------#2021-08-22">https://medium.com/analytics-vidhya/multiclass-text-classification-using-deep-learning-f25b4b1010e5?source=collection_archive---------0-----------------------#2021-08-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="6e08" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">在本文中，我们将使用各种深度学习方法来解决一个多类文本分类问题。</em> </strong> <em class="jc">所以让我们先理解它，并用python做一个简短的实现。</em></p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="fa54" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">数据集/问题描述</h1><p id="fdfa" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">在这篇文章中，我使用了来自Kaggle的<a class="ae kn" href="https://www.kaggle.com/jessicali9530/kuc-hackathon-winter-2018" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> UCI ML药物评论数据集</strong> </a>。它包含超过200，000个患者药物评论，以及相关条件。数据集有许多列，但是我们将只使用其中的两列来完成NLP任务。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es ko"><img src="../Images/d3ad0c167e99f69d7f309f809cbe44e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XPejyAtpattkIKYI.png"/></div></figure><p id="98b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以，我们的数据集看起来像这样:</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es kw"><img src="../Images/3b72a17092ef431a55704601e45104ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TVEeFDFGis4uxBQL.png"/></div></div></figure><p id="b31d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">任务:我们希望根据药物综述对主要疾病进行分类。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lb"><img src="../Images/b7431e1070c113b8e008e066c0fe67a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FT-sqA6YtbQaiGY4.png"/></div></div></figure></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="8c64" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">word2vec嵌入入门:</h1><p id="9dfe" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">在我们进一步研究文本分类之前，我们需要一种在词汇表中用数字表示单词的方法。为什么？因为我们大部分的ML模型都需要数字，而不是文本。</p><p id="247f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">实现这个目标的一种方法是使用单词向量的一次性编码，但这不是正确的选择。给定大量的词汇，这种表示将占用大量的空间，并且它不能准确地表达不同单词之间的相似性，例如如果我们想要找到数字单词x和y之间的余弦相似性:</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es lc"><img src="../Images/f39215bdb663f5b3ed8ecd87ed8501e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/0*3sdkK6Qh9R-b-COe.png"/></div></figure><p id="2b1c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">给定独热编码向量的结构，不同单词之间的相似度总是为0。</p><p id="984a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Word2Vec通过为我们提供一个固定长度(通常比词汇表大小小得多)的单词向量表示来克服上述困难。它还捕捉不同单词之间的相似性和类比关系。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es ld"><img src="../Images/a789fb5ddd35f9fe10011828c7ff4761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*v39ZHU2lGS16FJBg.png"/></div></div></figure><p id="7f86" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">单词向量的学习方式允许我们学习不同的类比。这使我们能够对单词进行代数运算，这在以前是不可能的。</p><p id="e2f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">比如:什么是王者——男人+女人？结果是皇后。</p><p id="a96b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Word2Vec向量也帮助我们找到单词之间的相似性。如果我们寻找与“好”相似的词，我们会发现很棒，很棒，等等。word2vec的这一特性使得它在文本分类中具有不可估量的价值。有了这个，我们的深度学习网络就明白了“好”和“伟大”是意思相近的词。</p><p id="cd11" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">简单地说，word2vec为单词创建了固定长度的向量，为字典中的每个单词(和常见的二元模型)提供了一个d维向量。</p><p id="9f52" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些词向量通常是预先训练好的，并由其他人在像维基百科、推特等大型文本语料库上训练后提供。最常用的预训练词向量有<a class="ae kn" href="https://www.kaggle.com/takuok/glove840b300dtxt" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">手套</strong> </a>和300维词向量的快速文本。在这篇文章中，我们将使用手套词向量。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="b0a9" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">数据预处理</h1><p id="c93e" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">在大多数情况下，文本数据并不完全干净。来自不同来源的数据具有不同的特征，这使得文本预处理成为分类管道中最关键的步骤之一。例如，来自Twitter的文本数据不同于在Quora或其他新闻/博客平台上找到的文本数据，每种数据都需要区别对待。然而，我们将在这篇文章中讨论的技术对于你在NLP的丛林中可能遇到的几乎任何类型的数据都足够通用。</p><h1 id="a095" class="jk jl hh bd jm jn le jp jq jr lf jt ju jv lg jx jy jz lh kb kc kd li kf kg kh bi translated">a)清除特殊字符并删除标点符号</h1><p id="1dba" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">我们的预处理管道很大程度上依赖于我们将用于分类任务的word2vec嵌入。原则上，我们的预处理应该匹配在训练单词嵌入之前使用的预处理。由于大多数嵌入没有为标点符号和其他特殊字符提供向量值，所以我们要做的第一件事就是去掉文本数据中的特殊字符。</p><pre class="kp kq kr ks fd lj lk ll lm aw ln bi"><span id="9a41" class="lo jl hh lk b fi lp lq l lr ls"># Some preprocesssing that will be common to all the text classification methods you will see.</span><span id="29a0" class="lo jl hh lk b fi lt lq l lr ls">import re</span><span id="2b60" class="lo jl hh lk b fi lt lq l lr ls">def clean_text(x):<br/>    pattern = r'[^a-zA-z0-9\s]'<br/>    text = re.sub(pattern, '', x)<br/>    return x</span></pre><h1 id="6d59" class="jk jl hh bd jm jn le jp jq jr lf jt ju jv lg jx jy jz lh kb kc kd li kf kg kh bi translated">b)清洁编号</h1><p id="cc63" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">为什么我们要用#s代替数字？因为包括Glove在内的大多数嵌入都以这种方式对其文本进行了预处理。</p><p id="91d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">小Python绝招:</em> </strong>我们在下面的代码中使用了一个if语句来预先检查文本中是否存在数字，因为if总是比re.sub命令快，而且我们的大部分文本都不包含数字。</p><pre class="kp kq kr ks fd lj lk ll lm aw ln bi"><span id="c4ba" class="lo jl hh lk b fi lp lq l lr ls">def clean_numbers(x):<br/>    if bool(re.search(r'\d', x)):<br/>        x = re.sub('[0-9]{5,}', '#####', x)<br/>        x = re.sub('[0-9]**{4}**', '####', x)<br/>        x = re.sub('[0-9]**{3}**', '###', x)<br/>        x = re.sub('[0-9]**{2}**', '##', x)<br/>    return x</span></pre><h1 id="5421" class="jk jl hh bd jm jn le jp jq jr lf jt ju jv lg jx jy jz lh kb kc kd li kf kg kh bi translated">c)消除收缩</h1><p id="2404" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">缩写是我们用撇号写的单词。缩写的例子是像“不是”或“不是”这样的词。因为我们想标准化我们的文本，所以扩展这些缩写是有意义的。下面我们使用收缩映射和正则表达式函数来完成。</p><pre class="kp kq kr ks fd lj lk ll lm aw ln bi"><span id="6725" class="lo jl hh lk b fi lp lq l lr ls">contraction_dict = {"ain't": "is not", "aren't": "are not","can't": "cannot", "'cause": "because", "could've": "could have"}</span><span id="0b55" class="lo jl hh lk b fi lt lq l lr ls">def _get_contractions(contraction_dict):<br/>    contraction_re = re.compile('(**%s**)' % '|'.join(contraction_dict.keys()))<br/>    return contraction_dict, contraction_re<br/>contractions, contractions_re = _get_contractions(contraction_dict)</span><span id="4a7a" class="lo jl hh lk b fi lt lq l lr ls">def replace_contractions(text):<br/>    def replace(match):<br/>        return contractions[match.group(0)]<br/>    return contractions_re.sub(replace, text)</span><span id="e9cf" class="lo jl hh lk b fi lt lq l lr ls"># Usage<br/>replace_contractions("this's a text with contraction")</span></pre><p id="ba38" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">除了以上技巧，你可能还想做拼写纠正。但是因为我们的帖子已经很长了，我们现在就离开它。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="162d" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">数据表示:序列创建</h1><p id="8203" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">使深度学习成为NLP的首选的一件事是，我们不必从我们的文本数据中手工设计特征；深度学习算法将一系列文本作为输入，像人类一样学习其结构。由于机器不能理解文字，它们希望数据是数字形式的。因此，我们需要将文本数据表示为一系列数字。</p><p id="5e57" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了理解这是如何做到的，我们需要了解一些关于Keras Tokenizer函数的知识。其他记号赋予器也是可行的，但是Keras记号赋予器对我来说是个不错的选择。</p><h1 id="c91b" class="jk jl hh bd jm jn le jp jq jr lf jt ju jv lg jx jy jz lh kb kc kd li kf kg kh bi translated">a)记号赋予器</h1><p id="7497" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">简而言之，记号赋予器是一个将句子拆分成单词的实用函数。keras . preprocessing . text . tokenizer将文本标记化(拆分)为标记(单词)，同时只保留文本语料库中出现次数最多的单词。</p><pre class="kp kq kr ks fd lj lk ll lm aw ln bi"><span id="fe8c" class="lo jl hh lk b fi lp lq l lr ls">#Signature:<br/>Tokenizer(num_words=None, filters='!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n',<br/>lower=True, split=' ', char_level=False, oov_token=None, document_count=0, **kwargs)</span></pre><p id="640a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">num_words参数只保留文本中预先指定数量的单词。这是有帮助的，因为我们不希望我们的模型因为考虑不经常出现的单词而受到很多干扰。在真实世界的数据中，我们使用num_words参数留下的大多数单词通常都是拼写错误的单词。缺省情况下，记号赋予器还会过滤掉一些不想要的记号，并将文本转换成小写。</p><p id="c08b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦适合数据，tokenizer还保留一个单词索引(一个我们可以用来为单词分配唯一编号的字典)，可以通过tokenizer.word_index访问。索引词典中的单词按出现频率排列。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es ca"><img src="../Images/61d38d1656d5507272ed280f3f87a80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*c1o1ff1yupRvTLkY.png"/></div></div></figure><p id="27bf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以使用记号赋予器的全部代码如下:</p><pre class="kp kq kr ks fd lj lk ll lm aw ln bi"><span id="8f18" class="lo jl hh lk b fi lp lq l lr ls">from keras.preprocessing.text import Tokenizer</span><span id="1130" class="lo jl hh lk b fi lt lq l lr ls">## Tokenize the sentences<br/>tokenizer = Tokenizer(num_words=max_features)<br/>tokenizer.fit_on_texts(list(train_X)+list(test_X))<br/>train_X = tokenizer.texts_to_sequences(train_X)<br/>test_X = tokenizer.texts_to_sequences(test_X)</span></pre><p id="b756" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中train_X和test_X是语料库中的文档列表。</p><h1 id="2e78" class="jk jl hh bd jm jn le jp jq jr lf jt ju jv lg jx jy jz lh kb kc kd li kf kg kh bi translated">b)填充序列</h1><p id="a621" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">通常，我们的模型期望每个文本序列(每个训练示例)具有相同的长度(相同数量的单词/标记)。我们可以使用maxlen参数对此进行控制。</p><p id="2a22" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如:</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es lu"><img src="../Images/569ef30111231e3f9db078b49c33fbc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/0*KL-8g0HlN6tnlMDk.png"/></div></figure><pre class="kp kq kr ks fd lj lk ll lm aw ln bi"><span id="5067" class="lo jl hh lk b fi lp lq l lr ls">train_X = pad_sequences(train_X, maxlen=maxlen)<br/>test_X = pad_sequences(test_X, maxlen=maxlen)</span></pre><p id="20ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们的训练数据包含了一个数字列表。每个列表都有相同的长度。我们还有word_index，它是文本语料库中出现最多的单词的字典。</p><h1 id="d757" class="jk jl hh bd jm jn le jp jq jr lf jt ju jv lg jx jy jz lh kb kc kd li kf kg kh bi translated">c)编码目标变量的标签</h1><p id="a2e3" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">Pytorch模型期望目标变量是一个数字，而不是一个字符串。我们可以使用sklearn的标签编码器来转换我们的目标变量。</p><pre class="kp kq kr ks fd lj lk ll lm aw ln bi"><span id="9e63" class="lo jl hh lk b fi lp lq l lr ls">from sklearn.preprocessing import LabelEncoder<br/>le = LabelEncoder()<br/>train_y = le.fit_transform(train_y.values)<br/>test_y = le.transform(test_y.values)</span></pre></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="e625" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">负载嵌入</h1><p id="1bb3" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">首先，我们需要加载所需的手套嵌入。</p><pre class="kp kq kr ks fd lj lk ll lm aw ln bi"><span id="e263" class="lo jl hh lk b fi lp lq l lr ls">def load_glove(word_index):<br/>    EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'<br/>    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]<br/>    embeddings_index = dict(get_coefs(*o.split(" ")) for o in   open(EMBEDDING_FILE))</span><span id="7343" class="lo jl hh lk b fi lt lq l lr ls">    all_embs = np.stack(embeddings_index.values())<br/>    emb_mean,emb_std = -0.005838499,0.48782197<br/>    embed_size = all_embs.shape[1]</span><span id="7c74" class="lo jl hh lk b fi lt lq l lr ls">nb_words = min(max_features, len(word_index)+1)<br/>    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))<br/>    for word, i in word_index.items():<br/>        if i &gt;= max_features: continue<br/>        embedding_vector = embeddings_index.get(word)<br/>        if embedding_vector is not None:<br/>            embedding_matrix[i] = embedding_vector<br/>        else:<br/>            embedding_vector = embeddings_index.get(word.capitalize())<br/>            if embedding_vector is not None:<br/>                embedding_matrix[i] = embedding_vector<br/>    return embedding_matrix</span><span id="a7a4" class="lo jl hh lk b fi lt lq l lr ls">embedding_matrix = load_glove(tokenizer.word_index)</span></pre><p id="e4ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一定要把下载这些手套向量的文件夹路径放进去。嵌入索引包含什么？这是一个字典，其中的键是单词，值是单词向量，一个长度为300的np.array。这部词典的长度大约是十亿。</p><p id="8a93" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为我们只需要word_index中单词的嵌入，所以我们将使用我们的tokenizer中的word index创建一个只包含所需嵌入的矩阵。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lb"><img src="../Images/46b81afd11f261f8df7d2c21e8a07671.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rAgNKeRR6Dm_PR0b.png"/></div></div></figure></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="15c7" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">深度学习模型</h1><h1 id="1558" class="jk jl hh bd jm jn le jp jq jr lf jt ju jv lg jx jy jz lh kb kc kd li kf kg kh bi translated">1.TextCNN</h1><p id="e8ca" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">使用CNN对文本进行分类的想法首先在Yoon Kim的论文<a class="ae kn" href="https://www.aclweb.org/anthology/D14-1181" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"/></a>中提出。</p><p id="8505" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">表示:这个想法的中心概念是把我们的文档看作图像。但是怎么做呢？假设我们有一个句子，我们有maxlen = 70，嵌入大小= 300。我们可以创建一个形状为70×300的数字矩阵来表示这个句子。图像也有一个矩阵，其中各个元素是像素值。但是任务的输入不是图像像素，而是用矩阵表示的句子或文档。矩阵的每一行对应一个单词向量。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lb"><img src="../Images/ae181ced0e61ac848c79ff94aed38cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IHt184Toyx7CTaPs.png"/></div></div></figure><p id="e730" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">卷积思想:对于图像，我们移动我们的conv。水平和垂直过滤，但对于文本，我们将内核大小固定为filter_size x embed_size，即(3，300)，我们将垂直向下移动卷积，同时查看三个单词，因为我们在这种情况下的过滤器大小为3。这个想法似乎是正确的，因为我们的卷积滤波器不分裂字嵌入；它会查看每个单词的完整嵌入。此外，我们可以将过滤器的大小想象成一元、二元、三元等。因为我们正在分别查看1、2、3和5个单词的上下文窗口。</p><p id="14ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里是CNN网络编码在<a class="ae kn" href="https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79%EF%BF%BD" rel="noopener" target="_blank"> <strong class="ig hi"> Pytorch </strong> </a>的文本分类。</p><pre class="kp kq kr ks fd lj lk ll lm aw ln bi"><span id="34bd" class="lo jl hh lk b fi lp lq l lr ls">class CNN_Text(nn.Module):    <br/>    def __init__(self):<br/>        super(CNN_Text, self).__init__()<br/>        filter_sizes = [1,2,3,5]<br/>        num_filters = 36<br/>        n_classes = len(le.classes_)<br/>        self.embedding = nn.Embedding(max_features, embed_size)<br/>        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))<br/>        self.embedding.weight.requires_grad = False<br/>        self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])<br/>        self.dropout = nn.Dropout(0.1)<br/>        self.fc1 = nn.Linear(len(filter_sizes)*num_filters, n_classes)</span><span id="53c4" class="lo jl hh lk b fi lt lq l lr ls">def forward(self, x):<br/>        x = self.embedding(x)  <br/>        x = x.unsqueeze(1)  <br/>        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]<br/>        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  <br/>        x = torch.cat(x, 1)<br/>        x = self.dropout(x)  <br/>        logit = self.fc1(x)<br/>        return logit</span></pre><h1 id="42d8" class="jk jl hh bd jm jn le jp jq jr lf jt ju jv lg jx jy jz lh kb kc kd li kf kg kh bi translated">2.双向RNN (LSTM/GRU)</h1><p id="88de" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">TextCNN很适合文本分类，因为它处理近距离的单词。比如它可以一起看《纽约》。然而，它仍然不能处理特定文本序列中提供的所有上下文。它仍然没有学习数据的顺序结构，其中每个单词都依赖于前一个单词或前一个句子中的一个单词。</p><p id="cf3f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">RNNs可以帮助我们。他们可以使用隐藏状态记住以前的信息，并将其与当前任务联系起来。</p><p id="fd6b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">长短期记忆网络(LSTM)是RNN的一个子类，专门用于长时间记忆信息。此外，双向LSTM保持了两个方向的上下文信息，这在文本分类任务中非常有用(但是，它不适用于时间序列预测任务，因为在这种情况下我们无法看到未来)。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lv"><img src="../Images/86acf317b998f27d90ae83859ae756c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gJ75tCL2D6zFLU09.png"/></div></div></figure><p id="1f32" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了简单解释双向RNN，可以把RNN单元想象成一个黑盒，它接受一个隐藏状态(向量)和一个字向量作为输入，给出一个输出向量和下一个隐藏状态。这个盒子有一些权重需要使用损耗的反向传播来调整。此外，相同的单元格应用于所有单词，以便句子中的单词共享权重。这种现象被称为重量共享。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es lw"><img src="../Images/83e3a26dff38c4534155877273d915a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/0*L0pcA6jEQYpmeIN0.png"/></div></figure><p id="13b1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">隐藏状态，单词向量--&gt;(RNN单元格)-&gt;输出向量，下一个隐藏状态</p><p id="de72" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于像“你永远不会相信”这样的长度为4的序列，RNN单元给出4个输出向量，这些向量可以连接起来，然后用作密集前馈架构的一部分。</p><p id="2989" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在双向RNN中，唯一的变化是我们以通常的方式以及相反的方式阅读文本。所以我们并行堆叠两个rnn，我们得到8个输出向量来附加。</p><p id="86f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦我们得到了输出向量，我们将它们通过一系列密集层，最后是softmax层，以构建文本分类器。</p><p id="feae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在大多数情况下，您需要了解如何在神经网络中堆叠一些层以获得最佳结果。如果性能更好，我们可以在网络中尝试多个双向GRU/LSTM层。</p><p id="806e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于RNNs的局限性，例如不记得长期依赖关系，在实践中，我们几乎总是使用LSTM/GRU来建模长期依赖关系。在这种情况下，您可以将上图中的RNN单元格想象为LSTM单元格或GRU单元格。</p><p id="64a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是Pytorch中用于该网络的一些代码:</p><pre class="kp kq kr ks fd lj lk ll lm aw ln bi"><span id="f03d" class="lo jl hh lk b fi lp lq l lr ls">class BiLSTM(nn.Module):<br/>    def __init__(self):<br/>        super(BiLSTM, self).__init__()<br/>        self.hidden_size = 64<br/>        drp = 0.1<br/>        n_classes = len(le.classes_)<br/>        self.embedding = nn.Embedding(max_features, embed_size)<br/>        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))<br/>        self.embedding.weight.requires_grad = False<br/>        self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True)<br/>        self.linear = nn.Linear(self.hidden_size*4 , 64)<br/>        self.relu = nn.ReLU()<br/>        self.dropout = nn.Dropout(drp)<br/>        self.out = nn.Linear(64, n_classes)<br/></span><span id="fba6" class="lo jl hh lk b fi lt lq l lr ls">    def forward(self, x):<br/>        *#rint(x.size())*<br/>        h_embedding = self.embedding(x)<br/>        *#_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))*<br/>        h_lstm, _ = self.lstm(h_embedding)<br/>        avg_pool = torch.mean(h_lstm, 1)<br/>        max_pool, _ = torch.max(h_lstm, 1)<br/>        conc = torch.cat(( avg_pool, max_pool), 1)<br/>        conc = self.relu(self.linear(conc))<br/>        conc = self.dropout(conc)<br/>        out = self.out(conc)<br/>        return out</span></pre></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="7412" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">培养</h1><p id="3c71" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">下面是我们用来训练BiLSTM模型的代码。代码注释的很好，请大家通读代码理解。你可能也想看看我在Pytorch  上的帖子。</p><pre class="kp kq kr ks fd lj lk ll lm aw ln bi"><span id="650c" class="lo jl hh lk b fi lp lq l lr ls">n_epochs = 6<br/>model = BiLSTM()<br/>loss_fn = nn.CrossEntropyLoss(reduction='sum')<br/>optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)<br/>model.cuda()</span><span id="a3b9" class="lo jl hh lk b fi lt lq l lr ls"># Load train and test in CUDA Memory<br/>x_train = torch.tensor(train_X, dtype=torch.long).cuda()<br/>y_train = torch.tensor(train_y, dtype=torch.long).cuda()<br/>x_cv = torch.tensor(test_X, dtype=torch.long).cuda()<br/>y_cv = torch.tensor(test_y, dtype=torch.long).cuda()</span><span id="0099" class="lo jl hh lk b fi lt lq l lr ls"># Create Torch datasets<br/>train = torch.utils.data.TensorDataset(x_train, y_train)<br/>valid = torch.utils.data.TensorDataset(x_cv, y_cv)</span><span id="c934" class="lo jl hh lk b fi lt lq l lr ls"># Create Data Loaders<br/>train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)<br/>valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)</span><span id="e524" class="lo jl hh lk b fi lt lq l lr ls">train_loss = []<br/>valid_loss = []</span><span id="95e7" class="lo jl hh lk b fi lt lq l lr ls">for epoch in range(n_epochs):<br/>    start_time = time.time()<br/>    # Set model to train configuration<br/>    model.train()<br/>    avg_loss = 0.  <br/>    for i, (x_batch, y_batch) in enumerate(train_loader):<br/>        # Predict/Forward Pass<br/>        y_pred = model(x_batch)<br/>        # Compute loss<br/>        loss = loss_fn(y_pred, y_batch)<br/>        optimizer.zero_grad()<br/>        loss.backward()<br/>        optimizer.step()<br/>        avg_loss += loss.item() / len(train_loader)</span><span id="3cdf" class="lo jl hh lk b fi lt lq l lr ls">    # Set model to validation configuration -Doesn't get trained here<br/>    model.eval()        <br/>    avg_val_loss = 0.<br/>    val_preds = np.zeros((len(x_cv),len(le.classes_)))</span><span id="c3d5" class="lo jl hh lk b fi lt lq l lr ls">    for i, (x_batch, y_batch) in enumerate(valid_loader):<br/>        y_pred = model(x_batch).detach()<br/>        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)<br/>        # keep/store predictions<br/>        val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred).cpu().numpy()</span><span id="3463" class="lo jl hh lk b fi lt lq l lr ls">    # Check Accuracy<br/>    val_accuracy = sum(val_preds.argmax(axis=1)==test_y)/len(test_y)<br/>    train_loss.append(avg_loss)<br/>    valid_loss.append(avg_val_loss)<br/>    elapsed_time = time.time() - start_time<br/>    print('Epoch {}/{} \t loss={:.4f} \t val_loss={:.4f}  \t val_acc={:.4f}  \t time={:.2f}s'.format(<br/>                epoch + 1, n_epochs, avg_loss, avg_val_loss, val_accuracy, elapsed_time))</span></pre><p id="96c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">培训输出如下所示:</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lx"><img src="../Images/fb83c40959a3cbc431acd76773408528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ef9vSLDbvC7XIlry.png"/></div></div></figure><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es ly"><img src="../Images/e940cd54099554518a54bd83b1026bc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rnvCwnDZkKryFzyH.png"/></div></div></figure></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="cf0b" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">结果/预测</h1><pre class="kp kq kr ks fd lj lk ll lm aw ln bi"><span id="5aae" class="lo jl hh lk b fi lp lq l lr ls">import scikitplot as skplt<br/>y_true = [le.classes_[x] for x **in** test_y]<br/>y_pred = [le.classes_[x] for x **in** val_preds.argmax(axis=1)]<br/>skplt.metrics.plot_confusion_matrix(<br/>    y_true,<br/>    y_pred,<br/>    figsize=(12,12),x_tick_rotation=90)</span></pre><p id="cfc2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是BiLSTM模型结果的混淆矩阵。我们可以看到，我们的模型做得相当好，在验证数据集上有87%的准确率。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lz"><img src="../Images/b0bb71943efa594b8c10aecd9cfc6cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FzWdfY--7qSrW1zJ.png"/></div></div></figure><p id="c2b3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有趣的是，即使在模型表现不佳的地方，这也是可以理解的。例如，该模型混淆了减肥和肥胖、抑郁和焦虑、抑郁和双相情感障碍。我不是专家，但是这些病确实感觉挺像的。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="718d" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">结论</h1><p id="a88f" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">在这篇文章中，我们介绍了用于文本分类的深度学习架构，如LSTM和比尔斯特姆，并解释了NLP深度学习中使用的不同步骤。</p><p id="989d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">仍然有很多可以做的来改进这个模型的性能。改变学习速率、使用学习速率表、使用额外的特征、丰富嵌入、去除拼写错误等。我希望这段样板代码能为您可能面临的任何文本分类问题提供一个参考基准。</p><p id="2fa1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢阅读！将来我也会写更多的<strong class="ig hi"> NLP文章</strong>。<a class="ae kn" rel="noopener" href="/@vijay_choubey"> <strong class="ig hi">跟着</strong> </a>我上来了解他们。我也是一名自由职业者，如果有一些数据相关项目的自由职业工作，请随时通过<a class="ae kn" href="https://www.linkedin.com/in/vijay-choubey-3bb471148/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> Linkedin </strong> </a>联系。没有什么比做真正的项目更好的了！</p></div></div>    
</body>
</html>