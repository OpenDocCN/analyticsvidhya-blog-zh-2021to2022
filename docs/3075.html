<html>
<head>
<title>Polynomial Regression In Python. — ML For Lazy 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的多项式回归。— ML代表懒惰2021</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/polynomial-regression-in-python-ml-for-lazy-2021-84d5bb4346ed?source=collection_archive---------4-----------------------#2021-06-03">https://medium.com/analytics-vidhya/polynomial-regression-in-python-ml-for-lazy-2021-84d5bb4346ed?source=collection_archive---------4-----------------------#2021-06-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/205156841e3d5d34bc0c9ea986e4b58f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N111RwvYTmdl8qhs9nYdMw.png"/></div></div></figure><p id="ee1b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">多项式回归是指简单的直线不能很好地拟合所有数据的回归。这是什么意思-这意味着输入要素和输出要素没有线性关系。</p><p id="5578" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们知道的最简单的线性关系是——Y = mX+c，这恰好也是一条简单直线的方程。</p><p id="ae2d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在许多情况下，数据不服从这个方程，我们不得不转向或倾向于一些比直线方程更复杂的其它方程。某些数据服从二次方程，而有些数据可能倾向于三次方程。在这些情况下，我们不得不将简单的线性回归放在一边，让它变得更加复杂，以适应这种数据类型。使用这些方程，我们得到最佳拟合线，不是直线，而是曲线或S形。</p><p id="8600" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">随着次数的增加，我们得到了很多曲线，这是这里要掌握的另一个概念。简而言之，就像从二次方程到三次方程到四次方程的转变一样，我们得到了更复杂的关系以及对不具有线性关系(即直线关系)的数据的更复杂的拟合。</p><h1 id="d342" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">一些重要的概念。</h1><h1 id="4c4d" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">多项式。</h1><p id="cf24" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">多项式是一个方程，其中我们有一些未知的变量，整个表达式是该变量中小表达式的总和。这些变量有不同的功效。</p><h2 id="fd82" class="kq jo hh bd jp kr ks kt jt ku kv kw jx ja kx ky kb je kz la kf ji lb lc kj ld bi translated">多项式的次数。</h2><p id="2462" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">多项式的次数被定义为该多项式的最高次幂。这是一个简单的概念，但在理解多项式回归和作为一个整体的回归时，却是一个引人注目的重要概念。</p><p id="a2e2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设，我有一个多项式— y = ax + bx + cx + d</p><p id="faa8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后，我的多项式的最高次数是3，它的次数是3。这样，我们就定义了多项式的次数。</p><p id="3fc8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里有一个重要的概念，就是根。它意味着这个多项式的解。</p><blockquote class="le lf lg"><p id="a18b" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm ha bi translated"><em class="hh">任意多项式的根/解的个数等于该多项式的次数。</em></p></blockquote><p id="ace4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这意味着，二次方程有两个根，三次方程有三个根，等等。</p><h2 id="633a" class="kq jo hh bd jp kr ks kt jt ku kv kw jx ja kx ky kb je kz la kf ji lb lc kj ld bi translated">几种常见的多项式图形</h2><figure class="lm ln lo lp fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ll"><img src="../Images/9586a2b44de527a802d928d4b57d82ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kvisrHqY1TFKjbzx.jpg"/></div></div></figure><h1 id="ccd9" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">什么是多项式回归？</h1><p id="8312" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">现在我们对多项式及其次数有了一个概念。如何知道多项式的次数，以及该多项式有多少根，即解。现在是时候了解多项式回归本身了。</p><p id="f9c8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">伟大的维基百科说，</p><blockquote class="le lf lg"><p id="05ff" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm ha bi translated"><em class="hh">在统计学中，</em> <strong class="ir hi"> <em class="hh">多项式回归</em> </strong> <em class="hh">是</em> <strong class="ir hi"> <em class="hh">回归</em> </strong> <em class="hh">分析的一种形式，其中自变量x与因变量y之间的关系建模为x中的n次</em> <strong class="ir hi"> <em class="hh">多项式</em></strong><em class="hh"/></p><p id="567c" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm ha bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Polynomial_regression#:~:text=In%20statistics%2C%20polynomial%20regression%20is,nth%20degree%20polynomial%20in%20x." rel="noopener ugc nofollow" target="_blank"><em class="hh"/>Wikipedia.com</a></p></blockquote><p id="af79" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">简而言之，这意味着当我们的数据不是使用简单的线性方程(即y = mx + c)来近似时，我们需要一个更复杂的高阶方程。上图是多项式次数增加的结果。</p><h1 id="b3fd" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">为什么我们需要多项式回归？</h1><p id="89c0" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">假设我们有一个数据-</p><figure class="lm ln lo lp fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/d7aa814e7d3f024dccdf446f4e5fcbe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/0*hWHKXE9OlRYDKKKa.png"/></div></figure><p id="f864" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在很清楚，如果我们通过这些数据画一条简单的线，这样的线在近似数据点方面不会做很大的工作。让我看看-</p><figure class="lm ln lo lp fd ii er es paragraph-image"><div class="er es ls"><img src="../Images/55e0a847d88c8739eb2e96359585263f.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/0*P2ijGbOTZJrZed57.png"/></div></figure><p id="17eb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这种情况下，如果我们有这种数据以及独立数据和从属数据之间的关系，我们需要更复杂的多项式来进行更好的近似。举个这样的例子，用一个2次多项式。</p><figure class="lm ln lo lp fd ii er es paragraph-image"><div class="er es lt"><img src="../Images/30cc8a84cf2f6b074c29ef6b2340071a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*15ktcrhWw00roe5N.png"/></div></figure><h1 id="c9de" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">如何实现多项式回归？</h1><p id="8733" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">现在我们已经了解了多项式回归是如何工作的，以及为什么我们需要进行多项式回归。我们现在需要做的是用代码实现它；正如我们在多项式回归的幕后所看到的，现在我们已经准备好编写代码了。当我们编写代码时，我们将了解它如何工作以及为什么工作的每个细节，以及它如何以这种方式工作，它如何以这种方式工作，以及它为什么以这种方式工作。所以让我们开始吧。首先，我们需要导入库，我们将导入NumPy Pandas和matplotlib，所以让我们导入它们。</p><pre class="lm ln lo lp fd lu lv lw lx aw ly bi"><span id="e84e" class="kq jo hh lv b fi lz ma l mb mc">import numpy as np import pandas as pd import matplotlib.pyplot as plt</span></pre><p id="82d5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在是时候将数据加载到我们的环境中了。所以让我们利用熊猫图书馆来做这件事。</p><pre class="lm ln lo lp fd lu lv lw lx aw ly bi"><span id="c185" class="kq jo hh lv b fi lz ma l mb mc">path = 'path' data = pd.read_csv(path) data.head()</span></pre><p id="d1f9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们的数据看起来像这样-</p><figure class="lm ln lo lp fd ii er es paragraph-image"><div class="er es md"><img src="../Images/e6b4eb189eb32dcb8f28f9cf68ece3ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/0*zkQZ4eF7pP9AHvvF.png"/></div></figure><p id="bcb8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在要做的事情是获取输入特征和输出特征。我们可以简单地得到它们，然后用点(。)值将它们转换成numpy数组。这里需要注意的是，我们只需要level列和salary列。</p><pre class="lm ln lo lp fd lu lv lw lx aw ly bi"><span id="95c2" class="kq jo hh lv b fi lz ma l mb mc">X = data.iloc[:, 1:-1].values y = data.iloc[:,-1].values</span></pre><p id="09cd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">绘制散点图看起来像这样-</p><figure class="lm ln lo lp fd ii er es paragraph-image"><div class="er es me"><img src="../Images/7f13f548ae080c3d6fc00648d5b2a515.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/0*q0F7jGjlpcNZqULt.png"/></div></figure><p id="4adb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们研究一下简单的回归模型，看看这条线是如何拟合的。</p><pre class="lm ln lo lp fd lu lv lw lx aw ly bi"><span id="d834" class="kq jo hh lv b fi lz ma l mb mc">from sklearn.linear_model import LinearRegression model1 = LinearRegression() model1.fit(X, y) plt.scatter(X, y) plt.plot(X, model1.predict(X), 'r') plt.title("Linear Model")</span></pre><figure class="lm ln lo lp fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/185493b2500612c8bdaaf98f52fe35c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/0*eXh6MlDDSegIrM6m.png"/></div></figure><p id="b00b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">ooo…这款看起来没那么棒，肯定不是最适合的。</p><p id="42cf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们需要做的是创建一定次数的多项式特征。这很简单，可以用sklearn这样做</p><pre class="lm ln lo lp fd lu lv lw lx aw ly bi"><span id="5a34" class="kq jo hh lv b fi lz ma l mb mc">from sklearn.preprocessing import PolynomialFeatures xpoly = PolynomialFeatures(degree=2) xpolyFeat = xpoly.fit_transform(X)</span></pre><p id="9148" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，如果我们看看xpolyFeat，这个特征向量有每个数据点— 1，n，n</p><figure class="lm ln lo lp fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/63c1bfaed209b8c1ce6e7e0234822b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/0*9ylMKdRPvJ9v5B1x.png"/></div></figure><p id="45c0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们看看，如果按照这些特征训练，模型会发生什么</p><pre class="lm ln lo lp fd lu lv lw lx aw ly bi"><span id="388a" class="kq jo hh lv b fi lz ma l mb mc">polyModel1 = LinearRegression() polyModel1.fit(xpolyFeat, y) plt.scatter(X, y) plt.plot(X, polyModel1.predict(xpolyFeat), 'g')</span></pre><figure class="lm ln lo lp fd ii er es paragraph-image"><div class="er es mh"><img src="../Images/b2ee2996f592cfb9645313d10bcb2114.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/0*SrP5ctjkXabDPSaL.png"/></div></figure><p id="b429" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该模型远优于之前基于相同数据训练的简单线性回归模型。如果我们继续增加度数，我们会越来越好地适应数据。让我们看看3度是什么样子的-</p><figure class="lm ln lo lp fd ii er es paragraph-image"><div class="er es mi"><img src="../Images/21f484e31de4819f02935cc5d9029f84.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/0*xdWIqNXZa9VTk1TB.png"/></div></figure><p id="9950" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">就这样，我们越来越契合。</p><h1 id="cac1" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结论</h1><p id="1757" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">这样，我们在Python中实现了多项式回归。</p><p id="18d2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，我们回答了什么是多项式回归以及我们为什么需要它的问题。现在让我们站起来，用代码弄脏我们的手，将多项式回归粘贴到头脑中。</p><p id="b477" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你喜欢这篇文章，并真正理解了它，请与你的朋友分享，享受学习的乐趣。</p><p id="17d5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">想看我之前的帖子，看这个— <a class="ae lq" href="https://mlforlazy.in/how-to-generate-mfcc-from-audio/" rel="noopener ugc nofollow" target="_blank">如何从音频生成MFCC。</a></p><figure class="lm ln lo lp fd ii er es paragraph-image"><div class="er es mj"><img src="../Images/1c47796f5ef210aef2c5a5597fec1a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:192/0*2hNvlQ2xFOocgssF"/></div></figure><p id="08a3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我是一名来自克什米尔的计算机科学研究生。在这些日子里，我转向传播关于机器学习的信息，这是我的激情和未来的研究。目的是让人们了解和理解机器和深度学习本身的基本概念，这些概念对该领域的进一步成功至关重要。</p></div><div class="ab cl mk ml go mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ha hb hc hd he"><p id="90cc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="lh">原载于2021年6月3日</em><a class="ae lq" href="https://mlforlazy.in/polynomial-regression-in-python/" rel="noopener ugc nofollow" target="_blank"><em class="lh">https://mlforlazy . in</em></a><em class="lh">。</em></p></div></div>    
</body>
</html>