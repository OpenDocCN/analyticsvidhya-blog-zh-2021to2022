<html>
<head>
<title>K-Nearest Neighbor with Scratch (KNN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">有划痕的k-最近邻(KNN)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-nearest-neighbor-with-scratch-knn-75bf088729b?source=collection_archive---------5-----------------------#2021-01-09">https://medium.com/analytics-vidhya/k-nearest-neighbor-with-scratch-knn-75bf088729b?source=collection_archive---------5-----------------------#2021-01-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/cf3c06e2d0552806706f1574a6518d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*LewEF-6BpWhyQWHnGP134A.png"/></div></figure><p id="66d4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你好，欢迎来到我们的<strong class="io hj">带划痕的K近邻</strong>，我们带划痕的机器学习系列的第三篇文章。在本文中，我们将和您一起研究并编写K-最近邻算法。</p><p id="08f5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在编码我们的算法时，我们将使用<strong class="io hj">虹膜数据集</strong>作为数据集。Iris数据集是分类问题的理想数据集。您可以在此 或在<strong class="io hj"> sklearn库</strong>中访问数据集<a class="ae jk" href="https://www.kaggle.com/uciml/iris" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj">。下面你可以看到如何将数据集导入你的代码。</strong></a></p><blockquote class="jl jm jn"><p id="4952" class="im in jo io b ip iq ir is it iu iv iw jp iy iz ja jq jc jd je jr jg jh ji jj hb bi translated"><strong class="io hj">从</strong> <strong class="io hj"> sklearn </strong> <strong class="io hj">导入</strong>数据集<br/> iris = datasets.load_iris()</p></blockquote><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es js"><img src="../Images/85bd0d71245e5b35f2b167aff4248873.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/0*BwKtz5FHVZFKQwUv.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">df.describe()</figcaption></figure><h2 id="8d7e" class="kb kc hi bd kd ke kf kg kh ki kj kk kl ix km kn ko jb kp kq kr jf ks kt ku kv bi translated">k-最近的<strong class="ak">邻居</strong></h2><p id="554c" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">使用K-最近邻算法的主要目的是找出我们的分类数据属于哪个类别。<br/>我们的算法<em class="jo">(K-最近邻，或简称KNN)</em>通过查看其在坐标系中最近的邻居来预测数据。</p><h2 id="7b2d" class="kb kc hi bd kd ke kf kg kh ki kj kk kl ix km kn ko jb kp kq kr jf ks kt ku kv bi translated">这意味着什么呢？</h2><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/cd4fc1ceb22cee6a3440f97f7407ebb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/0*Zb3--v3RGd9Z9ktp.png"/></div></figure><p id="5c08" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果我们将数据视为在一个坐标系中，中间(星号)是我们将预测的数据在坐标系中的位置。最接近它的<strong class="io hj"> K </strong>元素告诉我们我们的数据实际上接近哪一个类别。在这幅图中，我们看到对K处理了2个不同的值，3和6，如果K是3，那么在最接近我们可以估计的值的点上有<strong class="io hj"> 1 A和2 B </strong>类成员。通过查看此不等式，我们可以说我们的star数据属于B类。在此图中，我们看到6个变量用于K。对于K的6个值，情况会发生变化，我们的数据与来自<strong class="io hj"> 4 A和2 B </strong>类的数据相邻。在这种情况下，对于我们的星形数据来说，似乎最合理的决策是a类。</p><p id="27bf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当使用我们的算法时，我们的目标是找到最佳K值并最小化误差幅度。</p></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><p id="575f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">简要地；在KNN算法的基础上，有2个不同的基本极点:<strong class="io hj">距离和K </strong>(最近邻数)。正如我在课文前面部分所写的，我们决定用k。</p><h2 id="9544" class="kb kc hi bd kd ke kf kg kh ki kj kk kl ix km kn ko jb kp kq kr jf ks kt ku kv bi translated">那么我们用什么作为距离变量呢？</h2><p id="7cf7" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">作为这个问题的答案，在KNN算法中有多个距离测量公式要使用。这些:</p><ol class=""><li id="52d4" class="lj lk hi io b ip iq it iu ix ll jb lm jf ln jj lo lp lq lr bi translated">闵可夫斯基距离</li><li id="677a" class="lj lk hi io b ip ls it lt ix lu jb lv jf lw jj lo lp lq lr bi translated">曼哈顿距离</li><li id="0bb1" class="lj lk hi io b ip ls it lt ix lu jb lv jf lw jj lo lp lq lr bi translated">欧几里得距离</li><li id="612e" class="lj lk hi io b ip ls it lt ix lu jb lv jf lw jj lo lp lq lr bi translated">余弦距离</li><li id="34d2" class="lj lk hi io b ip ls it lt ix lu jb lv jf lw jj lo lp lq lr bi translated">雅克卡距离</li><li id="b1f5" class="lj lk hi io b ip ls it lt ix lu jb lv jf lw jj lo lp lq lr bi translated">汉娩距</li></ol><p id="1a12" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">像这些有不同的距离测量公式。我们将使用<strong class="io hj">欧几里德距离</strong>，它在我们的代码中经常使用。要观察其他距离度量对代码的影响，我建议您阅读文章<a class="ae jk" href="https://arxiv.org/pdf/1708.04321.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj"> <em class="jo">“距离度量选择对KNN分类器性能的影响——综述”。</em> </strong> </a></p><h1 id="e8b2" class="lx kc hi bd kd ly lz ma kh mb mc md kl me mf mg ko mh mi mj kr mk ml mm ku mn bi translated">欧几里得距离</h1><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/02e8a5550c1c83719ca54e8d1b296d4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/0*Uc2TfI7y6JVfiKnA.png"/></div></figure><p id="4280" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">欧氏距离是KNN算法中最常用的距离类型。它采用几何学中的测距原理作为工作原理。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/8ce6d70b58fa855753b279c9295cc2f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/0*WeKpEg7Lg2cK8Axo.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">欧几里德距离公式</figcaption></figure><h2 id="ae87" class="kb kc hi bd kd ke kf kg kh ki kj kk kl ix km kn ko jb kp kq kr jf ks kt ku kv bi translated">为什么K值很重要？</h2><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es if"><img src="../Images/209e02f9a1713dc645bdc677db07c8ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/0*6l888czd_iC3lEVN.png"/></div></figure><p id="e0a8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">给出了该图中选择的不同K值的误差率。选择接近1且等于1的K值会导致代码中的<strong class="io hj">过拟合</strong>。那么什么是过度拟合呢？过度拟合是一种情况，其中代码对可能通过记忆数据得到的不同数据无能为力。在这种情况下，我们的代码将查看最近的单个数据，并在不分析数据的情况下做出错误的预测。结果，我们的错误率增加了。</p><p id="5c26" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了找到最佳的K值，我们需要在代码中尝试多个K值，并选择最佳的一个。</p></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><h2 id="94b3" class="kb kc hi bd kd ke kf kg kh ki kj kk kl ix km kn ko jb kp kq kr jf ks kt ku kv bi translated">让我们编码:</h2><p id="9fc4" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated"><strong class="io hj"> <em class="jo"> 1。首先，让我们导入必要的库。</em>T13】</strong></p><figure class="jt ju jv jw fd ij"><div class="bz dy l di"><div class="mq mr l"/></div></figure><p id="e737" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">②<em class="jo">。我们上传我们的数据，并在预处理后将它们提供给模型😊</em> </strong></p><figure class="jt ju jv jw fd ij"><div class="bz dy l di"><div class="mq mr l"/></div></figure><p id="3295" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这个代码片段中，在加载我们的数据集之后，我们将它们分成两部分，分别为df和df_test。之后，我们尝试范围(3，10)中的值来确定K值并计算错误分数。</p><p id="35cb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="jo"> 3。我们编码我们的类结构。</em> </strong></p><figure class="jt ju jv jw fd ij"><div class="bz dy l di"><div class="mq mr l"/></div></figure><p id="5056" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们开始用“__init__”函数中的预定义来编写我们的类结构。下一步，我们添加“__distance”函数，并定义我们将在类中使用的欧几里德函数。最后一步，我们将用“预测”函数结束代码。在Predict函数中，我们使用“heapq”结构对测量的距离进行排序。在这里，您可以将距离排序到一个普通的列表中，但是“heapq”结构会对您的代码有所改进。将测得的距离从小到大排序后，通过处理第一个K元素就可以得出你的数据属于哪一类的结果。</p><p id="1e2d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">这篇文章就是这样😊</strong></p><p id="41a3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">要查看完整版代码，请访问我的Github页面:<a class="ae jk" href="https://github.com/capogluuu/ml_scratch" rel="noopener ugc nofollow" target="_blank"><strong class="io hj">https://github.com/capogluuu/ml_scratch</strong></a></p><h1 id="8c8e" class="lx kc hi bd kd ly lz ma kh mb mc md kl me mf mg ko mh mi mj kr mk ml mm ku mn bi translated">谢谢你</h1></div></div>    
</body>
</html>