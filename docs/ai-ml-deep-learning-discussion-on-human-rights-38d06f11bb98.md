# AI、ML 和深度学习关于人权的讨论

> 原文：<https://medium.com/analytics-vidhya/ai-ml-deep-learning-discussion-on-human-rights-38d06f11bb98?source=collection_archive---------21----------------------->

## 采访 [OSCE](https://www.osce.org/fom/ai-free-speech) 谈人工智能的影响

![](img/6343709867b2e6f043945bc9645b497b.png)

我很高兴今天做了一个关于人工智能、人工智能和深度学习模型的影响以及它们如何影响我们的社会的虚拟演讲。视频将在几周内发布，这是今天讨论的文字记录。

## 什么是 AI，机器学习&深度学习？

基于规则的算法自 50 年代以来一直存在，当我们今天谈论人工智能时，我们谈论的主要是机器学习和深度学习模型的部署。当我们使用术语“人工智能”时，我们谈论的是一套试图模仿人类智能的算法和智能。

一般来说，大多数人对人工智能的想法实际上是我们所认为的一般人工智能或强人工智能，但现在所有的人工智能都是狭义的人工智能。关于我们何时能实现通用人工智能还有很多争论，但目前，所有的人工智能都是某种版本的机器学习和深度学习。

简而言之，机器学习向计算机输入数据，并使用统计技术来帮助它“学习”如何逐步更好地完成任务，而无需为该任务专门编程，从而消除了对数百万行书面代码的需求。机器学习包括监督学习(使用标记数据集)和非监督学习(使用未标记数据集)。

深度学习是一种机器学习，通过生物启发的神经网络架构运行输入。神经网络包含许多隐藏层，通过这些隐藏层处理数据，使机器能够“深入”学习，建立连接并加权输入以获得最佳结果。

示例:

*   谷歌搜索
*   图像/面部识别软件
*   Siri、Alexa 和其他个人助理
*   无人驾驶汽车
*   IBM 的沃森

## 人工智能(AI)等机器学习技术正在成为塑造和仲裁在线信息的主要工具。内容审核和内容监管通常是一个自动化的过程，由算法和人工智能决定哪些内容被删除或向谁传播。如果负责任地实施，人工智能可以以各种积极的方式造福社会。然而，也存在这种技术对基本自由产生有害影响的真实风险。为什么这个话题如此重要？

基于精选内容创建的回音室是一个哲学问题:如果我们不断给人们提供泡沫，泡沫什么时候会破裂，这对人们来说会是什么样子。无论是政治分歧、科学否认还是类似 Plandemic、Q-Anon、Social Dillema、The Great Hack 这样的事情，我们都在见证培养和培育网络意识形态泡沫的后果。

> 理解不道德的人工智能采用的文化风险至关重要，因为它对我们开始认为理所当然的事情产生了深远的影响。
> 
> 比如民主，科学或者客观现实。

如果我们的选民中有很大一部分被误导，我们如何确保我们有公平和自由的选举？在很多方面，数据和人工智能完整性、透明度和道德是人权的下一个前沿。我们的生活越来越数字化，我们在日常生活中采用人工智能已经成为现实。

算法以可见和不可见的方式参与到我们的生活中。算法决定了你有资格获得多少信用额度，你的抵押贷款利率是多少，如果你被逮捕，你的刑期应该是多长，或者你的工作表现如何被衡量。如果没有对人工智能系统的权重和分配的坚实理解，我们正在创造的是一个进一步延续压迫的环境。如果我们想要掌控它的力量，控制我们的集体生活，我们都需要更多地了解人工智能及其用途。

## 我们对这个话题了解的足够多吗？是否影响人权？

总的来说，我希望人们更多地了解人工智能和数据。如果我们要把握人工智能给我们带来的机遇和风险，我希望我们有我们需要的那种民主化。

你从不工作或不理解这种范式转变的人那里听到的大多数恐惧都是关于一台超级计算机将暴力毁灭人类，但事实实际上比那更可怕。算法现在正以各种方式被使用，大多数人甚至都没有意识到。从消费者和立法的角度来看，围绕人工智能部署创造透明度和道德将是至关重要的。

另一个主要趋势将出现在我们如何设计人工智能以及我们如何优化这些模型的使用的形式中。目前用户采用是王道，但我们看到这种优化如何在人道主义层面伤害我们。媒体多元化、假新闻和耸人听闻对我们的社会产生了真正的后果，但它们确实完成了任务:它们让你保持关注。如果我们要纠正今天影响我们的一些问题，围绕移情和伦理的人工智能设计将需要成为基础，我们将需要创造一种文化转变，远离我们今天看到的轰动效应，并坚持公司(包括媒体公司)在设计他们的平台时考虑人类精神健康和信息。

另一个需要强调的领域是偏见和种族主义和歧视在人工智能和我们的生活中的“烘焙”。我们迫切需要学术界和工业界思想和背景的多样性，这样我们才能确保算法是以一种包容性的方式构建的，从而减少我们在社会中已经存在的偏见。

我经营一个每月一次的读书俱乐部，如果你想了解更多，我强烈推荐以下书籍:

*   每个人都在撒谎:大数据、新数据和互联网能告诉我们的真实身份
*   数学毁灭的武器:大数据如何增加不平等并威胁民主
*   《看不见的女人:在一个为男人设计的世界里揭露数据偏见》
*   伦理算法:社会意识算法设计的科学
*   《九大巨头:科技巨头和他们的思维机器如何扭曲人类》,艾米·韦伯
*   技术上的错误:性别歧视的应用程序，有偏见的算法，以及其他有毒技术的威胁
*   真实:我们对世界误解的十个原因——以及为什么事情比你想象的要好，作者安娜·罗斯林·伦隆德和汉斯·罗斯林

## 关于数据和 AI，我们应该了解些什么？

更多！我想鼓励好奇心和灵感。有一种错误的、有问题的信念，认为为了理解这些概念，你需要成为一名博士，或者真正了解数学。那里有非常容易阅读的书籍和博客。人工智能正在以如此多的方式改变我们的生活，我希望更多的人能够看到人工智能的灵感和紧迫性。

普通人需要知道更多关于人工智能同时创造的机会和问题。如果我们要在数据和人工智能文化中看到真正切实的变化，我们的集体将需要清醒地认识到其中的一些现实。只有到那时，我们才能在全球范围内进行有意义的政策变革，以抓住机遇，缓解问题，艾说。

未来是开源的！我们必须保持开源协作、透明和共享的文化。区块链技术正在发挥作用。万维网先驱蒂姆·伯纳斯·李本周宣布，他想创建第二个版本的万维网，让个人控制自己的数据。互联网是一个巨大的突破！伯纳斯-李和欧洲核子研究中心使万维网开源和免费的事实，使得互联网像现在这样快速发展成为可能。

消费者无法控制他们的数据。每当私人公民承认某个应用程序的条款和条件时，他们就放弃了对自己数据的权利。反过来，该应用程序从人们自由放弃的数据中赚钱，而抓取器能够免费抓取数据。每个人都是赢家，除了消费者，他们基本上是通过创造引人注目的用户采用客户之旅来获取数据。

> 一旦我们准确理解了人工智能带来的风险和问题，接下来的挑战就是培养我们的乐观精神。
> 
> 如果我们不相信我们可以改变世界和现状，我们永远不会。

## AI 分析数据吗？

是的，机器学习和深度学习都使用大量数据，我们现在的情况是，数据可以免费获得，可以收集，我们在美国对我们的数据和如何使用数据几乎没有控制权。除了 GDPR 和 CCPA(在加利福尼亚州)，我们没有健全的数据保护政策。从消费者保护的角度来看，这一政策变化应该是一个巨大的优先事项。

我们也完全沉浸在大数据时代，现在可以利用它的算法已经到来。深度学习的新发展涉及如何创建深度学习模型，这些模型不需要那么多数据，计算量也不那么大。

## 为了保护我们在互联网上的权利，我们应该寻求什么样的监管？

我们应该努力确保我们有可靠的数据保护。作为一名消费者，作为一个用法律来管理和保护你的国家的公民，你应该能够确切地知道你的数据是如何被使用的，卖给了谁以及为什么，并且你应该能够每次都对此表示同意，而不仅仅是在你签署条款和条件时一次。GDPR 和 CCPA 是伟大的，但他们只是触及表面。

## 我们有多样性吗？

简而言之:没有。

纽约大学 AI Now 研究所最近发表的一项研究得出结论,“多样性灾难”导致了有缺陷的人工智能系统，这些系统使性别和种族偏见永久化。该报告发现，超过 80%的人工智能教授是男性，只有 15%的脸书人工智能研究人员和 10%的谷歌人工智能研究人员是女性。

挑战之一是，当缺乏多样性时，就缺乏多样化的思想。如果创造技术的人群是同质的，我们将获得由特定人群设计并为其良好工作的技术。我们将需要大胆地走向未来，拥有包容和透明的模型，这些模型一直由不同的人工智能开发者团队进行微调。我们将需要在工业和学术界的人工智能团队中雇用不同的候选人。