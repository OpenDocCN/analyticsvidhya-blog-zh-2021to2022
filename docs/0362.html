<html>
<head>
<title>Decision Tree &amp; Random Forests</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树和随机森林</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-tree-random-forests-9c5d1225c435?source=collection_archive---------19-----------------------#2021-01-13">https://medium.com/analytics-vidhya/decision-tree-random-forests-9c5d1225c435?source=collection_archive---------19-----------------------#2021-01-13</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="f0fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从头开始完成实施</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/518618ccd1fb323645af95e4db580838.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*dknuIMqtCrKXBPNN"/></div></figure><p id="5f62" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">决策树是用于分类和回归的最强大和最流行的工具。它就像一个流程图，或者我们可以像一个树状模型，其中每个节点描述一个功能，而顶部节点被称为根节点。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jk"><img src="../Images/5557817dbd07a4fd7e321ba5886dd436.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*RpJgqrDza35sG2kE.png"/></div></figure><p id="e8f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi jl translated">关于决策树的关键点</p><ul class=""><li id="30ef" class="ju jv hh ig b ih ii il im ip jw it jx ix jy jb jz ka kb kc bi translated">简单的树状结构，模型可以在每一点上做出决策</li><li id="d3a6" class="ju jv hh ig b ih kd il ke ip kf it kg ix kh jb jz ka kb kc bi translated">易于解释，因为它显示了如何作出决定！</li><li id="a4c5" class="ju jv hh ig b ih kd il ke ip kf it kg ix kh jb jz ka kb kc bi translated">递归贪婪算法</li><li id="86de" class="ju jv hh ig b ih kd il ke ip kf it kg ix kh jb jz ka kb kc bi translated">定义明确的逻辑</li><li id="d35c" class="ju jv hh ig b ih kd il ke ip kf it kg ix kh jb jz ka kb kc bi translated">模仿人类逻辑</li><li id="02b2" class="ju jv hh ig b ih kd il ke ip kf it kg ix kh jb jz ka kb kc bi translated">特征最好是分类的</li></ul><h2 id="5d36" class="ki kj hh bd kk kl km kn ko kp kq kr ks ip kt ku kv it kw kx ky ix kz la lb lc bi translated">构建决策树可能涉及两个标准中的任何一个:</h2><ol class=""><li id="d355" class="ju jv hh ig b ih ld il le ip lf it lg ix lh jb li ka kb kc bi translated">基尼指数(CART——分类和回归树)</li><li id="f173" class="ju jv hh ig b ih kd il ke ip kf it kg ix kh jb li ka kb kc bi translated">熵(ID3 —迭代二色性3)</li></ol><blockquote class="lj lk ll"><p id="e1fc" class="ie if lm ig b ih ii ij ik il im in io ln iq ir is lo iu iv iw lp iy iz ja jb ha bi translated">决策树中最关键的一点是如何决定在每一点选择哪个特征！因此，上面指定的这些标准有助于我们做到这一点。</p></blockquote><h2 id="0267" class="ki kj hh bd kk kl km kn ko kp kq kr ks ip kt ku kv it kw kx ky ix kz la lb lc bi translated">基尼指数</h2><p id="7c8e" class="pw-post-body-paragraph ie if hh ig b ih ld ij ik il le in io ip lq ir is it lr iv iw ix ls iz ja jb ha bi translated">基尼系数通过分裂产生的群体中反应类别的混合程度，给出了分裂有多好的概念。下面的公式给出了基尼系数的值。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es lt"><img src="../Images/f1d3eeb17730d6542ce5476e5826456d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uJmiGOKQrhVUs1s5.png"/></div></div><figcaption class="ly lz et er es ma mb bd b be z dx translated">源数据-统计</figcaption></figure><h2 id="4965" class="ki kj hh bd kk kl km kn ko kp kq kr ks ip kt ku kv it kw kx ky ix kz la lb lc bi translated">熵</h2><p id="c4bb" class="pw-post-body-paragraph ie if hh ig b ih ld ij ik il le in io ip lq ir is it lr iv iw ix ls iz ja jb ha bi translated">熵是系统中随机性的一种度量。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es mc"><img src="../Images/86cbb313faa81dc9e184e57ad3b17d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9uJgF3izfoDr0vCk.jpg"/></div></div></figure><blockquote class="lj lk ll"><p id="0661" class="ie if lm ig b ih ii ij ik il im in io ln iq ir is lo iu iv iw lp iy iz ja jb ha bi translated">我们使用熵来计算<strong class="ig hi">信息增益</strong>，熵将被最大化以获得节点的正确特征。</p></blockquote><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es md"><img src="../Images/c473dd3dc19ba9ee129f09856371b088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*_qD0sjf2GbBGyoYt"/></div></figure><blockquote class="lj lk ll"><p id="49b3" class="ie if lm ig b ih ii ij ik il im in io ln iq ir is lo iu iv iw lp iy iz ja jb ha bi translated">我们将为上述任何标准赋予所有特征，然后对于具有最大信息增益的特征，我们将基于阈值或平均值分割数据。</p></blockquote><h1 id="5259" class="me kj hh bd kk mf mg mh ko mi mj mk ks ml mm mn kv mo mp mq ky mr ms mt lb mu bi translated">让我们来看看熵的代码:</h1><pre class="jd je jf jg fd mv mw mx my aw mz bi"><span id="249f" class="ki kj hh mw b fi na nb l nc nd">def entropy(col):<br/>    <br/>    counts = np.unique(col,return_counts=True)<br/>    N = float(col.shape[0])<br/>    entropy = 0.0<br/>    <br/>    for i in counts[1]:<br/>        p = i/N<br/>        entropy += (-1.0 * p * np.log2(p))<br/>    <br/>    return entropy</span></pre><ul class=""><li id="32ed" class="ju jv hh ig b ih ii il im ip jw it jx ix jy jb jz ka kb kc bi translated">我们将计算唯一条目的数量和条目总数。</li><li id="00a6" class="ju jv hh ig b ih kd il ke ip kf it kg ix kh jb jz ka kb kc bi translated">然后应用熵的公式。</li></ul><h2 id="1b7f" class="ki kj hh bd kk kl km kn ko kp kq kr ks ip kt ku kv it kw kx ky ix kz la lb lc bi translated">分割数据</h2><p id="1d79" class="pw-post-body-paragraph ie if hh ig b ih ld ij ik il le in io ip lq ir is it lr iv iw ix ls iz ja jb ha bi translated">我们知道，将数据分为左右两个子树，因此大于阈值的值位于右侧，小于阈值的值位于左侧。</p><blockquote class="lj lk ll"><p id="5b83" class="ie if lm ig b ih ii ij ik il im in io ln iq ir is lo iu iv iw lp iy iz ja jb ha bi translated">fkey →功能名称</p><p id="06ec" class="ie if lm ig b ih ii ij ik il im in io ln iq ir is lo iu iv iw lp iy iz ja jb ha bi translated">fval →阈值</p></blockquote><pre class="jd je jf jg fd mv mw mx my aw mz bi"><span id="6bd8" class="ki kj hh mw b fi na nb l nc nd">def divide_data(x_data,fkey,fval):<br/>    # work with Pandas dataframe<br/>    # creating 2 empty dataframe<br/>    x_right = pd.DataFrame([],columns=x_data.columns)<br/>    x_left = pd.DataFrame([],columns=x_data.columns)<br/>    <br/>    for i in range(x_data.shape[0]):<br/>        val = x_data[fkey].loc[i]<br/>        if val &gt; fval:<br/>            x_right = x_right.append(x_data.loc[i])<br/>        else:<br/>            x_left = x_left.append(x_data.loc[i])<br/>    <br/>    return x_left,x_right</span></pre><h1 id="c2c5" class="me kj hh bd kk mf mg mh ko mi mj mk ks ml mm mn kv mo mp mq ky mr ms mt lb mu bi translated">信息增益</h1><pre class="jd je jf jg fd mv mw mx my aw mz bi"><span id="14f8" class="ki kj hh mw b fi na nb l nc nd">def information_gain(x_data,fkey,fval):<br/>    # Split data <br/>    left,right = divide_data(x_data,fkey,fval)<br/>    # we will compute reduction in entropy after this entropy<br/>    # % fo people on left and right<br/>    l = float(left.shape[0])/x_data.shape[0]<br/>    r = float(right.shape[0])/x_data.shape[0]<br/>    <br/>    # All examples can come to one side<br/>    if left.shape[0] == 0 or right.shape[0] == 0:<br/>        return -1000000 # Min information Gain<br/>    <br/>    i_gain = entropy(x_data.Survived) - (l*entropy(left.Survived) + r*entropy(right.Survived))<br/>    return i_gain</span></pre><ul class=""><li id="96fe" class="ju jv hh ig b ih ii il im ip jw it jx ix jy jb jz ka kb kc bi translated">我们将数据分成左右两组，然后计算熵。</li><li id="ebfa" class="ju jv hh ig b ih kd il ke ip kf it kg ix kh jb jz ka kb kc bi translated">应用信息公式。</li></ul><blockquote class="ne"><p id="1774" class="nf ng hh bd nh ni nj nk nl nm nn jb dx translated">现在我们都准备好跳到决策树的代码了！！</p></blockquote><pre class="no np nq nr ns mv mw mx my aw mz bi"><span id="b799" class="ki kj hh mw b fi na nb l nc nd">class DecisionTree:<br/>    <br/>    # Constructor<br/>    def __init__(self,depth=0,max_depth=5):<br/>        self.left = None<br/>        self.right = None<br/>        self.fkey = None<br/>        self.fval = None<br/>        self.max_depth = max_depth<br/>        self.depth = depth<br/>        self.target = None<br/>    <br/>    def train(self,X_train):<br/>        <br/>        features = ["Pclass","Sex","Age","SibSp","Parch","Fare"]<br/>        info_gain = []<br/>        <br/>        for i in features:<br/>            i_gain = information_gain(X_train,i,X_train[i].mean())<br/>            info_gain.append(i_gain)<br/>        <br/>        self.fkey = features[np.argmax(info_gain)]<br/>        self.fval = X_train[self.fkey].mean()<br/>        print("Making tree Feature is",self.fkey)<br/>        # Split Data<br/>        data_left,data_right = divide_data(X_train,self.fkey,self.fval)<br/>        data_left = data_left.reset_index(drop=True)<br/>        data_right = data_right.reset_index(drop=True)<br/>        <br/>        # we have reached leaf node<br/>        if data_left.shape[0] == 0 or data_right.shape[0] ==0 :<br/>            if X_train.Survived.mean() &gt;=0.5:<br/>                self.target = "Survived"<br/>            else:<br/>                self.target = "Dead"<br/>            return<br/>        <br/>        # Stop early when depth &gt;=maxDepth<br/>        if self.depth &gt;= self.max_depth:<br/>            if X_train.Survived.mean() &gt;=0.5:<br/>                self.target = "Survived"<br/>            else:<br/>                self.target = "Dead"<br/>            return<br/>        # Recursive Case<br/>        self.left = DecisionTree(depth=self.depth+1,max_depth=self.max_depth)<br/>        self.left.train(data_left)<br/>        <br/>        self.right = DecisionTree(depth=self.depth+1,max_depth=self.max_depth)<br/>        self.right.train(data_right)<br/>        <br/>        # Setting Target at every node<br/>        if X_train.Survived.mean() &gt;=0.5:<br/>            self.target = "Survived"<br/>        else:<br/>            self.target = "Dead"<br/>        return<br/>    <br/>    def predict(self,test):<br/>        if test[self.fkey]&gt;self.fval:<br/>            # right<br/>            if self.right is None:<br/>                return self.target<br/>            return self.right.predict(test)<br/>        else:<br/>            if self.left is None:<br/>                return self.target<br/>            return self.left.predict(test)</span></pre><p id="db4e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在决策树中面临过拟合的问题，这意味着在训练和测试数据集中，直到某一点，精度都增加了，但是在某一点之后，测试部分的精度开始下降。这意味着我们的代码不是通用的。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nt"><img src="../Images/3d39eb7c817dd0405526c0a7902f0fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/0*_x5F8SGbAk64yf0v"/></div></figure><p id="677b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了克服这一点，我们可以使用两种方法</p><ul class=""><li id="21a6" class="ju jv hh ig b ih ii il im ip jw it jx ix jy jb jz ka kb kc bi translated">后期修剪→在这种情况下，我们让树充分成长，然后删除无用的节点</li><li id="0059" class="ju jv hh ig b ih kd il ke ip kf it kg ix kh jb jz ka kb kc bi translated">预修剪→我们可以设置树的最大高度，防止树在超过一定限制后生长。</li></ul><p id="52e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在代码中使用了预修剪。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nu"><img src="../Images/df53f8afa482beb391a6bd741aa85f45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/0*JjoDY2rRjZEglvBh"/></div></figure><p id="009b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">除此之外，决策树也有很高的方差。即使我们在数据集中做了一个小的改变，结果也会有很大的变化。</p><p id="0723" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了克服这一点，我们提出了集合的概念。集成方法，它将几个决策树结合起来，以产生比使用单个决策树更好的预测性能。</p><blockquote class="lj lk ll"><p id="2ca4" class="ie if lm ig b ih ii ij ik il im in io ln iq ir is lo iu iv iw lp iy iz ja jb ha bi translated">集成模型背后的主要原理是一组弱学习者聚集在一起形成一个强学习者。</p></blockquote><ol class=""><li id="51d0" class="ju jv hh ig b ih ii il im ip jw it jx ix jy jb li ka kb kc bi translated"><strong class="ig hi"> Bagging </strong> (Bootsrap聚合)这里我们的主要目标是最小化树的方差。使用来自不同树的所有预测的平均值，这比单个决策树更健壮。</li></ol><p id="badf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="lm">随机森林</em> </strong>是套袋的延伸。这需要一个额外的步骤，除了获取数据的随机子集，还需要随机选择特征，而不是使用所有特征来生成树。当你有很多随机的树时。叫做随机森林。</p><p id="f160" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.Boosting是另一种创建预测器集合的集成技术。在这种技术中，学习者是顺序学习的，早期的学习者将简单的模型与数据拟合，然后分析数据的错误。换句话说，我们拟合连续的树(随机样本),并且在每一步，目标都是解决来自先前树的净误差。</p><p id="4102" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如adaboost、梯度增强器。在adaboost中，我们需要记录权重。最初，所有人都被赋予相同的权重并创建决策树，但高度=1。这些被称为<em class="lm">树桩</em>。对于每一个特征，我们都创建stump。</p><ol class=""><li id="846c" class="ju jv hh ig b ih ii il im ip jw it jx ix jy jb li ka kb kc bi translated">我们将选择具有最小熵或ginni系数的树桩。</li><li id="650d" class="ju jv hh ig b ih kd il ke ip kf it kg ix kh jb li ka kb kc bi translated">我们计算总误差。</li><li id="3432" class="ju jv hh ig b ih kd il ke ip kf it kg ix kh jb li ka kb kc bi translated">计算树桩的性能</li></ol><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es nv"><img src="../Images/f7406cfd46cd7eac48978dcfa6622f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oTmHJ1hPAgNU3d_U"/></div></div></figure><p id="7a36" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.我们将通过<em class="lm">新样本权重=树桩的权重x ^性能</em>来增加错误预测类别的权重</p><p id="3bc8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">5.我们将通过<em class="lm">新样本权重=树桩的权重X e^-性能</em>来更新正确预测类别的权重</p><p id="05fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">6.现在，我们除以新权重的总和，得到归一化的权重。</p><p id="edfa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">7.在此之后，我们创建一个新的决策树，循环重复。</p><p id="f53b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后我们以多数票决定。</p></div></div>    
</body>
</html>