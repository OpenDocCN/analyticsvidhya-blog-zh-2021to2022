<html>
<head>
<title>Converting Novel Neural Network Architectures to TensorRT — AdderNet to TensorRT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将新型神经网络架构转换为tensort——AdderNet to tensort</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/converting-novel-neural-network-architectures-to-tensorrt-addernet-to-tensorrt-20b11d7fe5cf?source=collection_archive---------11-----------------------#2021-03-09">https://medium.com/analytics-vidhya/converting-novel-neural-network-architectures-to-tensorrt-addernet-to-tensorrt-20b11d7fe5cf?source=collection_archive---------11-----------------------#2021-03-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="8a12" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当涉及到边缘人工智能设备中预训练神经网络(NN)的部署和推理时，我们必须减少NN的内存占用(大小)以适应边缘设备的内存。此外，我们必须减少神经网络中的计算数量，以实现更高的吞吐量，同时保持所需的精度。为此，我们可以使用以下神经网络压缩方法。</p><ul class=""><li id="8cb0" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">修剪神经网络的冗余权重、过滤器、层。</li><li id="c9b7" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">量化神经网络的权重。(浮点64至浮点32，浮点16，整数8)</li><li id="55c9" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">开发一个有效的神经网络结构。</li><li id="ff47" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">知识的升华。</li></ul><p id="4974" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你不熟悉上述方法，最好读一点文献，了解它们的基本概念，因为这将有助于你在人工智能生产线上部署高性能模型。</p></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h2 id="363a" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ip ki kj kk it kl km kn ix ko kp kq kr bi translated">NVIDIA TensorRT</h2><p id="5594" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">NVIDIA TensorRT是一款高性能深度学习推理的SDK。它包括一个深度学习推理优化器和运行时，为深度学习推理应用程序提供低延迟和高吞吐量。换句话说，TensorRT为我们做了NN压缩工作。您可能已经知道，卷积神经网络(CNN)部署到NVIDIA Jetson开发套件需要将我们的CNN模型转换为NVIDIA-TensorRT格式。那么只有它可以在NVIDIA Jetson部署套件中运行，如TX1、TX2、Nano、Xavier AGX和Xavier NX。</p></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h2 id="367b" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ip ki kj kk it kl km kn ix ko kp kq kr bi translated">AdderNet</h2><p id="eb0f" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated"><a class="ae kx" href="https://arxiv.org/abs/1912.13200" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> AdderNet </strong> </a> <strong class="ig hi"> </strong>是一种用<strong class="ig hi">新型加法滤波器</strong>代替传统<strong class="ig hi">卷积滤波器</strong>的细胞神经网络。通常<strong class="ig hi">加法运算</strong>在计算上<strong class="ig hi">比<strong class="ig hi">乘法运算</strong>便宜</strong>。加法器滤波器使用这个概念，并且<strong class="ig hi">在卷积滤波器中用加法代替乘法。</strong>由加法滤波器组成的卷积层称为<strong class="ig hi">加法层</strong>。</p><p id="f53e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">卷积运算</strong> —计算输入特征图和卷积滤波器之间的<strong class="ig hi">互相关</strong>。</p><p id="bbc6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">加法器操作</strong> —首先，计算输入特征图和加法器滤波器之间的<strong class="ig hi">绝对L1范数距离</strong>。第二，取绝对值的负值计算出来。</p><blockquote class="ky kz la"><p id="6829" class="ie if lb ig b ih ii ij ik il im in io lc iq ir is ld iu iv iw le iy iz ja jb ha bi translated">加法器操作-&gt; | 2–1 |+| 4–2 |+| 2–4 |+| 1–5 | = 9 *(-1)=-9</p><p id="12a7" class="ie if lb ig b ih ii ij ik il im in io lc iq ir is ld iu iv iw le iy iz ja jb ha bi translated">卷积运算-&gt; (2*1) + (4*2) + (2*4) + (1*5) = 23</p></blockquote><figure class="lg lh li lj fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lf"><img src="../Images/6e6ad5eb83859edef9622206457c74fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pgbyBo4qIug1oPt69NAtYg.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">加法器运算与卷积运算</figcaption></figure><p id="5d60" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">AdderNet使用<strong class="ig hi">全精度梯度</strong>来更新反向传播过程中的加法器滤波器。它还使用<strong class="ig hi">自适应学习速率</strong>方法提高了加法器滤波器的学习速率。除此之外，加法器层应该是<strong class="ig hi">，然后是批量标准化</strong> <strong class="ig hi">层</strong>，以将负输出缩放到更好的范围，以便与传统CNN架构中的Relu激活一起工作。</p></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h2 id="72a8" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ip ki kj kk it kl km kn ix ko kp kq kr bi translated"><strong class="ak"> AdderNetMnist </strong></h2><p id="dd23" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">在我的实验中，我使用了一个手工制作的AdderNet神经网络架构，该架构在<strong class="ig hi"> PyTorch </strong>中实现，用于MNIST分类。从这里开始，我将把这个NN称为<strong class="ig hi"> AdderNetMnist </strong>。<strong class="ig hi"> AdderNetMnist </strong>如以下代码片段所示。该体系结构由两个加法器层组成，后面是批量标准化层和最大池层。然后有两个密集层用于SoftMax分类。你可以在我的Github repo中的源文件<a class="ae kx" href="https://github.com/chinthysl/AdderNetTensorRT/blob/master/addernet_mnist.py" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">addernet _ mnist . py</strong></a>中找到这个实现。</p><pre class="lg lh li lj fd lv lw lx ly aw lz bi"><span id="636b" class="jx jy hh lw b fi ma mb l mc md">class Net(nn.Module):<br/>    def __init__(self):<br/>        super(Net, self).__init__()<br/>        self.adder1 = adder.adder2d(1, 20, kernel_size=5, stride=1, padding=0, bias=False)<br/>        self.bn1 = nn.BatchNorm2d(20)<br/>        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)<br/>        self.adder2 = adder.adder2d(20, 50, kernel_size=5, stride=1, padding=0, bias=False)<br/>        self.bn2 = nn.BatchNorm2d(50)<br/>        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)<br/>        self.fc1 = nn.Linear(800, 500)<br/>        self.relu1 = nn.ReLU(inplace=True)<br/>        self.fc2 = nn.Linear(500, 10)<br/><br/>    def forward(self, x):<br/>        x = self.adder1(x)<br/>        x = self.bn1(x)<br/>        x = self.pool1(x)<br/>        x = self.adder2(x)<br/>        x = self.bn2(x)<br/>        x = self.pool2(x)<br/>        x = x.view(-1, 800)<br/>        x = self.fc1(x)<br/>        x = self.relu1(x)<br/>        x = self.fc2(x)<br/>        return F.softmax(x, dim=1)</span></pre><p id="bd9e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">加法器层的实现(<strong class="ig hi"> adder2d </strong>)可以在<a class="ae kx" href="https://github.com/huawei-noah/AdderNet/blob/master/adder.py" rel="noopener ugc nofollow" target="_blank">原AdderNet研究论文实现</a>中找到。这个实现只使用CPU完成，没有在CUDA中实现GPU加速。</p></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h2 id="96d0" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ip ki kj kk it kl km kn ix ko kp kq kr bi translated">将AdderNet转换为TensorRT</h2><p id="7cd1" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">要将神经网络架构转换为TensorRT，首先我们必须使用所需的数据集(在我的例子中是MNIST)训练现有的神经网络，并保存网络及其权重以供将来使用。</p><p id="a5af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后我们将保存的模型转换成TensorRT。在TensorRT中，有两种方法可以处理来自不同DL框架的已保存模型。</p><ol class=""><li id="45e5" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb me ji jj jk bi translated">张量流— tf2onnx转换，TF-TRT转换，TF-UFF转换。</li><li id="2ce2" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb me ji jj jk bi translated">PyTorch或其他框架——转换成ONNX，然后转换成TRT。</li></ol><p id="77d0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我的实验中，我使用了第二种方法。首先，我尝试使用下面的脚本<a class="ae kx" href="https://github.com/chinthysl/AdderNetTensorRT/blob/master/addernet_mnist_onnx.py" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">addernet _ mnist _ ONNX . py</strong></a>将我的PyTorch模型直接转换为ONNX。</p><pre class="lg lh li lj fd lv lw lx ly aw lz bi"><span id="f135" class="jx jy hh lw b fi ma mb l mc md">model = MnistModel()<br/>model.network.load_state_dict(torch.load('./saved_models/addernet_mnist.pth'))<br/>model.network.to('cuda')<br/><br/>dummy_input = torch.randn(1, 1, 28, 28, device='cuda')<br/><br/># convert pytorch model to onnx format<br/>torch.onnx.export(model.network, dummy_input,<br/>                  "./saved_models/addernet_mnist.onnx",<br/>                  verbose=True, opset_version=OPSET)</span></pre><p id="0e5f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不幸的是<strong class="ig hi"><em class="lb">torch . onnx . export</em></strong><em class="lb"/>方法给出了一个错误，因为ONNX不支持我的神经网络架构中的新型加法器层的转换。如果我们的神经网络架构中有了新的层类型或者新的运算类型，<strong class="ig hi">你们大多数人！！！可能会以ONNX错误</strong>结束这个阶段。为了从这个阶段开始，我们需要使用tensort<strong class="ig hi">API</strong>从头开始实现tensort<strong class="ig hi">不支持的自定义层或操作，以使其可转换。</strong></p><p id="616d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种情况下，我们可以通过使用tensort<a class="ae kx" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/classnvinfer1_1_1_i_plugin_v2_ext.html" rel="noopener ugc nofollow" target="_blank">c++</a>和<a class="ae kx" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/python_api/infer/Plugin/IPluginV2Ext.html" rel="noopener ugc nofollow" target="_blank">Python</a>API提供的<strong class="ig hi"> IPluginV2 </strong>接口实现自定义层来扩展tensort功能。<strong class="ig hi">自定义层</strong>在TensorRT中通常被称为<strong class="ig hi">插件</strong>。如果你熟悉OOP编程的话，<strong class="ig hi"> IPluginV2 </strong>就是接口或者父类。我们要做的是创建一个继承父类功能的子类。所以我们的<strong class="ig hi">自定义层类</strong>可以根据我们自定义NN层的功能扩展并实现<strong class="ig hi"> IPluginV2基类</strong>中的虚拟方法。</p><p id="6c27" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我的实验中，我在repo中将加法器层实现为<a class="ae kx" href="https://github.com/chinthysl/AdderNetTensorRT/blob/master/plugin/Adder2dPlugin.h" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">Adder 2d plugin . h</strong></a><strong class="ig hi"/>。<strong class="ig hi"> Adder2dPlugin </strong>子类扩展IPluginV2基类。为了从<strong class="ig hi"> Adder2dPlugin创建一个对象，</strong> TensortRT给了我们另一个名为<strong class="ig hi">iplugcincrator的接口。adder 2 dplugcincrator</strong>是扩展<strong class="ig hi">iplugcincrator的子类。</strong>这两个类的实现都可以在源文件<a class="ae kx" href="https://github.com/chinthysl/AdderNetTensorRT/blob/master/plugin/Adder2dPlugin.cu" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">adder 2d plugin . Cu</strong></a><strong class="ig hi">中找到。</strong>总之，这两个类负责<strong class="ig hi">为我们的加法器层定义特征映射维度、数据类型、管理GPU内存和正向推理计算</strong>。请看一下<strong class="ig hi">adder 2d plugin::enqueue()</strong>方法，该方法使用我设计的自定义<strong class="ig hi">并行CUDA算法</strong>进行正向推理计算。</p><p id="abdd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">用于定制层的并行CUDA算法的实现包括CUDA编程接口中的并行编程模式的知识。我已经使用一个CUDA内核<strong class="ig hi">_ _ global _ _ void Adder filter()</strong>映射了加法器操作，它可以在source<a class="ae kx" href="https://github.com/chinthysl/AdderNetTensorRT/blob/master/plugin/Adder2dPlugin.cu" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">Adder 2 dplugin . Cu</strong></a>中<strong class="ig hi"> </strong>找到<strong class="ig hi"> </strong>。我的加法器操作的CUDA映射可以如下所示。</p><figure class="lg lh li lj fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mf"><img src="../Images/2b77dfccef0ced06adf0d5d635b8b2d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ri-spqB8hlwqRjcU2xB_yQ.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">将加法器过滤器映射到并发CUDA内核</figcaption></figure><p id="9302" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在使用C++成功实现并测试了<strong class="ig hi"> Adder2dPlugin </strong>之后，我们可以创建一个<strong class="ig hi"> C++共享库</strong>对象(。so文件)包含此功能。然后，我们必须使用py bind 11(<a class="ae kx" href="https://github.com/chinthysl/AdderNetTensorRT/blob/master/plugin/Adder2dPyTrt.cpp" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">adder 2 dpytrt . CPP</strong></a>)创建将这个<strong class="ig hi"> Adder2dPlugin </strong>作为Python包加载的能力。</p><p id="7013" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下一步是使用Python导入这个<strong class="ig hi"> Adder2dPlugin </strong>。更多信息请参考源代码<a class="ae kx" href="https://github.com/chinthysl/AdderNetTensorRT/blob/master/addernet_mnist_trt.py" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">addernet _ mnist _ TRT . py</strong></a><strong class="ig hi"/>中名为<strong class="ig hi"> def get_adder2d_plugin() </strong>的函数。</p><p id="dfba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这一点上，我们已经完成了我们的自定义神经网络层-加法器层的实现。现在我们要做的是使用这一层，将整个<strong class="ig hi"> AdderNetMnist </strong>网络架构转换为TensorRT。为此，我们在TensorRT python API中复制了整个py torch<strong class="ig hi">addernet NIST</strong>网络架构。<a class="ae kx" href="https://github.com/chinthysl/AdderNetTensorRT/blob/master/addernet_mnist_trt.py" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">addernet _ mnist _ TRT . py</strong></a><strong class="ig hi"/>里面的函数<strong class="ig hi"> def populate_network() </strong>为<strong class="ig hi"> </strong> us做这个工作。</p><p id="a667" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">艰难的工作现在结束了。因此，我们可以使用PyTorch模型中AdderNetMnist的预训练权重来构建TensorRT推理机，并运行推理。这项工作的整个开发流程如下图所示。</p><figure class="lg lh li lj fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mg"><img src="../Images/f1f32437274e8922bc8c694afcf25f76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rOxl7uV4VQKiuLmOgNIpMw.jpeg"/></div></div></figure></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h2 id="a2b0" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ip ki kj kk it kl km kn ix ko kp kq kr bi translated">结果和结论备注</h2><figure class="lg lh li lj fd lk er es paragraph-image"><div class="er es mh"><img src="../Images/f8b78d77bccf30db75a1b7181dc4496d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*7wHiMsmG3K72XmnateA1vg.jpeg"/></div></figure><ul class=""><li id="6653" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">结果表明， TensorRT模型在精度损失<strong class="ig hi"> 0.83%的情况下，可以实现比AdderNet Mnist模型极低的推理延迟(<strong class="ig hi"> 5000x </strong>)。</strong></li><li id="cbd9" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">TensorRT模型中的更高精度显示了加法器层实现的<strong class="ig hi">正确性</strong>。</li><li id="7784" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">这种<strong class="ig hi">实现可以扩展</strong>以部署任何其他具有加法器层的NN架构，如原始研究论文中提到的ResNet20和ResNet50。</li><li id="dcf9" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">目前，<strong class="ig hi">这是AdderNet唯一成功的CUDA和TensorRT实施</strong>。</li><li id="2f43" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">这项工作花了我三个星期才完成。我希望这篇文章能帮助AI行业和学术界的每个人，那些尝试使用NVIDIA Jetson开发套件的人。</li></ul></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><h2 id="d09e" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ip ki kj kk it kl km kn ix ko kp kq kr bi translated">下一步</h2><p id="9c30" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">我想将加法器过滤器的CUDA实现集成到现有的DL库中，如Tensorflow和PyTorch。在这里，我只在CUDA中为部署实现了正向推理。但是我需要在CUDA中实现反向传播，也是为了训练的目的。</p></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><p id="fa04" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">给我一个掌声❤️，如果你喜欢这个职位:)希望你会拉代码，并尝试自己。<a class="ae kx" href="https://github.com/chinthysl/AdderNetTensorRT" rel="noopener ugc nofollow" target="_blank">https://github.com/chinthysl/AdderNetTensorRT</a>。</p><p id="8621" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">关注我的GitHub页面，开始对你重要的项目。</p><p id="cd35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我做深度学习咨询、研究、开发，喜欢研究有趣的问题。如果你有一个我们可以合作的项目，请通过我的电子邮件联系我:<strong class="ig hi">gamanayaka.chinthaka@gmail.com</strong></p><p id="771e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以后工作用LinkedIn联系我:【https://www.linkedin.com/in/chinthaka-gamanayake】<strong class="ig hi"/>/</p><p id="62d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我必须提到并感谢我的同事，Nuran，作为一名经验丰富的生产级人工智能工程师，他帮助我完成了这个项目！</p></div><div class="ab cl jq jr go js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="ha hb hc hd he"><p id="8928" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参考文献:</strong></p><ul class=""><li id="ed98" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1912.13200.pdf" rel="noopener ugc nofollow" target="_blank"> AdderNet研究论文</a></li><li id="5585" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><a class="ae kx" href="https://github.com/huawei-noah/AdderNet/" rel="noopener ugc nofollow" target="_blank">华为原始AddeNet文件实现</a></li><li id="9a89" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><a class="ae kx" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/" rel="noopener ugc nofollow" target="_blank">腾索特开发者指南</a></li><li id="f7b0" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><a class="ae kx" href="https://docs.nvidia.com/deeplearning/tensorrt/best-practices/index.html" rel="noopener ugc nofollow" target="_blank">腾索特最佳实践</a></li><li id="ff09" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><a class="ae kx" href="https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/" rel="noopener ugc nofollow" target="_blank"> TensorRT python API </a></li><li id="1b33" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><a class="ae kx" href="https://github.com/NVIDIA/TensorRT/tree/master/samples/opensource/samplePlugin" rel="noopener ugc nofollow" target="_blank"> TensorRT样本层插件</a></li><li id="2381" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><a class="ae kx" href="https://github.com/zerollzeng/tiny-tensorrt" rel="noopener ugc nofollow" target="_blank">https://github.com/zerollzeng/tiny-tensorrt</a></li><li id="527a" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><a class="ae kx" href="https://github.com/NVIDIA-AI-IOT/torch2trt" rel="noopener ugc nofollow" target="_blank">https://github.com/NVIDIA-AI-IOT/torch2trt</a></li><li id="ac36" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><a class="ae kx" href="https://github.com/TrojanXu/onnxparser-trt-plugin-sample" rel="noopener ugc nofollow" target="_blank">https://github.com/TrojanXu/onnxparser-trt-plugin-sample</a></li><li id="7643" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><a class="ae kx" href="https://github.com/ceccocats/tkDNN" rel="noopener ugc nofollow" target="_blank">https://github.com/ceccocats/tkDNN</a></li></ul></div></div>    
</body>
</html>