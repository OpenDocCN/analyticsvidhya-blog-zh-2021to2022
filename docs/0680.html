<html>
<head>
<title>Implementing K-Nearest Neighbours from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始实现K-最近邻</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementing-k-nearest-neighbours-from-scratch-1c5d62503067?source=collection_archive---------20-----------------------#2021-01-26">https://medium.com/analytics-vidhya/implementing-k-nearest-neighbours-from-scratch-1c5d62503067?source=collection_archive---------20-----------------------#2021-01-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="9be1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNN是实践中最简单、最常用的算法之一。您以前一定听说过或实现过这种技术。在任何领域，直观地理解技术总是更好的，因此在本文中，我尝试在只知道理论知识的情况下从头实现KNN。</p><p id="be40" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于不知道什么是KNN的人，我将用几句话来总结一下。</p><p id="5b17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设您有一个包含两个数字要素和两个分类标签的2D数据集。假设你绘制了这些数据的散点图，你会得到这样的结果。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/38939933d7c4647d4b4addab23fb2838.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*V690GfjkuszNlu8zHpoV8Q.png"/></div></figure><p id="a490" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从图中，你可以很容易地将它分为两类，如果我问你，在蓝色星团附近给出一个新的点Q，你能告诉我它属于哪一类吗？</p><p id="f96f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你会很快说蓝课。我们可以说，只看新数据点的位置。但是你能告诉我们你是如何得出这个结论的吗？是的，你的答案是KNN！</p><p id="27df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以KNN主要做的是计算给定查询数据点的K个最近邻(在这个例子中是Q)。在找到邻居之后，Q被认为是在它的大多数邻居所属的类中。我们可以根据数据集取最适合的K值。我们通常选择奇数值。</p><p id="580b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNN被广泛用于ML目的，但它主要依赖于我们正在处理的数据。特定的模型不是在KNN创建的，但是训练数据集就是模型！这就是为什么KNN被称为基于实例的学习。</p><p id="baae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了从头开始实施KNN，我们将完成以下任务:</p><ol class=""><li id="d332" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb jp jq jr js bi translated">为测试目的导入虚拟数据。</li><li id="f7d2" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">创建类KNN并为其实现方法。</li><li id="4dea" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">最终确定结果</li></ol><p id="124b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们开始一步一步地实现我们的算法。</p><ol class=""><li id="2e74" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb jp jq jr js bi translated"><strong class="ig hi">为测试目的导入数据。</strong></li></ol><p id="33f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了检查我们的模型之前和之间的工作情况，我们将使用著名的Iris数据集。Iris数据集是通过考虑花瓣和萼片的长度和宽度对三种花进行分类。导入数据后，我们将按照80-20的比例对数据进行分类，以便进行培训和测试。</p><pre class="jd je jf jg fd jy jz ka kb aw kc bi"><span id="3758" class="kd ke hh jz b fi kf kg l kh ki">from sklearn <em class="kj">import</em> datasets</span><span id="8c1e" class="kd ke hh jz b fi kk kg l kh ki">from sklearn.model_selection <em class="kj">import</em> train_test_split</span><span id="0316" class="kd ke hh jz b fi kk kg l kh ki">iris = datasets.load_iris()<br/>X, y = iris.data, iris.target</span><span id="c8b0" class="kd ke hh jz b fi kk kg l kh ki">X_train, X_test, y_train, y_test = train_test_split(X, y, <em class="kj">test_size</em>=0.2, <em class="kj">random_state</em>=1)</span></pre><p id="5f75" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 2。创建类KNN并为其实现方法。</strong></p><p id="5781" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，我们将创建一个接受k值的KNN类</p><pre class="jd je jf jg fd jy jz ka kb aw kc bi"><span id="a0a9" class="kd ke hh jz b fi kf kg l kh ki">class KNN:</span><span id="8970" class="kd ke hh jz b fi kk kg l kh ki">    def <strong class="jz hi">__init__</strong>(<em class="kj">self</em>,<em class="kj">K</em>=5):</span><span id="35d3" class="kd ke hh jz b fi kk kg l kh ki">        super().__init__()</span><span id="fe5f" class="kd ke hh jz b fi kk kg l kh ki">        self.K=K</span></pre><p id="46d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在创建了KNN类之后，我们需要为它创建两个在实践中常用的方法。这些方法是。适合和。预测</p><p id="114b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Fit-特定于KNN fit方法的作用是将训练数据和目标存储在模型中。正如我前面所说，KNN是一个基于实例的学习模型，这意味着它只在运行时学习特定查询点的数据。这意味着我们的fit方法将只包含存储特定对象的数据。</p><pre class="jd je jf jg fd jy jz ka kb aw kc bi"><span id="4933" class="kd ke hh jz b fi kf kg l kh ki">def <strong class="jz hi">fit</strong>(<em class="kj">self</em>,<em class="kj">X</em>,<em class="kj">y</em>):</span><span id="a4d2" class="kd ke hh jz b fi kk kg l kh ki">    self.XTrain=X</span><span id="adca" class="kd ke hh jz b fi kk kg l kh ki">    self.yTrain=y</span></pre><p id="27da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">predict——我们算法的关键就在于这个函数。我们想要得到的是所有点离查询点的距离。我们可能有一个单一的查询点或多个查询的可迭代数据，我们将处理这两种类型的数据。首先，我们将迭代这些查询，找到所有的距离并将它们存储在一个列表中。</p><pre class="jd je jf jg fd jy jz ka kb aw kc bi"><span id="b5ab" class="kd ke hh jz b fi kf kg l kh ki">def <strong class="jz hi">predict</strong>(<em class="kj">self</em>,<em class="kj">X</em>):</span><span id="1161" class="kd ke hh jz b fi kk kg l kh ki">    predicted=[] #To store final results </span><span id="3de5" class="kd ke hh jz b fi kk kg l kh ki"><em class="kj">    for</em> single_X in X:</span><span id="e6b0" class="kd ke hh jz b fi kk kg l kh ki">        distances=[dist(single_X,tP) <em class="kj">for</em> tP in self.XTrain]</span><span id="e7c1" class="kd ke hh jz b fi kk kg l kh ki">        minimum=np.argsort(distances)[:self.K]</span></pre><p id="6884" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上面的代码中，我调用了一个名为dist()的函数来计算欧几里德距离，您将需要创建这个简单的函数，它将两个点作为输入，并给出它们之间的距离。你可以自己创建这个函数，也可以参考我的全部代码。您可能会想上面代码中的最后一行是做什么的。argsort()函数基本上返回我们作为参数传入的排序列表中的原始索引。此外，我们已经完成了列表切片。简而言之，这一行将返回距离列表中最小的K个距离的索引。</p><p id="0e87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们要做的最后一件事是找到列表中的大多数点。我们将从存在于我们的yTrain中的点的索引中找到点的类别，并在我们的最终输出列表中附加最常出现的类别标签。这个最常见的类标签只是我们处理的单点的输出。</p><pre class="jd je jf jg fd jy jz ka kb aw kc bi"><span id="71ff" class="kd ke hh jz b fi kf kg l kh ki">labels=[self.yTrain[i] <em class="kj">for</em> i in minimum]</span><span id="7c11" class="kd ke hh jz b fi kk kg l kh ki">predicted.append(Counter(labels).most_common()[0][0])</span></pre><p id="c08b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们将返回最终的输出列表。</p><p id="44b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">嘭！！我们刚刚完成了算法的编码！</p><p id="d71b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 3。最终确定结果</strong></p><p id="80a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在是时候检查它是如何工作的了！请记住，我们已经将数据分成了4部分，现在是使用y_train和y_test的时候了。我们将y_train传递给我们的模型，我们的模型将给出预测值，稍后我们将检查有多少值实际上是正确预测的。</p><pre class="jd je jf jg fd jy jz ka kb aw kc bi"><span id="9508" class="kd ke hh jz b fi kf kg l kh ki">KNNmodel = KNN(<em class="kj">K</em>=5)</span><span id="51ff" class="kd ke hh jz b fi kk kg l kh ki">KNNmodel.fit(X_train, y_train)</span><span id="1cc6" class="kd ke hh jz b fi kk kg l kh ki">predictions = KNNmodel.predict(X_test)</span><span id="6f28" class="kd ke hh jz b fi kk kg l kh ki">print("Accuracy-", accuracy(y_test, predictions))</span></pre><p id="9595" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我已经创建了一个简单的accuracy()函数，它告诉我们两个列表中相似值的百分比。</p><p id="55c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在第一次尝试中，我获得了96.66%的准确度，但是后来我发现对于K=3，准确度总是100%，所以我们将只使用K=3的值。</p><p id="9acf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是我们成功实现K近邻的方法。这是我们遵循的最简单的方法。我们仍然可以改进我们的算法，例如我们可以更精确地处理错误。接下来，我将尝试从头开始实现KNN的变体。我已经提供了完整代码的Github链接。非常感谢你方的任何建议或修改。</p><p id="0398" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">谢谢大家！</p><p id="f536" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">完整代码-<a class="ae kl" href="https://github.com/MautKaFarishta/Simple-Data-Scince-Projects/tree/master/ML_From_Scratch/KNN" rel="noopener ugc nofollow" target="_blank">https://github . com/MautKaFarishta/Simple-Data-Scince-Projects/tree/master/ML _ From _ Scratch/KNN</a></p></div></div>    
</body>
</html>