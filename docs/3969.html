<html>
<head>
<title>End-to-end Speech Recognition model for Amharic using TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于张量流的阿姆哈拉语端到端语音识别模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/end-to-end-speech-recognition-model-for-amharic-using-tensorflow-e72e60775bd9?source=collection_archive---------5-----------------------#2021-08-14">https://medium.com/analytics-vidhya/end-to-end-speech-recognition-model-for-amharic-using-tensorflow-e72e60775bd9?source=collection_archive---------5-----------------------#2021-08-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/12d297d87134b460320f9b45634f9976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gKk8RJtGbgk6oydDzft26g.png"/></div></div></figure><p id="5248" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">深度学习通过引入端到端模型，改变了语音识别领域的游戏规则。这些模型接收音频信号并直接输出转录。在这篇博客中，我们将为阿姆哈拉语建立一个端到端的自动语音识别管道。我们完成的管道将接受原始音频作为输入，并返回预测的口语转录。</p><p id="de58" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">阿姆哈拉语是一种埃塞俄比亚闪米特语言，是亚非语系闪米特分支的一个分支。根据维基百科，该语言有3200万母语使用者和2500万L2使用者。这个数字可能不准确。我只是把它放在这里作为背景。</p><h1 id="e23f" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">GitHub链接</h1><div class="km kn ez fb ko kp"><a href="https://github.com/10acad-group3/speech_recognition" rel="noopener  ugc nofollow" target="_blank"><div class="kq ab dw"><div class="kr ab ks cl cj kt"><h2 class="bd hj fi z dy ku ea eb kv ed ef hh bi translated">GitHub-10a CAD-group 3/speech _ recognition</h2><div class="kw l"><h3 class="bd b fi z dy ku ea eb kv ed ef dx translated">在这个项目中，我们将建立深度学习模型来处理和转换非洲语言(阿姆哈拉语)…</h3></div><div class="kx l"><p class="bd b fp z dy ku ea eb kv ed ef dx translated">github.com</p></div></div><div class="ky l"><div class="kz l la lb lc ky ld io kp"/></div></div></a></div><h1 id="9058" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">数据</h1><p id="b411" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">在这个项目中，<a class="ae lj" href="https://github.com/getalp/ALFFA_PUBLIC" rel="noopener ugc nofollow" target="_blank"> ALFFA_PUBLIC </a>数据集用于模型的训练和验证。数据包含大约11k个音频剪辑，总计大约18个小时的阿姆哈拉语语音。每个音频剪辑持续几秒钟，并附带其转录。</p><p id="df0a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">数据特征</p><ul class=""><li id="6723" class="lk ll hi is b it iu ix iy jb lm jf ln jj lo jn lp lq lr ls bi translated">输入特征(X):口语的音频剪辑</li><li id="6c43" class="lk ll hi is b it lt ix lu jb lv jf lw jj lx jn lp lq lr ls bi translated">目标标签(y):所说内容的文本副本</li></ul><h1 id="3db0" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">预处理</h1><p id="3666" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">数据是语音识别最重要的方面之一。数据预处理在开发有效的自动语音识别系统中起着至关重要的作用。</p><p id="0208" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">数据集包含“中的音频文件。wav”格式，采样率为16 kHz。对于大多数基本的语音识别系统，8kHz就足够了。因此，我开始以8kHz的频率对音频信号进行重新采样。然后，我准备了元数据，其中包含音频文件的名称、相应的转录、每个转录的字符长度以及以秒为单位的音频长度。这是元数据的概述。</p><figure class="lz ma mb mc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/e9cdd819b950bcd5ec4929cf8ddfb61f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MCE3gI502PTEnowM7unwPA.png"/></div></div></figure><h2 id="bddf" class="md jp hi bd jq me mf mg ju mh mi mj jy jb mk ml kc jf mm mn kg jj mo mp kk mq bi translated"><strong class="ak">均值归一化</strong></h2><p id="1521" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">标准化将减少信号中不需要的噪声，给我们一个干净的音频文件。该图显示了来自训练集的之前和之后的样本音频文件。</p><figure class="lz ma mb mc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/0ae2e3c6c3ba2714bb33d6d72d4c51e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OdmJdMAk4IGeYI5Oib5Aaw.png"/></div></div></figure><h2 id="3c88" class="md jp hi bd jq me mf mg ju mh mi mj jy jb mk ml kc jf mm mn kg jj mo mp kk mq bi translated"><strong class="ak">分裂</strong></h2><p id="6263" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">我们将使用Libros来消除音频信号中的静音。Libros split函数返回一个包含音频信号中非无声区间的NumPy数组。该函数不返回任何音频，只返回音频中非无声片段的开始点和结束点。通过连接非静音段，我们可以减少每个音频信号中的静音。</p><figure class="lz ma mb mc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/f8fa2ca8782b7056c150bbe8abbe51bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hl6j-iD0i-T2EfzAGNePDQ.png"/></div></div></figure><h2 id="acc3" class="md jp hi bd jq me mf mg ju mh mi mj jy jb mk ml kc jf mm mn kg jj mo mp kk mq bi translated">极端值</h2><p id="c1b6" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">异常值是偏离其他数据观察值的极端值，它们可能表示测量值的可变性、实验误差或新奇事物。换句话说，异常值是偏离样本总体模式的观察值。在我们的数据集中，我们将根据持续时间、字符长度和速度来寻找异常值。</p><figure class="lz ma mb mc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/b77584b770c0e9feafc674ad063ff6a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*920_mIhb0XEnRO3rcU1zTA.png"/></div></div></figure><p id="88aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以看到大多数音频都在4到8秒之间。我已经删除了短于2秒和长于14秒的音频。这导致大约2%的数据丢失。</p><figure class="lz ma mb mc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/3f86ed88d34d5c3f9e63de3c47acc3ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ShOrZOHa-40bV8iXsnOUqQ.png"/></div></div></figure><p id="5811" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">大多数抄本的长度在30到70个字符之间。这包括没有转写的行和转写长度超过150个字符的行。删除转录长度小于10且大于125的行后，数据丢失小于1%。</p><figure class="lz ma mb mc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/114ed5ef8884ea1c8d3f95886b52466d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gl7C1VNdCGkYbAecNo63OQ.png"/></div></div></figure><p id="1854" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我尝试过用字符长度除以持续时间来估算音频速度。我们之所以能够做到这一点，是因为我们已经消除了音频信号中大部分额外的静音。上面的分布图显示了速度为零和大于16的行。我把速度小于四或者大于十二的行都去掉了。这也导致了大约2%的数据丢失。</p><h1 id="95c1" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">数据扩充</h1><p id="3de8" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">接下来，我们可以对原始音频信号进行数据扩充，方法是改变音高或音频速度，并随机添加噪声。数据扩充为我们的输入数据增加了更多种类，并有助于模型对更广泛的输入进行归纳。</p><figure class="lz ma mb mc fd ij"><div class="bz dy l di"><div class="mu mv l"/></div></figure><h1 id="defc" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">特征</h1><p id="5bf6" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">任何机器学习方法的关键是从数据中提取特征。特征表示数据，并作为模型的输入。这里我们将使用Mel频谱图从音频信号中提取特征。</p><p id="6358" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">频谱图是时间和频率之间的2D图，其中图中的每个点根据颜色强度表示特定时间特定频率的幅度。简单来说，光谱图是一个随时间变化的频谱(颜色范围很广)。</p><p id="f8b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Mel频谱图相对于绘制频率与时间关系的常规频谱图有两个重要变化。它使用Mel标度而不是y轴上的频率，并使用分贝标度而不是振幅来指示颜色。</p><p id="b340" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有一篇文章是关于如何用TensorFlow在你的GPU上轻松处理音频的，作者是David Schwertfeger。本文描述了如何利用GPU的能力，使用TensorFlow信号处理器处理音频数据。你可以从博客上阅读更多关于如何从音频信号生成Mel声谱图的细节。</p><figure class="lz ma mb mc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/8af8889310ff6a07c4f525b600201f90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vxX3YDNCZiIibyLCGR2PIg.png"/></div></div></figure><h1 id="815a" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">数据生成程序</h1><p id="55df" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">现在我们已经完成了所有的预处理步骤，我们将使用Keras定义一个定制的数据生成类。</p><figure class="lz ma mb mc fd ij"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="ef85" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">而不是通过用静音填充来调整所有音频样本的大小以使其具有相同的长度。该课程将根据批次中最长的填充音频和转录。因为填充会影响网络的工作方式，并对模型的性能和精度产生很大影响。根据持续时间对我们的数据进行排序，并用小填充生成每个巴赫，这将允许我们最小化填充的负面影响。</p><p id="3c1b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该类为单个音频文件返回三个增强版本。因为我们每次都在增加音频，而不是保存增加的文件，所以我们的数据会在每批中随机变化。例如，音频的速度可以随机加快或减慢。这将大大增加我们输入数据的可变性。</p><h1 id="351a" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">同ＣIＴＹ ＴＥＣＨＮＯＬＯＧＹ ＣＯＬＬＥＧＥ</h1><p id="3946" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">我们将使用CTC损失作为我们模型的基本损失函数。当输入是连续的而输出是离散的，并且没有清晰的元素边界可用于将输入映射到输出序列的元素时，CTC用于对齐输入和输出序列。</p><p id="9d45" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Keras CTC loss函数需要y_true、y_pred、input_length和label_length作为参数。问题是我们数据中每一批的输入长度都有变化。我们可以通过将音频长度除以跳数来计算输入长度。</p><p id="a959" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另一点是，如果我们在模型中使用卷积或最大池层，我们需要重新计算输入长度，因为它们可以减少特征图的维度。</p><p id="97a4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要记住的最后一点是输入长度(时间片的数量)应该大于两倍转录长度加一。反恐委员会的文件中提到了这一点。除此之外，我们可以根据Keras的文件计算CTC损失。</p><figure class="lz ma mb mc fd ij"><div class="bz dy l di"><div class="mu mv l"/></div></figure><h1 id="2aea" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">模型</h1><p id="22b0" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">我将使用与Deep Speech 2架构类似的模型。该模型将有两个主要的神经网络模块。三层残差卷积神经网络学习相关的音频特征。然后，一组双向递归神经网络利用所学习的音频特征。让我们使用Keras来研究模型的实现。</p><figure class="lz ma mb mc fd ij"><div class="bz dy l di"><div class="mu mv l"/></div></figure><figure class="lz ma mb mc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/54e156ef501ff6728c34ed1c7d0c825d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oD6VUPiA7sVccBIG0okYYw.png"/></div></div></figure><p id="1540" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这种可视化显示了模型在一段时间内的性能。仅使用一个GPU进行了大约8个小时的培训。我继续训练这个模型，但是验证损失并没有改善多少。在查看时要记住的一点是，损失不仅是针对干净数据计算的，也是针对增强版本计算的。</p><h1 id="0cec" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结果</h1><figure class="lz ma mb mc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/ed65e4ff7708c441c6098e788fcecd47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v9BEBpWVawNWgGFmKEeKJA.png"/></div></div></figure><p id="2a89" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以看到该模型在短句上表现良好</p><figure class="lz ma mb mc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/c62f4115409af8034da1bd0a31c37995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fGBbLIeZ6bt77K0x-xL4lQ.png"/></div></div></figure><p id="284e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">句子越长，单词错误率越高。然而，结果是有希望的，因为我们只根据18小时的数据训练了模型。</p></div><div class="ab cl na nb gp nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="hb hc hd he hf"><p id="9478" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[1]: David Schwertfeger:如何用TensorFlow在你的GPU上轻松处理音频。<a class="ae lj" href="https://towardsdatascience.com/how-to-easily-process-audio-on-your-gpu-with-tensorflow-2d9d91360f06" rel="noopener" target="_blank">https://towards data science . com/how-to-easy-process-on-your-GPU-tensor flow-2d9d 91360 f 06</a></p><p id="bbf9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[2]:Derrick mwi ti:2019年自动语音识别指南。<a class="ae lj" href="https://heartbeat.fritz.ai/a-2019-guide-for-automatic-speech-recognition-f1e1129a141c" rel="noopener ugc nofollow" target="_blank">https://heart beat . fritz . ai/a-2019-自动语音识别指南-f1e1129a141c </a></p><p id="154f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[3]:Derrick mwi ti:2019年自动语音识别指南。<a class="ae lj" href="https://heartbeat.fritz.ai/a-2019-guide-for-automatic-speech-recognition-f1e1129a141c" rel="noopener ugc nofollow" target="_blank">https://heart beat . fritz . ai/a-2019-自动语音识别指南-f1e1129a141c </a></p><p id="6dd7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[4]: Michael Nguyen:在PyTorch中构建端到端的语音识别模型。<a class="ae lj" href="https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch" rel="noopener ugc nofollow" target="_blank">https://www . assembly ai . com/blog/端到端语音识别-pytorch </a></p><p id="39ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[5]:柯坦<strong class="is hj">T5】Doshi:让音频深度学习变得简单。<a class="ae lj" href="https://towardsdatascience.com/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706" rel="noopener" target="_blank">https://towards data science . com/audio-deep-learning-made-simple-automatic-speech-recognition-ASR-how-it-works-716 cfce 4c 706</a></strong></p></div></div>    
</body>
</html>