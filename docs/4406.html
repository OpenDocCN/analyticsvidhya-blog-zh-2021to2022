<html>
<head>
<title>Imitate with Caution: Offline and Online Imitation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谨慎模仿:线下和线上模仿</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/imitate-with-caution-offline-and-online-imitation-ee20de054fdb?source=collection_archive---------2-----------------------#2021-10-06">https://medium.com/analytics-vidhya/imitate-with-caution-offline-and-online-imitation-ee20de054fdb?source=collection_archive---------2-----------------------#2021-10-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="a757" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">行为克隆，数据聚合方法:匕首。</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/d1edd0eaa4d449d89df94b095ac05d70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FFvn5CvMuffPQPmHGoFzBw.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated"><a class="ae jm" href="https://www.indiatvnews.com/trending/offbeat-dog-perfectly-mimics-owner-doing-yoga-adorable-video-breaks-the-internet-705813" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="60f1" class="jn jo hh bd jp jq jr js jt ju jv jw jx in jy io jz iq ka ir kb it kc iu kd ke bi translated">什么是模仿学习？</h1><p id="20e7" class="pw-post-body-paragraph kf kg hh kh b ki kj ii kk kl km il kn ko kp kq kr ks kt ku kv kw kx ky kz la ha bi translated">正如它本身所表明的，几乎所有的物种包括人类都通过模仿学习，也可以随机应变。一句话就是进化。同样，我们可以让机器模仿我们，向人类专家学习。自动驾驶就是一个很好的例子:我们可以让一个代理从数以百万计的司机演示中学习，并模仿专家司机。</p><p id="5014" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">这种从示范中学习也称为模仿学习(IL ),是强化学习和人工智能中的一个新兴领域。人工智能在机器人学中的应用无处不在，机器人可以通过分析人类管理者执行的策略演示来学习策略。</p><p id="4358" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated"><strong class="kh hi"> <em class="lg">专家缺席vs在场:</em> </strong>模仿学习采取两个方向，以专家是否在训练期间缺席或专家是否在场来纠正代理的动作为条件。先说专家不在的第一种情况。</p><h1 id="6897" class="jn jo hh bd jp jq jr js jt ju jv jw jx in jy io jz iq ka ir kb it kc iu kd ke bi translated">培训期间专家缺席</h1><p id="1566" class="pw-post-body-paragraph kf kg hh kh b ki kj ii kk kl km il kn ko kp kq kr ks kt ku kv kw kx ky kz la ha bi translated">专家缺席基本上意味着代理只能观看专家演示，仅此而已。在这些“专家不在”的任务中，代理试图使用专家演示的固定训练集(状态-动作对),以便学习一个策略并实现一个与专家尽可能相似的动作。这些“专家缺席”任务也可以被称为离线模仿学习任务。</p><p id="20f1" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">这个问题可以被框架化为通过分类的监督学习。专家演示包含许多训练轨迹，每个轨迹包括一系列观察和一系列由专家执行的动作。这些训练轨迹是固定的，不受代理策略的影响，这种“专家缺席”任务也可以称为离线模仿学习任务。</p><p id="5bf9" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">这个学习问题可以公式化为一个监督学习问题，其中可以通过解决一个简单的监督学习问题来获得一个策略:我们可以简单地训练一个监督学习模型，该模型直接将状态映射到动作，以通过专家的演示来模仿专家。我们称这种方法为“行为克隆”。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lh"><img src="../Images/fd41e1792a5ad81c00c674ba1dfed83d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1l-aPoRV2-mOvE7zAdpGBA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">由作者生成</figcaption></figure><p id="e42a" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">现在我们需要一个替代损失函数，它量化了被证明的行为和被学习的政策之间的差异。我们使用最大期望对数似然函数来计算损失。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lh"><img src="../Images/29c75449acfc0ab28df32f7ee5db2b53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fLxXu-oUTNH8GHmrSJ9GCw.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">L2误差～最大化对数似然</figcaption></figure><p id="6ffa" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">如果我们要解决一个分类问题，我们选择交叉熵，如果是回归问题，我们选择L2损失。显而易见，最小化l2损失函数相当于最大化高斯分布下的期望对数似然。</p><h1 id="09b0" class="jn jo hh bd jp jq jr js jt ju jv jw jx in jy io jz iq ka ir kb it kc iu kd ke bi translated">挑战</h1><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es li"><img src="../Images/5923e02eec9d8715f333155e2d3f8a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HoBl8EtA3bA0SzYAyRoVKg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">参考文献[1]</figcaption></figure><p id="b77b" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">到目前为止，一切看起来都很好，但是行为克隆的一个重要缺点是一般化。专家只是收集了代理可以经历的无限可能状态的子集。一个简单的例子是，专业汽车驾驶员不会通过偏离路线来收集不安全和有风险的状态，但代理可能会遇到这样的风险状态，因为没有数据，所以它可能没有学会纠正措施。这是因为“<strong class="kh hi">协变量移位</strong>”这是一个已知的挑战，其中在训练期间遇到的状态不同于在测试期间遇到的状态，降低了鲁棒性和通用性。</p><p id="47a7" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">解决这种“协方差转移”问题的一种方法是收集更多风险状态的演示，这可能会非常昂贵。培训期间的专家在场可以帮助我们解决这个问题，并弥合演示策略和代理策略之间的差距。</p><h1 id="bed3" class="jn jo hh bd jp jq jr js jt ju jv jw jx in jy io jz iq ka ir kb it kc iu kd ke bi translated">专家在场:在线学习</h1><p id="ecdf" class="pw-post-body-paragraph kf kg hh kh b ki kj ii kk kl km il kn ko kp kq kr ks kt ku kv kw kx ky kz la ha bi translated">本节我们将介绍最著名的在线模仿学习算法<strong class="kh hi"> <em class="lg">数据聚合方法:DAGGER。</em> </strong>这种方法对于缩小训练时遇到的状态和测试时遇到的状态之间的差距非常有效，即“<strong class="kh hi">协变量移位</strong>”。</p><p id="4388" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">如果专家在学习过程中评估学习者的策略会怎样？专家为来自学习者自身行为的例子提供正确的行动。这正是DAgger试图实现的目标。DAgger的主要优势是专家教学习者如何从过去的错误中恢复过来。</p><p id="0c30" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">步骤很简单，类似于行为克隆，除了我们根据代理到目前为止所学的知识收集更多的轨迹</p><p id="fbf6" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">1.该策略通过专家示范D的行为克隆来初始化，产生策略π1 <br/> 2。代理使用π1并与环境交互，以生成包含轨迹<br/> 3的新数据集D1。D1:我们将新生成的数据集D1添加到专家演示中。新演示D用于训练策略π2…..</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lj"><img src="../Images/ab8d57e597191cc60ecd786a73460b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GRuxE9BVupP-GZK9FISU3A.png"/></div></div></figure><p id="edc0" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">为了利用专家的存在，使用专家和学习者的混合来查询环境和收集数据集。因此，DAGGER从由学习到的策略诱导的状态分布下的专家演示中学习策略。如果我们设置β=0，在这种情况下，这意味着期间的所有轨迹都是从学习者代理生成的。</p><h2 id="4e72" class="lk jo hh bd jp ll lm ln jt lo lp lq jx ko lr ls jz ks lt lu kb kw lv lw kd lx bi translated">算法:</h2><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ly"><img src="../Images/77752a04b49d6e849fa37a0b6e0e30e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p4pJ7UPcuaZDhiQgQXZwYQ.png"/></div></div></figure><p id="662f" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">DAgger缓解了<strong class="kh hi">“协方差移位”</strong>的问题(学习者策略诱导的状态分布与初始演示数据中的状态分布不同)。这种方法大大减少了获得令人满意的性能所需的训练数据集的大小。</p><h1 id="2a31" class="jn jo hh bd jp jq jr js jt ju jv jw jx in jy io jz iq ka ir kb it kc iu kd ke bi translated">结论</h1><p id="dabb" class="pw-post-body-paragraph kf kg hh kh b ki kj ii kk kl km il kn ko kp kq kr ks kt ku kv kw kx ky kz la ha bi translated">DAgger在机器人控制方面取得了非凡的成功，也被应用于控制无人机。由于学习者会遇到专家没有演示如何行动的各种状态，因此DAGGER等在线学习方法在这些应用中是必不可少的。</p><p id="68fd" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">在本系列的下一篇博客中，我们将了解DAgger算法的缺点，重要的是，我们将强调DAgger算法的安全性。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lz"><img src="../Images/e97c72ce49b2d3d75ad37fa8497c6993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eEa1Hq-a8HXODgqPmCkMTg.png"/></div></div></figure></div></div>    
</body>
</html>