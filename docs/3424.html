<html>
<head>
<title>Mechanism of gradient descent optimization algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降优化算法的机理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mechanism-of-gradient-descent-optimization-algorithms-7baed9c9c35e?source=collection_archive---------12-----------------------#2021-06-30">https://medium.com/analytics-vidhya/mechanism-of-gradient-descent-optimization-algorithms-7baed9c9c35e?source=collection_archive---------12-----------------------#2021-06-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="9cf2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将一起探索不同类型基于梯度的优化算法。这篇文章背后的动机是给出优化算法工作背后的直觉。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/7f950d80b87eadce456c8a266a9ecb3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*uBpK4zBVQevNdwrVvx1zEQ.jpeg"/></div></figure><p id="9c2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">帖子如下:</strong></p><ol class=""><li id="77f1" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb jp jq jr js bi translated"><strong class="ig hi">简介</strong></li><li id="6d81" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated"><strong class="ig hi">梯度下降</strong></li><li id="d5c7" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated"><strong class="ig hi">带动量的新币</strong></li><li id="596c" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated"><strong class="ig hi">内斯特罗夫加速梯度</strong></li><li id="dd07" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated"><strong class="ig hi">阿达格拉德</strong></li><li id="39a0" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated"><strong class="ig hi"> Adadelta和RMSprop </strong></li><li id="947f" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated"><strong class="ig hi">亚当</strong></li><li id="cde4" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated"><strong class="ig hi">如何选择算法</strong></li><li id="cdf7" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated"><strong class="ig hi">结论</strong></li></ol><h1 id="36c8" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak"> 1。简介</strong></h1><p id="eb05" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated"><strong class="ig hi">梯度下降</strong>是一种寻找可微函数局部极小值的一阶迭代优化算法。最小化(或最大化)任何数学表达式的过程称为<strong class="ig hi">优化。</strong></p><p id="e414" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们从神经网络的角度来看优化。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lb"><img src="../Images/d3af5f7b559382e6099e69b787d177ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rptqFUQTMiruWZWKm-WMqQ.png"/></div></div></figure><p id="7430" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">优化器是用于改变神经网络属性的算法或方法，例如<strong class="ig hi">权重【w1，w2，w3】</strong>和<strong class="ig hi">学习率</strong> (η)以减少损失。</p><p id="fed2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们深入算法之前，让我们看看艾萨克·牛顿和戈特弗里德·莱布尼茨给了我们什么。</p><p id="72f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">是的，你答对了。让我们复习一下微积分。</p><h1 id="be28" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">下坡</strong></h1><p id="5389" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">假设我们有一个函数L(W ),我们想最小化它。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lg"><img src="../Images/9309f0e0a8c1d49692ef461439e4132b.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*oWrmA7h3TNeDg7OSmDeIGA.png"/></div></figure><p id="c0c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了简单起见，我们假设W是函数l的标量(1-D)输入。我们可以将这一思想扩展到多维。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lh"><img src="../Images/aa47f4b0008d5f97b9b13357ed103ee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*KB5AJN7-zLxO8ealhe029g.png"/></div></figure><p id="368c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在W = 0时，我们只有一个最小值</p><p id="d4e9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，假设如果L(W)看起来像这样。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="ab fe cl li"><img src="../Images/f3f2f05814c1403e4ffb4063014ac86e.png" data-original-src="https://miro.medium.com/v2/format:webp/0*z5VnIrsBP5wGQv4s.png"/></div></figure><p id="caf1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里我们有多个最小值和最大值。</p><p id="cf3f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在最小值和最大值时，dL/dW =0</p><p id="822f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">切线总是平行于X轴，即使在鞍点。</p><h1 id="ff93" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">凸函数</h1><p id="cc5d" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">直觉。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lh"><img src="../Images/70c415ad22a4550270a9fc15607ba34b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*MdYHcdtpNn9P5v4AOFuz6A.png"/></div></figure><p id="8e91" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">取同一区域上的任意两点{a和b}，用尽可能短的直线将它们连接起来，如果直线ab上的所有点都在同一区域上，则它是一个凸函数。</p><p id="fad5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">凸函数只有一个最小值或最大值。</p><p id="53af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所有与线性回归、逻辑回归、支持向量机相关的损失函数都可以被证明是凸的。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lj"><img src="../Images/dd01fb48707a05908a8921dd6da4d5d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*dNYjr8CV_TdMTNeh5NeDxg.png"/></div></div></figure><p id="cd48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">注意:在深度学习中是非凸曲面，这意味着我们可能有多个临界点。</strong></p><p id="f9bc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">除非是一层网，否则不凸。在一般的多层情况下，后面层的参数(权重和激活参数)可以是前面层中参数的高度递归函数。通常，由一些递归结构引入的决策变量的乘法往往会破坏凸性</strong></p><p id="825a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基于以上介绍，我们可以回答一个问题。</p><p id="5a42" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">问:为什么我们使用平方损失？</p><p id="a50d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Ans:)主要原因是平方损失导数产生单值参数集，因此给出一个唯一的解。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lk"><img src="../Images/e88579b206aba79a33b18278e1a7afe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DzB_KYdr449AYcH5oEFFdw.jpeg"/></div></div></figure><h1 id="66b7" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak"> 2。梯度下降</strong></h1><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ll"><img src="../Images/032b415d5880d318839dcfffd0df5f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*cHrV0EFeG-9_4dv0zYychw.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lm"><img src="../Images/c0b425583934fde74c9509a15a3a4714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DtqZvpVbIS570pGFl-c9dw.jpeg"/></div></div></figure><ol class=""><li id="ebe4" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb jp jq jr js bi translated">普通梯度下降，计算整个训练数据集的成本函数相对于参数X的梯度</li><li id="85b0" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">相反，随机梯度下降(SGD)随机地为每个训练样本的<em class="ln">执行参数更新</em></li><li id="2584" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">小批量梯度下降最终取两者之长，对数据集中k个点的每个随机子集进行更新。</li></ol><p id="9bd6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">注意:小批量梯度下降是普通梯度下降的近似</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lo"><img src="../Images/b48b5e1f49a0dad9997dbd00df8a6f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*ArMs4naewdDIgwPEnAF1NQ.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lp"><img src="../Images/8f0bce2d8ffcbef180c09ab609228a09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sNOlWp0XGC9AJNvgobo6Yg.png"/></div></div></figure><h1 id="2eca" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak"> 3。批量SGD带动量。</strong></h1><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lq"><img src="../Images/5ec31f7ff2b9f05fea47b990056e9faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*a1iOUyjHmG3UwnZ6.png"/></div></div></figure><p id="50e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们可以观察到的，SGD给了我们非常嘈杂的梯度更新，因此为了消除噪声，引入了这个<strong class="ig hi">动量</strong>。</p><p id="e5ca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设使用SGD，我们在每次迭代t都获得更新，如下所示:</p><p id="7c3b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在t =1时，我们得到a_1</p><p id="4412" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在t = 2时，我们得到a_2</p><p id="7dca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">诸如此类。</p><p id="1d0b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们能做的是:</p><p id="c0da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在t = 1时:</p><p id="03f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">设v_1 = a_1</p><p id="1177" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在t =2时</p><p id="cda7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">设v _ 2 =α* a _ 2且{ 0≤α≤1 }</p><p id="0aed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">情况— 1:如果alpha == 1 </strong></p><p id="f630" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们得到，v2 = v1+a2</p><p id="d1d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">情况— 2:如果alpha == 0 </strong></p><p id="05f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们得到，v2 = a2</p><p id="2aa6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">情况— 3:如果alpha ==0.5 </strong></p><p id="52db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们得到，v2 = 0.5 * v1+a2</p><p id="4b83" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">注意:我们从上述3个案例中观察到的是，alpha的值有助于我们量化“我们应该从以前的更新中考虑多少信息”。</strong></p><p id="1691" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">概括起来，我们得到:</p><ol class=""><li id="55de" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb jp jq jr js bi translated">v1 = a1</li><li id="eff6" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">v2 =α* v1+a2</li><li id="8771" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">v _ 3 =α* v _ 2+a _ 3</li></ol><p id="690f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">展开v_3我们得到:</p><p id="3c73" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.v_3 = alpha(alpha*v_1+a_2)+a_3</p><p id="6da8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> =阿尔法* a_1 +阿尔法* a_2 + alpha⁰ * a_3 </strong></p><p id="f0cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">所以，</strong></p><p id="9506" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">5.<strong class="ig hi"> v_t = alpha*v_t-1 +a_t </strong></p><p id="7aab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意:等式5本质上是递归的。</p><h1 id="5e52" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">现在，让我们将指数加权平均的思想与SGD结合起来。</h1><p id="8591" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">我们得到，</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lr"><img src="../Images/47b9bd3f2e29eab946c97e0206276b4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KUAYdbYOvIyKehRGgcPeRw.jpeg"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lm"><img src="../Images/c4ae5ed25458c9b022e07fd96bcb2e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GTaVlrvfHHY5lEVHyeGd4A.jpeg"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es ls"><img src="../Images/8fca1244a38b7cd65fb7e75e89684804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yk8Zc-RgKpSjJ_Ky6_gQng.jpeg"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lt"><img src="../Images/184f68910f99b53d58f4e05f5731487d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*L6yBZ4MmIUDKgtIlCKokcQ.png"/></div></figure><p id="f910" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于梯度指向相同方向的维度，动量项增加，而对于梯度改变方向的维度，动量项减少。因此，我们获得了更快的收敛和减少振荡。</p><h1 id="127e" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">4.内斯特罗夫加速梯度。</h1><p id="6481" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">尤里·内斯特罗夫是俄罗斯数学家，国际公认的凸优化专家，尤其擅长开发高效算法和数值优化分析。他目前是卢万大学(UCLouvain)的教授。</p><p id="e405" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在NAG的工作:</p><ol class=""><li id="91e8" class="jk jl hh ig b ih ii il im ip jm it jn ix jo jb jp jq jr js bi translated">首先计算动量。</li><li id="343b" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">向动量方向移动。</li><li id="ab37" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">然后计算我们移动的新点的梯度。</li></ol><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lu"><img src="../Images/28ec2e6551315a3fa29c8c4a3c260958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*mQyZg_eMsJ_zUrXCxV5AqQ.png"/></div></figure><p id="2bf0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">所以我们得到的是:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lv"><img src="../Images/ab75f62ae00bdfc48897847c37686a98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z8mXo1-sEkIZwGCjrtsjSg.jpeg"/></div></div></figure><p id="57f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">NAG首先在先前累积梯度的方向上进行大跳跃，测量梯度，然后进行校正，这导致完整的NAG更新。这种预先更新可以防止我们走得太快，从而提高响应能力。</p><h1 id="c5aa" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">5.阿达格拉德</h1><p id="4811" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">在SGD和SGD +动量中，学习率η =某个值，对于每个重量都是相同的。</p><p id="1c2b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">想法:我们对每个权重都有自适应的学习速率，即每个权重都有不同的学习速率。</p><p id="aa80" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为什么需要这个想法？</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lm"><img src="../Images/1a6c52052655caf72df096732bd32d02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mwaqZLKoOS9M8aJ-848wsQ.jpeg"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lw"><img src="../Images/74a9070f9d3c7c0e64b2f1560a76ed6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CBQZyc9m8J_dMGtSSX0F2Q.jpeg"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lm"><img src="../Images/341164ec54d1a09b27ae59ae2f20fd5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xt8-fjtDVvKLtfls22p8dg.jpeg"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lx"><img src="../Images/c54dcc86fee34b1aa0d5a2077e538cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OxgfOsX4vJpjCi6b_7MD2A.jpeg"/></div></div></figure><p id="2026" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Adagrad的一个主要好处是，它消除了手动调整学习速率的需要。大多数实现使用默认值0.01，并保持不变。</p><p id="c11b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Adagrad的主要缺点是它在分母中累积平方梯度:因为每个增加的项都是正的，所以累积和在训练期间保持增长。这反过来又导致学习率缩小，最终变得极小。</p><p id="c7a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随着t增加，t-1也增加，而η'_t减小，因此当迭代增加时，该权重的学习速率适当地减小。</p><h1 id="9070" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">6.Adadelta和RMSprop。</h1><p id="b75b" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">因此，我们看到了Adagrad的问题，即alpha_t变得非常小，导致收敛缓慢。</p><p id="5e68" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">RMSprop是Geoff Hinton在他的Coursera课堂的<a class="ae ly" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" rel="noopener ugc nofollow" target="_blank">讲座6e中提出的一种方法。</a></p><p id="1e05" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">RMSprop和Adadelta都是在同一时间独立开发的，因为需要解决Adagrad的学习率急剧下降的问题。RMSprop实际上与我们上面导出的Adadelta的第一个更新向量相同</p><p id="1d01" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">想法:如果使用指数衰减法而不是我们在阿达格拉德看到的平方和，会怎么样？</strong></p><p id="5e4e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看看它是如何工作的:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lz"><img src="../Images/b8cdb0bf58a26c1c35e6c49c8242d42b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PgqlgK2FIzEQnQk2UB5pYw.jpeg"/></div></div></figure><p id="a3cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在Adadelta中，我们采用gi的指数平均值，而不是我们在Adagrad中看到的gi之和，以避免η'_t中的大分母，从而避免缓慢收敛。</p><p id="baa9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用Adadelta，我们甚至不需要设置默认的学习速率，因为它已经从更新规则中删除了。</p><h1 id="b664" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak"> 7。Adam — —自适应矩估计:</strong></h1><p id="9dd1" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated"><strong class="ig hi">想法:如果我们把g_t(eda)的指数衰减平均值也存储起来会怎么样:</strong></p><p id="da00" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看看它是什么样子的:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es ma"><img src="../Images/9856236ef69bc3a42e60c7d480c08c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0_sHzbq9Y3-3lYn9Mcg9bg.jpeg"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es mb"><img src="../Images/437c0c5a0427d7be331fec54b5e04cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FKN_36b20NM0ntqEbEGmgw.jpeg"/></div></div></figure><p id="9f20" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者建议β1β1的默认值为0.9，β2β2的默认值为0.999，ϵϵ.的默认值为10-810-8他们从经验上表明，Adam在实践中表现良好，并且比其他自适应学习方法算法更好</p><h1 id="d962" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">8.如何选择算法</h1><ol class=""><li id="a654" class="jk jl hh ig b ih kw il kx ip mc it md ix me jb jp jq jr js bi translated">在大多数情况下，Adam比其他算法工作得更好</li><li id="9eac" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">如果我们的损失是严格凸的，那么动量会收敛得更快，但对于最小点，它可能会振荡一点</li><li id="877f" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">如果代价函数的维数非常高，那么自适应学习也将给出一些更好的结果，因为它给了我们在每个方向上下降多少的控制。</li><li id="d45c" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">可能存在阿达德尔塔和阿达格拉德在鞍形区域被击中的情况，但是在更高维度的特征空间中，概率非常低。</li><li id="0e28" class="jk jl hh ig b ih jt il ju ip jv it jw ix jx jb jp jq jr js bi translated">做梯度检查，在每个时期，看看如何优化工作与激活。有时它帮助我们发现消失梯度问题</li></ol><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mf"><img src="../Images/5ced5286be46c059022fec7161e0f4ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/1*SjtKOauOXFVjWRR7iCtHiA.gif"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">图片借用自cs231n.github.io</figcaption></figure><p id="4005" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">还有更多的算法可以探索，如AdaMax，Nadam，AMSGrad。在AMSGrad之后，已经提出了许多其他的优化器。其中包括AdamW，它修复了Adam中的重量衰减；QHAdam，它将标准SGD步长与动量SGD步长进行平均；以及AggMo，它结合了多个动量项γγ；和其他人。</p><h1 id="97ea" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">结论</h1><p id="6aa8" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">我们已经初步研究了梯度下降的三种变体，其中小批量梯度下降是最受欢迎的。然后，我们研究了最常用于优化SGD的算法:动量法、内斯特罗夫加速梯度法、Adagrad、Adadelta、RMSprop、Adam。我希望这篇博文能够让你对不同优化算法的动机和行为有一些直觉。</p><p id="05a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请随意评论并指出任何错误。:)</p></div></div>    
</body>
</html>