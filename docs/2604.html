<html>
<head>
<title>The Ultimate Transformer And The Attention You Need.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">终极变形金刚和你需要的关注。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-ultimate-transformer-and-the-attention-you-need-1dafaf8cb6c7?source=collection_archive---------4-----------------------#2021-05-07">https://medium.com/analytics-vidhya/the-ultimate-transformer-and-the-attention-you-need-1dafaf8cb6c7?source=collection_archive---------4-----------------------#2021-05-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/f20b414773939957a6697c2ba887634f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w6IykJrM0wOjq6HS"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">照片由<a class="ae it" href="https://unsplash.com/@notethanun?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> note thanun </a>在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="99d9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我会尽可能简单地解释这个话题，包括算术部分。</p><blockquote class="js jt ju"><p id="bb1d" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="hh">要讲的话题</em> </strong></p></blockquote><ul class=""><li id="2ce5" class="jz ka hh iw b ix iy jb jc jf kb jj kc jn kd jr ke kf kg kh bi translated">变压器介绍</li><li id="096d" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">了解变压器的编码器</li><li id="64a9" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">了解变压器的解码器</li></ul><blockquote class="kn"><p id="c855" class="ko kp hh bd kq kr ks kt ku kv kw jr dx translated">变压器初级读本</p></blockquote><p id="2488" class="pw-post-body-paragraph iu iv hh iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">Transformer是最流行的深度学习架构，主要用于NLP任务。自从变压器出现以来，它已经取代了RNN和LSTM的各种任务。几个新的NLP模型，如伯特，GPT，T5是基于变压器架构。</p><blockquote class="kn"><p id="edbf" class="ko kp hh bd kq kr lc ld le lf lg jr dx translated">变压器介绍</p></blockquote><p id="e241" class="pw-post-body-paragraph iu iv hh iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">RNN和LSTM被广泛用于顺序任务，如单词预测，机器翻译，文本生成。然而，他们面临的主要挑战之一是获取长期依赖。</p><p id="64cc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了克服这一点，在“关注是你所需要的”一文中引入了一种新的架构，名为Transformer。变压器目前是几个NLP任务的模型的艺术状态。变形金刚完全基于注意机制，完全摆脱了递归。变压器使用一种特殊类型的关注机制，称为<strong class="iw hi">自关注</strong>。我们将对此进行讨论，但首先让我们了解语言翻译实际上是如何与transformer一起工作的。</p><p id="2a4a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">该变换器由编码器-解码器结构组成。我们把输入的句子输入编码器。编码器学习输入句子的表示，并将该表示发送给解码器。解码器接收由编码器学习的表示，并产生输出。</p><p id="7351" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">假设我们需要将一个英语句子转换成法语。因此，我们的输入将是一个英语句子，我们将馈送给编码器，编码器将学习我们的英语句子的表示，并将表示转发给解码器，解码器将相应地产生一个输出。</p><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lh"><img src="../Images/17d2cfaf4cf53a6bad60d1413698a261.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iSHWnGmX7eiLCBwxywnUuw.png"/></div></div></figure><p id="5918" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="jv">那么究竟发生了什么？</em>T13】</strong></p><blockquote class="kn"><p id="eadc" class="ko kp hh bd kq kr lc ld le lf lg jr dx translated">了解变压器的编码器</p></blockquote><p id="e7b5" class="pw-post-body-paragraph iu iv hh iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">该变换器由N个编码器的堆栈组成。一个编码器的输出作为它上面的另一个编码器的输入发送。每个编码器将其输出发送给它上面的编码器，最终的编码器返回给定源句子的表示作为输出。</p><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lm"><img src="../Images/de9c48571a9fa4939d1f4d07d5bcd441.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QoYNB5Eqg9LGhrvwU8aRVw.png"/></div></div></figure><p id="1fa3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在论文《注意力是你所需要的全部》中，作者使用了6个编码器。然而，我们可以尝试n个编码器。为了简单起见，我们将只使用2个编码器。</p><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ln"><img src="../Images/8b2db3aad19bf8479df62517e8371661.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-2yCnURkwaM-O89WvGOOw.png"/></div></div></figure><p id="5a23" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="jv">于是问题又出现了它实际上是如何工作的？</em> </strong></p><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ln"><img src="../Images/9155103d723a24b2131836ceaa032ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nRsdI18T8j_uLkl-y7kulg.png"/></div></div></figure><p id="d466" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">从上图中你可以很容易地理解所有的编码器模块都是相同的。您还可以观察到每个编码器由两个子层组成</p><ul class=""><li id="8f0e" class="jz ka hh iw b ix iy jb jc jf kb jj kc jn kd jr ke kf kg kh bi translated">多头注意力</li><li id="42d2" class="jz ka hh iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">前馈网络</li></ul><p id="85cb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在了解子层之前，让我们先了解什么是<strong class="iw hi">自我关注机制。</strong></p><blockquote class="js jt ju"><p id="e713" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">自我注意机制</p></blockquote><p id="a37d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了更好地理解，让我们考虑一个例子</p><p id="ec5d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="jv">“一只狗因为饿了而吃了食物”</em></p><p id="644b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在上面的句子中，代词<em class="jv"> it </em>可能指狗或食物。通过阅读这个句子，我们很容易理解它指的是狗而不是食物。这就是自我关注机制帮助我们的地方。在句子“狗吃了食物，因为它饿了”中，我们的模型首先计算单词<em class="jv"> A </em>的表示，接下来计算单词<em class="jv"> dog、</em>的表示，然后是下一个单词，依此类推，直到句子结束。在计算单词的表示时，它将每个单词与句子中的所有其他单词相关联，以了解更多关于单词的信息。例如，假设在计算单词<em class="jv"> it </em>的表示时，我们的模型将单词<em class="jv"> it </em>与所有其他单词相关联，以了解更多关于它的信息，并且在计算时，我们的模型理解了单词it与狗的关系比食物更密切，因为我们可以看到连接单词dog的线比食物更粗。</p><figure class="li lj lk ll fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/2e46f76b43eadcb03146624865853354.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*T5Nxi13RybUK9u-RmFRwAQ.png"/></div></figure><p id="4eef" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在让我们理解它实际上是如何工作的。假设我们的输入是“我很好”。首先，我们得到每个单词在句子中的嵌入(嵌入只是单词的向量表示，嵌入的值将在训练中学习。)设x1为字<em class="jv"> I的嵌入，</em> x2为<em class="jv">am</em>x3为<em class="jv"> good </em>。</p><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lp"><img src="../Images/a70f39b40709a46a663f88ed43e5709e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g0_ZNRCv0f7dsWQzf68XLw.png"/></div></div></figure><p id="ad0e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们输入X的维数会是[句子长度*嵌入维数]。我们句子中的单词数是3，假设我们的嵌入维数是512，那么我们的输入矩阵的维数将是[3*512]。现在，我们从输入矩阵X中创建三个矩阵:查询矩阵<em class="jv"> Q、</em>密钥矩阵<em class="jv"> K </em>和值矩阵<em class="jv"> V </em>。</p><p id="76bd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">好的，但是我们如何创建三个矩阵呢？T29】</p><p id="31b0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了创建这三个矩阵，我们引入三个新的权重矩阵Wq、Wk和Wv。我们通过将权重(Wq，Wk，Wv)矩阵乘以我们的X来创建查询<em class="jv"> Q </em>，密钥<em class="jv"> k </em>和值<em class="jv"> V </em>。权重矩阵首先被随机初始化，并且在训练期间学习这些最优值。</p><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lq"><img src="../Images/2726e1d189f1b2f0e6dc4bf68b95ea22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*skMv3Gc3di0bJRYpDjyHJQ.png"/></div></div></figure><p id="d1d5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个图像非常清楚地表明了q1、k1、v1代表字<strong class="iw hi"> <em class="jv"> I </em> </strong>，q2、k2、v2代表字<strong class="iw hi"> <em class="jv"> am </em> </strong> <em class="jv"> </em>以及q3、k3、v2代表字<strong class="iw hi"><em class="jv"/></strong>。注意，Wq、Wk和Wv的维数为64。因此，我们的Q、K和V矩阵的维数是[句子大小*维数]，即[3*64]。</p><p id="ba7e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在的问题是我们为什么要计算这个？ </p><blockquote class="js jt ju"><p id="5389" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">理解自我关注机制</p></blockquote><p id="af70" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们已经看到如何计算Q，K，V矩阵，我们也知道如何从输入矩阵中提取它们。现在让我们看看它们是如何用于自我注意机制的。我们知道，为了计算一个单词的表示，<em class="jv">自我注意机制</em>将这个单词与给定句子中的所有单词相关联，我们知道理解一个单词如何与所有其他单词相关联会给我们一个更好的表示。自关注<em class="jv">机构</em>包括四个步骤</p><blockquote class="js jt ju"><p id="dd12" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">第一步</p></blockquote><p id="b132" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">自我注意机制的第一步是计算Q和K矩阵之间的点积。</p><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lr"><img src="../Images/58cabf9015fcffcdfe77b5792a84fe67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b6RXyln5l_FBCibV0olY6w.png"/></div></div></figure><p id="0f6c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们观察QK矩阵的第一行，它显示了<em class="jv"> I的查询向量q1与向量K1、k2、k3有多相似。</em></p><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ls"><img src="../Images/59e3d1b3541c82f564cd1cff118dac0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r4zZpDA34dMKggAH6LCHvA.png"/></div></div></figure><p id="65ce" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">类似地，观察第二行，然后是第三行。第三行陈述单词good与其自身的关系，接着是<em class="jv"> I </em>和<em class="jv"> am。因此，我们可以说，计算点积给了我们每个单词与句子中所有单词的相似度矩阵。</em></p><blockquote class="js jt ju"><p id="2c70" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">第二步</p></blockquote><p id="9647" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了获得稳定的梯度，我们将QK矩阵除以K矩阵维数的平方根。在我们的例子中，K矩阵的维数是64，因此8是它的平方根。</p><blockquote class="js jt ju"><p id="f7c7" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">第三步</p></blockquote><p id="cb9d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过查看前面的相似性得分，我们可以理解它们处于非标准化形式，因此我们使用softmax函数进行标准化。应用softmax有助于将分数带到0到1的范围内，分数总和等于1。</p><blockquote class="js jt ju"><p id="b56b" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">第四步</p></blockquote><p id="3128" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在我们计算注意力矩阵z。注意力矩阵包含矩阵中每个单词的注意力值。我们可以通过将得分矩阵乘以V矩阵来计算注意力矩阵，我们有</p><figure class="li lj lk ll fd ii er es paragraph-image"><div class="er es lt"><img src="../Images/69ac3b7be10d7ddcc0fe987cff4b2757.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*BiTO5qWgphrbmSCy-tQQVw.png"/></div></figure><p id="3ed0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">注意力矩阵Z通过由分数加权的值向量的总和来计算。让我们通过逐行查看来理解它。首先让我们看看单词<em class="jv"> I </em>的第一个<em class="jv"> Z1 </em>是如何计算的。</p><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lu"><img src="../Images/71422db92a3cc889b44b88d26cb73dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YRsvlkoZF6MLLLjz3EpJ_A.png"/></div></div></figure><p id="af73" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由此统计出<em class="jv"> Z1包含90%来自矢量V1</em><strong class="iw hi"><em class="jv">I</em></strong><em class="jv">，7%来自V2</em><strong class="iw hi"><em class="jv">am</em></strong><em class="jv">，3%来自V3 </em> <strong class="iw hi"> <em class="jv"> good。</em>T53】</strong></p><p id="8387" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="jv">但是怎么有用呢？</em>T57】</strong></p><p id="d11b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了回答上面的问题，让我们绕过前面的例子</p><p id="a4c0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">“一只狗吃了食物，因为它饿了”</p><p id="a532" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当我们为上面的句子计算Z时，我们会意识到单词<strong class="iw hi"><em class="jv"/></strong>比<strong class="iw hi"> <em class="jv">食物更倾向于<strong class="iw hi"> <em class="jv">狗</em> </strong>的相似性。</em>T11】</strong></p><p id="bd26" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">自我关注机制图示如下</p><figure class="li lj lk ll fd ii er es paragraph-image"><div class="er es lv"><img src="../Images/abfbb112c9c25cb2813748badbb52b41.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*pHalCn2V0eLO4Vm7cdkntQ.png"/></div></figure><p id="715f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">自我注意机制也称为<strong class="iw hi">比例点积。</strong></p><p id="b0f3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为什么？</p><p id="6f5f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你可能已经有答案了。</p><blockquote class="js jt ju"><p id="194e" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">多头注意力机制</p></blockquote><p id="e9ad" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以使用多个注意力头来代替单个注意力，也就是说，我们可以计算多个注意力矩阵，因为这样会更准确。在多头注意力中，我们首先找到每个单词的注意力矩阵，然后将它们连接起来并乘以一个新的权重矩阵。</p><p id="e938" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="jv">学习位置编码</em> </strong></p><p id="6994" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果我们直接把x输入到我们的转换器，它不能理解词序，因此也不能理解句子。因此，我们添加一些指示词序的信息，而不是将输入矩阵直接输入到转换器。这种技术称为<strong class="iw hi"> <em class="jv">位置编码。</em> </strong></p><blockquote class="js jt ju"><p id="9706" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">前馈网络</p></blockquote><p id="03bd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">前馈网络由两个具有Relu激活密集层组成。前馈的参数在句子的不同位置上是相同的，而在编码器块上是不同的。</p><p id="2501" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="jv">添加与定额构件</em> </strong></p><p id="56b9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">编码器中一个更重要的组件是加法和范数组件。I连接子层的输入和输出。它基本上是一个通过层标准化流动的剩余连接。层标准化通过防止每层中的值剧烈变化来促进更快的训练。</p><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lw"><img src="../Images/281a8722bf85b1feb8ac62c306d3bcde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cw1ckZYUkEdhK7cl31Mh8A.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">编码器的完整再现</figcaption></figure><blockquote class="kn"><p id="d0e6" class="ko kp hh bd kq kr ks kt ku kv kw jr dx translated">了解变压器的解码器</p></blockquote><p id="da39" class="pw-post-body-paragraph iu iv hh iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr ha bi translated">现在，我们将编码器表示输入解码器。解码器从编码器获取输入并生成输入。像编码器一样，我们也可以在解码器上叠加N个数字。我们还可以观察到，输入句子的编码器表示(编码器输出)被发送到所有解码器。因此，解码器接收两个输入:一个来自前一个解码器，另一个是编码器的表示。</p><p id="bb69" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="jv">好吧但是我们的解码器到底是怎么目标句子的呢？</em>T29】</strong></p><p id="f335" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们更详细地探索它。在步骤t=1，解码器的输入将是<sos>，这表示句子的开始。解码器将<sos>作为输入，生成第一个字。在时间步长t=2以及当前输入时，解码器从先前的时间步长t-1中取出新生成的单词，并尝试生成句子中的下一个单词。类似地，在每个时间步上，解码器将新生成的单词组合到输入中，并预测下一个单词。一旦生成了表示句子结束的<eos>标记，这意味着解码器已经完成了目标句子的生成。</eos></sos></sos></p><p id="9d5e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在编码器部分，我们学习了如何将输入转换为嵌入矩阵，并向其添加位置嵌入，然后将其提供给编码器。类似地，这里不是将输入直接馈送到解码器，而是将输入转换为嵌入矩阵并添加位置嵌入。</p><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lx"><img src="../Images/c229bff8fccd78c21b1682e45c51fed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_mn-nMRASEx5qEpYb-Uh-Q.png"/></div></div></figure><p id="e376" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="jv">好吧，但最终的问题是解码器到底是如何工作的？</em> </strong></p><p id="cb4e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们探索一下解码器模块内部的情况。</p><figure class="li lj lk ll fd ii er es paragraph-image"><div class="er es ly"><img src="../Images/ad34db0ee18427a7ccc32e3cc762531d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*XvBaXbNVqKYWBLADH6F79Q.png"/></div></figure><p id="a30d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">与编码器模块类似，解码器也有相同的内层。现在我们有了解码器的基本概念，让我们逐一检查每个组件。</p><blockquote class="js jt ju"><p id="61cb" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">掩蔽的多头注意力</p></blockquote><p id="bc43" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在我们的英语到法语的数据集里，假设我们的数据集看起来像这样</p><figure class="li lj lk ll fd ii er es paragraph-image"><div class="er es lz"><img src="../Images/8209ae54aacca6b6f995e503f9abffcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*8h3Iqs3QATWjWWVbQZaU-w.png"/></div></figure><p id="33dc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在训练过程中，由于我们有正确的目标句子，我们可以将整个目标句子作为输入输入到解码器，但只需稍加修改。我们了解到，解码器将<sos>作为第一个标记，并在每个时间步将下一个预测单词与输入组合，以预测预测目标句子的时间步，直到到达<eos>标记。我们还了解到，我们不是直接将输入输入给解码器，而是将其转换为嵌入矩阵，然后添加位置编码。</eos></sos></p><p id="8bc4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">假设下面的矩阵X是由嵌入矩阵和位置矩阵相加得到的。</p><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lm"><img src="../Images/82b7896dbb6df41630aec9818f9478ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VPCCltpg8exutlIh3I2Mfw.png"/></div></div></figure><p id="bbd5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，我们将X馈送给解码器第一层，即掩蔽的多头注意力。这与多头注意力层的工作原理完全相同，但有一点不同。</p><p id="39da" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了创造自我关注，我们创造了向量。因为我们正在计算多头注意力，所以我们创建了<em class="jv"> N </em>个<em class="jv"> Q，K，V </em>向量。我们解码器的输入句子是<em class="jv"> &lt; sos &gt; je vais bien </em>，我们知道自我注意机制如何将每个单词与其他单词联系起来，以获得更好的理解。但在这种情况下，这里有一个陷阱。在测试期间，解码器将仅具有直到前一步骤之前生成的单词作为输入。例如，假设在时间步长t=2时，解码器将只有输入字[ <em class="jv"> &lt; sos &gt;，je </em>，而没有其他字。所以，我们必须以同样的方式训练我们的模特。因此，我们的注意力机制应该只与单词<em class="jv"> je </em>相关，而与其他单词无关。为此，我们屏蔽了右边所有模型尚未预测到的单词。</p><figure class="li lj lk ll fd ii er es paragraph-image"><div class="er es ma"><img src="../Images/69ef60490ab48d2e58307c1db106b243.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Xz1rfNKHOlsQshX-HBCt0g.png"/></div></figure><p id="4bc9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">像这样屏蔽单词将有助于注意力机制只注意那些在测试期间对模型可用的单词。</p><blockquote class="js jt ju"><p id="27d5" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">多头注意力层</p></blockquote><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mb"><img src="../Images/f30c1c18a725b7b3e09560b7398601b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7dDawZSIOo8GByweVs710w.png"/></div></div></figure><p id="adfd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们用R表示编码器表示，用m表示作为多头关注子层的结果而获得的关注矩阵。因为这里我们有编码器和解码器之间的交互，所以这一层也被称为<strong class="iw hi"> <em class="jv">编码器和解码器关注层。</em>T15】</strong></p><p id="b177" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们使用从前一子层获得的关注矩阵M创建查询矩阵Q，并且我们使用编码器表示r创建关键值矩阵。</p><blockquote class="js jt ju"><p id="220c" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">前馈网络&amp;加法和范数元件</p></blockquote><p id="aa15" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="jv">前馈层和添加与定额组件层</em> </strong>的工作原理与编码器完全相同。</p><blockquote class="js jt ju"><p id="49a9" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">线性和softmax层</p></blockquote><p id="512d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一旦解码器学习了目标句子的表示，我们就将从最顶层解码器获得的输出馈送到linear和softmax层。线性层生成大小等于我们的词汇大小的逻辑。</p><blockquote class="js jt ju"><p id="34f1" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">将编码器和解码器放在一起</p></blockquote><figure class="li lj lk ll fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mb"><img src="../Images/5cba3530cbc2dca06e21bb6a26a40658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LG6NH-UES7o2eUv0oBtsQg.png"/></div></div></figure><blockquote class="kn"><p id="a06d" class="ko kp hh bd kq kr ks kt ku kv kw jr dx translated">进一步阅读</p></blockquote><ul class=""><li id="a378" class="jz ka hh iw b ix kx jb ky jf mc jj md jn me jr ke kf kg kh bi translated">关注就是你需要的全部—<a class="ae it" href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/2017/file/3f 5ee 243547 dee 91 FBD 053 C1 C4 a 845 aa-paper . pdf</a></li></ul><p id="25c7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">谢谢你！</p></div></div>    
</body>
</html>