<html>
<head>
<title>Credit Card Fraud Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">信用卡欺诈检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/credit-card-fraud-detection-fd634f70327d?source=collection_archive---------4-----------------------#2021-03-18">https://medium.com/analytics-vidhya/credit-card-fraud-detection-fd634f70327d?source=collection_archive---------4-----------------------#2021-03-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/9b4a185d019daa7e0d7f1c04958d21af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lci4cCUXgb6zZRyKmgWfVA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://dataaspirant.com/credit-card-fraud-detection-classification-algorithms-python/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="b8f5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在本文中，我们将使用各种机器学习算法来解决信用卡欺诈检测问题，我们还将比较所有这些算法，并找到哪一个最适合这个问题。</p><p id="835a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">文章分为几个部分</p><ol class=""><li id="b920" class="js jt hh iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">问题理解</li><li id="b964" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">数据审查</li><li id="6cd5" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">数据处理</li><li id="3ecb" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">特征选择</li><li id="5251" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">模型构建</li><li id="1e7d" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">欠采样</li><li id="5e55" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">过采样</li><li id="1a00" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">摘要</li></ol><p id="259e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所以让我们开始吧。</p><h1 id="a66b" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">1.问题理解</h1><p id="1da9" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">我们先来了解一下什么是信用卡诈骗</p><blockquote class="lj lk ll"><p id="0bbf" class="iu iv lm iw b ix iy iz ja jb jc jd je ln jg jh ji lo jk jl jm lp jo jp jq jr ha bi translated">信用欺诈是指犯罪分子利用他人的个人凭据以及他们的信用状况来借钱或使用信用卡购买商品或服务，而无意偿还债务。</p></blockquote><p id="efca" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，信用卡公司能够识别欺诈性的信用卡交易非常重要，这样客户就不会为他们没有购买的商品付费。</p><h1 id="dc9b" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">2.数据审查</h1><p id="7d38" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">数据集包含欧洲持卡人通过信用卡进行的交易。你可以在这里 找到Kaggle信用卡欺诈检测竞赛数据集<a class="ae it" href="https://www.kaggle.com/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank"> <em class="lm">的全部数据。该数据集显示了两天内发生的交易，其中284，807笔交易中有492笔欺诈。</em></a></p><p id="409e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">它们是数据集中的31列，其中特征V1、V2…v 28是用PCA获得的主要成分，唯一没有用PCA转换的特征是“时间”和“数量”。它们是一个名为class的列，如果交易是合法的(非欺诈)，则该列的值为0，如果是欺诈，则该列的值为1。</p><h1 id="2dec" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">3.数据处理</h1><p id="be5a" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">首先，让我们将数据可视化</p><figure class="lr ls lt lu fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lq"><img src="../Images/632c38a2fa351e754fff7566be5a06b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kSoiQg39oQYqqYpRgE0rxA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">数据</figcaption></figure><p id="0e84" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过观察，特征量的数据值位于0到25691.16的范围内，这显然是非常宽的范围。因此，我们需要通过应用标准化技术在(-1，1)之间调整它的值。</p><p id="24d7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所以我们正在做的是</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="be7c" class="ma kh hh lw b fi mb mc l md me">from sklearn.preprocessing import StandardScaler</span><span id="387d" class="ma kh hh lw b fi mf mc l md me">data['scaled_Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))</span><span id="f1ae" class="ma kh hh lw b fi mf mc l md me">data = data.drop(['Amount'],axis=1)</span></pre><p id="a47f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，通过使用sklearn预处理模块中的StandardScaler函数，我们可以对数据集中的Amount列进行转换。</p><h2 id="1a76" class="ma kh hh bd ki mg mh mi km mj mk ml kq jf mm mn ku jj mo mp ky jn mq mr lc ms bi translated">什么是StandardScaler？</h2><p id="35ee" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">标准缩放器将要素缩放为平均值为0，标准差为1。它输出非常接近正态分布。</p><p id="5747" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">请注意，我们正在进行整形(-1，1)以将数组序列转换为2D数组。</p><figure class="lr ls lt lu fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mt"><img src="../Images/114f3cbc6555231b7d52c7577c025773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e6JYIFAleMnUWHhEbRsmkA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">比例金额</figcaption></figure><p id="106d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在上图中，你可以看到输出。</p><h1 id="ad22" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">4.特征选择</h1><h2 id="0ef0" class="ma kh hh bd ki mg mh mi km mj mk ml kq jf mm mn ku jj mo mp ky jn mq mr lc ms bi translated">时间</h2><p id="aaf0" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">让我们绘制图表，显示合法交易和欺诈交易在时间变量方面的相似性。</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="3c1d" class="ma kh hh lw b fi mb mc l md me">data["Time_Hr"] = data["Time"]/3600 # convert to hours<br/>print(data["Time_Hr"].tail(5))<br/>fig, (ax1, ax2) = plt.subplots(2, 1, sharex = True, figsize=(10,6))<br/>ax1.hist(data.Time_Hr[data.Class==0],bins=48,color='g',alpha=0.5)<br/>ax1.set_title('Genuine')<br/>ax2.hist(data.Time_Hr[data.Class==1],bins=48,color='r',alpha=0.5)<br/>ax2.set_title('Fraud')<br/>plt.xlabel('Time (hrs)')<br/>plt.ylabel('# Transactions')</span></pre><figure class="lr ls lt lu fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mu"><img src="../Images/3ffdf070280909d2cea92549e8b6669d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f3GWsrZsu6x36Dofhyay8w.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">时间变量</figcaption></figure><p id="d6b5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过看图表，我们可以说，他们没有太多的信息来证明交易是欺诈。所以它没有任何预测能力，因为我们可以简单地从数据集中删除时间变量。</p><p id="1df4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在让我们检查变量数量</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="98ff" class="ma kh hh lw b fi mb mc l md me">fig, (ax3,ax4) = plt.subplots(2,1, figsize = (10,6), sharex = True)<br/>ax3.hist(data.Amount[data.Class==0],bins=50,color='g',alpha=0.5)<br/>ax3.set_yscale('log') # to see the tails<br/>ax3.set_title('Genuine') # to see the tails<br/>ax3.set_ylabel('# transactions')<br/>ax4.hist(data.Amount[data.Class==1],bins=50,color='r',alpha=0.5)<br/>ax4.set_yscale('log') # to see the tails<br/>ax4.set_title('Fraud') # to see the tails<br/>ax4.set_xlabel('Amount ($)')<br/>ax4.set_ylabel('# transactions')</span></pre><figure class="lr ls lt lu fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mv"><img src="../Images/bc62406c0d3b1b65f1e28e16d194203d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EHOxdi34z4Hu8M4lUgiE1Q.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">数量变量</figcaption></figure><p id="cb7b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，你可以清楚地看到两个图表中的巨大差异，即真实和欺诈。同样重要的一点是，在真正的(合法交易)中，超过1万英镑的交易已经完成，但在欺诈案件中，没有交易超过1万英镑。</p><p id="a6b7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所以我们可以说它有预测能力。</p><p id="0a7c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">同样，我们需要检查V1到V28的所有功能，以便如果任何变量在合法和欺诈交易中没有差异，那么我们可以放弃该功能。你可以在我的<a class="ae it" href="https://github.com/kamlesh11/Credit-Card-Fraud-Detection" rel="noopener ugc nofollow" target="_blank"> github repo </a>看到每个变量的分析。</p><p id="e171" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，我们已经选择了所有可训练的特征，因此我们可以将数据分成训练集和测试集。</p><h2 id="8df8" class="ma kh hh bd ki mg mh mi km mj mk ml kq jf mm mn ku jj mo mp ky jn mq mr lc ms bi translated">分割数据</h2><p id="7014" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">我们将把数据分成80%的训练集和20%的测试集</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="43e1" class="ma kh hh lw b fi mb mc l md me">def split_data(df,drop_list):<br/>    df=df.drop(drop_list,axis=1)<br/>    X=df[df['Class']==1]<br/>    Y=df[df['Class']==0]<br/>    X=df.drop(['Class'], axis = 1) <br/>    Y=df["Class"] <br/>    xData = X.values <br/>    yData = Y.values <br/>    xTrain, xTest, yTrain, yTest = train_test_split( <br/>            xData, yData, test_size = 0.2, random_state = 42)<br/>    return xTrain, xTest, yTrain, yTest</span><span id="6a16" class="ma kh hh lw b fi mf mc l md me">drop_list = ['V13','V15','V22','V26','V25','V23']<br/>x_Train,x_Test,y_Train,y_Test=split_data(data,dropList)</span></pre><p id="137c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们正在创建helper函数print_scores，它将接受y_Test、y_preds和y_pred_probs(预测概率),并将为我们打印所有测量值，即precision、recall、f1_score、AUC_score、ROC_AUC score和kappa score(cohen kappa score)。</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="594d" class="ma kh hh lw b fi mb mc l md me"># print_scores function for printing scores<br/>def print_scores(y_test,y_pred,y_pred_prob):<br/>    precision,recall,_ =      precision_recall_curve(y_test,y_pred_prob[:,1])<br/>    print('precision_score : ',precision_score(y_test,y_pred))<br/>    print('recall_score : ',recall_score(y_test,y_pred))<br/>    print('f1 score : ', f1_score(y_test,y_pred))<br/>    print('AUC score : ', auc(recall,precision))<br/>    print('ROC_AUC score : ', roc_auc_score(y_test, <br/>y_pred_prob[:,1]))<br/>    print('kappa : ', cohen_kappa_score(y_test,y_pred))</span></pre><h1 id="2bd2" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">5.模型结构</h1><p id="0076" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">具体来说，我已经建立了7个不同的模型</p><h2 id="0538" class="ma kh hh bd ki mg mh mi km mj mk ml kq jf mm mn ku jj mo mp ky jn mq mr lc ms bi translated">1.朴素贝叶斯</h2><ul class=""><li id="5c12" class="js jt hh iw b ix le jb lf jf mw jj mx jn my jr mz jy jz ka bi translated">它是一种基于贝叶斯定理的分类技术，假设预测因子之间是独立的。简而言之，朴素贝叶斯分类器假设一个类中特定特征的存在与任何其他特征的存在无关。</li></ul><figure class="lr ls lt lu fd ii er es paragraph-image"><div class="er es na"><img src="../Images/67190b98e43c2d429467bf308e0ac658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/0*Yioy69l5MGqm-McD.jpg"/></div></figure><ul class=""><li id="0550" class="js jt hh iw b ix iy jb jc jf ju jj jv jn jw jr mz jy jz ka bi translated">具体来说，它直接从概率的角度使用贝叶斯定理的概念。</li><li id="8d07" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr mz jy jz ka bi translated">假设交易是合法的，我们正在计算交易被欺诈的概率。同样的道理，最终的预测是两者的最大值。</li><li id="147b" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr mz jy jz ka bi translated">更多信息可以访问<a class="ae it" href="https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/" rel="noopener ugc nofollow" target="_blank"> <em class="lm">这里</em> </a> <em class="lm">。</em></li></ul><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="eada" class="ma kh hh lw b fi mb mc l md me">from sklearn.naive_bayes import GaussianNB<br/>NBclf=GaussianNB()<br/>NBclf.fit(x_Train,y_Train)<br/>NB_pred,NB_pred_prob=NBclf.predict(x_Test),NBclf.predict_proba(x_Test)<br/>print_scores(y_Test,NB_pred,NB_pred_prob)</span></pre><h2 id="ff5d" class="ma kh hh bd ki mg mh mi km mj mk ml kq jf mm mn ku jj mo mp ky jn mq mr lc ms bi translated">2.逻辑回归</h2><ul class=""><li id="9fb2" class="js jt hh iw b ix le jb lf jf mw jj mx jn my jr mz jy jz ka bi translated"><strong class="iw hi">逻辑回归</strong>是因变量为二分变量(二元)时进行的适当回归分析。像所有回归分析一样，逻辑回归是一种预测分析。逻辑回归用于描述数据，并解释一个因变量与一个或多个名义变量、序数变量、区间变量或比率水平自变量之间的关系。</li><li id="e808" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr mz jy jz ka bi translated">简单地说，它使用逻辑函数(Sigmoid函数)进行预测。</li><li id="9f93" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr mz jy jz ka bi translated">点击可以了解更多关于<a class="ae it" href="https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc" rel="noopener" target="_blank"> <em class="lm">的逻辑回归知识。</em></a></li></ul><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="f00c" class="ma kh hh lw b fi mb mc l md me">from sklearn.linear_model import LogisticRegression<br/>lr = LogisticRegression(C = 0.01, penalty = 'l2',max_iter=1000)<br/>lr.fit(x_Train, y_Train)<br/>lr_pred,lr_prob=lr.predict(x_Test),lr.predict_proba(x_Test)<br/>print_scores(y_Test,lr_pred,lr_prob)</span></pre><h2 id="0099" class="ma kh hh bd ki mg mh mi km mj mk ml kq jf mm mn ku jj mo mp ky jn mq mr lc ms bi translated">3.线性判别分析</h2><ul class=""><li id="9519" class="js jt hh iw b ix le jb lf jf mw jj mx jn my jr mz jy jz ka bi translated"><a class="ae it" href="https://towardsdatascience.com/linear-discriminant-analysis-in-python-76b8b17817c2#:~:text=Linear%20Discriminant%20Analysis%20(LDA)%20is,as%20much%20information%20as%20possible." rel="noopener" target="_blank"> <em class="lm">线性判别分析</em> </a>，正态判别分析，或称判别函数分析是Fisher线性判别的推广，是统计学等领域使用的一种方法，寻找表征或区分两类或两类以上对象或事件的特征的线性组合。</li></ul><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="2360" class="ma kh hh lw b fi mb mc l md me">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis<br/>lda_clf=LinearDiscriminantAnalysis()<br/>lda_clf.fit(x_Train,y_Train)<br/>lda_pred, lda_prob = lda_clf.predict(x_Test),lda_clf.predict_proba(x_Test)<br/>lda_precision,lda_recall,_ = precision_recall_curve(y_Test,lda_prob[:,1])<br/>print_scores(y_Test,lda_pred,lda_prob)</span></pre><h2 id="4ede" class="ma kh hh bd ki mg mh mi km mj mk ml kq jf mm mn ku jj mo mp ky jn mq mr lc ms bi translated">4.决策图表</h2><figure class="lr ls lt lu fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nb"><img src="../Images/98e08bbb49cd6422c7d2790c3e3264db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Uu39HI9lf-uVFk5M.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">决策图表</figcaption></figure><ul class=""><li id="36f2" class="js jt hh iw b ix iy jb jc jf ju jj jv jn jw jr mz jy jz ka bi translated">我希望你能从上面的图片中了解什么是决策树。</li></ul><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="fdaa" class="ma kh hh lw b fi mb mc l md me">from sklearn.tree import DecisionTreeClassifier<br/>Dtree = DecisionTreeClassifier()<br/>Dtree.fit(x_Train,y_Train)<br/>DT_preds,DT_probs = Dtree.predict(x_Test),Dtree.predict_proba(x_Test)<br/>print_scores(y_Test,DT_preds,DT_probs)</span></pre><h2 id="7156" class="ma kh hh bd ki mg mh mi km mj mk ml kq jf mm mn ku jj mo mp ky jn mq mr lc ms bi translated">5.随机森林</h2><p id="e76d" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">随机森林(随机森林分类器)是许多这样的决策树的集合和许多决策树的组合。</p><figure class="lr ls lt lu fd ii er es paragraph-image"><div class="er es nc"><img src="../Images/cd339746c1a4fb354eb0476a1f70e8c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/0*p50C9QFrzuhIMTe4.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fcorporatefinanceinstitute.com%2Fresources%2Fknowledge%2Fother%2Frandom-forest%2F&amp;psig=AOvVaw0VQzHvQ5OoUbL9QUouLd2A&amp;ust=1616169824690000&amp;source=images&amp;cd=vfe&amp;ved=0CA0QjhxqFwoTCPCsvPmbuu8CFQAAAAAdAAAAABAx" rel="noopener ugc nofollow" target="_blank">谷歌</a></figcaption></figure><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="b335" class="ma kh hh lw b fi mb mc l md me">from sklearn.ensemble import RandomForestClassifier<br/>RF_clf = RandomForestClassifier()<br/>RF_clf.fit(x_Train, y_Train)<br/>RF_pred,RF_prob = RF_clf.predict(x_Test),RF_clf.predict_proba(x_Test)<br/>print_scores(y_Test,RF_pred,RF_prob)</span></pre><h2 id="8eb1" class="ma kh hh bd ki mg mh mi km mj mk ml kq jf mm mn ku jj mo mp ky jn mq mr lc ms bi translated">6.支持向量机</h2><p id="e11b" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">支持向量机算法的目标是在N维空间(N-特征的数量)中找到一个超平面，该超平面清楚地分类数据点。</p><p id="a54c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">它的概念是取N维输入并将其扩展到第N+1维，找到超平面，然后将其降低到第N维。我们得到了决策边界。</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="76ba" class="ma kh hh lw b fi mb mc l md me">from sklearn.svm import SVC<br/>clf = SVC(probability=True)<br/>SVM_pred,SVM_prob=SVMclf.predict(x_Test),SVMclf.predict_proba(x_Test)<br/>print_scores(y_Test,SVM_pred,SVM_prob)</span></pre><h2 id="8cf3" class="ma kh hh bd ki mg mh mi km mj mk ml kq jf mm mn ku jj mo mp ky jn mq mr lc ms bi translated">7.深度神经网络</h2><p id="7499" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">我已经建立了一个5层的神经网络，通过它我得到了非常好的结果。</p><p id="8e2a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">但是您可以微调以获得更好的性能。</p><p id="79e6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我使用4个密集层和1个丢弃层，阈值为0.5，优化器为adam，损失为binary_crossentropy，因为这是一个分类问题，特别是2个二进制分类问题。</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="3b96" class="ma kh hh lw b fi mb mc l md me">import keras<br/>from keras import layers<br/>from keras.models import load_model</span><span id="54b0" class="ma kh hh lw b fi mf mc l md me"># Deep Neural Network<br/><br/>model = keras.Sequential([<br/>layers.Dense(input_dim = 23,units= 23, activation = 'relu'),<br/>layers.Dense(units = 20,activation = 'relu'),<br/>layers.Dropout(0.5),<br/>layers.Dense(units = 16,activation = 'relu'),<br/>layers.Dense(units =1, activation = 'sigmoid')])</span><span id="138a" class="ma kh hh lw b fi mf mc l md me">model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])<br/>model.fit(x_Train, y_Train, batch_size = 16, epochs = 5)</span><span id="b9c9" class="ma kh hh lw b fi mf mc l md me">Dnn_preds,Dnn_prob= model.predict(x_Test),model.predict_proba(x_Test)print_scores(y_Test ,DNN_preds)</span><span id="b41d" class="ma kh hh lw b fi mf mc l md me">print('precision_score :',precision_score(y_Test,Dnn_preds.round()))<br/>print('recall_score : ',recall_score(y_Test,Dnn_preds.round()))<br/>print('f1_score : ',<br/>f1_score(y_Test,Dnn_preds.round()))<br/>print('AUC_score : ', auc(DNN_recall,DNN_precision))<br/>print('ROC_AUC_score : ', roc_auc_score(y_Test, Dnn_prob))<br/>print('kappa : ', cohen_kappa_score(y_Test,Dnn_preds.round()))</span></pre><p id="faf2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过将数据拟合到所有模型，我们得到了以下结果。</p><figure class="lr ls lt lu fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nd"><img src="../Images/b0ca3b1b4cc4b205fe620cdcf82d9e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WkEBAaioZaapbEwszGeehg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">表演</figcaption></figure><p id="d9a3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过查看上面的性能表，你可以肯定地说，随机森林正在赢得这场比赛。</p><p id="26fe" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们看看我们的赢家随机森林的精度与召回分数</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="cb29" class="ma kh hh lw b fi mb mc l md me">plt.figure(figsize=(8,6))<br/>plt.title('Precision Recall Curve of Random Forest Classifier')<br/>plt.plot(RF_recall, RF_precision, label='Random Forest',color='violet')<br/>plt.xlabel('Recall')<br/>plt.ylabel('Precision')<br/>plt.legend(loc='lower left')<br/>plt.show()</span></pre><figure class="lr ls lt lu fd ii er es paragraph-image"><div class="er es ne"><img src="../Images/d53bacbd034ea34c826db1d09fea2969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*Pw1gU7fXnYYrj_NA_B5pbw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">精确召回图</figcaption></figure><p id="bf0e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">同样，您可以绘制任何分类器的图形。我只是给你看其中最好的一个。</p><p id="b121" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你可以从<a class="ae it" href="https://github.com/kamlesh11/Credit-Card-Fraud-Detection/blob/main/Credit%20card%20fraud%20detection%20Oversampling.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="lm">这里</em>得到完整的代码。</a></p><h2 id="2103" class="ma kh hh bd ki mg mh mi km mj mk ml kq jf mm mn ku jj mo mp ky jn mq mr lc ms bi translated">但是在我们的数据集中有一个问题</h2><figure class="lr ls lt lu fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/91395bd2786dd820095a05a6069a275b.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*AXMlnu9sIRfJlsTdnvmMbA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">数据饼图</figcaption></figure><p id="6552" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这里，您可以看到数据高度偏向于真实负值，即真实(合法)交易。</p><p id="e460" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们稍后会看到我们能做些什么来克服这个问题。</p><p id="e788" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们知道该数据高度偏向于负成交，即合法交易，因此我们需要采取一些措施来平衡数据分布。</p><p id="d147" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有两种技术我们可以应用欠采样和过采样</p><p id="b810" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">先来看欠采样。</p><h1 id="5d3a" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">6.欠采样</h1><blockquote class="ng"><p id="cfea" class="nh ni hh bd nj nk nl nm nn no np jr dx translated"><a class="ae it" href="https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/" rel="noopener ugc nofollow" target="_blank"><em class="nq"/></a><em class="nq">欠采样，包括通过消除属于多数类的样本来减少数据，目的是均衡每个类的样本数量。</em></p></blockquote><p id="5c69" class="pw-post-body-paragraph iu iv hh iw b ix nr iz ja jb ns jd je jf nt jh ji jj nu jl jm jn nv jp jq jr ha bi translated">因此，我们将从合法交易中删除一些数据，以平衡数据。</p><p id="b12f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们看看欠采样的代码。请注意，我们选择随机选项并删除它们以获得相同的分布。我们这样做是以一些有用的信息为代价的。😢</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="8ae8" class="ma kh hh lw b fi mb mc l md me">df=data<br/>fraud_ind = np.array(df[df.Class == 1].index)<br/>gen_ind = df[df.Class == 0].index<br/>n_fraud = len(df[df.Class == 1])<br/># random selection from genuine class<br/>random_gen_ind = np.random.choice(gen_ind, n_fraud, replace = False)<br/>random_gen_ind = np.array(random_gen_ind)<br/># merge two class indices: random genuine + original fraud<br/>under_sample_ind = np.concatenate([fraud_ind,random_gen_ind])<br/># Under sample dataset<br/>undersample_df = df.iloc[under_sample_ind,:]<br/>y_undersample  = undersample_df['Class'].values #target<br/>X_undersample = undersample_df.drop(['Class'],axis=1).values #features</span><span id="1484" class="ma kh hh lw b fi mf mc l md me">print("# transactions in undersampled data: ", len(undersample_df))<br/>print("% genuine transactions: ",len(undersample_df[undersample_df.Class == 0])/len(undersample_df))<br/>print("% fraud transactions: ", sum(y_undersample)/len(undersample_df))</span></pre><p id="cdaa" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过应用非抽样，我们只剩下984个观察值。</p><p id="3fcd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">好了，现在让我们看看算法的性能。</p><figure class="lr ls lt lu fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nw"><img src="../Images/4f4d5b71c09d2246afb24b55dc4bc235.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0i2Xfs3kckUCXx9M8bY7bA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">欠采样性能</figcaption></figure><p id="89df" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过使用欠采样，我们得到了一些好的结果。但现在，兰登森林和DNN之间展开了激烈的竞争。尽管如此，兰登森林仍然在所有公司中表现良好。</p><p id="0c2d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在让我们看看它的精确回忆图</p><figure class="lr ls lt lu fd ii er es paragraph-image"><div class="er es nx"><img src="../Images/00460faa40524870a57b7e1286af0ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*VuMzhpgEP_u_BkzwOTwjDA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">应用欠采样后随机森林的精确召回图</figcaption></figure><p id="8082" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你可以看到它现在做得好多了。</p><p id="61d2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们看看过采样的性能如何。</p><h1 id="ecfe" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">7.过采样</h1><blockquote class="ng"><p id="d9b3" class="nh ni hh bd nj nk nl nm nn no np jr dx translated"><a class="ae it" href="https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/" rel="noopener ugc nofollow" target="_blank"> <em class="nq"> SMOTE </em> </a> <em class="nq">首先随机选择一个少数类实例a，找到其k个最近的少数类邻居。然后，通过随机选择k个最近邻居b中的一个并连接a和b以在特征空间中形成线段，来创建合成实例。合成实例生成为两个选定实例a和b的凸组合。</em></p></blockquote><p id="d3de" class="pw-post-body-paragraph iu iv hh iw b ix nr iz ja jb ns jd je jf nt jh ji jj nu jl jm jn nv jp jq jr ha bi translated">在这里，我们将一些样本添加到欺诈交易中，以均衡数据。</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="d214" class="ma kh hh lw b fi mb mc l md me">drop_list = ['V13','V15','V22','V26','V25','V23']<br/>Y = data['Class']<br/>X=data.drop(['Class']+drop_list, axis = 1)<br/>X_resample, y_resample = SMOTE().fit_resample(X, Y)<br/>print(X_resample.shape)</span></pre><p id="2919" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">就这样，我们的数据现在被过度采样了。过采样后的样本总数为568630。现在数据的分布是50-50。</p><figure class="lr ls lt lu fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nw"><img src="../Images/68f7ca669206a06bff61790e8b9fa53e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E0uDRvPPDHSh14kC6FkXdw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">过采样性能</figcaption></figure><p id="d468" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上述性能肯定是令人满意的，因为在它所有的算法都做得非常好，但这里值得注意的是，我们在随机森林的测量中获得了大约100%的准确性。</p><p id="b5a0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">另一个需要注意的重要事项是，在过采样中，我没有使用SVM，因为我们的数据大小增加到568630个样本，如果我们对其应用随机森林，它将无休止地运行。</p><p id="b3d0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在让我们看看精确回忆曲线。</p><figure class="lr ls lt lu fd ii er es paragraph-image"><div class="er es ny"><img src="../Images/19cfea5ca284ee09a24b2ae6b68f2906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*tN8elkt1aohUjzRYZiu95Q.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">应用过采样后随机森林的精确召回曲线</figcaption></figure><p id="192b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于我们的问题来说，它是一个近乎完美的分类器。</p><blockquote class="lj lk ll"><p id="b951" class="iu iv lm iw b ix iy iz ja jb jc jd je ln jg jh ji lo jk jl jm lp jo jp jq jr ha bi translated">结论声明是，在应用所有技术后，我们在过采样中获得了最佳结果，在分析上述结果后，该问题的最佳算法是<strong class="iw hi">随机森林。</strong></p></blockquote><h1 id="94b2" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">8.摘要</h1><p id="aafe" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">我们创建了7个模型来解决这个问题，并对其进行了分析，根据我们得到的结果，发现随机森林是迄今为止最好的。这就是信用卡欺诈检测之旅。🍻</p><p id="7225" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你可以在我的github库<a class="ae it" href="https://github.com/kamlesh11/Credit-Card-Fraud-Detection" rel="noopener ugc nofollow" target="_blank">这里</a>获得完整的代码。</p></div></div>    
</body>
</html>