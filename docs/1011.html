<html>
<head>
<title>Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-c37ef7011a2f?source=collection_archive---------28-----------------------#2021-02-09">https://medium.com/analytics-vidhya/gradient-descent-c37ef7011a2f?source=collection_archive---------28-----------------------#2021-02-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="4a5a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">梯度下降是机器学习领域中使用的基本参数优化技术。它实际上是基于成本函数相对于参数的斜率。让我们考虑一个例子:</p><p id="5036" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你是一个喜欢冒险的人，你有机会攀登珠穆朗玛峰，地球上海拔最高的山峰。你的团队开始攀登，不知何故，经过许多努力，你们都到达了顶端。你们都决定庆祝这次胜利，所以你的一个朋友开了一瓶香槟。作为一个敏锐的观察者，你看到软木塞从山上滚下来，它朝着地形最陡峭的方向移动。</p><p id="17c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">梯度下降算法显示了与丘陵地形相同的行为。但是这里的丘陵地形是成本函数。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/155b8f2562cd25a13acff8c4e67cbc35.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*g-xQNEmFdWwBPmpa1_NmSg.jpeg"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">价值函数</figcaption></figure><p id="a3d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们必须走下这个成本函数山，以达到成本函数的全局最小值或最优值。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jo"><img src="../Images/615eb898577df54bad33100b8f6702da.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/1*Wc6uQlExrN29HSzkAsp3oQ.gif"/></div></figure><p id="ed20" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的成本函数的斜率由参数的偏导数决定。让我们详细讨论一下成本函数。</p><h1 id="366b" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">机器学习模型</h1><p id="ef44" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">考虑两个变量X和Y，各有100个不同的值。其中X是特征变量，Y是目标变量。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ks"><img src="../Images/256df38312189bd339b40e4a9844646c.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*JXa_SuXY6stE1lod-QF9wQ.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">这是我们数据的散点图</figcaption></figure><blockquote class="kt ku kv"><p id="9a69" class="ie if kw ig b ih ii ij ik il im in io kx iq ir is ky iu iv iw kz iy iz ja jb ha bi translated">让我们考虑一个假设:Y' = mX + b</p></blockquote><p id="5a3a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个X和Y数据被输入到我们的假设模型中，我们的模型预测每个X值的Y ’,而<em class="kw"> m </em>和<em class="kw"> b </em>分别是假设函数和偏差的斜率。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es la"><img src="../Images/87c1bbdc5fe658f7775ff1f95fbe8a81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*e9ts1gijLPxSHY6wF28MGQ.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">假设模型</figcaption></figure><p id="bc8a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此<em class="kw"> Y </em>是对应的<em class="kw"> X </em>的实际值，而<em class="kw">Y’</em>是将<em class="kw"> X </em>输入模型时的预测结果。</p><p id="668d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，Y和Y’可能具有不同的值。对吗？</p><blockquote class="kt ku kv"><p id="c4e7" class="ie if kw ig b ih ii ij ik il im in io kx iq ir is ky iu iv iw kz iy iz ja jb ha bi translated">D <!-- -->差值= Y '(预测)- Y(实际)</p></blockquote><p id="fd3b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种差异就是模型的误差。更具体地说，这个错误帮助我们定义我们的<em class="kw">成本函数。</em></p><h1 id="7e27" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">价值函数</h1><p id="f407" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">成本函数/损失函数有助于评估模型的性能。代价函数给出了整个训练样本的平均损失，这里<em class="kw"> N </em>是训练样本的总数。</p><p id="15e9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">成本函数由误差平方和给出。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lb"><img src="../Images/d5f4885fefbb00ad492a7f379c71399f.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/1*YaDsxSKCTojM_bMBgVvGsA.gif"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">价值函数</figcaption></figure><blockquote class="kt ku kv"><p id="6146" class="ie if kw ig b ih ii ij ik il im in io kx iq ir is ky iu iv iw kz iy iz ja jb ha bi translated">我们采用总平方误差，因为它使误差距离更大，使得坏的预测比好的预测更引人注目，并且很容易计算平方函数的导数。</p></blockquote><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lc"><img src="../Images/66717a24a9a5db4b04205536284fd2e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*VrIZ2ZutOIj0srw3Fm7onw.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">成本函数山</figcaption></figure><p id="3e00" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们已经定义了我们的丘陵地形，我们准备沿着斜坡开始我们的旅程，让重力做所有的工作。Wheeeeeee。</p><h2 id="8f31" class="ld jq hh bd jr le lf lg jv lh li lj jz ip lk ll kd it lm ln kh ix lo lp kl lq bi translated">最小化成本函数</h2><p id="8f37" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">最终，我们的目的只是降低成本，以获得与成本函数值相对应的参数的最优值。</p><p id="b58d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了最小化成本，我们沿着成本函数的斜率得到最低点(全局最小值)。</p><p id="fa37" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">成本函数的斜率由成本相对于参数<strong class="ig hi"><em class="kw">【m】</em></strong><em class="kw">(假设函数的斜率)</em><strong class="ig hi"><em class="kw">b</em></strong><em class="kw">(偏差)的偏导数决定。</em></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/669dc6ef286acd83c75638df9e539177.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/1*27tr8E8EtwA6w2QB-7oQtw.gif"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">衍生产品</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ls"><img src="../Images/47dbfc2df011c6365c0785d7825d31c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/1*PI9haymSZsivJ8P7Ubsi8A.gif"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">衍生工具</figcaption></figure><p id="6057" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些导数给了我们成本函数的斜率。但是我们怎么下山呢？</p><p id="c60c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了沿着斜坡前进，我们需要沿着斜坡定义我们的步骤，这样我们将到达最低点(全局最小值)。</p><h1 id="2630" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">学习率:α</h1><blockquote class="kt ku kv"><p id="5f41" class="ie if kw ig b ih ii ij ik il im in io kx iq ir is ky iu iv iw kz iy iz ja jb ha bi translated">我们的步伐是由学习速度决定的。学习率越高，我们的步数就越大，因此我们可能会超过预期的点。然而，对于一个小的学习率，我们可能要走很多步才能到达最低点。</p></blockquote><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lt"><img src="../Images/aeccfb5962c3b5243514aba16cd58f8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*W9CcTtU7PbCiK9VcMRc-tA.jpeg"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">错误选择的学习率可能导致我们的成本函数永远不会达到全局最小值。</figcaption></figure><p id="772d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个值从来不是一个特定的值，它随成本函数而变化，因此必须调整这个值以达到他/她的成本函数的最佳结果。</p><h2 id="be6c" class="ld jq hh bd jr le lf lg jv lh li lj jz ip lk ll kd it lm ln kh ix lo lp kl lq bi translated">在每个步骤之后更新参数</h2><p id="3610" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">现在我们有了山坡，也定义了我们的步骤，我们可以沿着山坡走了。</p><p id="8c31" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们将我们的数据<em class="kw"> X </em>和<em class="kw"> Y </em>给假设函数时，我们的成本函数在每次迭代后计算损失，并且根据损失，我们借助于关于我们的参数<strong class="ig hi"> <em class="kw"> m </em> </strong>和<strong class="ig hi"> <em class="kw"> b. </em> </strong>的导数来获得我们的陡峭侧，因此在获得成本函数的陡峭侧之后，成本函数值更接近最小值和我们的参数<strong class="ig hi"> <em class="kw"> m</em></strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lu"><img src="../Images/717f1fd322170c37d2f4f9c687e93494.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/1*khnrxOGeUT7s4PYYZL_nAQ.gif"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">m的值被更新</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lv"><img src="../Images/538b111eea227c26d2ccbdf0c86f5744.png" data-original-src="https://miro.medium.com/v2/resize:fit:196/1*KHHqC9u67wG9HPFx_mG5zQ.gif"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">b的值被更新</figcaption></figure><blockquote class="kt ku kv"><p id="16d8" class="ie if kw ig b ih ii ij ik il im in io kx iq ir is ky iu iv iw kz iy iz ja jb ha bi translated">这就是线性回归线随斜率和偏差更新的方式。</p></blockquote><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lw"><img src="../Images/64e3c5e1e4001fd3468316c06893ccbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*asOuB7X1gXdmtA_HW-JXFQ.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lx"><img src="../Images/0066db1b033684ed7930beb793e56772.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*v8XXEBT2MTXqKUYHUS2QWQ.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ly"><img src="../Images/1c2b1c411386556552ca7c240fc1c62a.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*9vqO4UMADtQ74KGdBgGeBg.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lz"><img src="../Images/340820092ab8acbc84dbb59d504ca19c.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*SxTMd965psSPEHAoTib3Xg.png"/></div></figure><blockquote class="kt ku kv"><p id="2634" class="ie if kw ig b ih ii ij ik il im in io kx iq ir is ky iu iv iw kz iy iz ja jb ha bi translated">最后，我们得到了斜率和偏置的优化值，这是最终的图表</p></blockquote><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ma"><img src="../Images/f4c8b8dce3621ece2abfa573f49a95cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*gTFYx0fAchwb0bitNq1T_g.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">斜率和偏差的优化值</figcaption></figure><blockquote class="kt ku kv"><p id="ede9" class="ie if kw ig b ih ii ij ik il im in io kx iq ir is ky iu iv iw kz iy iz ja jb ha bi translated">这是梯度下降算法的python实现，你可以在我最后提供的链接中找到它。</p></blockquote><pre class="jd je jf jg fd mb mc md me aw mf bi"><span id="bbe4" class="ld jq hh mc b fi mg mh l mi mj">def gradient_descent(X,y):<br/>    m = b = 0<br/>    learning_rate = 0.01<br/>    epochs = 1000 <br/>    n = len(X)<br/>    for epoch in range(epochs):<br/>        y_predicted = m*X + b<br/>        cost = (1/(2*n))*sum([val**2 for val in (y_predicted-y)]) <br/>        derivative_wrt_m = (1/n)*sum((y_predicted-y)*X)<br/>        derivative_wrt_b = (1/n)*sum(y_predicted-y) <br/>        m = m - learning_rate * derivative_wrt_m<br/>        b = b - learning_rate * derivative_wrt_b <br/>    return m,b,cost</span></pre><p id="63d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以这种方式，我们得到参数<strong class="ig hi"> <em class="kw"> m </em> </strong>和<strong class="ig hi"> <em class="kw"> b </em> </strong>的最优值，对应于成本函数的最小值。因此我们得到了我们的最小值。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ly"><img src="../Images/f5989d49ae0c8c9907dea7b3ecf124ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/1*dJHi8v2L2ksun53QqB5JKg.gif"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">达到最低限度</figcaption></figure><p id="eb67" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我已经在我的<a class="ae mk" href="https://github.com/pradyyadav/Gradient_Descent/blob/main/gradient_descent.ipynb" rel="noopener ugc nofollow" target="_blank"> Github库</a>中的jupyter笔记本上实现了这个算法。请看一看。</p><p id="239c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，我们已经达到了全局最小值，这是成本函数的期望值。但是有时可能会发生这样的情况，即成本函数值不是达到全局最小值，而是停留在局部最小值。</p><p id="9726" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好了，这是下一部分的故事。</p><p id="b218" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">谢了。</p></div></div>    
</body>
</html>