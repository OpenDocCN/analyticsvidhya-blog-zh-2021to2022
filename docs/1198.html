<html>
<head>
<title>Implementation of Principal Component Analysis(PCA) in K Means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析在K均值聚类中的实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementation-of-principal-component-analysis-pca-in-k-means-clustering-b4bc0aa79cb6?source=collection_archive---------0-----------------------#2021-02-19">https://medium.com/analytics-vidhya/implementation-of-principal-component-analysis-pca-in-k-means-clustering-b4bc0aa79cb6?source=collection_archive---------0-----------------------#2021-02-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="af0f" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">初学者使用Python及其库将使用2个组件的PCA应用于K均值聚类算法的方法。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/4c5e43912ac9b2be414ce027cbef0343.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l3E9WcAUgw5dawBusDMTzg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">K表示聚类后的结果</figcaption></figure><p id="5855" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">先决条件</strong></p><p id="773a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">本文假设你熟悉PCA背后的基本理论，K表示算法，并了解Python编程语言。</p><p id="5baf" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">k均值聚类是最简单而有效的无监督算法之一。首先让我们简单描述一下这个算法的作用。</p><p id="a01a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj"> K表示算法<br/> </strong>假设我们有一个数据集，有两个特征x1和x2。这是未标记的数据，我们的目标是找到K个彼此相似的组或“<em class="kj">簇</em>”。假设我们的训练集如下所示</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kk"><img src="../Images/52005e038c672bfc2ac60d5988306118.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*KdoXYsvB4XpmCspSCFdWCA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">简单的K表示集群</figcaption></figure><p id="efee" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们可以清楚地看到有两个集群，让我们将它们命名为<em class="kj">集群0 </em>和<em class="kj">集群1 </em>。每个聚类与每个聚类唯一的<strong class="jp hj">质心</strong>相关联。这个算法迭代直到质心不改变它的位置。</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><p id="9403" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">既然我们已经简要描述了K的含义，让我们继续进行主成分分析，以下简称PCA。</p><p id="b122" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">主成分分析</strong></p><p id="6b40" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们可以很容易地看到前一个例子中的集群，因为它包含两个特征。比如说，1000个特性呢？我不知道你怎么想，但是我绝对不能想象1000维！</p><p id="bbea" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">因此，主成分分析开始发挥作用。这背后的基本思想是，它允许我们将数据集的维度(或特征)减少到小于当前特征数量的任何数量。</p><p id="e381" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">PCA的另一个用途是压缩数据，从而节省计算时间。在下面的文章中，我们将使用PCA来解决这两个问题。</p><p id="7478" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">为了检查哪个主成分(PC)比其他成分更重要，也就是说哪个分量更重，我们计算每个成分的方差，然后绘制屏幕图来比较这些值。需要注意的一点是，如果您的数据具有不同的比例，如下所示</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ks"><img src="../Images/78214cab8e834b19b09b9f5703c3e9c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IQtwm1_N3yFCal0IWIPwjQ.png"/></div></div></figure><p id="9f1d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">那么最好在继续之前进行特征缩放或均值归一化，因为如果不进行缩放，结果会偏向a。</p><p id="07c4" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我不会深入研究每件事背后的数学原理，但是在本文结束时，您一定能够编写代码。我假设你熟悉python及其著名的库——<strong class="jp hj">pandas</strong>、<strong class="jp hj"> numpy </strong>、<strong class="jp hj"> matplotlib </strong>和<strong class="jp hj"> sklearn </strong>。</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h1 id="1dff" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">让我们编码吧！</h1><p id="320b" class="pw-post-body-paragraph jn jo hi jp b jq ll ij js jt lm im jv jw ln jy jz ka lo kc kd ke lp kg kh ki hb bi translated"><strong class="jp hj">关于数据集</strong>:包含217列爱好，其中1表示是。</p><p id="7b68" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">因此，第一步将是导入所有必要的库。</p><pre class="iy iz ja jb fd lq lr ls lt aw lu bi"><span id="b28d" class="lv ku hi lr b fi lw lx l ly lz">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from sklearn.cluster import KMeans<br/>from sklearn.decomposition import PCA</span></pre><p id="be32" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下一步是加载我们的数据集。我从<a class="ae ma" href="https://www.kaggle.com/rainbowgirl/clustering-categorical-peoples-interests" rel="noopener ugc nofollow" target="_blank"> Kaggle下载了一个数据集。</a></p><pre class="iy iz ja jb fd lq lr ls lt aw lu bi"><span id="0c50" class="lv ku hi lr b fi lw lx l ly lz">df = pd.read_csv("kaggle_Interests_group.csv")</span></pre><p id="08d1" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下一步是数据预处理。数据有很多NaN值，因此我们无法训练模型。所以我们简单地用这些代码把它们替换成0。</p><pre class="iy iz ja jb fd lq lr ls lt aw lu bi"><span id="961b" class="lv ku hi lr b fi lw lx l ly lz">df.fillna(0, inplace = True)</span></pre><p id="623d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">正如我们在数据集中看到的，我们不需要前两列，所以我们将把剩余的217列赋给变量x。</p><pre class="iy iz ja jb fd lq lr ls lt aw lu bi"><span id="a62a" class="lv ku hi lr b fi lw lx l ly lz">x = df.iloc[:,2:]</span></pre><p id="eb0e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在让我们继续构建和训练模型。尽管在数据集中指定了包含4个组，但是我们仍然要实现“<em class="kj">肘方法</em>来确定聚类的数量。这可以通过使用WCSS(数据点距离的平方和)来完成</p><pre class="iy iz ja jb fd lq lr ls lt aw lu bi"><span id="ff34" class="lv ku hi lr b fi lw lx l ly lz">wcss = []<br/>for i in range(1,11):<br/>   model = KMeans(n_clusters = i, init = "k-means++")<br/>   model.fit(x)<br/>   wcss.append(model.inertia_)<br/>plt.figure(figsize=(10,10))<br/>plt.plot(range(1,11), wcss)<br/>plt.xlabel('Number of clusters')<br/>plt.ylabel('WCSS')<br/>plt.show()</span></pre><p id="6644" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这将给出下图作为输出</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mb"><img src="../Images/74b58f651f708c61517a38ee8c94e258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*nOEd788WyEZHxb7lfwrYLw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图表找出适当的集群数量</figcaption></figure><p id="f182" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">很明显，这个数据集没有特别的界限，所以在本文中，我将使用6个集群。作为练习，您可以使用4个集群来实现它。</p><p id="20f7" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下一步是将我们的数据集从多维转换成二维。</p><pre class="iy iz ja jb fd lq lr ls lt aw lu bi"><span id="e0a4" class="lv ku hi lr b fi lw lx l ly lz">pca = PCA(2)<br/>data = pca.fit_transform(x)</span></pre><p id="ebf7" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在初始化PCA()函数时，可以指定任意数量的维数/特征，但是为了简单起见，我使用了2。如果您使用2个以上的组件，您可以使用前两个具有最高方差值的组件来训练和可视化数据集。</p><p id="5e4f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里我们使用了fit_transform()函数来拟合数据集x并对其进行降维。这将返回一个2x2维度的ndarray。</p><p id="b83b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">接下来，我们绘制并检查组件的方差。</p><pre class="iy iz ja jb fd lq lr ls lt aw lu bi"><span id="10e3" class="lv ku hi lr b fi lw lx l ly lz">plt.figure(figsize=(10,10))<br/>var = np.round(pca.explained_variance_ratio_*100, decimals = 1)<br/>lbls = [str(x) for x in range(1,len(var)+1)]<br/>plt.bar(x=range(1,len(var)+1), height = var, tick_label = lbls)<br/>plt.show()</span></pre><p id="7431" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这将返回以下屏幕截图</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mc"><img src="../Images/3ee8e22eb5e506adda84374fe3c886d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*7VrH8VhMigFHht332KEBtg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">不同成分的差异水平</figcaption></figure><p id="042d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在，我们将根据PCA()生成的新特征来训练我们的模型。由于我们只有2个主成分(PC1和PC2)，我们将得到一个有6个聚类的2D图。</p><pre class="iy iz ja jb fd lq lr ls lt aw lu bi"><span id="721a" class="lv ku hi lr b fi lw lx l ly lz">centers = np.array(model2.cluster_centers_)<br/>model = KMeans(n_clusters = 6, init = "k-means++")<br/>label = model.fit_predict(data)<br/>plt.figure(figsize=(10,10))<br/>uniq = np.unique(label)<br/>for i in uniq:<br/>   plt.scatter(data[label == i , 0] , data[label == i , 1] , label = i)</span><span id="07e3" class="lv ku hi lr b fi md lx l ly lz">plt.scatter(centers[:,0], centers[:,1], marker="x", color='k')<br/>#This is done to find the centroid for each clusters.<br/>plt.legend()<br/>plt.show()</span></pre><p id="785b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这是我们得到的散点图。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es me"><img src="../Images/c745b975fef827af45dc30e92a00e759.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*psYmoWT9jr5sXQ3HWRzaoA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">PC1与PC2图，标有质心</figcaption></figure><p id="f861" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">万岁！我们已经成功地用PCA编码并实现了K均值聚类。拍拍自己的背；)</p><p id="3507" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">你可以在这个<a class="ae ma" href="https://github.com/wamz101/K-Means-Cluster" rel="noopener ugc nofollow" target="_blank"> GitHub </a>链接上查看python笔记本和数据集。</p><p id="4ab3" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里是上面文章中使用的数据集的<a class="ae ma" href="https://www.kaggle.com/rainbowgirl/clustering-categorical-peoples-interests" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>链接。</p><p id="c212" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">感谢您的阅读！</p></div></div>    
</body>
</html>