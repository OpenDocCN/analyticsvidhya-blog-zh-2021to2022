# 微调 GPT-2

> 原文：<https://medium.com/analytics-vidhya/fine-tune-gpt-2-225c09400cb6?source=collection_archive---------5----------------------->

在这篇文章中，我将尝试展示 GPT-2 的简单用法和训练。我假设你对 GPT-2 有基本的了解。GPT 是一个自回归的语言模型。它可以用它巨大的预训练模型为我们生成文本。我想微调 GPT-2，以便它为我的任务生成更好的文本。为此，我从维基百科下载了关于日本的页面，并创建了一个包含 4 万个句子的文件。我希望经过一些训练后，model 能为我生成更好的关于日本的句子。github 上的代码。([链接](https://github.com/mcelikkaya/medium_articles/blob/main/gtp2_training.ipynb))

由于训练需要大量的资源，没有 GPU 很难训练。建议试试 Colab。但是当我在尝试的时候，我用 GPU 太多了，Colab 拒绝给我新的 GPU 2 天。所以我放弃了修改代码。尝试时要小心。

GPT-2 是一个语言模型，意味着我们将创造新的文本。因此，对于互联网上的样本，你可以看到给语言模型的莎士比亚文本样本，它生成类似莎士比亚的文本。所以对于一个基本的测试，我将为一个特定的任务训练我的网络。我会下载维基百科中与日本相关的页面，并尝试生成关于日本的有意义的句子。

对于语言模型，我们检查输出句子的概率。它包括语义，语法…在这个简单的任务中，我将检查生成的句子是否变得更有意义。如果你想尝试不同的设置，改变代码中的**【all _ sentences】**，你可以尝试任何你想做的事情。您还必须为不同的序列更改“eval_keywords”。

**如何生成文本？**

GPT-2 记号赋予器为我们编码文本，但是根据参数我们得到不同的结果。在下面的代码中，你可以看到一个非常简单的循环。我们用 tokenizer 对文本进行编码(第 2 行)。我们给模型的输入张量加上一些参数(第 4 行)。和模型生成文本，我们需要将生成的张量转换回单词。(第 16 行)使用 truncate 参数提前停止。GPT-2 为我们生成 1024 个令牌序列。GPT-2 不会停止生成，因此在生成函数中使用 truncate 参数，以便 GPT-2 在生成结束令牌时停止。

**文本生成的简单循环**

![](img/9a85b1e3cd511c26e3ab2f1b4606cde7.png)

**样本代码输出**

**培训是如何进行的？**

语言模型是一个自回归模型。意思是我们给 x1，它生成 x2，然后我们给[x1，x2]，它生成 x3。然后我们给[x1，x2，x3]…这样我们就生成了新的文本。训练以相反的顺序进行。Huggingface 事实上使一切变得如此简单，我们不必像在其他神经网络问题中那样给出输入、目标。我们只输入句子，huggingface 生成输入，这是训练所必需的目标。

通常在训练集中，我们准备 x(特征)和 y(目标，目的)变量，GPT-2 hugginface 自动为我们做这项工作。所以我们只给 x，因为 target 也是 x。(我们是从 x 生成 x，就像我上面解释的那样。)所以数据集如果只是句子的编码，如下。没有目标变量。如果您检查代码片段“process_one_batch ”,我会将该值直接提供给 hugginface。正如你在下面第 15 行看到的，我通常只在一个数据集(x，y)中对句子(输入)进行编码，这里只有 x。

所以训练的主要循环如下。在下面的代码(第 10 行)你可以看到 1 个训练时期。我们从数据加载器获得一批数据。我们将这一批交给模型，模型返回输出。事实上，不需要更多的解释，正如你所看到的，这是神经网络训练的正常循环。我正在给出与“日本”相关的句子，网络学习生成关于**“日本】、**的句子，就这么简单。

经过一些训练后，我可以看到如下输出，你可以看到它正在生成更好的句子。

![](img/471b87372fbafdb0bb58ee91a57a84de.png)

培训后的输出

![](img/5d3f84012977089c5a6b9cd2531e6381.png)![](img/bf0edd56c6208fda5f86edf3785d5799.png)

如果我进行更多的训练，我会有更好的结果，但正如我之前所说，Colab 拒绝给我几天的 GPU。可能我这一周消耗了所有的资源:(。如果你有样本输入，并尝试足够长的时间，你会看到非常好的结果。这是一个非常简单的总结如何训练 GPT-2 为我们的特定任务。