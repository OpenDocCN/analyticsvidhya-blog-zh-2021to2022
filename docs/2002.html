<html>
<head>
<title>Activation Functions in deep learning.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的激活函数。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-in-deep-learning-d5d7450fbd0d?source=collection_archive---------4-----------------------#2021-03-31">https://medium.com/analytics-vidhya/activation-functions-in-deep-learning-d5d7450fbd0d?source=collection_archive---------4-----------------------#2021-03-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="124a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在人工神经网络中，激活函数帮助我们确定神经网络的输出。他们决定神经元是否应该被激活。它决定了模型的输出、精度和计算效率。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es jj"><img src="../Images/01921040d29f981e0d9cc9fd0ab65a6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*mAZyng7Y1aAKyrG2Iw2DXg.jpeg"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">单神经元结构。来源<a class="ae jv" href="https://wiki.tum.de/display/lfdv/Artificial+Neural+Networks" rel="noopener ugc nofollow" target="_blank"> wiki.tum.de </a></figcaption></figure><p id="b293" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">输入被输入到输入层的神经元中。然后，输入(Xi)乘以它们的权重(Wi)，加上偏差，得到神经元的输出(y=(Xi*Wi)+b)。我们在<strong class="in hi"> Y </strong>上应用我们的激活函数，然后它被转移到下一层。</p><h1 id="2ab3" class="jw jx hh bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">激活函数应该具有的属性？</h1><p id="e11b" class="pw-post-body-paragraph il im hh in b io ku iq ir is kv iu iv iw kw iy iz ja kx jc jd je ky jg jh ji ha bi translated"><strong class="in hi"> <em class="kz">微分或微分:</em></strong><em class="kz">y轴变化w . r . t . x轴变化。它也被称为斜坡。</em>(背道具)</p><p id="9eb0" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> <em class="kz">单调函数:</em> </strong> <em class="kz">要么完全非增，要么完全非减的函数。</em></p><blockquote class="la lb lc"><p id="736f" class="il im kz in b io ip iq ir is it iu iv ld ix iy iz le jb jc jd lf jf jg jh ji ha bi translated">深度神经网络中激活函数的选择对训练动态和任务性能有重要影响。</p></blockquote><h1 id="ea81" class="jw jx hh bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">最流行的激活功能</strong></h1><h2 id="3c6c" class="lg jx hh bd jy lh li lj kc lk ll lm kg iw ln lo kk ja lp lq ko je lr ls ks lt bi translated">1.乙状结肠函数(<strong class="ak">逻辑函数</strong>)</h2><p id="6c7d" class="pw-post-body-paragraph il im hh in b io ku iq ir is kv iu iv iw kw iy iz ja kx jc jd je ky jg jh ji ha bi translated">它是常用的激活功能之一。在20世纪90年代，<strong class="in hi"> Sigmoid </strong>函数被引入到ANN中，以取代<strong class="in hi"> Step </strong>函数。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es lu"><img src="../Images/165e489a77fea9ebaf292de5762ab006.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*hNW2pJx6I20DHCy3ayp2Ug.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">Sigmoid激活函数</figcaption></figure><p id="2d86" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">函数公式及其导数。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lv"><img src="../Images/3f9f4ca60289ed66f809f8712aa1d9f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2c8FYwF0nIp-fltwTpHUbw.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">Sigmoid函数及其导数</figcaption></figure><p id="a4e2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在sigmoid函数中，我们可以看到它的输出在开区间(0，1)中</p><ul class=""><li id="5697" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">输出的范围总是在0到1之间</li><li id="9fc7" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">乙状结肠呈S形，<strong class="in hi"> <em class="kz">单调性</em></strong>&amp;<strong class="in hi"><em class="kz">微分</em> </strong>功能</li><li id="77e6" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">sigmoid函数的导数(f'(x))将位于0和0.25之间</li><li id="793d" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">函数是<strong class="in hi">可微的</strong>。这意味着我们可以在任意两点找到sigmoid曲线的斜率</li><li id="ac4d" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">函数是<strong class="in hi">单调的</strong>，但函数的导数不是</li></ul><p id="32a6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">每当我们试图在反向传播中找到sigmoid的导数时，值的范围在0-0.25之间。每个激活函数都有其优缺点，sigmoid也不例外。</p><p id="aa0a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">优点</strong>:</p><ul class=""><li id="a8ad" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated"><strong class="in hi">输出值在0和1之间限制</strong></li><li id="de81" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">平滑渐变，防止输出值“跳跃”</li></ul><p id="a686" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">缺点</strong>:</p><ul class=""><li id="53c3" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">sigmoid函数的导数遭受“<strong class="in hi"> <em class="kz">”消失梯度。</em> </strong>查看函数图，可以看到当输入变小时或变大时，函数在0或1处饱和，导数非常接近0。因此，它几乎没有梯度传播回网络，所以几乎没有什么留给较低的层。即需要时间来达到收敛点(全局最小值)</li><li id="49ed" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated"><strong class="in hi">计算开销:</strong>该函数执行指数运算，因此需要更多的计算时间</li><li id="e5b9" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated"><strong class="in hi"> <em class="kz">中的Sigmoid函数非零居中</em> </strong>。这使得梯度更新在不同的方向上走得太远。<strong class="in hi"> 0 &lt;输出&lt; 1，这使得优化更加困难</strong></li></ul><blockquote class="la lb lc"><p id="152b" class="il im kz in b io ip iq ir is it iu iv ld ix iy iz le jb jc jd lf jf jg jh ji ha bi translated"><strong class="in hi">用<a class="ae jv" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">基于梯度的学习方法</a>和<a class="ae jv" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">反向传播</a>训练<a class="ae jv" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>时遇到消失梯度问题</strong>。在这种方法中，在训练的每次迭代中，每个神经网络的权重接收与相对于当前权重的误差函数的<a class="ae jv" href="https://en.wikipedia.org/wiki/Partial_derivative" rel="noopener ugc nofollow" target="_blank">偏导数</a>成比例的更新。问题是在某些情况下，梯度会变得非常小，有效地阻止了权重值的改变。在最坏的情况下，这可能会完全停止神经网络的进一步训练。</p></blockquote><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es mo"><img src="../Images/464a5de762b67b92ac90beef159edd8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*S3oLN40uWnGmP63lOFEDUg.gif"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">消失渐变。来源<a class="ae jv" href="https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/" rel="noopener ugc nofollow" target="_blank">点击</a></figcaption></figure><p id="5381" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> 2。Tanh激活功能:</strong></p><p id="820a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">双曲线正切<strong class="in hi"> ( </strong> Tanh)类似于s形，略有不同。基本上你可以说它克服了乙状结肠中存在的问题。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es mp"><img src="../Images/2d2f02412e97b20cdb665d17d191fc05.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*P7R1l7FOcOTNNwllUVu5NA.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">tanh函数</figcaption></figure><p id="79a9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">函数及其导数的绘图:</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lv"><img src="../Images/10c7962d6483ddf547d23ed8daf9fce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h9CBwL26E6Kj_GUfrsnIaQ.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">tanh及其导数的图</figcaption></figure><ul class=""><li id="b6a9" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">该函数也是常见的<strong class="in hi"> S形</strong>曲线</li><li id="ada7" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">不同之处在于<strong class="in hi"> Tanh </strong>的输出是以<strong class="in hi">零点为中心的</strong>，范围从<strong class="in hi"> -1 </strong>到<strong class="in hi"> 1 </strong>(而不是在Sigmoid函数的情况下从0到1)</li><li id="3f0c" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">函数<strong class="in hi">可微</strong>与Sigmoid相同</li><li id="cb82" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">与Sigmoid相同，函数是<strong class="in hi">单调的</strong>，但函数的导数不是</li></ul><p id="0d49" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">优点:</strong></p><ul class=""><li id="554d" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">整个功能是<strong class="in hi">零点</strong>比乙状结肠好</li><li id="ca9e" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">优化很容易</li><li id="8d48" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">双曲正切函数的导数在0到1之间</li></ul><p id="66a9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> Tanh </strong>倾向于使每一层的输出或多或少集中在0附近，这通常有助于加速收敛。</p><p id="ab8c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">因为sigmoid和tanh几乎相似，所以他们也面临相同的问题。</p><p id="dd2c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">缺点</strong>:</p><ul class=""><li id="6de5" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">双曲正切函数的导数遭受"<strong class="in hi"> <em class="kz">消失梯度</em> </strong>"</li><li id="8b20" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated"><strong class="in hi">计算量大</strong></li></ul><p id="cde9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">它用于二进制分类问题中的隐藏层，而sigmoid函数用于输出层。</p><p id="a8ab" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> 3 </strong>。<strong class="in hi"> ReLU( <em class="kz">整流线性单元</em>)激活功能:</strong></p><p id="53d0" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这是深度学习中最受欢迎的激活函数(用于隐藏层)。</p><pre class="jk jl jm jn fd mq mr ms mt aw mu bi"><span id="7154" class="lg jx hh mr b fi mv mw l mx my">f(x) = max(0,x)</span></pre><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es mz"><img src="../Images/ccdd76bd9f8b96ae6b2d7793ddd36c11.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*3Jwh6c8Og8O0u5YiamF60w.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">ReLU函数</figcaption></figure><p id="607f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这可以表示为:</p><p id="679d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">ReLU = max(0，x)</p><p id="4109" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果输入为负，该函数返回0，但对于任何正输入，它返回该值。</p><p id="aef1" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">函数及其导数的绘图:</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es na"><img src="../Images/0e33a6c4527d73786fc9f1dd531b6f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jN6uclYOw9kTJL7a0Lk5Qw.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">ReLU函数及其导数。来源<a class="ae jv" href="https://mmuratarat.github.io/2019-02-10/some-basic-activation-functions" rel="noopener ugc nofollow" target="_blank">这里</a></figcaption></figure><ul class=""><li id="f235" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">它不会同时激活所有的神经元</li><li id="a206" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">ReLU函数在0附近是非线性的，但斜率总是为0(负输入)或1(正输入)</li></ul><p id="fc33" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">优点:</strong></p><ul class=""><li id="25ac" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">计算效率高:该函数计算速度非常快(与Sigmoid和Tanh相比),它不计算指数</li><li id="beaf" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">收敛得非常快</li><li id="fac7" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">如果输入为正，则解决梯度饱和问题</li></ul><p id="e72d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">缺点:</strong></p><ul class=""><li id="6909" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">ReLU功能<strong class="in hi">不是以零为中心</strong></li><li id="89bd" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">主要的问题是它正遭受死亡的折磨</li></ul><blockquote class="la lb lc"><p id="32bb" class="il im kz in b io ip iq ir is it iu iv ld ix iy iz le jb jc jd lf jf jg jh ji ha bi translated"><strong class="in hi"> <em class="hh">垂死挣扎</em> </strong></p><p id="f875" class="il im kz in b io ip iq ir is it iu iv ld ix iy iz le jb jc jd lf jf jg jh ji ha bi translated"><strong class="in hi"> " </strong>每当我们在ReLU中得到输入为负时，输出将变为0。在反向传播网络中不学习任何东西(因为你不能反向传播到它里面),因为它只是为负输入保持输出0，梯度下降不再影响它。换句话说，如果导数为0，则整个激活变为零，因此该神经元对网络没有贡献。<strong class="in hi"/></p></blockquote><p id="19bc" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> 4。</strong> <strong class="in hi">漏热路:</strong></p><p id="c91a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">为了克服ReLU中的问题，引进了<strong class="in hi">泄漏ReLU </strong>。它拥有ReLU的所有属性和优点，加上它永远不会有<strong class="in hi">将死ReLU </strong>的问题。</p><blockquote class="la lb lc"><p id="330c" class="il im kz in b io ip iq ir is it iu iv ld ix iy iz le jb jc jd lf jf jg jh ji ha bi translated">ReLU函数的改进版本，引入了“恒定斜率”</p></blockquote><p id="5b47" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">泄漏ReLU定义为:</p><pre class="jk jl jm jn fd mq mr ms mt aw mu bi"><span id="5917" class="lg jx hh mr b fi mv mw l mx my">f(x) = max(<em class="kz">α</em>x, x) ,where <em class="kz">α is small value usually 0.01</em></span></pre><p id="cbd9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们给一些小的正α值，这样整个激活不会变成零。超参数<code class="du nb nc nd mr b"><em class="kz">α</em></code> <em class="kz"> </em>定义了函数泄漏的多少。它是x &lt; 0的函数斜率，通常设置为<code class="du nb nc nd mr b">0.01</code>。小斜率保证了漏的ReLU永远不死。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es ne"><img src="../Images/b968a1bd081fe03f67c41f1eff3bce73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PWi8aWh276AnM-92rtZ8Og.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">漏ReLU及其衍生物。来源<a class="ae jv" href="https://learnopencv.com/wp-content/uploads/2017/10/leaky-relu-activation.png" rel="noopener ugc nofollow" target="_blank">点击</a></figcaption></figure><p id="d408" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">优点:</strong></p><ul class=""><li id="d246" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">它解决了<strong class="in hi">死亡神经元/死亡神经元</strong>的问题</li><li id="8f7e" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">引入斜率，小α值确保神经元永不死亡，即帮助神经元“存活”</li><li id="8c5f" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">在反向传播期间，它允许负值</li></ul><p id="3a4e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">缺点:</strong></p><ul class=""><li id="b81c" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">倾向于消失梯度</li><li id="97e5" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">计算密集型</li></ul><p id="0f7b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> 5。</strong> <strong class="in hi">指数线性单位(ELU): </strong></p><p id="307f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">ELU还提议解决ReLU的问题。它是<strong class="in hi"> ReLU </strong>的变体，输出更好。ELU的表现优于所有的ReLU变体。</p><pre class="jk jl jm jn fd mq mr ms mt aw mu bi"><span id="9e4d" class="lg jx hh mr b fi mv mw l mx my">f(x)= |x             x&gt;0  |<br/>      <em class="kz">|α</em>(e^x -1)     x&lt;=0 |</span></pre><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es nf"><img src="../Images/1488fd881e3cf58158ea9e9b00d90fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*NSOrdn0xZ8XJxyjzzOuWyQ.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">ELU函数。来源<a class="ae jv" href="https://upload.wikimedia.org/math/8/0/a/80af52c2fada6c7d7f6971e8fce61807.png" rel="noopener ugc nofollow" target="_blank">点击</a></figcaption></figure><p id="ab8d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">ELU倾向于更快地将成本收敛到零，并产生更准确的结果。ELU有一个额外的阿尔法(<em class="kz"> α </em>)常数，它应该是正数。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es ng"><img src="../Images/fec4587e8124e00a1a5eaef4536b9040.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MCiTTWVss3ga72F9dlgnzA.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">ELU函数及其导数。来源<a class="ae jv" href="https://www.mdpi.com/sensors/sensors-20-01068/article_deploy/html/images/sensors-20-01068-g0A3.png" rel="noopener ugc nofollow" target="_blank">点击</a></figcaption></figure><p id="be94" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> ELU </strong>修改了函数负部分的斜率。对于负值，<strong class="in hi"> ELU </strong>使用的不是直线，而是对数曲线。虽然训练收敛速度较快，但由于使用了指数函数，计算速度较慢。</p><p id="43b4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">优点:</strong></p><ul class=""><li id="b957" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">没有死ReLU问题</li><li id="a287" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">解决<strong class="in hi">濒死神经元</strong>的问题</li><li id="b10e" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated"><strong class="in hi">零点居中</strong>输出</li></ul><p id="767c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">缺点:</strong></p><ul class=""><li id="bee8" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">计算密集型</li><li id="bfbb" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">指数函数导致收敛缓慢</li></ul><p id="852a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> 6。参数ReLU(PReLU) </strong>:</p><p id="9130" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">漏ReLU的概念可以进一步扩展。我们可以用一个超参数来代替x乘以一个常数项，这个超参数似乎比泄漏ReLU更有效。这种对泄漏ReLU的扩展被称为参数ReLU。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es nh"><img src="../Images/3ac15c3ebc3b6a0e9d97b797dd4c06aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*oOgiLrRbZi8LL6CBNX7YQg.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">PReLU函数</figcaption></figure><p id="b65b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这里<code class="du nb nc nd mr b"><em class="kz">α</em></code> <em class="kz"> </em>被授权在训练时学习(它不再是一个超参数，而是像其他任何参数一样成为一个可以通过反向传播修改的参数)。这在大型图像数据集上优于ReLU，但在较小的数据集上优于训练集。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es ni"><img src="../Images/960a83fa0784ac8c43da5e63dbc9f353.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*5KJtMBRtBlLVTmmXQzmmCw.jpeg"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">ReLU对PReLU。来源<a class="ae jv" href="https://www.i-programmer.info/images/stories/News/2015/Feb/B/Prelu.jpg" rel="noopener ugc nofollow" target="_blank">点击</a></figcaption></figure><p id="72a6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在负区，PReLU斜率小，也可以避免死ReLU的问题。斜率虽小，但不趋向于0，这是优点。</p><p id="6d84" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们看了PReLU的公式。参数α相对较小，通常是0到1之间的一个数。yᵢ是第I个通道上的任何输入，αᵢ是负斜率，它是一个可学习的参数。如果α=0，则PReLU变为ReLU。当正部分是线性的时，函数的负部分在训练阶段自适应地学习。</p><ul class=""><li id="b634" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">如果<strong class="in hi"> αᵢ=0 </strong>，f变成<strong class="in hi"> ReLU </strong></li><li id="abd2" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">如果<strong class="in hi"> αᵢ &gt; 0 </strong>，f变成<strong class="in hi">泄漏ReLU </strong></li><li id="64ed" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">如果αᵢ是一个可学习的参数，f变成PReLU</li></ul><p id="3a8e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">优点:</strong></p><ul class=""><li id="d0ec" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">避免<strong class="in hi">神经元死亡的问题</strong></li><li id="6dc9" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">更好地处理<strong class="in hi">大型数据集</strong></li><li id="b73c" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">增加了参数<strong class="in hi"> α </strong>(控制负斜率)，可在反向传播时修改</li></ul><p id="66b2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">缺点:</strong></p><ul class=""><li id="9df5" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">虽然下限参数<strong class="in hi"> α </strong>会引起变化，但是单侧饱和并不会导致更好的饱和</li></ul><p id="e382" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">7。Softmax: </p><p id="d64e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Softmax函数计算事件在“n”个不同事件中的概率分布。换句话说，这个函数将计算每个目标类在所有可能的目标类中的概率(这有助于确定目标类)。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es nj"><img src="../Images/460a2f0838c25928f05c19047386d445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*czZ6bk-SXQWcOB5MJ_y8nw.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">Softmax函数。来源<a class="ae jv" href="https://dataaspirant.com/how-logistic-regression-model-works/" rel="noopener ugc nofollow" target="_blank">点击</a></figcaption></figure><p id="137d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Softmax保证较小的值有较小的概率，不会被直接丢弃。是一个“软”的“max”。它返回属于每个单独类的数据点的概率。请注意，所有值的总和是1。</p><blockquote class="la lb lc"><p id="ae8d" class="il im kz in b io ip iq ir is it iu iv ld ix iy iz le jb jc jd lf jf jg jh ji ha bi translated">Softmax可以描述为多个sigmoid函数的组合。</p></blockquote><p id="4208" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Softmax将输入值归一化为一个向量值，该向量值遵循总和等于1的概率分布。输出值在范围[0，1]之间，这很好，因为我们能够避免二进制分类，并在我们的神经网络模型中容纳尽可能多的类或维度。</p><p id="64f7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">对于多类问题，输出层将具有与目标类一样多的神经元。通常，它用于神经网络的输出层，将输入分类为多个类别。</p><p id="c322" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">例如，假设您有4个类[A、B、C、D]。在输出层中将有4个神经元。假设你从神经元得到的输出为[2.5，5.7，1.6，4.3]。应用softmax函数后，您将得到[0.26，0.14，0.41，0.19]。这些表示数据点属于每个类别的概率。通过查看概率值，我们可以说输入属于c类。</p><p id="662e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> 8。唰功能:</strong></p><p id="4454" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Swish功能被称为一种自我门控激活功能，最近由谷歌的研究人员发布。数学上它被表示为</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es nk"><img src="../Images/c9c343faa9c08f48b852cd2103efdf3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*9YLH21Y108qK8kWlokQrvg.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">Swish函数</figcaption></figure><p id="09f8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">根据<a class="ae jv" href="https://arxiv.org/abs/1710.05941v1" rel="noopener ugc nofollow" target="_blank">的论文</a>，SWISH激活功能比ReLU表现更好。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es nl"><img src="../Images/7dd400d05c24cdc23dc8206f4c24f6b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*LfA3xK2Ueo1kHFKSVgC92A.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">嗖嗖激活功能。来源<a class="ae jv" href="https://learnopencv.com/wp-content/uploads/2017/10/swish.png" rel="noopener ugc nofollow" target="_blank">点击</a></figcaption></figure><p id="a6a7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">从图中，我们可以观察到，在x轴的负区域中，尾部的形状不同于ReLU激活函数，因此，即使输入值增加，Swish激活函数的输出也可能减少。大多数激活函数都是单调的，即它们的值不会随着输入的增加而减少。Swish在零点具有单边有界性，它是光滑的，非单调的。</p><p id="1935" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">公式是:</p><pre class="jk jl jm jn fd mq mr ms mt aw mu bi"><span id="bc3e" class="lg jx hh mr b fi mv mw l mx my"><strong class="mr hi">f(x) = x*sigmoid(x)</strong></span></pre><p id="9f6c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">从研究论文来看，他们的实验表明<strong class="in hi"> Swish在许多具有挑战性的数据集上往往比ReLU更好地工作。例如，简单地用Swish单元替换ReLUs，就可以将Mobile NASNetA的<a class="ae jv" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>上的top-1分类精度提高0.9%，将<a class="ae jv" href="https://github.com/Trangle/mxnet-inception-v4/blob/master/inception-resnet-v2.pdf" rel="noopener ugc nofollow" target="_blank"> Inception-ResNet-v2的分类精度提高0.6%。<strong class="in hi">Swish的简单性及其与ReLU的相似性，使得从业者很容易在任何神经网络中用Swish单元代替ReLU。</strong></a></strong></p><ul class=""><li id="ed26" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">Swish激活函数f(x)= x*sigmoid(x)</li><li id="69f9" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated"><strong class="in hi"> Swish的曲线是平滑的:</strong>这使得它对初始化权重和学习速率不太敏感。它在泛化和优化方面起着重要的作用</li><li id="f890" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated"><strong class="in hi">持续匹配或优于ReLU的非单调函数</strong></li><li id="49b8" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">它是<strong class="in hi">无界的上界(</strong>，这使得它在值接近0的梯度附近是有用的。该特征避免了<em class="kz">饱和，因为训练在接近0梯度值</em> <strong class="in hi">)时变得缓慢，并且在下面有界(</strong>有助于强正则化，并且较大的负输入将被解决<strong class="in hi"> ) </strong></li><li id="b9d5" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">正是非单调属性实际上造成了这种差异</li><li id="9bee" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">对于自门控，它只需要一个标量输入，而在多门控情况下，它需要多个双标量输入。它受到了<strong class="in hi">在LSTM(hoch Reiter&amp;schmid Huber，1997)和</strong> <a class="ae jv" href="http://people.idsia.ch/~rupesh/very_deep_learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="in hi">高速公路网(Srivastava等人，2015) </strong> </a> <strong class="in hi">中使用Sigmoid函数的启发，其中</strong> ' <em class="kz">自门控</em>'意味着门实际上是激活本身的'<em class="kz"> sigmoid </em>'</li><li id="0a99" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">无界有助于防止梯度在慢速训练时逐渐接近0</li></ul><p id="6949" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> 9。软加:</strong></p><p id="48d2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">和ReLU函数类似，但相对更流畅。Softplus的功能:</p><pre class="jk jl jm jn fd mq mr ms mt aw mu bi"><span id="510c" class="lg jx hh mr b fi mv mw l mx my"><strong class="mr hi"><em class="kz">f(x) = ln(1+exp x)</em></strong></span></pre><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es nm"><img src="../Images/554de819da7e270e52e72479e8526515.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*HiLIocHUCaEEPLDbREhxZg.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">Softplus激活功能，来源<a class="ae jv" href="https://www.gabormelli.com/RKB/Softplus_Activation_Function" rel="noopener ugc nofollow" target="_blank">点击</a></figcaption></figure><p id="5c63" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">软加函数的导数为</p><pre class="jk jl jm jn fd mq mr ms mt aw mu bi"><span id="e1e1" class="lg jx hh mr b fi mv mw l mx my">f’(x) is <a class="ae jv" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank">logistic function</a> (1/<strong class="mr hi"><em class="kz">(1+exp x)).</em></strong></span></pre><p id="9ff6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">函数的接受范围很广，从(0，+ inf)。</p><p id="23da" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">ReLU和Softplus在很大程度上是相似的，除了在0附近，softplus非常平滑和可微分。与具有log和exp的softplus函数相比，计算ReLU及其导数要容易和有效得多。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es nn"><img src="../Images/97e190b987453b0b5ea4bff493d0420c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*XCMvoY7hgo571v-N5Jf04g.gif"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">激活功能。来源<a class="ae jv" href="https://mlfromscratch.com/activation-functions-explained/" rel="noopener ugc nofollow" target="_blank">点击</a></figcaption></figure><p id="03b3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这些图像中所有激活的摘要。看一看，回忆一下我们到目前为止所学到的所有思想。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es no"><img src="../Images/b6e36f6f213be0ee752084fa29fc2630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*H_tDhUCOzeyIorxsuPJstw.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">激活功能。来源<a class="ae jv" href="https://sefiks.com/2020/02/02/dance-moves-of-deep-learning-activation-functions/" rel="noopener ugc nofollow" target="_blank">点击</a></figcaption></figure><h1 id="55c9" class="jw jx hh bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">如何决定选择哪个或者哪个才是正确的？</h1><p id="cab6" class="pw-post-body-paragraph il im hh in b io ku iq ir is kv iu iv iw kw iy iz ja kx jc jd je ky jg jh ji ha bi translated">激活功能的选择至关重要。所以，选哪个！！没有说明指出可以选择哪种激活功能。每个激活都有其利弊。所有的好与坏都将在尝试和错误的基础上决定。</p><p id="b20b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">但是根据问题的性质，我们也许能够做出更好的选择，使网络更容易、更快地收敛。</p><ul class=""><li id="3622" class="ma mb hh in b io ip is it iw mc ja md je me ji mf mg mh mi bi translated">Sigmoid函数和它们的组合通常在分类问题的情况下工作得更好。但是它遭受<em class="kz">消失梯度</em></li><li id="25cf" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">我们通常在隐藏层使用ReLU</li><li id="0319" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">在死神经元情况下，可以使用泄漏ReLU函数</li><li id="634c" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">如果你有一个庞大的训练集</li><li id="6246" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">Softmax在输出层用于分类问题(主要是多类)</li><li id="b5df" class="ma mb hh in b io mj is mk iw ml ja mm je mn ji mf mg mh mi bi translated">收敛和计算是折衷</li></ul><h1 id="ec96" class="jw jx hh bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">结论:</h1><p id="84d5" class="pw-post-body-paragraph il im hh in b io ku iq ir is kv iu iv iw kw iy iz ja kx jc jd je ky jg jh ji ha bi translated">我希望这篇文章能帮助你学到一些新的东西。</p><p id="915a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">感谢阅读。你觉得这篇文章有用吗，给个掌声。</p><p id="d346" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">总有改进的空间，有什么想法请告诉我。</p><p id="8010" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">快乐学习…..</p></div></div>    
</body>
</html>