<html>
<head>
<title>Detection and Recommendation — Where Catalogue Meets Real World</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">检测和推荐——产品目录与现实世界的交汇点</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/detection-and-recommendation-where-catalogue-meets-real-world-87ec6c864e4c?source=collection_archive---------32-----------------------#2021-06-30">https://medium.com/analytics-vidhya/detection-and-recommendation-where-catalogue-meets-real-world-87ec6c864e4c?source=collection_archive---------32-----------------------#2021-06-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="8f7e" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">利用未标注图像构建检测网络</h2></div><p id="74c4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">图像识别在过去10年里达到了一个新的高度，这主要是因为CNN的发展及其在解决真实世界图像识别和定位任务中的应用。由于多年的发展，研究人员和科学家正在努力在以下方面达到接近人类的性能:</p><ol class=""><li id="b61a" class="js jt hh iy b iz ja jc jd jf ju jj jv jn jw jr jx jy jz ka bi translated">通过训练<strong class="iy hi">创新的</strong>和<strong class="iy hi">复杂的</strong>神经网络架构来识别和定位图像</li><li id="47b0" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">提供<strong class="iy hi">实时</strong>解决方案(低推理时间)而不影响准确性</li><li id="f43b" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iy hi">跨不同环境和图像属性的一致性</strong>。</li></ol><p id="0741" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">开发人员已经确定了不同的方法和AI/ML技术，以便解决不同的图像识别场景。在我们的场景中，用例基于<strong class="iy hi">对象检测，然后是图像推荐</strong>，它可以是任何基于计算机视觉的推荐系统的独立组件。下面是这样一个图像推荐系统的处理流程</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kg"><img src="../Images/ae1296b540317059283cf1853b997a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*UH8mJ3i19PmsTZe0wGVgtg.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">基于检测的图像推荐流程</figcaption></figure><p id="5e2b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">具有<strong class="iy hi"> ~180k图像</strong>的图像目录被选择用于推荐，并且总共<strong class="iy hi"> 22个服装类别</strong>已经被确定包括用于训练。以下是产品目录图片的分布</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ks"><img src="../Images/7b2f32603eefc3417dae49e31b510d28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*Snkgac7vXnSJx3u6H-mmEw.png"/></div></figure><h1 id="aac7" class="kt ku hh bd kv kw kx ky kz la lb lc ld in le io lf iq lg ir lh it li iu lj lk bi translated">用于检测的YOLOv5</h1><p id="fafb" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi translated">YOLO执行单阶段检测，并在推理时间内通过实时预测为对象检测提供最先进的解决方案。YOLOv5的更多细节可以在<a class="ae lq" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank">这里</a>找到。如果你想了解更多关于YOLO的进化，你可以阅读这个。</p><h2 id="64ec" class="lr ku hh bd kv ls lt lu kz lv lw lx ld jf ly lz lf jj ma mb lh jn mc md lj me bi translated">利用目录图像进行YOLO检测:</h2><p id="7fcf" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi translated">目录图像是在<strong class="iy hi">受限环境</strong>中捕获的，图像也经过<strong class="iy hi">裁剪</strong>，仅包含服装对象。在这种情况下，在这种图像上训练的任何模型将<strong class="iy hi">遭受真实图像中的可变性</strong>。为了解决这个问题，下面提到的两种技术非常有用:</p><ol class=""><li id="848c" class="js jt hh iy b iz ja jc jd jf ju jj jv jn jw jr jx jy jz ka bi translated">整个目录中的每一幅图像都被视为对象。随后在整个图像周围绘制边界框，保持与图像边缘的最小间隙。</li><li id="f870" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">图像的每一侧都添加了随机数量的填充。通过随机填充每侧图像形状的10% — 50%之间的值，然后根据随机生成的标志为每侧添加2倍的形状，来确定填充值。添加到此处的随机填充代码片段:</li></ol><pre class="kh ki kj kk fd mf mg mh mi aw mj bi"><span id="a1be" class="lr ku hh mg b fi mk ml l mm mn"> img = cv.imread(input_path+img_name)<br/> y,x,_ = img.shape<br/> gap = 5<br/> rm = round(random.random())<br/> padding_top = random.randint(int(0.1*y),int(0.5*y)+rm*2*y)<br/> padding_bottom = random.randint(int(0.1*y),int(0.5*y)+rm*2*y)<br/> padding_right = random.randint(int(0.1*x),int(0.5*x)+rm*2*x)<br/> padding_left = random.randint(int(0.1*x),int(0.5*x)+rm*2*x)<br/> image = cv.copyMakeBorder(img, padding_bottom, padding_top,   padding_left, padding_right, cv.BORDER_CONSTANT)<br/> height,width,_ = image.shape<br/> w,h = x-2*gap,y-2*gap<br/> x,y = padding_left+gap,padding_bottom+gap<br/> x,y = int(x + w/2), int(y+h/2)<br/> x,y,w,h = x/width, y/height, w/width,h/height<br/> cv.imwrite(input_img_path+img_name,image)</span></pre><p id="cc58" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">添加填充后，对象通常以不同的形状放置在的不同部分。随后对填充的图像进行尺寸调整。x、y、w、h坐标也被保存以作为对象检测网络的参数。</p><h2 id="7ad0" class="lr ku hh bd kv ls lt lu kz lv lw lx ld jf ly lz lf jj ma mb lh jn mc md lj me bi translated">准备YOLO格式数据集:</h2><pre class="kh ki kj kk fd mf mg mh mi aw mj bi"><span id="56ce" class="lr ku hh mg b fi mk ml l mm mn">--data<br/>        --images<br/>            -- train<br/>                - image1.jpg<br/>                - image2.jpg<br/>            -- valid<br/>                - image11.jpg<br/>                - image22.jpg<br/>        --labels<br/>            -- train<br/>                - image1.txt<br/>                - image2.txt<br/>            -- valid<br/>                - image11.txt<br/>                - image22.txt<br/>           <br/>Example: for any file which is present in /images/train, filename should be same in /labels/train but the extension will be '.txt'</span><span id="5797" class="lr ku hh mg b fi mo ml l mm mn">What is there in .txt file:</span><span id="73c3" class="lr ku hh mg b fi mo ml l mm mn">YOLO expects a .txt file for each image saved in /images/train or /images/valid which contains the necessary information (l,x,y,w,h)<br/>    l : label of that image<br/>    x,y : Co-ordinate of centre pixel of the object<br/>    w,h : width and height of the object</span><span id="713a" class="lr ku hh mg b fi mo ml l mm mn">Note: All the values (x,y,w,h) are normalized to (0,1)</span><span id="47a0" class="lr ku hh mg b fi mo ml l mm mn">x,w = x/W,w/W &amp;<br/>    y,h = y/H,h/H<br/>    where W&lt;,H are the width &amp; height of the original image respectively</span></pre><p id="a9f4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对YOLO进行了训练，对配置进行了微小的改变，以调整旋转、透视变换等的增强参数。将图像缩放至320×320像素大小，并用YOLOv5中等大小模型训练40个时期。由于硬件限制，没有使用大尺寸模型。</p><h2 id="3fdc" class="lr ku hh bd kv ls lt lu kz lv lw lx ld jf ly lz lf jj ma mb lh jn mc md lj me bi translated">使用目录图像进行检测的挑战:</h2><p id="b54a" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi translated">随机填充有助于模型在一定程度上根据图像的位置和大小来学习相对于真实图像的可变性。然而，学习的检测模型仍然遭受了许多来自于正确识别真实世界图像的包围盒的<strong class="iy hi">。</strong>主要原因是由于在<strong class="iy hi">不同环境</strong>下拍摄的照片，图像在<strong class="iy hi">背景</strong>和像素差异方面缺乏变化。</p><h2 id="ccd6" class="lr ku hh bd kv ls lt lu kz lv lw lx ld jf ly lz lf jj ma mb lh jn mc md lj me bi translated">微调YOLO:</h2><p id="f002" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi translated">我们从Flickr、Reddit和其他流行网站收集了大约5000个样本，并使用<a class="ae lq" href="https://www.makesense.ai/" rel="noopener ugc nofollow" target="_blank"> makesense.ai </a>标记了这些图像。该网站提供了一个选择，下载YOLO耗材格式的注释。手动收集图像的主要想法是在以下方面增加训练图像的可变性</p><ol class=""><li id="00d6" class="js jt hh iy b iz ja jc jd jf ju jj jv jn jw jr jx jy jz ka bi translated">模型期望在一幅图像中出现的<strong class="iy hi">数量变化</strong></li><li id="4589" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">训练图像中<strong class="iy hi">背景</strong>的变化</li><li id="84fc" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">服装对象的<strong class="iy hi">尺寸</strong>、<strong class="iy hi">形状</strong>和<strong class="iy hi">位置</strong>的变化</li><li id="d2d7" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">单个类的不同<strong class="iy hi">类型对象</strong>的变化</li></ol><p id="173b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">更新的YOLOv5模型使用真实世界的图像进行了微调。在执行微调后，还观察到了<strong class="iy hi">图</strong>的巨大跳跃(增加到<strong class="iy hi"> 0.78 </strong>，而之前的场景中仅为0.42)。</p><p id="e56a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">检测网络的几个例子如下所示</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="er es mp"><img src="../Images/cc37f71db30141153ad7a364fc01d7a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nlb6DM7fWV9O7fWSjpm-lw.png"/></div></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="er es mu"><img src="../Images/6ca78b487b8c693ebc0c6beac14269a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cAhuQ0_tSpvefn15GBrPvA.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">置信度为0.4的YOLO检测</figcaption></figure><p id="1e79" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有助于进一步提升性能的一些技巧:</p><ol class=""><li id="ec69" class="js jt hh iy b iz ja jc jd jf ju jj jv jn jw jr jx jy jz ka bi translated">如果您有<strong class="iy hi">低分辨率</strong>图像，请使用<strong class="iy hi">较小比例</strong>的图像进行训练。在推断过程中，如果我们有高分辨率的图像，我们可以使用相对较高的像素大小</li><li id="3579" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">I <strong class="iy hi">增强的推理</strong>会带来大约30–50%的推理时间增加。然而，如果测试时间增加被激活，在识别能力方面可以观察到显著的改进。例如，在我们的应用中，没有增强的检测需要大约80-100毫秒，而具有测试时间增强的检测大约需要110-150毫秒。然而，在后一种情况下，准确性要好得多。</li><li id="1cd0" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">除非必要，否则不要更改超参数，因为YOLOv5网络是通过多年的辛勤工作和考虑各种情况进行微调的。</li></ol><h1 id="dbf8" class="kt ku hh bd kv kw kx ky kz la lb lc ld in le io lf iq lg ir lh it li iu lj lk bi translated">检测到的对象的建议</h1><p id="e8cc" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi translated">到目前为止，已经建立并验证了一个检测网络，它可以检测包围盒并为我们提供预测的服装类别。下一个任务是创建一个推荐网络，它将根据检测到的对象将我们带回目录图像进行推荐。这里的一些假设包括:</p><ol class=""><li id="29c2" class="js jt hh iy b iz ja jc jd jf ju jj jv jn jw jr jx jy jz ka bi translated">推荐应该是<strong class="iy hi">快</strong>。</li><li id="d24e" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">建议应考虑<strong class="iy hi">性别</strong>信息</li><li id="08f0" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">建议应该有<strong class="iy hi">定制</strong>的颜色偏好，季节偏好，尺寸偏好等。</li></ol><p id="cafe" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这个用例中，对象检测是我们的主要关注点，以验证我们是否可以使用裁剪的目录图像作为构建高效人工智能应用程序的起点，该应用程序也可以处理真实世界的图像。为了构建推荐系统，我们没有使用YOLOv5作为特征描述符。相反，我们试验了一个独立的基于CNN的分类模型来建立一个独立的图像推荐组件。在这种情况下，我们使用<strong class="iy hi"> VGG16 </strong>网络来构建两个独立的分类器。这两个模型的功能是:</p><ol class=""><li id="fa4e" class="js jt hh iy b iz ja jc jd jf ju jj jv jn jw jr jx jy jz ka bi translated"><strong class="iy hi">用于性别分类的VGG</strong>:对目录图像进行平行训练，用于性别分类。观察到目录图像的性别分类的准确率为92%</li><li id="1146" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iy hi">用于类别分类的VGG</strong>:目录图像也被训练用于类别分类。训练该模型的目的是将该模型用作特征描述符。我们使用独立模型进行类别分类，以便我们可以根据我们的建模选择或硬件配置替换模型或更改最后的特征嵌入层大小。观察到使用VGG 16的服装类别分类的总体准确度为84%。对于这个用例，我们提取了<strong class="iy hi"> 2048维特征向量</strong>用于测量图像间的距离。</li></ol><p id="4a57" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">此类网络的代码片段可从此处查阅:</p><pre class="kh ki kj kk fd mf mg mh mi aw mj bi"><span id="d6de" class="lr ku hh mg b fi mk ml l mm mn">class BotNet(nn.Module):<br/>    def __init__(self,n_classes=None):<br/>        super(BotNet, self).__init__()<br/>        self.model = models.vgg16(pretrained=True)<br/>        self.n_classes = n_classes<br/>    <br/>        for param in self.model.parameters():<br/>            param.requires_grad = False<br/>            <br/>        self.fc1 = nn.Linear(25088,2048)<br/>        self.fc2 = nn.Linear(2048,256)<br/>        self.fc3 = nn.Linear(256,self.n_classes)<br/>        self.relu = nn.ReLU()<br/>        self.logSoftMax = nn.LogSoftmax(dim=1)<br/>            <br/>    def forward(self, x):<br/>        x = self.model.features(x)<br/>        x = x.view(x.shape[0], -1)<br/>        y = self.fc1(x)<br/>        y = self.relu(y)<br/>        x = self.fc2(y)<br/>        x = self.relu(x)<br/>        x = self.fc3(x)<br/>        out = self.logSoftMax(x)<br/>        return y,out</span><span id="69e9" class="lr ku hh mg b fi mo ml l mm mn">model = BotNet(n_classes=22)<br/># print(model)</span></pre><p id="4110" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一旦类别分类模型被训练，所有的<strong class="iy hi">180，000张目录图像通过类别分类网络</strong>，并且为每张图像保存2048维特征向量。对于每个性别和类别组合，特征向量被保存在一个组中，以便在搜索排名最接近的图像时，我们可以只查找<strong class="iy hi">相关的特征组</strong>。</p><pre class="kh ki kj kk fd mf mg mh mi aw mj bi"><span id="ccea" class="lr ku hh mg b fi mk ml l mm mn">import pickle<br/>import time<br/>start=time.time()<br/>image_size = 224<br/>transform = A.Compose([<br/>    A.Resize(image_size,image_size),<br/>    A.Normalize(mean=[0.485, 0.456, 0.406],<br/>                        std=[0.229, 0.224, 0.225]),<br/>    ToTensorV2()<br/>])</span><span id="026f" class="lr ku hh mg b fi mo ml l mm mn">image_path = '/images/'<br/>#identifiers is a list of strings containing gender_category information</span><span id="97d8" class="lr ku hh mg b fi mo ml l mm mn">#data has image_id,gender,category information and in saved as a pandas dataframe</span><span id="3346" class="lr ku hh mg b fi mo ml l mm mn">for idnt in identifiers:<br/>    df_filt = data[data.identifier==idnt]<br/>    dict_to_save = {}<br/>    all_feats = []<br/>    all_images = []<br/>    for i,row in df_filt.iterrows():</span><span id="6c00" class="lr ku hh mg b fi mo ml l mm mn">img_name = row['image']<br/>        gender = row['gender']<br/>        label = row['final_category']<br/>        try:<br/>            image = cv2.imread(image_path+img_name)<br/>            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<br/>            x,y,_ = image.shape<br/>            if (x&gt;90) &amp; (y&gt;90):<br/>            # Augment an image<br/>                transformed = transform(image=image)<br/>                transformed_image = transformed["image"]<br/>                transformed_image = transformed_image.unsqueeze(0)  # if torch tensor<br/>                transformed_image = transformed_image.cuda()<br/>                feats,_ = model(transformed_image)<br/>                all_feats.append(feats.to(device))<br/>                all_images.append(img_name) <br/>        except Exception as e:<br/>            pass<br/>    dict_to_save['feats'] = all_feats<br/>    dict_to_save['images'] = all_images<br/>    print(idnt,len(all_images))<br/>    torch.save(dict_to_save, working_dir+idnt+'.pt')<br/>end = time.time()<br/>print('Time Taken: ', str(end-start))</span></pre><p id="0940" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然后基于预测的性别和预测的类别进行推荐，随后查找正确的<strong class="iy hi">性别_类别级别特征集</strong>。</p><pre class="kh ki kj kk fd mf mg mh mi aw mj bi"><span id="6c31" class="lr ku hh mg b fi mk ml l mm mn">#define euclidiean distance and find n closest matches<br/>def euclidean(p1,p2):<br/>    p1,p2 = p1.cuda(),p2.cuda()<br/>    dist =torch.dist(p1,p2)<br/>    return dist</span><span id="f076" class="lr ku hh mg b fi mo ml l mm mn">#find index corresponding to the closest ranked images<br/>def find_n_closest(tag,feats,n=5):<br/>    feats = feats.cuda()<br/>    targets = feats_dict[tag]['feats']<br/>    n_dist = [euclidean(feats,t.cuda()).cpu().numpy() for t in targets]<br/>    match_indices = np.argpartition(n_dist,range(n))[1:1+n]<br/>    match_images = np.array(feats_dict[tag]['images'])[match_indices]<br/>    return match_images</span><span id="4868" class="lr ku hh mg b fi mo ml l mm mn">#recommend based on predicted gender and category<br/>def get_recommendation(img,input_data_path=None):  <br/>    match_images=None<br/>    image = cv2.imread(input_data_path+img)<br/>    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<br/>    # Augment an image<br/>    transformed = transform(image=image)<br/>    transformed_image = transformed["image"]<br/>    transformed_image = transformed_image.unsqueeze(0)  # if torch tensor<br/>    transformed_image = transformed_image.cuda()<br/>    <br/>    gender_output = model_gender(transformed_image).cpu()<br/>    _, gender_pred = torch.max(gender_output, 1)<br/>    gender = gender_mapper[gender_pred.cpu().numpy()[0]]<br/>    <br/>    feats,_ = model_category(transformed_image)<br/>    #_, cat_pred = torch.max(category_output, 1)<br/>    #category = category_mapper[cat_pred.cpu().numpy()[0]]<br/>    category = ' '.join(Path(img).stem.split('_')[-1].split(' ')[:-1])<br/>    if category in ['skirt', 'dress']:<br/>        gender = 'women'<br/>    if category in ['tie']:<br/>        gender = 'men'<br/>    refer_tag = gender+'_'+category<br/>    match_images = find_n_closest(tag=refer_tag,feats=feats)<br/>    <br/>    return img,refer_tag,match_images</span></pre><p id="6b4f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这极大地减少了推理过程中的响应时间。然而，这也带来了由于<strong class="iy hi">服装类别</strong>或<strong class="iy hi">性别</strong>的<strong class="iy hi">错误分类</strong>而导致的不正确推荐的成本。错误分类的发生主要是由于模型的有限能力和背景<strong class="iy hi">的可变性</strong>与目录图像相对于真实世界图像。</p><h1 id="d073" class="kt ku hh bd kv kw kx ky kz la lb lc ld in le io lf iq lg ir lh it li iu lj lk bi translated"><strong class="ak">改善范围:</strong></h1><p id="7e2a" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi translated">改进的范围分为两个主要部分。它们如下所述:</p><p id="86ef" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">车型性能提升:</strong></p><ol class=""><li id="d6d6" class="js jt hh iy b iz ja jc jd jf ju jj jv jn jw jr jx jy jz ka bi translated">提高推荐准确度:当前的解决方案存在泛化问题，因为性别和类别分类网络仅在目录图像上进行训练。用真实世界的图像微调这些分类模型可以减轻泛化误差。</li><li id="eb60" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">包括更多类别，如年龄类别、季节等。为了提供更多的定制和用户特定的建议</li><li id="2b40" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">通过更改分类模型架构来提高性别分类的准确性</li><li id="31b7" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">通过在检测网络中包含类别信息来改进性别分类</li><li id="51d9" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">使用YOLOv5作为特征描述符</li></ol><p id="c48f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">应用性能提升:</strong></p><ol class=""><li id="766b" class="js jt hh iy b iz ja jc jd jf ju jj jv jn jw jr jx jy jz ka bi translated">响应时间最小化:据观察，90%的处理时间受I/O限制。当前解决方案需要大约5秒来提供检测或建议输出。响应时间可缩短90%,提供快速、实时的建议。</li><li id="f594" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">推荐的用户偏好(例如:可点击的边界框，特定图像的扩展推荐)</li></ol><h1 id="c3cc" class="kt ku hh bd kv kw kx ky kz la lb lc ld in le io lf iq lg ir lh it li iu lj lk bi translated">结论:</h1><p id="5bf5" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi translated">实验显示了YOLO执行最先进的对象检测的功效。通过适当的架构和代码设计，该应用程序只能用于实时推荐。我们可以将此类应用程序与任何基于聊天机器人或网络摄像头的应用程序相集成。</p><p id="b9fe" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">参考资料:</p><ol class=""><li id="c693" class="js jt hh iy b iz ja jc jd jf ju jj jv jn jw jr jx jy jz ka bi translated">https://dev . to/afrozchakure/all-you-need-to-know-on-yolo-v3-you-only-look-once-e4m</li><li id="65f2" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><a class="ae lq" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank">https://github.com/ultralytics/yolov5</a></li></ol></div></div>    
</body>
</html>