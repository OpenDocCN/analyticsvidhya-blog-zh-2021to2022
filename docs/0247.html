<html>
<head>
<title>Topic Modelling Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主题建模技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/topic-modelling-techniques-37826fbab549?source=collection_archive---------4-----------------------#2021-01-10">https://medium.com/analytics-vidhya/topic-modelling-techniques-37826fbab549?source=collection_archive---------4-----------------------#2021-01-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="8c94" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">NLP中用于主题建模的不同技术的简要概述以及抽象代码示例</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/a4c79b2ff4229ecb71a611c30167ef0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wCiUcZA_VoeB0S7CcgUNzw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="ccc8" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi kj translated">你是否曾经拥有大量来自各种来源的文本，并希望分析人们谈论的广泛主题/话题，并将它们分成特定的群组，那么主题建模就是为你准备的。那么什么是主题建模。主题建模是一个统计过程，通过它您可以从给定的文档集合中识别、提取和分析主题。</p><p id="3d6f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在本文中，我们将通过一些著名的技术来探索主题建模。</p><p id="0873" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">有一个细微的区别，人们可能会感到困惑。和话题分类是不同的还是一样的？首先，主题分类属于有监督的ML算法，主题建模属于无监督的ML算法。期望你已经知道其中的区别，万一你不知道，把它放在一行程序中主题建模不需要任何预先的训练，不像主题分类。</p><p id="8d6f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">因为对文本建模不需要任何训练，所以分析数据很容易。但是，谁也不能保证会得到精确的结果。</p><h1 id="7161" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">那么它是如何工作的:</h1><p id="ca09" class="pw-post-body-paragraph jn jo hi jp b jq lk ij js jt ll im jv jw lm jy jz ka ln kc kd ke lo kg kh ki hb bi translated">主题建模的基本假设是</p><ul class=""><li id="fbc8" class="lp lq hi jp b jq jr jt ju jw lr ka ls ke lt ki lu lv lw lx bi translated">每个文档由主题的<a class="ae ly" href="https://www.statisticshowto.datasciencecentral.com/mixture-distribution/" rel="noopener ugc nofollow" target="_blank">统计混合</a>(即所有主题的统计分布，可以通过“总结”语料库中覆盖的所有主题的所有分布来获得)</li></ul><p id="eb99" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">主题建模技术的作用是找出语料库中的文档中存在哪些主题，以及每个主题的优势是什么。</p><p id="8f91" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">让我们从最著名的LSA开始，跳到我们上面讨论过的几个这样的技术。</p></div><div class="ab cl lz ma gp mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hb hc hd he hf"><h1 id="d646" class="ks kt hi bd ku kv mg kx ky kz mh lb lc io mi ip le ir mj is lg iu mk iv li lj bi translated">潜在狄利克雷分配(LDA):</h1><p id="6548" class="pw-post-body-paragraph jn jo hi jp b jq lk ij js jt ll im jv jw lm jy jz ka ln kc kd ke lo kg kh ki hb bi translated">潜在狄利克雷分配是一个生成统计模型，它允许通过<a class="ae ly" href="https://en.wikipedia.org/wiki/Latent_variable" rel="noopener ugc nofollow" target="_blank">未观察到的</a>组来解释观察结果，这解释了为什么数据的某些部分是相似的。它假设任何文档都是主题和短语的组合。它使用优化的VEM(变分例外最大化)技术来评估整个文本语料库。这可以通过单词排名来克服。然而，这种断言在句子中缺乏语义。我们真的可以更好地了解主题之间的关系。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ml"><img src="../Images/1476ab50ccf6d546ffbfddb919c0fc91.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*xuPvPREvIdAj7hU1goOKMA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae ly" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation</a></figcaption></figure><p id="879b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">考虑到车牌符号。盒子是代表“重复实体”或“重复者”的板块外表面/板代表文档，而内表面/板代表给定文档中重复的单词位置。</p><p id="cd28" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">从出现在特定文档的主题中的狄利克雷分布中，我们抽取随机样本，其中α是表示文档分布的参数。这个话题分布是θ，我们根据这个谓语的分布从中选择一个具体的话题Z。</p><p id="1811" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">接下来，从另一个狄利克雷分布𝛽，像字数分布Dir(α)，我们挑一个随机样本代表题目z的字分布，这个字分布是φ。由此我们选择我们的单词w。</p><p id="03f9" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">从文本中产生每个单词的方法包括以下步骤:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mm"><img src="../Images/e4420692babd5a2968624577bee79ce8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mDGXagVnIGrSjdLvBVWcxQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae ly" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation</a></figcaption></figure><p id="2537" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里的多项式分布是指只有一个尾迹的多项式，也称为<a class="ae ly" href="https://en.wikipedia.org/wiki/Categorical_distribution" rel="noopener ugc nofollow" target="_blank">分类分布</a>。</p><h2 id="cacb" class="mn kt hi bd ku mo mp mq ky mr ms mt lc jw mu mv le ka mw mx lg ke my mz li na bi translated">代码片段:</h2><p id="7cbf" class="pw-post-body-paragraph jn jo hi jp b jq lk ij js jt ll im jv jw lm jy jz ka ln kc kd ke lo kg kh ki hb bi translated">你可以使用<a class="ae ly" href="https://radimrehurek.com/gensim/models/ldamodel.html" rel="noopener ugc nofollow" target="_blank"> genism </a>软件包，它有一个预定义的LDA模型</p><pre class="iy iz ja jb fd nb nc nd ne aw nf bi"><span id="26be" class="mn kt hi nc b fi ng nh l ni nj"><strong class="nc hj">from</strong> <strong class="nc hj">gensim.test.utils</strong> <strong class="nc hj">import</strong> common_texts<br/><strong class="nc hj">from</strong> <strong class="nc hj">gensim.corpora.dictionary</strong> <strong class="nc hj">import</strong> Dictionary<br/><br/><em class="nk"># Create a corpus from a list of texts</em><br/>dictionary = Dictionary(common_texts)<br/>corpus = [common_dictionary.doc2bow(text) <strong class="nc hj">for</strong> text <strong class="nc hj">in</strong> common_texts]<br/><br/><em class="nk"># Train the model on the corpus.</em><br/>lda = LdaModel(corpus, num_topics=10)</span></pre></div><div class="ab cl lz ma gp mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hb hc hd he hf"><h1 id="6ed7" class="ks kt hi bd ku kv mg kx ky kz mh lb lc io mi ip le ir mj is lg iu mk iv li lj bi translated">潜在语义分析(LSA):</h1><p id="d845" class="pw-post-body-paragraph jn jo hi jp b jq lk ij js jt ll im jv jw lm jy jz ka ln kc kd ke lo kg kh ki hb bi translated">LSA是一个发现文本及其相关术语之间关系的过程。它意味着具有相似含义的术语也将一起使用(分布假设)。从语料库中构建术语-文档矩阵(行代表单词，列代表每个特定文档)。该矩阵升级为<a class="ae ly" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> tf-idf </a>矩阵，其中每个单元格确定当前文档中单词的词频/该单词在所有文档中的词频。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nl"><img src="../Images/f6ec1bdf8ea2d52e47aa71d27e5f465e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NPLq30tTbg7CjrZ1.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">Pic鸣谢:Chris Albon，<a class="ae ly" href="https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558" rel="noopener" target="_blank">https://towards data science . com/TF-term-frequency-IDF-inverse-document-frequency-from-scratch-in-python-6c2b 61 b 78558</a></figcaption></figure><p id="0734" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">直观地说，如果单词在特定文档中的出现频率高于所有其他文档的出现频率，则该单词可以唯一地在特定主题识别中加权。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nm"><img src="../Images/e39cb0e41c5f4eb3d8a2087af2258d67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GPGj0m6UiT7wBu54.jpg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">奇异值分解，<a class="ae ly" href="https://read01.com/gRK7gJ2.html#.X_17g9gzZEY" rel="noopener ugc nofollow" target="_blank">https://read01.com/gRK7gJ2.html#.X_17g9gzZEY</a></figcaption></figure><p id="fb70" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">作为锦上添花，我们使用<a class="ae ly" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank">奇异值分解</a> (SVD)来最小化矩阵的维数，同时保持列间的相似性。</p><p id="ae23" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">然后，通过取它们之间角度的余弦(或它们的归一化值之间的点积)来比较文档对。当值接近1时，文档非常相似；值接近0，文档非常不同。</p><h2 id="9f5c" class="mn kt hi bd ku mo mp mq ky mr ms mt lc jw mu mv le ka mw mx lg ke my mz li na bi translated">代码片段:</h2><p id="b35a" class="pw-post-body-paragraph jn jo hi jp b jq lk ij js jt ll im jv jw lm jy jz ka ln kc kd ke lo kg kh ki hb bi translated">你可以使用<a class="ae ly" href="https://radimrehurek.com/gensim/models/ldamodel.html" rel="noopener ugc nofollow" target="_blank"> genism </a>软件包，它有一个预定义的LSA模型，也就是LSI(我指的是索引)</p><pre class="iy iz ja jb fd nb nc nd ne aw nf bi"><span id="b412" class="mn kt hi nc b fi ng nh l ni nj"><strong class="nc hj">from</strong> <strong class="nc hj">gensim.test.utils</strong> <strong class="nc hj">import</strong> common_dictionary, common_corpus<br/><strong class="nc hj">from</strong> <strong class="nc hj">gensim.models</strong> <strong class="nc hj">import</strong> LsiModel<br/><br/>model = LsiModel(common_corpus, id2word=common_dictionary)<br/>vectorized_corpus = model[common_corpus] </span></pre></div><div class="ab cl lz ma gp mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hb hc hd he hf"><h1 id="95ec" class="ks kt hi bd ku kv mg kx ky kz mh lb lc io mi ip le ir mj is lg iu mk iv li lj bi translated">概率潜在语义分析(PLSA):</h1><p id="8210" class="pw-post-body-paragraph jn jo hi jp b jq lk ij js jt ll im jv jw lm jy jz ka ln kc kd ke lo kg kh ki hb bi translated">PLSA是LSA的进步。这是一种用于双模式和共现数据分析的<a class="ae ly" href="https://en.wikipedia.org/wiki/Statistical_technique" rel="noopener ugc nofollow" target="_blank">统计技术</a>。它试图通过用概率模型代替LSA中的奇异值分解来发现潜在主题以填充术语-文档矩阵。</p><p id="7b7d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">考虑到单词和文档之间共现的可能性，PLS将这些单词和文档共现的概率建模为条件独立的<a class="ae ly" href="https://en.wikipedia.org/wiki/Multinomial_distribution" rel="noopener ugc nofollow" target="_blank">多项式分布</a>的混合:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nn"><img src="../Images/cec75b4c6da416dccb69ca4f4c6563d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*zkKjG4BOmufSk-L3rAjPCg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae ly" href="https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/probability _ latent _ semantic _ analysis</a></figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es no"><img src="../Images/396516d6bf283c0936618580a1b08ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/0*htovz2okMAhlCrSk.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae ly" href="https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/probability _ latent _ semantic _ analysis</a></figcaption></figure><p id="f56a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">代表PLSA模型的图版符号。(“不对称”提法)。d是文档向量，z是单词的主题，w是从单词的主题分布中抽取的单词，c是从单词的主题分布中抽取的单词。变量d和W是可测的，但潜变量的主语是c/z，这三个值就是我们模型中的参数。P(D)直接从我们的数据库中解析。可以使用<a class="ae ly" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">期望最大化</a>算法(EM)来训练参数P(Z|D)和P(W|Z)。EM是一种为依赖于潜在变量的模型寻找最可能的参数估计的方法。EM有两个步骤:(I)期望(E)步骤，计算潜在变量的后验概率，(ii)最大化(M)步骤，更新参数。</p><p id="4749" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">😂没问题，我并没有钻研太多的数学，现在请查看数学部分的维基链接😂</p><h2 id="7d90" class="mn kt hi bd ku mo mp mq ky mr ms mt lc jw mu mv le ka mw mx lg ke my mz li na bi translated">代码片段:</h2><p id="c3da" class="pw-post-body-paragraph jn jo hi jp b jq lk ij js jt ll im jv jw lm jy jz ka ln kc kd ke lo kg kh ki hb bi translated">您可以直接使用PyPI <a class="ae ly" href="https://pypi.org/project/plsa/" rel="noopener ugc nofollow" target="_blank"> plsa </a>包中的plsa模型</p><pre class="iy iz ja jb fd nb nc nd ne aw nf bi"><span id="0393" class="mn kt hi nc b fi ng nh l ni nj"><strong class="nc hj">from</strong> <strong class="nc hj">plsa</strong> <strong class="nc hj">import</strong> Corpus, Pipeline, Visualize<br/><strong class="nc hj">from</strong> <strong class="nc hj">plsa.pipeline</strong> <strong class="nc hj">import</strong> DEFAULT_PIPELINE<br/><strong class="nc hj">from</strong> <strong class="nc hj">plsa.algorithms</strong> <strong class="nc hj">import</strong> PLSA</span><span id="bc9d" class="mn kt hi nc b fi np nh l ni nj">corpus = Corpus.from_csv(csv_file, pipeline)<br/>n_topics = 5</span><span id="76d9" class="mn kt hi nc b fi np nh l ni nj">plsa = PLSA(corpus, n_topics, <strong class="nc hj">True</strong>)<br/>result = plsa.fit()</span></pre></div><div class="ab cl lz ma gp mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hb hc hd he hf"><h1 id="0a14" class="ks kt hi bd ku kv mg kx ky kz mh lb lc io mi ip le ir mj is lg iu mk iv li lj bi translated">非负矩阵分解(NMF):</h1><p id="93ce" class="pw-post-body-paragraph jn jo hi jp b jq lk ij js jt ll im jv jw lm jy jz ka ln kc kd ke lo kg kh ki hb bi translated">NMF属于无监督机器学习的范畴。NMF是一个线性代数算法家族，用于定义结果中的潜在结构。NMF的工作原理是将高维数组分解成低维数组。分解成两个矩阵W和H，前提是这三个矩阵都由非负元素组成。这里V指的是术语文档矩阵，通常是TF-IDF标准化。你已经在上面看到了。w表示找到的主题，H表示该主题的系数。换句话说，V按词代表文章(原始文档)，H按主题代表文章，W按词代表主题。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nq"><img src="../Images/f3ded9111bba53a746bf94300949f4aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/0*e3UNIMlf3Smhd1ym.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae ly" href="https://www.researchgate.net/publication/312157184_Crime_Topic_Modeling/figures?lo=1" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/publication/312157184 _ Crime _ Topic _ Modeling/figures？lo=1 </a></figcaption></figure><h2 id="4a91" class="mn kt hi bd ku mo mp mq ky mr ms mt lc jw mu mv le ka mw mx lg ke my mz li na bi translated">代码片段:</h2><p id="8191" class="pw-post-body-paragraph jn jo hi jp b jq lk ij js jt ll im jv jw lm jy jz ka ln kc kd ke lo kg kh ki hb bi translated">参考代码示例链接<a class="ae ly" href="https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/applications/plot _ topics _ extraction _ with _ NMF _ LDA . html</a></p><pre class="iy iz ja jb fd nb nc nd ne aw nf bi"><span id="3e8c" class="mn kt hi nc b fi ng nh l ni nj"><strong class="nc hj">import</strong> <strong class="nc hj">numpy</strong> <strong class="nc hj">as</strong> <strong class="nc hj">np</strong><br/>X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])</span><span id="a1e2" class="mn kt hi nc b fi np nh l ni nj"><strong class="nc hj">from</strong> <strong class="nc hj">sklearn.decomposition</strong> <strong class="nc hj">import</strong> NMF</span><span id="2fc0" class="mn kt hi nc b fi np nh l ni nj">model = NMF(n_components=2, init='random', random_state=0)<br/>W = model.fit_transform(X)<br/>H = model.components_</span></pre><h1 id="96e6" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">lda2vec:</h1><p id="f152" class="pw-post-body-paragraph jn jo hi jp b jq lk ij js jt ll im jv jw lm jy jz ka ln kc kd ke lo kg kh ki hb bi translated">这个模型可以被认为是word2vec模型的扩展，包含了我们上面讨论的LDA算法[凭直觉，你已经知道什么是word2vec。(注:如果你不知道我在说什么，参考这个<a class="ae ly" href="https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa" rel="noopener" target="_blank">word2veclick</a>)我不会过多讨论word 2 vec部分。Word2vec捕获了清晰的词与词之间的关系，但是产生的向量基本上是不可解释的，并且不代表记录。另一方面，LDA是非常容易理解的，但是不模拟本地单词关系，比如word2vec。Lda2vec是作为一个模型构建的，它创建单词和文档主题，使它们可解释，创建主题，并使它们成为客户机、时间和文档的监督主题。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nr"><img src="../Images/74dc60701eace4b20486aa2816040afe.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*xDienmy51GUMet02Y-arkg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">跳格模型，作者图片，</figcaption></figure><p id="fe77" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">LDA2Vec是skip-gram word2vec算法的修改版本。该模型被学习以基于原始跳格过程中的中枢词来预测背景词。在lda2vec中，为了获得背景向量，插入了中枢单词向量和文档向量。为了预测上下文表达式，然后使用这个上下文向量。</p><p id="08c2" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">像线性判别分析模型一样，将文档向量【参考<a class="ae ly" rel="noopener" href="/@gidishperber?source=post_page-----db3e8c0cce5e--------------------------------"> Gidi Shperber </a>的<a class="ae ly" rel="noopener" href="/@amarbudhiraja/understanding-document-embeddings-of-doc2vec-bfe7237a26da"> doc2vec </a>分解为文档权重向量和主题矩阵。文档权重向量指示讨论了多少不同的主题，而主题矩阵以分类的方式对主题进行分类。因此，通过组合文档出现的不同上下文来构建上下文向量。</p><p id="b7c3" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">简言之</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ns"><img src="../Images/8017821bebef9c920244a87f3b3f3fc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*nE-8nddnOWhVNDK4.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">【https://github.com/cemoody/lda2vec T4】</figcaption></figure><p id="ed36" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">Word2vec试图对词与词之间的关系进行建模。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ns"><img src="../Images/9f30683262450d12074570e53014c36f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*QGVtANQpN1Mvkzad.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae ly" href="https://github.com/cemoody/lda2vec" rel="noopener ugc nofollow" target="_blank">https://github.com/cemoody/lda2vec</a></figcaption></figure><p id="a070" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">LDA对文档到单词的关系进行建模。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ns"><img src="../Images/35c5bb26d68c13e423d8e87fe136bb23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*lhmT_hxIxlDlRzUv.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae ly" href="https://github.com/cemoody/lda2vec" rel="noopener ugc nofollow" target="_blank">https://github.com/cemoody/lda2vec</a></figcaption></figure><p id="13a7" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">LDA在每个文档上创建主题。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ns"><img src="../Images/b9d38c25462aacbca05b6a199dba67c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*BYM-4ow0tmibO6C3.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae ly" href="https://github.com/cemoody/lda2vec" rel="noopener ugc nofollow" target="_blank">https://github.com/cemoody/lda2vec</a></figcaption></figure><p id="7c7f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">lda2vec不仅创建文档主题，还创建区域主题。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ns"><img src="../Images/2c891d30c1a60175890bfef231242a59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*qbAI9fqXfjvIlIAB.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae ly" href="https://github.com/cemoody/lda2vec" rel="noopener ugc nofollow" target="_blank">https://github.com/cemoody/lda2vec</a></figcaption></figure><p id="39fa" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">lda2vec还在客户机上创建主题</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ns"><img src="../Images/e1fabbac1ed67761ddd4d5ac3df943fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*42atTjQ1Pp39l17z.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae ly" href="https://github.com/cemoody/lda2vec" rel="noopener ugc nofollow" target="_blank">https://github.com/cemoody/lda2vec</a></figcaption></figure><p id="130d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在lda2vec中，主题可以被“监督”,并被迫预测另一个目标。</p><p id="352a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">GitHub代码和解释的确认和引用<a class="ae ly" href="https://github.com/cemoody/lda2vec" rel="noopener ugc nofollow" target="_blank">https://github.com/cemoody/lda2vec</a>。你可能会像我一样喜欢它。</p><p id="a306" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">参考lda2vec官方文件<a class="ae ly" href="https://lda2vec.readthedocs.io/en/latest/?badge=latest" rel="noopener ugc nofollow" target="_blank">https://lda2vec.readthedocs.io/en/latest/?badge=latest</a></p><pre class="iy iz ja jb fd nb nc nd ne aw nf bi"><span id="476d" class="mn kt hi nc b fi ng nh l ni nj">model = LDA2Vec(n_words, max_length, n_hidden, counts)<br/>model.add_component(n_docs, n_topics, name='document id')<br/>model.fit(clean, components=[doc_ids])<br/>topics = model.prepare_topics('document_id', vocab)<br/>prepared = pyLDAvis.prepare(topics)<br/>pyLDAvis.display(prepared)</span></pre><h1 id="5a27" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">指标:</h1><p id="852a" class="pw-post-body-paragraph jn jo hi jp b jq lk ij js jt ll im jv jw lm jy jz ka ln kc kd ke lo kg kh ki hb bi translated">既然我们已经学习了主题建模的各种技术。我们也需要一些依据来衡量他们的表现。虽然我们很少有方法来衡量主题建模的性能，如目测法，内在评价指标，人工判断，外在评价指标。我想集中讨论两种方法，特别是抽象层次上的困惑和主题连贯性，不要深入探讨。</p><h2 id="93b1" class="mn kt hi bd ku mo mp mq ky mr ms mt lc jw mu mv le ka mw mx lg ke my mz li na bi translated">困惑:</h2><p id="e264" class="pw-post-body-paragraph jn jo hi jp b jq lk ij js jt ll im jv jw lm jy jz ka ln kc kd ke lo kg kh ki hb bi translated">困惑是用来衡量一个词的意思的计算方法之一。它捕捉了这个模型对它以前没有见过的新数据有多惊讶，并被计算为拒不接受的测试集的可能性。</p><p id="c567" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">困惑度量用于衡量新的、看不见的数据如何影响先前建立的模型的概率。举个例子，这个模型与数据的吻合程度如何？</p><p id="6d70" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">然而，最近的研究发现，人类的判断并不总是与预测的可能性密切相关。一个例子是优化并不总是产生人类可以解释的主题。</p><p id="818f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">有了话题连贯，我们就有可能模拟人类的决策，这就克服了困惑的局限性，从而产生了话题连贯理论。</p><h2 id="d4f9" class="mn kt hi bd ku mo mp mq ky mr ms mt lc jw mu mv le ka mw mx lg ke my mz li na bi translated">话题连贯性:</h2><p id="796b" class="pw-post-body-paragraph jn jo hi jp b jq lk ij js jt ll im jv jw lm jy jz ka ln kc kd ke lo kg kh ki hb bi translated">主题连贯的框架包含了从主题连贯和文章整体连贯中得出的各种考虑。它测试样本段落中的术语在上下文中的一致程度。度量允许我们区分语义上可解释的主题和统计推断的对象。它可以被描述为一般的利用或者成对的单词相似性分数的标准。一个成功的模型能够产生连贯的材料。请阅读下面的单词，以获得对这些主题的更详细的描述。</p><h1 id="d6d6" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">致谢和参考:</h1><div class="nt nu ez fb nv nw"><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hj fi z dy ob ea eb oc ed ef hh bi translated">期望值最大化算法</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">在统计学中，期望最大化(EM)算法是一种寻找(局部)最大似然或…</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">en.wikipedia.org</p></div></div><div class="of l"><div class="og l oh oi oj of ok jh nw"/></div></div></a></div><div class="nt nu ez fb nv nw"><a href="https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hj fi z dy ob ea eb oc ed ef hh bi translated">概率潜在语义分析</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">概率潜在语义分析(PLSA)，也称为概率潜在语义索引(PLSI，尤其是…</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">en.wikipedia.org</p></div></div><div class="of l"><div class="ol l oh oi oj of ok jh nw"/></div></div></a></div><p id="5010" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><a class="ae ly" href="https://pypi.org/project/plsa/" rel="noopener ugc nofollow" target="_blank">https://pypi.org/project/plsa/</a>https://arxiv.org/abs/1301.6705T2</p><div class="nt nu ez fb nv nw"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hj fi z dy ob ea eb oc ed ef hh bi translated">sk learn . decomposition . NMF-sci kit-learn 0 . 24 . 0文档</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">非负矩阵分解(NMF)。找出两个非负矩阵(W，H ),它们的乘积逼近非负矩阵的乘积</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">scikit-learn.org</p></div></div><div class="of l"><div class="om l oh oi oj of ok jh nw"/></div></div></a></div><div class="nt nu ez fb nv nw"><a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization#:~:text=Non%2Dnegative%20matrix%20factorization%20%28NMF,matrices%20have%20no%20negative%20elements." rel="noopener  ugc nofollow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hj fi z dy ob ea eb oc ed ef hh bi translated">非负矩阵分解</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">非负矩阵分解(NMF或NNMF)，也非负矩阵近似是一组算法在…</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">en.wikipedia.org</p></div></div><div class="of l"><div class="on l oh oi oj of ok jh nw"/></div></div></a></div><div class="nt nu ez fb nv nw"><a href="https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0" rel="noopener follow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hj fi z dy ob ea eb oc ed ef hh bi translated">评估主题模型:潜在狄利克雷分配(LDA)</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">构建可解释主题模型的分步指南</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">towardsdatascience.com</p></div></div><div class="of l"><div class="oo l oh oi oj of ok jh nw"/></div></div></a></div><div class="nt nu ez fb nv nw"><a href="http://www.aclweb.org/anthology/N10-1012" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hj fi z dy ob ea eb oc ed ef hh bi translated">话题连贯性的自动评估</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">戴维·纽曼，刘济汉，卡尔·格里泽，蒂莫西·鲍德温。人类语言技术:2010年年度会议…</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">www.aclweb.org</p></div></div><div class="of l"><div class="op l oh oi oj of ok jh nw"/></div></div></a></div><div class="nt nu ez fb nv nw"><a href="https://github.com/cemoody/lda2vec" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hj fi z dy ob ea eb oc ed ef hh bi translated">cemoody/lda2vec</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">lda2vec模型试图将word2vec和lda的最佳部分混合到一个框架中。word2vec捕捉强大的…</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">github.com</p></div></div><div class="of l"><div class="oq l oh oi oj of ok jh nw"/></div></div></a></div><div class="nt nu ez fb nv nw"><a href="https://towardsdatascience.com/lda2vec-word-embeddings-in-topic-models-4ee3fc4b2843" rel="noopener follow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hj fi z dy ob ea eb oc ed ef hh bi translated">LDA2vec:主题模型中的词嵌入</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">了解有关LDA2vec的更多信息，LDA 2 vec是一种与狄利克雷分布潜在向量联合学习密集词向量的模型…</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">towardsdatascience.com</p></div></div><div class="of l"><div class="or l oh oi oj of ok jh nw"/></div></div></a></div><div class="nt nu ez fb nv nw"><a href="https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558" rel="noopener follow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hj fi z dy ob ea eb oc ed ef hh bi translated">python中从头开始的TF(词频)-IDF(逆文档频)。</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">从头开始创建TF-IDF模型</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">towardsdatascience.com</p></div></div><div class="of l"><div class="os l oh oi oj of ok jh nw"/></div></div></a></div></div></div>    
</body>
</html>