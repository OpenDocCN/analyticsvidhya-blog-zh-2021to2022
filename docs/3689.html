<html>
<head>
<title>Unpacking DenseNet to understand and then creating using TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解包DenseNet以了解，然后使用TensorFlow创建</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/unpacking-densenet-to-understand-and-then-creating-using-tensorflow-670d88aace5c?source=collection_archive---------5-----------------------#2021-07-18">https://medium.com/analytics-vidhya/unpacking-densenet-to-understand-and-then-creating-using-tensorflow-670d88aace5c?source=collection_archive---------5-----------------------#2021-07-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/dab9609a288af328cad4912fe703cbda.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*QV8PMuH256McRo7n6lVUPg.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">图1:一个5层的致密块(来源:DenseNet原始文件)</figcaption></figure><p id="ef10" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">DenseNet是什么？这是一个密集的网络。就这样结束了，再见。</p><p id="d202" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">开玩笑的。但事实离它并不遥远。Densenet是密集连接卷积网络的缩写。Densenet具有其中多个卷积层相互连接的块。块中的每一个<strong class="ir hi"> i </strong>层都连接到它的所有连续层，即<strong class="ir hi"> i+1，i+2，… </strong>直到最后。这种类型的连接称为剩余网络。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jn"><img src="../Images/812277a88f4664253ccc4ec2add5e61f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xz0YeWwEZtV2wv-jMfvmGg.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">图2:Densenet中致密嵌段的表示(来源:Dense net原始论文)</figcaption></figure><p id="cab0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该论文的作者在块中使用了剩余网络，但是为了进一步改善层之间的信息流，我们提出了一种不同的连接模式:从任何层到所有后续层引入直接连接。背后的核心思想是<strong class="ir hi">特征重用</strong>，这导致非常紧凑的模型。因此，它比其他CNN需要更少的参数，因为没有重复的特征映射。挤奶直到特征变干！！！</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jw"><img src="../Images/f41752ddae746991df1582cb34b22fc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfZ5WdutFu4JtxetkrWyOQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">图3:密集网层结构</figcaption></figure><p id="b6ca" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">第一卷积块有64个大小为7×7的滤波器，跨距为2。接下来是最大池层，最大池为3×3，跨距为2。这两行可以用python中的以下代码来表示。</p><p id="c05d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Densenet有两个主要的嵌段—致密嵌段和过渡层。</p><pre class="jo jp jq jr fd jx jy jz ka aw kb bi"><span id="dec0" class="kc kd hh jy b fi ke kf l kg kh">    input = Input(input_shape)<br/>    x = Conv2D(64, 7, strides=2, padding="same")(input)<br/>    x = BatchNormalization()(x)<br/>    x = ReLU()(x)<br/>    x = MaxPool2D(3, strides=2, padding="same")(x)</span></pre><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es ki"><img src="../Images/fc5a06942d798295f0bb6de4edfaf24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*0yiGmAIBxYk5Xmeq-l2FSQ.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">图4:密集块和过渡层</figcaption></figure><p id="e3b6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">致密块有两部分。1×1卷积块和3×3卷积块。与所有以前的特征映射连接可能会导致内存爆炸。1x1卷积可以抑制内存爆炸。</p><p id="b24f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先让我们在Python中定义卷积块。</p><pre class="jo jp jq jr fd jx jy jz ka aw kb bi"><span id="f58f" class="kc kd hh jy b fi ke kf l kg kh"><strong class="jy hi">def</strong> conv_layer(x, filters, kernel=1, strides=1):</span><span id="b62c" class="kc kd hh jy b fi kj kf l kg kh">    x = BatchNormalization()(x)<br/>    x = ReLU()(x)<br/>    x = Conv2D(filters, kernel, strides=strides, padding="same")(x)<br/>    <strong class="jy hi">return</strong> x</span></pre><p id="b066" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在来到密集街区。在这个块中，有一个参数决定了块的深度。叫做<strong class="ir hi">成长</strong> <strong class="ir hi">因子</strong>。增长因子是可调的，默认值是32。增长因子只是过滤器的数量。1x1模块的滤波器数量是4倍。紧随其后的是3x3卷积。然后是论文的主要亮点，每一层都连接到它的下一层。这里，使用张量流函数将输出连接到输入。</p><p id="2a58" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">根据图像大小，每个密集块重复n次。为了实现这一点，我们将创建一个重复列表，并对其应用for循环。重复次数为[6，12，24，16]。</p><pre class="jo jp jq jr fd jx jy jz ka aw kb bi"><span id="b85d" class="kc kd hh jy b fi ke kf l kg kh"><strong class="jy hi">def</strong> dense_block(x, repetition,filters):<br/>    <strong class="jy hi">for</strong> _ in range(repetition):<br/>      y = conv_layer(x, 4 * filters)<br/>      y = conv_layer(y, filters, 3)<br/>      x = concatenate([y, x])<br/>    <strong class="jy hi">return</strong> x</span></pre><p id="0c5c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">密集块的输出被传递到过渡块。有一个1x1卷积层和一个2x2平均池层，步长为2(缩小图像)。1x1的内核大小已经在函数中设置好了，所以我们不需要明确地再次定义它。在过渡层中，我们必须将通道减少到现有通道的一半。</p><pre class="jo jp jq jr fd jx jy jz ka aw kb bi"><span id="ef89" class="kc kd hh jy b fi ke kf l kg kh"><strong class="jy hi">def</strong> transition_layer(x):<br/>    x = conv_layer(x, x.shape[-1]/ 2)<br/>    x = AvgPool2D(2, strides=2, padding="same")(x)<br/>    <strong class="jy hi">return</strong> x</span></pre><p id="a03a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">完整的DenseNet 121架构:</strong></p><p id="221b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们已经将所有模块放在一起，让我们将它们合并，以查看整个Densenet 121架构。</p><p id="c549" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">完整的DenseNet 121架构:</strong></p><pre class="jo jp jq jr fd jx jy jz ka aw kb bi"><span id="5bd8" class="kc kd hh jy b fi ke kf l kg kh"><strong class="jy hi">def</strong> conv_layer(x, filters, kernel=1, strides=1):</span><span id="a1c2" class="kc kd hh jy b fi kj kf l kg kh">    x = BatchNormalization()(x)<br/>    x = ReLU()(x)<br/>    x = Conv2D(filters, kernel, strides=strides, padding="same")(x)<br/>    <strong class="jy hi">return</strong> x</span><span id="7ad7" class="kc kd hh jy b fi kj kf l kg kh"><strong class="jy hi">def</strong> dense_block(x, repetition, filters):<br/>    <strong class="jy hi">for</strong> _ in range(repetition):<br/>      y = conv_layer(x, 4 * filters)<br/>      y = conv_layer(y, filters, 3)<br/>      x = concatenate([y, x])<br/>    <strong class="jy hi">return</strong> x</span><span id="249d" class="kc kd hh jy b fi kj kf l kg kh"><strong class="jy hi">def</strong> transition_layer(x):<br/>    x = conv_layer(x, x.shape[-1]/ 2)<br/>    x = AvgPool2D(2, strides=2, padding="same")(x)<br/>    <strong class="jy hi">return</strong> x</span><span id="84bf" class="kc kd hh jy b fi kj kf l kg kh"><strong class="jy hi">def</strong> densenet(input_shape, n_classes, filters=32):</span><span id="f552" class="kc kd hh jy b fi kj kf l kg kh">    input = Input(input_shape)<br/>    x = Conv2D(64, 7, strides=2, padding="same")(input)<br/>    x = BatchNormalization()(x)<br/>    x = ReLU()(x)<br/>    x = MaxPool2D(3, strides=2, padding="same")(x)</span><span id="0d36" class="kc kd hh jy b fi kj kf l kg kh">    <strong class="jy hi">for</strong> repetition in [6, 12, 24, 16]:<br/>       d = dense_block(x, repetition,filters)<br/>       x = transition_layer(d)</span><span id="c84b" class="kc kd hh jy b fi kj kf l kg kh">     x = GlobalAveragePooling2D()(d)<br/>     output = Dense(n_classes, activation="softmax")(x)</span><span id="39aa" class="kc kd hh jy b fi kj kf l kg kh">     model = Model(input, output)<br/>     <strong class="jy hi">return</strong> model</span></pre><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kk"><img src="../Images/184a16711d99293ebba3f8f03e4eec7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nkojt8-R-cWnGgQPAFY1BA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">图5:模型总结</figcaption></figure><p id="7e2b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这就是我们如何使用TensorFlow实现Densenet 121架构。</p><p id="a048" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">要以更好的方式查看代码，请查看github上的<a class="ae kl" href="https://github.com/Haikoitoh/paper-implementation/blob/main/Densenet%20121.py" rel="noopener ugc nofollow" target="_blank">代码。</a></p><p id="9d1a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">参考文献:</strong></p><p id="ba2c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">黄高和刘庄以及劳伦斯·范德马腾和基利安·q·温伯格，密集连接的卷积网络，arXiv 1608.06993 (2016年)</p></div></div>    
</body>
</html>