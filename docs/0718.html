<html>
<head>
<title>[Pytorch] Contiguous vs Non-Contiguous Tensor / View — Understanding view(), reshape(), transpose()</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[Pytorch]连续与非连续张量/视图—了解视图()、整形()、转置()</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pytorch-contiguous-vs-non-contiguous-tensor-view-understanding-view-reshape-73e10cdfa0dd?source=collection_archive---------2-----------------------#2021-01-28">https://medium.com/analytics-vidhya/pytorch-contiguous-vs-non-contiguous-tensor-view-understanding-view-reshape-73e10cdfa0dd?source=collection_archive---------2-----------------------#2021-01-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2f61cf6a2577cfa8eb286c94df381453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e92qaBl4Kly5CKzGRNRZIQ.png"/></div></div></figure><h1 id="a689" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">张量和视图</h1></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="5d1a" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jx hj"> <em class="kt">视图使用来自原始张量的相同数据块，只是“查看”其维度的方式不同。</em>T9】</strong></p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="159e" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">在我们深入讨论邻接和非邻接意味着什么之前，我们需要先了解Pytorch中的<strong class="jx hj"> <em class="kt">张量</em> </strong>和<strong class="jx hj"> <em class="kt">视图</em> </strong>之间的关系。</p><p id="2b58" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jx hj"> <em class="kt">视图</em> </strong>无非是<strong class="jx hj"> <em class="kt">解释</em> </strong>原张量的维度<strong class="jx hj"> <em class="kt">而不在内存中制作物理副本</em> </strong>的一种替代方式。比如我们可以有一个1x12的张量，即[1，2，3，4，5，6，7，8，9，10，11，12]然后用<code class="du ku kv kw kx b">.view(4,3)</code>把张量的形状改成4x3的结构。</p><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="93fc" class="lg ir hi kx b fi lh li l lj lk">x = torch.arange(1,13)<br/>print(x)<br/>&gt;&gt; tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])</span><span id="a7b3" class="lg ir hi kx b fi ll li l lj lk">x = torch.arange(1,13)<br/>y = x.view(4,3)<br/>print(y)<br/>&gt;&gt;<br/>tensor([[ 1,  2,  3],<br/>        [ 4,  5,  6],<br/>        [ 7,  8,  9],<br/>        [10, 11, 12]])</span></pre><p id="5f96" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">如果你改变原始张量<code class="du ku kv kw kx b">x</code>中的数据，它也会反映在视图张量<code class="du ku kv kw kx b">y</code>中，因为视图张量<code class="du ku kv kw kx b">y</code>不是创建原始张量<code class="du ku kv kw kx b">x</code>的另一个副本，而是从与原始张量<code class="du ku kv kw kx b">x</code>相同的内存地址读取数据。反之亦然，视图张量中的值的改变将同时改变原始张量中的值，因为<strong class="jx hj"> <em class="kt">视图张量和它的原始张量共享同一块存储块</em> </strong>。</p><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="403a" class="lg ir hi kx b fi lh li l lj lk">x = torch.arange(1,13)<br/>y = x.view(4,3)<br/>x[0] = <strong class="kx hj">100</strong><br/>print(y)<br/>&gt;&gt; <br/>tensor([[<strong class="kx hj">100</strong>,   2,   3],<br/>        [  4,   5,   6],<br/>        [  7,   8,   9],<br/>        [ 10,  11,  12]])</span><span id="2df9" class="lg ir hi kx b fi ll li l lj lk">x = torch.arange(1,13)<br/>y = x.view(4,3)<br/>y[-1,-1] = <strong class="kx hj">1000</strong><br/>print(x)<br/>&gt;&gt; tensor([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11, <strong class="kx hj">1000</strong>])</span></pre><h1 id="4bdd" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">可以以连续的方式用不同的维度来查看数据序列</h1></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="8bab" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jx hj"> <em class="kt">张量数据被存储为1D数据序列。<br/>技术上来说</em> </strong> <em class="kt">，。view()是一个指令，告诉机器如何</em> <strong class="jx hj"> <em class="kt">跨越1D数据序列</em> </strong> <em class="kt">并提供给定维度的张量视图。</em></p><p id="7f48" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jx hj"> <em class="kt">直觉上</em> </strong> <em class="kt">，你可以想象一下。view()函数定义了新的尺寸，比如说</em> <strong class="jx hj"> <em class="kt"> (2，2，3)，作为空模板框</em> </strong> <em class="kt">，如下图所示。然后，</em> <strong class="jx hj"> <em class="kt">的数据从</em> </strong> <em class="kt">的1D数据序列开始消耗，并从最里面的点一个一个地填充到这些盒子里，直到盒子满了，才移动到下一个维度(盒子)。这遵循了一个</em><strong class="jx hj"><em class="kt"/></strong><em class="kt">的连续顺序。</em></p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><figure class="ky kz la lb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/ad9934f4a2b63c1f370fc3e1a81a3b6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DCP5OwQGZJ991pEUpHNJww.png"/></div></div></figure><figure class="ky kz la lb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ln"><img src="../Images/1defed2c6d31fdec1cd4f4053ec1223b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xm3ZfWtIn7TUD9lcwhpacg.png"/></div></div></figure><p id="d9b9" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">因此，只要盒子的总数与1D数组中的元素数相匹配，例如2x2x3 = 6x2 = 12，就可以在视图()中任意组合维度。您也可以拥有(3，2，2)或(4，3)，只要元素总数相加。</p><h1 id="85c2" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">大步</h1><p id="b7b5" class="pw-post-body-paragraph jv jw hi jx b jy lo ka kb kc lp ke kf kg lq ki kj kk lr km kn ko ls kq kr ks hb bi translated">如果你不熟悉计算机如何在内存中大步前进(1D序列)以形成N-D维张量，我鼓励你花5分钟阅读下面文章中的一节:</p><p id="112d" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">【读取】<strong class="jx hj"> <em class="kt">一个Numpy 2D数组的数据结构</em> </strong>在此<a class="ae lt" rel="noopener" href="/analytics-vidhya/a-thorough-understanding-of-numpy-strides-and-its-application-in-data-processing-e40eab1c82fe"> <strong class="jx hj"> <em class="kt">后</em> </strong> </a> <strong class="jx hj"> <em class="kt">。</em>T13】</strong></p><figure class="ky kz la lb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/17c09b0caf2f611b85ec568cb09f5659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L25Ztzox_e_bdvqv6EqjWw.png"/></div></div></figure></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="18d8" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">numpy<strong class="jx hj"><em class="kt">strides()</em></strong>returns(<strong class="jx hj"><em class="kt">N</em></strong><strong class="jx hj">字节</strong>到下一行，<em class="kt"/><strong class="jx hj"><em class="kt">M</em></strong><strong class="jx hj">字节</strong><br/>py torch<strong class="jx hj"><em class="kt">stride()</em></strong>returns(<strong class="jx hj"><em class="kt">N</em></strong><strong class="jx hj">元素</strong></p><p id="ed48" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jx hj">让我们来看看2D阵列的步幅</strong></p><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="43c8" class="lg ir hi kx b fi lh li l lj lk"># x is a contiguous data. Recall that view() doesn't change data arrangement in the original 1D tensor, i.e. the sequence from 1 to 12.</span><span id="65fc" class="lg ir hi kx b fi ll li l lj lk">x = torch.arange(1,13).view(6,2)<br/>x<br/>&gt;&gt;<br/><strong class="kx hj">tensor([[ 1,  2],<br/>        [ 3,  4],<br/>        [ 5,  6],<br/>        [ 7,  8],<br/>        [ 9, 10],<br/>        [11, 12]])</strong></span><span id="109a" class="lg ir hi kx b fi ll li l lj lk"># Check stride<br/>x.stride()<br/>&gt;&gt; <strong class="kx hj">(2, 1)</strong></span></pre><p id="6f95" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jx hj"> strides (2，1) </strong>告诉我们:我们需要跨越<strong class="jx hj"> 1 </strong>(最后一个维度，即维度0)的数字到达沿0轴的下一个数字，我们需要跨越<strong class="jx hj"> 2 </strong>(维度1)的数字行进到沿1轴的下一个数字。</p><figure class="ky kz la lb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/3eb4b70811be6c4c941f84856483215a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AnQ2KLhfYmt17ldTKpKvfQ.png"/></div></div></figure><p id="cc43" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jx hj">3D阵列的步幅如何</strong></p><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="6e58" class="lg ir hi kx b fi lh li l lj lk">y = torch.arange(0,11).view(2,2,3)<br/>y<br/>&gt;&gt;<br/><strong class="kx hj">tensor([[[ 0,  1,  2],<br/>         [ 3,  4,  5]],<br/><br/>        [[ 6,  7,  8],<br/>         [ 9, 10, 11]]])</strong></span><span id="121c" class="lg ir hi kx b fi ll li l lj lk"># Check stride<br/>y.stride()<br/>&gt;&gt; <strong class="kx hj">(6, 3, 1)</strong></span></pre><p id="6809" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">由步幅(6，3，1)表示，我们可以从每个位置开始观察<strong class="jx hj"><em class="kt"/></strong>，<em class="kt"> (i+1) </em>引导你沿0轴移动，<em class="kt"> (i+3) </em>沿1轴移动，<em class="kt"> (i+6)沿2轴移动。</em><strong class="jx hj"><em class="kt">公式检索(A，B，C) </em> </strong>在1D张量中的位置是通过:<strong class="jx hj"> A * 6 + B * 3 + C * 1 </strong>完成的</p><p id="8d5e" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">例如，在上面的张量中，我们特意选择了一个从1到12的数字序列，因为我们可以用它来表示它们在1D数组中的位置。</p><ul class=""><li id="abbe" class="lw lx hi jx b jy jz kc kd kg ly kk lz ko ma ks mb mc md me bi translated">索引<strong class="jx hj"> (0，0，0)</strong><br/>1D位置:0 * 6 + 0 * 3 + 0 * 1 = 0</li><li id="9acc" class="lw lx hi jx b jy mf kc mg kg mh kk mi ko mj ks mb mc md me bi translated">索引<strong class="jx hj"> (1，0，0) </strong> <br/>在1D的位置:1* 6 + 0 * 3 + 0 * 1 = 6</li><li id="7001" class="lw lx hi jx b jy mf kc mg kg mh kk mi ko mj ks mb mc md me bi translated">索引<strong class="jx hj"> (0，1，0) </strong> <br/>在1D的位置:0 * 6 + 1* 3 + 0 * 1 = 3</li><li id="ace2" class="lw lx hi jx b jy mf kc mg kg mh kk mi ko mj ks mb mc md me bi translated">索引<strong class="jx hj"> (0，0，1) </strong> <br/>在1D的位置:0 * 6 + 0* 3 + 1* 1 = 1</li><li id="40d8" class="lw lx hi jx b jy mf kc mg kg mh kk mi ko mj ks mb mc md me bi translated">索引<strong class="jx hj"> (1，1，1)<br/></strong>1D位置:1* 6 + 1* 3 + 1* 1 = 10</li></ul><p id="4e89" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">好了，现在我们完成了<strong class="jx hj"> <em class="kt">连续</em> </strong>视图的介绍，也学习了Pytorch中的<strong class="jx hj"> <em class="kt">步长</em> </strong>如何在N维张量中工作。现在我们来看看非连续数据是什么样的。</p><h1 id="214e" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">非连续数据结构:Transpose( ) </strong></h1><p id="d205" class="pw-post-body-paragraph jv jw hi jx b jy lo ka kb kc lp ke kf kg lq ki kj kk lr km kn ko ls kq kr ks hb bi translated"><strong class="jx hj"> <em class="kt">首先，Transpose(axis1，axis2)简单来说就是“交换axis1和axis2跨越的方式。</em>T19】</strong></p><figure class="ky kz la lb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/03fffffffe4a090eabd9dd4c430a7d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HRWWBxD3H0rkO4r5J64dVg.png"/></div></div></figure></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="af79" class="lg ir hi kx b fi lh li l lj lk"># Initiate a contiguous tensor<br/>x = torch.arange(0,12).view(2,2,3)<br/><strong class="kx hj">x</strong><br/>&gt;&gt;<br/><strong class="kx hj">tensor([[[ 0,  1,  2],<br/>         [ 3,  4,  5]],<br/><br/>        [[ 6,  7,  8],<br/>         [ 9, 10, 11]]])</strong></span><span id="0ff0" class="lg ir hi kx b fi ll li l lj lk">x.stride()<br/><strong class="kx hj">&gt;&gt; (6,</strong>3,<strong class="kx hj">1)</strong></span><span id="40c1" class="lg ir hi kx b fi ll li l lj lk"># Now let's transpose axis 0 and 1, and see how the strides swap<br/>y = <strong class="kx hj">x.transpose(0,2)</strong><br/><strong class="kx hj">y</strong><br/>&gt;&gt;<br/><strong class="kx hj">tensor([[[ 0,  6],<br/>         [ 3,  9]],<br/><br/>        [[ 1,  7],<br/>         [ 4, 10]],<br/><br/>        [[ 2,  8],<br/>         [ 5, 11]]])</strong></span><span id="5179" class="lg ir hi kx b fi ll li l lj lk">y.stride()<br/><strong class="kx hj">&gt;&gt; (1,</strong>3,<strong class="kx hj">6)</strong></span></pre><p id="51f9" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">好的，<strong class="jx hj"> y </strong>是一个<strong class="jx hj"> <em class="kt"> x.transpose(0，2) </em> </strong>，它交换了x张量在0轴和2轴的步幅，因此得到的<strong class="jx hj"> y </strong>的步幅是(<strong class="jx hj"> 1，3，6 </strong>)。这意味着我们需要跳转<strong class="jx hj"> 6个</strong>数字来获取0轴的下一个数字，跳转<strong class="jx hj"> 3个</strong>数字来获取1轴的下一个数字，跳转<strong class="jx hj"> 1 </strong>来获取2轴的下一个数字。(大步走公式:<strong class="jx hj"> A * 1+ B * 3+ C * 6) </strong></p><p id="6725" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">转置的不同方面是:现在数据序列是<strong class="jx hj"> <em class="kt">不再遵循连续的顺序</em> </strong>。它不会从最里面的维度开始逐个填充顺序数据，填充完后会跳到下一个维度。现在它在最里面的维度跳了6个数，所以不是连续的。</p><p id="9732" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jx hj">转置( )具有非连续的数据结构，但仍然是视图而不是副本</strong></p><p id="12a8" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">transpose()仍然返回一个视图，但不是原始张量的副本。因此，它是非连续的“视图”。它改变了原始数据的步长，对原始张量的任何数据修改都会影响视图，反之亦然。</p><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="acd9" class="lg ir hi kx b fi lh li l lj lk"># Change the value in a transpose tensor y<br/>x = torch.arange(0,12).view(2,6)<br/>y = x.transpose(0,1)<br/>y[0,0] = 100<br/>y<br/>&gt;&gt;<br/>tensor([[<strong class="kx hj">100</strong>,   2,   4,   6,   8,  10],<br/>        [  1,   3,   5,   7,   9,  11]])</span><span id="7b32" class="lg ir hi kx b fi ll li l lj lk"># Check the original tensor x<br/>x<br/>&gt;&gt;<br/>tensor([[<strong class="kx hj">100</strong>,   1],<br/>        [  2,   3],<br/>        [  4,   5],<br/>        [  6,   7],<br/>        [  8,   9],<br/>        [ 10,  11]])</span></pre><p id="8b71" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">使存储在张量中的1D数据序列达到峰值的另一种方式是通过方法<code class="du ku kv kw kx b">.storage()</code></p><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="1274" class="lg ir hi kx b fi lh li l lj lk">y.storage()<br/>&gt;&gt;<br/> 100<br/> 1<br/> 2<br/> 3<br/> 4<br/> 5<br/> 6<br/> 7<br/> 8<br/> 9<br/> 10<br/> 11<br/>[torch.LongStorage of size 12]</span></pre><h1 id="60b8" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">检查Pytorch中的连续和非连续</h1><p id="9072" class="pw-post-body-paragraph jv jw hi jx b jy lo ka kb kc lp ke kf kg lq ki kj kk lr km kn ko ls kq kr ks hb bi translated">Pytorch有一个方法<code class="du ku kv kw kx b">.is_contiguous()</code>告诉你张量是否是连续的。</p><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="283b" class="lg ir hi kx b fi lh li l lj lk">x = torch.arange(0,12).view(2,6)<br/>x.<strong class="kx hj">is_contiguous()</strong><br/>&gt;&gt; True</span><span id="2b59" class="lg ir hi kx b fi ll li l lj lk">y = x.transpose(0,1)<br/>y.<strong class="kx hj">is_contiguous()</strong><br/>&gt;&gt; False</span></pre><p id="8335" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jx hj">将不连续的张量(或视图)转换成连续的</strong></p><p id="8afa" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">Pytorch有一个方法<code class="du ku kv kw kx b">.contiguous()</code>，可以将不连续的张量或视图转换为连续的。</p><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="90c8" class="lg ir hi kx b fi lh li l lj lk">z = y<strong class="kx hj">.contiguous()</strong><br/>z.is_contiguous()<br/>&gt;&gt; TRUE</span></pre><p id="67be" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">它<strong class="jx hj">复制一份原始‘非连续’张量的副本</strong>，然后按照<strong class="jx hj">连续顺序</strong>将其保存到新的内存块中。我们可以通过它的步伐来观察它。</p><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="2c8a" class="lg ir hi kx b fi lh li l lj lk"># This is <strong class="kx hj">contiguous</strong><br/>x = torch.arange(1,13).view(2,3,2)<br/>x.stride()<br/>&gt;&gt; (6, 2, 1)</span><span id="a8e4" class="lg ir hi kx b fi ll li l lj lk"># This is <strong class="kx hj">non-contiguous</strong><br/>y = x.transpose(0,1)<br/>y.stride()<br/>&gt;&gt; (2, 6, 1)</span><span id="0069" class="lg ir hi kx b fi ll li l lj lk"># This is a <strong class="kx hj">converted contiguous</strong> tensor with new stride<br/>z = y.contiguous()<br/>z.stride()<br/>&gt;&gt; (4, 2, 1)</span></pre><p id="829e" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">我用来区分张量/视图是否连续的一种方法是通过观察步长中的(A，B，C)是否满足A &gt; B &gt; C。如果不满足，这意味着至少一个维度比它上面的维度跳过了更长的距离，这使得它不连续。</p><p id="8555" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">我们还可以观察转换后的连续张量<strong class="jx hj"> <em class="kt"> z </em> </strong>如何以新的顺序存储数据。</p><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="2f62" class="lg ir hi kx b fi lh li l lj lk"># y is a non-contiguous 'view' (remember view uses the original chunk of data in memory, but its strides implies 'non-contiguous', (2,6,1).<strong class="kx hj"><br/>y.storage()</strong><br/>&gt;&gt;<br/> 1<br/> 2<br/> 3<br/> 4<br/> 5<br/> 6<br/> 7<br/> 8<br/> 9<br/> 10<br/> 11<br/> 12</span><span id="30e1" class="lg ir hi kx b fi ll li l lj lk"># Z is a 'contiguous' tensor (not a view, but a new copy of the original data. Notice the order of the data is different). It strides implies 'contiguous', (4,2,1)<br/><strong class="kx hj">z.storage()</strong><br/>&gt;&gt;<br/> 1<br/> 2<br/> 7<br/> 8<br/> 3<br/> 4<br/> 9<br/> 10<br/> 5<br/> 6<br/> 11<br/> 12</span></pre><h1 id="ac6c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">视图( )和整形( )之间的区别</h1><p id="32ed" class="pw-post-body-paragraph jv jw hi jx b jy lo ka kb kc lp ke kf kg lq ki kj kk lr km kn ko ls kq kr ks hb bi translated">虽然这两个函数都可以<strong class="jx hj"> <em class="kt">改变张量的维度</em> </strong>(基本上这只是在1D数据中的不同大步方式)，但两者之间的主要区别是:</p><p id="2b2f" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">1/ <strong class="jx hj"> view(): </strong>难道<strong class="jx hj"> <em class="kt">不是</em> </strong>做了一个<strong class="jx hj">副本</strong>的原张量。它改变了对原始数据的维度解释(跨度)。换句话说，它使用与原始张量相同的数据块，因此它<strong class="jx hj"> <em class="kt">只</em> </strong>处理<strong class="jx hj">连续数据</strong>。</p><p id="c24c" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">2/ <strong class="jx hj"> reshape(): </strong> <strong class="jx hj">返回一个可能的视图</strong>(即当数据连续时)。<strong class="jx hj">如果不是</strong>(即数据不是连续的)，那么它<strong class="jx hj">将数据复制</strong>到连续的数据块中，并且作为副本，它将占用存储空间，并且新张量的变化也不会影响原始张量中的值。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="bc5c" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">使用<strong class="jx hj">连续数据</strong>，reshape()返回一个视图。</p><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="5e3b" class="lg ir hi kx b fi lh li l lj lk"># When data is contiguous<br/>x = torch.arange(1,13)<br/>x<br/>&gt;&gt; tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])</span><span id="fba1" class="lg ir hi kx b fi ll li l lj lk"># Reshape returns a view with the new dimension<br/>y = x.reshape(4,3)<br/>y<br/>&gt;&gt;<br/>tensor([[ 1,  2,  3],<br/>        [ 4,  5,  6],<br/>        [ 7,  8,  9],<br/>        [10, 11, 12]])</span><span id="92a0" class="lg ir hi kx b fi ll li l lj lk"># How do we know it's a view? Because the element change in new tensor y would affect the value in x, and vice versa<br/>y[0,0] = 100<br/>y<br/>&gt;&gt;<br/>tensor([[100,   2,   3],<br/>        [  4,   5,   6],<br/>        [  7,   8,   9],<br/>        [ 10,  11,  12]])</span><span id="e596" class="lg ir hi kx b fi ll li l lj lk">print(x)<br/>&gt;&gt;<br/>tensor([100,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12])</span></pre><p id="aca1" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">接下来，让我们看看reshape()如何处理<strong class="jx hj">非连续数据。</strong></p><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="3e6d" class="lg ir hi kx b fi lh li l lj lk"># After transpose(), the data is non-contiguous<br/>x = torch.arange(1,13).view(6,2).transpose(0,1)<br/>x<br/>&gt;&gt;<br/>tensor([[ 1,  3,  5,  7,  9, 11],<br/>        [ 2,  4,  6,  8, 10, 12]])</span><span id="3bd4" class="lg ir hi kx b fi ll li l lj lk"># Reshape() works fine on a non-contiguous data<br/>y = x.reshape(4,3)<br/>y<br/>&gt;&gt;<br/>tensor([[ 1,  3,  5],<br/>        [ 7,  9, 11],<br/>        [ 2,  4,  6],<br/>        [ 8, 10, 12]])</span><span id="5771" class="lg ir hi kx b fi ll li l lj lk"># Change an element in y<br/>y[0,0] = 100<br/>y<br/>&gt;&gt;<br/>tensor([[100,   3,   5],<br/>        [  7,   9,  11],<br/>        [  2,   4,   6],<br/>        [  8,  10,  12]])</span><span id="153b" class="lg ir hi kx b fi ll li l lj lk"># Check the original tensor, and nothing was changed<br/>x<br/>&gt;&gt;<br/>tensor([[ 1,  3,  5,  7,  9, 11],<br/>        [ 2,  4,  6,  8, 10, 12]])</span></pre><p id="4641" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">最后，我们来看看<strong class="jx hj"> view() </strong>能否对<strong class="jx hj">非连续数据</strong>起作用。<br/> <strong class="jx hj"> <em class="kt">不，不行！</em> </strong></p><pre class="ky kz la lb fd lc kx ld le aw lf bi"><span id="d373" class="lg ir hi kx b fi lh li l lj lk"># After transpose(), the data is non-contiguous<br/>x = torch.arange(1,13).view(6,2).transpose(0,1)<br/>x<br/>&gt;&gt;<br/>tensor([[ 1,  3,  5,  7,  9, 11],<br/>        [ 2,  4,  6,  8, 10, 12]])</span><span id="67af" class="lg ir hi kx b fi ll li l lj lk"># Try to use view on the non-contiguous data<br/>y = x.view(4,3)<br/>y<br/>&gt;&gt;<br/>-------------------------------------------------------------------<br/>RuntimeError Traceback (most recent call last)<br/>----&gt; 1 y = x.view(4,3)<br/><strong class="kx hj">      </strong>2 y<br/><br/>RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.</span></pre></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h1 id="b408" class="iq ir hi bd is it ml iv iw ix mm iz ja jb mn jd je jf mo jh ji jj mp jl jm jn bi translated">摘要</h1><ul class=""><li id="cb01" class="lw lx hi jx b jy lo kc lp kg mq kk mr ko ms ks mb mc md me bi translated">一个'<strong class="jx hj">视图</strong>'使用<strong class="jx hj">内存块</strong>的相同块作为原始张量，因此这个内存块中的任何变化都会影响所有视图和与之相关联的原始张量。</li><li id="a408" class="lw lx hi jx b jy mf kc mg kg mh kk mi ko mj ks mb mc md me bi translated">一个'<strong class="jx hj">视图</strong>'可以是连续的，也可以是不连续的。</li><li id="9836" class="lw lx hi jx b jy mf kc mg kg mh kk mi ko mj ks mb mc md me bi translated">一个<strong class="jx hj">不连续的</strong>张量视图可以<strong class="jx hj">转换</strong>成一个连续的，并且它会制作一个<strong class="jx hj">副本</strong>，因此数据将不再与原始数据块相关联。</li><li id="1b82" class="lw lx hi jx b jy mf kc mg kg mh kk mi ko mj ks mb mc md me bi translated"><strong class="jx hj">步距位置公式:</strong>给定一个步距(A，B，C)，索引(j，k，v)在1D数据数组中的位置为(A *j + B*k + C*v)</li><li id="24dc" class="lw lx hi jx b jy mf kc mg kg mh kk mi ko mj ks mb mc md me bi translated"><strong class="jx hj">视图()</strong>和<strong class="jx hj">整形()</strong> : <br/> <strong class="jx hj">视图()</strong>不能应用于“非连续”张量/视图。它返回一个视图。<br/><strong class="jx hj"/>shape()可应用于‘连续’和‘非连续’张量/视图。如果可能，它将返回一个视图；当数据不连续时，它会制作一个新的副本。</li></ul><p id="0dee" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">我希望这篇文章能增强你的张量直觉，让你在工作中快速识别张量维度！如果你觉得这个帖子有用，请给这个帖子留个“掌声”。你的鼓励是我前进的动力！感谢您的阅读！</p></div></div>    
</body>
</html>