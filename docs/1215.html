<html>
<head>
<title>A Complete Guide to Adam and RMSprop Optimizer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Adam和RMSprop优化器完全指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-complete-guide-to-adam-and-rmsprop-optimizer-75f4502d83be?source=collection_archive---------0-----------------------#2021-02-20">https://medium.com/analytics-vidhya/a-complete-guide-to-adam-and-rmsprop-optimizer-75f4502d83be?source=collection_archive---------0-----------------------#2021-02-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="cc2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从算法开始到实现。</p><p id="4642" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最优化是一门数学学科，它在定量的明确定义的意义上确定“最佳”解决方案。由偏微分方程支配的过程的数学优化在过去十年中已经取得了相当大的进展，并且从那时起，它已经被应用于各种各样的学科，例如，科学、工程、数学、经济学，甚至商业。最优化理论提供了解决结构良好的最优化问题的算法以及对这些算法的分析。典型的优化问题包括在给定约束下最小化或最大化的目标函数。最优化理论提供了解决结构良好的最优化问题的算法以及对这些算法的分析。机器学习(尤其是神经网络)中的优化算法旨在最小化一个目标函数(通常称为损失或成本函数)，该目标函数直观上是预测数据和期望值之间的差异</p><p id="ecea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于随机梯度的优化在许多科学和工程领域具有重要的实际意义。这些领域中的许多问题都可以归结为要求参数最大化或最小化的标量参数化目标函数的优化。梯度下降是一种优化算法，它使用目标函数的梯度来导航搜索空间。文献中存在几种基于梯度下降的优化算法，但仅举几个例子，梯度下降优化算法的分类如下:</p><h1 id="13b7" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">一阶优化算法</strong></h1><p id="fa0a" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">一阶方法使用函数的一阶导数来最小化。</p><ol class=""><li id="67a9" class="kg kh hi ih b ii ij im in iq ki iu kj iy kk jc kl km kn ko bi translated">动力</li><li id="4b55" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">内斯特罗夫加速梯度</li><li id="747a" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">阿达格拉德</li><li id="01c3" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">阿达德尔塔</li><li id="1890" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated"><strong class="ih hj"> RMSprop </strong></li><li id="ab99" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated"><strong class="ih hj">亚当</strong></li><li id="34a0" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">阿达马克斯</li><li id="70d3" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">那达慕</li><li id="bf8d" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">阿姆斯格勒</li></ol><h1 id="2d18" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">二阶优化算法</strong></h1><p id="bf0a" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">二阶方法利用Hessian矩阵(损失函数相对于其参数的二阶导数矩阵)的估计。</p><ol class=""><li id="6eaf" class="kg kh hi ih b ii ij im in iq ki iu kj iy kk jc kl km kn ko bi translated">牛顿法</li><li id="019a" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">共轭梯度</li><li id="9fda" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">拟牛顿法</li><li id="837f" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">Levenberg-Marquardt算法。</li></ol><p id="e2c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将从Adam和RMSprop的算法开始，到它在python中的实现，然后我们将比较它的性能。</p><blockquote class="ku kv kw"><p id="9d71" class="if ig kx ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">设J(θ)是一个由模型的参数θ ∈ Rn参数化的函数，该函数是充分可微的，可以求其最小值。梯度法建立的序列原则上应该接近最小值。为此，我们从任意值x_0(例如随机值)开始，并通过以下方式构造递归序列:</p></blockquote><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es lb"><img src="../Images/fee84ee1b4bd463a8e467f4827bc5720.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*YmInlbwUxWNyBtf7cHBmDg.png"/></div></figure><p id="de31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中η是学习率。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es lj"><img src="../Images/db0974f1fc87e311ab77b2e74e20b8d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*SzzUmvSQZY-U4eq5RuFbxw.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">梯度下降的Python代码</figcaption></figure><p id="9177" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在正常的随机梯度下降算法中，我们固定了所有递归序列的学习率值，因此导致收敛缓慢。对于像Adam和RMSprop 这样的<strong class="ih hj">自适应方法，每个参数的学习率是可变的。即使输入样本不是线性可分的，这种方法也能保证收敛到一个精心选择的学习率的误差函数的最小值。</strong></p><p id="69d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然<strong class="ih hj"> ADAGRAD </strong>对于稀疏设置工作良好，但是已经观察到它的性能在损失函数非凸且梯度密集的设置中恶化，这是由于在这些设置中学习速率的快速衰减，因为<strong class="ih hj">在更新</strong>中使用了所有过去的梯度。为了解决这个问题，已经提出了ADAGRAD的几个变体，例如<strong class="ih hj"> RMSprop、ADAM、ADADELTA </strong>等，这些变体使用过去梯度平方的<strong class="ih hj">指数移动平均值</strong>来减轻学习速率的快速衰减，本质上将更新的可靠性限制在仅仅过去的几个梯度。</p><h1 id="ac90" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">RMSprop优化器</h1><p id="6695" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">RMSprop是一种用于训练神经网络的基于梯度的优化技术。它是由反向传播之父杰弗里·辛顿提出的。当数据通过函数传播时，像神经网络这样非常复杂的函数的梯度有消失或爆炸的趋势(参考消失梯度问题)。Rmsprop是作为小批量学习的随机技术开发的。</p><p id="3bee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RMSprop通过使用平方梯度的移动平均值来归一化梯度，从而处理上述问题。这种标准化平衡了步长(动量)，减少了大梯度的步长以避免爆炸，增加了小梯度的步长以避免消失。</p><p id="97ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，RMSprop使用自适应学习率，而不是将学习率视为超参数。这意味着学习率随着时间而变化。</p><p id="0fed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RMSprop的更新规则:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es lo"><img src="../Images/3dc345066f5ed552512b3bde1c7589f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*bdgqAQdEgpyBZscgNgx6hQ.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">RMSprop优化器的更新规则</figcaption></figure><h1 id="c25b" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">让我们用Python编码</strong></h1><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lp"><img src="../Images/33222fb535a6454bdbbb2499c7ab0283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AP6HZfwrf_QcEGNdjEoS_A.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">RMSprop的Python代码</figcaption></figure><h1 id="9c5a" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">亚当优化器</strong></h1><p id="c9a0" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">Adam (Kingma &amp; Ba，2014)是一种基于一阶梯度的随机目标函数算法，基于低阶矩的自适应估计。Adam是许多机器学习实践者正在使用的最新最先进的优化算法之一。由第二矩归一化的第一矩给出了更新的方向。</p><p id="cfef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">亚当的更新规则:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es lu"><img src="../Images/8fbdbb6bd6def3a532476b9f9b7d8d86.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*dAIl_nEgxW0H2GafvqSgEQ.png"/></div></figure><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lv"><img src="../Images/491482d80d15e32ae70a21d980ea302b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FnZxx-ssZXdTzIuuf1NEzw.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图1:亚当算法[1]</figcaption></figure><h1 id="94f2" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">让我们用Python编码</strong></h1><p id="ee10" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">ADAM优化器的python代码如下所示，</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lw"><img src="../Images/a59278d4ca71bb06e08b294fde7e86af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2nVzuQ8oxaUSXQTDQG9xKw.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">ADAM优化器的Python代码</figcaption></figure><p id="551e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是啊，就这么简单。</p><h1 id="6cd3" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">结果</strong></h1><p id="9ce7" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">我已经从头开始编码神经网络，并实现了上述优化器。然后，在包含60，000个训练样本和10，000个测试样本的MNIST手写数据集上，对该神经网络进行100个时期的训练。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lx"><img src="../Images/3c49d73d7e98d3214f21dc4001ab4dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sokR20ZAyFDTNm9WGrnoPA.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图2:结果</figcaption></figure><h1 id="76d5" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">延长</h1><p id="a3cb" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">仅供参考，我还实现了Adamax优化器，它是Adam优化器的扩展，正如您从结果中看到的那样。如果您对Adamax的实现更感兴趣，我推荐读者阅读迪德里克·p·金马、吉米·巴雷的论文。亚当:一种随机优化方法。</p><h1 id="f0a7" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">结论</strong></h1><p id="79ed" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">在本文中，我们已经看到了基于梯度优化的简单且计算高效的算法。我们已经看到了RMSprop和ADAM优化器是如何简单且易于实现的。实验证实了对凸问题收敛速度的分析。</p><p id="b5c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你更有兴趣从头开始看神经网络的详细代码，你可以在我的Github上找到。</p><p id="cd72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【https://github.com/sanghvirajit/Feedforward_Neural_Network T4】</p><h1 id="e058" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">参考文献</strong></h1><p id="a924" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">[1] <a class="ae ly" href="https://arxiv.org/pdf/1412.6980.pdf" rel="noopener ugc nofollow" target="_blank">迪德里克·p·金马，吉米·巴雷。亚当:一种随机优化方法。</a></p><p id="8a29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2] <a class="ae ly" href="https://docs.google.com/viewer?url=http%3A%2F%2Fwww.cs.toronto.edu%2F~hinton%2Fcoursera%2Flecture6%2Flec6.pdf" rel="noopener ugc nofollow" target="_blank"> Tieleman，t .和Hinton，g .讲座6.5 — RMSProp，COURSERA:用于机器学习的神经网络。2012年技术报告。</a></p></div></div>    
</body>
</html>