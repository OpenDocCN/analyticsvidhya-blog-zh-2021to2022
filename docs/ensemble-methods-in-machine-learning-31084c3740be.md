# 机器学习中的集成方法

> 原文：<https://medium.com/analytics-vidhya/ensemble-methods-in-machine-learning-31084c3740be?source=collection_archive---------7----------------------->

在本文中，我们将尝试熟悉不同的集成技术和一些常见的算法。

集成方法是对来自训练数据集的多个样本多次使用算法或模型。“集成方法的目标是将几个基本估计量的预测与给定的学习算法结合起来，以提高单个估计量的可推广性/稳健性。”

例如，如果给你一个 5 人团队的任务“A ”,你将有多种选择来完成这个任务。选项 1 :你试图独自完成这件事，你会找到一个更简单的方法，但最终不会有好的结果。
**选项 2** :或者你可能会全力以赴完美地完成任务，但当你被分配到一个类似的新任务‘B’时，你将会一无所知，或者你将不得不为那个任务重新训练自己(过度适应)。选项 3 :你把你的任务分配给你的团队成员，然后把他们结合起来，得到好的结果。现在，当类似的任务再次被分配时，同样的成员可以更有效地完成它。

从以上三个选项来看，你足够聪明，可以选择第三个。对吗？如果我们能同样训练模型。这些集合方法允许我们做同样的事情。

![](img/23d9ad65e67e686ee355c69a563f2b82.png)

集成学习是解决偏差-方差权衡的一种方法。一个好的模型应该在这两种类型的错误之间保持平衡。这就是所谓的偏差-方差误差的权衡管理。

**系综方法主要可以分为两类:** 1。并行集成方法2。顺序集成方法

# **1。并行集成方法:**

在并行集成方法中，基本学习器或算法是并行生成的。几个估计或模型是独立建立的，然后平均他们的预测。平均而言，组合估计量通常比任何单基估计量都要好，因为它的**方差减少了。
组合预测的方差减少到 1/n (n 是模型或样本的数量)。**

并行集成技术中最常见的方法之一是自举聚合(Bagging)。

# **自举汇总(装袋):**

在这种方法中，使用相同的算法生成模型，使用 bootstrap 采样方法对数据集的随机子样本进行采样，以减少方差。在 bootstrap 抽样中，有些原始样本出现不止一次，有些原始样本在样本中不存在。

装袋技术对回归和分类都有用。

![](img/0d7d64ef6e21b27c38c0d16457d13bcc.png)

在回归中，它取所有模型的平均值，在分类中，它考虑每个模型的投票。

![](img/f7d6bc6a85c6b3fb25785a6169a786da.png)

分类投票

随机森林是流行的装袋算法之一。

**随机森林(Bagging 算法):** 在每个样本的随机森林中，使用共同形成森林的决策树，因此形成随机森林。Rest 几乎类似于一个简单的决策树。

使用随机森林时要调整的主要超参数:
n_estimators:树的数量
max_features:采样时要考虑的要素的数量。

其他的超参数有
max_depth:树的最大深度(默认= 'None')
min_samples_split:节点的分裂数(默认= 2)。
标准:衡量分割质量的函数(默认值=' Gini ')

然而，当适当的程序被忽略时，结果模型可能经历许多偏差。此外，它的计算量也很大。

# **2。顺序集成方法(Boosting 算法):**

在顺序集成方法中，基本学习器是顺序产生的。通过用更高的权重对先前错误标记的示例进行加权，可以提高整体性能。总的来说，它减少了组合估计量的偏差。
简单来说，boosting 指的是能够将弱学习者转化为强学习者的算法家族。boosting 的主要原理是拟合一系列在不同版本的数据中表现更好的弱学习器/模型。前几轮被错误分类的例子会得到更多的重视。
这里每个模型都依赖于前一个模型。

![](img/d7aa0d58b2484c6b81cee437c54366c8.png)

在 Boosting 中，ML 模型建立在每个阶段的错误分类数据上。最后，聚合每个阶段的所有正确分类器，形成一个强学习器。

**算法:**

# **1。梯度推进决策树(GBDT)**

在决策树上使用梯度下降算法来减少每一层的损失。它通过允许优化任意可微损失函数来概括模型。下一个模型建立在前一个模型的错误之上。

![](img/70404310b71af8db82352b63a4ad8637.png)

# **②*。XG 升压***

> XG Boost 的工作方式类似于 GBDT，但它有更多的功能(见下文),这使它更加高效。
> i) **正则化:**标准 GBM 实现没有 XGBoost 那样的正则化，因此也有助于减少过拟合。事实上，XGBoost 也被称为“正则化增强”技术。
> **ii)并行处理:** XGBoost 在根节点实现并行处理，与 GBM 相比速度更快。
> iii) **高灵活性:** XGB 允许定制评估标准。
> iv) **处理缺失值:**可以自己处理缺失值。
> v) **树修剪:** XGB 自己做树修剪，减少方差。
> vi) **内置交叉验证:** XGBoost 允许用户在 boosting 过程的每次迭代中运行交叉验证。
> vii) **继续现有模型:**用户可以从上次运行的最后一次迭代开始训练 XGBoost 模型。

# **3。Ada Boost**

这里，通常，每个弱学习器被开发为决策树桩(树桩是只有一个裂口和两个终端节点的树)，用于对观察结果进行分类。
训练完每个分类器后，根据其准确度计算分类器的权重。准确度越高，分类器的权重就越大，反之亦然。

# **4。轻型 GBM**

它以最佳拟合的方式按叶分割树，而其他 boosting 算法按深度或级别而不是按叶分割树，并试图尽快获得纯叶。这使得 Light GBM 比其他增强算法更快，因此有了“Light”这个词。

# **5。卡特彼勒助力**

“CatBoost”名字来源于两个词“Category”和“Boosting”。CatBoost 使用分类特征组合以及分类和数字特征组合的各种统计数据将分类值转换为数字。因此，我们不需要像在其他模型中那样将分类变量转换成数字变量。它生成了一棵平衡树，可以像 XGB 一样处理丢失的值。

# **3。堆叠**

堆叠是一种集成学习技术，它通过元分类器或元回归器组合多个分类或回归模型。基于完整的训练集来训练基础级模型，然后在基础级模型的输出上训练元模型作为特征。

![](img/6d3806961ebb01ee7fe02ecbc39d1b67.png)

我希望这篇文章有助于对不同的集成方法和常用算法有一个基本的了解。如果您有任何疑问，请在下面留下。