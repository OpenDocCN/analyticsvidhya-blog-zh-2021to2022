<html>
<head>
<title>Intro to Bayesian Parameter Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯参数估计简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/intro-to-bayesian-parameter-estimation-f324498bb505?source=collection_archive---------13-----------------------#2021-01-26">https://medium.com/analytics-vidhya/intro-to-bayesian-parameter-estimation-f324498bb505?source=collection_archive---------13-----------------------#2021-01-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/cecea396f205766348a06b5765171e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oxwfFjFeYQAsGpYnjsLVAA.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://unsplash.com/photos/TBHOuN6URGU" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/TBHOuN6URGU</a></figcaption></figure><h1 id="e705" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">什么是贝叶斯参数估计？</h1><p id="b462" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">还记得上一篇文章中的最大似然估计吗？在MLE中，我们假设训练数据是总体数据的良好表示。如果我们有先验信息呢？我们如何在参数估计中利用这些先验信息？这就是贝叶斯参数估计的用武之地。在贝叶斯参数估计中，θ是一个随机变量，其中关于θ的先验信息是给定的或假设的。我们基于新的训练样本更新先验假设/知识。这背后的想法是对θ的知识有一个起点，但我们用通过训练样本看到的数据来提炼这个先验知识。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kq"><img src="../Images/5768eba1b1d3aa8882d01d274fd94d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_8y6GANp95F-6yErnJ2zgQ.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">贝叶斯定理</figcaption></figure><p id="4d91" class="pw-post-body-paragraph js jt hh ju b jv kv jx jy jz kw kb kc kd kx kf kg kh ky kj kk kl kz kn ko kp ha bi translated">这个公式实际上是贝叶斯定理。P(D|θ)也是MLE中的相同公式。P(θ)是θ上的先验概率。P(D)被称为证据，但用于规范化方程(使方程成为概率)。如果你需要复习如何计算<a class="ae it" rel="noopener" href="/analytics-vidhya/intro-to-maximum-likelihood-estimate-64616f8a624">极大似然估计</a>或<a class="ae it" rel="noopener" href="/analytics-vidhya/bayes-theorem-made-simple-b1b403bf5ffb">贝叶斯定理</a>，我有关于这两个问题的博客文章，所以一定要去看看！那么，我们该何去何从？</p><h1 id="734e" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">贝叶斯参数估计示例</h1><p id="edb0" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">让我们再来看一下<a class="ae it" rel="noopener" href="/analytics-vidhya/intro-to-maximum-likelihood-estimate-64616f8a624"> MLE </a>博客中的例子，我们假设抛硬币的P(正面)= 0.65。假设你的朋友使用了同样的硬币，他昨天将硬币抛了100次，得到了50次正面，所以他假设P(正面)= 0.50。知道MLE没有考虑先验知识，我们决定迁就这个朋友。我们把他所说的关于100次抛硬币的话作为先验。那么，我们如何计算新的θ来最大化向我们的朋友证明50%并不总是最佳估计的可能性呢？</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es la"><img src="../Images/8502a6550aa304d35642a5a19a997be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ZO_gejjOUkBa2OflHzgJA.jpeg"/></div></div></figure><p id="fde1" class="pw-post-body-paragraph js jt hh ju b jv kv jx jy jz kw kb kc kd kx kf kg kh ky kj kk kl kz kn ko kp ha bi translated">上面的公式是我们最大化θ的方法。该等式对θs相对于P(θ|D)或给定训练数据的θ概率进行平均，从而得出平均θ值(这是最有可能的)。让我们开始求解hat。</p><h1 id="6d7f" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">贝叶斯参数估计的计算</h1><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lb"><img src="../Images/caccd3dbbee0dd9c4cd9b7eb46af0e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhCg_SlNQIkacVEDojVWDQ.jpeg"/></div></div></figure><p id="399c" class="pw-post-body-paragraph js jt hh ju b jv kv jx jy jz kw kb kc kd kx kf kg kh ky kj kk kl kz kn ko kp ha bi translated">首先，我们需要找到P(θ|D)。我们只能用贝叶斯定理来解决这个问题。我们知道P(D|θ)与来自<a class="ae it" rel="noopener" href="/analytics-vidhya/intro-to-maximum-likelihood-estimate-64616f8a624"> MLE </a>博客的可能性相同，但是P(θ)和P(D)是什么呢？我们来关注P(θ)。你可能会想，这个奇怪的贝塔函数是什么？由于θ是一个概率，我们需要使用一个模拟概率的分布，因此我们使用β。至于对P(D)的解释，我建议查看我在<a class="ae it" rel="noopener" href="/analytics-vidhya/bayes-theorem-made-simple-b1b403bf5ffb">贝叶斯定理</a>上的帖子。现在有了P(D|θ)，我们来找θhat。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lb"><img src="../Images/ab08c29b8d7bb0382f12cfe82a395946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mrwifw6zZMmSpag8ImcrdQ.jpeg"/></div></div></figure><p id="dfb7" class="pw-post-body-paragraph js jt hh ju b jv kv jx jy jz kw kb kc kd kx kf kg kh ky kj kk kl kz kn ko kp ha bi translated">我们抛硬币1000次作为我们的训练数据。我们得到正面650次，反面450次。这导致θ的最大估计值为:</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lc"><img src="../Images/967a8f61da9e302adfb3b68aa516bcd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oOaBEAdVV_XLaz1LgNUFzg.jpeg"/></div></div></figure><p id="3d15" class="pw-post-body-paragraph js jt hh ju b jv kv jx jy jz kw kb kc kd kx kf kg kh ky kj kk kl kz kn ko kp ha bi translated">因此，我们更接近了，但更重要的是，我们有一个公式，当我们有先验信息时，可以找到θ最可能的估计值。如果这位朋友多抛几次硬币，他可能是对的吗？技术上来说是的，但我会假设随着更多的翻转，硬币会收敛到P(正面)= 0.65。</p><h1 id="a138" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">结论</h1><p id="bb02" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">希望你能留下来完成这个职位。这篇文章有点重数学，但我希望你能够带走一些信息。和往常一样，如果你喜欢这个材料，请给它一些掌声，如果你喜欢它，我建议跟随我。我会一周发布几次。下次见！</p></div></div>    
</body>
</html>