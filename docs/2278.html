<html>
<head>
<title>Building Deep Neural Network from Scratch using python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用python从头开始构建深度神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-deep-neural-network-from-scratch-using-python-b6256a60879?source=collection_archive---------3-----------------------#2021-04-16">https://medium.com/analytics-vidhya/building-deep-neural-network-from-scratch-using-python-b6256a60879?source=collection_archive---------3-----------------------#2021-04-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="58c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章是关于不使用Tensorflow、keras或Pytorch等库，从头开始构建深度神经网络。它由两部分组成。在第一部分中，我们将看到什么是深度神经网络，它如何从数据中学习，它背后的数学，在第二部分中，我们将谈论使用python从头构建一个。</p><blockquote class="jc jd je"><p id="1cf5" class="ie if jf ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">如果你熟悉神经网络的概念，可以跳过第一部分，直接跳到“建立一个识别手写数字的网络”一节。</p></blockquote><h1 id="3c6d" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">什么是深度神经网络？</h1><p id="9a8e" class="pw-post-body-paragraph ie if hh ig b ih kh ij ik il ki in io ip kj ir is it kk iv iw ix kl iz ja jb ha bi translated">在我们真正跳到什么是人工神经元和神经网络之前，让我们看看我们的生物神经网络是如何工作的。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es km"><img src="../Images/b30707fc4d03a7cd258cbd402b61df20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nyyppH7g58EQyuL8UXEkcw.jpeg"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">生物神经元。我<a class="ae lc" href="https://www.google.com/imgres?imgurl=https%3A%2F%2Fstatic.packt-cdn.com%2Fproducts%2F9781787121393%2Fgraphics%2FB06139_05_01.jpg&amp;imgrefurl=https%3A%2F%2Fsubscription.packtpub.com%2Fbook%2Fbig_data_and_business_intelligence%2F9781787121393%2F5%2Fch05lvl1sec38%2Fthe-biological-neuron&amp;tbnid=ldstlSPfn1kFyM&amp;vet=12ahUKEwjZnbjE3YXwAhVJ0FMKHbelCjgQMygCegUIARDJAQ..i&amp;docid=lk9j8uP5HqsJ7M&amp;w=1000&amp;h=547&amp;q=biological%20neuron%20image&amp;ved=2ahUKEwjZnbjE3YXwAhVJ0FMKHbelCjgQMygCegUIARDJAQ" rel="noopener ugc nofollow" target="_blank">图像来源</a></figcaption></figure><p id="e684" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">生物神经网络是相互连接神经元的网络。每个神经元都有一种名为<strong class="ig hi"> <em class="jf">的树突</em> </strong>从周围环境中收集信息。信息以电/化学信号的形式到达神经元。一旦一个神经元收到信号，它就会处理信号，如果信号达到一定的阈值，它就会通过连接到下一个神经元的<strong class="ig hi"> <em class="jf">轴突</em> </strong>发出输出信号。下一个神经元在接收到信号时做同样的事情，并且该过程继续。</p><p id="e338" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jf">人工神经网络</em> </strong>是隐约受到生物神经网络的启发。它是连接的人工神经元的集合。就像生物神经元一样，人工神经元也从一个神经元获得输入，进行一些计算，并将信号传输到与之相连的另一个神经元。</p><p id="4da9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jf">深度神经网络(DNN) </em> </strong>是一种输入输出层之间有多层的人工神经网络。一层中的每个神经元都连接到下一层中的所有神经元。输入层和输出层之间的一层或多层称为隐藏层。</p><p id="11fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将一层神经元连接到前一层神经元的每个连接都有一个称为权重<strong class="ig hi"> w </strong>的东西，它告诉我们当前神经元的激活对前一层神经元的激活有多敏感。给定层中的每个神经元都有一种叫做bias <strong class="ig hi"> b. </strong>如果你熟悉线性回归，那么bias项就像是<strong class="ig hi">T5】y = MX+cT7】中的拦截器“c”。如果sum(mx)没有越过阈值，但神经元需要触发，则将调整bias以降低该神经元的阈值，使其触发。</strong></p><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es ld"><img src="../Images/ac6febb77988954de8a23e9a12857450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YsZQmjh45lrS1EPDIm8h1Q.png"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">深度神经网络。<a class="ae lc" href="https://www.researchgate.net/publication/329216193/figure/fig3/AS:697582816870406@1543328112943/Architecture-of-multilayer-artificial-neural-network-with-error-backpropagation.png" rel="noopener ugc nofollow" target="_blank">图像来源</a></figcaption></figure><p id="3a6e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">整个网络看起来很复杂吧！但事实并非如此。把它想象成一个巨型函数<strong class="ig hi"> <em class="jf"> y = f(x) </em> </strong>，其中x是你的输入，y是输出。在函数<em class="jf"> f(x) </em>内部，它调用一系列函数，其中一个函数的输出传递给另一个函数。这些内部函数不过是隐藏层。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es le"><img src="../Images/501cfd251fc2515024c68ce9d7bb5e59.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*wg7WoHOGwJs63XYjuYjabA.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">单个人工神经元</figcaption></figure><p id="2c1d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们放大到单个人工神经元。人工神经元有两个部分。在第一部分中，它接受来自前一层的输入、相应的权重、偏差，然后对它们进行线性变换。线性变换只不过是加权输入和偏差的总和。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es lf"><img src="../Images/fb6c009f4314d91bd5456e8fff5e43be.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/1*Ojpxg09TwA21cZ9VtnC7Ag.gif"/></div></figure><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es lg"><img src="../Images/fc1052a8dfdacf1000a1f93918affaa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:252/1*lVafRrcMKPZ4D3QRI50fYg.gif"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">输入的线性变换</figcaption></figure><p id="48d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在第二部分中，它通过使用类似于<strong class="ig hi"> <em class="jf"> sigmoid </em> </strong> <em class="jf">的激活函数将该线性变换转换为非线性变换，并发出激活函数</em>的输出。还有类似<strong class="ig hi"> <em class="jf"> ReLu </em> </strong>的其他各种激活函数，但我们在本帖中使用的是sigmoid。因为这种线性和非线性变换以及多层的组合使得深度神经网络如此强大，以至于它可以拟合任何复杂的数据。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es lh"><img src="../Images/8ebcd12a06049aaaf1681d931ee25088.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/1*PUS5vwOZCFNNeoR8dUp7oQ.gif"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">激活功能</figcaption></figure><p id="497f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">sigmoid函数采用加权和，并在0和1之间转换值。它将<strong class="ig hi"> -infinity </strong>转换为0，将<strong class="ig hi"> +infinity </strong>转换为1。0到1之间的值表示特定神经元的激活强度。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es li"><img src="../Images/10557b9aeb24d9ce7bc17e69aa4953b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/1*TANPkXSjkEsV-LFZA8DJew.gif"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">Sigmoid函数</figcaption></figure><p id="f7fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">给定层的神经元的激活可以写成如下</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es lj"><img src="../Images/52432021d146a7b451e134d74a457859.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/1*O6RZf0qGL9NJxqXi-3Xmqg.gif"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">L层单个神经元的激活函数</figcaption></figure><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es lk"><img src="../Images/c20e3bcdc683120b39bfbb2fbc616104.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/1*hYD1UUD_sUXCUx0kNLuwgQ.gif"/></div></figure><p id="3dea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在一个典型的神经网络中，我们在一个给定的层中会有一个以上的神经元。上述等式可以用矩阵形式表示，以包括所有神经元。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es ll"><img src="../Images/4f0c6c0631b69d0a2171114608c9980a.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/1*kC1RiXRNoC4_HlUoDrIddg.gif"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">L层激活的矩阵形式</figcaption></figure><h1 id="6991" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">训练深度神经网络</h1><p id="1e1e" class="pw-post-body-paragraph ie if hh ig b ih kh ij ik il ki in io ip kj ir is it kk iv iw ix kl iz ja jb ha bi translated">深度神经网络将从给定的数据中自我学习，并用于预测未知的数据。<strong class="ig hi"> <em class="jf">但是我们所说的从数据中学习是什么意思呢？</em>T3】</strong></p><p id="4862" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们已经讨论过的，DNN在每一层都有一组权重和偏好。神经元的激活依赖于相应的权重和偏置。因此，从数据中学习意味着找出网络的最佳权重和偏差。<strong class="ig hi"> <em class="jf">但是我们如何找到权重和偏差呢？</em> </strong></p><p id="7603" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了找到权重和偏差，深度神经网络执行以下操作:</p><ol class=""><li id="8eee" class="lm ln hh ig b ih ii il im ip lo it lp ix lq jb lr ls lt lu bi translated">为权重和偏差分配一些随机值</li><li id="b8b4" class="lm ln hh ig b ih lv il lw ip lx it ly ix lz jb lr ls lt lu bi translated">使用这些随机分配的权重和偏差在网络上运行训练数据(具有输入和实际输出)。在此期间，一层中激活函数的输出将作为输入传递到下一层，直到我们从输出层获得输出。这个过程叫做<strong class="ig hi"> <em class="jf">正向传播</em> </strong>。</li><li id="3f16" class="lm ln hh ig b ih lv il lw ip lx it ly ix lz jb lr ls lt lu bi translated">网络的初始输出总是很糟糕，因为我们使用了随机权重和偏差。我们通过使用某种成本或误差函数来计算误差(网络预测和实际输出之间的差异)。在这篇文章中，我们将使用<strong class="ig hi"> <em class="jf">误差平方和</em> </strong>。</li></ol><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es ma"><img src="../Images/39c10dbaf907044ff213ecc127f7d204.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/1*mwgPUlFmyO3yg-UyVCrv6w.gif"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">误差平方和</figcaption></figure><p id="9939" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.由于网络中的所有神经元都对上述误差有贡献，误差比例(误差梯度)将从输出层传递回除输入层之外的所有层，以便可以调整权重和偏差。这个传播误差以调整权重和偏差的过程被称为<strong class="ig hi"> <em class="jf">反向传播。</em>T19】</strong></p><p id="2dac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于成本函数是权重和偏差的函数，因此将使用成本函数相对于权重和偏差的偏导数来计算误差梯度。</p><p id="7fea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了更好地理解，我们来看一个简单的网络，它有一个输入层、一个隐藏层和一个输出层。在前向传播的第一遍之后，我们将有错误。现在我们需要将误差比例传回给所有层中的所有神经元。</p><p id="d43a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，让我们计算输出层权重和偏差的微小变化的误差梯度。为了简单起见，让我们把激活函数写成函数的函数。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es mb"><img src="../Images/2118ce142ca0ff83c547c3a19ac1688d.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/1*dBuZgkgooUsEozE3Q1YZ-A.gif"/></div></figure><p id="de1f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">是时候刷新我们高中/大学的多元微积分了，求成本函数C关于权重和偏倚的偏导数。利用<strong class="ig hi"> <em class="jf">链式法则</em> </strong>，C w.r.t w和b的偏导数可以写成如下。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es mc"><img src="../Images/6e9eaebd2f5dec26e5492f3acadb5dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/1*Cihq50JVYY8UrCWqkW5ULA.gif"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">链式法则</figcaption></figure><p id="3bbb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上式中每个分量的偏导数为</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es md"><img src="../Images/f44631f5865ad436b20bcd449c2a1b2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/1*Xry3bbZiNjy87gYsX4HZyg.gif"/></div></figure><p id="148b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将上述偏导数的值代入链式法则方程，输出层的误差梯度w.r.t .权重和偏差为</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es me"><img src="../Images/c6b01f30530d0f24c8dfb8f6a9aa208d.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/1*Mdp0yr2_Cr4AL8jJiAzkqw.gif"/></div></figure><p id="eb7b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们计算隐藏层</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es mf"><img src="../Images/2a5c9524718121e4b4d7a4a31ce475bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/1*zVdGnkci_aj8CH7QmmZN-A.gif"/></div></figure><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es mg"><img src="../Images/a3af9da77b5af34ca8f9070a23516123.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/1*pQnxUto1ycbQDYsJneHoUw.gif"/></div></figure><blockquote class="jc jd je"><p id="3468" class="ie if jf ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">注意:虽然L和L-1分别代表输出层和隐藏层，为了更清楚起见，我使用了输出为o，隐藏层为h的子符号</p></blockquote><p id="4401" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">类似地，如果我们有多个隐藏层，我们可以计算所有隐藏层的误差梯度。由于我们只有一个隐藏层，反向传播到此为止。</p><p id="d762" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">5.上述前向和后向传播将迭代进行，权重和偏差将被调整，直到我们找到最佳值。我们将使用<strong class="ig hi"> <em class="jf">梯度下降</em> </strong>算法，而不是使用蛮力方法。</p><h2 id="f985" class="mh jk hh bd jl mi mj mk jp ml mm mn jt ip mo mp jx it mq mr kb ix ms mt kf mu bi translated">梯度下降</h2><p id="7e43" class="pw-post-body-paragraph ie if hh ig b ih kh ij ik il ki in io ip kj ir is it kk iv iw ix kl iz ja jb ha bi translated">梯度下降是一种迭代优化技术，可以找到<strong class="ig hi"> </strong>函数的<em class="jf">最小值</em>。当很难通过代数方法找到函数参数的最优值时，可以使用它。</p><p id="9a97" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jf">直觉:</em> </strong>想象一个人站在一个陡峭的山谷上。这个人想要到达谷底，但是他不知道哪个方向会把他带到谷底。他走一步，根据当前位置决定下一个位置。如果他朝底部走，他会继续朝那个方向走，否则他会改变方向。当山谷的坡度很陡时，他会迈较大的步子，当到达谷底时，他会迈较小的步子。最终在到达谷底时停止。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es mv"><img src="../Images/19bbb2de1cc883c66dc0b64ea2f14bbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*1jcEcUgNnzNRVnQMKMgMiw.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">梯度下降</figcaption></figure><p id="f141" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的目标是找到权重和偏差的最佳值，使得成本函数最小。梯度下降算法包括以下步骤</p><ol class=""><li id="0339" class="lm ln hh ig b ih ii il im ip lo it lp ix lq jb lr ls lt lu bi translated">为权重<strong class="ig hi"> <em class="jf"> w </em> </strong>和偏差<strong class="ig hi"> <em class="jf"> b </em> </strong> <em class="jf">分配随机值，并为学习率分配常量</em>值</li><li id="e5cf" class="lm ln hh ig b ih lv il lw ip lx it ly ix lz jb lr ls lt lu bi translated">通过使用梯度(我们使用偏导数计算)和学习率来更新权重和偏差。</li></ol><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es mw"><img src="../Images/f43a801bf906ea013efa3291419f2486.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/1*Z3tLEGoTeEZO3DjS7m8gyg.gif"/></div></figure><p id="31b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.重复步骤2，直到找到最小值或达到最大迭代次数</p><h2 id="99db" class="mh jk hh bd jl mi mj mk jp ml mm mn jt ip mo mp jx it mq mr kb ix ms mt kf mu bi translated">培训总结</h2><p id="70ef" class="pw-post-body-paragraph ie if hh ig b ih kh ij ik il ki in io ip kj ir is it kk iv iw ix kl iz ja jb ha bi translated">让我们通过为具有1个输入、1个隐藏和1个输出层的网络编写伪代码来总结整个训练过程</p><pre class="kn ko kp kq fd mx my mz na aw nb bi"><span id="4d26" class="mh jk hh my b fi nc nd l ne nf"><strong class="my hi">initialize_weights_and_biases()</strong>:<br/>  output_w = initialize_random_w<br/>  output_b = initialize_random_b<br/>  hidden_w = initialize_random_w<br/>  hidden_b = initialize_random_b</span><span id="732a" class="mh jk hh my b fi ng nd l ne nf"><strong class="my hi">train(x_train, y_train, no_of_iterations, learning_rate):</strong><br/>   <strong class="my hi"><em class="jf"># 1. initialize network weights and biases</em></strong><br/>   initialize_weights_and_biases()</span><span id="18b2" class="mh jk hh my b fi ng nd l ne nf">   for iteration in range(no_of_iterations): <strong class="my hi">#<em class="jf">Run gradient descent algorithm no_of_iterations times</em></strong></span><span id="1f81" class="mh jk hh my b fi ng nd l ne nf">       <strong class="my hi"><em class="jf">#initialize delta of weights and biases</em></strong><br/>       wo_delta = initialize_random_w_delta<br/>       bo_delta = initialize_random_b_delta<br/>       wh_delta = initialize_random_w_delta<br/>       wh_delta = initialize_random_b_delta</span><span id="a5c2" class="mh jk hh my b fi ng nd l ne nf">       for x, y in zip (x_train, x_train): <strong class="my hi"><em class="jf">#Iterate through each sample in the training data</em></strong><em class="jf"><br/>           </em><strong class="my hi"># 2.forward propagation</strong><em class="jf"><br/>           z_h = </em>hidden_w * x + hidden_b<br/>           a_h = sigmoid(z_h )</span><span id="ed45" class="mh jk hh my b fi ng nd l ne nf">           z_o = output_w * a+ output_b<br/>           predicted = sigmoid(z_o)</span><span id="0448" class="mh jk hh my b fi ng nd l ne nf">           <strong class="my hi"><em class="jf"># 3.find the error</em></strong><br/>           error = (predicted - y)</span><span id="6e28" class="mh jk hh my b fi ng nd l ne nf"><strong class="my hi"><em class="jf">           # 4.Back propagate the error</em></strong><br/>           delta = 2 error * sigmoid_prime(z_o)<br/>           wo_delta+= delta * a_h<br/>           bo_delta+= delta <br/>           wh_delta+= delta * output_w * sigmoid_prime(z_h) * x<br/>           bh_delta+= delta * output_w * sigmoid_prime(z_h)<br/>      <br/>       <strong class="my hi"><em class="jf"># 5. after 1 pass of all the inputs, update the network weights</em></strong><br/>       output_w = output_w - learning_rate * wo_delta<br/>       output_b = output_b - learning_rate * bo_delta<br/>       hidden_w = hidden_w - learning_rate * wh_delta<br/>       hidden_b = hidden_b - learning_rate * bh_delta</span></pre><h1 id="2a2d" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">预言；预测；预告</h1><p id="e30d" class="pw-post-body-paragraph ie if hh ig b ih kh ij ik il ki in io ip kj ir is it kk iv iw ix kl iz ja jb ha bi translated">在训练神经网络之后，我们将得到每一层的权重和偏差的最优值。预测只不过是对测试数据执行一遍前向传播。</p></div><div class="ab cl nh ni go nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ha hb hc hd he"><h1 id="9649" class="jj jk hh bd jl jm no jo jp jq np js jt ju nq jw jx jy nr ka kb kc ns ke kf kg bi translated">建立识别手写数字的网络</h1><p id="b2a8" class="pw-post-body-paragraph ie if hh ig b ih kh ij ik il ki in io ip kj ir is it kk iv iw ix kl iz ja jb ha bi translated">理论讲够了，让我们通过编写一个python程序来构建一个深度神经网络。我们将使用<strong class="ig hi"> mnist </strong>数据集，构建一个识别手写数字的网络，深度神经网络 的<strong class="ig hi"> <em class="jf"> hello world程序。</em></strong></p><p id="9031" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">mnist数据由大小为28 x 28像素的扫描手写图像组成。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es nt"><img src="../Images/52780cf9e27ea372d16780082c537cc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qdvquJr3Rge0HCkt5mTaNA.png"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">mnist数据。<a class="ae lc" href="http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png" rel="noopener ugc nofollow" target="_blank">图像来源</a></figcaption></figure><p id="4b90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将再次考虑建立一个具有<strong class="ig hi"> 1个输入层、1个隐藏层和1个输出层</strong>的网络。</p><p id="ea14" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面的程序是我们上面讨论的伪代码的python版本。唯一的区别是我们引入了batch，因为mnist数据有60000行数据。每次迭代都将全部60000行加载到内存中会耗尽内存。</p><figure class="kn ko kp kq fd kr"><div class="bz dy l di"><div class="nu nv l"/></div></figure><p id="25a0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jf"> __init__ </em> </strong>为输出层和隐藏层随机初始化权重和偏差。</p><p id="4314" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jf">正向传播</em> </strong>对给定的输入执行正向传播</p><p id="302f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"><em class="jf">update _ mini _ batch</em></strong>为给定批次中的每条记录运行向前和向后传播。我们做误差增量之和是因为我们使用误差平方和，偏导数是所有样本的误差梯度之和。</p><pre class="kn ko kp kq fd mx my mz na aw nb bi"><span id="2436" class="mh jk hh my b fi nc nd l ne nf">            o_del_b, h_del_b, o_del_w, h_del_w = self.backprop(x,y)<br/>            <br/>            o_b = o_b + o_del_b<br/>            h_b = h_b + h_del_b<br/>            o_w = o_w + o_del_w<br/>            h_w = h_w + h_del_w</span></pre><p id="1584" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每次批量运行后，它都会更新网络权重和偏差</p><pre class="kn ko kp kq fd mx my mz na aw nb bi"><span id="dafb" class="mh jk hh my b fi nc nd l ne nf"> self.o_weights = self.o_weights — (l_rate/len(batch))*o_w<br/> self.h_weights = self.h_weights — (l_rate/len(batch))*h_w<br/> self.o_biases = self.o_biases — (l_rate/len(batch))*o_b<br/> self.h_biases = self.h_biases — (l_rate/len(batch))*h_b</span></pre><p id="6e93" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jf">反向投影</em> </strong>将误差梯度传播回除输入层以外的所有层。它是神经网络的核心。正如我们前面讨论的，我们将计算误差函数相对于每一层的权重和偏差的偏导数。在代码中我们使用了<strong class="ig hi"> <em class="jf">。transpose() </em> </strong>方法，使其遵循矩阵乘法法则(<em class="jf"> A X B只有在A是mXn的矩阵，B是nXp的矩阵时才有可能。结果矩阵将是mXp </em>。</p><pre class="kn ko kp kq fd mx my mz na aw nb bi"><span id="57a5" class="mh jk hh my b fi nc nd l ne nf">        delta = (predicted - y) * sigmoid_prime(z_o)<br/>        <br/>        o_del_b = delta<br/>        o_del_w = np.dot(delta, a_h.transpose())<br/>        <br/>        delta = np.dot(self.o_weights.transpose(), delta) * sigmoid_prime(z_h)<br/>        <br/>        h_del_b = delta<br/>        h_del_w = np.dot(delta, x.transpose())</span></pre><p id="10d9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jf">契合</em> </strong>方法训练网络。它接受输入，随机打乱，把数据分成几批。为每个批处理调用update_mini_batch函数。它为每个时期执行这些步骤。</p><p id="196a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了读取mnist数据，我们将从<strong class="ig hi"><em class="jf">sk learn . datasets</em></strong>包中使用<strong class="ig hi"> <em class="jf"> fetch_openml </em> </strong>。我们将使用sklearn将数据拆分成训练和测试</p><figure class="kn ko kp kq fd kr"><div class="bz dy l di"><div class="nu nv l"/></div></figure><p id="fcb8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">mnist数据具有手写数字的数字化图像，因此它将具有从0到255的值。要归一化数据，请将输入除以255，使图像分布介于0和1之间</p><pre class="kn ko kp kq fd mx my mz na aw nb bi"><span id="ff21" class="mh jk hh my b fi nc nd l ne nf">X = (X/255).astype('float32')</span></pre><p id="313b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于每个图像都是28 x 28像素，并且深度神经网络期望输入为矢量格式，因此输入被转换为(784，1)形状，因为28 * 28 = 784。</p><pre class="kn ko kp kq fd mx my mz na aw nb bi"><span id="304c" class="mh jk hh my b fi nc nd l ne nf">X = [np.reshape(x, (784, 1)) for x in X]</span></pre><p id="9cc6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们要建立的网络在输出层有10个神经元，因为我们需要识别从0到9的数字。如果网络将给定的数字识别为3，那么意味着3的输出神经元将具有值1，而所有其他神经元将具有值0。</p><pre class="kn ko kp kq fd mx my mz na aw nb bi"><span id="d394" class="mh jk hh my b fi nc nd l ne nf">[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</span></pre><p id="a580" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于mnist数据集有数字形式的y值，我们需要对其进行矢量化，使其具有上述形式。</p><figure class="kn ko kp kq fd kr"><div class="bz dy l di"><div class="nu nv l"/></div></figure><p id="0df8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们通过指定每层神经元的数量来创建一个网络对象，并用训练数据来训练网络</p><figure class="kn ko kp kq fd kr"><div class="bz dy l di"><div class="nu nv l"/></div></figure><p id="af4b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里我们有一个网络，输入层有784个神经元，隐藏层有100个神经元(为什么是100个神经元？这是一个选择，我们可以使用任何数量的神经元，看看网络如何表现)和10个神经元的输出层</p><pre class="kn ko kp kq fd mx my mz na aw nb bi"><span id="8cfb" class="mh jk hh my b fi nc nd l ne nf">network.fit(train_data, 30, 10, 3.0)</span></pre><p id="0b37" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上面的语句将输入分成10批，以3的学习率运行30次迭代。迭代次数、批次和学习率是网络的超参数。我们需要做超参数调整，找出最佳组合。</p><h2 id="3bd4" class="mh jk hh bd jl mi mj mk jp ml mm mn jt ip mo mp jx it mq mr kb ix ms mt kf mu bi translated">准确(性)</h2><figure class="kn ko kp kq fd kr"><div class="bz dy l di"><div class="nu nv l"/></div></figure><p id="65fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了找到模型相对于测试数据的准确性，对每个数据执行正向传播，获得最大值。如果该值与y测试匹配，则表明模型预测正确。总测试数据的正确预测总和给出了准确度</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es nw"><img src="../Images/73cda09c707b65be14489291d64f0cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E-a57prnWpVX2Y3usbc_Hw.png"/></div></div></figure><p id="5613" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于我们搭建的网络，测试准确率为<strong class="ig hi"> <em class="jf"> 96.59 % </em> </strong>非常好。</p><p id="3f7d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">完整的程序可以在我的git存储库中获得</p><div class="nx ny ez fb nz oa"><a href="https://github.com/nagakedari/Machine-Learning/tree/main/Deep%20Learning" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab dw"><div class="oc ab od cl cj oe"><h2 class="bd hi fi z dy of ea eb og ed ef hg bi translated">nagakedari/机器学习</h2><div class="oh l"><h3 class="bd b fi z dy of ea eb og ed ef dx translated">在GitHub上创建一个帐户，为nagakedari/机器学习开发做贡献。</h3></div><div class="oi l"><p class="bd b fp z dy of ea eb og ed ef dx translated">github.com</p></div></div></div></a></div><blockquote class="jc jd je"><p id="9433" class="ie if jf ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated"><strong class="ig hi">注意:</strong>这篇文章的灵感来自于<a class="ae lc" href="http://michaelnielsen.org/" rel="noopener ugc nofollow" target="_blank">迈克尔·尼尔森</a>的《神经网络和深度学习</p></blockquote><p id="e48e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我希望你喜欢这篇文章。快乐学习！！</p></div></div>    
</body>
</html>