<html>
<head>
<title>Beginner’s Guide on Databricks: Spark Using Python &amp; PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Databricks初学者指南:使用Python和PySpark的Spark</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/beginners-guide-on-databricks-spark-using-python-pyspark-de74d92e4885?source=collection_archive---------0-----------------------#2021-04-16">https://medium.com/analytics-vidhya/beginners-guide-on-databricks-spark-using-python-pyspark-de74d92e4885?source=collection_archive---------0-----------------------#2021-04-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/dddf2911b0edb70d2f265e10781b160e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*TG3zu7NTLU2u4SD-ZQQRWA.png"/></div></figure><p id="d217" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这篇博客中，我们将回顾Apache Spark和Databricks的一般概念，它们之间的关系，以及如何使用这些工具来分析和模拟大数据。</p><h1 id="008e" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">什么是火花？</h1><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es ki"><img src="../Images/6998d60f401e582782b292a3112c57a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*_msZmLv99RuG8L8XquCjdw.png"/></div></figure><p id="43fa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae kn" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj"> Apache Spark </strong> </a>是一个开源的分布式通用集群计算框架。如果将所有数据放在RAM上没有意义，并且将所有数据放在本地机器上也没有意义，那么您应该使用Spark。从高层次来说，它是一个用于大数据处理的统一分析引擎，内置了用于流、SQL、机器学习和图形处理的模块。Spark是用于快速轻松处理大数据的最新技术之一，可以与Scala、Python和r等语言外壳进行交互。</p><h1 id="7d8b" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">什么是数据块？</h1><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ko"><img src="../Images/af64b962fbc18e986850de6deab20f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gaEusfetXumnAjevbN6yCA.png"/></div></div></figure><p id="09f2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae kn" href="https://databricks.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj"> Databricks </strong> </a>是一款行业领先的基于云的数据工程工具，用于处理、探索和转换大数据，并将数据用于机器学习模型。它是一个工具，提供了一种快速简单的方法来设置和使用集群来分析和模拟大数据。简而言之，这个平台将允许我们使用py Spark(Apache Spark和Python的合作)来处理大数据。我们将在这个博客中使用的版本将是社区版(完全免费使用)。事不宜迟…</p><h1 id="8c80" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">我们开始吧！</h1><ol class=""><li id="198e" class="kt ku hi io b ip kv it kw ix kx jb ky jf kz jj la lb lc ld bi translated">使用Databricks我们必须做的第一步是:创建一个帐户。您可以访问<a class="ae kn" href="https://databricks.com/try-databricks" rel="noopener ugc nofollow" target="_blank">https://databricks.com/try-databricks</a>，或者点击<a class="ae kn" href="https://databricks.com/try-databricks" rel="noopener ugc nofollow" target="_blank">此链接</a>创建一个帐户——如果您已经有了一个帐户，那么可以跳过这一步！</li></ol><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es le"><img src="../Images/f1c4f6fefb643f5dae48e4e178238428.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ScT9lG5u36pXHH6crjmzKQ.jpeg"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">创建帐户</figcaption></figure><p id="d158" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一旦您输入了您的信息，它将要求您选择您想要使用的Databricks版本以及电子邮件地址验证。我强烈推荐使用社区版，因为Databricks社区版是免费的。您无需为平台付费，也不会产生AWS成本。</p><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es lj"><img src="../Images/b3988dc221b4d3e4555babcc595499cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ALxcHqU1nPUFF3JlpiSsTQ.jpeg"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">免费试用与社区版选择</figcaption></figure><p id="784f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一旦我们成功创建了一个帐户并选择了平台的首选版本，下一步就是创建一个集群。</p><h1 id="29ea" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">创建集群</h1><p id="a3e9" class="pw-post-body-paragraph im in hi io b ip kv ir is it kw iv iw ix lk iz ja jb ll jd je jf lm jh ji jj hb bi translated">Databricks集群是一组计算资源和配置，您可以在其上运行数据工程、数据科学和数据分析工作负载，如生产ETL管道、流分析、即席分析和机器学习。</p><p id="d0d9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">要创建我们的第一个集群，请单击“新建集群”按钮:</p><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ln"><img src="../Images/a81c91185379f4aa7470d25350c98aa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VnZciULaSrVRSc16KjK-Ag.jpeg"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">常见任务列表下的“新集群”选项</figcaption></figure><p id="4cf1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这将把我们带到一个新的页面，在这里我们定义新的集群。您可以随意命名这个集群，我将把这个新集群命名为“myfirstcluster”。我将保留其余选项，并单击“create cluster”按钮:</p><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es lo"><img src="../Images/b11956347bb426365de77e779553bd0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*np8mO3ksw9c2c1i3DbIERw.jpeg"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">定义和创建我们的集群</figcaption></figure><p id="eda2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">请注意，创建集群可能需要几秒钟的时间，因此请耐心等待。如果集群实例化失败，您可以尝试在下面的选项中更改可用性区域。如果您使用的是Community Edition，群集将在120分钟不活动后终止，并且一旦终止，您将无法重新启动群集。避免这个问题的一个方法是每次创建一个终止集群的克隆，或者您也可以创建一个新的集群。在我看来，为Databricks的免费版付出的代价很小。</p><h1 id="3050" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">创建新笔记本</h1><p id="1515" class="pw-post-body-paragraph im in hi io b ip kv ir is it kw iv iw ix lk iz ja jb ll jd je jf lm jh ji jj hb bi translated">一旦我们的集群启动并运行，我们现在就可以创建一个新的笔记本了！只需点击左上方的数据块图标，然后点击“常见任务”列表下方的“新笔记本”:</p><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/a9708d381c9641c89161029b260f2c3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*Mr1G1Dy4c5kQmS2T4BXgbA.jpeg"/></div></figure><p id="a0a1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">要实例化笔记本，我们需要做的就是给它起一个名字(我给我的名字是“myfirstnotebook”)，选择语言(我选择Python)，然后选择我们创建的活动集群。现在，我们需要做的就是点击“创建”按钮:</p><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/3ba86d23f5970fd53757c5a9a0341981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*D0aI66kmDMasyvfeQE-GGg.jpeg"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">创建新笔记本</figcaption></figure><h1 id="e627" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">选择样本数据集</h1><p id="2bab" class="pw-post-body-paragraph im in hi io b ip kv ir is it kw iv iw ix lk iz ja jb ll jd je jf lm jh ji jj hb bi translated">现在，我们的笔记本已经创建并成功连接到我们的集群，我们终于可以开始玩了！在这个笔记本中，我们要做的第一件事是导入必要的库。所以让我们从导入PySpark开始:</p><pre class="kj kk kl km fd lr ls lt lu aw lv bi"><span id="34f0" class="lw jl hi ls b fi lx ly l lz ma">import pyspark<br/>from pyspark.sql.functions import col<br/>from pyspark.sql.types import IntegerType, FloatType</span></pre><p id="2dc1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于本笔记本，我们不会将任何数据集上传到笔记本中。相反，我们将选择一个Databricks提供给我们的样本数据集进行研究。我们可以通过键入以下内容来查看不同的样本数据集:</p><pre class="kj kk kl km fd lr ls lt lu aw lv bi"><span id="fa87" class="lw jl hi ls b fi lx ly l lz ma"># A list of folders containing sample datasets we can use<br/>display(dbutils.fs.ls(“/databricks-datasets/samples/")</span></pre><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es mb"><img src="../Images/8685c8c36e5b1f7a7498967b10263fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bjFiKlGkuAyKr3yf4jJD1Q.jpeg"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">数据块提供了一个样本数据集列表</figcaption></figure><h1 id="f134" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">探索数据</h1><p id="b6db" class="pw-post-body-paragraph im in hi io b ip kv ir is it kw iv iw ix lk iz ja jb ll jd je jf lm jh ji jj hb bi translated">在这篇博客中，我们将在“人口与价格/”样本数据集中做一些基本的探索。因此，让我们继续定义一个名为“df”的变量，它将引用我们笔记本中的数据帧。</p><pre class="kj kk kl km fd lr ls lt lu aw lv bi"><span id="c5fa" class="lw jl hi ls b fi lx ly l lz ma"># Loading in a sample table into the dataframe<br/>df = spark.read.csv(“/databricks-datasets/samples/population-vs-price/data_geo.csv”, header=True)</span></pre><p id="0d41" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将调用df.show()来查看dataframe，而不是调用df.head()来查看前5行。默认情况下。show()方法显示数据帧的前20行。</p><pre class="kj kk kl km fd lr ls lt lu aw lv bi"><span id="b670" class="lw jl hi ls b fi lx ly l lz ma"># To view the first 20 rows of the df<br/>df.show()</span><span id="2ff0" class="lw jl hi ls b fi mc ly l lz ma"># OR we can add an integer into the parentheses to view a specific <br/># number of rows<br/>df.show(5)</span></pre><p id="74f9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">要查看数据帧中的列名，我们可以调用“<em class="md">df . columns</em>”—这将返回数据帧中的列名列表:</p><pre class="kj kk kl km fd lr ls lt lu aw lv bi"><span id="aafc" class="lw jl hi ls b fi lx ly l lz ma"># Viewing the column names<br/>df.columns</span></pre><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es me"><img src="../Images/b2464fb520e04e1e73f04705c6adf816.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*A7IJ1Hnt3ZoR1Q3_cKpm6Q.jpeg"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">列名的列表</figcaption></figure><p id="0671" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">请注意，许多列名包含空格；如果我们想要实现SQL来从这个数据帧创建查询，这对于我们来说并不理想。要更改列名，我们可以实现<em class="md">。withColumnRenamed() </em>"方法:</p><pre class="kj kk kl km fd lr ls lt lu aw lv bi"><span id="54d2" class="lw jl hi ls b fi lx ly l lz ma">df.withColumnRenamed(‘2014 rank’, ‘2014_rank’)</span></pre><p id="4533" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">请注意，我们必须创建一个新变量(df2)来在新的数据帧中保存这些更改。如果我们只是简单地“<em class="md"> df.withColumnRenamed… </em>”,(就像我们上面做的那样)这将只是一个暂时的改变——没有“<em class="md"> inplace=True </em>”参数。我们还可以为我们想要更改的每个列名一次链接所有这些:</p><pre class="kj kk kl km fd lr ls lt lu aw lv bi"><span id="c01c" class="lw jl hi ls b fi lx ly l lz ma">df2 = df.withColumnRenamed(‘2014 rank’, ‘2014_rank’)\<br/>.withColumnRenamed(‘State Code’, ‘state_code’)\<br/>.withColumnRenamed(‘2014 Population estimate’, ‘2014_pop_estimate’)\<br/>.withColumnRenamed(‘2015 median sales price’, ‘2015_median_sales_price’)</span></pre><p id="e38e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">太好了！如果我们想在df2中查看选定的列，我们可以说:</p><pre class="kj kk kl km fd lr ls lt lu aw lv bi"><span id="ff7b" class="lw jl hi ls b fi lx ly l lz ma">df2.select([‘2014_rank’, ‘2014_pop_estimate’]).show()</span></pre><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/7b9354615de295a24b94b8c1454e4dd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*50HJDcXUQMxnLIZYI97-5w.jpeg"/></div></figure><p id="896a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这将只显示所选列的前20行的值。现在让我们来看看每一列中值的类型。我们可以这样做的一个方法是使用“T4”。printSchema() "在我们的df2变量上。</p><pre class="kj kk kl km fd lr ls lt lu aw lv bi"><span id="bf5f" class="lw jl hi ls b fi lx ly l lz ma"># Printing out the schema of the dataframe<br/>df2.printSchema()</span></pre><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/85bc1815363e5b3d58db3c6f850674ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*P3IFZqfr8LSocnDC-m5Yvw.jpeg"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">查看df2的模式</figcaption></figure><p id="53be" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">哦，不，我们注意到我们所有的列都包含字符串值——甚至是应该包含数值的列！我们可以手动调整列中值的类型，这有点类似于我们处理调整列名的方式:使用"<em class="md">。withColumn() </em>方法并在“<em class="md">上链接。</em>“法投()”。在我们一次在多个列上开始之前，让我们分解一个例子:</p><pre class="kj kk kl km fd lr ls lt lu aw lv bi"><span id="0b80" class="lw jl hi ls b fi lx ly l lz ma">df2.withColumn(“2014_rank”, col(“2014_rank”).cast(IntegerType()))</span></pre><p id="b9b7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">在上面的例子中，我们说:</strong></p><ul class=""><li id="e37e" class="kt ku hi io b ip iq it iu ix mh jb mi jf mj jj mk lb lc ld bi translated">使用此选定列:“2014_rank”</li><li id="9d5f" class="kt ku hi io b ip ml it mm ix mn jb mo jf mp jj mk lb lc ld bi translated">创建一个名为“2014_rank”的新列(替换旧列)</li><li id="4a79" class="kt ku hi io b ip ml it mm ix mn jb mo jf mp jj mk lb lc ld bi translated">这个新列将包含旧列的值，但是我们将把它们重新转换为整数类型</li></ul><p id="4db7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">就像我们以前使用的方法一样。withColumnRenamed”，除非我们创建一个新变量来保存更改后的数据帧，否则更改只是暂时的。就像前面的方法一样，我们可以一次链接多个列(在下面的代码块中看起来有点乱):</p><pre class="kj kk kl km fd lr ls lt lu aw lv bi"><span id="9298" class="lw jl hi ls b fi lx ly l lz ma">df3 = df2.withColumn(“2014_rank”,col(“2014_rank”).cast(IntegerType()))\<br/> .withColumn(“2014_pop_estimate”, col(“2014_pop_estimate”).cast(IntegerType()))\<br/> .withColumn(‘2015_median_sales_price’, col(‘2015_median_sales_price’).cast(FloatType()))</span></pre><h1 id="f451" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">使用SQL语法</h1><p id="d7ac" class="pw-post-body-paragraph im in hi io b ip kv ir is it kw iv iw ix lk iz ja jb ll jd je jf lm jh ji jj hb bi translated">在我们结束本教程之前，让我们最后在我们的数据帧上运行一些SQL查询！为了让SQL正常工作，我们需要确保df3有一个表名。为此，我们简单地说:</p><pre class="kj kk kl km fd lr ls lt lu aw lv bi"><span id="9f83" class="lw jl hi ls b fi lx ly l lz ma"># Giving our df3 the table name 'pop_price'<br/>df3.createOrReplaceTempView(‘pop_price’)</span></pre><p id="0324" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在我们终于可以运行SQL查询了！在PySpark中运行SQL查询非常简单。让我们运行一个基本查询来看看它是如何工作的:</p><pre class="kj kk kl km fd lr ls lt lu aw lv bi"><span id="064c" class="lw jl hi ls b fi lx ly l lz ma"># Viewing the top 10 cities based on the '2014_rank' column<br/>top_10_results = spark.sql("""SELECT * FROM pop_price <br/>                              WHERE 2014_rank &lt;= 10<br/>                              SORT BY 2014_rank ASC""")</span><span id="936f" class="lw jl hi ls b fi mc ly l lz ma">top_10_results.show()</span></pre><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es mq"><img src="../Images/2aa764bd4137a925e89ad7acb6b1555d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pjA5ECAJq9O8JnNDlB5OCg.jpeg"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">2014年十大排名</figcaption></figure><p id="e983" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当我们使用"<em class="md"> spark.sql() </em>"从数据帧中查询时，它会在查询条件内返回一个新的数据帧。我们只需保存查询的结果，然后使用“<em class="md">查看这些结果。</em>显示()"方法。如果你想看我写这篇博客用的笔记本，你可以点击下面的链接(从这篇文章发表之日起6个月内有效):</p><div class="mr ms ez fb mt mu"><a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2970399034525015/114269276412404/3853367662178032/latest.html" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab dw"><div class="mw ab mx cl cj my"><h2 class="bd hj fi z dy mz ea eb na ed ef hh bi translated">myfirstnotebook -数据块</h2><div class="nb l"><p class="bd b fp z dy mz ea eb na ed ef dx translated">databricks-prod-cloudfront.cloud.databricks.com</p></div></div></div></a></div></div><div class="ab cl nc nd gp ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="hb hc hd he hf"><p id="daa3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">感谢您的阅读——如果您对设置有任何疑问，请随时联系我！</p></div></div>    
</body>
</html>