<html>
<head>
<title>Long Short-Term Memory Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">长短期记忆网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/long-short-term-memory-networks-23119598b66b?source=collection_archive---------3-----------------------#2021-11-30">https://medium.com/analytics-vidhya/long-short-term-memory-networks-23119598b66b?source=collection_archive---------3-----------------------#2021-11-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/118d4bd19230c0c1e713066d49a72cef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KVJQPkO4sFk3lKOK.jpg"/></div></div></figure><div class=""/><h1 id="1a4d" class="ip iq hs bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">介绍</h1><p id="68db" class="pw-post-body-paragraph jn jo hs jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">神经网络旨在模仿人类大脑的行为，理解数据中的各种关系。这些网络有能力理解复杂的非线性关系，可以帮助我们做出更明智的决定。神经网络用于图像处理、自然语言处理等领域。，并能优于传统的机器学习算法。但是传统神经网络的一个基本缺点是它不能记忆东西。</p><p id="8380" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">比方说，我们正在玩一个游戏，我们需要一个模型来预测玩家的下一步行动。这在很大程度上取决于前面的移动，传统的神经网络在这里不会表现得很好。在这种情况下，我们需要一些模型，可以记住以前的事件，并相应地做出明智的决定。这就是递归神经网络(RNN)出现的原因。rnn可以存储先前的状态信息。然后它可以用它来预测下一个事件，在这种情况下，就是玩家的下一步行动。</p><figure class="kr ks kt ku fd hj er es paragraph-image"><div class="er es kq"><img src="../Images/dccb15df42e5b612ea3135a4662a0cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*9Sn1Y4FGrxjVh8ETlmB7rQ.png"/></div></figure><p id="742b" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在上图的左侧，您可以看到有一个输入Xt和输出ht。循环表示这个网络重复了t次，如右图所示。神经网络A在时间t = 0获取输入X，并产生输出h0。现在，来自A在t=0和输入X1的信息被传递到下一个时间戳t=1，以生成输出h1。似乎是记忆问题的完整解决方案。</p><p id="c129" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">RNNs已经被用于许多序列建模任务，如图像字幕、机器翻译、语音识别等。</p><h1 id="3e04" class="ip iq hs bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">RNN的缺点</h1><p id="2cb6" class="pw-post-body-paragraph jn jo hs jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">正如我们所见，rnn越来越受欢迎，并被用于大多数序列相关的任务。但是这种模式也有一些缺点。</p><h2 id="87f2" class="kv iq hs bd ir kw kx ky iv kz la lb iz jy lc ld jd kc le lf jh kg lg lh jl li bi translated">消失渐变</h2><p id="a6a3" class="pw-post-body-paragraph jn jo hs jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">通过时间的反向传播来训练rnn(BPTT)。像反向传播一样，神经网络的权重也通过梯度得到更新。但是在RNN，我们可以穿越时间和层面。如果序列非常大(许多时间步长)，并且如果神经网络有一个以上的隐藏层，那么当我们反向传播时，梯度很可能会变得越来越小。这最终会导致渐变消失的问题。一旦出现这个问题，权重会更新的非常慢，阻止网络学习。消失梯度问题也可能发生在深度神经网络中。但这种效应在RNN非常普遍，甚至更糟，因为这里我们也需要穿越时间。</p><h2 id="5772" class="kv iq hs bd ir kw kx ky iv kz la lb iz jy lc ld jd kc le lf jh kg lg lh jl li bi translated">处理长期依赖关系</h2><p id="7315" class="pw-post-body-paragraph jn jo hs jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">RNNs可以利用先前的状态来预测下一个事件。这在NLP中非常有用，在NLP中，特定单词的预测依赖于上下文或先前的单词。考虑一下“苹果是一种水果”这句话。现在假设我们通过RNN网络预测每个单词。为了预测单词“水果”，我们需要上下文单词“苹果”。因为上下文单词就在事件之前。所以，在这种情况下，RNNs会表现的很好。</p><p id="2ae2" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">但是考虑一下“我在法国长大，我说一口流利的法语”这句话。这里预测“法国”这个词，上下文词是“法国”。这些被称为长期依赖，在语言模型中非常常见。发现rnn处理长期依赖性很差。RNNs的性能会随着事件和上下文之间的差距的增加而下降。</p><h1 id="c3ca" class="ip iq hs bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">LSTM简介</h1><p id="e81b" class="pw-post-body-paragraph jn jo hs jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">长短期记忆网络是专门为克服RNN的缺点而设计的。与RNN相比，LSTMs可以将信息保存更长时间。LSTMs还可以解决消失梯度问题。让我们看看LSTM如何实现这些目标。</p><p id="8b10" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">当我们考虑RNN时，它有一个神经网络层或一个重复t时间步的模块。在这个模块中，RNN有一个带有简单激活层的神经网络。激活层可以是tanh或sigmoid或任何其他函数。从下图中可以更好地理解这一点。</p><figure class="kr ks kt ku fd hj er es paragraph-image"><div class="er es lj"><img src="../Images/71ffa58e0ddc7f98f9e84311357e47c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*FS6HOHBOVcvbsYDNBw7Ygw.png"/></div></figure><p id="3bef" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">然而，在LSTMS中，架构并不那么简单。考虑下图所示的LSTM网络架构。</p><figure class="kr ks kt ku fd hj er es paragraph-image"><div class="er es lk"><img src="../Images/4ec900e59d49d836ce6f86a9768fbfb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*DiChAYg-DGp5z8FPwEflgw.png"/></div></figure><p id="e9cd" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">不要过度紧张。在下节课中，我们将尝试逐一理解这些单元。</p><p id="688b" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">现在让我们回到预测博弈下一步的第一个例子。下一步可以被认为是我们需要预测的事件。显然，这一事件有各种长期和短期的相关性。你可以将这些依赖视为上下文，或者在这种情况下，我们从以前的关卡收集的工具和配件，或者游戏中发生的以前的事件等。</p><p id="b894" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">假设我们有一个正在发生的事件，我们需要预测游戏中的下一个事件。我们还有在比赛中收集的长期和短期信息。长期记忆是从很久以前收集的记忆，而短期记忆是从几个时间戳以前收集的信息。显然，为了预测下一个事件，我们需要当前事件。但是只有一些长期和短期的记忆会有用。我们怎么得到这个？</p><p id="8936" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在LSTMs中，我们将使用这个长时记忆和短时记忆，以及当前事件，来生成一个新的经过修改的长时记忆。这样做的时候，我们只会记住那些有用的东西，我们会丢弃所有不相关的信息。同样，我们会通过使用一些信息和丢弃其他信息来更新短时记忆。简而言之，在每个时间步，我们将过滤需要传递到下一个时间步的内存。这个修改的信息用于预测下一个事件。一旦你经历了下一个阶段，这将会变得更加清晰。</p><h1 id="944e" class="ip iq hs bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">LSTM建筑</h1><p id="40ce" class="pw-post-body-paragraph jn jo hs jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">现在，让我们深入LSTM模型架构，并尝试理解它将如何处理长期和短期的依赖关系。考虑下图。</p><figure class="kr ks kt ku fd hj er es paragraph-image"><div class="er es ll"><img src="../Images/b2d81e54b66c29ca44759b869fee3e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*X04h1M-LfgyRmVCGSlY9-w.png"/></div></figure><p id="6e26" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">您可以看到一个LSTM单元有两个输出。Ct和ht。如果你记得在RNN我们只有一个输出，那就是ht。隐藏状态ht是从之前的步骤中获得的短期记忆。现在，LSTM的额外产出是多少？向量Ct被称为单元状态。</p><figure class="kr ks kt ku fd hj er es paragraph-image"><div class="er es lm"><img src="../Images/968cbfe693e22d7fe2dc6657be6118de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*QJORP5vcNxiIuKjU9cfvKA.png"/></div></figure><p id="55a5" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">现在LSTM中的细胞状态负责储存长期记忆事件。LSTMs将利用一种称为gates的机制在该单元状态中添加和删除某些信息。让我们试着详细理解这一点。</p><h2 id="c84b" class="kv iq hs bd ir kw kx ky iv kz la lb iz jy lc ld jd kc le lf jh kg lg lh jl li bi translated">忘记大门</h2><p id="8d4c" class="pw-post-body-paragraph jn jo hs jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">正如我们前面讨论的，我们可以使用gates向单元状态添加和移除信息。下图只不过是“遗忘之门”,它只过滤需要的信息，去掉其余的。这是如何实现的？</p><figure class="kr ks kt ku fd hj er es paragraph-image"><div class="er es ln"><img src="../Images/c27b4d4b0647c535517a0478410b21aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*F2rOuTe2i8QngMk-nHzq7A.png"/></div></figure><p id="d5a1" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在该图中，来自前一时间步的细胞状态或长期记忆乘以函数ft以获得新的过滤记忆，其中ft是遗忘因子。遗忘因子的计算公式如下所示。</p><figure class="kr ks kt ku fd hj er es paragraph-image"><div class="er es lm"><img src="../Images/34bb18cc6df0c4bc6bdff8fe929f4769.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*nh0Ln2QHrhObRYJxIViZfg.png"/></div></figure><p id="d7c8" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">来自先前时间戳和当前事件的短期记忆或ht-1用于计算遗忘因子。短期记忆和当前事件连接在一起，并在该向量的顶部应用一个sigmoid层。sigmoid函数将产生范围从0到1的输出，该输出将与先前单元状态中的每个值相乘。值0表示信息将被完全丢弃，值1表示信息保持原样。</p><h2 id="4b03" class="kv iq hs bd ir kw kx ky iv kz la lb iz jy lc ld jd kc le lf jh kg lg lh jl li bi translated">学习门</h2><p id="4ed1" class="pw-post-body-paragraph jn jo hs jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">现在我们知道了丢弃什么信息，我们的下一个目标是找到我们需要添加什么新信息。这是通过学习门完成的。现在学习门有两个部分</p><figure class="kr ks kt ku fd hj er es paragraph-image"><div class="er es lm"><img src="../Images/938f104e3511503ee36e8ab56e85be1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*9qewnlchAPVO3dNjmdBnbQ.png"/></div></figure><p id="afb2" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">1.之前的短时记忆和当前事件串联起来，然后经过一个tan h层。这将产生新的值c~t，这是新的信息。这里我们也不需要全新的信息。我们如何忽略这其中的一部分？</p><p id="a097" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">2.每一个新的候选人得到多少更新是由另一个遗忘门决定的。来自遗忘门的输出将与我们的新信息相乘，并生成最终输出</p><p id="f513" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">现在，为什么我们在一些地方使用tanh激活函数？Tanh激活函数将输出以0为中心的向量(在-1到1的范围内)。这将很好地分布梯度，并允许单元状态运行更长时间。这将最终解决消失或爆炸梯度的问题。</p><h2 id="f08b" class="kv iq hs bd ir kw kx ky iv kz la lb iz jy lc ld jd kc le lf jh kg lg lh jl li bi translated"><strong class="ak">记门</strong></h2><p id="800a" class="pw-post-body-paragraph jn jo hs jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">既然我们知道了什么该保留，什么该丢弃，那么我们就该更新新的细胞状态或长期记忆了。我们如何做到这一点？我们只是从遗忘门和学习门获取输出，然后将它们相加。</p><figure class="kr ks kt ku fd hj er es paragraph-image"><div class="er es lo"><img src="../Images/4dcaea1b24117d7b10b4f93c1baf5a01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*MHXmpIiC54IEd-V2oymrbQ.png"/></div></figure><p id="0d28" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我们快到了。现在剩下的唯一一步是计算输出。</p><h2 id="00f7" class="kv iq hs bd ir kw kx ky iv kz la lb iz jy lc ld jd kc le lf jh kg lg lh jl li bi translated">输出门</h2><p id="9b5d" class="pw-post-body-paragraph jn jo hs jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">输出是单元状态的过滤版本。来自记忆门的单元状态值乘以双曲正切激活函数。tanh的输出将介于-1和1之间。然后，我们再次将输出乘以sigmoid函数，以忘记一些值。</p><figure class="kr ks kt ku fd hj er es paragraph-image"><div class="er es lm"><img src="../Images/b6a8508dd583f48e07def2e2aeaf7d96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*msP1DMTZIVjkiWJeqjQwWA.png"/></div></figure><p id="32be" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">就是这样！我们已经完成了架构。</p><h1 id="2713" class="ip iq hs bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">LSTM建筑的其他变体</h1><p id="57d3" class="pw-post-body-paragraph jn jo hs jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">除了上述架构之外，还有一些其他网络也可以很好地处理序列数据。其中一些是</p><p id="9bc9" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><a class="ae lp" href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" rel="noopener ugc nofollow" target="_blank">门控循环单元(GRU) </a></p><p id="ea68" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><a class="ae lp" href="https://www.tensorflow.org/api_docs/python/tf/keras/experimental/PeepholeLSTMCell" rel="noopener ugc nofollow" target="_blank">窥视孔LSTMs </a></p><p id="af8c" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><a class="ae lp" href="https://arxiv.org/abs/1508.03790" rel="noopener ugc nofollow" target="_blank">深度门控RNNs </a></p><p id="12fb" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><a class="ae lp" href="https://arxiv.org/abs/1402.3511" rel="noopener ugc nofollow" target="_blank">发条RNNs </a></p><p id="04a5" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我不会深入这些模型的细节。但是你可以在上面的链接中读到它们。</p><h1 id="ff65" class="ip iq hs bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">最后的想法</h1><p id="1146" class="pw-post-body-paragraph jn jo hs jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">LSTM是自然语言处理和序列模型领域的一个伟大的里程碑。但是像所有其他模型一样，LSTMs也不是完美的。训练次数较长，内存需求大，无法并行训练等。是LSTMs的一些缺点。新的改进模型和技术被开发出来，一个流行的方法是注意力。让我们希望越来越多有趣的工作将围绕RNNs和序列数据出现。</p><p id="3f4b" class="pw-post-body-paragraph jn jo hs jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我希望你喜欢这个博客。如有任何疑问或澄清，请随时通过我的<a class="ae lp" href="https://www.linkedin.com/in/vinitha-v-n-5a0560179/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系</p></div></div>    
</body>
</html>