<html>
<head>
<title>Buy Me That Look</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">给我买那个表情</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/buy-me-that-look-137c1e7030ec?source=collection_archive---------4-----------------------#2021-05-11">https://medium.com/analytics-vidhya/buy-me-that-look-137c1e7030ec?source=collection_archive---------4-----------------------#2021-05-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/49b567e18b2826be09eb3c78efe73ee8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*vv6yq_eSaIGkucIy-v3TIw.jpeg"/></div></figure><p id="303e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个博客是关于时尚推荐系统的。与其他系统相比，这是一个独特的推荐系统，因为这里基于提供的照片/图片，系统推荐照片中模特穿的类似衣服或物品。架构和设计组件的灵感来自一篇论文:<a class="ae jk" href="https://arxiv.org/pdf/2008.11638.pdf" rel="noopener ugc nofollow" target="_blank">给我买那个样子:推荐类似时尚产品的方法</a>。</p><h1 id="d160" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">议程</h1><ol class=""><li id="4c22" class="kj kk hi io b ip kl it km ix kn jb ko jf kp jj kq kr ks kt bi translated">商业问题</li><li id="f00e" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">ML/DL公式</li><li id="36d1" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">业务限制</li><li id="6de6" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">数据采集和分析</li><li id="3e52" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">研究科</li><li id="0cf1" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">我的方法</li><li id="0f4a" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">最终结果</li><li id="2227" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">未来的工作</li><li id="5395" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">参考</li></ol><h1 id="3324" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">1.业务问题:</h1><p id="be0e" class="pw-post-body-paragraph im in hi io b ip kl ir is it km iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">简而言之，该研究论文指出，检测图像中的所有产品，并通过产品购买链接从数据库中检索相似的时尚服装。</p><p id="7d2c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在线商务已经成为每个人日常生活中的重要部分。虚拟商店允许人们在家里舒适地购物，没有售货员的压力。在本文中，作者关注于同时检索多个时尚商品，并提出了一种从图像中检测所有商品并推荐相似种类商品的体系结构。</p><p id="ec45" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在商店里，我们可以拿一块布，要求售货员给我们看颜色、图案、厚度等类似的产品；但是在网上搜索同类产品是不可能的，而且很费时间。所以，我们可以上传一张图片，用计算机视觉搜索相似的种类。</p><h1 id="b05c" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">2.ML/DL公式</h1><p id="e424" class="pw-post-body-paragraph im in hi io b ip kl ir is it km iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">让我们在本次会议中讨论模型的架构。将这个问题分成不同的阶段:</p><blockquote class="lc ld le"><p id="e8cf" class="im in lf io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated"><strong class="io hj"> <em class="hi">阶段1:(姿态估计)</em> </strong></p><p id="4af6" class="im in lf io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated"><em class="hi">在这个阶段，我们将检测图像是否是全正面姿态图像。所以这将是一个二元分类器(是/否)</em></p><p id="4ca9" class="im in lf io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated"><strong class="io hj"> <em class="hi">第二阶段:(本地化)</em> </strong></p><p id="560e" class="im in lf io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated"><em class="hi">在这个阶段，我们检测所有的物品(衣服)以及物品被放置或定位的特定位置。这将是一个分类和回归问题。由于物品检测的分类和由于定位的回归(边界框坐标)</em></p><p id="8b83" class="im in lf io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated"><strong class="io hj"> <em class="hi">第三阶段:(图像_嵌入)</em> </strong></p><p id="9006" class="im in lf io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated"><em class="hi">在这个阶段，我们将为图像生成嵌入(密集向量),如下所述。</em></p><p id="3183" class="im in lf io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated"><strong class="io hj"> <em class="hi">第四阶段:(获取相似图像)</em> </strong></p><p id="d158" class="im in lf io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated"><em class="hi">在此阶段，我们将使用Faiss库根据搜索查询获取相似的服装。</em></p></blockquote><h1 id="5649" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">3.业务限制</h1><ul class=""><li id="f5c2" class="kj kk hi io b ip kl it km ix kn jb ko jf kp jj lj kr ks kt bi translated">我们的系统架构应该是可伸缩的，因为每天都有成千上万的新图片被添加到网站上。</li><li id="db6e" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj lj kr ks kt bi translated"><strong class="io hj">低延迟:</strong>客户不会等待几分钟甚至超过5-10秒来获得建议。所以我们的架构应该能够在给定的时间框架内检索推荐。</li><li id="48c8" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj lj kr ks kt bi translated">因为它是一个离线推荐系统，所以必须记住实时限制。从客户的角度来看，可解释性是很重要的，它有助于说明提出建议的原因。当向用户推荐产品时，系统会添加一个链接到用户观看的产品，并触发推荐。</li></ul><h1 id="cb89" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">4.数据采集和分析</h1><p id="25dc" class="pw-post-body-paragraph im in hi io b ip kl ir is it km iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">数据来自Myntra。为此我用了硒。这个数据没有标注。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/ea9952842e6218c22c31a703e1b76629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*8-qExsut5GacB42MHVtCcA.png"/></div></figure><p id="a69b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">该数据包含不同类型的女士服装(上装、下装和鞋类)。由于这些数据来自Myntra，所以我们有用于物品/衣服定位/检测的遮罩或边界框。所以，对于文章检测和定位部分，我是从ka ggle competition:<a class="ae jk" href="https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6/data" rel="noopener ugc nofollow" target="_blank">imate rialist(Fashion)2019在FGVC6 </a>取的数据。该时尚数据大约有45.2k的文件，以带有class_labels的<a class="ae jk" href="https://www.kaggle.com/c/data-science-bowl-2018/discussion/47699" rel="noopener ugc nofollow" target="_blank">编码像素格式</a>输出。</p><h1 id="d5c9" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">5.研究科</h1><ol class=""><li id="85a8" class="kj kk hi io b ip kl it km ix kn jb ko jf kp jj kq kr ks kt bi translated"><strong class="io hj">主要论文:</strong><a class="ae jk" href="https://arxiv.org/pdf/2008.11638.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="io hj">https://arxiv.org/pdf/2008.11638.pdf</strong></a></li></ol><p id="fd06" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我从一篇研究论文中截取了下面这张照片。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lp"><img src="../Images/a05eae8565ffba2c7252d2ff0797defb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2oBzrd8KTORW2aNN"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">给我买那个look，时尚推荐系统蓝图。</figcaption></figure><p id="499c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">给我买那个look，时尚推荐系统蓝图。</p><p id="6a7d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">根据该论文，体系结构如下所述。</p><ol class=""><li id="5d53" class="kj kk hi io b ip iq it iu ix ly jb lz jf ma jj kq kr ks kt bi translated">通过使用姿态检测分类器，我们必须检测全镜头图像，并且基于FFS，我们必须找到正面图像。</li><li id="f5e3" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">正面图像通过主动学习传递到CNN网络，检测图像中的时尚对象并进行定位。</li><li id="f855" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">为目录中所有可用的图像创建图像嵌入，并存储在数据库中。基于三元组网络的嵌入学习用于生成测试数据。我们也可以使用一个简单的基于CNN的自动编码器。</li><li id="ada7" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">传递一个查询图像，并从数据库中检索相似的图像。这里，他们使用余弦相似性从数据库中获得相似的度量。</li></ol><p id="9a25" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 2。姿态检测:</strong><a class="ae jk" href="https://nanonets.com/blog/human-pose-estimation-2d-guide/" rel="noopener ugc nofollow" target="_blank"><strong class="io hj">https://nanonets.com/blog/human-pose-estimation-2d-guide/</strong></a></p><p id="2ee8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这个博客中，各种方法被用来提出检测问题。通过使用这些预先训练好的模型，我们可以节省大量的时间。选择最适合数据集的架构，然后微调或修改该架构以获得最佳结果。</p><p id="bd24" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 3。本地化/物品检测:</strong><a class="ae jk" href="https://valohai.com/blog/clothes-detection-for-fashion-recommendation/" rel="noopener ugc nofollow" target="_blank"><strong class="io hj">https://valo hai . com/blog/衣服检测-时尚-推荐/ </strong> </a></p><p id="f4dc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这篇博客中，用户解释了用于时尚物体检测的不同标记数据集。这种预先训练的数据集可以用在我们的数据之上，以提高模型的准确性。该博客详细解释了如何使用Tensorflow的对象检测API，并提供了良好的代码片段，这将有助于首先作为黑盒尝试上述模型，然后选择在我们的数据集上提供最佳结果的架构。</p><p id="0dc6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Tensorflow的对象检测API提供了几个预实现的架构，并在COCO(上下文中的公共对象)数据集上预先训练了权重，例如</p><blockquote class="lc ld le"><p id="da9e" class="im in lf io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated"><em class="hi">带移动网络的SSD(单次多盒探测器)</em></p><p id="199a" class="im in lf io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated"><em class="hi">采用Inception V2的固态硬盘。</em></p><p id="7cec" class="im in lf io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated"><em class="hi">带Resnet 101的R-FCN(基于区域的全卷积网络)。</em></p><p id="b25f" class="im in lf io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated"><em class="hi">使用Resnet 101实现更快的RCNN(基于区域的卷积神经网络)。</em></p><p id="e183" class="im in lf io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated"><em class="hi">采用Inception Resnet v2的更快RCNN</em></p></blockquote><p id="16dc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 4。三重损失:</strong><a class="ae jk" href="https://towardsdatascience.com/image-similarity-using-trip%20let-loss-3744c0f67973" rel="noopener" target="_blank"><strong class="io hj">https://towards data science . com/image-similarity-using-trip % 20 let-Loss-3744 c0f 67973</strong></a></p><p id="859e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这篇博客很好地解释了如何使用三重损失来解决图像相似性问题。所以，我对三重损失结构的理解帮助我们通过相似和相异的概念来学习分布式嵌入。这是一种神经网络架构，其中多个并行网络被训练，彼此共享权重。在预测期间，输入数据通过一个网络来计算输入数据的分布式嵌入表示。</p><p id="c287" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">损失函数:三重损失的成本函数如下:</p><p id="c41d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">L(a，p，n) = max(0，D(a，p) — D(a，n) +边距)</p><p id="b5a2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">其中D(x，y):x和y的学习向量表示之间的距离。可以使用L2距离或(1-余弦相似性)作为距离度量。该功能的目的是保持锚点和正极之间的距离小于锚点和负极之间的距离。</p><h1 id="6818" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">6.我的方法</h1><p id="2bc6" class="pw-post-body-paragraph im in hi io b ip kl ir is it km iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">在这里，我将解释我的业务问题的实现。</p><p id="5e42" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">模块1: </strong></p><p id="72ad" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在模块1中，对于姿势检测，我尝试使用HRNet和TensorFlow lite模型。两个模型的输出几乎相似。于是，我拿起了HRNet。从下面的片段可以清楚地看出，两个模型有相似的结果。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/db0798d44dbb5e6376c958eb6f5d4ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*JjXQ4Js7wk7CqQDf_vQQzA.png"/></div></figure><p id="dea3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以，在这里，我使用了我的研究部分的HRNet，从我的语料库中找到所有完整的姿势和正面姿势图像。</p><figure class="ll lm ln lo fd ij"><div class="bz dy l di"><div class="mc md l"/></div></figure><p id="d024" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">结果:</strong></p><figure class="ll lm ln lo fd ij"><div class="bz dy l di"><div class="mc md l"/></div></figure><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es me"><img src="../Images/090722c63d26488b42ea9a3c4dc2e7df.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*WDCKSYapbi2tHgyM3hNTwg.png"/></div></figure><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es me"><img src="../Images/090722c63d26488b42ea9a3c4dc2e7df.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*WDCKSYapbi2tHgyM3hNTwg.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">模块1的结果</figcaption></figure><p id="43df" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果图像被发现是全姿态的，则该图像被发送到模块2</p><p id="5408" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">模块2: </strong></p><p id="93d1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在模块2中，我们必须检测所有文章并对其进行定位。为此，我使用了MaskRcnn模型。我从Kaggle大赛“FGVC6上的iMaterialist (Fashion) 2019”中取的数据。定位后，我们必须裁剪图像，并将其传递给模块3以生成嵌入。</p><p id="9ba6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">【MaskRCNN是如何工作的？</p><p id="a733" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Mask R-CNN(区域卷积神经网络)是一个两阶段框架:第一阶段扫描图像并生成建议(可能包含某个对象的区域)。第二阶段对提议进行分类并生成边界框和遮罩。Mask R-CNN论文是其前身fast R-CNN的扩展，由同一作者完成。更快的R-CNN是一个流行的对象检测框架，Mask R-CNN用实例分割等扩展了它。</p><p id="4d9a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">本教程需要TensorFlow版本1.15.3和Keras 2.2.4。它不能与TensorFlow 2.0+或Keras 2.2.5+一起使用，因为在撰写本文时第三方库尚未更新。</p><p id="82f4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">！pip安装—无深度张量流==1.15.3</p><p id="e3bc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">！pip安装—无deps keras==2.2.4</p><p id="b523" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Mask R-CNN基本上是更快的R-CNN的扩展。更快的R-CNN广泛用于对象检测任务。Mask R-CNN框架建立在更快的R-CNN之上。因此，对于给定的图像，除了每个对象的类标签和边界框坐标之外，Mask R-CNN还将返回对象遮罩。</p><ol class=""><li id="a1c0" class="kj kk hi io b ip iq it iu ix ly jb lz jf ma jj kq kr ks kt bi translated">更快的R-CNN首先使用ConvNet从图像中提取特征地图</li><li id="e76c" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">这些特征图然后通过区域提议网络(RPN)传递，该网络返回候选边界框</li><li id="7fd6" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">然后，我们在这些候选边界框上应用RoI(感兴趣区域)池层，以使所有候选对象具有相同的大小</li><li id="5f85" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">最后，建议被传递到完全连接的层，以分类和输出对象的包围盒</li></ol><p id="6ba7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">类似于我们在更快的R-CNN中使用的从图像中提取特征映射的ConvNet，我们在Mask R-CNN中使用ResNet 101架构从图像中提取特征。因此，第一步是拍摄图像，并使用ResNet 101架构提取特征。这些特征作为下一层的输入。</p><p id="e0d2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们采用上一步中获得的特征地图，并应用区域建议网络(RPM)。这基本上预测了该区域中是否存在物体。在这一步中，我们获得模型预测包含一些对象的那些区域或特征地图。</p><p id="2738" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从RPN获得的区域可能具有不同的形状，对吗？因此，我们应用一个池层，并将所有区域转换为相同的形状。接下来，这些区域通过完全连接的网络，以便预测类别标签和边界框。</p><p id="100a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">到目前为止，这些步骤几乎和R-CNN的工作速度一样快。现在来看两个框架之间的区别。除此之外，掩模R-CNN还产生分段掩模。</p><p id="6716" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为此，我们首先计算感兴趣区域，以便可以减少计算时间。对于所有预测的区域，我们用基础真值盒计算并集上的交集(IoU)。我们可以这样计算借据:</p><p id="d758" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">IoU =交叉点面积/并集面积</p><p id="2625" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，只有当IoU大于或等于0.7+时，我们才将其视为目标区域。否则，我们就会忽视该地区。我们对所有区域都这样做，然后只选择IoU大于0.7+的一组区域。</p><p id="1d89" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">结果:</strong></p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/8c392b006436411e9ebd14c73f5f0dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*TZNRuqjk4N4PdpnNcGiESA.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">定位/物品检测</figcaption></figure><p id="84ed" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">模块3: </strong></p><p id="64a7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在模块3中，我尝试了DenseNet121、ResNet50、ResNet101、MobileNet和InceptionV3。在所有这些中。DenseNet121给出了良好的结果。</p><p id="b319" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">DenseNet121与其他相比稀疏性较低。我选择Densenet121。</p><p id="ac19" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">DenseNet生成了低稀疏度的1024维嵌入。</p><figure class="ll lm ln lo fd ij"><div class="bz dy l di"><div class="mc md l"/></div></figure><p id="f389" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">正如我们看到的，我们有8类数据，我将它们分为3个超级类别进行索引，如下所示。</p><p id="16f8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">上_穿</strong>:女_衬衫_上衣_苔丝<br/> <strong class="io hj">下_穿</strong>:女牛仔裤、女裙子、女长裤<br/> <strong class="io hj">脚_穿</strong>:女休闲鞋、平底鞋、高跟鞋</p><p id="83c2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">模块4: </strong></p><p id="820b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在模块4中，我使用了FAISS(脸书人工智能相似性搜索)库来检索相似的文章。</p><p id="16d9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Faiss仅适用于float32型ndarray。所以，首先，我们把嵌入转换成ndarray类型的float32。</p><p id="ef60" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">用faiss为上层器皿、下层器皿和鞋类创建了3个索引。</p><div class="mg mh ez fb mi mj"><a href="https://github.com/facebookresearch/faiss/wiki/Faiss-indexes" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hj fi z dy mo ea eb mp ed ef hh bi translated">facebookresearch/faiss</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">下面给出了基本的索引:索引可以用类构造函数显式地构造，或者用…</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">github.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx ik mj"/></div></div></a></div><p id="8aab" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从上面提供的脸书Github页面来看，与其他使用欧几里德距离计算最近距离的方法相比，IndexFlatL2是一种蛮力方法。所以，我用了。为了降低空间复杂度，还使用了<strong class="io hj"> IndexIVFPQ </strong>量化器。我们也可以使用余弦，但是在使用余弦之前，我们必须将向量归一化。通常余弦距离用于文本相似度。</p><p id="1775" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">索引只接受一个参数，它只是一个任意形状的向量，但是如果我们传递多个向量，要确保所有的向量都是相同的形状。</p><figure class="ll lm ln lo fd ij"><div class="bz dy l di"><div class="mc md l"/></div></figure><p id="5fa9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们有一个Faiss的搜索方法，它依赖于索引值来检索相似的文章。在搜索方法中，我们也必须传递形状与索引向量相匹配的向量。</p><p id="424c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因为模块3中的生成嵌入返回长度为2014的列表，形状(行向量)和类型(浮点32 ndarray)不同。在搜索之前，我们必须先转换它。</p><figure class="ll lm ln lo fd ij"><div class="bz dy l di"><div class="mc md l"/></div></figure><h1 id="443a" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">7.最终结果</h1><p id="212f" class="pw-post-body-paragraph im in hi io b ip kl ir is it km iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">所以我们有一个最终的解决方案。模型能够从给定的图像中检测和检索时尚对象。所以有一些错误的对象检测和错误的检索，但这是因为模型被训练了更少的时期。一些错误的检索是因为嵌入的是整个图像，而不是对象。此外，数据库的大小也很小。但总的来说，我们有一个可以进一步扩展和优化的首选解决方案。请在Github链接中查看。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es my"><img src="../Images/39e27ad63df35eebee1e4fa39a905e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-hDA_w7xPK91-FPgQiAT-g.png"/></div></div></figure><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mz"><img src="../Images/ddee1c3ac83dd95f6389c715beb1e42c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yqnUER4m1UE-W8DyoBHCGg.png"/></div></div></figure><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es na"><img src="../Images/a056697d3a4cedd1f29443d623a6ca05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZfYP63JaOVD6ho4DbftF3Q.png"/></div></div></figure><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es nb"><img src="../Images/cf3c658e27667de4b10e94039a586081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p5NyVGbYzSd0Vu12whuqwg.png"/></div></div></figure><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es nc"><img src="../Images/6b29789db013192ebdc4c1ab844eb8c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IYtEKf3xjzPu6X_MUL312w.png"/></div></div></figure><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es nd"><img src="../Images/1ca4d2662ec561c2dca711e31a57c9c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wTJHELI8-8v_Ojy6tuXHSg.png"/></div></div></figure><h1 id="caa3" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">8.未来的工作</h1><ol class=""><li id="58d8" class="kj kk hi io b ip kl it km ix kn jb ko jf kp jj kq kr ks kt bi translated">减少端到端应用程序的延迟。</li><li id="3a4b" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">对于嵌入，尝试不同的方法，比如构建自己的模型，并取得一些好成绩。</li><li id="3eb6" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">为对象检测收集更多数据，并使用最新的分割方法，而不是掩模RCNN。</li><li id="2a9c" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">根据研究论文，训练基于三元组网络的嵌入层网络，以获得相似的图像。</li><li id="9f50" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">将模型部署到生产环境中进行实时推荐</li></ol><h1 id="433a" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">9.参考</h1><ol class=""><li id="f28e" class="kj kk hi io b ip kl it km ix kn jb ko jf kp jj kq kr ks kt bi translated"><a class="ae jk" href="https://arxiv.org/pdf/2008.11638.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2008.11638.pdf</a></li><li id="6dc6" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated"><a class="ae jk" href="https://www.tensorflow.org/lite/examples/pose_estimation/overview" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/lite/examples/pose _ estimation/overview</a></li><li id="065e" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated"><a class="ae jk" href="https://programmer.group/analysis-of-official-post-energy-model-of-tensorflow.html" rel="noopener ugc nofollow" target="_blank">https://programmer . group/analysis-of-official-post-energy-model-of-tensor flow . html</a></li><li id="e5aa" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated"><a class="ae jk" href="https://github.com/matterport/Mask_RCNN" rel="noopener ugc nofollow" target="_blank">https://github.com/matterport/Mask_RCNN</a></li><li id="df10" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated"><a class="ae jk" href="https://towardsdatascience.com/mask-rcnn-implementation-on-a-custom-dataset-fd9a878123d4" rel="noopener" target="_blank">https://towardsdatascience . com/mask-rcnn-implementation-on-a-custom-dataset-fd9a 878123d 4</a></li><li id="ae1c" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated"><a class="ae jk" href="https://www.tensorflow.org/api_docs/python/tf/keras/applications" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/keras/applications</a></li><li id="f8c8" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated"><a class="ae jk" href="https://github.com/facebookresearch/faiss/wiki" rel="noopener ugc nofollow" target="_blank">https://github.com/facebookresearch/faiss/wiki</a></li><li id="56b7" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated"><a class="ae jk" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></li></ol><p id="7d88" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果您有任何疑问，请随时发表评论，或者您也可以通过<a class="ae jk" href="https://www.linkedin.com/in/naman-gupta-ds/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我。你可以在这里找到我的完整项目<a class="ae jk" href="https://github.com/NamanGuptacs/buy-me-that-look/tree/master" rel="noopener ugc nofollow" target="_blank"/>。</p></div></div>    
</body>
</html>