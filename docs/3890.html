<html>
<head>
<title>Rainforest Species Audio Detection-A Self Case Study</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">雨林物种音频检测-自我案例研究</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/rainforest-species-audio-detection-a-self-case-study-aef5260ce496?source=collection_archive---------6-----------------------#2021-08-05">https://medium.com/analytics-vidhya/rainforest-species-audio-detection-a-self-case-study-aef5260ce496?source=collection_archive---------6-----------------------#2021-08-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ff066a6b0ec4076e12c6ed7a3dd1f3df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VBG9OjU9NyDWqONm9m5IGQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://wildfor.life/collaborator/rainforest-connection" rel="noopener ugc nofollow" target="_blank">https://wildfor.life/collaborator/rainforest-connection</a></figcaption></figure><h1 id="7276" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">目录:</h1><p id="46d8" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">商业问题</p><p id="bfd5" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">数据集的来源</p><p id="f72f" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">关于使用的数据集和指标</p><p id="1751" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">解决方案方法概述</p><p id="0a8c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">探索性数据分析</p><p id="6c29" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">数据预处理和特征工程</p><p id="dd4f" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">系统模型化</p><p id="ad1e" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">结果</p><p id="60b9" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">部署</p><p id="d847" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">源代码</p><p id="67c1" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">未来的工作</p><p id="3f7b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">参考</p><h1 id="7ced" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">业务问题:</h1><p id="ef15" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">大多数时候，声音比环顾一大片森林来观察某些物种并采取适当的行动更容易识别和接近。在这个案例研究中，给定来自热带雨林的音频记录，我们将检测并分类某些物种的声音。并不是每个人都是仅仅通过听觉来对不同物种的声音进行分类的专家。这就是需要自动语音检测的地方，它可以帮助森林中或某些危险区域中的人在需要时采取适当的行动。这也有助于发现人类对生态系统的某些影响，并有助于保护环境。</p><p id="67d2" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们可以用有限但声学上复杂的数据创建ML/DL模型来自动检测物种，以帮助每个不是该领域专家的人。我们将这个问题映射为多标签分类问题。这里我们必须检测每个物种在给定音频文件中出现的概率。</p><h1 id="c75d" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">数据来源:</h1><p id="d3e3" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">这个数据集是由雨林连接(RFCx)提供的，它存在于Kaggle中，可以从下面的链接下载。</p><div class="kw kx ez fb ky kz"><a href="https://www.kaggle.com/c/rfcx-species-audio-detection/data" rel="noopener  ugc nofollow" target="_blank"><div class="la ab dw"><div class="lb ab lc cl cj ld"><h2 class="bd hj fi z dy le ea eb lf ed ef hh bi translated">雨林连接物种音频检测</h2><div class="lg l"><h3 class="bd b fi z dy le ea eb lf ed ef dx translated">自动检测热带声景中的鸟类和青蛙种类</h3></div><div class="lh l"><p class="bd b fp z dy le ea eb lf ed ef dx translated">www.kaggle.com</p></div></div><div class="li l"><div class="lj l lk ll lm li ln io kz"/></div></div></a></div><h1 id="d1a5" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">关于数据集:</h1><p id="6d2b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><strong class="jv hj"> train_tp.csv </strong> —真阳性物种标签的训练数据，对应时间定位</p><p id="d8ff" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">train _ FP . CSV</strong>——假阳性物种标签的训练数据，带有相应的时间定位</p><p id="6124" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">训练/ </strong> —训练音频文件</p><p id="9379" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">/</strong>—测试音频文件；任务是预测在每个音频文件中发现的物种</p><p id="2319" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">train_tp.csv和train_fp.csv的列</p><ul class=""><li id="8c71" class="lo lp hi jv b jw kr ka ks ke lq ki lr km ls kq lt lu lv lw bi translated">recording_id —记录的唯一标识符</li><li id="cbd1" class="lo lp hi jv b jw lx ka ly ke lz ki ma km mb kq lt lu lv lw bi translated">物种id —物种的唯一标识符</li><li id="4c1c" class="lo lp hi jv b jw lx ka ly ke lz ki ma km mb kq lt lu lv lw bi translated">歌曲类型id —歌曲类型的唯一标识符</li><li id="c9ee" class="lo lp hi jv b jw lx ka ly ke lz ki ma km mb kq lt lu lv lw bi translated">t_min —注释信号的开始秒</li><li id="e171" class="lo lp hi jv b jw lx ka ly ke lz ki ma km mb kq lt lu lv lw bi translated">f _ min——注释信号的较低频率</li><li id="9287" class="lo lp hi jv b jw lx ka ly ke lz ki ma km mb kq lt lu lv lw bi translated">t_max —注释信号的结束秒</li><li id="a87c" class="lo lp hi jv b jw lx ka ly ke lz ki ma km mb kq lt lu lv lw bi translated">f_max-注释信号的上限频率</li><li id="7dfb" class="lo lp hi jv b jw lx ka ly ke lz ki ma km mb kq lt lu lv lw bi translated">is _ TP-[仅tfrecords指示标签是来自train_tp (1)还是train_fp (0)文件。</li></ul><p id="86d2" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"> tfrecords/{train，test }</strong>-TF record格式的比赛数据，包括recording_id、audio_wav(以16位PCM格式编码)和label_info(仅用于train)，label _ info提供了以-分隔的下列字符串(减去recording_id)，其中recording_id的多个标签是；-分隔。</p><p id="95c9" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">本案例研究中使用的指标是标签平均排名精度。它是分配给每个样本的每个基本事实标签与总分数较低的标签之比的平均值。这种度量通常用于多标签分类问题。如果您能够给与每个样本相关联的标签更好的排名，则性能度量将会更高。</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/3c074fbc72dfa75813ae22ce2aceab3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*VJKwNB1C28NloxSkdB57DA.png"/></div></figure><h1 id="a3ad" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">解决方案方法概述:</h1><p id="d25d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在这个案例研究中，我采用了两种不同特征的迁移学习方法，然后建立了一个模型集合，最后取类预测的平均值。</p><p id="b1b0" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">对于集合中的CNN部分，我使用Mel-spectrogram作为特征，将它们转换为RGB图像，执行数据扩充，并使用迁移学习训练3个独立的CNN模型。对于模型的LSTM部分，我使用了google提供的预训练Yamnet模型，该模型是在大型音频语料库上训练的。它产生音频嵌入，然后可以用作训练LSTM模型的特征。</p><p id="606f" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">最后，我取了所有模型的预测值，取了预测值的平均值，得到了最终结果。</p><h1 id="f9e8" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">探索性数据分析:</h1><p id="af68" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">EDA是任何数据科学项目中非常关键的一步。它有助于更好地理解我们的数据，探索我们数据中的各种模式，并发现变量之间有趣的关系。这反过来帮助我们设计更好的功能并获得更好的结果。</p><p id="cf8b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们有真阳性和假阳性标记数据。在本案例研究中，我们仅使用了真正的阳性标记数据。</p><p id="3959" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">检查空值:</strong></p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/be853ebc51441e38f047e776f0df2797.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*gmoK97PNRHNX9MXgeUPGXw.png"/></div></figure><p id="6ff9" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们的数据中没有任何空值。</p><p id="26db" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">分类标签分配</strong></p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/6e8cca4c4db9acef4ce3d018c394a79f.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*YGg15YxobpDua7kcLIhT5Q.png"/></div></figure><p id="8302" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们在第23课有最大阅读量，在第10课有最小阅读量。其余的课程分布相当均匀。</p><p id="14c0" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在让我们检查一下每个文件中存在的物种数量是多少。</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/9aeec597f1d89f636aeb6fdd7450556e.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*clMSWOW_wWiRKy9yL1IJGA.png"/></div></figure><p id="6ab5" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们的训练数据中用于真阳性标记的大多数记录只有1个种类。</p><p id="82e9" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在我们将检查t_min和t_max的分布。t_min表示注释信号的开始，t_max表示注释信号的结束。</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/614bebc705f2c6e0b33df0931d477b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*X2JqGlMRIQCEojt1FwAPMQ.png"/></div></figure><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/c4cf9281c27b9d0cff96bc5df3bc8bc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*C1dWj2aHPg1vOc5_XeQEkQ.png"/></div></figure><p id="4e08" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">观察-</p><ol class=""><li id="49c8" class="lo lp hi jv b jw kr ka ks ke lq ki lr km ls kq mm lu lv lw bi translated">对于大多数记录，注释信号的开始位于第50-55秒，随后是第0-5秒。</li><li id="0fdf" class="lo lp hi jv b jw lx ka ly ke lz ki ma km mb kq mm lu lv lw bi translated">对于大多数记录，注释信号的结束位于第55-60秒，随后是第50-55秒。</li><li id="3906" class="lo lp hi jv b jw lx ka ly ke lz ki ma km mb kq mm lu lv lw bi translated">我们还可以从这些图中推断出我们的记录持续时间为1分钟。</li></ol><p id="5d13" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">让我们检查分布f_min和f_max。f_min表示带注释信号的较低频率，f_max表示带注释信号的较高频率。</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/f35e58b937bc66f7c6d78e9e551d2ee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*s9O6mg88nvjKkRwsOLY6pw.png"/></div></figure><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/6e727a205883fc1bebcbe885e59f64aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*8PGxQ7XEyVN1_Vf73ATNIA.png"/></div></figure><p id="634b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">观察结果:</p><p id="6f0c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">1.对于大多数记录，标注信号的较低频率位于0–800Hz之间。</p><p id="6b3d" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">2.对于大多数记录，注释信号的上限频率在4.2 KHz到5KHz之间。</p><p id="b78f" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在我们将检查类标签和f_min，f_max之间的关系。</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/4c185c977c926fc25d70bb327e53076c.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*3QKZ-ha9zN5n7acqRnGL-A.png"/></div></figure><p id="197f" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">观察结果:</p><ol class=""><li id="8f7e" class="lo lp hi jv b jw kr ka ks ke lq ki lr km ls kq mm lu lv lw bi translated">基于f_min值，很少有类可以被分离。</li></ol><p id="3a1c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">2.如果f_min位于5.5 KHz到5.8 KHz之间，则该物种属于0类。</p><p id="34fe" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">3.如果f_min大于10 KHz，则该物种属于第22类</p><p id="1572" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">4.如果f_min在6 KHz到7.8 KHz之间，则该物种属于第23类。</p><p id="948f" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">5.如果f_min位于4.8 KHz到5.6 Khz之间，则该物种属于类别5。</p><p id="dfdf" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">6.13级和15级在较低频率的基础上是不可分的。</p><p id="731e" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">7.大多数种类的低频范围很窄。</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/51298b9d1a51bfaee92a3d96160a82a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*IEPFmbE5WPtwatKPZssfIQ.png"/></div></figure><ol class=""><li id="a38f" class="lo lp hi jv b jw kr ka ks ke lq ki lr km ls kq mm lu lv lw bi translated">根据f_max值，很少有类可以被分开。</li></ol><p id="5cc2" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">2.第二类和第三类很容易分开。</p><p id="0335" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">3.第14类和第17类的值范围很广，可能包含一些异常值。</p><p id="f5dc" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">4.如果f_max大于12 KHz，则数据属于类别22。</p><h1 id="89ca" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">数据预处理和特征工程；</h1><p id="ec55" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">当我们构建模型集合时，我们将把我们的方法分成两部分。第一部分是将原始音频信号转换成mel光谱图，然后再转换成RGB图像。然后我们将在这些图像上训练单独的CNN模型。</p><p id="b2bd" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">该方法的第二部分将是将原始音频信号转换为音频嵌入，该音频嵌入将由tensorflow hub中可用的Yamnet模型生成。该嵌入将作为特征传递给LSTM模型。</p><p id="1fca" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们先来了解一下什么是mel谱图。频谱图是一种直观的方式，表示在特定波形中出现的频率下信号强度或响度随时间的变化。我们可以从光谱图推断不同频率的能量是多还是少，以及能量如何随时间变化。现在我们知道了什么是声谱图，让我们进一步了解梅尔标度和梅尔声谱图。</p><p id="4402" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">研究表明，人类不会在线性尺度上感知频率。例如，我们很容易区分500KHz和1000 KHz，但我们很难区分10000 KHz和10500 KHz，即使它们之间的距离相同。因此，有人提出了一种音高单位，使得音高距离相等时，听者听起来距离相等。这就是所谓的梅尔标度。从赫兹标度到梅尔标度的转换如下:</p><p id="dcf7" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">mel声谱图以Mel标度显示声音。</p><p id="c715" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在我们已经了解了什么是mel光谱图，让我们看看将原始音频信号转换为mel光谱图，然后转换为RGB图像的代码。</p><figure class="md me mf mg fd ij"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="18c5" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们将把数据分成训练集和验证集，并在训练数据集上应用图像增强。</p><figure class="md me mf mg fd ij"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="01e1" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在调用这些函数并准备数据管道来训练CNN模型。</p><figure class="md me mf mg fd ij"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="81f1" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在我们将转向我们的嵌入特性。在此之前让我们了解一下Yamnet。</p><p id="8ff3" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">YAMNet(又一个移动网络)是由Dan Ellis在AudioSet数据集上训练的预训练神经网络模型，该数据集包含来自超过200万个youtube视频的标记数据。它采用移动V1深度方向可分离卷积结构。它采用音频形式作为输入，并对521个音频事件进行独立预测。它返回概率得分、嵌入和频谱图作为输出。该模型接受包含任意长度波形的1-D float32张量或NumPy数组，表示为范围<code class="du mt mu mv mw b">[-1.0, +1.0]</code>内的单通道(单声道)16 kHz样本。我们将使用YAMNet生成的这个嵌入作为我们的LSTM模型的特性。</p><p id="a62e" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">请参考下面的链接，了解更多关于YAMNet的信息，以及如何将其用于迁移学习。</p><div class="kw kx ez fb ky kz"><a href="https://www.tensorflow.org/tutorials/audio/transfer_learning_audio" rel="noopener  ugc nofollow" target="_blank"><div class="la ab dw"><div class="lb ab lc cl cj ld"><h2 class="bd hj fi z dy le ea eb lf ed ef hh bi translated">环境声音分类的YAMNet迁移学习</h2><div class="lg l"><h3 class="bd b fi z dy le ea eb lf ed ef dx translated">YAMNet是一个预先训练好的深度神经网络，可以预测来自521个类别的音频事件，如笑声、犬吠声…</h3></div><div class="lh l"><p class="bd b fp z dy le ea eb lf ed ef dx translated">www.tensorflow.org</p></div></div><div class="li l"><div class="mx l lk ll lm li ln io kz"/></div></div></a></div><p id="1044" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在让我们看看将原始音频转换成嵌入内容的代码。</p><figure class="md me mf mg fd ij"><div class="bz dy l di"><div class="mr ms l"/></div></figure><h1 id="fc4e" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">建模:</h1><p id="95fa" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">让我们也定义一下我们的度量标准。</p><figure class="md me mf mg fd ij"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="6418" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">对于CNN模型，我使用了Sigmoid焦点交叉熵损失和优化器作为修正的Adam。</p><p id="2237" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">sigmoid焦交叉熵损失基于分类误差来衡量每个样本对损失的贡献。这背后的想法是减少正确分类的样本的贡献，并且更多地关注难以分类的样本。与正确分类的样本相比，被分类器错误分类的样本的损失值将高得多。此损失函数在tensorflow附加模块中可用。请参考下面的链接了解更多关于sigmoid焦交叉熵损失。</p><div class="kw kx ez fb ky kz"><a href="https://gombru.github.io/2018/05/23/cross_entropy_loss/" rel="noopener  ugc nofollow" target="_blank"><div class="la ab dw"><div class="lb ab lc cl cj ld"><h2 class="bd hj fi z dy le ea eb lf ed ef hh bi translated">了解分类交叉熵损失，二元交叉熵损失，软最大损失，逻辑…</h2><div class="lg l"><h3 class="bd b fi z dy le ea eb lf ed ef dx translated">人们喜欢用很酷的名字，这些名字常常让人迷惑。当我开始玩CNN超越单一标签…</h3></div><div class="lh l"><p class="bd b fp z dy le ea eb lf ed ef dx translated">gombru.github.io</p></div></div><div class="li l"><div class="my l lk ll lm li ln io kz"/></div></div></a></div><p id="ebfc" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在让我们开始定义我们的架构。我尝试了各种型号的组合，但EfficientNetB2、EfficientNetB3和DenseNet121的组合效果很好。我已经使用了imagenet权重，并在最后添加了几层。</p><figure class="md me mf mg fd ij"><div class="bz dy l di"><div class="mr ms l"/></div></figure><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/9904a65aeeec29255aebd04b96c9da58.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*pGml5y_oKcBeI-T9Twq4ag.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">效率网络b</figcaption></figure><figure class="md me mf mg fd ij"><div class="bz dy l di"><div class="mr ms l"/></div></figure><figure class="md me mf mg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/9d558fd5dfa3755e8d9e38fdfa2e6677.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*0IP8kGjajQZLXh0Fim8pyQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">B3效率网</figcaption></figure><figure class="md me mf mg fd ij"><div class="bz dy l di"><div class="mr ms l"/></div></figure><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es nb"><img src="../Images/0e0227bd1e7a903581f49c8d0997c3db.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*bSlhXFwrnShhNKf-y5UDnA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">DenseNet 121</figcaption></figure><p id="65ec" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在对模型个体进行训练后，损失和指标公布如下:</p><p id="c274" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">【DenseNet121的损失图:</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es nc"><img src="../Images/dbf20b197fa0b903d4257afb43caded1.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*nhMd-4gHRQ-s_Ty862CBNg.png"/></div></figure><p id="5968" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">【DenseNet121的公制图:</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/1372bff44a4aab553f1732d24ac6fa61.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*dCyBpMC5llsA6e_eDjSyow.png"/></div></figure><p id="c7ea" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">效率损失图B2: </strong></p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/6111ee3372ae13c625977bcd6904a40f.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*-2_fNNEKk-8e6aEx_PClyg.png"/></div></figure><p id="3ea7" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">效率的度量图B2: </strong></p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/d6cfec0ec7ac66ef59d8ee260b1d4c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*---4E2bYwIAY9ICF8pZH3Q.png"/></div></figure><p id="27a6" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">效率网络的损失图B3: </strong></p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/8235eab3145de8390284058dc2f9f54f.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*wzOIv7RaEJLpkYwWlpKPcA.png"/></div></figure><p id="2b69" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">效率网络的度量图B3: </strong></p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/8d7cf6f39639885bcba4c91bde5da0c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*B-7D0GAiSMNM_BROIfx57A.png"/></div></figure><p id="d49d" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在让我们继续构建嵌入YAMNet的LSTM模型。我们在这里使用了二元交叉熵损失和Adam作为优化器。</p><figure class="md me mf mg fd ij"><div class="bz dy l di"><div class="mr ms l"/></div></figure><figure class="md me mf mg fd ij"><div class="bz dy l di"><div class="mr ms l"/></div></figure><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/5e74e1ed61adf012438e5197ffaf22e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*i_QAEj8lX66TgTxDEr295w.png"/></div></figure><p id="849a" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">LSTM模型的损失图:</strong></p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/9e1d98cd646702a155ae1e459be9f847.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*rjxlvnw9S2ay1xWSTYDtUQ.png"/></div></figure><p id="7566" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">LSTM模型的度量图:</strong></p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/de1c8579672a935db05d90991999e57c.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*GQ8u93WHRD3r8ydG1fhNJg.png"/></div></figure><p id="8188" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">从上面的图中我们可以看出，我们的单个模型倾向于过度拟合。这可能是由于我们真正的阳性标记数据非常少的原因。因此，我们将在集合方法中使用它们，最终减少过度拟合并更好地概括。</p><p id="4b40" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在，让我们建立我们的集合，对训练集和验证集进行预测。</p><figure class="md me mf mg fd ij"><div class="bz dy l di"><div class="mr ms l"/></div></figure><h1 id="ba82" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">结果:</h1><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/c5995b493a60e342c4451a55f20becec.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*vzl4S9AAku9PChuY7pzoYw.png"/></div></figure><h1 id="de67" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">部署:</h1><p id="df4b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我们的集合模型表现最好。我们现在将部署我们的集合方法。我已经使用streamlit将模型部署为webapp。下面是使用streamlit框架将模型部署为简单webapp的代码。app.py文件分为3个主要功能。load_files函数加载包含模型架构和权重文件的json文件。</p><p id="fa9d" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">数据预处理功能将文件索引作为输入，并从yamnet创建谱图图像、嵌入。最后，它返回图像和嵌入。</p><p id="b3d4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">最后，预测函数将id作为输入。它调用加载文件和data_preprocess函数。然后，它运行预测并返回前三个预测标签和预测所用的时间。</p><p id="1b38" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">webapp将文件的索引作为用户输入，并输出前三个预测物种和预测所用的时间。由于模型的计算量很大，在CPU资源上运行时，预测物种需要更多的时间。请参考演示视频，看看最终结果。</p><figure class="md me mf mg fd ij"><div class="bz dy l di"><div class="mr ms l"/></div></figure><figure class="md me mf mg fd ij"><div class="bz dy l di"><div class="nk ms l"/></div></figure><h1 id="9581" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">源代码:</h1><p id="a550" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">项目源代码请参考下面提到的github资源库。</p><div class="kw kx ez fb ky kz"><a href="https://github.com/vdhar1992/Rainforest_species_audio_detection" rel="noopener  ugc nofollow" target="_blank"><div class="la ab dw"><div class="lb ab lc cl cj ld"><h2 class="bd hj fi z dy le ea eb lf ed ef hh bi translated">GitHub-vdhar 1992/雨林_物种_音频_检测</h2><div class="lg l"><h3 class="bd b fi z dy le ea eb lf ed ef dx translated">在GitHub上创建一个帐户，为vdhar 1992/rain forest _ species _ audio _ detection开发做贡献。</h3></div><div class="lh l"><p class="bd b fp z dy le ea eb lf ed ef dx translated">github.com</p></div></div><div class="li l"><div class="nl l lk ll lm li ln io kz"/></div></div></a></div><h1 id="72d8" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">未来工作:</h1><p id="ab1b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">交叉验证可用于减少单个模型的过度拟合。</p><p id="3429" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">也可以使用基于注意力的模型。</p><p id="468d" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们只使用了真正的正面数据，只有1216个样本。很少的数据使单个模型过度拟合。我们也可以使用假阳性数据，重新标记它们，并在训练数据之外使用它们。</p><h1 id="be2a" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">参考资料:</h1><p id="b4a7" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><a class="ae iu" href="https://www.tensorflow.org/tutorials/audio/transfer_learning_audio#load_the_audio_files_and_retrieve_embeddings" rel="noopener ugc nofollow" target="_blank"> 1。https://www . tensor flow . org/tutorials/audio/transfer _ learning _ audio # load _ the _ audio _ files _ and _ retrieve _ embeddings</a></p><p id="82c6" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><a class="ae iu" href="https://github.com/cerlymarco/MEDIUM_NoteBook/blob/master/NeuralNet_Ensemble/NeuralNet_Ensemble.ipynb" rel="noopener ugc nofollow" target="_blank"> 2。https://github . com/cerlymarco/MEDIUM _ NoteBook/blob/master/neural net _ Ensemble/neural net _ Ensemble . ipynb</a></p><p id="ea5e" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><a class="ae iu" href="https://arxiv.org/abs/1608.04363" rel="noopener ugc nofollow" target="_blank"> 3。https://arxiv.org/abs/1608.04363</a></p><p id="17e0" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><a class="ae iu" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank"> 4。https://www.appliedaicourse.com/</a></p></div></div>    
</body>
</html>