<html>
<head>
<title>Why you should consider studying Linear Algebra as a data scientist : SVD</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么你应该考虑作为数据科学家学习线性代数:奇异值分解</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/why-you-should-consider-studying-linear-algebra-as-a-data-scientist-part-1-svd-9d0ba3189a3e?source=collection_archive---------17-----------------------#2021-05-12">https://medium.com/analytics-vidhya/why-you-should-consider-studying-linear-algebra-as-a-data-scientist-part-1-svd-9d0ba3189a3e?source=collection_archive---------17-----------------------#2021-05-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/b27af4610f808ae1e8fcaf749fb14f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dARg_WJjjlOBenKT"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">照片由<a class="ae it" href="https://unsplash.com/@comparefibre?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上对比纤维</a></figcaption></figure><p id="6a5c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js">这篇文章是为了让读者更深入地探索这个话题，自己使用结尾提到的资源。</em></p><p id="ada3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">无论你是在做最小二乘近似，还是在阅读深度学习的新论文，对线性代数的基本理解对于内化关键概念至关重要。</p><h1 id="af1d" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">线性代数的要诀</strong></h1><p id="b2c1" class="pw-post-body-paragraph iu iv hh iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr ha bi translated">首先，一些我们都应该知道的定义</p><p id="7acd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">列空间:你可以把矩阵的每一列想象成空间中的一个向量。现在，矩阵中所有独立向量的线性组合所形成的空间构成了列空间。</p><figure class="kx ky kz la fd ii er es paragraph-image"><div class="er es kw"><img src="../Images/b66f4a081539c36bec3acb75b9f136f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*_2A17jGpQLUrAAnr3L_sjA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">红色向量是黑色向量的线性组合</figcaption></figure><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="938f" class="lg ju hh lc b fi lh li l lj lk">import matplotlib.pyplot as plt<br/>from mpl_toolkits.mplot3d import Axes3D</span><span id="dd0d" class="lg ju hh lc b fi ll li l lj lk">fig = plt.figure(figsize=(10,10))<br/>ax = fig.gca(projection='3d')<br/>ax.set_xlim3d(-1, 1)<br/>ax.set_xlabel('x-axis')<br/>ax.set_ylabel('y-axis')<br/>ax.set_zlabel('z-axis')<br/>ax.set_ylim3d(-1, 1)<br/>ax.set_zlim3d(-1, 1)<br/>A=np.array([[1,0],[0,1],[0,0]]) # Initial Column vectors<br/>C=np.copy(A)<br/>ax.quiver(0, 0, 0,*C[:,0],normalize=True,color='k')<br/>ax.quiver(0, 0, 0,*C[:,1],normalize=True,color='k')<br/>for i in range(50): # Taking 50 random linear combinations  </span><span id="7c09" class="lg ju hh lc b fi ll li l lj lk"> x1=np.random.uniform(-100,100,1)<br/> x2=np.random.uniform(-100,100,1)<br/> ax.quiver(0, 0, 0,*(C[:,0]*x1 + [:,1]*x2),normalize=True,color='r')</span><span id="2296" class="lg ju hh lc b fi ll li l lj lk">plt.show()</span></pre><p id="4d59" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">示例中所示的矩阵的每一列([1，0，0] &amp; [0，1，0])由黑色向量表示。红色向量是这两列的线性组合。看起来是不是所有的红色向量都在同一平面上？因为他们有！所有这些向量构成了矩阵的列空间。在这种情况下，列空间是一个平面(因为有两个独立的列)。但是，如果独立列的数量更多，则空间可以存在于更高维度中。</p><p id="4a32" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">秩:</strong>矩阵中独立列(或行)的数量。矩阵中的列条目不能是彼此的线性组合。本质上，您不能使用其他列来构成矩阵中的特定列。</p><figure class="kx ky kz la fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lm"><img src="../Images/8ab274d0bced358c5732c65fdc05d86b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X1Uzu1tjwbApkSEN9IeISA.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">一个很小的3x3矩阵，其中秩为2，因为第3列依赖于其他两列。</figcaption></figure><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="6642" class="lg ju hh lc b fi lh li l lj lk"><strong class="lc hi">from</strong> <strong class="lc hi">numpy.linalg</strong> <strong class="lc hi">import</strong> matrix_rank<br/>matrix_rank(np.eye(3)) </span><span id="9f4d" class="lg ju hh lc b fi ll li l lj lk">An identinty matrix with rank 3, since all the columns are independent of each other.</span></pre><p id="31be" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在很高的层次上，我们可以将矩阵的秩视为“信息量”。例如，如果有一个1000x1000像素值的图像，如果我可以用一半的值获得足够的信息，我为什么要存储所有这些值呢？</p><p id="64b1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了得到这样一个低阶近似，我们可以使用几种方法，如奇异值分解，或NMF。下面的图片向我们展示了一个低秩近似可能并不总是那么糟糕。</p><figure class="kx ky kz la fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ln"><img src="../Images/89f9aa90fca77ded7c01fd5557cfa62c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_F4yLnsOzAJWw4CElkWtcQ.png"/></div></div></figure><figure class="kx ky kz la fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ln"><img src="../Images/17644ce988a9f87241ba5f76cff77576.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VlkgE7liW2NPDqp5WS2uQA.png"/></div></div></figure><p id="4c72" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">差别不大，是吗？</p><p id="5ca0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">线性变换</strong></p><p id="8a97" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">任何矩阵都可以被认为是线性变换<em class="js">(相对于固定基</em>)。每当我们将一个向量乘以一个矩阵时，我们可以想到应用于该向量的缩放、剪切或旋转效果。</p><p id="1e91" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们看一个简单的例子:</p><p id="1e57" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们形成一个2 x (N*N)矩阵，并用散点图来表示。然后我们对矩阵进行变换，看看结果。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="bbb1" class="lg ju hh lc b fi lh li l lj lk">def build_array(startx,endx,starty,endy,N):</span><span id="a333" class="lg ju hh lc b fi ll li l lj lk">''' Build an array of 2 x (N*N) '''</span><span id="f56a" class="lg ju hh lc b fi ll li l lj lk">xvals = np.array(np.linspace(startx, endx, N))<br/>yvals = np.array(np.linspace(starty, endy, N))<br/>arr=np.vstack(np.meshgrid(xvals,yvals)).reshape(2,N*N)</span><span id="29fb" class="lg ju hh lc b fi ll li l lj lk">return arr</span><span id="bbe4" class="lg ju hh lc b fi ll li l lj lk">def visualize_array(arr):</span><span id="fcb4" class="lg ju hh lc b fi ll li l lj lk">''' scatterplot to visualize array '''</span><span id="4b51" class="lg ju hh lc b fi ll li l lj lk">plt.figure(figsize=(7, 7))<br/>plt.xlim([0,30 ])<br/>plt.ylim([0,20])<br/>plt.scatter(arr[0],arr[1])</span><span id="0f4a" class="lg ju hh lc b fi ll li l lj lk">arr_1=build_array(0,10,0,10,11) # array<br/>visualize_array(arr_1)<br/>transformation=np.array([[1,1],[0,1]]) # transformation<br/>final_array=np.dot(transformation,arr_1)# Applied the trasnformation<br/>visualize_array(final_array)</span></pre><div class="kx ky kz la fd ab cb"><figure class="lo ii lp lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><img src="../Images/fbc1d40b6e5aaa8e563d4f2fa8d584a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*gqSi57Vke-0T5GE03FvPTg.jpeg"/></div></figure><figure class="lo ii lp lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><img src="../Images/9f5d8bf915b9bf857707a465976b41fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*qBiNauG0RpYR60F13w-N0Q.jpeg"/></div><figcaption class="ip iq et er es ir is bd b be z dx lu di lv lw translated">原始空间在右边，转换后在左边</figcaption></figure></div><p id="66ea" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以看到，由于矩阵第一列中的<strong class="iw hi"><em class="js">【1，0】</em></strong>(类似于恒等式)，x轴保持在它的位置，我们可以看到由于第二列中的<strong class="iw hi"><em class="js">【1，1】</em></strong>发生了剪切。我强烈建议您亲自尝试代码，应用各种转换，并查看矩阵的最终形状。</p><p id="25d8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">特征值和特征向量:</strong></p><figure class="kx ky kz la fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lx"><img src="../Images/1f4b8445b9978fbe10d2ea1bc614c7a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9kL-uYaGsSiaNY1OOWlElw.png"/></div></div></figure><p id="43a7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是我们在高中学到的方程式。但不清楚的是(至少当时对我来说)，它们背后的视觉直觉。继续我们将矩阵视为线性变换的思路，特征向量是当我们对它们应用变换A时<em class="js">不受</em>影响的向量，特征值只是一个比例因子(方向保持不变)。让我们快速看一个例子。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="8ec2" class="lg ju hh lc b fi lh li l lj lk">def transform(A,B):</span><span id="e6a3" class="lg ju hh lc b fi ll li l lj lk">''' plot the transformation of a 2x1 vector, given a 2x2 matrix '''</span><span id="f963" class="lg ju hh lc b fi ll li l lj lk">fig = plt.figure(figsize=(7,7))<br/>param=np.amax(A)*np.amax(B)<br/>plt.xlim([-1,param])<br/>plt.ylim([-1,param])<br/>plt.quiver(0, 0, B[0],B[1],color='k',scale=1,units='xy',label='original_vector')<br/>C= A[:,0]*B[0]  + A[:,1]*B[1]  # Expressing as a linear combination <br/>plt.quiver(0, 0, C[0],C[1],color='B',scale=1,units='xy',label='post<br/>transformation_vector')<br/>plt.legend()</span><span id="e2d4" class="lg ju hh lc b fi ll li l lj lk">A=np.array([[2,4],[0,2]]) #Transformation<br/>B=[[1],[0]] #Eigenvector<br/>transform(A,B)</span><span id="c3a1" class="lg ju hh lc b fi ll li l lj lk">B=[[2],[1]] # Other vector<br/>transform(A,B)</span></pre><div class="kx ky kz la fd ab cb"><figure class="lo ii lp lq lr ls lt paragraph-image"><img src="../Images/ab3b88d9c9ab74718158dc919f086d39.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*Ocrl_Uv6qvSsnYwsUiPGsQ.png"/></figure><figure class="lo ii lp lq lr ls lt paragraph-image"><img src="../Images/7ab278d08cb811d03601e36aba2bee68.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*0zAYOZ-8NepapozTFbtahA.png"/><figcaption class="ip iq et er es ir is bd b be z dx lu di lv lw translated">不受变换A影响的特征向量</figcaption></figure></div><p id="8078" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以看到，特征向量只是被变换“拉伸”了。比例因子就是特征值。</p><p id="1d52" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当我们有<strong class="iw hi"> <em class="js">对称矩阵(Transpose(A) = A) </em> </strong>时，这些特征向量出现了一个有趣的性质。这些情况下的特征向量看起来是<strong class="iw hi">正交</strong> <em class="js"> </em>或者换句话说是垂直的。我们使用上面相同的函数来说明一个例子。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="577c" class="lg ju hh lc b fi lh li l lj lk">A=np.array([[2,1],[1,2]]) # Symmetric Matrix<br/>B=[[1],[1]] #Eigenvector<br/>transform(A,B)<br/>A=np.array([[2,1],[1,2]]) # Symmetric Matrix<br/>B=[[-1],[1]] #Eigenvector<br/>transform(A,B)</span></pre><div class="kx ky kz la fd ab cb"><figure class="lo ii lp lq lr ls lt paragraph-image"><img src="../Images/15d5fbd1a7175f1c5e021408b8c3bdfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*VJcbgzv5qs6bITu9Oin-dQ.png"/></figure><figure class="lo ii lp lq lr ls lt paragraph-image"><img src="../Images/c50520ed4880de764469d187eb855ba0.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*cW9LF3_1wXqm2BgW4RMO3A.png"/><figcaption class="ip iq et er es ir is bd b be z dx lu di lv lw translated">特征向量是正交的(彼此成90度)</figcaption></figure></div><p id="7399" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，为什么我们会关心特征向量是正交的，想象一下，如果你能把一个矩阵分解成它的特征向量和特征值的因子，并且这个矩阵是对称的，你会得到彼此正交的列。这将表明该特定矩阵的每一列彼此完全不同<strong class="iw hi">。<em class="js">在无监督学习的背景下，这就是你如何对来自原始矩阵的条目进行“分类”。</em> </strong></p><p id="9dbd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">但是有一个问题，所有的矩阵都不是对称的，也绝对不是正方形的(非正方形矩阵不存在特征向量)。你能想象一个具有相同行数和列数的数据集吗？在现实世界中，这几乎不会发生。那我们该怎么办？</p><p id="76d3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们强行使其对称，将矩阵的转置与自身相乘得到我们想要的对称。这就是奇异值和奇异值分解的全部概念。</p><h1 id="9d0b" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">奇异值分解:</strong></h1><figure class="kx ky kz la fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ly"><img src="../Images/e12898b05a72fa02e91533eb5ceedc0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*datP69fmstsHSvwVof5g5g.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">特征分解</figcaption></figure><p id="8495" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于对称矩阵,“特征分解”如上，但对于其他矩阵，我们使用:</p><figure class="kx ky kz la fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lz"><img src="../Images/77017792c1bbc20e1af47e6d9b58f23a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9VKfgPHcd0mp5Wy-A5RQ-w.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">一般n×d矩阵的奇异值分解</figcaption></figure><p id="ebd7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">向量U和转置(V)具有正交列。在中间，我们有我们的‘奇异值’(特征值的替代品)，但重要的因素是它们是有序的(按照惯例)。取前“n”个奇异值，我们将得到a的最佳n阶近似值。这很重要，因为我们的大部分信息将从前几个“奇异值”中重建。如果矩阵的秩是r，我们可以想象<strong class="iw hi"> <em class="js">矩阵V在A的列空间，U在A的行空间，我没解释行空间？只是Transpose(A)的列空间。</em> </strong>列空间中除了‘r’之外的向量(n-r)的其余部分是什么？那么，这就进入了零空间(这是另一天)</p><p id="49b8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们来看一个小例子，看看我们如何将它用于主题建模。我们将使用来自sci-kit-learn的fetch_20newsgroups数据集。这给了我们大约13000个我们的模型不知道的各种类别的文档。为了构建我们的初始矩阵，我们使用Tf-Idf，因此我们的矩阵是以<strong class="iw hi"> <em class="js">文档x单词的形式。W </em> </strong></p><p id="1544" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">同样，行是文档，列是单词，条目是特定单词的tf-idf分数。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="95e1" class="lg ju hh lc b fi lh li l lj lk"><strong class="lc hi">#### BUILDING UP DATAFRAME</strong></span><span id="2df1" class="lg ju hh lc b fi ll li l lj lk">from sklearn.datasets import fetch_20newsgroups<br/>dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))<br/>documents = dataset.data<br/>vectorizer = TfidfVectorizer(stop_words='english',max_features= 1000,max_df = 0.5,smooth_idf=True) #picking 1000 words</span><span id="e1e7" class="lg ju hh lc b fi ll li l lj lk">df = pd.DataFrame({'document':documents})<br/>df['docs'] = df['document'].str.replace("[^a-zA-Z#]", " ") #cleaning<br/>vectors = vectorizer.fit_transform(df['docs']).todense() </span><span id="77c6" class="lg ju hh lc b fi ll li l lj lk">tf_dict={}<br/>for i,k in enumerate(vectors):<br/> a=np.squeeze(np.array(k))<br/> tf_dict[f'Doc{i}']= a</span><span id="e5c2" class="lg ju hh lc b fi ll li l lj lk">documents=pd.DataFrame.from_dict(tf_dict)<br/>documents.index=words<br/>documents.T #Final DataFrame</span></pre><p id="e4cd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">之后，我们会得到一个稀疏矩阵，看起来像这样。</p><figure class="kx ky kz la fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ma"><img src="../Images/08aa3f6860bcc9343e1c123a310493b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0pO_nu9sVYL-gzDDdifVw.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">奇异值分解的数据帧</figcaption></figure><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="1a81" class="lg ju hh lc b fi lh li l lj lk"><strong class="lc hi">### SVD AND VIEWING TOPICS</strong></span><span id="b637" class="lg ju hh lc b fi ll li l lj lk">from scipy.linalg import svd</span><span id="dbc4" class="lg ju hh lc b fi ll li l lj lk">U, s, VT = svd(vectors)</span><span id="fd7a" class="lg ju hh lc b fi ll li l lj lk">def display_topics(model, feature_names, no_top_words):<br/> topic_dict = {}</span><span id="3bc4" class="lg ju hh lc b fi ll li l lj lk">for topic_idx, topic in enumerate(model):</span><span id="db6a" class="lg ju hh lc b fi ll li l lj lk"> topic_dict["Topic %d words" % (topic_idx)]=      ['{}'.format(feature_names[i])</span><span id="478f" class="lg ju hh lc b fi ll li l lj lk">for i in topic.argsort()[:-no_top_words - 1:-1]]</span><span id="f358" class="lg ju hh lc b fi ll li l lj lk"> topic_dict["Topic %d weights" % (topic_idx)]= ['{:.1f}'.format(topic[i])</span><span id="eeaf" class="lg ju hh lc b fi ll li l lj lk"> for i in topic.argsort()[:-no_top_words - 1:-1]]</span><span id="09f2" class="lg ju hh lc b fi ll li l lj lk">return pd.DataFrame(topic_dict)</span></pre><p id="0b7b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">VT将包含我们的<strong class="iw hi"> <em class="js">右奇异向量</em> </strong> <em class="js">。</em></p><figure class="kx ky kz la fd ii er es paragraph-image"><div class="er es mb"><img src="../Images/24e56d50009b584f29daab248ecaf3d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*YFxdfFGHe-EX6ad-HkxsqQ.png"/></div></figure><p id="ae96" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Transpose(A) x A   <em class="js">(原来是对称的)</em>的<strong class="iw hi"> <em class="js">特征向量给了我们正交的右奇异向量。回想一下我们文档的形状，13000 x 1000。你能猜出变形战斗机的形状吗？</em></strong></p><p id="d391" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果检查VT，并根据权重将它们按顶部单词排序，我们会得到对主题最重要的单词。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="0d15" class="lg ju hh lc b fi lh li l lj lk">#Examining the top 10 words<br/>display_topics(VT[1:3],words,10) </span></pre><figure class="kx ky kz la fd ii er es paragraph-image"><div class="er es mc"><img src="../Images/8f294f40e462c21703e941c3590f0c8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*jLwoOtxgLfGDd_0grLT3XA.png"/></div></figure><p id="b848" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以看到，第一个话题似乎是关于计算机，第二个话题似乎是关于体育。</p><p id="521f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们检查一下主题是否像我们期望的那样是正交的。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="ce6b" class="lg ju hh lc b fi lh li l lj lk">np.dot(VT[2:3],VT[:1].T) Multiplying two columns</span><span id="b927" class="lg ju hh lc b fi ll li l lj lk">Output: array([[-1.73472348e-17]])</span></pre><p id="29de" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">很难在一篇博文中清楚地表达出幕后发生了什么。如果你是线性代数的新手，这大部分对你来说可能听起来很奇怪。因此，我将推荐一条线性代数的学习途径，帮助你深入这个主题。</p><h1 id="5b61" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">资源</strong></h1><p id="6a21" class="pw-post-body-paragraph iu iv hh iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr ha bi translated">有三个主要的资源来实现这一点，我建议这样的顺序:</p><ol class=""><li id="c4aa" class="md me hh iw b ix iy jb jc jf mf jj mg jn mh jr mi mj mk ml bi translated"><a class="ae it" href="https://www.youtube.com/watch?v=e50Bj7jn9IQ" rel="noopener ugc nofollow" target="_blank"> 3Blue1Brown线性代数系列</a>:对我在这里提到的概念的一种视觉直觉，也是一个很好的起点。</li><li id="7af8" class="md me hh iw b ix mm jb mn jf mo jj mp jn mq jr mi mj mk ml bi translated">吉尔伯特·斯特朗线性代数讲座:这门课程旨在帮助数据科学家理解线性代数。你也可以得到与讲座配套的书。这里讨论的概念是理论上的，从长远来看，它们肯定会有所帮助。</li><li id="2ff0" class="md me hh iw b ix mm jb mn jf mo jj mp jn mq jr mi mj mk ml bi translated"><a class="ae it" href="https://www.youtube.com/watch?v=8iGzBMboA0I&amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="noopener ugc nofollow" target="_blank">快速人工智能计算线性代数:</a>与Gilbert Strang视频不同，这侧重于概念的实际应用。一旦你从吉尔伯特·斯特朗的视频中学到了所有的理论概念，这些讲座就很容易理解了。</li></ol><p id="73d2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你可以在这里找到帖子<a class="ae it" href="https://github.com/ArnabPushilal/LinAlg/tree/main" rel="noopener ugc nofollow" target="_blank">中提到的所有代码。</a></p></div></div>    
</body>
</html>