<html>
<head>
<title>Neural networks forward propagation deep dive 102</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络前向传播深潜102</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/forward-propagation-deep-dive-102-bbeabe4d2fb2?source=collection_archive---------11-----------------------#2021-07-08">https://medium.com/analytics-vidhya/forward-propagation-deep-dive-102-bbeabe4d2fb2?source=collection_archive---------11-----------------------#2021-07-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/8f5aadfd095287cc57a68ea4769a1d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7XryJtBBZPn25PkKNJWGWQ.jpeg"/></div></div></figure><p id="4afb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">前向传播是神经网络的重要组成部分。这并不像听起来那么难；-)</p><p id="4a09" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是我的神经网络系列的第2部分。如果你只是想要代码，欢迎从第一部分的开始，或者跳到第五部分的<a class="ae jn" href="https://shaun-enslin.medium.com/implementing-neural-networks-in-matlab-105-6b71c5872b3c" rel="noopener"/>。</p><p id="2a65" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，为了执行梯度下降或成本优化，我们需要编写一个成本函数，它执行:</p><ol class=""><li id="f639" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><a class="ae jn" href="https://shaun-enslin.medium.com/forward-propagation-deep-dive-102-bbeabe4d2fb2" rel="noopener">正向传播</a></li><li id="8d65" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><a class="ae jn" href="https://shaun-enslin.medium.com/backward-propagation-deep-dive-103-60390714d2b0" rel="noopener">反向传播</a></li><li id="f0b1" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><a class="ae jn" href="https://shaun-enslin.medium.com/cost-and-gradient-calculation-in-neural-networks-deep-dive-104-2e16f26ce3f3" rel="noopener">计算成本&amp;坡度</a></li></ol><p id="8a14" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">在本文中，我们正在处理(1)正向传播。</strong></p><p id="0e2a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在图1中，我们可以看到去掉了很多细节的网络图。我们将重点介绍第二级的一个单元和第三级的一个单元。这种理解可以复制到所有单位。(ps。一个单位是下面的一个圆圈)</p><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es kc"><img src="../Images/f2a3bb9252da71eff5d170231241bc5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*cLjMA6l8emBWZKm2dnSAtw.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图1:神经网络正向传播</figcaption></figure><p id="f3ef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">我们在正向道具中的目标是计算A1，Z2，A2，Z3 &amp; A3 </strong></p><p id="b4ab" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这样我们就可以看到X特征，见图2，更多数据信息，见<a class="ae jn" href="https://shaun-enslin.medium.com/explaining-neural-networks-101-a36356113cbd" rel="noopener">第1部分</a>。</p><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es kl"><img src="../Images/9bac01d6e18c364023da941a40d968bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*oIPOL2ggLCEwhtSBdZCxIw.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图2</figcaption></figure><h1 id="dbb6" class="km kn hh bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">初始重量(θs)</h1><p id="eef9" class="pw-post-body-paragraph ip iq hh ir b is lk iu iv iw ll iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">事实证明，对于梯度下降来说，这是一个相当重要的话题。如果你没有处理过梯度下降，那么先检查<a class="ae jn" rel="noopener" href="/geekculture/gradient-descent-in-matlab-octave-954160e2d3fa">这篇文章</a>。从上面可以看出，我们需要2组砝码。(用表示)。<em class="lp">我们仍然经常称这些重量为θ，它们的意思是一样的</em>。</p><p id="8ec3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们需要一套2级的thetass和一套3级的theta。每个θ是一个矩阵，大小为(L) * size(L-1)。从而形成上面:<br/>-θ1 = 6×4矩阵<br/>-θ2 = 7×7矩阵</p><p id="69c9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们现在必须猜测哪个初始thetas应该是我们的起点。这里，epsilon来拯救我们，下面是matlab代码，可以轻松地为我们的初始权重生成一些随机的小数字。</p><pre class="kd ke kf kg fd lq lr ls lt aw lu bi"><span id="4033" class="lv kn hh lr b fi lw lx l ly lz">function weights = initializeWeights(inSize, outSize)<br/>  epsilon = 0.12;<br/>  weights = rand(outSize, 1 + inSize) * 2 * epsilon - epsilon;<br/>end</span></pre><p id="a5a2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如上所述，在对每个θ的大小运行上述函数后，我们将得到一些很好的小随机初始值，如图3所示</p><p id="3640" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">。对于上面的图1，我们提到的权重指的是下面矩阵的第1行。</p><figure class="kd ke kf kg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ma"><img src="../Images/376cb2476e8fadbe6a01532c5ca02b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e-U4ktMEdqpYXJfPVpig5g.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图3</figcaption></figure><p id="5150" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们有了初始重量，我们可以继续进行梯度下降。然而，这需要一个成本函数来帮助计算成本和梯度。在我们计算成本之前，我们需要执行正向传播来计算我们的A1、Z2、A2、Z3和A3，如图1所示。</p><p id="faac" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">正向传播</strong></p><p id="5492" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">根据图1，让我们计算A1。你可以看到它几乎是我的X的特征，我们在前面添加了硬编码为“1”的bias列。下面是实现这一点的matlab代码:</p><pre class="kd ke kf kg fd lq lr ls lt aw lu bi"><span id="f2c5" class="lv kn hh lr b fi lw lx l ly lz">a1 = [ones(m, 1) X];</span></pre><p id="fcba" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，结果将为您提供图4中A1的结果。特别注意前面增加的偏差栏“1”。</p><figure class="kd ke kf kg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mb"><img src="../Images/c452cae02d7142402f5eaf9c6bbf865b.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*MZJ7MkfRmJ3e27bXsUIJjg.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图4</figcaption></figure><p id="a1ce" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">很好，A1完成了，让我们继续A2。在我们得到A2之前，我们将首先运行一个假设来计算Z2。一旦你有了假设，你就可以通过sigmoid函数得到A2。同样，根据图1，将bias列添加到前面。</p><pre class="kd ke kf kg fd lq lr ls lt aw lu bi"><span id="fae2" class="lv kn hh lr b fi lw lx l ly lz">z2 = a1*Theta1';<br/>a2 = [ones(size(z2, 1), 1) sigmoid(z2)];</span></pre><p id="a40b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您可以看到下面的sigmoid函数:</p><pre class="kd ke kf kg fd lq lr ls lt aw lu bi"><span id="afb7" class="lv kn hh lr b fi lw lx l ly lz">function g = sigmoid(z)<br/> g = 1.0 ./ (1.0 + exp(-z));<br/>end</span></pre><p id="4424" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您的结果将如图5所示，注意第1列中的偏差。</p><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es mc"><img src="../Images/ed7ee3e1af9b6fe277bcd65dbc2db1ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*8sum3SJ3CkrH09WshrJNkw.png"/></div></figure><p id="ca0c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好了，我们快到了…现在在A3上，让我们做与A2相同的事情，但是这一次，我们不担心添加偏差列。</p><pre class="kd ke kf kg fd lq lr ls lt aw lu bi"><span id="ff90" class="lv kn hh lr b fi lw lx l ly lz">z3 = a2*Theta2';<br/>a3 = sigmoid(z3);</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es md"><img src="../Images/47fb10eab3f6202bddb2db88bd535424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*YTLmgVz_X1mELFrAOV0Dcg.png"/></div></figure><p id="120e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你可能会问，“为什么我们保留Z2 &amp; Z3”。嗯，我们在反向传播中需要这些。所以我们最好把它们放在手边；-).</p><h1 id="76e6" class="km kn hh bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">结论</h1><p id="68f6" class="pw-post-body-paragraph ip iq hh ir b is lk iu iv iw ll iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ha bi translated">所以，我们已经学会了神经网络的前两步，也就是</p><ol class=""><li id="5542" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">初始化我们的重量(θ)</li><li id="24a0" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">执行正向传播</li></ol><p id="43a4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您可以继续阅读我系列的下一篇文章，那就是<a class="ae jn" href="https://shaun-enslin.medium.com/backward-propagation-deep-dive-103-60390714d2b0" rel="noopener">执行反向传播</a>。</p><p id="d14d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="lp">顺便说一句，如果你正在寻找一门关于机器学习的伟大课程，我可以强烈推荐这门</em> <a class="ae jn" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank"> <em class="lp">课程</em> </a> <em class="lp">。</em></p></div></div>    
</body>
</html>