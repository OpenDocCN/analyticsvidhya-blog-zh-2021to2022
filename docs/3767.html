<html>
<head>
<title>Creating ShuffleNet in Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Tensorflow中创建ShuffleNet</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/creating-shufflenet-in-tensorflow-f59123bd555b?source=collection_archive---------1-----------------------#2021-07-25">https://medium.com/analytics-vidhya/creating-shufflenet-in-tensorflow-f59123bd555b?source=collection_archive---------1-----------------------#2021-07-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/c5cf1883ec01f79d379978ba859d567d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*H8HuJ7QI1Spr6nfv"/></div></div></figure><p id="7f7c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Magvi公司(Face++)推出的ShuffleNet是一种计算效率极高的CNN架构，专门为计算能力非常有限(即10-150m flops)的移动设备设计。新的体系结构利用两种操作来实现降低的计算成本，并保持或提高相同的精度——逐点组卷积和信道混洗。ShuffleNet设法在ImageNet分类上获得比MobileNet更低的top-1错误(6.7%)。</p><p id="4897" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">通道混洗是本文的主要亮点，一种新的操作被应用于努力产生更多的特征映射通道，这有助于编码更多的信息并使特征检测更加鲁棒。</p><p id="013d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">ShuffleNet论文链接—<a class="ae jn" href="https://arxiv.org/pdf/1707.01083v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1707.01083v1.pdf</a></p><p id="beb3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在让我们来看看ShuffleNet的构建模块。</p><h2 id="abf4" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">点态群卷积</h2><p id="f059" class="pw-post-body-paragraph ip iq hh ir b is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji kn jk jl jm ha bi translated">在微型网络中，昂贵的逐点卷积导致有限数量的信道来满足复杂性约束，这可能会显著损害准确性。为了解决这个问题，一个简单的解决方案是在1 × 1层上应用通道稀疏连接，例如组卷积。通过确保每个卷积仅在<br/>对应的输入通道组上操作，组卷积显著降低了计算成本。</p><p id="4c0f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是什么是群卷积呢？</p><p id="205a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">AlexNet中引入的组卷积是一种卷积类型，用于将通道分成组，然后在每个组上单独卷积核，然后级联回来。此操作有助于稀疏连接并减少连接数。</p><p id="b367" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里有一个直观的图片，便于理解。</p><figure class="kp kq kr ks fd ii er es paragraph-image"><div class="er es ko"><img src="../Images/cce1b87cabe2222bbeb8d1cf0fe4fa30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ulMYSbtQi3slbRmY.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图3 —具有两个滤波器组的分组卷积</figcaption></figure><p id="0143" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是它有一个缺点——某个组的输出只与该组内的<br/>输入相关。这种特性阻碍了频道组之间的信息流，削弱了代表性。</p><p id="e86c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们在应用组卷积之前混合不同组的输入，以便完全表示从输入到输出的信息流，会怎么样？从而洗牌。</p><h2 id="1806" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated"><strong class="ak">频道洗牌</strong></h2><figure class="kp kq kr ks fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kx"><img src="../Images/a7df5146c2efb02337d3fe6462a242d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Lpf3_QbBQrbm3_zIsHyow.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图3—使用组卷积的信道混洗(来源:ShuffleNet论文)</figcaption></figure><p id="618e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设一个卷积层有g个群，其输出有g × n个信道；我们首先将输出通道的维数整形为(g，n)，<br/>转置，然后将其展平，作为下一层的输入。</p><p id="7c5e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如何理解渠道洗牌？这是一张图表。</p><figure class="kp kq kr ks fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ky"><img src="../Images/6bf66a3698debb21cdc91cf05dbb2584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*clCHyIHXno2RkPbc7iEAeg.jpeg"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图4 —频道随机播放</figcaption></figure><p id="3feb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于组=3，对于3个通道，RGB以这样的方式被分割，即原始RGB被表示为更小的3个代表性RGB通道。红色通道的第一次分离将是第一组的红色通道，第二次分离将是第二组的红色通道，第三次分离将是第三组的红色通道。</p><h2 id="5f95" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">洗牌网单元</h2><figure class="kp kq kr ks fd ii er es paragraph-image"><div class="er es kz"><img src="../Images/5afbb14e34a41a3dd0efd09838d6ad29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*jTYjfO5MS9rALuVORB10sw.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图5-ShuffleNet装置(来源:原始shuffle net文件)</figcaption></figure><p id="d5c9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> a) </strong>部分是具有深度方向卷积的原始瓶颈单元。<strong class="ir hi"> b) </strong>部分是具有逐点组卷积和通道混洗的ShuffleNet的新瓶颈单元。<strong class="ir hi"> c) </strong>部分是跨距=2的ShuffleNet单元。</p><p id="f576" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在让我们看看python代码</p><p id="bc37" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于频道洗牌。首先，我们将信道维度整形为(g，n)，使用置换来形成较小的表示，然后将其整形为原始格式。</p><pre class="kp kq kr ks fd la lb lc ld aw le bi"><span id="b084" class="jo jp hh lb b fi lf lg l lh li"><strong class="lb hi">def</strong> channel_shuffle(x, groups):</span><span id="e4d7" class="jo jp hh lb b fi lj lg l lh li">    _, width, height, channels = x.get_shape().as_list()</span><span id="3a49" class="jo jp hh lb b fi lj lg l lh li">    group_ch = channels // groups</span><span id="623f" class="jo jp hh lb b fi lj lg l lh li">    x = Reshape([width, height, group_ch, groups])(x)</span><span id="d48a" class="jo jp hh lb b fi lj lg l lh li">    x = Permute([1, 2, 4, 3])(x)</span><span id="f71f" class="jo jp hh lb b fi lj lg l lh li">    x = Reshape([width, height, channels])(x)</span><span id="a52a" class="jo jp hh lb b fi lj lg l lh li">    <strong class="lb hi">return</strong> x</span></pre><p id="6f74" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在洗牌机的代码。如果跨距= 2，则瓶颈单元输入和输出被连接，如果跨距= 1，则使用加法功能。</p><pre class="kp kq kr ks fd la lb lc ld aw le bi"><span id="c2a0" class="jo jp hh lb b fi lf lg l lh li"><strong class="lb hi">def</strong> shuffle_unit(x, groups, channels,strides):</span><span id="cf76" class="jo jp hh lb b fi lj lg l lh li">    y = x</span><span id="b847" class="jo jp hh lb b fi lj lg l lh li">    x = Conv2D(channels//4, kernel_size = 1, strides = (1,1),padding = 'same', groups=groups)(x)<br/>    x = BatchNormalization()(x)<br/>    x = ReLU()(x)</span><span id="24c1" class="jo jp hh lb b fi lj lg l lh li">    x = channel_shuffle(x, groups)</span><span id="8d3c" class="jo jp hh lb b fi lj lg l lh li">    x = DepthwiseConv2D(kernel_size = (3,3), strides = strides, padding = 'same')(x)<br/>    x = BatchNormalization()(x)</span><span id="4f89" class="jo jp hh lb b fi lj lg l lh li">    <strong class="lb hi">if</strong> strides == (2,2):<br/>       channels = channels - y.shape[-1]<br/>    x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', groups=groups)(x)<br/>    x = BatchNormalization()(x)<br/>    <br/>    <strong class="lb hi">if</strong> strides ==(1,1):<br/>        x = Add()([x,y])</span><span id="c19d" class="jo jp hh lb b fi lj lg l lh li">    <strong class="lb hi">if</strong> strides == (2,2):  <br/>        y = AvgPool2D((3,3), strides = (2,2), padding = 'same')(y)<br/>        x = concatenate([x,y])</span><span id="33ad" class="jo jp hh lb b fi lj lg l lh li">    x = ReLU()(x)</span><span id="5173" class="jo jp hh lb b fi lj lg l lh li">    <strong class="lb hi">return</strong> x</span></pre><h1 id="0b8b" class="lk jp hh bd jq ll lm ln ju lo lp lq jy lr ls lt kb lu lv lw ke lx ly lz kh ma bi translated">洗牌网架构</h1><figure class="kp kq kr ks fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mb"><img src="../Images/769b95ccd558ad3403439310ebddcf6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhicqgdgLwaiJ9Z9-UsU3Q.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图6 — ShuffleNet架构(来源:原始ShuffleNet论文)</figcaption></figure><p id="9675" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">四个洗牌网单元被放置在架构中。每个单元的输出通道加倍，每个单元的尺寸减半。</p><p id="bb49" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是groups = 2的完整代码—</p><pre class="kp kq kr ks fd la lb lc ld aw le bi"><span id="8de7" class="jo jp hh lb b fi lf lg l lh li"><strong class="lb hi">def</strong> Shuffle_Net(nclasses, start_channels ,input_shape = (224,224,3)):</span><span id="ee7c" class="jo jp hh lb b fi lj lg l lh li">    groups = 2<br/>    input = Input (input_shape)</span><span id="d46e" class="jo jp hh lb b fi lj lg l lh li">    x =  Conv2D (24,kernel_size=3,strides = (2,2), padding = 'same', use_bias = True)(input)<br/>    x =  BatchNormalization()(x)<br/>    x =  ReLU()(x)</span><span id="3633" class="jo jp hh lb b fi lj lg l lh li">    x = MaxPool2D (pool_size=(3,3), strides = 2, padding='same')(x)</span><span id="44a0" class="jo jp hh lb b fi lj lg l lh li">    repetitions = [3,7,3]</span><span id="a7e9" class="jo jp hh lb b fi lj lg l lh li">    <strong class="lb hi">for</strong> i,repetition in enumerate(repetitions):</span><span id="b954" class="jo jp hh lb b fi lj lg l lh li">       channels = start_channels * (2**i)</span><span id="d1e2" class="jo jp hh lb b fi lj lg l lh li">       x  = shuffle_unit(x, groups, channels,strides = (2,2))</span><span id="85ec" class="jo jp hh lb b fi lj lg l lh li">         <strong class="lb hi">for</strong> i in range(repetition):<br/>            x = shuffle_unit(x, groups, channels,strides=(1,1))</span><span id="27a6" class="jo jp hh lb b fi lj lg l lh li">     x = GlobalAveragePooling2D()(x)</span><span id="e3a2" class="jo jp hh lb b fi lj lg l lh li">     output = Dense(n_classes,activation='softmax')(x)</span><span id="8ff1" class="jo jp hh lb b fi lj lg l lh li">     model = Model(input, output)</span><span id="c720" class="jo jp hh lb b fi lj lg l lh li">     <strong class="lb hi">return</strong> model</span></pre><figure class="kp kq kr ks fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mc"><img src="../Images/3c11d6d596193fd7cfbd13e674c24941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ycAqyuwe8VWv9fxOpZfvdQ.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图7 —模型总结</figcaption></figure><p id="85ef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这就是我们如何在TensorFlow中实现ShuffleNet。</p><p id="1f5e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">但是渠道洗牌的效果如何呢？</strong></p><p id="073e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">论文作者做了有/无频道洗牌的比较。</p><figure class="kp kq kr ks fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es md"><img src="../Images/33189ad19a4c54fc6c8fdba0f4b5508b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rrV-TcrlVXwJzRLzKJKkpA.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图8 —对比</figcaption></figure><p id="f0bc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该表比较了具有/不具有信道混洗的混洗网络结构(例如组号被设置为3或8)的性能。评估是在三种不同的复杂程度下进行的。很明显，频道混洗持续提升不同设置的分类分数。</p><p id="b787" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在<a class="ae jn" href="https://github.com/Haikoitoh/paper-implementation" rel="noopener ugc nofollow" target="_blank"> github上检查代码。</a></p></div></div>    
</body>
</html>