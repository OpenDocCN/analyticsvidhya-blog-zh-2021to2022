<html>
<head>
<title>An overview on VQ-VAE: Learning Discrete Representation Space</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">VQ-VAE研究综述:学习离散表征空间</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-overview-on-vq-vae-learning-discrete-representation-space-8b7e56cc6337?source=collection_archive---------11-----------------------#2021-04-03">https://medium.com/analytics-vidhya/an-overview-on-vq-vae-learning-discrete-representation-space-8b7e56cc6337?source=collection_archive---------11-----------------------#2021-04-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="0d5c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">关键概念</h1><p id="1b1c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">本文提出了一种自动编码器，它学习一个<strong class="je hi">离散的</strong>潜在空间，并提出了一种损耗和一种通过本文提出的不可微流水线反向传播的方法。是的，这篇论文提出了用argmin函数离散表示潜在空间的“0或1”。</p><ul class=""><li id="b99d" class="ka kb hh je b jf kc jj kd jn ke jr kf jv kg jz kh ki kj kk bi translated">再次，本文介绍了一个简单的VQ-VAE模型，使用离散的潜在的，因此，没有遭受后验崩溃和方差问题。</li><li id="3f81" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">该论文提出了具有3个分量的损失，其将通过矢量量化(VQ)损失来训练嵌入空间。</li><li id="06af" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">VQ-VAE可与经典的VAE相媲美，具有连续的潜在表现。</li></ul><p id="a731" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">你可能想知道我们如何能像我一样反向传播和训练由离散的、底层变量组成的模型。这似乎是一个与DL的一个非常不同的领域相关的问题:强化学习。让我们深入探讨作者在VQ-VAE学习离散向量的想法。</p><h1 id="21d8" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">VQ-VAE</h1><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/2bdb1ff7a9fd646c542bb013eeaf88e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*miNfFc9qT5PrS7ectJa_kw.png"/></div></div></figure><p id="32a2" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">编码器网络将图像x编码成zₑ(x ),解码器解码矢量zq(x ),目的是重建图像。与经典的VAE不同的是，向量zq(x)是通过嵌入zₑ(x).来计算的我们定义e为形状(K，D)的潜在嵌入空间，这意味着维数为D的K个嵌入。</p><p id="1342" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">如下面第一个等式所示，通过使用共享嵌入空间e的最近邻查找来计算离散潜在z。总之，解码器的输入是eₖ的相应嵌入，其最小化到给定嵌入的L2距离{zₑ(x)-eₖ}。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lf"><img src="../Images/1981f032fb4897975fa10d67d871e386.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*hc2-Z7_2u1OmKBYUuaIxjg.png"/></div></figure><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lg"><img src="../Images/f1f1c42d9d06f473e2ac1d2df437d139.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*0dI2pyZp1_CqK0pxBgiZmQ.png"/></div></figure><p id="1103" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">因此，编码的每个通道都被映射到最近的嵌入向量中。</p><h2 id="3c0d" class="lh if hh bd ig li lj lk ik ll lm ln io jn lo lp is jr lq lr iw jv ls lt ja lu bi translated">反向传播</h2><p id="8921" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这个使用argmin映射编码的推理过程的明显问题是这个过程是不可微分的。总之，梯度∇ᶻL直接从解码器传递到编码器，忽略了嵌入过程。如上图所示。我相信这种怪异的、不那么严格的抽象是通过假设嵌入类似于编码(zₑ(x)≈eₖ)而成为可能的，因为这在嵌入的训练损失中是强制的，我们将很快讨论这一点。</p><h1 id="1754" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">培训损失</h1><p id="40cb" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">建议的培训损失由三部分组成。我们将按顺序讨论每个部分。sg表示<a class="ae lv" href="https://www.tensorflow.org/api_docs/python/tf/stop_gradient" rel="noopener ugc nofollow" target="_blank">停止梯度操作</a>，不进行梯度计算输出相同的值。</p><ol class=""><li id="d699" class="ka kb hh je b jf kc jj kd jn ke jr kf jv kg jz lw ki kj kk bi translated">第一项是重建损失，它优化了编码器和解码器。然而，由于梯度反向传播中的跳跃，嵌入e没有接收到这种损失的梯度。</li><li id="10d2" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz lw ki kj kk bi translated">第二项是VQ损耗，它将嵌入向量移向编码器输出zₑ(x，用L2损耗测量。</li><li id="bf9f" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz lw ki kj kk bi translated">最后一个组件确保编码不会任意增长，并且编码器只进行一次嵌入。</li></ol><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lx"><img src="../Images/23e694a214a5ab8817dea7f3bb288b68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*rbYXuBzEA6DIx6drlq3fRA.png"/></div><figcaption class="ly lz et er es ma mb bd b be z dx translated">最终训练目标(损失函数)</figcaption></figure><h1 id="a693" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mc"><img src="../Images/4a4a731647fb4c49e149c0fb45a22128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sEACw2A0Es7m9Fht1Cu5Cw.png"/></div></div><figcaption class="ly lz et er es ma mb bd b be z dx translated">左:ImageNet 128x128x3图像，右:用32x32x1潜在空间重建的VQ-VAE，K=512。</figcaption></figure><p id="25aa" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">上图是在ImageNet数据集上训练的VQ-VAE的一些重建图像的示例。VAE、VQ-VAE和VIMCO型号分别获得4.51位/dim、4.67位/dim和5.14位/dim。描述了音频和视频数据重建的实验和模型结构。</p><p id="bf64" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">原文:<a class="ae lv" href="https://arxiv.org/abs/1711.00937" rel="noopener ugc nofollow" target="_blank">神经离散表示学习</a></p><p id="0a81" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">*令人惊讶的是，<a class="ae lv" href="https://www.quora.com/Why-is-there-no-character-for-superscript-q-in-Unicode" rel="noopener ugc nofollow" target="_blank">下标q(小q)在Unicod </a> e中并不存在！因此，我不得不使用常规的q符号。</p></div></div>    
</body>
</html>