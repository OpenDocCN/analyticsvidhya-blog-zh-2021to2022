<html>
<head>
<title>What is Eigen Game?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是Eigen Game？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-is-eigen-game-d6dda6c980b1?source=collection_archive---------3-----------------------#2021-06-22">https://medium.com/analytics-vidhya/what-is-eigen-game-d6dda6c980b1?source=collection_archive---------3-----------------------#2021-06-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div class="er es hf"><img src="../Images/daceaf0623b60cb37b09533044bb083f.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*D5GJjbc7JMsaZ4zpke_w4w.jpeg"/></div></figure><div class=""/><blockquote class="il im in"><p id="a5d1" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hp">这篇文章的目的是让读者详细了解主成分分析作为一种纳什均衡，并使用Sklearn的Mnist数据进行必要的数学证明和实现</strong></p><p id="b996" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hp">伊恩·金普、布莱恩·麦克威廉姆斯、克莱尔·弗纳德的原创作品&amp;托雷·格雷佩尔@DeepMind。在ICLR 2021(国际学习表示会议)上以“特征博弈:作为纳什均衡的PCA”的形式发表:</strong></p></blockquote><div class="hg hh ez fb hi jn"><a href="https://arxiv.org/abs/2010.00554" rel="noopener  ugc nofollow" target="_blank"><div class="jo ab dw"><div class="jp ab jq cl cj jr"><h2 class="bd hp fi z dy js ea eb jt ed ef hn bi translated">本征博弈:作为纳什均衡的PCA</h2><div class="ju l"><h3 class="bd b fi z dy js ea eb jt ed ef dx translated">我们提出了一个新颖的观点，即主成分分析(PCA)是一个竞争游戏，其中每个近似…</h3></div><div class="jv l"><p class="bd b fp z dy js ea eb jt ed ef dx translated">arxiv.org</p></div></div><div class="jw l"><div class="jx l jy jz ka jw kb hk jn"/></div></div></a></div><p id="92be" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">所有的符号和数字都是从论文本身借来的，以防读者想要浏览论文中提出的所有证明。</strong></p><h1 id="d280" class="kf kg ho bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">博客的流量</strong></h1><blockquote class="il im in"><p id="d0ba" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hp"> 1。概述主成分分析和纳什均衡。</strong></p><p id="fdfb" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hp"> 2。将PCA伪装成一个特征游戏。</strong></p><p id="9919" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hp"> 3。PCA解是唯一的严格纳什均衡</strong></p><p id="7040" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hp"> 4。算法</strong></p><p id="7eeb" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hp"> 5。Python代码和测试</strong></p><p id="dbeb" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hp"> 6。纳什证明</strong></p><p id="a655" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">7。本征博弈的收敛性</p><p id="2cfb" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">8。收敛证明</p><p id="a23b" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">9。结论</p></blockquote><h1 id="8da6" class="kf kg ho bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">1.让我们回顾一下主成分分析和纳什均衡。</h1><h1 id="57e9" class="kf kg ho bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">纳什均衡概述。</strong></h1><p id="a808" class="pw-post-body-paragraph io ip ho ir b is ld iu iv iw le iy iz kc lf jc jd kd lg jg jh ke lh jk jl jm ha bi translated">在<a class="ae li" href="https://en.wikipedia.org/wiki/Game_theory" rel="noopener ugc nofollow" target="_blank">博弈论</a>中，以数学家<a class="ae li" href="https://en.wikipedia.org/wiki/John_Forbes_Nash_Jr." rel="noopener ugc nofollow" target="_blank">约翰·福布斯·纳什二世</a>命名的<strong class="ir hp">纳什均衡</strong>，是定义涉及两个或更多玩家的<a class="ae li" href="https://en.wikipedia.org/wiki/Non-cooperative_game" rel="noopener ugc nofollow" target="_blank">非合作博弈</a>的<a class="ae li" href="https://en.wikipedia.org/wiki/Solution_concept" rel="noopener ugc nofollow" target="_blank">解</a>的最常见方式。在纳什均衡中，每个参与者都知道其他参与者的均衡策略，没有人会因为只改变自己的策略而获益。纳什均衡的原理可以追溯到古诺的时代，他把它应用于竞争公司选择产量。</p><p id="65d5" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">如果每个玩家都选择了<a class="ae li" href="https://en.wikipedia.org/wiki/Strategy_(game_theory)" rel="noopener ugc nofollow" target="_blank">一个策略</a>——一个根据游戏中目前发生的事情选择他们自己行动的行动计划——并且没有玩家可以通过改变他们的策略来增加他们自己的预期收益，而其他玩家保持他们的策略不变，那么当前的策略选择集就构成了纳什均衡。</p><p id="58d6" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">如果两个参与者<a class="ae li" href="https://en.wikipedia.org/wiki/Alice_and_Bob" rel="noopener ugc nofollow" target="_blank">爱丽丝和鲍勃</a>选择策略A和B，(A，B)是一个纳什均衡，如果爱丽丝没有比A更好的策略来最大化她对鲍勃选择B的收益，鲍勃也没有比B更好的策略来最大化他对爱丽丝选择A的收益。在卡罗尔和丹也是参与者的博弈中，(A，B，C， D)是纳什均衡，如果A是爱丽丝对(B，C，D)的最佳对策，B是鲍勃对(A，C，D)的最佳对策，依此类推。</p><h2 id="22fb" class="lj kg ho bd kh lk ll lm kl ln lo lp kp kc lq lr kt kd ls lt kx ke lu lv lb lw bi translated">竞赛游戏示例</h2><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es lx"><img src="../Images/525d21e18fd9f9d178476c16fbd2daff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*HYYJgl5MbrO4eRZ0p6txvg.png"/></div></figure><p id="fc5f" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">这可以通过一个两人游戏来说明，在这个游戏中，两个玩家同时从0到3中选择一个整数，并且他们都赢得两个数字中较小的一个。此外，如果一个玩家选择的数字比另一个大，那么他们必须让出两分给另一个玩家。</p><p id="07ba" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">这个博弈有一个独特的纯策略纳什均衡:双方都选择0(用浅红色突出显示)。任何其他的策略都可以通过一个玩家将他们的号码比另一个玩家的号码少一个来改进。在邻桌中，如果游戏从绿色方块开始，移动到紫色方块符合玩家1的利益，移动到蓝色方块符合玩家2的利益。虽然这不符合竞赛游戏的定义，但如果游戏被修改，如果两个玩家都选择相同的数字，他们将赢得指定的金额，否则将一无所获，则有4个纳什均衡:(0，0)、(1，1)、(2，2)和(3，3)。</p><p id="3aff" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">因此，与纳什均衡相反，这里将主成分分析作为一个竞争游戏，其中每个近似特征向量由一个参与者控制，其目标是最大化他们自己的效用函数。</strong></p><p id="98e1" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">主成分分析概述。</strong></p><blockquote class="il im in"><p id="5b45" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">数据的<strong class="ir hp">主成分</strong>是与最大方差方向对齐的向量。</p><p id="0591" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这些有两个主要目的</p><p id="e650" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">作为可解释的特征</p><p id="9518" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于数据压缩</p></blockquote><p id="5dcd" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">主成分分析(PCA) </strong>的主要思想是降低由大量相关变量组成的数据集的维度，同时保留数据集中存在的最大可能变化。</p><p id="f8e1" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">让我们定义一个对称矩阵a。</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es mc"><img src="../Images/d5e6e38d3c16243cf5ed3499166f62b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/format:webp/1*rXHsMGF_tSmGror5iP6mfg.png"/></div></figure><p id="10ec" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">其中X是独立变量的m×n矩阵，m是列数，n是数据点数。矩阵A可以分解成以下形式</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es md"><img src="../Images/8ff4d175896508d873d070274b780ee8.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*LRhL_lvg3qIUysgjHMXJSA.png"/></div></figure><p id="ad13" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">其中D是对角矩阵，E是按列排列的A的特征向量矩阵。</p><p id="2136" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">x的主成分(PCs)是XX <strong class="ir hp"> ᵀ </strong>的特征向量，这表明特征向量/主成分的方向取决于独立变量(x)的变化。</p><p id="e2a8" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">因此，<strong class="ir hp">主成分分析(PCA) </strong>集中于明确地陈述目标，以找到捕捉最大方差的<em class="iq"> k </em>维子空间，并将在该子空间内旋转的问题留给例如更有效的下游奇异值(SVD)分解</p><p id="551a" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">例如，任何一对二维正交向量都跨越整个ℝ，因此可以获取任何二维数据集的最大方差。然而，为了使这些向量成为主分量，它们还必须与最大方差的方向对齐，这取决于数据的协方差。通过学习最优子空间，而不是主成分本身，聚焦于子空间误差的目标忽略了PCA的第一个目的。</p><p id="332a" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">不切实际的是，计算完整SVD的成本与0(min{nd，n d})时间和0( <em class="iq"> nd </em>)空间成比例。</p><p id="70c1" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">与这些方法相反，我们将每个主成分(等价地，特征向量)视为游戏中的参与者，其目标是在与其他向量的受控竞争中最大化它们的局部效用函数。</p><h1 id="6792" class="kf kg ho bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak"> 2。将PCA伪装成特征游戏</strong></h1><p id="e958" class="pw-post-body-paragraph io ip ho ir b is ld iu iv iw le iy iz kc lf jc jd kd lg jg jh ke lh jk jl jm ha bi translated">我们将使用的符号为:</p><blockquote class="il im in"><p id="7d27" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">旨在近似主分量的向量和矩阵(相当于特征向量)用帽子来表示。V̂和v̂，而真正的主成分是<em class="ho"> v </em>和<em class="ho"> V </em></p><p id="9aad" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下标表示向量与哪个特征值相关。例如，vi是第<em class="ho"> i </em>个最大特征向量。这里，我们将假设每个特征值是不同的。</p><p id="141e" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">vj <i refers="" to="" set="" of="" vectors="" and="" are="" also="" referred="" as="" the="" parent="" vi="" is="" their="" child=""/></p><p id="7431" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Sums over indices should be clear from context</p></blockquote><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es me"><img src="../Images/2a253ecaa55e8d1062879db013b65bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*ohXm-usxmZnNe8DTUtXIsA.png"/></div></figure><p id="0ae3" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">5. The Euclidean inner product is written <em class="iq"> (u，v) </em> = <em class="iq"> uTv (T表示转置)</em></p><p id="5167" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">6.在<em class="iq"> d </em>维环境空间中，我们用<em class="iq">Sd—</em>1表示单位球，用∈<em class="iq">d—</em>1表示单形</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mf"><img src="../Images/19d649680450a223f46a42ad1121dc8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MCN8gdvFQXtpZoTm"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated">图片借用自<a class="ae li" href="https://deepmind.com/blog/article/EigenGame" rel="noopener ugc nofollow" target="_blank"> Deepmind </a></figcaption></figure><blockquote class="il im in"><p id="b18b" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在特征游戏中，每个玩家控制一个特征向量。玩家通过解释数据中的差异来增加他们的分数，但如果他们与其他玩家过于接近，就会受到惩罚。我们还建立了一个等级制度:参与人1只关心方差的最大化，而其他参与人也必须担心最小化他们与等级制度中比他们高的参与人的联盟。这种奖励和惩罚的结合定义了每个玩家的效用。</p></blockquote><h1 id="2350" class="kf kg ho bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">我们开始吧</strong></h1><p id="ef65" class="pw-post-body-paragraph io ip ho ir b is ld iu iv iw le iy iz kc lf jc jd kd lg jg jh ke lh jk jl jm ha bi translated"><strong class="ir hp">框架的推导</strong></p><p id="ee6d" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">如引言中所述，PCA问题通常被误解为学习将数据投影到捕捉最大方差(Equiv)的子空间中。最大化下面介绍的合适矩阵R的轨迹)。这与学习主成分的最初目标相反。我们首先通过(I)表明仅最大化R的迹不足以恢复所有主分量(Equiv ),来产生确定我们的效用函数的直觉。特征向量)，以及(ii)表明降低R中的非对角项是最大化迹的补充意图，并且可以恢复所有分量。然后，我们承认只学习前k名，并构建与(I)和(ii)中的发现一致的效用，等于我们构建的博弈的Nash的真实特征值，并在响应分析的博弈中成功。</p><p id="9648" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">播放器实用程序的来源</strong></p><p id="3888" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">对称矩阵的特征值问题</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es mo"><img src="../Images/c22f899de4861aefc46386d0283f057b.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*luigxqyyIJoSPGfU8LaWkg.png"/></div></figure><p id="ae78" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">找到d个正交列向量V的矩阵(意味着V是满秩的),使得MV = Vλ，对角线为λ。给定这个问题的解决方案，V的列被称为特征向量，λ中的对应条目是特征值。通过左乘V .T(转置)并调用</p><p id="62bf" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">通过正交归一(即，V是酉的)，我们可以将等式改写为</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mp"><img src="../Images/926e8bfb2fe3dc152681e1426c1c8c44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xmmunKb9tzzhTT8-0V667g.png"/></div></div></figure><p id="cd60" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">让V-表示真实特征向量V的猜测或估计，并定义</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es mq"><img src="../Images/1760189f93c5d702a7abcbb4c9a462f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*9OAo-AlnKHxltvJWA25KJg.png"/></div></figure><p id="e1aa" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">主成分分析问题通常被认为是最大化R (equiv。最小化重建误差):</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mr"><img src="../Images/9d897d3f29249365b54524c59cfbd530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X75OzPCXRHTwRE3ZKrnmYw.png"/></div></div></figure><p id="8b28" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">注</strong>:</p><p id="f802" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">令人惊讶的是,( 2)中的目标与vˇ无关，因此不能用于恢复M-(I)的所有(即k = d)特征向量。或者，等式(1)暗示特征值问题可以表述为确保R的所有非对角项为零，从而确保R是对角的— (ii):</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es ms"><img src="../Images/22f7ca24b8c2b36c149c09c10db417fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5X0zNy0eIUjEJy2dnc2cg.png"/></div></div></figure><p id="811e" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">值得进一步详细检查R的条目。对角元素Rii = { vˇI，mvˇI }被认为是瑞利商，因为| | vˇI | | = 1。非对角元素Rij = { vˇI，mvˇj }测量广义内积{，}M下vˇI和vˇj之间的对齐</p><p id="8342" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">每个玩家I的效用函数依赖于它的双亲，这里用有向无环图表示。每个父节点必须以固定的顺序广播它的向量“位置”。</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es mt"><img src="../Images/1f3e12824c24dc115a35acdc76697e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*JPpJp6XJAU32ixGxYt6AtQ.png"/></div></figure><p id="c397" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">到目前为止，我们已经考虑学习所有的特征向量。如果我们用k &lt; d, then by Equation (1), R must still be diagonal. V is not square, so</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es mu"><img src="../Images/699680753a3a9cb5448c8891dfffdabf.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*qea1c-JBrA0PjNV606cWfQ.png"/></div></figure><p id="6a70" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">but assuming V is orthonormal as before, we have</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es mv"><img src="../Images/7f1dafc62025b234da095541b2066575.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*kde_37B5Hu8uIbvDWbFFZQ.png"/></div></figure><p id="4070" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">Where P is a projection matrix. Left-multiplying Equation (1) by V now reads as</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es mw"><img src="../Images/225b89db51c42b16a26381d4d0cbeadc.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*GnkXIP7sbMtwCjdXnGGf3Q.png"/></div></figure><p id="8d86" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">so we are solving an eigenvalue problem for a subspace of M.</p><p id="69a2" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">If we only aspire to the top-k eigenvectors, maximizing the trace backs learning a subspace spanned by the top-k eigenvectors, but does not recover the eigenvectors themselves. On the other hand, Equation (3) places no preference on recovering large over small eigenvectors but does enforce the columns of Vˆ to be eigenvectors. The preceding exercise is intended to introduce minimizing the off-diagonal terms of R as a possible complementary objective for solving top-k PCA. Next, we will use these two objectives to construct utility functions for each eigenvector vˆi.</p><p id="a7bb" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">We canto consolidate the objectives to take advantage of both their strengths. Proposing as:</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es mx"><img src="../Images/018f797817b5b1e241fc06f28d12170d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*7IDvrV6npmQ7EeWJxPANQA.png"/></div></figure><p id="3997" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">We can observe that this objective ignores the natural hierarchy of the top-k eigenvectors. For example, vˆ1 is penalized for aligning with vˆk and vice versa.</p><p id="2bea" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">vˆ1, being the estimate of the largest eigenvector, should be free to search for the direction that captures the most variance independent of the locations of the other vectors.</p><p id="576d" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">Instead, first consider solving for the top-1 eigenvector “vˆ1” in which case R = [{vˆ1, Mvˆ1}] is a 1 × 1 matrix. In this setting, Equation (3) is not applicable because there are no off-diagonal elements.</p><p id="7bc3" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">So,</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es my"><img src="../Images/9502cb9bb59cd0f6450052e64e8e4e12.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*NXozQMnzYGRmaM0M8CMU_Q.png"/></div></figure><p id="c892" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">is a sensible utility function for vˆ1.</p><p id="0b1d" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">If considering the top-2 eigenvectors, vˆ1’s utility remains as before, and we advance a new utility for vˆ2. Equation (3) is now applicable, so vˆ2’s utility is</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mz"><img src="../Images/4c5635c90ce4b2a6f87e0ed248ecbbd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XIac6RcCgxlRRYgZqLLWyQ.png"/></div></div></figure><p id="9822" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">where we have divided the off-diagonal penalty by {v1, Mv1} so</p><ol class=""><li id="5c38" class="na nb ho ir b is it iw ix kc nc kd nd ke ne jm nf ng nh ni bi translated">the two terms in Equation (5) are on a similar scale</li><li id="1985" class="na nb ho ir b is nj iw nk kc nl kd nm ke nn jm nf ng nh ni bi translated">for reasons that ease analysis.</li></ol><p id="6953" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">重复前k个特征向量的逻辑，注意</strong>:</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es no"><img src="../Images/98f044264e0901437b132155cb5e00dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GSHWiCqGcOYrzCibPi6Vzg.jpeg"/></div></div></figure><p id="e64d" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">所以对于每个向量i ∈ {1，.。。，k}我们有</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es np"><img src="../Images/bc8303707a996305a25af94c4cfcc97d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vg59SkYDMWDIeSsUTR6Ebw.png"/></div></div></figure><p id="0840" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">其中符号ui(ai |b)强调参与人I调整ai以最大化b的效用。</p><p id="e847" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">有趣的是，通过包括自然层次的知识(参见非循环图)，我们立即被引导去构造不对称的效用，并因此受到启发，将PCA问题公式化为一个游戏，而不是像等式(4)中的直接优化问题。</p><p id="92ec" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">博弈中的一个关键概念是纳什均衡。纳什均衡为每个参与者指定了一个变量，任何参与者都不能单方面偏离这个变量来改善他们的结果。在这种情况下，vˇ是一个(严格)纳什均衡当且仅当对于所有I，</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es nq"><img src="../Images/478c4a4d253be8d90d3321d664fdce19.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*lzS_ZKYlkCsPIs5Nt92sZQ.png"/></div></figure><h1 id="b3ba" class="kf kg ho bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak"> 3。PCA解是唯一的严格纳什均衡</strong></h1><p id="3d8c" class="pw-post-body-paragraph io ip ho ir b is ld iu iv iw le iy iz kc lf jc jd kd lg jg jh ke lh jk jl jm ha bi translated">假设{X(转置)X}的前k个特征值是正的且不同的。然后，前k个特征向量形成等式(6)中提出的博弈的唯一严格纳什均衡。3证明将在后面的tis部分显示</p><p id="df64" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">该游戏是分层的，每个玩家的效用仅取决于其父母，可以通过依次解决每个玩家的优化问题来构造收敛的顺序算法。</p><p id="1a78" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">已经提到，对等式(5)中的惩罚项进行归一化的动机不仅仅是缩放。除以{ vˇj，mvˇj }得到参与人I的梯度如下</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es nr"><img src="../Images/c5d97eb925d2b37d152f1b2bc18165c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hLBYXHoliKaSQQIsbtt8jg.png"/></div></div></figure><p id="77be" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">顺序算法。每个特征向量可以通过最大化其效用来学习。向量被约束到单位球，非凸黎曼流形，因此我们可以使用黎曼梯度上升，其梯度由等式(7)给出。在这种情况下，黎曼优化理论只需要一个中间步骤，将梯度∇vˆi投影到球面的切空间，以计算黎曼梯度∇r vˇI。<strong class="ir hp">回想一下，每个ui都依赖于vˇj</strong></p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es ns"><img src="../Images/de29ba76c06b304680782930501a838b.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*k3VePqzhxgymn_Y82u45RQ.png"/></div></figure><p id="688d" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">具有归一化惩罚项的结果梯度具有直观的意义。它包括一个广义的Gram-Schmidt步骤，然后是幂迭代中的标准矩阵乘积和Oja规则。</p><p id="9747" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp"> Oja规则</strong>，以芬兰计算机科学家<a class="ae li" href="https://en.wikipedia.org/wiki/Erkki_Oja" rel="noopener ugc nofollow" target="_blank">埃尔基·Oja</a>命名，是大脑或<a class="ae li" href="https://en.wikipedia.org/wiki/Artificial_neural_networks" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>中的神经元如何随着时间的推移改变连接强度或学习的模型。它是标准Hebb规则(参见<a class="ae li" href="https://en.wikipedia.org/wiki/Hebbian_learning" rel="noopener ugc nofollow" target="_blank"> Hebbian学习</a>)的修改，通过乘法归一化，解决所有稳定性问题，并生成用于<a class="ae li" href="https://en.wikipedia.org/wiki/Principal_components_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析</a>的算法</p><p id="5998" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">有关Oja规则的更多详情，请参考</strong><a class="ae li" href="https://en.wikipedia.org/wiki/Oja%27s_rule" rel="noopener ugc nofollow" target="_blank"><strong class="ir hp">https://en.wikipedia.org/wiki/Oja%27s_rule</strong></a></p><h1 id="a0ac" class="kf kg ho bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">4.算法</h1><figure class="ly lz ma mb fd hj er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es nt"><img src="../Images/c76e65fe914af09890ee251bb025bbf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XtPI5KE-5wVOdDQL26kYdw.jpeg"/></div></div></figure><h1 id="a1b2" class="kf kg ho bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak"> 5。Python实现和测试</strong></h1><ol class=""><li id="0621" class="na nb ho ir b is ld iw le kc nu kd nv ke nw jm nf ng nh ni bi translated"><strong class="ir hp">算法1从第一篇论文开始。使用Mnist是因为为什么不使用</strong></li><li id="3dc1" class="na nb ho ir b is nj iw nk kc nl kd nm ke nn jm nf ng nh ni bi translated"><strong class="ir hp">算法1返回一组正交特征向量</strong></li></ol><p id="ddcf" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">导入所需的包</strong></p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es nx"><img src="../Images/766468cdbb446724a5335d76d17dc55f.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*RB0FG2J2xHCJxJwi70ne0A.png"/></div></figure><p id="1e60" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">准备好数据</strong></p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es ny"><img src="../Images/17f02c12b45f828766f605d94cb0cba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*P5Nl4LDKga4rOKqe2xltXg.png"/></div></figure><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es nx"><img src="../Images/8af3227b420f02d29f9f313c419fb1ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*aItdF49aEeDJ0ZvfI2DhGQ.png"/></div></figure><p id="2985" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">v _ I的初始化</strong></p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es nz"><img src="../Images/aef5a9210b8c9b3a51f5ee4819100a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*RFNTIA7M8uJ0b0vW55m3JQ.png"/></div></figure><p id="1085" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">使算法1起作用:-) </strong></p><figure class="ly lz ma mb fd hj er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es oa"><img src="../Images/82c5c2212b0df46cd148271e64dbb6dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r3bwVp5LGDGckMRBg-qYvw.png"/></div></div></figure><figure class="ly lz ma mb fd hj er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es ob"><img src="../Images/b7027e6ae8d78c03284d37ead445086a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5giHG_Yc7_r_aGGq1V3X2Q.png"/></div></div></figure><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es oc"><img src="../Images/4aaae8a9d6855446c31cca2a4c96bb4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*pbvKNG9dPuB6MAmg0ZO8Dw.png"/></div></figure><p id="6532" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">测试正交性</strong></p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es od"><img src="../Images/690d8c3fb52fd318e61bed1b3967d209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*2kHpkUjvAvkHnsgnTiPSvg.png"/></div></figure><p id="b646" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">如果你热爱数学和游戏，请随意使用代码</strong></p><p id="3690" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">你可以在这里找到代码:</strong></p><p id="3972" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><a class="ae li" href="https://github.com/Khanamin-XOR" rel="noopener ugc nofollow" target="_blank">https://github.com/Khanamin-XOR</a></p><h1 id="99f9" class="kf kg ho bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">6.纳什证明</h1><p id="65f0" class="pw-post-body-paragraph io ip ho ir b is ld iu iv iw le iy iz kc lf jc jd kd lg jg jh ke lh jk jl jm ha bi translated">设vˇ是任意单位长度列向量(vˇj)的矩阵，M(对称)对角化为UλU &gt;且U是酉矩阵。然后，</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es oe"><img src="../Images/af40e3f074aa1140a19b51dbc7ecae22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*Bu5aNWgaBrfRnNIEMeQd3w.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd kh"> 1 </strong></figcaption></figure><p id="8476" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">其中Z也是单位长度列向量的矩阵，因为酉矩阵保持内积</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es of"><img src="../Images/26a57dc7b8bc22d55b88b2a6bede52a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*ADG5ZMvGN6yUrgDTeDP_vg.png"/></div></figure><p id="bd22" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">因此，与其考虑任意矩阵vˇ对M的作用，不如考虑任意矩阵Z对λ的作用。这简化了分析。</p><p id="952f" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">假设{X.transpose，X}的前k个特征值是正的且不同的。然后，前k个特征向量形成方程中所提出的博弈的唯一严格纳什均衡</p><p id="a314" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">在下文中，设p，q = {1，.。。，d}和i ∈ {1，.。。，k}。我们将用归纳法证明vi的最优性</p><p id="2c24" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">显然，v1是u1的最佳值，因为</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es og"><img src="../Images/f117576fe915cffa3c81fff1f4e28ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*HIy6aj85JugDyZJwVdvjlw.png"/></div></figure><p id="50b7" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">λ11是瑞利商，已知最大特征值最大。</p><p id="2e3e" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">现在，考虑一下</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es oh"><img src="../Images/955ed55660fc8c78b6bfbbea6cbdc251.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*f992xiWqPmPPpDC_mLH7sA.png"/></div></figure><p id="5c17" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">作为真实特征向量的线性组合。为了确保| | vˇI | | = 1，我们要求||w|| = 1。然后，</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es oi"><img src="../Images/dbfbcff16b1ed64ea00cc10b06cf679d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*amQ7T2a-7rUSBhwi9E9JFw.png"/></div></div></figure><p id="02c3" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">在哪里，</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es oj"><img src="../Images/8a22e2c74e2afb0636dc309dfb2fe42a.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*TK56LgD7o4SZQafdQ1-BEw.png"/></div></figure><p id="0a6c" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">这是单纯形上的线性优化问题。对于不同的</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es ok"><img src="../Images/834cf2fa4ac922f9f3b2cc7478fd0e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*_WscyZKp-2UBTeTjAIx6TQ.png"/></div></figure><p id="33b8" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">ei是唯一的。假设每个参与人I都参与了ei。任何单方面偏离ej的参与者j都会严格降低其效用，因此，由于z∫= ei = w 2 I，纳什均衡在符号变化之前是唯一的，这是意料之中的，因为vi和vi都是主成分。</p><h1 id="e568" class="kf kg ho bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">7.特征对策的收敛性</h1><p id="59b0" class="pw-post-body-paragraph io ip ho ir b is ld iu iv iw le iy iz kc lf jc jd kd lg jg jh ke lh jk jl jm ha bi translated">这里，首先表明等式(6)具有简单的形式，使得u_i的任何局部最大值也是全局最大值。参与人I的效用取决于它的父母，所以我们接下来解释父母的错误是如何通过参与人I效用的错误设定传播给孩子的。使用第一个结果并考虑这个误差，然后我们能够通过利用最近的非凸黎曼优化理论，在全批次设置中给出全局的有限样本收敛保证</p><p id="e415" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">等式(6)是深奥的，但是我们证明了在vˇI与最优值的角度偏差中，参与人I的效用的形状是简单的正弦曲线。正弦曲线的振幅随着沿着单位球面的角度偏差的方向而变化，并且依赖于玩家j的准确度</p><p id="ecfd" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">。不失一般性，设vˇI = cos(θI)VI+sin(θI)⇼I。然后，</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es ol"><img src="../Images/50433335f88c3ee258027a23df440068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*wsTLDdEKL8RVc2Tiy6JkHg.png"/></div></figure><p id="d7c5" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">其中，θi为角度偏差，z∈D1表示偏差方向。注意，sin(square)的周期为π，而不是2π，这只是反映了vi和vi都是本征向量这一事实。</p><p id="baaf" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">误差传播分析表明，以给定的准确度学习双亲是至关重要的。vi和具有近似双亲的参与人I的效用的最大值之间的角距离具有tan-1依赖性</p><p id="dd9f" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">算法1实现了前k个主分量的有限样本收敛到θtol角误差内，而与初始化无关。此外，如果每个vˇI初始化至vi的π 4范围内，算法1会返回T中角度误差小于θtol的分量，其中T为</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es om"><img src="../Images/9127084711c3a4214a5f3d1eddfe6110.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*jQWiE1ETo0-RkH5ssDtfbQ.png"/></div></figure><p id="e886" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">角度误差定义为vˇI和vi之间的角度:θ，由</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es on"><img src="../Images/9abfc83d4404b5fb53fff19e7295b254.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*vVpKmJqMW7429TmQfad2tQ.png"/></div></figure><p id="b3fd" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">T公式中的第一个k来自学习每个VJ所需迭代次数的最差情况界限的简单求和，常数16来自误差传播分析；父向量vˇj必须被学习到低于标准误差阈值的1/16</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es oo"><img src="../Images/f6f014057869ac89ab71261e53be40c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:188/format:webp/1*Id1OgU0ulKu47X7kVK3Bgg.png"/></div></figure><p id="ab9f" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">对于gi所在的子VI</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es op"><img src="../Images/7dbf6c696f4e55bad40c52e54540c51c.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*MUfEXdTL9TVjSDOuYki5bQ.png"/></div></figure><p id="c967" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">我们利用的黎曼优化理论规定，需要1/ρ*p次迭代来满足O(ρ)误差阈值。这就是这里出现误差阈值平方倒数的原因。</p><h1 id="0912" class="kf kg ho bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak"> 8。收敛证明</strong></h1><p id="1af9" class="pw-post-body-paragraph io ip ho ir b is ld iu iv iw le iy iz kc lf jc jd kd lg jg jh ke lh jk jl jm ha bi translated"><strong class="ir hp">非凸黎曼最优化理论</strong></p><p id="af59" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp"> <em class="iq">假设</em> </strong>:</p><p id="611f" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">1 .存在f∞使得f(x)≥f∞对于所有的x ∈ M</p><p id="e293" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">2.存在ξ，ξ0 &gt; 0，使得对于所有k ≥ 0，f(xk)—f(xk+1)≥min(ξ||∇rf(xk)||，ξ0 )||∇Rf(xk)||.</p><p id="cd9a" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">在这两个假设下，一般的黎曼下降(下面的算法)返回x ∈ M最多满足f(x) ≤ f(x0)和||∇Rf(x)|| ≤ ρ in</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div class="er es oq"><img src="../Images/fd49da097c42f47cdfb1815f1a27671d.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*V7kYHDl4aGFGe1JzTEnQ4A.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd kh">迭代次数</strong></figcaption></figure><p id="449c" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">通用黎曼下降算法</p><figure class="ly lz ma mb fd hj er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es or"><img src="../Images/ccf973dd259d9c99c0c6de43934b153c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IkQO-ft4p2qdTcW4Z6B4dw.jpeg"/></div></div></figure><p id="9b99" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">注意:EIGENGAME的收敛证明可以在提供的链接中找到。</strong></p><p id="c1aa" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">参考第31页第0.2节</strong></p><div class="hg hh ez fb hi jn"><a href="https://arxiv.org/abs/2010.00554" rel="noopener  ugc nofollow" target="_blank"><div class="jo ab dw"><div class="jp ab jq cl cj jr"><h2 class="bd hp fi z dy js ea eb jt ed ef hn bi translated">本征博弈:作为纳什均衡的PCA</h2><div class="ju l"><h3 class="bd b fi z dy js ea eb jt ed ef dx translated">我们提出了一个新颖的观点，即主成分分析(PCA)是一个竞争游戏，其中每个近似…</h3></div><div class="jv l"><p class="bd b fp z dy js ea eb jt ed ef dx translated">arxiv.org</p></div></div><div class="jw l"><div class="os l jy jz ka jw kb hk jn"/></div></div></a></div><h1 id="2121" class="kf kg ho bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak"> 9。结论</strong></h1><p id="f5da" class="pw-post-body-paragraph io ip ho ir b is ld iu iv iw le iy iz kc lf jc jd kd lg jg jh ke lh jk jl jm ha bi translated"><strong class="ir hp">PCA作为可微分博弈的新观点可以导致进一步的算法开发和见解。</strong></p><p id="026f" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">它是从多人游戏的角度激发的，启发了一种能够进行大规模主成分估计的分散算法。</strong></p><p id="09b5" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated">如果你发现错误或漏洞，请不吝赐教。请原谅我所犯的任何错误。</p><p id="e797" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">跟我来:</strong><a class="ae li" href="https://www.linkedin.com/in/mdaminkhan/" rel="noopener ugc nofollow" target="_blank"><strong class="ir hp">https://www.linkedin.com/in/mdaminkhan/</strong></a></p><p id="2eb4" class="pw-post-body-paragraph io ip ho ir b is it iu iv iw ix iy iz kc jb jc jd kd jf jg jh ke jj jk jl jm ha bi translated"><strong class="ir hp">感谢阅读:)</strong></p></div></div>    
</body>
</html>