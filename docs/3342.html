<html>
<head>
<title>Understanding K-Nearest Neighbour Algorithm in Detail</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">详细理解K-最近邻算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-k-nearest-neighbour-algorithm-in-detail-fc9649c1d196?source=collection_archive---------3-----------------------#2021-06-26">https://medium.com/analytics-vidhya/understanding-k-nearest-neighbour-algorithm-in-detail-fc9649c1d196?source=collection_archive---------3-----------------------#2021-06-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="a6da" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">先决条件:</h1><ol class=""><li id="f223" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">对机器学习或数据科学有基本的了解</li><li id="9419" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">基本Python编程</li><li id="bfe9" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">机器学习的性能度量</li></ol><h1 id="f019" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">目录:</strong></h1><ol class=""><li id="77e1" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">介绍</li><li id="ca06" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">KNN的失败案例</li><li id="8577" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">在KNN使用不同的距离测量</li><li id="88ed" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">如何选择K值(超参数调谐)</li><li id="24fd" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">时间复杂性和空间复杂性</li><li id="44a9" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">KNN算法的利弊</li><li id="5c88" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">执行k-NN的不同方式</li><li id="23e0" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">为什么需要交叉验证？</li><li id="26e3" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">偏差方差权衡</li><li id="5d1b" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">KNN的代码实现</li></ol><h1 id="1a13" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak"> 1。简介</strong></h1><p id="5476" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">k近邻(KNN)是一种监督学习算法，用于回归和分类目的，但主要用于分类问题。给定一个具有不同类的数据集，KNN试图通过计算测试数据和所有训练点之间的距离来预测测试数据的正确类。然后，它选择最接近测试数据的k个点。一旦选择了这些点，该算法计算测试点属于k个训练点的类的概率(在分类的情况下)，并且选择具有最高概率的类。在回归问题的情况下，预测值是k个选定训练点的平均值。</p><p id="aa43" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">让我们通过一个分类问题的例子来理解这一点:</p><p id="f349" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">1)给定如下所示的训练数据集。我们有新的测试数据需要分配给两个类中的一个。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es kt"><img src="../Images/796842a808b3875ef36f9c7e09e8a774.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*X2WeNqTUWXHg9pQyZJ-hoA.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">红点和绿点是训练数据中的两类</figcaption></figure><p id="1560" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">2)现在，k-NN算法计算测试数据和给定训练数据之间的距离。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lf"><img src="../Images/155226fee13a37e3e891a7ca850fbea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*ZLGjoSZUKWBgIziZ7mlSQA.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">计算相邻点之间的距离</figcaption></figure><p id="4384" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">3)在计算距离之后，它将选择与测试数据最近的k个训练点。在我们的例子中，假设k的值是3。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lg"><img src="../Images/d7ce2338c07901ba2b9a065304a1e01b.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*8H1yMfhplEAVI5QKKQoUwQ.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">选择k=3，其中1个红色，2个绿色</figcaption></figure><p id="755c" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">4)现在，选择3个最近邻，如上图所示。让我们看看我们的测试数据将被分配到哪个类中:</p><p id="6443" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">绿色类值的数量= 2，红色类值的数量= 1概率(绿色)= 2/3，概率(红色)= 1/3</p><p id="b372" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">由于绿色类的概率高于红色类，k-NN算法会将测试数据分配给绿色类。</p><p id="b5c0" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi"> KNN回归</strong></p><p id="dc49" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">在<strong class="je hi">回归问题</strong>的情况下，测试数据的预测值将只是所有3个最近值的平均值或所有3个最近值的中值。中位数不太容易出现异常值</p><p id="abf0" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">这是k-NN的基本工作算法，但是距离是如何计算的呢？在进行距离计算之前，首先让我们了解KNN在哪些情况下会失败。</p><h1 id="510b" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2.KNN的失败案例</h1><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lh"><img src="../Images/3196dc1845e528f096210f290c74c59e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2hZNVUw-gp_owQnepID5qQ.png"/></div></div></figure><ol class=""><li id="e883" class="jc jd hh je b jf ko jh kp jj lm jl ln jn lo jp jq jr js jt bi translated">当查询点远离数据集中的点时，数据集仍然被很好地分类，KNN算法不能正确预测。</li><li id="3a62" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">当数据集以杂乱、随机分布、混杂的形式存在时，如果我们发送查询点来预测它们的KNN，则无法预测，因为算法无法有效地计算距离，这是由于数据杂乱。不仅KNN在这种类型的分布中大多数算法都失败了</li></ol><h1 id="b0f4" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">3.让我们了解一下距离是如何计算的</h1><ol class=""><li id="d2e7" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated"><strong class="je hi">欧氏距离:</strong>是计算两点间距离最常用的方法。两点“x1(x11，x12)”和“x2(x21，x22)”之间的欧几里德距离计算如下:</li></ol><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lp"><img src="../Images/aa373db329eb4ff4cc3610be7c266e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*99YngW3PWaQtN18bupTWvw.png"/></div></div></figure><p id="9a17" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">类似地，对于n维空间，欧几里德距离被给出为:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lq"><img src="../Images/b8fc2b6d80677182e78b688e2234da74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*Q9u-rA_eIcb8oEnmbzRf4Q.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">图片来源:应用ai ML课程</figcaption></figure><p id="4c40" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">欧几里德距离是向量(x1-x2)的L2范数</p><p id="ed5b" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi"> 2。汉明距离:</strong>根据维基百科，距离度量是测量两个向量之间不匹配的数量。它主要用于分类数据的情况。</p><p id="8c89" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">通常，如果我们将特征作为分类数据，那么如果两个值相同，我们认为差异为0，如果两个值不同，我们认为差异为1。</p><p id="64a6" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我们使用汉明距离在文本处理和当我们想要找到布尔向量之间的距离。</p><p id="c35e" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi"> 3。曼哈顿距离:</strong> A/c to Wikipedia，曼哈顿距离，也称为L1标准、出租车标准、直线距离或城市街区距离。这个距离代表向量中相反值之间的绝对差之和。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lr"><img src="../Images/68d71852f8f680b0cdaffcbc0b4243a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*_EiTQpfAFEUbBLXVXZFNQQ.png"/></div></figure><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es ls"><img src="../Images/7703a9954eac5754e2235d4ff3e99955.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*8Qsn8D_D1F4-aLgvF4ICoA.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">图片来源:谷歌搜索</figcaption></figure><p id="d32d" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">曼哈顿距离受异常值的影响比欧几里德距离小。对于非常高维的数据，这是更优选的。</p><p id="1d58" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">曼哈顿距离是向量(x1-x2)的L1范数</p><p id="ca12" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi"> 4。闵可夫斯基距离:</strong>两点间的闵可夫斯基距离无非是向量(x1-x2)的LP-范数。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lt"><img src="../Images/1de251f74a61b66a4ef98b3e4503be0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*PUCoEQu_IazRj3pSA6-klA.png"/></div></figure><p id="6cdf" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi">向量的LP-范数是什么？</strong></p><p id="e4d0" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我们使用两点之间的距离，即(x1，x2)，范数是两个向量之间的距离，即</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lu"><img src="../Images/82b907bc0354d85a66be14df5863fb73.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*X1d1ZmXFfe6jUtllzUZExQ.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">向量x1的LP范数</figcaption></figure><p id="26bd" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">如果p = 2，闵可夫斯基距离与欧几里德距离相同</p><p id="db58" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">如果p =，明柯夫斯基距离与曼哈顿距离相同</p><p id="9506" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi"> 5。余弦-相似度和余弦-距离:</strong>我们先来了解一下相似度和距离的关系。</p><p id="53ef" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">如果两点之间的距离增加，则这些点之间的相似性降低，如果距离减小，则相似性将增加</p><p id="a62f" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi"> 1-cos相似度(x1，x2) = cos距离(x1，x2) </strong></p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lv"><img src="../Images/6209d7020a8c228917f7457e5b0c3513.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ndRC755OMlFDU8CpDupgBQ.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx translated">余弦相似性</figcaption></figure><p id="cb82" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi"> 6。懒惰学习者:</strong> k-NN算法通常被称为懒惰学习者。让我们来理解这是为什么。大多数算法像贝叶斯分类、逻辑回归、SVM等。，被称为求知者。这些算法在接收测试数据之前对训练集进行概括，即它们在接收测试数据之前基于训练数据创建模型，然后对测试数据进行预测/分类。但是k-NN算法却不是这种情况。它不会为训练集创建一个通用模型，而是等待测试数据。一旦提供了测试数据，那么只有它开始概括训练数据以对测试数据进行分类。所以，懒惰的学习者只是存储训练数据并等待测试集。这种算法在训练时工作较少，而在分类给定的测试数据集时工作较多。</p><p id="eb7c" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi"> 7。加权最近邻:</strong>在加权k-NN中，我们给k个最近邻分配权重。权重通常基于距离来分配。有时，其余的数据点也被赋予0的权重。主要的直觉是邻居中的点应该比更远的点具有更大的权重。</p><h1 id="9e48" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">4.选择k的值</h1><p id="5f7b" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">k的值极大地影响k-NN分类器。模型的灵活性随着‘k’的增加而降低。“k”值越低，方差越高，偏差越小，但随着“k”值的增加，方差开始减小，偏差开始增加。“k”值非常低时，算法有可能过拟合数据，而“k”值非常高时，有可能欠拟合。让我们设想一下“1/k”、训练错误率和测试错误率之间的权衡:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lw"><img src="../Images/3bce6f4bc3d2ff63f383176e88041994.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*j3toe9uXj-OWS1LoLNsjzQ.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">图片来源:<a class="ae lx" href="https://www.mathstat.dal.ca/~aarms2014/StatLearn/docs/0102_annotated.pdf" rel="noopener ugc nofollow" target="_blank"> KNN </a></figcaption></figure><p id="5c10" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我们可以清楚地看到，训练错误率随着“k”值的增加而增加，而测试错误率最初降低，然后再次增加。因此，我们的目标应该是选择这样一个“k”值，使我们得到误差的最小值，避免过拟合和欠拟合。我们使用不同的方法来计算最佳k值，如交叉验证、误差与k曲线、检查每个k值的准确性等。</p><h1 id="cfd4" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak"> 5。时空复杂度</strong></h1><p id="7f92" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">为什么我们需要关心时间和空间的复杂性？</p><p id="1456" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">时间是宝贵的，RAM空间是昂贵的。</p><p id="af23" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi">时间复杂度:</strong>时间复杂度是<strong class="je hi"> </strong>在KNN给定一个查询点需要多少时间，由KNN模型来预测产量。</p><p id="c850" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi">空间复杂度:</strong>空间复杂度是在给定查询点时评估输出所需的KNN RAM空间。</p><p id="0742" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">因此，<strong class="je hi">时间复杂度是O(nd)并且空间复杂度也是O(nd) </strong>其中n是数据集的大小，d是数据集的维度或特征。</p><h1 id="6f3b" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">6.k-NN算法的利弊</h1><p id="ddfe" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated"><strong class="je hi">优点:</strong></p><ul class=""><li id="a9cc" class="jc jd hh je b jf ko jh kp jj lm jl ln jn lo jp ly jr js jt bi translated">它可用于回归和分类问题。</li><li id="9513" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">它非常简单，易于实现。</li><li id="2df0" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">算法背后的数学很容易理解。</li><li id="3476" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">不需要创建模型或进行超参数调整。</li><li id="74ef" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">KNN没有对给定数据的分布做任何假设。</li><li id="bd45" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">在培训阶段没有太多的时间成本。</li></ul><p id="be63" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi">缺点:</strong></p><ul class=""><li id="ce19" class="jc jd hh je b jf ko jh kp jj lm jl ln jn lo jp ly jr js jt bi translated">寻找“k”的最佳值</li><li id="beac" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">计算每个测试样本与所有训练样本之间的距离需要花费大量时间(时间复杂度巨大)。</li><li id="5793" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">由于我们需要存储每个测试集的整个训练集，所以需要很大的空间(空间复杂度巨大)。</li><li id="a2fa" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">由于在这种算法中模型不是预先保存的(懒惰学习者)，所以每次预测一个测试值时，它都重复相同的步骤。</li><li id="981a" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">不适合高维数据。</li><li id="9e94" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">测试阶段成本高昂</li></ul><h1 id="c053" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">6.执行k-NN的不同方式</h1><p id="e87b" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">上面我们研究了k-NN通过计算来自每个观察值的测试数据的距离并选择“k”值来分类数据的方法。这种方法也被称为“暴力k-NN”。这在计算上非常昂贵。因此，还有其他方法来执行k-NN，这些方法相对来说比暴力方法更便宜。对k-NN分类器使用其他算法背后的想法是通过预处理训练数据来减少测试期间的时间，以这种方式可以容易地将测试数据分类到适当的聚类中。</p><p id="2ac9" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">让我们来讨论和理解两个最著名的算法:</p><ol class=""><li id="31cb" class="jc jd hh je b jf ko jh kp jj lm jl ln jn lo jp jq jr js jt bi translated"><strong class="je hi"> k维树(kd树):</strong> k-d树是一种层次二叉树。当该算法用于k-NN分类时，它以二叉树结构重新排列整个数据集，因此当提供测试数据时，它将通过遍历树给出结果，这比蛮式搜索花费更少的时间。</li></ol><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lz"><img src="../Images/e8e86f1d1d5a27f10d5ce9886c79382a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kw5n7J6RmjFdPlYl_j2S_w.png"/></div></div></figure><p id="61e7" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">如上图所示，数据集像树一样被划分。假设我们有三维数据，即(x，y，z ),那么树的根节点是其中一个维度，这里我们从“x”开始。然后在下一个层次，分裂是在第二维度的基础上完成的，在我们的例子中是y。同样，第三个层次也有第三维度，依此类推。在“k”维的情况下，每次分割都是基于“k”维进行的。我们用一个例子来理解k-d树是如何形成的:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ma"><img src="../Images/e612b1bc63b80d99515ab7e3a73820ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4I_ldglKtcMxa-t8YBSCVw.png"/></div></div></figure><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mb"><img src="../Images/3673cca84f0b6ddb408154ec2c62c71e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*dKl2sQ2dpnImiUH6_pRdgw.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">上述示例的K-d树</figcaption></figure><p id="e90d" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">一旦树形成，算法很容易通过遍历树来搜索可能的最近邻居。k-d树的主要问题是它给出了可能的最近邻，但可能遗漏了实际的最近邻。</p><p id="b57f" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi"> 2。球树:</strong>和k-d树类似，球树也是层次数据结构。这些是非常有效的，尤其是在更高的维度的情况下。</p><p id="dc70" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">这些由以下步骤形成:</p><ul class=""><li id="6a06" class="jc jd hh je b jf ko jh kp jj lm jl ln jn lo jp ly jr js jt bi translated">最初创建了两个集群</li><li id="dc75" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">所有数据点必须属于至少一个聚类。</li><li id="af56" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">一个点不能同时位于两个簇中。</li><li id="f6d8" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">该点的距离是根据每个聚类的质心计算的。靠近质心的点进入那个特定的簇。</li><li id="a889" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">然后，将每个聚类再次划分为子聚类，然后基于距质心的距离将这些点分类到每个聚类中。</li><li id="d991" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp ly jr js jt bi translated">这就是集群被划分到一定深度的方式。</li></ul><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mc"><img src="../Images/65305d23b69d9b8a1474e5011ddcec72.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*y1GEo0XJeoh3eNQVTjl_rg.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">图片来源:谷歌搜索</figcaption></figure><p id="56bc" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">球树的形成最初需要很多时间，但是一旦创建了嵌套聚类，找到最近的邻居就变得容易了。</p><p id="4aab" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi"> 3。局部敏感散列(LSH): </strong>这种技术更强大，即使在维度很大的情况下也能很好地工作。LSH使用哈希技术来识别近似的最近邻居。要了解更多关于LSH的信息，请阅读<a class="ae lx" href="https://towardsdatascience.com/locality-sensitive-hashing-for-music-search-f2f1940ace23" rel="noopener" target="_blank">这篇文章</a>。</p><h1 id="f79e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">7.交叉验证</h1><p id="e961" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">假设您使用任何特定的算法在给定的数据集上训练模型。您尝试使用相同的训练数据来查找训练模型的准确性，并发现准确性为95%，甚至可能是100%。</p><p id="e3dc" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">这是什么意思？你的模型准备好预测了吗？答案是否定的。</p><p id="c034" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi">为什么？</strong></p><p id="e1ba" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">因为你的模型已经根据给定的数据进行了自我训练，也就是说，它知道这些数据，并且已经很好地概括了这些数据。但是当你试图预测一组新的数据时，它很可能会给你很差的准确性，因为它以前从未见过这些数据，因此它不能很好地概括这些数据。这就是<strong class="je hi">过拟合</strong>的问题。</p><p id="d63e" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">为了解决这样的问题，交叉验证应运而生。交叉验证是一种重采样技术，其基本思想是将训练数据集分为两部分，即训练和测试。在第一部分(训练)中，您尝试训练模型，在第二部分(测试)中，即模型看不到的数据，您进行预测并检查您的模型在这方面的工作情况。如果模型对您的测试数据具有良好的准确性，这意味着模型没有过度拟合训练数据，可以信任预测，而如果它的准确性较差，则我们的模型不可信，我们需要调整我们的算法。</p><p id="bbdb" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">让我们看看交叉验证的不同方法:</p><ul class=""><li id="3200" class="jc jd hh je b jf ko jh kp jj lm jl ln jn lo jp ly jr js jt bi translated"><strong class="je hi">撑出方法:</strong></li></ul><p id="c3b8" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">这是最基本的CV技术。它只是将数据集分成两组训练集和测试集。训练数据集用于训练模型，然后将测试数据拟合到训练好的模型中以进行预测。我们检查准确性，并在此基础上评估我们的模型。使用这种方法是因为它的计算成本较低。但是基于拒绝集的评估可能有很大的差异，因为它很大程度上取决于哪些数据点最终出现在训练集中，哪些数据点出现在测试数据中。每次这个划分发生变化，评价都会不一样。</p><ul class=""><li id="8039" class="jc jd hh je b jf ko jh kp jj lm jl ln jn lo jp ly jr js jt bi translated"><strong class="je hi"> k倍交叉验证</strong></li></ul><p id="97f6" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">为了解决持有法的<strong class="je hi">高方差</strong>，使用了k倍法。这个想法很简单，将整个数据集分成“k”个大小相等的集合。然后选择第一组作为测试组，其余的“k-1”组用于训练数据。针对该特定数据集计算误差。然后重复这些步骤，即选择第二组作为测试数据，剩余的“k-1”组用作训练数据。再次计算误差。类似地，该过程持续“k”次。最后，CV误差作为单独计算的总误差的平均值给出，数学上表示为:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es md"><img src="../Images/7d9518e86328c0a94fe73c35b26c2b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*-Q1cRjJAjozVYLog2SvZfA.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">图片来源:谷歌搜索</figcaption></figure><ul class=""><li id="8500" class="jc jd hh je b jf ko jh kp jj lm jl ln jn lo jp ly jr js jt bi translated"><strong class="je hi">留一个出来交叉验证(LOOCV) </strong></li></ul><p id="e49e" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">LOOCV是k倍CV的一个特例，其中k等于n(观察次数)。因此，它没有创建两个子集，而是选择单个观察值作为测试数据，其余的数据作为训练数据。计算这些测试观察值的误差。现在，选择第二个观察值作为测试数据，其余数据作为训练集。同样，误差是针对这个特定的测试观察值计算的。这个过程持续“n”次，最后，CV误差计算如下:</p><p id="85d0" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">使用LOOCV的优点是我们利用了所有的数据点，因此模型的结果偏差很低。</p><p id="8ec8" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">LOOCV的缺点是它会导致更高的变化，因为我们只针对一个数据点进行测试，并且它会花费大量的执行时间，因为它会迭代数据点的次数。</p><h1 id="b1e3" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">8.k倍CV、LOOCV和维持集CV的偏置方差权衡</h1><p id="f3f5" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">k &lt; n的k重CV比LOOCV具有计算优势。但是把计算问题放在一边，k倍CV的一个不太明显但可能更重要的优势是，它通常比LOOCV给出更准确的测试错误率估计。</p><p id="ccde" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">验证集方法可能导致对测试错误率的过高估计，因为在这种方法中，用于拟合统计学习方法的训练集仅包含整个数据集的一半观察值。</p><p id="4fce" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">使用这种逻辑，不难看出LOOCV将给出检验误差的近似无偏估计，因为每个训练集包含n-1个观测值，这几乎与完整数据集中的观测值数量一样多。</p><p id="a105" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">对k = 5或k = 10执行k倍CV将导致中等程度的偏差，因为每个训练集包含(k1)n/k个观察值，比LOOCV方法少，但比验证集方法多得多。</p><p id="6d23" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">因此，从减少偏倚的角度来看，很明显LOOCV优于k倍CV。然而，我们知道，在估算过程中，偏差并不是唯一的关注点；我们还必须考虑程序的变化。</p><p id="ce41" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">事实证明，LOOCV比使用Python实现的K重CV具有更高的方差。</p><p id="4f14" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">显示前5行数据</p><p id="30d1" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我们使用pandas read_csv通过索引号0读取数据。数据集属于二元分类问题(0，1)，其中目标特征是“目标类”，数据集包含10个特征。</p><h2 id="7258" class="me if hh bd ig mf mg mh ik mi mj mk io jj ml mm is jl mn mo iw jn mp mq ja mr bi translated">标准化数据集后</h2><p id="d74b" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">通过使用skit-learn库的标准标量()来标准化数据集。标准化有助于数据在范围内。</p><p id="4009" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">在应用任何机器学习模型之前，我们需要以70:30的比例将数据集分成训练和测试数据集。使用scikit-learn库<code class="du nf ng nh mt b">sklearn.neighbors</code>应用KNN算法。KNeighborsClassifier。如果数据集是回归问题，我们可以使用'<code class="du nf ng nh mt b"><a class="ae lx" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html" rel="noopener ugc nofollow" target="_blank">sklearn.neighbors.</a></code> <a class="ae lx" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html" rel="noopener ugc nofollow" target="_blank"> KNeighborsRegressor </a>'。</p><pre class="ku kv kw kx fd ms mt mu mv aw mw bi"><span id="96eb" class="me if hh mt b fi mx my l mz na">#importing libraries<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>%matplotlib inline</span><span id="c1d9" class="me if hh mt b fi nb my l mz na">#reading data<br/>df = pd.read_csv("Classified Data",index_col=0)<br/>df.head()</span></pre><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es nc"><img src="../Images/c8c386951665e974701bda6af15f5d68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CzhiykOoMH5FnP3KLyHuOQ.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx translated">在导入带有所需参数的初始化KNeighborsClassifier之后，这里n_neighbors就是我们上面讨论过的“k”。为了获得更好的精度，我们需要调整K，所以，K在KNN是超参数。</figcaption></figure><p id="3bc5" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">在输出中，我们可以看到KNNClassifier使用以下参数构建了一个模型:</p><pre class="ku kv kw kx fd ms mt mu mv aw mw bi"><span id="e318" class="me if hh mt b fi mx my l mz na">#standarize te dataset<br/>from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()</span><span id="97d9" class="me if hh mt b fi nb my l mz na">#fitting standarization after removing target feature<br/>scaler.fit(df.drop(‘TARGET CLASS’,axis=1))<br/>scaled_features = scaler.transform(df.drop(‘TARGET CLASS’,axis=1))</span><span id="c99d" class="me if hh mt b fi nb my l mz na">#reading the data<br/>df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])<br/>df_feat.head()</span></pre><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es nd"><img src="../Images/74b211a924309a6acb446d48dc26e3b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VHuA1uUSy6qnihQHH0qhpg.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx translated"><strong class="je hi">算法</strong>参数让我们选择想要计算的算法(球树、kd树、蛮力、自动)。默认情况下它是自动的。自动算法决定球树、kd树、蛮力中哪种算法对我们的数据集是最好的，并在拟合模型中使用它。</figcaption></figure><p id="13d1" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi"> leaf_size </strong>参数用于球树或kd树。</p><pre class="ku kv kw kx fd ms mt mu mv aw mw bi"><span id="0050" class="me if hh mt b fi mx my l mz na">#importig libraries<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.neighbors import KNeighborsClassifier</span><span id="b132" class="me if hh mt b fi nb my l mz na">#splitting dataset into train and test data<br/>X_train, X_test, y_train, y_test = train_test_split(scaled_features,df[‘TARGET CLASS’],<br/> test_size=0.30)</span><span id="8c11" class="me if hh mt b fi nb my l mz na">#initalizing KNN algorithm <br/>knn = KNeighborsClassifier(n_neighbors=1)<br/>knn.fit(X_train,y_train)</span></pre><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ne"><img src="../Images/8f35323a7a0765b6fc655314d5a25113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLBpfVSFVYVWR9pjidQ-yw.png"/></div></div></figure><p id="58a1" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi">公制</strong>参数是闵可夫斯基</p><p id="6208" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi"> p </strong>参数定义了闵可夫斯基度规的功效。我们在上面讨论了如果p=1，那么是曼哈顿距离，如果p=2，那么是欧几里德距离。默认情况下p=2，所以闵可夫斯基p=2就是欧几里德距离。对于我们的数据模型，使用欧几里德距离来计算数据点之间的距离。</p><p id="2be6" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><strong class="je hi">权重</strong>如果我们想要使用加权KNN，使用的参数，我们已经在制作权重:距离中讨论过。默认情况下，权重是统一的。</p><p id="959f" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">K = 1时KNN模型的预测结果</p><p id="4b99" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">这里使用混淆矩阵和分类报告(给出精确度、召回率、f1分数、准确度、宏观平均值和加权平均值分数)，使用这些性能指标，我们可以决定我们的模型表现是好还是坏。</p><p id="1030" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">上图中，我们对1到40范围内的k值运行循环，并存储模型的准确度分数。</p><p id="d1a8" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">在上面我们正在计算错误率(1-score.mean())</p><p id="e519" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我们在这里可以看到，当k值为23时，准确率不会下降，所以我们可以选择k =23来训练模型。</p><pre class="ku kv kw kx fd ms mt mu mv aw mw bi"><span id="f476" class="me if hh mt b fi mx my l mz na">from sklearn.metrics import classification_report,confusion_matrix<br/>from sklearn.model_selection import cross_val_score</span><span id="f87c" class="me if hh mt b fi nb my l mz na">#prediction of output using test data<br/>pred = knn.predict(X_test)</span><span id="b54b" class="me if hh mt b fi nb my l mz na">print(confusion_matrix(y_test,pred))</span><span id="bd7c" class="me if hh mt b fi nb my l mz na">print(classification_report(y_test,pred))</span></pre><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es ni"><img src="../Images/d54a860bff2644f980d3fd31db33542c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*44XCUpeujUHzk39xaGLFVA.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">在错误率图中，我们可以观察到在k = 23时，与其他值相比，错误率非常低，因此，在可视化错误率和准确率图之后，我们可以使用k = 23来训练knn模型。</figcaption></figure><p id="089c" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">K = 23时KNN模型的预测结果</p><pre class="ku kv kw kx fd ms mt mu mv aw mw bi"><span id="3a18" class="me if hh mt b fi mx my l mz na">accuracy_rate = []</span><span id="14d4" class="me if hh mt b fi nb my l mz na">for i in range(1,40):<br/> <br/> knn = KNeighborsClassifier(n_neighbors=i)<br/> score=cross_val_score(knn,df_feat,df[‘TARGET CLASS’],cv=10)<br/> accuracy_rate.append(score.mean())</span></pre><p id="25dd" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">通过比较K = 1和K= 23时模型的预测结果，我们可以观察到K = 23给出了比K = 1更好的结果。</p><pre class="ku kv kw kx fd ms mt mu mv aw mw bi"><span id="4f69" class="me if hh mt b fi mx my l mz na">error_rate = []<br/># Will take some time<br/>for i in range(1,40):<br/> <br/> knn = KNeighborsClassifier(n_neighbors=i)<br/> score=cross_val_score(knn,df_feat,df[‘TARGET CLASS’],cv=10)<br/> error_rate.append(1-score.mean())</span></pre><p id="1e38" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">我强烈建议浏览一下<a class="ae lx" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener ugc nofollow" target="_blank"> sci-kit learn </a>的官方文档，这将有助于更好地理解代码部分。</p><pre class="ku kv kw kx fd ms mt mu mv aw mw bi"><span id="b15c" class="me if hh mt b fi mx my l mz na">plt.figure(figsize=(10,6))<br/>plt.plot(range(1,40),accuracy_rate,color='blue', linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)<br/>plt.title('Accuracy Score vs. K Value')<br/>plt.xlabel('K')<br/>plt.ylabel('Accuracy Score')</span></pre><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es nj"><img src="../Images/ec03bd597ca46565bd8869d38113ba00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SxAwb1FrufwT1iHYyPSNag.png"/></div></div></figure><p id="098b" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">谢谢你看这篇文章…快乐学习！！</p><pre class="ku kv kw kx fd ms mt mu mv aw mw bi"><span id="f3b5" class="me if hh mt b fi mx my l mz na">plt.figure(figsize=(10,6))</span><span id="41a9" class="me if hh mt b fi nb my l mz na">plt.plot(range(1,40),error_rate,color=’blue’, linestyle=’dashed’, marker=’o’,<br/> markerfacecolor=’red’, markersize=10)<br/>plt.title(‘Error Rate vs. K Value’)<br/>plt.xlabel(‘K’)<br/>plt.ylabel(‘Error Rate’)</span></pre><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es nk"><img src="../Images/2e84e13b1212337df9d1d5835e156235.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*huKLKWSu779jHCBTVDINCg.png"/></div></div></figure><p id="6559" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">参考:</p><pre class="ku kv kw kx fd ms mt mu mv aw mw bi"><span id="66c5" class="me if hh mt b fi mx my l mz na"># NOW WITH K=23<br/>knn = KNeighborsClassifier(n_neighbors=23)</span><span id="187c" class="me if hh mt b fi nb my l mz na">knn.fit(X_train,y_train)<br/>pred = knn.predict(X_test)</span><span id="e179" class="me if hh mt b fi nb my l mz na">print(‘WITH K=23’)<br/>print(‘\n’)<br/>print(confusion_matrix(y_test,pred))<br/>print(‘\n’)<br/>print(classification_report(y_test,pred))</span></pre><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es nl"><img src="../Images/5a98d3f9178f12de9775453a499aefdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*g0PIOEfvPqJRrTziQOhDbQ.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated"><a class="ae lx" href="https://www.ineuron.ai/home/coursedetail/-machine--learning-and-deep-learning-masters-118" rel="noopener ugc nofollow" target="_blank"> https://www.ineuron.a </a>我</figcaption></figure><p id="b0fa" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated">【https://www.appliedaicourse.com/ T4】</p><p id="d035" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><a class="ae lx" href="https://laptrinhx.com/understanding-the-knn-algorithm-258072474/" rel="noopener ugc nofollow" target="_blank">https://laptrinhx . com/understanding-the-KNN-algorithm-258072474/</a></p><p id="ba2c" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj kq kg kh jl kr kj kk jn ks km kn jp ha bi translated"><a class="ae lx" href="https://www.kaggle.com/discussion/226805" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/discussion/226805</a></p><h1 id="f9f6" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><a class="ae lx" href="https://jovian.ai/ocheotote97/01-k-nearest-neighbors-with-python" rel="noopener ugc nofollow" target="_blank">https://jovian . ai/ocheotote 97/01-k-nearest-neighbors-with-python</a></h1><ol class=""><li id="ee0f" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi"><a class="ae lx" href="https://www.ineuron.ai/home/coursedetail/-machine--learning-and-deep-learning-masters-118" rel="noopener ugc nofollow" target="_blank">https://www.ineuron.a</a>i</li><li id="7bea" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi"><a class="ae lx" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></li><li id="a8b9" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi"><a class="ae lx" href="https://laptrinhx.com/understanding-the-knn-algorithm-258072474/" rel="noopener ugc nofollow" target="_blank">https://laptrinhx.com/understanding-the-knn-algorithm-258072474/</a></li><li id="8ff6" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi"><a class="ae lx" href="https://www.kaggle.com/discussion/226805" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/discussion/226805</a></li><li id="d6d4" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi"><a class="ae lx" href="https://jovian.ai/ocheotote97/01-k-nearest-neighbors-with-python" rel="noopener ugc nofollow" target="_blank">https://jovian.ai/ocheotote97/01-k-nearest-neighbors-with-python</a></li></ol></div></div>    
</body>
</html>