# 机器学习#4 —无监督学习、聚类、K 均值算法、降维、主成分分析

> 原文：<https://medium.com/analytics-vidhya/machine-learning-4-unsupervised-learning-clustering-k-means-algorithm-dimensional-reduction-238a54e8cfcd?source=collection_archive---------1----------------------->

![](img/edf38a6c6a900e2ccd4d40ca26fccae9.png)

捕捉自[https://adimadimgurme.com/2020/09/20/sante-wine-more/](https://adimadimgurme.com/2020/09/20/sante-wine-more/)

*这篇文章是我的机器学习系列的第四篇文章，我写它是为了巩固和分享我所学到的东西。对于发表在 Analytics Vidhya 上的系列文章的第三篇:* [*链接*](/analytics-vidhya/machine-learning-3-linear-regression-ridge-lasso-functions-2fa7fda624a0)

如果你对这篇文章的土耳其语不感兴趣，可以跳过这一段。

*Bu yaz，orendklerimi peki TIR MEK ve pay lamak I in kale me ALD m 机器学习 yaz dizimin rdnüyaz SDR . serin in yaynlananüüncy as I in:*[*链接*](/software-development-turkey/makine-öğrenmesi-3-lineer-regresyon-ridge-lasso-fonksiyonları-9dbf6036e29e)

bu yazūn n türke si isi in:[链接](https://gokerguner.medium.com/machine-learning-4-gözetimsiz-öğrenme-kümeleme-k-means-algoritması-boyut-azaltma-temel-44598c53ee0c)

你好，在我们机器学习系列的第四集中，

*   检查无监督学习的概念，
*   将学习聚类方法和包含该方法的 K-Means 算法，
*   我们将触及降维和主成分分析(PCA)的概念，它们是聚类方法的关键问题。

我们将在本系列的第二篇文章中使用葡萄酒质量数据集，但这一次我们将使用无监督学习技术检查数据集，而不进行标记。

在这篇文章中，我们将讨论一个不同于该系列前几篇文章的学习模型，**无监督学习**。**无监督学习**与监督学习不同，它是学习数据中存在的关系和结构，而不会将数据标记为因果或输入输出。

与监督学习不同，我们不需要训练数据。无监督学习算法解释和分组数据。然后它猜测新数据属于哪个组。在进行这种估计时，它试图找出其特征与哪些组更相似，但无法对这个样本进行评论，因为数据没有被标记。虽然这种解释留给了实现者，但是它非常有用，因为它很容易可视化。

对于聚类分析算法，重要的是所有值都是非空的，并且不包含任何字符串。我们所有的值都必须是数字，因为聚类将根据分析空间中的距离计算来完成。

## **K-Means 聚类算法**

K-means 聚类方法是将数据集划分为 K 个作为输入参数给出的聚类。目的是确保在划分过程结束时获得的聚类在聚类内具有最大相似性，而在聚类之间具有最小相似性。

根据 K-means 算法的工作机制，随机选取 K 个对象来代表每个聚类的中心点或均值。考虑到它们与聚类平均值的距离，剩余的对象被包括在与它们最相似的聚类中。然后，通过计算每个聚类的平均值，确定新的聚类中心，并再次检查对象到中心的距离。该算法继续重复，直到没有变化。

该算法基本上由 4 个阶段组成:

1.  **聚类中心的确定**
2.  **根据数据的距离将数据聚集在中心之外**
3.  **根据聚类确定新中心(或将旧中心转移到新中心)**
4.  **重复步骤 2 和 3，直到稳定。**

在我们使用该数据集的上一篇文章中，我们说过，通过使用 *df.describe()* 方法并解释表中的数值，我们数据集的大多数质量属性都具有诸如 5 和 6 这样的值。为了直观地表现这一点，我们可以用 seaborn 库来做这样一个可视化。

我们在本系列的前几篇文章中已经提到了相关矩阵的概念。这一次，我们用不同的方法来画它，没有上半部分。

为了更直观地表达相关性，我选择了相关性更强的变量，而不是我们的目标变量“质量”。从相关矩阵可以看出，**固定酸度**特征与**密度**呈强正( **0.67** )相关，与 pH 呈强负( **-0.68** )相关。

## 质心和惯性概念

K-均值聚类算法的目的只是尽可能好地选择 K 个聚类的质心。这些重心叫做质心。尽可能选择最佳时，损失函数(即惯性值)最小。

**注:**当然，我们的最终目的不仅仅是尽可能的收缩惯性。如果我们将聚类或质心的数量设置为等于数据集元素的数量，自然惯性将为零。但结果是，我们的数据没有被聚类。

理想的集群数量可以通过**肘法**找到。

我们从数据集中的样本属性中提取“质量”属性，然后将这些值分配给一个 **X** 变量，并通过**缩放**来消除标量的影响，从而准备好我们的算法将用来理解数据的输入集。

**注意:**在确定最佳聚类数时，我们没有使用缩放后的 X 值，因为这种方法是一种数值修正，我们将对其进行修正以提高模型性能。但是首先，我们要检查数据本身，而不是被篡改的版本，以了解我们要将这些数据分成多少个簇。

似乎我们可以将我们的聚类数设置为 3，因为在这个数之后，惯性值的下降率减小。

我们的 K-Means 模型将使用我们为训练准备的数据集的前 1000 个样本，并根据它从前 1000 个样本中做出的推断对其他 500 个样本进行分类。

您可以在图上看到以正方形表示的簇的质心。虽然我们能够正确地聚类一些清晰的样本，但颜色看起来有点混合。我们如何帮助我们的算法获得更好的性能？

## 降维与主成分分析

降维发现数据中的模式，并以“压缩”格式重新表述它们。因此，它可以更有效地计算时间和大小。在处理大得多的数据集时，这不仅对准确性很重要，对时间也很重要。

它最重要的功能是从数据集中移除包含相对较少信息的要素。降维功能中使用的最基本的技术是 PCA(主成分分析)。

我们将尝试做的主要事情是找到包含数据重要信息的“关键组件”的数量，并将数据集中的要素减少到该数量。

在某些资料中，这些成分可能会被视为“内在维度”。对于这个例子，我们可以说我们的数据集实际上是一个多维数据集，而“本质上”它实际上是二维的。

在这里，我们看到相当一部分特征不包含信息。如果我们将组件计数设置为 2，看起来我们可以保留对我们有用的大部分信息。那么，我们到底保存了多少这种“信息”？

当我们将分量数设置为 2 时，我们可以保留大约 99.8%的总方差，也就是几乎所有的方差。

质心和簇看起来都比我们第一次尝试时更好。

## 分层聚类

分层聚类在从小到大的聚类层次结构中组织样本，对于非技术人员来说，这是一种可视化数据的好方法。就像我们在高中生物中看到的对生活世界的分类一样，从最小(或最大)的共同特征开始，按分类继续。

在完全链接(在上面的链接方法中称为“完全”)中，簇之间的距离是簇的最远点之间的距离。这被称为凝聚法。在这种方法的开始，所有的对象都是相互分离的。也就是说，每个样本本身就是一个集合。随后，将具有相似属性的聚类放在一起以获得单个聚类。这种方法的另一个名字是**最远邻居方法**。

在单个链接中(在下面的链接方法中称为“单个”)，簇之间的距离是簇的最近点之间的距离。在分裂方法中，分裂策略占主导地位。在这种方法中，开始时只有一个集群。在每个阶段，根据距离/相似性矩阵将对象从主聚类中分离出来，并形成不同的子集。该过程的结果是，每个数据成为一个集合。这种方法的另一个名字是**最近邻法**。

下一篇文章再见。

[](https://github.com/gokerguner) [## gokerguner -概述

随着时间的推移，这些领域将会丰富 Python、数据科学和机器 Learning.github.com 的内容](https://github.com/gokerguner)