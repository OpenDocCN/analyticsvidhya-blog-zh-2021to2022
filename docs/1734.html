<html>
<head>
<title>Data Wrangling with Python Pandas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">与Python熊猫的数据角力</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/data-wrangling-with-python-pandas-e58a7058408a?source=collection_archive---------9-----------------------#2021-03-15">https://medium.com/analytics-vidhya/data-wrangling-with-python-pandas-e58a7058408a?source=collection_archive---------9-----------------------#2021-03-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="cc40" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">非技术人员的视角</h2></div><p id="6eaf" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated">最近，我完成了一门侧重于数据分析师技能的MOOC课程；主要是<em class="kc"> SQL，实用统计&amp; Python </em>。课程结束后，我决定通过使用托管在bikeshare.com<a class="ae kd" href="https://www.citibikenyc.com/system-data" rel="noopener ugc nofollow" target="_blank">的自行车共享活动数据集来练习Python熊猫在数据清理/争论方面的技能。</a></p><p id="9242" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意:</p><ol class=""><li id="970f" class="ke kf hi iz b ja jb jd je jg kg jk kh jo ki js kj kk kl km bi translated">我的工作基于纽约2019年的月度数据</li><li id="0cc6" class="ke kf hi iz b ja kn jd ko jg kp jk kq jo kr js kj kk kl km bi translated">我在<a class="ae kd" href="https://colab.research.google.com/notebooks/intro.ipynb" rel="noopener ugc nofollow" target="_blank">谷歌实验室</a>上执行任务</li></ol><p id="9fd0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是数据集的数据定义:</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ks"><img src="../Images/08529e4432e5d3fe07bb6233cc0292e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0bwIctIfuIBHcVzc71ZWJg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">自行车共享的数据定义</figcaption></figure><p id="494b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们开始这个故事之前，我想强调一下，我的作品旨在强调:</p><ol class=""><li id="6dfc" class="ke kf hi iz b ja jb jd je jg kg jk kh jo ki js kj kk kl km bi translated">我的发现关于我们如何利用好Python熊猫进行数据角力，特别是对于<strong class="iz hj"> <em class="kc">没有编程背景的人</em> </strong>。</li><li id="41d4" class="ke kf hi iz b ja kn jd ko jg kp jk kq jo kr js kj kk kl km bi translated">相同进程使用<strong class="iz hj"> <em class="kc">代码优化</em> </strong>(执行时间&amp;内存使用方面的改进)前后。毕竟，combine数据集的大小超过1 GB，包含2000万条记录。</li></ol></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><p id="1afc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是用于该任务的python库:</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="1dae" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># install haversine if required</em><br/><em class="kc"># use memory_profiler to check memory usage for each command</em><br/><br/>!pip install haversine memory_profiler</span><span id="9516" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc"># data analysis</em> <br/><strong class="lq hj">import</strong> <strong class="lq hj">numpy</strong> <strong class="lq hj">as</strong> <strong class="lq hj">np</strong> <br/><strong class="lq hj">import</strong> <strong class="lq hj">pandas</strong> <strong class="lq hj">as</strong> <strong class="lq hj">pd</strong>  </span><span id="8d48" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc"># Operating System</em> <br/><strong class="lq hj">from</strong> <strong class="lq hj">glob</strong> <strong class="lq hj">import</strong> glob <br/><strong class="lq hj">from</strong> <strong class="lq hj">os</strong> <strong class="lq hj">import</strong> path <br/><strong class="lq hj">from</strong> <strong class="lq hj">pathlib</strong> <strong class="lq hj">import</strong> Path  </span><span id="d3ff" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc"># calculate distance between geo coordinates</em> <br/><strong class="lq hj">from</strong> <strong class="lq hj">haversine</strong> <strong class="lq hj">import</strong> haversine_vector, Unit  </span><span id="1dd1" class="lu lv hi lq b fi ma lx l ly lz"># from memory_profiler<br/>%load_ext memory_profiler</span></pre></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><h1 id="61bf" class="mb lv hi bd mc md me mf mg mh mi mj mk io ml ip mm ir mn is mo iu mp iv mq mr bi translated">第一课:系统高效地导入CSV</h1><p id="bd8c" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated">经过反复试验，我找到了导入CSV文件的各种方法，特别是多个CSV文件。大多数课程教授的<strong class="iz hj"> <em class="kc">熊猫read_csv() </em> </strong>方法都是针对单个文件，通常文件大小很小，用于教学目的..</p><p id="e8a3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">实际上，分析师可能需要收集多个CSV文件，每个文件都可能非常大。在这种情况下，自行车共享月文件是100 MB+。我们需要分析12个月的数据。</p><p id="c38e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里我演示了<strong class="iz hj"> <em class="kc">不重复自己的概念(干)</em></strong>——避免类似任务的长&amp;重复代码，在实际语境中是适用的。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="5b0f" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># list files available in the 'New York Bike Share' directory</em><br/><em class="kc"># 13 files in the directory which we only need file in format <br/>"*-citibike-tripdata.csv" (i.e. 201901-citibike-tripdata.csv etc)</em><br/><em class="kc"># each file is 100 MB+</em><br/><br/>!ls -lh /content/drive/MyDrive/'New York Bike Share'</span><span id="93de" class="lu lv hi lq b fi ma lx l ly lz">total 5.4G<br/>-rw------- 1 root root 176M Feb 11  2019 201901-citibike-tripdata.csv<br/>-rw------- 1 root root 172M Mar  4  2019 201902-citibike-tripdata.csv<br/>-rw------- 1 root root 242M Apr 15  2019 201903-citibike-tripdata.csv<br/>-rw------- 1 root root 322M May  6  2019 201904-citibike-tripdata.csv<br/>-rw------- 1 root root 351M Jun 11  2019 201905-citibike-tripdata.csv<br/>-rw------- 1 root root 387M Jul 15  2019 201906-citibike-tripdata.csv<br/>-rw------- 1 root root 397M Aug  5  2019 201907-citibike-tripdata.csv<br/>-rw------- 1 root root 426M Sep 16  2019 201908-citibike-tripdata.csv<br/>-rw------- 1 root root 443M Oct 11  2019 201909-citibike-tripdata.csv<br/>-rw------- 1 root root 379M Nov  5  2019 201910-citibike-tripdata.csv<br/>-rw------- 1 root root 268M Dec 20  2019 201911-citibike-tripdata.csv<br/>-rw------- 1 root root 173M Jan 21  2020 201912-citibike-tripdata.csv<br/>-rw------- 1 root root 1.7G Feb 19 02:04 new_york_bikeshare_2019.csv</span></pre><h1 id="98bf" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">以前:低效的方式</h1><p id="db2f" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated">我的编码之旅的第一次尝试是用Pandas导入所有的月度文件，并将它们合并到一个主数据框架中。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="5b2f" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc">Issue: repetitive codes that require copy + paste and are error prone.</em><br/><em class="kc">Note: Imagine we have files for 5 years (60 months) within the same directory ??</em></span><span id="77f9" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc">file_01 = pd.read_csv('/content/drive/MyDrive/New York Bike Share/201901-citibike-tripdata.csv')</em><br/><em class="kc">file_02 = pd.read_csv('/content/drive/MyDrive/New York Bike Share/201902-citibike-tripdata.csv')</em><br/><em class="kc">file_03 = pd.read_csv('/content/drive/MyDrive/New York Bike Share/201903-citibike-tripdata.csv')</em><br/><em class="kc">file_04 = pd.read_csv('/content/drive/MyDrive/New York Bike Share/201904-citibike-tripdata.csv')</em><br/><em class="kc">file_05 = pd.read_csv('/content/drive/MyDrive/New York Bike Share/201905-citibike-tripdata.csv')</em><br/><em class="kc">file_06 = pd.read_csv('/content/drive/MyDrive/New York Bike Share/201906-citibike-tripdata.csv')</em><br/><em class="kc">file_07 = pd.read_csv('/content/drive/MyDrive/New York Bike Share/201907-citibike-tripdata.csv')</em><br/><em class="kc">file_08 = pd.read_csv('/content/drive/MyDrive/New York Bike Share/201908-citibike-tripdata.csv')</em><br/><em class="kc">file_09 = pd.read_csv('/content/drive/MyDrive/New York Bike Share/201909-citibike-tripdata.csv')</em><br/><em class="kc">file_10 = pd.read_csv('/content/drive/MyDrive/New York Bike Share/201910-citibike-tripdata.csv')</em><br/><em class="kc">file_11 = pd.read_csv('/content/drive/MyDrive/New York Bike Share/201911-citibike-tripdata.csv')</em><br/><em class="kc">file_12 = pd.read_csv('/content/drive/MyDrive/New York Bike Share/201912-citibike-tripdata.csv')</em><br/><br/><em class="kc">df = pd.concat([file_01, file_02, file_03, ....], ignore_index=True)</em></span></pre><figure class="kt ku kv kw fd kx"><div class="bz dy l di"><div class="nc nd l"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">繁琐的方法，不是很聪明…</figcaption></figure><h1 id="26ed" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">之后:系统有效的方法</h1><p id="548a" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated">我发现做同样的事情更简洁系统的方法。我把这个过程分成几个步骤。不再需要复制+粘贴，因此效率更高，可扩展性更强。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="d10c" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc">'''</em><br/><em class="kc">Step 1: Getting file names within directory that are required for analysis</em><br/><br/><em class="kc">'''</em><br/><br/><em class="kc"># replace 'file_dir' with your working directory</em><br/><em class="kc"># current path of working directory for jupyter notebook and CSV files in Google Colab</em><br/>file_dir = '/content/drive/MyDrive/New York Bike Share'<br/><br/><em class="kc"># getting file names within the directory</em><br/>file_names = glob(path.join(file_dir, '*-citibike-tripdata.csv'))<br/>file_names</span><span id="2a15" class="lu lv hi lq b fi ma lx l ly lz">['/content/drive/MyDrive/New York Bike Share/201901-citibike-tripdata.csv',<br/> '/content/drive/MyDrive/New York Bike Share/201902-citibike-tripdata.csv',<br/> '/content/drive/MyDrive/New York Bike Share/201903-citibike-tripdata.csv',<br/> '/content/drive/MyDrive/New York Bike Share/201904-citibike-tripdata.csv',<br/> '/content/drive/MyDrive/New York Bike Share/201905-citibike-tripdata.csv',<br/> '/content/drive/MyDrive/New York Bike Share/201906-citibike-tripdata.csv',<br/> '/content/drive/MyDrive/New York Bike Share/201907-citibike-tripdata.csv',<br/> '/content/drive/MyDrive/New York Bike Share/201908-citibike-tripdata.csv',<br/> '/content/drive/MyDrive/New York Bike Share/201909-citibike-tripdata.csv',<br/> '/content/drive/MyDrive/New York Bike Share/201910-citibike-tripdata.csv',<br/> '/content/drive/MyDrive/New York Bike Share/201911-citibike-tripdata.csv',<br/> '/content/drive/MyDrive/New York Bike Share/201912-citibike-tripdata.csv']</span></pre><p id="eb62" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里有一个额外步骤，我相信它对于理解变量和数据类型方面的数据集是有用的。我们只导入对分析有意义的变量。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="256a" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># only import 100 line items for quick view of column names and data type</em><br/><em class="kc"># let's leave columns such as 'stop_time', station_id, station_name &amp; bike_id out of this demo</em><br/><br/>pd.read_csv('/content/drive/MyDrive/New York Bike Share/201901-citibike-tripdata.csv', nrows=100).info()</span><span id="4a21" class="lu lv hi lq b fi ma lx l ly lz">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 100 entries, 0 to 99<br/>Data columns (total 15 columns):<br/> #   Column                   Non-Null Count  Dtype  <br/>---  ------                   --------------  -----  <br/> 0   tripduration             100 non-null    int64  <br/> 1   starttime                100 non-null    object <br/> 2   stoptime                 100 non-null    object <br/> 3   start station id         100 non-null    int64  <br/> 4   start station name       100 non-null    object <br/> 5   start station latitude   100 non-null    float64<br/> 6   start station longitude  100 non-null    float64<br/> 7   end station id           100 non-null    int64  <br/> 8   end station name         100 non-null    object <br/> 9   end station latitude     100 non-null    float64<br/> 10  end station longitude    100 non-null    float64<br/> 11  bikeid                   100 non-null    int64  <br/> 12  usertype                 100 non-null    object <br/> 13  birth year               100 non-null    int64  <br/> 14  gender                   100 non-null    int64  <br/>dtypes: float64(4), int64(6), object(5)<br/>memory usage: 11.8+ KB</span></pre><p id="1c99" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你是Python编程的新手，你可能想阅读我以前关于<a class="ae kd" href="https://ks-ong86.medium.com/python-pandas-read-csv-fb0fc322c8c1" rel="noopener"> <strong class="iz hj"> <em class="kc">的帖子。</em></strong></a></p><div class="ne nf ez fb ng nh"><a href="https://ks-ong86.medium.com/python-pandas-read-csv-fb0fc322c8c1" rel="noopener follow" target="_blank"><div class="ni ab dw"><div class="nj ab nk cl cj nl"><h2 class="bd hj fi z dy nm ea eb nn ed ef hh bi translated">蟒蛇熊猫{read_csv}</h2><div class="no l"><h3 class="bd b fi z dy nm ea eb nn ed ef dx translated">不同的视角</h3></div><div class="np l"><p class="bd b fp z dy nm ea eb nn ed ef dx translated">ks-ong86.medium.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv lc nh"/></div></div></a></div><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="1dd2" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc">'''</em><br/><em class="kc">Step 2: Define function to create Pandas DataFrame</em><br/><br/><em class="kc">'''</em><br/><br/>col_index = [0, 1, 5, 6, 9 ,10, 12, 13, 14]<br/><br/>col_name = ['duration', <br/>            'time_start', <br/>            'station_latitude_start', <br/>            'station_longitude_start',<br/>            'station_latitude_end', <br/>            'station_longitude_end', <br/>            'user_type', <br/>            'birth_year', <br/>            'gender']<br/><br/>col_type = {<br/>    'duration': np.int32,<br/>    'station_latitude_start': np.float32,<br/>    'station_longitude_start': np.float32,<br/>    'station_latitude_end': np.float32,<br/>    'station_longitude_end': np.float32,<br/>    'user_type': 'category',<br/>    'birth_year': np.int32,<br/>    'gender': 'category'<br/>}<br/><br/><em class="kc"># self defined function to create dataframe</em><br/><strong class="lq hj">def</strong> create_df(f, size = 100_000):<br/><br/><em class="kc"># create chunks of data frame with 100K per chunk. Result is an iteratable of dataframes</em><br/>    result = pd.read_csv(f, chunksize=size, usecols=col_index,      names=col_name, dtype=col_type, parse_dates=['time_start'], header=0)<br/><br/>    <strong class="lq hj">return</strong> result</span><span id="8eed" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc">'''</em><br/><br/><em class="kc">Step 3: Use .map() to apply 'create_df' function to each file in 'file_names'</em><br/><br/><em class="kc">'''</em><br/><br/><em class="kc"># .map() will apply 'create_df' function to each file in file_names</em><br/><em class="kc"># ref: https://realpython.com/python-map-function/#getting-started-with-pythons-map</em><br/><em class="kc"># result &gt;&gt; list of iteratable. Each iterable contains many dataframes with 100,000 rows</em><br/><br/>df = list(map(create_df, file_names))<br/>df</span><span id="420c" class="lu lv hi lq b fi ma lx l ly lz">[&lt;pandas.io.parsers.TextFileReader at 0x7f20ce82ae10&gt;,<br/> &lt;pandas.io.parsers.TextFileReader at 0x7f20ccd9ced0&gt;,<br/> &lt;pandas.io.parsers.TextFileReader at 0x7f20ccdd7390&gt;,<br/> &lt;pandas.io.parsers.TextFileReader at 0x7f20ccde1dd0&gt;,<br/> &lt;pandas.io.parsers.TextFileReader at 0x7f20ccde1b90&gt;,<br/> &lt;pandas.io.parsers.TextFileReader at 0x7f20ccd6b5d0&gt;,<br/> &lt;pandas.io.parsers.TextFileReader at 0x7f20cefd4290&gt;,<br/> &lt;pandas.io.parsers.TextFileReader at 0x7f20ccdd7a90&gt;,<br/> &lt;pandas.io.parsers.TextFileReader at 0x7f20cd622a90&gt;,<br/> &lt;pandas.io.parsers.TextFileReader at 0x7f20ce8a68d0&gt;,<br/> &lt;pandas.io.parsers.TextFileReader at 0x7f20cee91550&gt;,<br/> &lt;pandas.io.parsers.TextFileReader at 0x7f20ce82aa90&gt;]</span><span id="6057" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc">'''</em><br/><em class="kc">Step 4: apply python 'list comprehension' to get list of DataFrame</em><br/><br/><em class="kc">'''</em><br/><br/><em class="kc"># loop through each iteratable and store each dataframe to list with 'list comprehension'</em><br/><br/>df = [chunk <strong class="lq hj">for</strong> ls <strong class="lq hj">in</strong> df <strong class="lq hj">for</strong> chunk <strong class="lq hj">in</strong> ls]</span><span id="b31f" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc"># let's check one of the dataframe</em><br/><em class="kc"># each dataframe contains up to 100,000 line items</em><br/><em class="kc"># df[0] index number range from 0 to 99,999</em><br/><br/>df[0].info()</span><span id="893b" class="lu lv hi lq b fi ma lx l ly lz">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 100000 entries, 0 to 99999<br/>Data columns (total 9 columns):<br/> #   Column                   Non-Null Count   Dtype         <br/>---  ------                   --------------   -----         <br/> 0   duration                 100000 non-null  int32         <br/> 1   time_start               100000 non-null  datetime64[ns]<br/> 2   station_latitude_start   100000 non-null  float32       <br/> 3   station_longitude_start  100000 non-null  float32       <br/> 4   station_latitude_end     100000 non-null  float32       <br/> 5   station_longitude_end    100000 non-null  float32       <br/> 6   user_type                100000 non-null  category      <br/> 7   birth_year               100000 non-null  int32         <br/> 8   gender                   100000 non-null  category      <br/>dtypes: category(2), datetime64[ns](1), float32(4), int32(2)<br/>memory usage: 3.2 MB</span><span id="6d80" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc"># df[1] index number range from 100,000 to 199,999</em><br/><br/>df[1].info()</span><span id="6c01" class="lu lv hi lq b fi ma lx l ly lz">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 100000 entries, 100000 to 199999<br/>Data columns (total 9 columns):<br/> #   Column                   Non-Null Count   Dtype         <br/>---  ------                   --------------   -----         <br/> 0   duration                 100000 non-null  int32         <br/> 1   time_start               100000 non-null  datetime64[ns]<br/> 2   station_latitude_start   100000 non-null  float32       <br/> 3   station_longitude_start  100000 non-null  float32       <br/> 4   station_latitude_end     100000 non-null  float32       <br/> 5   station_longitude_end    100000 non-null  float32       <br/> 6   user_type                100000 non-null  category      <br/> 7   birth_year               100000 non-null  int32         <br/> 8   gender                   100000 non-null  category      <br/>dtypes: category(2), datetime64[ns](1), float32(4), int32(2)<br/>memory usage: 3.2 MB</span><span id="ec02" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc">'''</em>  <br/><em class="kc">Step 5: use pd.concat() to merge list of dataframes</em>  </span><span id="17c5" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc">'''</em>  </span><span id="dfda" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc"># p.concat() &gt;&gt; append list of dataframe on top of each other to produce master dataframe</em> <br/><em class="kc"># ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html</em> <br/><em class="kc"># note: each dataframe has different index number. parameter 'ignore_index' is set to 'True' and pd.concat() will reset index number after merge.</em>  df = pd.concat(df, ignore_index=<strong class="lq hj">True</strong>) df.info()</span><span id="aad3" class="lu lv hi lq b fi ma lx l ly lz">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 20551697 entries, 0 to 20551696<br/>Data columns (total 9 columns):<br/> #   Column                   Dtype         <br/>---  ------                   -----         <br/> 0   duration                 int32         <br/> 1   time_start               datetime64[ns]<br/> 2   station_latitude_start   float32       <br/> 3   station_longitude_start  float32       <br/> 4   station_latitude_end     float32       <br/> 5   station_longitude_end    float32       <br/> 6   user_type                category      <br/> 7   birth_year               int32         <br/> 8   gender                   category      <br/>dtypes: category(2), datetime64[ns](1), float32(4), int32(2)<br/>memory usage: 666.4 MB</span></pre><h1 id="20a8" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">第2课:熊猫“类别”数据类型</h1><p id="0d98" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated">导入CSV时，性别数据类型设置为<strong class="iz hj"> <em class="kc">【类别】</em> </strong>。read_csv()方法。我了解到，即使我明确排除了性别类别为“0”的行项目，类别仍将保留在pandas的内存/数据帧中。</p><p id="d148" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以用<strong class="iz hj"> <em class="kc"> .cat.categories </em> </strong>属性检查变量的类别，用<strong class="iz hj"><em class="kc">. cat . remove _ categories()</em></strong>方法去掉不需要的类别。最后，我们还可以使用<strong class="iz hj"><em class="kc">. cat . rename _ categories()</em></strong>方法来重命名类别。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="2708" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># Based on data dictionary, 'gender' consists category 0, 1, 2 which category 0 is 'unknown'</em><br/><em class="kc"># .unique() method can easily shows unique category for 'gender'</em><br/><br/>df.gender.unique()</span><span id="720b" class="lu lv hi lq b fi ma lx l ly lz">['1', '2', '0']<br/>Categories (3, object): ['1', '2', '0']</span><span id="4d73" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc"># .cat.categories attribute is an alternative to .unique() method</em><br/><em class="kc"># ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cat.categories.html</em><br/><br/>df.gender.cat.categories</span><span id="3d2f" class="lu lv hi lq b fi ma lx l ly lz">Index(['0', '1', '2'], dtype='object')</span><span id="4e40" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc"># let's remove category '0' from dataframe</em><br/><br/>df = df.loc[df.gender != '0', :]<br/><br/><em class="kc"># check again unique values in 'gender'</em><br/><br/>df.gender.unique()</span><span id="6911" class="lu lv hi lq b fi ma lx l ly lz">['1', '2']<br/>Categories (2, object): ['1', '2']</span></pre><p id="bb28" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们用<em class="kc">做一个按性别划分的用户百分比汇总。value _counts() </em>方法。尽管结果显示为0%,但我们会惊讶地发现类别“0”是摘要的一部分。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="82b2" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># 1 &gt;&gt; 'Male', 2 &gt;&gt; 'Female'</em></span><span id="2e86" class="lu lv hi lq b fi ma lx l ly lz">df.gender.value_counts(normalize=<strong class="lq hj">True</strong>)</span><span id="8506" class="lu lv hi lq b fi ma lx l ly lz">1    0.739935<br/>2    0.260065<br/>0    0.000000<br/>Name: gender, dtype: float64</span></pre><p id="e593" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">原因是，类别' 0 '仍然在熊猫的内存中，尽管我们从数据帧中显式删除了类别' 0 '的性别。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="57c3" class="lu lv hi lq b fi lw lx l ly lz">df.gender.cat.categories</span><span id="4902" class="lu lv hi lq b fi ma lx l ly lz">Index(['0', '1', '2'], dtype='object')</span></pre><h1 id="67e6" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">移除_类别</h1><p id="6078" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated"><strong class="iz hj"><em class="kc">. cat . remove _ categories()</em></strong>帮助删除熊猫内存中未使用的类别</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="4153" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># set parameter inplace = True; pandas doesn't print result of new category</em><br/><br/>df.gender.cat.remove_categories(removals = '0', inplace = <strong class="lq hj">True</strong>)<br/><br/><em class="kc"># category '0' no longer in pandas memory</em><br/><br/>df.gender.cat.categories</span><span id="344d" class="lu lv hi lq b fi ma lx l ly lz">Index(['1', '2'], dtype='object')</span><span id="ebb4" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc"># sumamrise gender percentage....again</em><br/><br/>df.gender.value_counts(normalize=<strong class="lq hj">True</strong>)</span><span id="d753" class="lu lv hi lq b fi ma lx l ly lz">1    0.739935<br/>2    0.260065<br/>Name: gender, dtype: float64</span></pre><h1 id="98ed" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">重命名_类别</h1><p id="b5de" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated"><em class="kc">类别‘1’&amp;‘2’都不是自明的，所以我们用</em><strong class="iz hj"><em class="kc">. cat . rename _ categories()</em></strong><em class="kc">的方法给类别重新命名。</em></p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="be1a" class="lu lv hi lq b fi lw lx l ly lz">df.gender.cat.rename_categories({'1': 'Male', '2': 'Female'}, inplace = <strong class="lq hj">True</strong>)<br/><br/><em class="kc"># new category name for gender</em><br/><br/>df.gender.cat.categories</span><span id="f3aa" class="lu lv hi lq b fi ma lx l ly lz">Index(['Male', 'Female'], dtype='object')</span><span id="4898" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc"># sumamrise gender percentage....again</em><br/><em class="kc"># gender category becomes 'Male' &amp; 'Female'</em><br/><br/>df.gender.value_counts(normalize=<strong class="lq hj">True</strong>)</span><span id="b43b" class="lu lv hi lq b fi ma lx l ly lz">Male      0.739935<br/>Female    0.260065<br/>Name: gender, dtype: float64</span></pre><h1 id="f77b" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">第3课:熊猫特征工程. cut()</h1><p id="7e38" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated">在自行车共享数据框架中，我们被赋予“出生年份”。出生年份对于数据分析/可视化不是很有用。</p><p id="19b8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">想象一下从1970年到2020年的出生年份，无论是柱状图还是柱状图都很难显示。</p><p id="947e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这种情况下，我们可以执行特征工程，将出生年份分组为<em class="kc">代</em>。例如:任何出生于1981-1996年的人都是y一代。因此，我们可以按代而不是出生年份来分析自行车共享活动。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="4af3" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># summary of birth_year (min &amp; max)</em></span><span id="2fdf" class="lu lv hi lq b fi ma lx l ly lz">df.birth_year.min(), df.birth_year.max()</span><span id="c309" class="lu lv hi lq b fi ma lx l ly lz">(1857, 2003)</span><span id="877b" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc"># let's only include birth_year from 1928 (silent generation) onward</em><br/><br/>df = df.query(" birth_year &gt;= 1928 ").reset_index(drop = <strong class="lq hj">True</strong>)</span><span id="d2a5" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc"># total observations after exclude birth_year &lt; 1928</em><br/><em class="kc"># approximately 19M observations</em><br/><br/>df.shape</span><span id="bd89" class="lu lv hi lq b fi ma lx l ly lz">(18982169, 9)</span></pre><h1 id="8543" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">之前:将自定义函数应用于“出生年份”</h1><p id="0fb7" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated">这里我们创建了一个函数，将出生年份分成几代:silent，baby-boomer，gen_x，gen_y &amp; gen_z。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="fbbc" class="lu lv hi lq b fi lw lx l ly lz"><strong class="lq hj">def</strong> define_gen(year):</span><span id="708a" class="lu lv hi lq b fi ma lx l ly lz">      gen = <strong class="lq hj">None</strong>    <br/>      <strong class="lq hj">if</strong> year &lt; 1946:<br/>         gen = 'silent'   <br/>      <strong class="lq hj">elif</strong> (year &gt;= 1946) &amp; (year &lt; 1965):<br/>         gen = 'baby_boomer'<br/>      <strong class="lq hj">elif</strong> (year &gt;= 1965) &amp; (year &lt; 1981):<br/>         gen = 'gen_x'<br/>      <strong class="lq hj">elif</strong> (year &gt;= 1981) &amp; (year &lt; 1997):<br/>         gen = 'gen_y'<br/>      <strong class="lq hj">else</strong>:<br/>         gen = 'gen_z'<br/>      <strong class="lq hj">return</strong> gen</span></pre><p id="4ecd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这样做没有错，但是，这种方法提出了一个巨大的挑战…在大型数据集上执行操作需要很长时间，在这种情况下，有1900万行。</p><p id="a1f4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以使用<em class="kc"> %timeit来找出对每个观察值应用该函数所花费的总时间。</em></p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="be31" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># 6+ second for each observation; how much time required for 19M observations ??</em><br/><br/>%timeit df.birth_year.apply(define_gen)</span><span id="6713" class="lu lv hi lq b fi ma lx l ly lz">1 loop, best of 5: 6.07 s per loop</span></pre><h1 id="7ed1" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">之后:应用<strong class="ak"> pd.cut() </strong></h1><p id="1c05" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated">用<em class="kc"> pd.cut() </em>方法可以达到同样的效果，但时间更短。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="a9dc" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># 528 milisecond (0.528 second) for each observation; this is 11x faster than applying function to 'birth_year'</em><br/><br/>%%timeit <br/><br/>pd.cut(df.birth_year, <br/>       bins = [1928, 1946, 1965, 1981, 1997, 2016], <br/>       labels = ['silent', 'baby_boomer', 'gen_x', 'gen_y',                'gen_z'], <br/>       include_lowest=<strong class="lq hj">True</strong>, ordered=<strong class="lq hj">False</strong>)</span><span id="a222" class="lu lv hi lq b fi ma lx l ly lz">1 loop, best of 5: 528 ms per loop</span></pre><p id="dbf9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">各代自行车共享用户汇总如下:</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="1326" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># Gen Y is the largest user group, accounted for 54%</em><br/><br/>pd.cut(df.birth_year, <br/>       bins = [1928, 1946, 1965, 1981, 1997, 2016], <br/>       labels = ['silent', 'baby_boomer', 'gen_x', 'gen_y', 'gen_z'], <br/>       include_lowest=<strong class="lq hj">True</strong>, <br/>       ordered=<strong class="lq hj">False</strong>).value_counts(normalize=<strong class="lq hj">True</strong>)</span><span id="9e46" class="lu lv hi lq b fi ma lx l ly lz">gen_y          0.543800<br/>gen_x          0.293460<br/>baby_boomer    0.128881<br/>gen_z          0.029339<br/>silent         0.004520<br/>Name: birth_year, dtype: float64</span></pre><h1 id="4da6" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">第4课:Numpy获得更好的性能&amp; Python哈弗辛</h1><blockquote class="nw"><p id="ef96" class="nx ny hi bd nz oa ob oc od oe of js dx translated">哈弗辛= &gt;计算地球上两点之间的距离</p></blockquote><p id="1aec" class="pw-post-body-paragraph ix iy hi iz b ja og ij jc jd oh im jf jg oi ji jj jk oj jm jn jo ok jq jr js hb bi translated">使用Numpy是我在这个项目中学到的最重要的经验之一，尽管熊猫是我这篇文章的主要焦点。它有助于<em class="kc"> (1)减少内存使用&amp; (2)加快进程。</em></p><h1 id="b165" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">之前:使用元组和列表创建坐标</h1><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="794d" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># produce coordinates of start_station</em><br/><em class="kc"># memory usage: +/- 5GB</em><br/><br/>%%memit<br/><br/>list(zip(df.station_latitude_start.values, df.station_longitude_start.values))</span><span id="23e2" class="lu lv hi lq b fi ma lx l ly lz">peak memory: 5406.20 MiB, increment: 2562.55 MiB</span></pre><h1 id="c78d" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">之后:使用Numpy &amp; Haversine创建坐标</h1><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="1e66" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># produce coordinates of start_station</em><br/><em class="kc"># memory usage: +/- 3GB</em><br/><br/>%%memit<br/><br/>np.column_stack((df.station_latitude_start.values, df.station_longitude_start.values))</span><span id="21a0" class="lu lv hi lq b fi ma lx l ly lz">peak memory: 2746.66 MiB, increment: 0.00 MiB</span></pre><p id="3ec6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本节中，我将介绍一个python库<em class="kc"> haversine </em>，它用于测量两个坐标之间的距离(即纬度&amp;经度)。我们可以将<em class="kc">station _ latitude _(start/end)</em>转换成几何坐标，这些将成为哈弗辛公式的输入。也许你正在思考哈弗辛的数学，就像你现在在学校做的一样。我来告诉你:不用麻烦了。你需要做的就是把坐标输入哈弗辛，让Python计算距离。</p><p id="195d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，您确实需要了解一下创建坐标的两种方法之间的区别:<em class="kc"> list + zip </em>与<em class="kc"> np.column_stack </em>。正如你所看到的，在<em class="kc"> np.column_stack的帮助下，</em>与前一种方法相比，同样的过程只消耗2.7GB的内存。</p><p id="17e5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以用一个函数来包装一切:创建坐标&amp;计算距离的哈弗线；所以一切都会一气呵成。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="b878" class="lu lv hi lq b fi lw lx l ly lz"><strong class="lq hj">def</strong> measure_distance(lat_1, long_1, lat_2, long_2):<br/><br/>    <em class="kc"># use numpy .column_stack() instead of list to optimize code operation<br/></em>    coordinate_start = np.column_stack((lat_1, long_1))<br/>    coodrinate_end = np.column_stack((lat_2, long_2))<br/>    <br/>    <em class="kc"># ref: https://pypi.org/project/haversine/</em><br/>    distance = haversine_vector(coordinate_start, coodrinate_end, Unit.KILOMETERS)<br/><br/>    <strong class="lq hj">return</strong> distance</span></pre><p id="2ecb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">嗯，我已经知道结果了；所以我向您展示这个操作的内存使用情况，而不是结果。令人惊讶的是，该函数仅占用创建坐标所需的内存(2.8GB)。而且，计算只需几秒钟就完成了。这真的很神奇。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="7067" class="lu lv hi lq b fi lw lx l ly lz">%%memit<br/><br/>measure_distance(df.station_latitude_start.values, <br/>                 df.station_longitude_start.values, <br/>                 df.station_latitude_end.values, <br/>                 df.station_longitude_end.values)</span><span id="f609" class="lu lv hi lq b fi ma lx l ly lz">peak memory: 2766.87 MiB, increment: 20.20 MiB</span></pre><h1 id="1954" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">第5课:用pandas.assign()创建/替换变量</h1><p id="db66" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated">在pandas中，我们倾向于使用<strong class="iz hj"><em class="kc">df[' variable ']= pandas action</em></strong>来创建/替换变量。如果我们想创建几个变量，可以多次使用这种方法:</p><p id="057b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="kc">df[' var _ 01 ']= action _ 01</em><br/><em class="kc">df[' var _ 02 ']= action _ 02</em><br/><em class="kc">df[' var _ 03 ']= action _ 03</em></p><p id="24ef" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">带<em class="kc">。assign() </em>方法，我们可以用一个命令创建多个变量。在此查看文件<a class="ae kd" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="1896" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对了，assign()方法是受R Tidyverse <em class="kc"> mutate() </em>的启发。更多详细信息，请参考此处的<a class="ae kd" href="https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_r.html#transforming" rel="noopener ugc nofollow" target="_blank">和</a>。下面，我在一个命令中创建变量<em class="kc">生成，持续时间&amp;速度</em>。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="c349" class="lu lv hi lq b fi lw lx l ly lz">hour = np.divide(df.duration, 3600)<br/><br/><em class="kc"># create/replace variable with one command</em></span><span id="d133" class="lu lv hi lq b fi ma lx l ly lz">df = df.assign(generation = pd.cut(df.birth_year, bins = [1928, 1946, 1965, 1981, 1997, 2016], labels = ['silent', 'baby_boomer', 'gen_x', 'gen_y', 'gen_z'], include_lowest=<strong class="lq hj">True</strong>, ordered=<strong class="lq hj">False</strong>), <br/>                duration = np.divide(df.duration.values, 60), <br/>                speed = np.divide(df.distance.values, hour))</span></pre><h1 id="237c" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">第6课:方法链接</h1><p id="28be" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated">正确的方法链接提高了代码的可读性，减少了中间变量的赋值；当我们需要一次执行多个动作时，这尤其有用。</p><p id="1e04" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据之前的经验，我们创建了新的变量(距离、代和速度)并重新设计了持续时间(从几秒钟到几分钟)。现在我们观察到持续时间、距离和速度是超过4位小数的数值变量。分析不再需要纬度&amp;经度和出生年份等变量。</p><p id="80da" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以用<em class="kc">方法链接删除&amp; format变量。</em></p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="e63c" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc">'''<br/>Proper Method Chaining with back slash '\'</em><br/><br/><em class="kc">1. .drop() to remove variables</em><br/><em class="kc">2. .round() to round variables in float type to 2 decimals</em><br/><em class="kc">3. change data type of 'speed' &amp; 'duration' from 'float64' to 'float32'</em><br/><br/><em class="kc">'''</em><br/><br/>df = df.drop(columns=['station_latitude_start', 'station_longitude_start', 'station_latitude_end', 'station_longitude_end', 'birth_year'])\<br/>        .round(2)\<br/>        .astype({'speed': np.float32, 'duration': np.float32})</span></pre><h1 id="834d" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">第八课:重采样时间序列()</h1><p id="877c" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated"><em class="kc">。pandas中的resample() </em>方法对于按小时、天、周、月等组织数据非常有用。例如，下面的代码有助于总结2019年每月的自行车使用情况。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="6258" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># 'M' &gt;&gt; group by "month"</em><br/><em class="kc"># parameter 'on' must be set to a variable with 'datetime64' data type</em><br/><br/>df.resample('M', on = 'time_start')['gender'].count()</span><span id="afe8" class="lu lv hi lq b fi ma lx l ly lz">time_start<br/>2019-01-31     932320<br/>2019-02-28     906456<br/>2019-03-31    1255656<br/>2019-04-30    1627078<br/>2019-05-31    1755529<br/>2019-06-30    1925806<br/>2019-07-31    1984048<br/>2019-08-31    2117821<br/>2019-09-30    2237988<br/>2019-10-31    1938000<br/>2019-11-30    1393111<br/>2019-12-31     908356<br/>Freq: M, Name: gender, dtype: int64</span></pre><h1 id="558b" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">第9课:用熊猫操纵日期时间</h1><p id="ebf2" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated">在我们的数据框架中，“时间开始”变量采用<em class="kc">日期时间64 </em>格式。我们可以用<em class="kc">提取日期/时间。dt </em>后跟日期时间属性/方法(秒、小时、日、周、日名称、月&amp;月名称)。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="3497" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># example: day name (i.e. Monday, Tuesday)</em></span><span id="d6da" class="lu lv hi lq b fi ma lx l ly lz">df.time_start[0:100].dt.day_name()</span><span id="51db" class="lu lv hi lq b fi ma lx l ly lz">0     Tuesday <br/>1     Tuesday <br/>2     Tuesday <br/>3     Tuesday <br/>4     Tuesday        <br/>...    <br/>95    Tuesday <br/>96    Tuesday <br/>97    Tuesday <br/>98    Tuesday <br/>99    Tuesday <br/>Name: time_start, Length: 100, dtype: object</span></pre><h1 id="dab2" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">第十课:熊猫小组</h1><p id="4ead" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated">Groupby可以和<em class="kc">计数/求和/变换</em>一起使用。它可以替代<em class="kc"> resample() </em>按照日期时间变量对数据进行分组。如果方法正确，我们可以将groupby与datetime操作结合起来。</p><p id="de86" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="kc">group by&lt;user _ type&gt;&amp;&lt;gender&gt;+Count =&gt;</em>其中一个实用的例子<em class="kc"> </em>来了解性别混合by user_type。该表按性别列出了用户总数绝对值。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="af7a" class="lu lv hi lq b fi lw lx l ly lz"># Groupby + Count</span><span id="2dc4" class="lu lv hi lq b fi ma lx l ly lz">user_by_gender = df.groupby(by = ['user_type', 'gender'], as_index=<strong class="lq hj">False</strong>)['duration'].count()</span><span id="bd29" class="lu lv hi lq b fi ma lx l ly lz">user_by_gender</span><span id="0a62" class="lu lv hi lq b fi ma lx l ly lz">  user_type  gender duration<br/>0 Customer   Male   1045906<br/>1 Customer   Female 585312<br/>2 Subscriber Male   12999422<br/>3 Subscriber Female 4351529</span></pre><p id="6e71" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过使用函数对<em class="kc"> user_by_gender </em>应用<em class="kc"> groupby + transform，我们得到了用户类型的用户总数。</em></p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="ac83" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># function col.sum() is used to 'transform' total user by user_type when we group the dataframe by 'user_type'</em><br/><br/>user_by_gender.groupby(by = 'user_type').transform(<strong class="lq hj">lambda</strong> col: col.sum())</span><span id="c37c" class="lu lv hi lq b fi ma lx l ly lz">  duration<br/>0 1631218<br/>1 1631218<br/>2 17350951<br/>3 17350951</span></pre><p id="d8a5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以通过一个小技巧来获取用户的性别比例。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="b590" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># we take each number in the 'col' (i.e duration), divide by the sum of each user_type group</em><br/><em class="kc"># example: Male Customer 1,046,076 divided by total user 1,631,433 of 'Customer' user_type == &gt; 64%</em><br/><br/><em class="kc"># result == &gt; percentage of Male/Female user by user_type</em><br/><em class="kc"># note: result is in float type, hence we use .round() method to round the number to ZERO decimal &amp; use .astype() method to convert to integer.</em><br/><br/>user_by_gender.groupby(by = 'user_type')\<br/>.transform(<strong class="lq hj">lambda</strong> col: col / col.sum() * 100)\<br/>.round()\<br/>.astype('int')</span><span id="4109" class="lu lv hi lq b fi ma lx l ly lz">  duration<br/>0 64<br/>1 36<br/>2 75<br/>3 25</span></pre><h1 id="d945" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">第11课:字符串方法</h1><p id="cd3b" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated">熊猫串法，<em class="kc">。str </em>是一种处理熊猫系列(<strong class="iz hj"> NOT list或Numpy array </strong>)的便捷方式。之所以称之为<em class="kc">‘string’</em>方法:它只对数据类型为‘string’的熊猫系列有效。</p><p id="16be" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以用字符串方法做什么？熊猫系列的一些常见应用有:</p><ol class=""><li id="9aa9" class="ke kf hi iz b ja jb jd je jg kg jk kh jo ki js kj kk kl km bi translated">将字符串从小写转换为大写</li><li id="fb6c" class="ke kf hi iz b ja kn jd ko jg kp jk kq jo kr js kj kk kl km bi translated">从文本中切分子字符串</li><li id="2036" class="ke kf hi iz b ja kn jd ko jg kp jk kq jo kr js kj kk kl km bi translated">过滤数据</li><li id="cdfe" class="ke kf hi iz b ja kn jd ko jg kp jk kq jo kr js kj kk kl km bi translated">查找和替换特定文本</li></ol><p id="9c9e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还有更多…</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="242f" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc">'''<br/>In Lesson 03, we group 'birth_year' into 'generation' category. </em> <em class="kc">However, category names are in lower case and separated by underscore '_'</em>  </span><span id="d321" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc">'''</em>  </span><span id="3804" class="lu lv hi lq b fi ma lx l ly lz">pd.Series(df.generation.unique())</span><span id="b58a" class="lu lv hi lq b fi ma lx l ly lz">1    baby_boomer<br/>2          gen_y<br/>3          gen_z<br/>4         silent<br/>dtype: category<br/>Categories (5, object): ['gen_x', 'baby_boomer', 'gen_y', 'gen_z', 'silent']</span></pre><h1 id="c06a" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">将文本从小写转换为大写并替换字符串</h1><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="5ffe" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc">'''</em><br/><em class="kc">with .str.title(), we convert first letter of each word to upper case</em><br/><em class="kc">with .str.replace(), we replace underscore '_' with space ' '</em><br/><br/><em class="kc">reminder !!! .str only works for Series</em><br/><em class="kc">'''</em><br/><br/>pd.Series(df.generation.unique())\<br/>.str.title()\<br/>.str.replace('_', ' ')</span><span id="2f8b" class="lu lv hi lq b fi ma lx l ly lz">0          Gen X<br/>1    Baby Boomer<br/>2          Gen Y<br/>3          Gen Z<br/>4         Silent<br/>dtype: object</span></pre><p id="6cda" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">嗯，实际上，我们可以通过对级数应用lambda函数来完成同样的事情，就像下面这样。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="dc7b" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc">'''</em><br/><em class="kc">note: .str is a more convenient way to the 'apply' function 'title' &amp; 'replace'</em><br/><br/><em class="kc">'''</em><br/><br/>pd.Series(df.generation.unique()).apply(<strong class="lq hj">lambda</strong> word: word.title().replace('_', ' '))</span><span id="8a45" class="lu lv hi lq b fi ma lx l ly lz">0          Gen X<br/>1    Baby Boomer<br/>2          Gen Y<br/>3          Gen Z<br/>4         Silent<br/>dtype: category<br/>Categories (5, object): ['Gen X', 'Baby Boomer', 'Gen Y', 'Gen Z', 'Silent']</span></pre><p id="4139" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在你一定想知道为什么要用<em class="kc">。str </em>代替<em class="kc"> for-loop + list comprehension或者apply + function </em>如果每种方法都能把事情搞定？在大多数情况下，他们有能力做到这一点。</p><p id="5077" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面的代码告诉你<em class="kc">为什么… </em></p><h1 id="b1ca" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">str vs for-loop +列表理解vs应用+函数</h1><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="50a4" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc">'''</em><br/><em class="kc">imagine you have list of month names, which NaN is part of the list...</em><br/><br/><em class="kc">'''</em><br/><br/>month_name = ['January', 'February', np.nan, 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']  <br/><br/>month_name</span><span id="4f1b" class="lu lv hi lq b fi ma lx l ly lz">['January',<br/> 'February',<br/>  nan, ## this is the root cause<br/> 'April',<br/> 'May',<br/> 'June',<br/> 'July',<br/> 'August',<br/> 'September',<br/> 'October',<br/> 'November',<br/> 'December']</span></pre><h1 id="2f14" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">for循环+列表理解</h1><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="7b71" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># use of for-loop within list comprehension</em><br/><em class="kc"># operation fail if there is NaN</em><br/><br/>[word[0:3] <strong class="lq hj">for</strong> word <strong class="lq hj">in</strong> month_name]</span><span id="5bec" class="lu lv hi lq b fi ma lx l ly lz">---------------------------------------------------------------------------<br/>TypeError                                 Traceback (most recent call last)<br/>&lt;ipython-input-57-398382afecd8&gt; in &lt;module&gt;()<br/>      2 # operation fail if there is NaN<br/>      3 <br/>----&gt; 4 [word[0:3] for word in month_name]<br/><br/>&lt;ipython-input-57-398382afecd8&gt; in &lt;listcomp&gt;(.0)<br/>      2 # operation fail if there is NaN<br/>      3 <br/>----&gt; 4 [word[0:3] for word in month_name]<br/><br/>TypeError: 'float' object is not subscriptable</span></pre><h1 id="79d4" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">应用+功能</h1><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="e931" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># apply function to slice the substring</em><br/><em class="kc"># operation fail if there is NaN</em><br/><br/>pd.Series(month_name).apply(<strong class="lq hj">lambda</strong> word: word[0:3])</span><span id="4d5e" class="lu lv hi lq b fi ma lx l ly lz">---------------------------------------------------------------------------<br/>TypeError                                 Traceback (most recent call last)<br/>&lt;ipython-input-58-f030d5097502&gt; in &lt;module&gt;()<br/>      2 # operation fail if there is NaN<br/>      3 <br/>----&gt; 4 pd.Series(month_name).apply(lambda word: word[0:3])<br/><br/>/usr/local/lib/python3.7/dist-packages/pandas/core/series.py in apply(self, func, convert_dtype, args, **kwds)<br/>   4211             else:<br/>   4212                 values = self.astype(object)._values<br/>-&gt; 4213                 mapped = lib.map_infer(values, f, convert=convert_dtype)<br/>   4214 <br/>   4215         if len(mapped) and isinstance(mapped[0], Series):<br/><br/>pandas/_libs/lib.pyx in pandas._libs.lib.map_infer()<br/><br/>&lt;ipython-input-58-f030d5097502&gt; in &lt;lambda&gt;(word)<br/>      2 # operation fail if there is NaN<br/>      3 <br/>----&gt; 4 pd.Series(month_name).apply(lambda word: word[0:3])<br/><br/>TypeError: 'float' object is not subscriptable</span></pre><h1 id="4d47" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">最后，<em class="ol">字符串</em>方法</h1><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="e8b4" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># .str will skip NaN item</em><br/><em class="kc"># everything just works ...</em><br/><br/>pd.Series(month_name).str[0:3]</span><span id="cbb9" class="lu lv hi lq b fi ma lx l ly lz">0     Jan<br/>1     Feb<br/>2     NaN<br/>3     Apr<br/>4     May<br/>5     Jun<br/>6     Jul<br/>7     Aug<br/>8     Sep<br/>9     Oct<br/>10    Nov<br/>11    Dec<br/>dtype: object</span></pre><h1 id="9068" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">第12课:select_dtypes</h1><p id="1cd0" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated"><em class="kc">。select_dtypes() </em>对按特定数据类型的子集列很有用</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="6ce2" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># imagine we want to find out correlation between variables: duration, distance &amp; speed</em><br/><br/>df.dtypes</span><span id="9e3f" class="lu lv hi lq b fi ma lx l ly lz">time_start    datetime64[ns]<br/>user_type           category<br/>gender              category<br/>generation          category<br/>duration             float32<br/>distance             float32<br/>speed                float32<br/>dtype: object</span><span id="2771" class="lu lv hi lq b fi ma lx l ly lz"><em class="kc"># data type for duration, distance &amp; speed is 'float32'</em><br/><em class="kc"># ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html</em><br/><br/>df.select_dtypes(include = np.float32).head()</span><span id="ecb1" class="lu lv hi lq b fi ma lx l ly lz"> duration distance speed<br/>0 5.33    1.07     12.00 <br/>1 5.27    0.58     6.58 <br/>2 9.85    2.03     12.39<br/>3 45.32   1.40     1.86<br/>4 5.05    1.32     15.64 </span></pre><p id="6f4f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过使用<em class="kc"> select_dtypes </em>，我们节省了手工输入子集列的工作量(假设我们有20+列)。</p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="e5de" class="lu lv hi lq b fi lw lx l ly lz"><em class="kc"># .corr() method is used to obtain correlation matrix</em></span><span id="b9af" class="lu lv hi lq b fi ma lx l ly lz">df.select_dtypes(include = np.float32).corr().round(2)</span><span id="ca0e" class="lu lv hi lq b fi ma lx l ly lz">         duration distance speed<br/>duration 1.000000 0.061444 -0.057840<br/>distance 0.061444 1.000000  0.306654<br/>speed.  -0.057840 0.306654  1.000000</span></pre><h1 id="5c12" class="mb lv hi bd mc md mx mf mg mh my mj mk io mz ip mm ir na is mo iu nb iv mq mr bi translated">结束了</h1><p id="c9e3" class="pw-post-body-paragraph ix iy hi iz b ja ms ij jc jd mt im jf jg mu ji jj jk mv jm jn jo mw jq jr js hb bi translated">最后，我的分享告一段落。TQVM为您的时间通读我的超长文章。如果你喜欢探索所有的细节，你可以在我的github @<a class="ae kd" href="https://github.com/ongks-useR/united_states_bike_share/blob/main/data_cleaning.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="iz hj">ongks-useR</strong></a>中找到jupyter笔记本。</p></div></div>    
</body>
</html>