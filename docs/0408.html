<html>
<head>
<title>Building an Artificial Neural Network using Tensorflow 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Tensorflow 2.0构建人工神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-an-artificial-neural-network-in-tensorflow2-0-77b7dea80bd4?source=collection_archive---------16-----------------------#2021-01-15">https://medium.com/analytics-vidhya/building-an-artificial-neural-network-in-tensorflow2-0-77b7dea80bd4?source=collection_archive---------16-----------------------#2021-01-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/d677ed6e8a8a45e493925de6b9412ad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*akHYKaEdaOUazZBmdoNlzg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图片来自Pixabay的Ahmed Gad</figcaption></figure><p id="7c68" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在这本笔记本中，我们将看到如何使用Tensorflow 2.0实现一个人工神经网络。我们将使用从Tensorflow数据集直接导入的时尚MNIST数据集</p><h1 id="7b7a" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">导入库</h1><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="f8ba" class="ky js hh ku b fi kz la l lb lc">import numpy as np<br/>import datetime<br/>import tensorflow as tf<br/>from tensorflow.keras.datasets import fashion_mnist</span><span id="92a1" class="ky js hh ku b fi ld la l lb lc">tf.__version__</span><span id="82f5" class="ky js hh ku b fi ld la l lb lc">'2.3.0'</span></pre><h1 id="56c5" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">数据预处理</h1><h2 id="d3f3" class="ky js hh bd jt le lf lg jx lh li lj kb je lk ll kf ji lm ln kj jm lo lp kn lq bi translated">加载数据集</h2><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="41ac" class="ky js hh ku b fi kz la l lb lc">(X_train, y_train),(X_test, y_test)=fashion_mnist.load_data()</span><span id="3613" class="ky js hh ku b fi ld la l lb lc">Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz<br/>32768/29515 [=================================] - 0s 3us/step<br/>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz<br/>26427392/26421880 [==============================] - 12s 0us/step<br/>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz<br/>8192/5148 [===============================================] - 0s 0us/step<br/>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz<br/>4423680/4422102 [==============================] - 2s 0us/step</span></pre><h2 id="37bd" class="ky js hh bd jt le lf lg jx lh li lj kb je lk ll kf ji lm ln kj jm lo lp kn lq bi translated">标准化图像</h2><p id="28fb" class="pw-post-body-paragraph it iu hh iv b iw lr iy iz ja ls jc jd je lt jg jh ji lu jk jl jm lv jo jp jq ha bi translated">规范化的目标是将数据集中数值列的值更改为一个通用的比例，而不会扭曲值范围的差异。对于机器学习来说，并不是每个数据集都需要归一化。仅当要素具有不同的范围时才需要。</p><p id="57d2" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">这里，我们将训练集和测试集中图像的每个像素除以最大像素数(255)，这样每个像素将在范围[0，1]内。通过标准化图像，我们的模型训练得更快</p><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="167a" class="ky js hh ku b fi kz la l lb lc">X_train=X_train/255<br/>X_test=X_test/255</span></pre><h2 id="0d75" class="ky js hh bd jt le lf lg jx lh li lj kb je lk ll kf ji lm ln kj jm lo lp kn lq bi translated">重塑数据</h2><p id="599c" class="pw-post-body-paragraph it iu hh iv b iw lr iy iz ja ls jc jd je lt jg jh ji lu jk jl jm lv jo jp jq ha bi translated">我们正在建立一个完全连接的网络，我们将训练集和测试集改造成矢量格式</p><p id="6c2c" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">由于每个图像的尺寸是28x28，我们将整个数据集的形状调整为(-1，高x宽)</p><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="1fb8" class="ky js hh ku b fi kz la l lb lc">X_train.shape</span><span id="c04d" class="ky js hh ku b fi ld la l lb lc">(60000, 28, 28)</span><span id="4c0b" class="ky js hh ku b fi ld la l lb lc">X_train=X_train.reshape(-1,28*28)</span><span id="93ba" class="ky js hh ku b fi ld la l lb lc">X_train.shape</span><span id="4267" class="ky js hh ku b fi ld la l lb lc">(60000, 784)</span><span id="f5aa" class="ky js hh ku b fi ld la l lb lc"># Also reshape the X_test<br/>X_test=X_test.reshape(-1,28*28)<br/>X_test.shape</span><span id="ca40" class="ky js hh ku b fi ld la l lb lc">(10000, 784)</span></pre><h1 id="3556" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">构建人工神经网络</h1><h2 id="7e76" class="ky js hh bd jt le lf lg jx lh li lj kb je lk ll kf ji lm ln kj jm lo lp kn lq bi translated">定义模型</h2><p id="97b7" class="pw-post-body-paragraph it iu hh iv b iw lr iy iz ja ls jc jd je lt jg jh ji lu jk jl jm lv jo jp jq ha bi translated">让我们从一个序列模型开始。顺序模型适用于简单的层堆叠，其中每层都有一个输入张量和一个输出张量。顺序API允许您为大多数问题逐层创建模型。它的局限性在于它不允许您创建共享层或具有多个输入或输出的模型。</p><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="28d9" class="ky js hh ku b fi kz la l lb lc">model=tf.keras.models.Sequential()</span></pre><h2 id="fac2" class="ky js hh bd jt le lf lg jx lh li lj kb je lk ll kf ji lm ln kj jm lo lp kn lq bi translated">添加第一个完全连接的隐藏层</h2><p id="da01" class="pw-post-body-paragraph it iu hh iv b iw lr iy iz ja ls jc jd je lt jg jh ji lu jk jl jm lv jo jp jq ha bi translated">层超参数</p><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="5edf" class="ky js hh ku b fi kz la l lb lc">1) number of units/neurons: 128<br/>2) activation function: ReLU<br/>3) input_shape: (784,)</span><span id="97d1" class="ky js hh ku b fi ld la l lb lc">model.add(tf.keras.layers.Dense(units=128, activation='relu',input_shape=(784,)))</span></pre><h2 id="4946" class="ky js hh bd jt le lf lg jx lh li lj kb je lk ll kf ji lm ln kj jm lo lp kn lq bi translated">添加第二层，并去掉</h2><p id="0272" class="pw-post-body-paragraph it iu hh iv b iw lr iy iz ja ls jc jd je lt jg jh ji lu jk jl jm lv jo jp jq ha bi translated">对于用相对较小的数据集训练深度网络来说，辍学是一种简单但有用的技术。丢弃的想法是在每次训练迭代中随机停用网络中的一部分单元，例如50%(图1.10B)。这有助于防止单元之间复杂的共同适应，即，对特定其他单元的存在的不期望的依赖。通过防止复杂的共同适应与退出，它自然有助于避免过度拟合，从而使训练模型更好地推广。dropout的另一个值得注意的影响是提供了一种有效地将许多不同的网络架构指数级地结合起来的方法。训练中单元的随机和临时移除导致不同的网络架构，因此在每次迭代中，可以认为训练了不同的网络，但是它们的连接权重是共享的。在测试中，网络中的所有单元都应该打开，即没有丢失，但权重减半以保持相同的输出范围。</p><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="9b2b" class="ky js hh ku b fi kz la l lb lc">model.add(tf.keras.layers.Dropout(0.2))</span></pre><h2 id="5037" class="ky js hh bd jt le lf lg jx lh li lj kb je lk ll kf ji lm ln kj jm lo lp kn lq bi translated">添加输出图层</h2><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="1f34" class="ky js hh ku b fi kz la l lb lc">1) units: number of classes (10 in Fashion MNIST dataset)<br/>2) activation: softmax</span><span id="515d" class="ky js hh ku b fi ld la l lb lc">model.add(tf.keras.layers.Dense(units=10, activation='softmax'))</span></pre><h2 id="fcb3" class="ky js hh bd jt le lf lg jx lh li lj kb je lk ll kf ji lm ln kj jm lo lp kn lq bi translated">编译模型</h2><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="ec54" class="ky js hh ku b fi kz la l lb lc">1) Optimizer: Adam<br/>2) Loss: Sparse softmax (categorical) crossentropy</span></pre><p id="60a6" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">Adam优化算法是随机梯度下降的扩展，最近在计算机视觉和自然语言处理的深度学习应用中得到了更广泛的采用。Adam不同于经典的随机梯度下降。随机梯度下降为所有权重更新保持单一学习率(称为alpha ),并且学习率在训练期间不变。为每个网络权重(参数)保持一个学习速率，并随着学习的展开而单独调整。Adam意识到AdaGrad和RMSProp的优势。与RMSProp中基于平均一阶矩(平均值)调整参数学习率不同，Adam还利用了梯度二阶矩的平均值(无中心方差)。</p><p id="4491" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">稀疏分类交叉熵和分类交叉熵之间的唯一区别是真实标签的格式。当我们有一个单标签、多类分类问题时，标签对于每个数据是互斥的，这意味着每个数据条目只能属于一个类。然后我们可以用一键嵌入来表示y_true。</p><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="9795" class="ky js hh ku b fi kz la l lb lc">model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])</span><span id="235f" class="ky js hh ku b fi ld la l lb lc">model.summary()</span><span id="8aab" class="ky js hh ku b fi ld la l lb lc">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>dense (Dense)                (None, 128)               100480    <br/>_________________________________________________________________<br/>dropout (Dropout)            (None, 128)               0         <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 10)                1290      <br/>=================================================================<br/>Total params: 101,770<br/>Trainable params: 101,770<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><h2 id="b7ea" class="ky js hh bd jt le lf lg jx lh li lj kb je lk ll kf ji lm ln kj jm lo lp kn lq bi translated">训练模型</h2><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="7c71" class="ky js hh ku b fi kz la l lb lc">model.fit(X_train, y_train, epochs=10)</span><span id="c2be" class="ky js hh ku b fi ld la l lb lc">Epoch 1/10<br/>1875/1875 [==============================] - 4s 2ms/step - loss: 0.5286 - sparse_categorical_accuracy: 0.8122<br/>Epoch 2/10<br/>1875/1875 [==============================] - 5s 3ms/step - loss: 0.3987 - sparse_categorical_accuracy: 0.8558<br/>Epoch 3/10<br/>1875/1875 [==============================] - 4s 2ms/step - loss: 0.3625 - sparse_categorical_accuracy: 0.8688<br/>Epoch 4/10<br/>1875/1875 [==============================] - 4s 2ms/step - loss: 0.3433 - sparse_categorical_accuracy: 0.8730<br/>Epoch 5/10<br/>1875/1875 [==============================] - 4s 2ms/step - loss: 0.3284 - sparse_categorical_accuracy: 0.8804<br/>Epoch 6/10<br/>1875/1875 [==============================] - 5s 2ms/step - loss: 0.3183 - sparse_categorical_accuracy: 0.8833<br/>Epoch 7/10<br/>1875/1875 [==============================] - 4s 2ms/step - loss: 0.3081 - sparse_categorical_accuracy: 0.8849<br/>Epoch 8/10<br/>1875/1875 [==============================] - 5s 3ms/step - loss: 0.2960 - sparse_categorical_accuracy: 0.8903<br/>Epoch 9/10<br/>1875/1875 [==============================] - 4s 2ms/step - loss: 0.2888 - sparse_categorical_accuracy: 0.8917<br/>Epoch 10/10<br/>1875/1875 [==============================] - 4s 2ms/step - loss: 0.2831 - sparse_categorical_accuracy: 0.8944<br/><br/><br/><br/><br/><br/>&lt;tensorflow.python.keras.callbacks.History at 0x7ffa570fd130&gt;</span></pre><h2 id="2df5" class="ky js hh bd jt le lf lg jx lh li lj kb je lk ll kf ji lm ln kj jm lo lp kn lq bi translated">模型评估和预测</h2><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="325f" class="ky js hh ku b fi kz la l lb lc">test_loss, test_accuracy=model.evaluate(X_test, y_test)</span><span id="2eda" class="ky js hh ku b fi ld la l lb lc">313/313 [==============================] - 0s 777us/step - loss: 0.3337 - sparse_categorical_accuracy: 0.8796</span><span id="43f8" class="ky js hh ku b fi ld la l lb lc">print("Test accuracy: {}".format(test_accuracy))</span><span id="3a87" class="ky js hh ku b fi ld la l lb lc">Test accuracy: 0.8795999884605408</span></pre><h1 id="5fd9" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">参考</h1><p id="e8c4" class="pw-post-body-paragraph it iu hh iv b iw lr iy iz ja ls jc jd je lt jg jh ji lu jk jl jm lv jo jp jq ha bi translated"><a class="ae lw" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">https://machinelingmastery . com/Adam-optimization-algorithm-for-deep-learning/</a><br/><a class="ae lw" href="https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff" rel="noopener ugc nofollow" target="_blank">https://towardsai . net/p/data-science/how-when-and-why-should-you-normalize-rescale-your-data-3f 083 def 38 ff</a><br/><a class="ae lw" href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machinelingmastery . com/dropout-for-regulating-deep-neural-networks/</a><br/><a class="ae lw" href="https://keras.io/guides/sequential_model/" rel="noopener ugc nofollow" target="_blank">https://keras.io/guides/sequential_model/</a><br/></p></div></div>    
</body>
</html>