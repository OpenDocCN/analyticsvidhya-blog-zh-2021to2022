<html>
<head>
<title>How to scrape articles from data science publications using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用Python从数据科学出版物中抓取文章</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-data-science-articles-attract-more-attention-part-1-efe8faf209d0?source=collection_archive---------28-----------------------#2021-01-17">https://medium.com/analytics-vidhya/what-data-science-articles-attract-more-attention-part-1-efe8faf209d0?source=collection_archive---------28-----------------------#2021-01-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="fa84" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">哪些数据科学文章更受关注(第1部分)</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/3652702e5dc448d7e4f7067e4a882bf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3ybGLlbePGudkiPB"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">Elena Mozhvilo 在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="4efb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="kk">你有没有想过，是什么让一篇文章变得伟大？</em> <em class="kk">数据科学世界的读者对哪些特定领域更感兴趣？我当然做了！</em>我的目标是通过分析媒体上数据科学出版物的文章来找到这些问题的答案。这一系列文章将涵盖诸如网络抓取、清理文本数据和主题建模等领域。</p><p id="e90e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在本系列的第1部分中，我们将通过使用Python的web抓取从各种数据科学出版物中获取历史文章。</p><h2 id="b195" class="kl km hi bd kn ko kp kq kr ks kt ku kv jx kw kx ky kb kz la lb kf lc ld le lf bi translated"><strong class="ak">数据科学出版物</strong></h2><p id="bbd1" class="pw-post-body-paragraph jo jp hi jq b jr lg ij jt ju lh im jw jx li jz ka kb lj kd ke kf lk kh ki kj hb bi translated">有很多关于Medium的出版物，涵盖了数据科学和编程等主题。我们将从三个出版物获取文章，<a class="ae jn" href="https://towardsdatascience.com/" rel="noopener" target="_blank"> TDS </a>、<a class="ae jn" href="https://medium.com/towards-artificial-intelligence" rel="noopener">走向AI </a>和<a class="ae jn" href="https://medium.com/analytics-vidhya" rel="noopener"> Analytics Vidhya </a>。这应该为我们未来的分析提供足够大的数据集。</p><p id="b742" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> <em class="kk">导航到档案</em> </strong></p><p id="8205" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">通过简单的谷歌搜索，我们可以获得我们感兴趣的出版物档案的链接。除了TDS之外，几乎所有出版物都有以下形式的URL“https://medium . com/publication-name/archive”。但是页面的布局保持不变。</p><p id="fe44" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们检查出版物的存档页面，在页面顶部，我们可以找到文章发表的年份，通过单击其中一年，我们将看到月份，类似地，要获得天数，我们需要单击特定的月份。现在，需要注意的一件事是，不是所有的年份都有月份，也不是所有的月份都有日子。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ll"><img src="../Images/68c0258a3358a4ccbe110c44770ca613.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l3nRhYNBTDg35peZqVZYMg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://towardsdatascience.com/archive" rel="noopener" target="_blank">https://towardsdatascience.com/archive</a></figcaption></figure><h2 id="6125" class="kl km hi bd kn ko kp kq kr ks kt ku kv jx kw kx ky kb kz la lb kf lc ld le lf bi translated">我们如何获得所有日期的链接？</h2><p id="555c" class="pw-post-body-paragraph jo jp hi jq b jr lg ij jt ju lh im jw jx li jz ka kb lj kd ke kf lk kh ki kj hb bi translated">我们首先检查归档页面，我们可以看到链接存储在带有类<em class="kk">‘time bucket…’</em>的<em class="kk">‘div’</em>容器中。三个容器的类的命名约定的唯一区别是宽度。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lm"><img src="../Images/f101c7eb997523d61f68e5899b636018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*61hcbpUHXBXfMczo-AfhzA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">检查<a class="ae jn" rel="noopener" href="https://medium.com/analytics-vidhya/archive">分析Vidhya </a>存档页面。</figcaption></figure><p id="322e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们已经获得了获取链接所需的所有信息，我们可以编写代码了。让我们从这个任务所需的导入开始，我们将使用Python库请求和BeautifulSoup来发出HTTP请求并从HTML中提取数据。</p><pre class="iy iz ja jb fd ln lo lp lq aw lr bi"><span id="ac95" class="kl km hi lo b fi ls lt l lu lv">from bs4 import BeautifulSoup<br/>import requests</span></pre><p id="98f0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">接下来我们定义名为<strong class="jq hj"> <em class="kk"> get_all_links </em> </strong>的函数，它使用出版物存档的url作为输入，并返回所有日期的链接。它分为三部分—获取年、获取月和获取日。在代码中，我们还有额外的变量，如<em class="kk"> years_no_months </em>和<em class="kk"> all_links_no_days </em>，它们收集没有月份的年份和没有日期的月份的链接。</p><pre class="iy iz ja jb fd ln lo lp lq aw lr bi"><span id="2ce4" class="kl km hi lo b fi ls lt l lu lv">def get_all_links(url):<br/>    '''function to obtain all the links to archive pages'''<br/>    # url - url to publication archive<br/>    r = requests.get(url)<br/>    # get years<br/>    soup = BeautifulSoup(r.text, 'html.parser')<br/>    search = soup.find_all('div', class_='timebucket u-inlineBlock u-width50')<br/>    years = []<br/>    for h in search:<br/>        years.append(h.a.get('href'))<br/> <br/>    # get months <br/>    years_months = []<br/>    years_no_months = [] # for the years that don't have months<br/>    for year in years:<br/>        y_soup =BeautifulSoup(requests.get(year).text,'html.parser')<br/>        search_months = y_soup.find_all('div', class_='timebucket u-inlineBlock u-width80')<br/>        months = []<br/>        if search_months:<br/>            for month in search_months:<br/>                try:<br/>                    months.append(month.a.get('href'))<br/>                except:<br/>                    pass<br/>            years_months.append(months)<br/>        else:<br/>            years_no_months.append(year)</span><span id="afda" class="kl km hi lo b fi lw lt l lu lv">years_months = [item for sublist in years_months for item in sublist] <br/>    <br/>    # get days<br/>    all_links = []<br/>    all_links_no_days = [] # for the month that don't have days<br/>    for month_url in years_months:<br/>        m_soup = BeautifulSoup(requests.get(month_url).text, 'html.parser')<br/>        all_days = m_soup.find_all('div', class_='timebucket u-inlineBlock u-width35')<br/>        days = []<br/>        if all_days:<br/>            for day in all_days:<br/>                try:<br/>                    days.append(day.a.get('href'))<br/>                except:<br/>                    pass<br/>            all_links.append(days)<br/>        else:<br/>            all_links_no_days.append(month_url)    <br/>    all_links = [item for sublist in all_links for item in sublist]<br/>    final_links = years_no_months+all_links_no_days+all_links<br/>    return final_links</span><span id="8e45" class="kl km hi lo b fi lw lt l lu lv">towards_ai_links = get_all_links('<a class="ae jn" rel="noopener" href="/towards-artificial-intelligence/archive'">https://medium.com/towards-artificial-intelligence/archive'</a>)</span></pre><p id="8823" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最终输出是出版物档案中所有页面的链接列表。</p><pre class="iy iz ja jb fd ln lo lp lq aw lr bi"><span id="2db7" class="kl km hi lo b fi ls lt l lu lv">['https://medium.com/towards-artificial-intelligence/archive/2015', ...]</span></pre><h2 id="3ce7" class="kl km hi bd kn ko kp kq kr ks kt ku kv jx kw kx ky kb kz la lb kf lc ld le lf bi translated">我们如何获得单个文章的数据？</h2><p id="0108" class="pw-post-body-paragraph jo jp hi jq b jr lg ij jt ju lh im jw jx li jz ka kb lj kd ke kf lk kh ki kj hb bi translated">检查页面也是这个任务的一个很好的起点。所有文章框都有标准格式，因此获得一个框的信息将允许我们收集所有其他文章的所有信息。</p><p id="e8ac" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在我们编写代码之前，我们需要建立存储我们需要的信息的元素和它们的类。此时，我们只对每篇文章的标题、副标题、鼓掌次数和回复感兴趣。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/dcee5dbe46e156eabe0140d651e63160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jA59l80Nputabv4gVT-gaw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">检查单个物品</figcaption></figure><p id="a302" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们现在有了编写代码的所有信息，我们从主脚本的较小片段开始。为了从单个链接中获取所有文章，我们使用类“streamItem …”搜索“div”容器。</p><pre class="iy iz ja jb fd ln lo lp lq aw lr bi"><span id="4d42" class="kl km hi lo b fi ls lt l lu lv">link = 'https://medium.com/towards-artificial-intelligence/archive/2015'</span><span id="3a63" class="kl km hi lo b fi lw lt l lu lv">soup_ = BeautifulSoup(requests.get(link).text, 'html.parser')<br/>articles = soup_.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')</span></pre><p id="6a73" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">接下来，我们获取一篇文章的标题和副标题，在下面的代码中，我们有两个元素用于标题<em class="kk">‘H3’</em>和<em class="kk">‘H2’</em>，这是因为一些作者选择了一种<em class="kk">不同的方法来编写标题</em>。对于缺少标题或副标题的情况，我们将其设置为空，但是该值可以更改为任何值。</p><pre class="iy iz ja jb fd ln lo lp lq aw lr bi"><span id="0553" class="kl km hi lo b fi ls lt l lu lv">article = articles[0] #get single article<br/>if article.h3:<br/>    title = article.h3.getText()<br/>elif article.h2:<br/>      title = article.h2.getText()<br/>else:<br/>    title = ''</span><span id="cf0c" class="kl km hi lo b fi lw lt l lu lv">if article.h4:<br/>    subtitle = article.h4.getText()<br/>else:<br/>    subtitle = ''</span></pre><p id="6d7d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了获得掌声和响应，我们找到相应的元素及其类，在这种情况下，我们不只是对文本感兴趣，我们需要获得整数值。有时，单个文章的点击量达到数千，在这种情况下，它们以下列形式表示' 1k '，' 2.2k '等，为了处理这种情况，我们用空条目替换' K '，将字符串转换为整数，然后乘以1000。对于所有其他值，我们要么将其转换为整数，要么将其设置为零。</p><p id="0941" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">当一篇文章有一些来自读者的回应时，它们会有以下几种形式:1个回应、2个回应等等。所以在这里，我们只想获得整数，我们这样做是通过在字符串中搜索<em class="kk"> '\d+' </em>正则表达式。</p><pre class="iy iz ja jb fd ln lo lp lq aw lr bi"><span id="2666" class="kl km hi lo b fi ls lt l lu lv">import re<br/>s_clap =article.find('button', class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents')<br/>if s_clap:<br/>    s_clap = s_clap.getText()<br/>    if 'K' in s_clap:<br/>         clap = int(float(s_clap.replace('K', '')))*1000<br/>    else:<br/>        clap = int(s_clap)       <br/>else:<br/>    clap = 0</span><span id="4f18" class="kl km hi lo b fi lw lt l lu lv">s_response = article.find('a', class_='button button--chromeless u-baseColor--buttonNormal')<br/>if s_response:<br/>    s_response = s_response.getText()<br/>    response =int(re.search(r'\d+', s_response).group()))<br/>else:<br/>    response = 0</span></pre><p id="be9b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们将所有的代码片段集成到一个名为<strong class="jq hj"><em class="kk">get _ data _ all _ articles</em></strong>的函数中，它的主要参数是我们前面提到的<strong class="jq hj"><em class="kk">get _ all _ links</em></strong>函数的输出(链接到单个出版物的所有存档页面)。</p><pre class="iy iz ja jb fd ln lo lp lq aw lr bi"><span id="a740" class="kl km hi lo b fi ls lt l lu lv">from tqdm import tnrange #to keep track of the progress <br/>import pandas as pd<br/>import re</span><span id="b995" class="kl km hi lo b fi lw lt l lu lv">def get_data_all_articles(final_links):<br/>    titles, sub_titles, claps, responses = [], [], [], []<br/><br/>    for link, z in zip(final_links, tnrange(len(final_links))):<br/>        soup_ =BeautifulSoup(requests.get(link).text, 'html.parser')<br/>        articles = soup_.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')<br/>        title, subtitle, clap, response = [], [], [], []<br/>        for article in articles:<br/>            if article.h3:<br/>                title.append(article.h3.getText())<br/>            elif article.h2:<br/>                title.append(article.h2.getText())<br/>            else:<br/>                title.append('')</span><span id="3957" class="kl km hi lo b fi lw lt l lu lv">            if article.h4:<br/>                subtitle.append(article.h4.getText())<br/>            else:<br/>                subtitle.append('')</span><span id="bfe2" class="kl km hi lo b fi lw lt l lu lv">            s_clap =article.find('button', class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents')<br/>            if s_clap:<br/>                s_clap = s_clap.getText()<br/>                if 'K' in s_clap:<br/>                    clap.append(int(float(s_clap.replace('K', '')))*1000)<br/>                else:<br/>                    clap.append(int(s_clap))              <br/>            else:<br/>                clap.append(0)<br/>            s_response = article.find('a', class_='button button--chromeless u-baseColor--buttonNormal')<br/>            if s_response:<br/>                s_response = s_response.getText()<br/>                response.append(int(re.search(r'\d+', s_response).group()))<br/>            else:<br/>                response.append(0)<br/>        titles.append(title)<br/>        sub_titles.append(subtitle) <br/>        claps.append(clap)<br/>        responses.append(response) <br/>    titles = [item for sublist in titles for item in sublist]<br/>    sub_titles = [item for sublist in sub_titles for item in sublist]<br/>    claps = [item for sublist in claps for item in sublist]<br/>    responses = [item for sublist in responses for item in sublist]<br/>    frame = pd.DataFrame([titles, sub_titles, claps, responses]).transpose()<br/>    frame.columns = ['Title', 'Subtitle', 'Claps', 'Responses']<br/>    return frame</span><span id="c306" class="kl km hi lo b fi lw lt l lu lv">data_set = get_data_all_articles(towards_ai_links)</span></pre><p id="9712" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">运行脚本可能需要一些时间，这取决于HTTP请求的数量，每页包含10篇文章，一些出版物有相当大的存档。该脚本包括<em class="kk"> tqdm </em>包来跟踪进度。</p><p id="67b4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">运行脚本后，我们得到以下数据集</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ly"><img src="../Images/cb62e0715d84fa76415757972983c6c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xvspjF1U_uU7arkrCxKQ-w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">面向AI 发布的<a class="ae jn" href="https://medium.com/towards-artificial-intelligence" rel="noopener">最终数据集</a></figcaption></figure><p id="8ff3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">总结</strong></p><p id="1522" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们通过从一些出版物的档案中获取数据集，开始了我们的旅程，以揭示读者对数据科学中的哪些领域更感兴趣。关于文章的信息并不局限于我们所介绍的特性，还有更多的指标需要获取，比如作者是否有图片，文章有多长等等。这可以以类似于获得标题、副标题、掌声和响应的方式来完成。</p><p id="8a10" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">请关注本系列的后续文章，我们将继续进行数据清理和主题建模。</p></div></div>    
</body>
</html>