<html>
<head>
<title>An overview of NTIRE 2020 Extreme Super-Resolution Challenge</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NTIRE 2020极限超分辨率挑战综述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-overview-of-ntire-2020-extreme-super-resolution-challenge-c69b4daf41fd?source=collection_archive---------14-----------------------#2021-04-05">https://medium.com/analytics-vidhya/an-overview-of-ntire-2020-extreme-super-resolution-challenge-c69b4daf41fd?source=collection_archive---------14-----------------------#2021-04-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="5435" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">对抗挑战</h1><p id="7ea4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">2020 NTIRE极限超分辨率挑战[1]是关于以x16的缩放因子超分辨率图像。挑战赛论文回顾了19种方法，这些方法旨在解决这一问题并争夺感知性能。我们将概述比赛是如何进行的，以及挑战参与者提出的一些高分方法的直觉。</p><p id="ea15" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">与SISR对x4等中等因子的积极研究相比，对极端超分辨率的研究并不多。传统的基于MSE的解决方案继承了为SR输出平滑图像的行为，这种现象在这个挑战中变得更加极端，因为可以从给定的LR图像生成更多的HR片。</p><p id="d2e0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><a class="ae kf" href="https://data.vision.ee.ethz.ch/cvl/ntire20/" rel="noopener ugc nofollow" target="_blank"> NTIRE 2020 </a>，<a class="ae kf" href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Zhang_NTIRE_2020_Challenge_on_Perceptual_Extreme_Super-Resolution_Methods_and_Results_CVPRW_2020_paper.pdf" rel="noopener ugc nofollow" target="_blank"> NTIRE 2020感知极限超分辨率挑战:方法与结果</a></p><h2 id="1083" class="kg if hh bd ig kh ki kj ik kk kl km io jn kn ko is jr kp kq iw jv kr ks ja kt bi translated">资料组</h2><p id="5648" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">具有1，700个8K图像的Div8K数据集用于挑战中的训练和测试数据。该数据集只有在参加了<a class="ae kf" href="https://competitions.codalab.org/competitions/22217" rel="noopener ugc nofollow" target="_blank"> Codalabs挑战赛</a>后才可用。我也在Google Drive上传了数据集，尽管由于我不确定数据集提供商的政策，我不会公开分享链接。</p><h2 id="ade3" class="kg if hh bd ig kh ki kj ik kk kl km io jn kn ko is jr kp kq iw jv kr ks ja kt bi translated">感知评价</h2><p id="a21c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这一挑战是基于感知测量而不是PSNR/SSIM来排名的。这两个测量是学习感知图像块相似性(LPIPS)[2]和感知指数(PI)。LPIPS通过专门训练的CNN的激活来测量，PI通过用户调查来评估。</p><h1 id="72de" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">趋势和概述</h1><p id="13f2" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">主要地，网络结构和损失函数被修改以增加模型的能力和改善超分辨率图像的感知质量。</p><p id="765b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">几个团队扩展了现有的架构，如RCAB、ESRGAN，并针对该问题逐步升级，一些团队直接重建了16倍的缩放因子。</p><p id="28f6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">大多数团队采用了L1损失，或者通过结合使用L1、VGG感知和相对论GAN损失，使用了ESRGAN的相同损失。CIPLAB用LPIPS损失替换了VGG损失，一些团队用类似U-Net的架构来区分鉴别器。</p><p id="b5f4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">与2019年AIM感知极限SR挑战赛的结果相比，感知和PSNR测量的SOTA都有所提高。虽然，从结果可以看出，挑战远未解决。特别是对于空间信息的恢复，所提出的方法似乎没有输出视觉上令人满意的结果(第二图)。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/306154822fa63b5f6c7581ed2cc1777e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CEhReiAcLWCHi4ywlUDgpQ.png"/></div></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/d6dd72af424181f288b82ac45d442b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dnMgCptN1jCurlyxuKAedw.png"/></div></div></figure><h1 id="a2c7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">方法</h1><p id="200b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们将回顾挑战中提出的前3种方法。下表对通过不同方法评估的各种指标的性能进行了排名。官方文件[1]中提供了所有方法的详细描述。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lg"><img src="../Images/322a59c4da360e9d020ee2003d8e038b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2qaJXdZhUk70nHH54oJZpA.png"/></div></div></figure><h2 id="7fbe" class="kg if hh bd ig kh ki kj ik kk kl km io jn kn ko is jr kp kq iw jv kr ks ja kt bi translated">OPPO-Research[3]</h2><p id="b254" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">像挑战赛中的许多其他作品一样，提议的RFB-斯尔根架构也是基于ESRGAN的。第一个Trunk-A模型由ESRGAN中提出的16个RRDBs组成。以下Trunk-RFB模块由一个基于感受野块(RFB)的DenseNet架构组成，该架构可以有效地从不同尺度捕获信息。然后，通过亚像素卷积和最近邻插值的交替层来重构特征图，这两种方法都可以大大减少时间开销。提议的RFB区块和网络管道如下图所示。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lh"><img src="../Images/489ec509f53158bf1a80e0b3fec021a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*WmcU1MpTi9DhJ9qZ1arQcw.png"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es li"><img src="../Images/cd7e9910694fd3b12a0dd2a60d43483e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*Ky7VtVIQIiIkbJi7lvzXJQ.png"/></div></figure><h2 id="de8a" class="kg if hh bd ig kh ki kj ik kk kl km io jn kn ko is jr kp kq iw jv kr ks ja kt bi translated">CIPLAB[4]</h2><p id="b93b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">CIPLAB团队使用LPIPS损失代替VGG损失来计算感知损失。因为VGG网络是为图像分类而训练的，所以它可能不是SR的最佳选择。建议的生成器由两个ESRGAN生成器组成，鉴别器是具有连续下采样和上采样操作的U-Net架构，旨在提供逐像素和全局上下文鉴别。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lj"><img src="../Images/1d1d97aba2a233caf749ec96e8b8673d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*XinEMDVboO6BDdZzVzBmKw.png"/></div></figure><h2 id="6568" class="kg if hh bd ig kh ki kj ik kk kl km io jn kn ko is jr kp kq iw jv kr ks ja kt bi translated">HiImageTeam[5]</h2><blockquote class="lk ll lm"><p id="c7f0" class="jc jd ln je b jf ka jh ji jj kb jl jm lo kc jp jq lp kd jt ju lq ke jx jy jz ha bi translated">HiImageTeam提出了用于感知极限SR的级联SR-GAN (CSRGAN)。如图6所示，CSRGAN通过四个连续的×2子网(CSRB)实现了×16的升级。为了提高性能，提出了一种新的剩余密集信道关注块(见图7)。最终CSRGAN使用VGG感知损失和GAN损失来增强超分辨率图像的感知质量。</p></blockquote><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lj"><img src="../Images/4ebab1617a39b347a729b35b2fa93776.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*4706mt4Am1DjDeH0_tgQSw.png"/></div></figure><h1 id="609a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考</h1><p id="57b0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">[1]张，顾，苏，，等(2020).感知极限超分辨率的NTIRE 2020挑战:方法和结果。IEEE/CVF计算机视觉和模式识别研讨会会议录(第492–493页)。</p><p id="9131" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">[2]张，r .，伊索拉，p .，埃夫罗斯，A. A .，谢克特曼，e .，&amp;王，O. (2018)。深度特征作为感知度量的不合理有效性。在<em class="ln">IEEE计算机视觉和模式识别会议论文集</em>(第586–595页)。</p><p id="0b80" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">[3]尚，戴，朱，杨，郭，杨(2020).具有感受野块的感知极限超分辨率网络。在<em class="ln">IEEE/CVF计算机视觉和模式识别研讨会论文集</em>(第440–441页)。</p><p id="3735" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">[4]乔，杨，s .，，金，S. J. (2020)。研究超高分辨率的损失函数。在<em class="ln">IEEE/CVF计算机视觉和模式识别研讨会论文集</em>(第424–425页)。</p><p id="699f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">[5]王，郑，叶，米，杨，方，白，谢，佐藤信一.(2018年7月)。用于尺度自适应低分辨率个人再识别的级联SR-GAN。在<em class="ln"> IJCAI </em> (Vol. 1，№2，p. 4)。</p></div></div>    
</body>
</html>