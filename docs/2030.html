<html>
<head>
<title>How to achieve state of the art results on NLP classification tasks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在自然语言处理分类任务中获得最先进的结果</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-achieve-state-of-the-art-results-on-nlp-classification-tasks-342fa07c7fe9?source=collection_archive---------13-----------------------#2021-04-01">https://medium.com/analytics-vidhya/how-to-achieve-state-of-the-art-results-on-nlp-classification-tasks-342fa07c7fe9?source=collection_archive---------13-----------------------#2021-04-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="26eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用拥抱人脸库释放迁移学习的力量</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/9c7c4d48b54799d43fb5a85829312abc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XWFCPAE550K9KiQU"/></div></div></figure><p id="420e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者安东尼奥·利斯</p><h1 id="970b" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">介绍</h1><p id="4278" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">大家好，在我之前的帖子“如何在不到10分钟的时间内创建一个最先进的图像分类器”之后，我决定为NLP空间做一些类似的事情。</p><p id="8779" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，在这篇文章中，我们将看到如何使用拥抱人脸库和利用迁移学习的力量轻松创建一个最先进的模型分类器，就像我们在Rover分类挑战中所做的那样。</p><h1 id="0173" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">资料组</h1><p id="1c37" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">我们将用来测试我们模型的数据集是我们从<a class="ae kr" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载的经典<a class="ae kr" href="https://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank"> IMDB数据集</a>。数据集有50K个电影评论，根据评论的投票，用二进制标签正面/负面，&gt;= 10分之7是正面，&lt; 7是负面。这是一个二元情感分类的数据集。</p><h1 id="a204" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">数据探索</h1><p id="1021" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">让我们来看看数据:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ks"><img src="../Images/80ba088bb859c2ee6f804afad52c3a36.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/0*f_vyCF3nbQrfUCJl"/></div></figure><p id="5724" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里没有什么复杂的，让我们看看一些关于评论和情绪变量的统计数据。</p><p id="e48a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">查看我们的目标变量“情绪”，我们可以看到数据完全平衡，我们有25k正面和25k负面评论:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kt"><img src="../Images/43406dd70462f7bb3b7734ec3d8d9a2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/0*uBcZSi5FL2s43-WT"/></div></figure><p id="7b69" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们还可以看看评论长度的分布:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ku"><img src="../Images/642f1cfe4d8f3c93df15103611aa6a4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/0*jwnkINUQlX3oeR_T"/></div></div></figure><p id="d6bf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">查看超过7000个字符的评论，我们在整个数据集上获得71/29的分布，有利于正面评论的50/50分布:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kv"><img src="../Images/2a35fc24e134758592cbe684a25e8398.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/0*_IA_BqaMfok7gXZ9"/></div></figure><p id="d243" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以，看起来长评论有更高的概率有一个积极的情绪。在我们的建模中，这可能是一个很好的变量，但是这里我们对使用纯NLP管道感兴趣，所以我们将忽略它。</p><p id="1770" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在进入建模阶段之前，我们需要将数据集分为训练集和测试集:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="fb61" class="lb jp hh kx b fi lc ld l le lf">df_train = df[:25000]<br/>df_test = df[25000:]</span></pre><p id="d51e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好了，我们现在准备启动一些神经网络。</p><h1 id="b2c7" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">开始简单</h1><p id="ceb1" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">我们将非常简单地从NLP世界中使用的一些标准架构开始。首先，我们定义一些参数:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="e89d" class="lb jp hh kx b fi lc ld l le lf">MAX_SEQUENCE_LENGTH = 1000<br/>MAX_NUM_WORDS = 20000<br/>EMBEDDING_DIM = 100</span></pre><p id="baaf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">MAX_SEQUENCE_LENGTH参数是我们将在模型中考虑的单词序列的最大长度。MAX_NUM_WORDS是分词器根据词频考虑的最大字数。EMBEDDING_DIM是我们将在模型中使用的嵌入层的大小。</p><h1 id="cf8c" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">数据预处理</h1><p id="e747" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">让我们从标记我们的评论开始:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="a419" class="lb jp hh kx b fi lc ld l le lf">tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) tokenizer.fit_on_texts(df_train.review.values) </span><span id="924c" class="lb jp hh kx b fi lg ld l le lf">train_sequences = tokenizer.texts_to_sequences(df_train.review.values) </span><span id="99b5" class="lb jp hh kx b fi lg ld l le lf">test_sequences = tokenizer.texts_to_sequences(df_test.review.values) </span><span id="06aa" class="lb jp hh kx b fi lg ld l le lf">word_index = tokenizer.word_index</span></pre><p id="e855" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用Keras的标记器来识别评论中的单词。然后，我们使用texts_to_sequences方法将文本数组转换为数字数组，其中每个单词都有自己的索引保存在字典中(word_index)。</p><p id="e685" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将它应用于训练集，并将其应用于训练集和测试集。然后我们填充生成的序列。这是必要的，因为GPU希望所有矩阵的长度相同，所以我们将使用MAX _ SEQUENCE _ LENGHT参数来定义数组的形状，当序列较短时，我们用一串零填充开头。</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="f9bd" class="lb jp hh kx b fi lc ld l le lf">X_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH) X_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)</span></pre><p id="5999" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于目标变量，我们需要从一个单值变量(1，0，1等等)转换成一种hot编码形式([0，1]，[1，0]，[0，1])。我们可以使用Keras中的to _ categorical函数:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="ab99" class="lb jp hh kx b fi lc ld l le lf">y_train = to_categorical(np.asarray(df_train.sentiment.map(labels_index)))</span><span id="7a89" class="lb jp hh kx b fi lg ld l le lf">y_test = to_categorical(np.asarray(df_test.sentiment.map(labels_index)))</span></pre><p id="6681" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好了，我们基本上完成了数据预处理，现在我们可以定义我们的第一个神经网络。</p><h1 id="1213" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">一维卷积神经网络</h1><p id="0a3f" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">我们的第一个模型是一维卷积神经网络:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="b28e" class="lb jp hh kx b fi lc ld l le lf">model = Sequential() model.add(Embedding(num_words, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)) <br/>model.add(Conv1D(128, 5, activation='relu')) model.add(MaxPooling1D(5)) model.add(Conv1D(128, 5, activation='relu')) <br/>model.add(MaxPooling1D(5)) <br/>model.add(Conv1D(128, 5, activation='relu')) model.add(GlobalMaxPooling1D()) <br/>model.add(Dense(128, activation='relu')) model.add(Dense(len(labels_index), activation='softmax'))</span></pre><p id="05b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以使用summary()方法很容易地看到模型的架构:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lh"><img src="../Images/f3ab6e2a9ff066b814b6729e92b5efca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/0*A2MWXFHwNgtX2Exo"/></div></figure><p id="f861" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好，让我们来训练它:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="5964" class="lb jp hh kx b fi lc ld l le lf">model = Sequential() <br/>model.add(Embedding(MAX_NUM_WORDS, 128)) <br/>model.add(LSTM(128, dropout=0.2)) model.add(Dense(2, activation='sigmoid')) </span><span id="5191" class="lb jp hh kx b fi lg ld l le lf">model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(X_train, y_train, batch_size=128, epochs=4)</span></pre><p id="0180" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">纪元1/5<br/>196/196[= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 27s 53 ms/step—损耗:0.6013 — acc: 0.6137 <br/>纪元2/5<br/>196/196[= = = = = = = = = = = = = = = = = = = = = = = = = = = = 10s 53 ms/step—损耗:0.2510 — acc: 0.9005 <br/>纪元3/5【历元</p><p id="338c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们可以在测试集上评估它:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="f902" class="lb jp hh kx b fi lc ld l le lf">score, acc = model.evaluate(X_test, y_test, batch_size=128)</span></pre><p id="75e3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">196/196[= = = = = = = = = = = = = = = = = = = = = = = = = = =]—4s 18ms/步—损耗:0.3681 — acc: 0.8878</p><p id="c030" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">获得了88.78%的准确率。第一次尝试还不错，我们的模型现在过拟合，我们应该使用一些下降层来限制它，但这不是我们的最终目标。让我们尝试另一种通常用于NLP任务的架构，LSTM网络。</p><h1 id="52cc" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">LSTM</h1><p id="fc14" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">让我们从定义我们的LSTM模型开始:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="7ec7" class="lb jp hh kx b fi lc ld l le lf">model.add(Embedding(MAX_NUM_WORDS, 128)) <br/>model.add(LSTM(128, dropout=0.2)) model.add(Dense(2, activation='sigmoid'))</span><span id="7fb3" class="lb jp hh kx b fi lg ld l le lf">model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])</span></pre><p id="51ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">和前面一样，我们可以使用model.summary()来查看模型:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es li"><img src="../Images/56a6b20b8be6a7d1503703116efd93b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/0*PoZmlAHC53lt-zmk"/></div></figure><p id="af27" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们现在可以安装它:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="f6c1" class="lb jp hh kx b fi lc ld l le lf">model.fit(X_train, y_train, batch_size=128, epochs=4)</span></pre><p id="ae18" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">历元1/5<br/>196/196[= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 25s 121 ms/step—损耗:0.5804 —精度:0.6935 <br/>历元2/5<br/>196/196[= = = = = = = = = = = = = = = = = = = = = = = = = = = 24s 121 ms/step—损耗:0.2336 —精度:0.9126 【T12</p><p id="8484" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在测试集上评估它:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="901b" class="lb jp hh kx b fi lc ld l le lf">score, acc = model.evaluate(X_test, y_test, batch_size=128)</span></pre><p id="790f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">196/196[= = = = = = = = = = = = = = = = = = = = = = = = = = =]—8s 41 ms/步—损耗:0.4413 —精度:0.8777</p><p id="3c76" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">和以前一样，有过度拟合，这里我们更少概括。好吧，我们换个方法。</p><h1 id="b4c2" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">语言模型</h1><p id="84fc" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">我们现在要改变方法。到目前为止，我们只是定义了模型并训练它，从随机权重开始。</p><p id="5a54" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一种不同的方法是使用来自评论的文本来训练语言模型，然后调整分类任务的权重。最简单的形式是，语言模型学会根据前面的单词预测下一个单词，这就是我们在这里要实现的。</p><p id="564f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">出于两个主要原因，我们将构建一个非常基本和简单的语言模型:</p><ul class=""><li id="f9fb" class="lj lk hh ig b ih ii il im ip ll it lm ix ln jb lo lp lq lr bi translated">更大的网络意味着更多的训练时间和GPU内存</li><li id="15df" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">语言模型的真正优势在于在一个大的不同数据集(如wikitext)上进行训练，然后利用迁移学习，就像我们在rover分类上对ResNet所做的那样</li></ul><p id="dcbc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章的最后一部分，我们将使用预先训练好的语言模型。现在，让我们在评论的基础上建立我们最基本的语言模型。</p><p id="63c3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们像以前一样从标记文本开始:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="6741" class="lb jp hh kx b fi lc ld l le lf">tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) tokenizer.fit_on_texts(df_train.review.values) </span><span id="5a1a" class="lb jp hh kx b fi lg ld l le lf">train_sequences = tokenizer.texts_to_sequences(df_train.review.values) </span><span id="e706" class="lb jp hh kx b fi lg ld l le lf">test_sequences = tokenizer.texts_to_sequences(df_test.review.values)</span></pre><p id="f298" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们需要为语言模型创建一个输入序列。首先，我们对评论进行标记化，然后我们创建一个向量，从前两个词(标记化)开始，然后添加其他词:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="75ea" class="lb jp hh kx b fi lc ld l le lf">input_sequences = []</span><span id="9ce2" class="lb jp hh kx b fi lg ld l le lf">for line in df_train.review.values:<br/>    token_list = tokenizer.texts_to_sequences([line])[0]<br/>    for i in range(1, len(token_list)):<br/>        n_gram_sequence = token_list[:i+1]<br/>        input_sequences.append(n_gram_sequence)</span></pre><p id="51c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">结果是这样的:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lx"><img src="../Images/06723467eecbd03285093d785d743418.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/0*wfRbEEiQRCvk2PxI"/></div></figure><p id="aaa4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是第一次复习，我们有12个单词，因为我们在考虑第11个元素。第12个元素将等于第11个加上一个额外的字。当我们到达评审的末尾并开始新的评审时，第一个元素将是long 2。这是必要的，因为它们之间的审查是不相关的。</p><p id="04f5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了清楚起见，让我们看看第二个和第三个元素:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ly"><img src="../Images/a15ac6fbdd07d307986732ad07767aa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/0*UATBiy1FOVtG-vXt"/></div></figure><p id="ccb3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">同样的事情，第二个元素有三个单词，第三个元素只有一个单词。</p><p id="e79b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与以前不同的是，我们不能只给模型一个numpy数组，这会消耗太多内存。因此，我们创建一个批处理生成器来生成一批数据:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="2eaf" class="lb jp hh kx b fi lc ld l le lf">class KerasBatchGenerator(object):<br/>    def __init__(self, input_sequences, batch_size):<br/>        self.input_sequences = input_sequences<br/>        self.batch_size = batch_size<br/>        self.current_idx = 0<br/>    <br/>    def generate(self):<br/>        while True:<br/>            if (i + 1)*self.batch_size &lt; len(self.input_sequences):<br/>                in_seq = input_sequences[i*self.batch_size: (i + 1)*self.batch_size]<br/>                in_seq = np.array(pad_sequences(in_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='pre'))<br/>                <br/>            else:<br/>                in_seq = input_sequences[i*self.batch_size:]<br/>                in_seq = np.array(pad_sequences(in_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='pre'))<br/>                <br/>            X = in_seq[:,:-1]<br/>            y = in_seq[:,-1]<br/>            y = to_categorical(y, num_classes=num_words)<br/>            self.current_idx += len(self.input_sequences) // self.batch_size<br/>            yield (X, y)</span></pre><p id="ec57" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们现在可以创建生成器并定义我们的第一个语言模型。我们将使用之前使用的相同的LSTM，但在最后，我们将有一个包含n+1个单词的密集层，其中n是保留在标记器中的最大单词数，+1表示未知单词(不在适合的词汇中的单词):</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="e525" class="lb jp hh kx b fi lc ld l le lf">gen = KerasBatchGenerator(input_sequences, 2048) <br/>model = Sequential() <br/>model.add(Embedding(num_words, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)) <br/>model.add(LSTM(128, dropout=0.2)) <br/>model.add(Dense(MAX_NUM_WORDS + 1, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam')</span></pre><p id="0416" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们现在可以训练我们的语言模型:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="7cde" class="lb jp hh kx b fi lc ld l le lf">model.fit(gen.generate(), steps_per_epoch=len(input_sequences)//2048, epochs=1, verbose=1)</span></pre><p id="dec2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2777/2777[= = = = = = = = = = = = = = = = = = = = = = = = = = = =]—2267s 815 ms/步—损耗:3.8493</p><p id="2ca6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦训练了我们的语言模型，我们就可以构建一个新的模型，它使用训练好的嵌入层，并在此基础上构建与以前相同的LSTM:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="de56" class="lb jp hh kx b fi lc ld l le lf">new_model = Sequential()<br/>for layer in model.layers[:-2]:<br/>    new_model.add(layer)<br/>new_model.add(LSTM(128, dropout=0.2))<br/>new_model.add(Dense(2, activation='sigmoid'))<br/>print(new_model.summary())<br/>new_model.compile(loss='binary_crossentropy',<br/>              optimizer='adam',<br/>              metrics=['accuracy'])</span></pre><p id="151a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以使用summary()方法来查看新模型:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lh"><img src="../Images/63bbb056054dd89fde27633094d6aa18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/0*IeEW4XplkN7vc75H"/></div></figure><p id="902f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们对它进行五个时期的训练，并像以前一样在训练集上对它进行评估:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="ea89" class="lb jp hh kx b fi lc ld l le lf">new_model.fit(X_train, y_train, batch_size=128, epochs=5) <br/>score, acc = new_model.evaluate(X_test, y_test)</span></pre><p id="b914" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">历元1/5<br/>196/196[= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 22s 105 ms/step—损耗:0.6124 —精度:0.6411 <br/>历元2/5<br/>196/196[= = = = = = = = = = = = = = = = = = = = = = = = = = = = 21s 105 ms/step—损耗:0.4067 —精度:0.8216 <br/>历元3/5</p><p id="9dfb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们获得了87.48%的准确率。这与之前的结果相同，我们也过度适应了。但是这里的重点不是提高性能。我们想介绍语言模型的概念以及它是如何被训练的。</p><p id="75a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在是时候拿出大枪，从根本上提高我们的分数。</p><h1 id="ad2f" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">释放迁移学习的力量</h1><p id="2c85" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">像谷歌和OpenAI这样的公司训练并提供在从互联网上搜集的非常大的文本数据集上训练的非常大的语言模型。例如，著名的GPT-3训练有4990亿个令牌:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lz"><img src="../Images/6bf7053c20c8f35d1eadac5caccf9549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/0*baX5jdu-pEC-xIRT"/></div></figure><p id="5c94" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">GPT-3和GPT-2使用下一个单词预测进行训练，就像我们的语言模型一样。不幸的是，GPT-3不是开源模型，OpenAI选择通过商业API提供它，所以我们不得不满足于GPT-2。</p><p id="4e2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将使用三个预训练的语言模型，它们都基于Vaswani等人在2017年发表的著名论文<a class="ae kr" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">“注意力是你所需要的一切”</a>中提出的transformer架构:</p><ul class=""><li id="4f57" class="lj lk hh ig b ih ii il im ip ll it lm ix ln jb lo lp lq lr bi translated">由谷歌人工智能伯特</li><li id="20c9" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">OpenAI的GPT-2</li><li id="2d94" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">卡耐基梅隆大学和谷歌大脑团队的XLNet</li></ul><p id="d0d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用这些模型的最好方法是使用拥抱脸的<a class="ae kr" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">变形金刚库。通过这种方式，我们可以构建一个标准管道，只需更改所用的模型。</a></p><h1 id="1b58" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">伯特(男子名ˌ等于Burt)</h1><p id="ee91" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">让我们从伯特开始。</p><p id="c8c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用拥抱脸的标准管道。我们加载Hugging Face提供的配置，并将关于从隐藏状态返回输出的参数更改为False:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="3e6a" class="lb jp hh kx b fi lc ld l le lf">model_name = 'bert-base-uncased' <br/>max_length = 1000 <br/>config = BertConfig.from_pretrained(model_name) config.output_hidden_states = False</span></pre><p id="58cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们加载用于训练bert的同一个记号赋予器，并使用之前定义的配置加载预训练模型。</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="4250" class="lb jp hh kx b fi lc ld l le lf">tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path = model_name, config = config) </span><span id="335d" class="lb jp hh kx b fi lg ld l le lf"># Load the Transformers BERT model <br/>transformer_model = TFBertModel.from_pretrained(model_name, config = config)</span></pre><p id="a6e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以像往常一样使用汇总方法来查看模型:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ma"><img src="../Images/7b1c31e4e3da0c727eb5d88b743302d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/0*d2xgqQjoH78BAaQa"/></div></figure><p id="7b5e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们只取主要的一层:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="8bb0" class="lb jp hh kx b fi lc ld l le lf">bert = transformer_model.layers[0]</span></pre><p id="9b40" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们定义我们的输入需要遵循拥抱脸所需的格式(您可以在<a class="ae kr" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">文档</a>中找到它):</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="777a" class="lb jp hh kx b fi lc ld l le lf">input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32') </span><span id="593a" class="lb jp hh kx b fi lg ld l le lf">attention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32') </span><span id="9f87" class="lb jp hh kx b fi lg ld l le lf">inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}</span></pre><p id="ec65" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们现在可以定义我们的模型:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="271a" class="lb jp hh kx b fi lc ld l le lf">bert_model = bert(inputs)[1] <br/>dropout = Dropout(config.hidden_dropout_prob, name='pooled_output') pooled_output = dropout(bert_model, training=False) <br/>dense_inter = Dense(128, activation='relu')(pooled_output) <br/>logits = Dense(2)(dense_inter) model = Model(inputs=inputs, outputs=logits)</span></pre><p id="ce2a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们看到了:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/214dad55260bc365317eab289109a82a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*T6RuLhiPZ6jWLeni"/></div></div></figure><p id="6f9c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们定义优化器、损耗和指标，并对其进行编译:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="3bd0" class="lb jp hh kx b fi lc ld l le lf">optimizer = Adam(learning_rate=1e-05, epsilon=1e-08, decay=0.01, clipnorm=1.0)<br/> <br/>loss = CategoricalCrossentropy(from_logits=True) metric = 'accuracy' <br/>model.compile(optimizer=optimizer, loss=loss, metrics=metric)</span></pre><p id="4868" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用BertTokenizer来标记我们的训练和测试输入:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="f17f" class="lb jp hh kx b fi lc ld l le lf">X_train = tokenizer(text=df_train['review'].to_list(), add_special_tokens=True, max_length=max_length, truncation=True, padding=True, return_tensors='tf', return_token_type_ids = False, return_attention_mask = True, verbose = True) </span><span id="3f7f" class="lb jp hh kx b fi lg ld l le lf">X_test = tokenizer(text=df_test['review'].to_list(), add_special_tokens=True, max_length=max_length, truncation=True, padding=True, return_tensors='tf', return_token_type_ids = False, return_attention_mask = True, verbose = True)</span></pre><p id="7191" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们符合这个模型:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="5e9a" class="lb jp hh kx b fi lc ld l le lf">model.fit({"input_ids": X_train["input_ids"], "attention_mask": X_train["attention_mask"]}, y_train, batch_size=4, epochs=2)</span></pre><p id="aadf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">历元1/2<br/>6250/6250[= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =-4153s 663 ms/step—损耗:0.3503 —精度:0.8488 <br/>历元2/2<br/>6250/6250[= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 4141s 663 ms/step—损耗:0.2460 —精度:0.9130</p><p id="337d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们来看看测试集的准确性:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="9d2e" class="lb jp hh kx b fi lc ld l le lf">score, acc = model.evaluate({"input_ids": X_test["input_ids"], "attention_mask": X_test["attention_mask"]}, y_test)</span></pre><p id="2bcd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">782/782[= = = = = = = = = = = = = = = = = = = = = = = = = = =]—1186s 2s/step—损耗:0.2610 —精度:0.9129</p><p id="b795" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好的，我们有91.29%的准确率，还不错。我们正朝着正确的方向前进。这里的问题是训练这些模型的时间。我们只能用4的小批量训练2个纪元。如果批量更大，GPU将会耗尽内存。</p><h1 id="a6b7" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">GPT-2</h1><p id="7577" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">是时候使用GPT 2号了。为了让事情更有趣，我们使用GPT-2的更大版本，而对于伯特，我们使用标准版本。但是因为我们有一个更大的模型，我们使用最大序列长度300来训练它:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="4257" class="lb jp hh kx b fi lc ld l le lf">model_name = 'gpt2-large' <br/>max_length = 300 <br/>config = GPT2Config.from_pretrained(model_name) config.output_hidden_states = False</span><span id="d96c" class="lb jp hh kx b fi lg ld l le lf">tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name, config=config)</span><span id="fded" class="lb jp hh kx b fi lg ld l le lf">transformer_model = TFGPT2Model.from_pretrained(model_name, config=config)</span></pre><p id="23b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">流水线是一样的，取主层，定义模型，输入，训练和评估它:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="3ae4" class="lb jp hh kx b fi lc ld l le lf">gpt2 = transformer_model.layers[0]<br/>input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')<br/>inputs = {'input_ids': input_ids} </span><span id="698d" class="lb jp hh kx b fi lg ld l le lf">gpt2_model = gpt2(inputs)[0] <br/>dense_inter = Flatten()(gpt2_model) logits = Dense(2)(dense_inter) model = Model(inputs=inputs, outputs=logits) <br/>optimizer = Adam( learning_rate=1e-05, epsilon=1e-08, decay=0.01, clipnorm=1.0) <br/>loss = CategoricalCrossentropy(from_logits=True) metric = 'accuracy' model.compile( optimizer = optimizer, loss = loss, metrics = metric) </span><span id="cfe6" class="lb jp hh kx b fi lg ld l le lf">X_train = tokenizer( text=df_train['review'].to_list(), add_special_tokens=True, max_length=max_length, truncation=True, padding=True, return_tensors='tf', return_token_type_ids = False, return_attention_mask = False, verbose = True) </span><span id="b1bf" class="lb jp hh kx b fi lg ld l le lf">X_test = tokenizer( text=df_test['review'].to_list(), add_special_tokens=True, max_length=max_length, truncation=True, padding=True, return_tensors='tf', return_token_type_ids = False, return_attention_mask = False, verbose = True)</span><span id="f5c8" class="lb jp hh kx b fi lg ld l le lf">model.fit(X_train["input_ids"], y_train, batch_size=2, epochs=2) </span><span id="0219" class="lb jp hh kx b fi lg ld l le lf">score, acc = model.evaluate(X_test["input_ids"], y_test)</span></pre><p id="3282" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">历元1/2<br/>12500/12500[= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 7667s 611 ms/step—损耗:0.3798 —精度:0.8894 <br/>历元2/2<br/>12500/12500[= = = = = = = = = = = = = = = = = = = = = = = = = 7637s 611 ms/step—损耗:0.2418 —精度</p><p id="6ae3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">准确率跃升至92.74%。好的，让我们看看最后一个模型。</p><h1 id="b144" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">XLNet</h1><p id="6cc6" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">流水线是相同的，我们使用标准模型，因为这是一个更大的模型，即使在其基础形式中，我们也需要将序列长度减少到500来训练它。</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="f656" class="lb jp hh kx b fi lc ld l le lf">model_name = 'xlnet-base-cased' <br/>max_length = 500 <br/>config = XLNetConfig.from_pretrained(model_name) config.output_hidden_states = False</span><span id="ce54" class="lb jp hh kx b fi lg ld l le lf">tokenizer = XLNetTokenizer.from_pretrained(pretrained_model_name_or_path=model_name, config=config) </span><span id="3e98" class="lb jp hh kx b fi lg ld l le lf">transformer_model = TFXLNetModel.from_pretrained(model_name, config=config)</span></pre><p id="0b44" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们训练模型:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="ec42" class="lb jp hh kx b fi lc ld l le lf">model.fit(X_train["input_ids"], y_train, batch_size=4, epochs=2)</span></pre><p id="a9b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">纪元1/2<br/>6250/6250[= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =-3524s 562 ms/step—损耗:0.4341 —精度:0.8810 <br/>纪元2/2<br/>6250/6250[= = = = = = = = = = = = = = = = = = = = = = = = = = = = 3512s 562 ms/step—损耗:0.2526 —精度:0</p><p id="ed2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在测试集上评估它:</p><pre class="jd je jf jg fd kw kx ky kz aw la bi"><span id="7cfb" class="lb jp hh kx b fi lc ld l le lf">score, acc = model.evaluate(X_test["input_ids"], y_test)</span></pre><p id="8976" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">782/782[= = = = = = = = = = = = = = = = = = = = = = = = = = =]—941秒/步—损耗:0.2861 —精度:0.9370</p><p id="53a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好的，所以最终我们能够达到93.70%的准确率。</p><h1 id="6ed4" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结论</h1><p id="58df" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">我们最好的成绩是93.70%，一点都不差。但是我们可以通过三种方式得到更好的结果:</p><ul class=""><li id="64da" class="lj lk hh ig b ih ii il im ip ll it lm ix ln jb lo lp lq lr bi translated">为更多时代而训练</li><li id="cb39" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">用更大的序列长度训练</li><li id="6b70" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">微调评论的语言模型</li></ul><p id="ef49" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们无法探索这些选项中的任何一个，原因很简单，我们没有足够强大的GPU。目前，这是NLP的最大问题。为了训练大型模型，你需要在云GPU上花费大量资金(根据<a class="ae kr" href="https://lambdalabs.com/blog/demystifying-gpt-3/" rel="noopener ugc nofollow" target="_blank">一项估计</a>，训练GPT-3至少需要460万美元)。</p><p id="0aa0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以在我的<a class="ae kr" href="https://github.com/antonai91/nlp/tree/main/imdb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到所有代码。有任何问题，你可以通过<a class="ae kr" href="https://www.linkedin.com/in/lisiantonio/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我。</p><p id="b19e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在说再见之前，如果你想在我发布东西的时候得到通知，你可以在下面订阅。</p><p id="f677" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你喜欢这篇文章，分享给你的朋友和同事吧！我会在下一篇文章中看到你。与此同时，要小心，保持安全，记住<em class="mc">不要成为另一块墙砖</em>。</p><p id="793b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Anton.ai</p></div><div class="ab cl md me go mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ha hb hc hd he"><p id="bae8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="mc">原载于2021年4月1日</em><a class="ae kr" href="https://antonai.blog/how-to-achieve-state-of-the-art-results-on-nlp-classification-tasks/" rel="noopener ugc nofollow" target="_blank"><em class="mc">https://antonai . blog</em></a><em class="mc">。</em></p></div></div>    
</body>
</html>