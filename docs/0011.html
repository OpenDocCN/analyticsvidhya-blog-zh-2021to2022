<html>
<head>
<title>How Self Attention works in Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自我关注在《变形金刚》中是如何工作的</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-self-attention-works-in-transformer-6c76a12396b0?source=collection_archive---------10-----------------------#2021-01-01">https://medium.com/analytics-vidhya/how-self-attention-works-in-transformer-6c76a12396b0?source=collection_archive---------10-----------------------#2021-01-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="fd80" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在之前的帖子中，我试图展示变形金刚的一些方面。现在我将把重点放在自我关注上。如果你想热身的话，可以查看之前的帖子，了解更多关于transformer架构的知识。(<a class="ae jc" rel="noopener" href="/analytics-vidhya/nlp-transformer-unit-test-95459fefbea9">上一篇文章链接</a>)</p><p id="cad4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我从<a class="ae jc" href="#https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb" rel="noopener ugc nofollow">这里</a>得到基础代码。但是我变化太大了，所以你可能跟不上我的台词。代码在github，但使用nbviewer.com查看颜色。(github有时不能支持所有视觉)<br/>代码<a class="ae jc" href="https://nbviewer.jupyter.org/github/mcelikkaya/medium_articles/blob/main/transformer_debugging.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>在nbviewer <br/>代码<a class="ae jc" href="https://github.com/mcelikkaya/medium_articles/blob/main/transformer_debugging.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>在Github</p><p id="6ec7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在RNN编码器-解码器中，在编码器端我们总结整个句子，在解码器的每一步我们希望网络使用这个总结。在transformer中，我们给出了完整的步骤，这样网络就可以学习每个步骤中哪个部分是最重要的。这意味着在RNN中，我们给出一个在所有步骤中创建的最终向量，而在transformer中，我们给出在所有步骤中创建的所有向量。</p><p id="3e49" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我在上一篇文章中所做的，我将想象立正学习的步骤。起初，网络随机产生注意力向量。然后通过检查损失，它对权重进行更新，最后你可以看到网络对单词进行了很好的关注。</p><p id="e719" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我有一个方法叫做translate_info，它展示了翻译的逻辑性和自我关注。<strong class="ig hi"> Logits </strong>是Softmax之前分类模型的原始(非标准化)分数。然后softmax生成这些分数的概率总和为1。</p><p id="dc8f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">翻译是从<strong class="ig hi"> </strong>输入<strong class="ig hi">【SOS】</strong>直到生成<strong class="ig hi">【EOS】</strong>输出(或达到我们尝试的最大长度)生成下一个单词的过程。所以当你看到下图时，它显示了每个翻译步骤的逻辑和注意事项。所以如果输入句子是<strong class="ig hi">“我能吃苹果”</strong>，翻译过程有5个步骤，<strong class="ig hi">“ich konnen apfel essen&lt;EOS&gt;</strong>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/bd2b6a27812e90f06152725d1d455a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABiL3ofnXN3J4p1QJo8QGg.png"/></div></div></figure><p id="cb35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">查看表格第一行。</p><p id="112c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> "1)sos- &gt; ich" </strong>显示当前预测步骤，" sos "是当前单词" ich "是该步骤将要生成的单词，颜色显示最可能的列。在其他教程里他们写的是“sos”，但我觉得这种方式更好。</p><p id="a964" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果查<strong class="ig hi">“ich”</strong>最高概率8.07分。检查其他步骤以更好地理解。<br/>上面的热图显示了对所有步骤的关注。在翻译步骤1，注意力在<strong class="ig hi">“我”</strong>，在步骤2，注意力在<strong class="ig hi">“能”</strong>。检查每一步，并理解网络如何在每一步运用注意力。由于我们在这一步生成了<strong class="ig hi"> "ich" </strong>，在第二行u参见<strong class="ig hi"> "2)ich- &gt; konnen" </strong>，这意味着第二步解码器已经生成了【sos，ich】并将生成“konnen”。</p><p id="03f7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以我们试着一步一步来写。在翻译过程中，主要模块如下。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jp jq l"/></div></figure><p id="2aaa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">生成所有编码器状态(第3、4行)，解码器以“sos”启动。(第7行)现在让我们看看for循环的每次迭代(第11行)</p><p id="0dd9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第一步:</strong> <br/>网络在这一步会生成什么？</p><p id="9a37" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们一直展示网络例句:<br/> <strong class="ig hi">主语+动词(我吃苹果，我喝水……)<br/>主语+ can +动词(我能吃苹果)<br/>主语+想要+动词(我想吃苹果)</strong></p><p id="8c56" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以在这一步主题是最有可能的。编码器状态也有“I”，所以在这一步网络将产生“ich”。(同时检查逻辑和注意力可视化)<strong class="ig hi"> <br/>输出=【&lt;SOS&gt;，ich】</strong></p><p id="5cd5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第二步:</strong>现在我们在解码器<strong class="ig hi">中有了[ <strong class="ig hi"> &lt; sos &gt;，ich】</strong>。</strong>通过训练过程网络得知，在像这样的解码状态下，最好是把注意力放在【动词】、【能】、【想要】中的一个。所以现在在我们的来源，我们有动词和“可以”。<br/>通过训练它还了解到，如果编码器中有<strong class="ig hi">【can】</strong>或<strong class="ig hi">【want】</strong>它必须现在生成。如果您检查第2行logits，您会看到所有动词(essen、lesen、trinken)、<strong class="ig hi">【can】</strong>和<strong class="ig hi">【want】</strong>都具有高值。因为在训练中，这一步(第二步)生成的所有单词都是这类单词。<br/> <strong class="ig hi">输出= [ &lt; sos &gt;，ich，konnen] </strong></p><p id="a1d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第三步</strong>:现在我们有[" &lt; sos &gt;，" ich "，" konnen"]，我们的网络被训练成，" konnen "后面带宾语。如果你检查逻辑，这一步的对象(apfel，brot，beer，wasser…)得分很高。你也会看到，现在网络对苹果公司非常重视，所以产生了“apfel”。<br/> <strong class="ig hi">输出= [ &lt; sos &gt;，ich，konnen，apfel] </strong></p><p id="a0b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第四步</strong>:现在我们有[" &lt; sos &gt;，" ich "，" konnen "，" apfel"]，我们的网络被训练成，宾语后接动词，这样的模式。从编码器状态，我们知道我们的源有动词" eat "，所以网络生成" essen "<br/><strong class="ig hi">Output =[&lt;SOS&gt;，ich，konnen，apfel，essen] </strong></p><p id="b257" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第五步</strong>:现在我们有[" &lt; sos &gt;，" ich "，" konnen "，" apfel "，" essen"]，我们的网络被训练成，这个状态是一个完整的步骤，source "eos "是在这样一个状态之后。按以下顺序[“SOS”、“我”、“能”、“吃”、“苹果”]“EOS”来了之后这样想。所以在一个[" &lt; sos &gt;、" ich "、" konnen "、" apfel "、" essen"]之后，" eos "最有可能。<br/> <strong class="ig hi">输出= [ &lt; sos &gt;，ich，konnen，apfel，essen，&lt; eos &gt; ] </strong></p><p id="143a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以上是对transformer如何生成事物的逐步评估。现在让我们尝试一些不同的东西来检查我们的理解。</p><p id="5b65" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">理论</strong>:即使我改变了句子的顺序，网络还是会关注正确的单词。我将给出一个输入句子为<strong class="ig hi">“吃苹果可以吗”</strong>，我声称它将生成正确的输出(至少是正确的意思)。通常注意力向量(Q，K，V)的计算是置换不变的。(即使你改变顺序，它也会计算同样的事情)但是这里不仅注意而且网络可以预测正确的句子。因为我们的布景太小了。不要误解。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jr"><img src="../Images/c8e40b4a59d1eb98f20e63b5bffd2373.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aFF8zP6tjfXEEXbjyVxkhg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><strong class="bd jw">无序句子的结果</strong></figcaption></figure><p id="abd7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于我们的小集合，即使顺序被改变，注意力也在正确的位置(根据注意力的性质),甚至我们给出了正确的输出(因为我们严格地用相同的模式训练网络),这也表明我们的网络不是简单地复制位置上的项目。</p></div><div class="ab cl jx jy go jz" role="separator"><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc"/></div><div class="ha hb hc hd he"><h1 id="a856" class="ke kf hh bd jw kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">腐蚀自我关注</h1><p id="3d39" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">让我们更深入地了解注意力，如果注意力是正确地将重量应用到正确的位置，如果我改变重量应用会发生什么？我将改变变形金刚运用注意力的方式。注意力只是一个向量，所以我会生成一些向量，而不是网络学了什么。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jp jq l"/></div></figure><p id="cb90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面我们对所有的单词一视同仁。这里我们正在划分，注意向量形状的第三维度。所以它意味着对每个单词给予同等的重视。如果维度是5，我们会把注意力放在所有单词的1/5上。</p><p id="2144" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它适用于小模式“我能吃苹果”，但不适用于如下更复杂的情况。也注意图表是全黑的，因为处处平等。由于我们对每一步都给予同等的关注，网络正在产生两次“埃森”。为什么？在“ich”之后，如果你检查逻辑“essen”和“apfel”总是有很大的分数。(“apfel”即使在第一步中也有很高的分数)因为在我们认为的所有步骤中，编码器数据同样重要，所以我们无法创建一个合适的句子，因为网络不是这样训练的。网络总是根据解码器状态(根据翻译过程的流程)改变注意力，但是这里即使解码器状态改变，注意力也不改变，这是误导。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lg"><img src="../Images/d61eeb717047f18d8ac6946a5ba3bc9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YZoK3JqPQBidqC4SH2O-Tw.png"/></div></div></figure><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="01fd" class="lm kf hh li b fi ln lo l lp lq">ATTENTION_AT_BEGINNING</span></pre><p id="f627" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们检查一下开始时的注意模式，我把它解释为，只注意前三个单词。1、2和3个单词得到1/3的关注，其他得到1/7的关注。</p><p id="b140" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你能猜出结果吗？由于模型只使用开始注意，解码器只知道句子的开始，所以只是作出一个非常糟糕的猜测。下面的句子只注意开头，所以除了<strong class="ig hi">“我”</strong>之外没有其他意思。这就像只听到一个句子的开头。还可以看到，它甚至没有生成正确的动词。！！试试“我能吃苹果”</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/ddef5773beb8531cd6f18861f5cb91b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*um4_aewDR6UYAeg8OBTaNQ.png"/></div></figure><p id="8814" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你检查下面，你可以看到网络生成的“埃森”两次。如果您查看最后一行，可以看到network在“apfel”方面的得分很高。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ls"><img src="../Images/a0473f87bd8aa3fe662e2ec4564a30d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BBAj_cjMrnTLP1VemLK70w.png"/></div></div></figure><p id="5fd3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，如果您检查下面的句子，网络没有任何关于句子的想法。在第2行，“mochten”得分较高，但“trinken”和“essen”得分更高。因为我们从来没有这样训练过网络，它会做出非常糟糕的猜测。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lt"><img src="../Images/fd4e164a8b1f3979e8a65c04444701cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t7EwmrMX3PllXSMg01RCRg.png"/></div></div></figure><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="f972" class="lm kf hh li b fi ln lo l lp lq">ATTENTION_AT_END</span></pre><p id="89c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以另一个随机的尝试，如果我们把更多的注意力放在最后一个单词上，会发生什么，它会很快跳到句尾。(但是要注意，它并没有立即跳转，它首先生成了<strong class="ig hi">【ich】</strong>，然后生成了<strong class="ig hi">【lesen】</strong>，由于lesen是某个句子的最后一个单词，所以它生成了<strong class="ig hi">【EOS】</strong>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lu"><img src="../Images/ef9b36409d797a26826a667db6536894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rQYeKZQ8-9J265SONPhRWw.png"/></div></div></figure></div><div class="ab cl jx jy go jz" role="separator"><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc"/></div><div class="ha hb hc hd he"><h1 id="8582" class="ke kf hh bd jw kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">不断变化的网络架构</h1><p id="0a50" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">现在我们玩了注意力向量，并有了一个想法，网络模型如何对我们关于自我注意力的虚拟变化作出反应。现在让我们试着启用/禁用网络的其他部分。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lv"><img src="../Images/1a925470ce4238a3d86631adacb9f244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5-5IsET3XbXxBXaTz8SwIw.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><strong class="bd jw">建筑</strong></figcaption></figure><p id="f2d9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面我将禁用，改变架构的一些部分，以测试网络的不同模块的效果。老实说，我没有写一个非常整洁的代码，我们只是在尝试，所以我用最简单的方法做了。只需在网络中放置一些组件的测试效果的专用路径。检查下面解码器的正向方法。我放了很多条件来改变计算。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jp jq l"/></div></figure><p id="52bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我还把这些类if条件流交给DecoderLayer检查它们。我的意图是选择性地应用/不应用某些部分。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jp jq l"/></div></figure><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="537f" class="lm kf hh li b fi ln lo l lp lq">trg3 = self.enc_attn_layer_norm(trg2 + self.dropout(_trg2))</span></pre><p id="d0e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通常计算如上。这是else块(默认)，我通过仅使用该计算的一部分尝试了不同的组合。</p><ol class=""><li id="7f89" class="lw lx hh ig b ih ii il im ip ly it lz ix ma jb mb mc md me bi translated"><strong class="ig hi">不适用注意。</strong>这里我所做的有点不同，我在应用注意力之前保持trg向量，然后在应用注意力之后，我恢复。(这似乎是打开/关闭部分代码的最简单方式)</li></ol><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="4187" class="lm kf hh li b fi ln lo l lp lq">APPLY_NOT_ATTENTION # check code for this part<br/></span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mf"><img src="../Images/09a90199f12857b3f2c192732b89406f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VkfokkHvN5kaaztBkYD-Ig.png"/></div></div></figure><p id="d77e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如你在上面看到的，网络只是随机行走。因为它对源句子一无所知。这就像翻译一个句子却不知道源句一样。</p><p id="fab0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2) <strong class="ig hi">仅应用解码器自我关注</strong>。这里，解码器也不知道源句子。所以在第一步，大多数可能是主语(" ich "，" wir ")，在第二步(" konnen "，" mochten "，动词)，在第三步是宾语(" apfel "，" bier "，" brot" …)。正如你所看到的，它在每一步都选择最可能的单词，只通过解码器端的权重。因此，它实际上又是在做一些随机的事情(或者更好地说，在每一步获得最高权重的项目)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mg"><img src="../Images/5220d35a16d69967aabdb5fc277ed4ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AGZ1wnQG-aVvJr0qEb7zPg.png"/></div></div></figure><p id="7583" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3) <strong class="ig hi">仅应用退出</strong>。除此之外，我们申请退学。由于辍学没有增加任何东西，我们有更多不可靠和奇怪的解决方案。但是这一步捕获了我们源句子的更多部分。我的猜测是，因为辍学是我们学到的一层，所以事实上它是防止网络，使完全随机猜测如上，所以这些猜测更好。事实上，如果你检查逻辑，网络是失踪产生“eos”与一个非常小的差异。)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mh"><img src="../Images/a314723a9e300c021ed229b1a9bda0c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wzkqd73SaxAFAjX4mgwqgA.png"/></div></div></figure><p id="6289" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4) <strong class="ig hi">仅适用于规范自我关注</strong>。这给出了与第二个样本相同的输出，因为实际上包含的向量是相同的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mi"><img src="../Images/a5255da65a87a59a49d7718259897155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fDtFQ37_qA2NsnohRerimw.png"/></div></div></figure><p id="1773" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">5) <strong class="ig hi">仅应用源编码。</strong>我在这里做了一些东西，很假。如果我直接复制源编码在解码器上使用会怎样？它会直接生成“eos”</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mj"><img src="../Images/28d0478ed32d6d1188a1f96419f6bf14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jWm0l2BlK7ZvbWxnxBreLA.png"/></div></div></figure><p id="0373" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们来看一个更有趣的案例，下面我们跳过PositionwiseFeedforwardLayer。网络仍然能够生成“想要”的正确答案。但是对于另外两种情况，它产生1个以上“苹果”。如果你检查logits<strong class="ig hi">“EOS”</strong>和<strong class="ig hi">“apple”</strong>的概率非常相似。位置层是具有Relu的层(其在解码器层的末端应用非线性)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mk"><img src="../Images/09c72e05d58cd7c27f3053940974e265.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ENH-1HnvjZzfdnwKvvl2rQ.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/a916eba7120a76dded5c14947f76a260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*4kCsAJYyGO_GOzlwfV5FIg.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mm"><img src="../Images/cd4a4554fea715e373a8e8bd13331532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*DA40UEXGWFFBo45th9etxQ.png"/></div></figure><p id="5772" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我试图展示注意力层产生了什么，以及它们是如何随着时间而变化的。还禁用了网络的某些部分，以显示它们如何影响输出和注意力。起初，逻辑可能有点难以理解，但实际上它非常简单。</p></div></div>    
</body>
</html>