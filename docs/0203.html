<html>
<head>
<title>Ultimate Guide for Setting up PySpark in Google Colab</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Google Colab中设置PySpark的终极指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ultimate-guide-for-setting-up-pyspark-in-google-colab-7637f697daf1?source=collection_archive---------8-----------------------#2021-01-08">https://medium.com/analytics-vidhya/ultimate-guide-for-setting-up-pyspark-in-google-colab-7637f697daf1?source=collection_archive---------8-----------------------#2021-01-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/18e2b630f09c00055d7a7eb7e72328bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OgIaiWSmQBASAnLFuVYi4Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Google Colab安装指南</figcaption></figure><p id="977d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">PySpark是一个用于Apache Spark的Python API，允许您利用Python的简单性和Apache Spark的强大功能来驯服大数据。</p><p id="20e2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">学习PySpark可以提升你的市场价值。根据奥赖利的调查，学习火花比获得博士学位对你的薪水有更大的影响！</p><p id="4045" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然而，在个人电脑上安装Pyspark是一个<a class="ae js" href="https://www.dataquest.io/blog/pyspark-installation-guide/" rel="noopener ugc nofollow" target="_blank">漫长的任务</a>，它会占用你电脑的大量内存，可能会阻碍你开始学习。希望你可以很容易地在Google Collab上设置它，在这篇文章中，你将学习如何去做。我们开始吧！</p><h1 id="ff9f" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">安装PySpark</h1><p id="5238" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">在<a class="ae js" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> Google Colab中打开一个新笔记本</a>运行下面的代码:</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="8395" class="lf ju hi lb b fi lg lh l li lj">!apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null<br/>!wget -q <a class="ae js" href="https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz</a><br/>!tar -xvf spark-3.1.1-bin-hadoop2.7.tgz<br/>!pip install -q findspark<br/>!pip install pyspark</span></pre><p id="381e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">您可能知道，当我们想要在Jupyter笔记本中运行命令shells时，我们用符号(<strong class="iw hj">)开始一行。)</strong>让GC知道我们想要一个shell。然后第一行将安装<strong class="iw hj"> JDK Java </strong>。</p><p id="c692" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">第二行将使用命令<strong class="iw hj"> wget </strong>下载<strong class="iw hj"> PySpark </strong>。在这里，检查链接是否启用以及我们使用的是最新的PySpark版本是非常重要的。要验证其可用性，请访问<a class="ae js" href="https://downloads.apache.org/spark/" rel="noopener ugc nofollow" target="_blank">官方网站</a>并选择最新的3.x版本。<strong class="iw hj"> tar </strong>行将解压所有下载的PySpark文件。最后，最后两行将使用<strong class="iw hj"> pip </strong>命令安装<strong class="iw hj"> findspark </strong>和<strong class="iw hj"> pyspark </strong>模块。</p><h1 id="9ccd" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">运行PySpark</h1><p id="0c54" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">现在您已经在Colab中安装了Spark和Java，是时候设置环境路径了，这样您就可以在您的Colab环境中运行Pyspark。通过运行以下代码设置Java和Spark的位置:</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="7e52" class="lf ju hi lb b fi lg lh l li lj">import os<br/>os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"<br/>os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop2.7"</span></pre><p id="89e5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">最后，运行一个本地SparkSession来测试您的安装:</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="00c3" class="lf ju hi lb b fi lg lh l li lj">import findspark<br/>findspark.init()<br/>from pyspark.sql import SparkSession<br/>spark = SparkSession.builder.master("local[*]").getOrCreate()</span></pre><p id="bb1b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">恭喜你。您的笔记本已经可以运行和使用PySpark了！:)</p></div></div>    
</body>
</html>