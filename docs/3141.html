<html>
<head>
<title>Predict the next sentence for President’s Speech dataset.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测总统演讲数据集的下一句话。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/predicting-the-next-character-using-temporal-convolution-network-e97a8bdec9b0?source=collection_archive---------2-----------------------#2021-06-11">https://medium.com/analytics-vidhya/predicting-the-next-character-using-temporal-convolution-network-e97a8bdec9b0?source=collection_archive---------2-----------------------#2021-06-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="4d1f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用时间卷积网络生成下一个句子。(TCN)</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/df5a5e33539f786e1fd8e9a7bbaceb31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*2COcFNg6M0PzSmHEOj6Ypw.gif"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><a class="ae jo" href="https://towardsdatascience.com/transformers-141e32e69591?gi=f28ac807e88d" rel="noopener" target="_blank">序列</a>模型</figcaption></figure><p id="20ec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">时间卷积网络是用于序列建模任务的卷积神经网络的变体。这是一个强大的替代RCNNs(循环网络)和没有遭受消失或爆炸梯度问题。</p><p id="b4eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">TCN是使用扩展的因果块实现的。因果块是只能看到过去而不能看到未来的卷积块。因此，TCN是一个自回归模型。因果块通过直接看下一个词来防止模型作弊！</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jp"><img src="../Images/cb4d40d02b55ea7c451c28862c640a1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eouYogKccTUzHeAdsVEO7g.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">因果卷积只看过去。</figcaption></figure><p id="1329" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">简单的因果层只能查看线性深度，因此不适用于需要更长历史的任务。我们通过使用膨胀卷积来解决这个问题，其中膨胀在每一层中呈指数增加。由于TCN的感受野在很大程度上取决于网络深度(由于扩张随着每一层而增加，因此模型越深，其感受野越大)，剩余连接用于防止消失或爆炸梯度问题。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ju"><img src="../Images/dca1aca39f0d4bf248880776798f3f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uys2mwXhXadG8DBWIXUaAQ.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">膨胀因子d = 1，2，4且滤波器大小k = 3的膨胀因果卷积。感受野能够覆盖来自输入序列的所有值。</figcaption></figure><p id="cb1f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">TCN是个性生成的典范。它输出英语中26个字符中每一个的概率。</p><blockquote class="jv jw jx"><p id="4d35" class="ie if jy ig b ih ii ij ik il im in io jz iq ir is ka iu iv iw kb iy iz ja jb ha bi translated">P(y0|x)。P(y1|x，y0)。P(y2|x，y0，y1)……</p></blockquote><p id="de78" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个输出的概率取决于输入和前一个输出，依次进行。我们可以通过将字母移动1来使用同一个句子作为我们的输入和标签！！。这里为了简单起见，假设模型已经看到了“th”。因此，我们的输入是“th ”,标签变成了“e”。这类似于我们培训LSTMs的方式。TCN相对于LSTM的额外好处是现在我们在训练期间没有任何经常性的联系。输出是相互独立的。这也意味着它现在可以被<strong class="ig hi">平行</strong>训练，而不是LSTM。因此，我们可以以完全卷积的方式训练我们的时间网络。这也具有将本地信息添加到时间信息的好处。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es kc"><img src="../Images/4393e34a28bcf33a7ee0ff444a180a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wybQHdNc6XXSTuW1t28gqw.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><a class="ae jo" href="https://towardsdatascience.com/character-level-language-model-1439f5dd87fe" rel="noopener" target="_blank">字符级语言模型</a></figcaption></figure><p id="66ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有几种方法可以使用你的语言模型(TCN)的输出来生成一个句子。最优选的方式是使用<strong class="ig hi">波束搜索</strong>。</p><p id="f3c3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在每一步(对于每个字符)，波束搜索扩展所有可能的字符。这意味着对于每一个字符，我们看它与所有其他字符的对数似然性。比如我们从a开始。然后我们看看“aa”，“ab”，“ac”的对数似然性…..这将需要指数空间。为了解决这个问题，我们只保留最上面的几个句子(称为梁)，去掉其余的。</p><p id="a25f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们通常使用2个标准来选择子串—</p><ol class=""><li id="03e6" class="kd ke hh ig b ih ii il im ip kf it kg ix kh jb ki kj kk kl bi translated">每字符对数似然</li><li id="9d33" class="kd ke hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">总体对数-对数可能性</li></ol><p id="d643" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Log -Likelihood是在TCN语言模型下该字符串的对数概率。</p><p id="8701" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们看看<strong class="ig hi">如何实现TCN </strong>。</p><p id="ce01" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">TCN是由非常小的通道的因果卷积组成的(一般来说，&lt; 50) repeated to get a large depth. In 1d ( we need only 1 dimension to take care of time), we implement causal convolutions using Conv1d Convolutions and by shifting. We use padding to shift the network. My Char length is 20 to add “ “ (space) and “.” (period).</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="kr ks l"/></div></figure><p id="c19a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Let us first look into the major differentiating components of the TCN class and then we will go into the building blocks.</p><ol class=""><li id="d3a9" class="kd ke hh ig b ih ii il im ip kf it kg ix kh jb ki kj kk kl bi translated">We have used many layers with small channels (8 filters repeated 10 times.) It is preferred to have your filter size &lt; = 50.</li><li id="8f7a" class="kd ke hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">The dilation at each level is exponential (2^i) to increase our receptive field.</li><li id="c195" class="kd ke hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">The padding at each level also changes according to the dilation. So padding = (kernel_size — 1) * dilation_size.</li><li id="c7f0" class="kd ke hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">Due to the exponential increase in size in each layer, we have implemented a Chomp class to reduce the size and we are also using Dropout.</li></ol><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="kr ks l"/></div></figure><p id="0a2c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Our CausalConv1Block is thus a sequential Conv1d layer with dilation and padding, followed by non-linearity and dropout.<strong class="ig hi">我使用了</strong><a class="ae jo" href="https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad1d.html" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">constant 1 DP adding</strong></a><strong class="ig hi">，并且添加了一个(kernel _ size-1)*膨胀的填充，只在左边保留填充数0到右边。这改变了网络。</strong></p><p id="be34" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了训练我们的模型，我们对输入和标签使用相同的句子。我们将标签移动1。例如，如果我们的句子是“苹果从树上掉下来了”，那么我们的输入是“苹果从树上掉下来了”，采用的是一键编码格式。而我们的标签是“苹果从树上掉下来”。训练从空字符串开始，模型的第一个预测是“T”。我们通过将第一个字符编码为torch.nn.Parameter以自动将其添加到模块参数列表中来实现这一点。您可以在TCN.py的第21行看到这一点(第一个代码块)。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="kr ks l"/></div></figure><p id="9be9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们将使用波束搜索来生成由我们的TCN网络生成的热门句子。我们知道，TCN一次只能生成一个角色。它输出28个字符中的每一个的对数概率，26个在英语字母表和空格和句号中。在每一步，Beam Search将扩展所有可能的字符，并根据每个字符的平均对数似然性存储每一级的最佳候选子串。光束搜索的停止标准是当它预测到“.”时或者达到指定的最大长度。你可以在这里找到贪婪波束搜索实现<a class="ae jo" href="https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/" rel="noopener ugc nofollow" target="_blank">。你可以通过DeepLearningAI在这个</a><a class="ae jo" href="https://www.youtube.com/watch?v=RLWuzLLSIgw" rel="noopener ugc nofollow" target="_blank">视频</a>中了解更多关于光束搜索的信息。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es kt"><img src="../Images/9d8902a63bf638b86dd5907d8e919b65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WgkNyHOmaXWpeme3ObnCwA.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">波束搜索的输出</figcaption></figure><p id="09b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以在这里找到完整实现的代码—</p><div class="ku kv ez fb kw kx"><a href="https://github.com/anki08/Temporal_Convolution_Network" rel="noopener  ugc nofollow" target="_blank"><div class="ky ab dw"><div class="kz ab la cl cj lb"><h2 class="bd hi fi z dy lc ea eb ld ed ef hg bi translated">anki 08/时态_卷积_网络</h2><div class="le l"><h3 class="bd b fi z dy lc ea eb ld ed ef dx translated">在GitHub上创建一个帐户，为anki 08/Temporal _ Convolution _ Network开发做贡献。</h3></div><div class="lf l"><p class="bd b fp z dy lc ea eb ld ed ef dx translated">github.com</p></div></div><div class="lg l"><div class="lh l li lj lk lg ll ji kx"/></div></div></a></div><p id="63b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我已经根据奥巴马的演讲训练了我的模型。你可以在Kaggle上找到<a class="ae jo" href="https://www.kaggle.com/binaicrai/pres-speeches" rel="noopener ugc nofollow" target="_blank">数据集</a>。</p></div></div>    
</body>
</html>