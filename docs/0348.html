<html>
<head>
<title>Multicollinearity, Regularization, Lasso, Ridge and Polynomial Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多重共线性、正则化、套索、岭和多项式回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multicollinearity-regularization-lasso-ridge-and-polynomial-regression-93bc90d2e4d?source=collection_archive---------5-----------------------#2021-01-13">https://medium.com/analytics-vidhya/multicollinearity-regularization-lasso-ridge-and-polynomial-regression-93bc90d2e4d?source=collection_archive---------5-----------------------#2021-01-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/4203b105343c227a48aabc78d6517a4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sYEOFYVE2N1O2g8G.jpg"/></div></div></figure><h2 id="0fee" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">多重共线性</strong></h2><p id="1354" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">两个或多个自变量之间通常会出现高度相关。这在回归模型中有广泛的暗示。在实时情况下，数据可能具有共线性属性。在应用任何模型之前，需要校正共线性，否则将导致错误的结果和较低的准确性。</p><blockquote class="kj kk kl"><p id="8b20" class="jo jp km jq b jr kn jt ju jv ko jx jy kp kq ka kb kr ks kd ke kt ku kg kh ki hb bi translated">考虑日常活动以更好地解释多重共线性。汤姆通常喜欢甜食。他一边看电视一边享受甜食。我们如何确定汤姆的幸福等级？这个可以边看电视边吃甜食两种方式养。这两个变量是相互关联的。最后，这涉及到多重共线性的情况。</p></blockquote><p id="dc02" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">一旦发现数据中存在多重共线性，那么如何去除呢？。有什么可行的方法吗？是的，它是。这可以通过使用VIF因子来实现。</p><p id="95e7" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated"><strong class="jq hj">可变通货膨胀因素</strong></p><p id="8b05" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">确定<strong class="jq hj"> R </strong>值是为了找出一个自变量被其他自变量描述得有多好。高值的<strong class="jq hj"> R </strong>意味着该变量与其他变量高度相关。这由下面表示的<strong class="jq hj"> VIF </strong>捕获:</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es kv"><img src="../Images/001a2133784f6094ad50982c3aa9e52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/0*-uhAQ808ee1kzMh5.png"/></div></figure><ul class=""><li id="9848" class="la lb hi jq b jr kn jv ko jb lc jf ld jj le ki lf lg lh li bi translated">VIF从1开始，没有上限</li><li id="5bb8" class="la lb hi jq b jr lj jv lk jb ll jf lm jj ln ki lf lg lh li bi translated">VIF = 1，自变量和其他变量之间没有相关性</li><li id="0ac8" class="la lb hi jq b jr lj jv lk jb ll jf lm jj ln ki lf lg lh li bi translated">VIF超过5或10表示该自变量与其他变量之间存在高度多重共线性</li></ul><p id="9843" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">让我们看看数据集中关于预测房价的实现。将独立变量应用于VIF因子的以下代码片段。</p><pre class="kw kx ky kz fd lo lp lq lr aw ls bi"><span id="0eea" class="iq ir hi lp b fi lt lu l lv lw">from statsmodels.stats.outliers_influence import variance_inflation_factor</span><span id="a19b" class="iq ir hi lp b fi lx lu l lv lw">def applyVIF():</span><span id="8fbe" class="iq ir hi lp b fi lx lu l lv lw">   vif = pd.DataFrame()<br/>   vif["Features"] = X.columns<br/>   vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(variables.shape[1])]<br/>   <br/>   print(vif)</span></pre><p id="e110" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">您可以清楚地看到卧室、主要道路、浴室、楼层和具有高度多重共线性的区域。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/2572ad01ef1efe0cab3ef4e538c45ac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*qkfkIL_dg6i-bSsoVPp6Wg.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">超低频辐射</figcaption></figure><p id="4bd4" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated"><strong class="jq hj">修复多重共线性</strong></p><p id="2b65" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">通过移除保存大于5的VIF值的列。</p><pre class="kw kx ky kz fd lo lp lq lr aw ls bi"><span id="50db" class="iq ir hi lp b fi lt lu l lv lw">X.drop(['area','bedrooms','bathrooms','stories','mainroad'], axis=1, inplace=True)</span></pre><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es md"><img src="../Images/9541647cd8fa237787611759966ddfc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*aCEcxM13BOujmxMUyrMMdA.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">移除VIF后&gt; 5</figcaption></figure><p id="c9d5" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated"><strong class="jq hj">正规化</strong></p><p id="8151" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">用于解决机器学习模型中过拟合问题的技术。</p><p id="e252" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated"><strong class="jq hj">什么是过度拟合？</strong></p><p id="32fc" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">过度拟合是一种现象，当模型学习训练数据中的细节和噪声达到一定程度时，它会对模型在新数据上的性能产生负面影响。</p><p id="c06c" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">因此，过拟合是一个主要问题，因为它会对性能产生负面影响。</p><p id="52a1" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">救援的正规化技术。</p><p id="3109" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated"><strong class="jq hj">正则化有以下两种类型:</strong></p><ul class=""><li id="4685" class="la lb hi jq b jr kn jv ko jb lc jf ld jj le ki lf lg lh li bi translated"><strong class="jq hj"> L1正则化或拉索正则化</strong></li><li id="1f47" class="la lb hi jq b jr lj jv lk jb ll jf lm jj ln ki lf lg lh li bi translated"><strong class="jq hj"> L2正则化或岭正则化</strong></li></ul><h1 id="938e" class="me ir hi bd is mf mg mh iw mi mj mk ja ml mm mn je mo mp mq ji mr ms mt jm mu bi translated">L1正则化或拉索正则化</h1><p id="82a5" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">L1正则化或套索正则化给误差函数增加了一个惩罚。罚分是权重的<strong class="jq hj">绝对值</strong>的总和。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/3c3cd7c0f19bf767bc48f6f68213381e.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*VVzEAiLvxz-nIz2M.png"/></div></figure><p id="7788" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated"><code class="du mw mx my lp b">p</code>是调整参数，它决定了我们想要惩罚模型的程度。</p><h1 id="0c40" class="me ir hi bd is mf mg mh iw mi mj mk ja ml mm mn je mo mp mq ji mr ms mt jm mu bi translated">L2正则化或岭正则化</h1><p id="a103" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">L2正则化或岭正则化也给误差函数增加了惩罚。但是这里的惩罚是权重值的<strong class="jq hj">平方</strong>之和。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/700b9bd387f41d75c797d72fbba0b8e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/0*dC6dRJoHcRGr-NQB.png"/></div></figure><p id="7d5a" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">与L1相似，在L2，<code class="du mw mx my lp b">p</code>是决定我们想要惩罚模型多少的调整参数。</p><p id="e9b8" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">这就是<strong class="jq hj">正规化</strong>。</p><p id="11ad" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated"><strong class="jq hj">多项式回归</strong></p><p id="9471" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">多项式回归是一种回归算法，将因变量(y)和自变量(x)之间的关系建模为n次多项式。多项式回归方程如下所示:</p><pre class="kw kx ky kz fd lo lp lq lr aw ls bi"><span id="931e" class="iq ir hi lp b fi lt lu l lv lw">y = b0+b1x1+ b2x12+ b2x13+...... bnx1n</span></pre><p id="0861" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated"><strong class="jq hj">多项式回归有什么必要？</strong></p><p id="a1f0" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">如果我们在线性数据集中应用线性回归模型，将会产生很好的结果。而对于非线性数据集，应用线性回归。它会产生剧烈的结果。为了解决这个问题，多项式回归应运而生。</p><p id="e2f8" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">当数据点以非线性方式排列时，我们需要多项式回归模型。使用下面的线性数据集和非线性数据集的对比图，我们可以更好地理解它。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es na"><img src="../Images/dbd6de403d2d4e521a9110a5f3bb5ba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*OI53ZgTSFlFsCKU5.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">线性与多项式</figcaption></figure><p id="b727" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">用于简单和多元线性回归的公式。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/7e217c8ec23f4cb61a40a946506dde75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VYXL3Ac4HbQYf6fj1x6l0w.png"/></div></div></figure><p id="f5bf" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">用于多项式回归的公式</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nc"><img src="../Images/9b7b159314f38306e6fa14a059c69082.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pW0NIj6ivo4qVo97PGnUOA.png"/></div></div></figure><p id="d39a" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">当我们比较以上三个方程时，我们可以清楚地看到，三个方程都是多项式方程，只是变量的<strong class="jq hj">次不同。一元和多元线性方程也是一次多项式方程，多项式回归方程是n次线性方程。所以如果我们给我们的线性方程组加一个度，那么它就会转化为多项式线性方程组。</strong></p><p id="fec0" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">在数据集中实现多项式回归</p><pre class="kw kx ky kz fd lo lp lq lr aw ls bi"><span id="ff29" class="iq ir hi lp b fi lt lu l lv lw">from sklearn.preprocessing <strong class="lp hj">import</strong> PolynomialFeatures<br/>poly_regs= PolynomialFeatures(degree= 2)<br/>x_poly= poly_regs.fit_transform(x)</span></pre><p id="562e" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">我们使用了<strong class="jq hj">poly _ regs . fit _ transform(x)</strong>，因为首先，我们将特征矩阵转换为多项式特征矩阵，然后将其拟合到多项式回归模型中。参数值(degree= 2)取决于我们的选择。</p><pre class="kw kx ky kz fd lo lp lq lr aw ls bi"><span id="b516" class="iq ir hi lp b fi lt lu l lv lw">#Visualizing the result <strong class="lp hj">for</strong> Polynomial Regression</span><span id="5bb2" class="iq ir hi lp b fi lx lu l lv lw">mtp.scatter(x,y,color=”blue”)<br/>mtp.plot(x, lin_reg_2.predict(poly_regs.fit_transform(x)), color=”red”)<br/>mtp.title(“Bluff detection model(Polynomial Regression)”)<br/>mtp.xlabel(“Position Levels”)<br/>mtp.ylabel(“Salary”)<br/>mtp.show()<br/>lin_reg_2.predict(poly_regs.fit_transform(x))</span></pre><p id="4d61" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">我们采用Lin _ reg _ 2 . predict(poly _ regs . fit _ transform(x ),而不是x_poly，因为我们需要一个线性回归对象来预测多项式特征矩阵。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/a16f80a9038e532a7a67a16fce7f6a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*sDgVd3KwkD7nEgZVCTpmOg.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">多项式回归—2次</figcaption></figure><p id="2d1b" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">因此，我们可以通过增加多项式的次数来获得更精确的结果。我们将得到度=4的精确图，如下所示。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/0314f63080791ba517238b553a3bf538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*x038ukUwCJmteOxe9vvACA.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">多项式回归—4次</figcaption></figure><p id="18f9" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">希望这篇文章给出了机器学习的具体定义。</p><p id="c88e" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">下一个话题再见。</p><p id="99db" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jb kq ka kb jf ks kd ke jj ku kg kh ki hb bi translated">快乐学习:)</p></div></div>    
</body>
</html>