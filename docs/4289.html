<html>
<head>
<title>Principal Component Analysis Simplified.. !</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简化的主成分分析..！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/principal-component-analysis-simplified-78afa8118ddd?source=collection_archive---------7-----------------------#2021-09-17">https://medium.com/analytics-vidhya/principal-component-analysis-simplified-78afa8118ddd?source=collection_archive---------7-----------------------#2021-09-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="9314" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过这个故事，我想探讨什么可能是无监督学习的重要算法之一，即主成分分析(PCA)。PCA算法非常专业，但我试图用简化的形式来解释它。接下来，我将介绍这种方法的理论、数学直觉以及这种方法的主要优点和缺点。</p><p id="0fc9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在走向主成分分析之前，我想解释一下<strong class="ig hi">维度缩减。<br/> </strong>让我们来理解它的实际含义。<br/> <strong class="ig hi">降维:</strong>简单来说就是减少训练数据集中输入变量或特征的数量。拥有大量变量可能会导致模型过度拟合，我们在研究变量之间的关系时也可能会面临问题。所以我们的主要问题是如何从大量的输入变量中选择变量。从技术上讲，我们如何降低特征空间的维数。这种特殊情况被称为降维。</p><p id="755b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有多种技术可以实现降维。然而，为了简单起见，我将集中于最流行的两种技术:<br/> <strong class="ig hi"> 1)特征选择:</strong>特征选择使用统计或评分技术来选择保留哪些特征或删除哪些特征。<br/>特征选择也称为特征消除，因为我们倾向于通过消除特征来减少特征空间。我们尝试从数据集中消除或过滤冗余或不需要的要素。<br/> <strong class="ig hi"> 2)特征提取:</strong>特征提取是为了创建一个新的、更小的特征集合，该集合仍然捕获大部分有用信息。<br/>有很多算法内置了特征选择和特征提取。<br/> <strong class="ig hi">主成分分析</strong>是一种用于特征提取的技术。</p><p id="d7f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">既然我们对什么是降维有了一个概念，让我们深入了解一下<strong class="ig hi"> PCA。</strong></p><p id="61f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如上所述，PCA是一种降维算法，这意味着它通过将大量输入变量转换为仍包含大量输入变量中大多数信息的较小变量来降低大型数据集的维数。PCA还被用作可视化、噪声过滤、特征提取和工程等等的工具。</p><p id="2a27" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PCA背后的主要思想非常清楚；它减少了数据集中输入变量的数量，同时保留了尽可能多的信息。这就是PCA被认为是特征提取技术的原因！</p><p id="28b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PCA的理论就是这么多。接下来我们将了解PCA是如何工作的。</p><p id="b746" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我将用下面的例子解释PCA背后的数学直觉。我使用二维数据，因为它更容易可视化。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/a411553d6c4602921cb8903f9da9d049.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/0*pxAQ7jBFI7mu1w8g.png"/></div></figure><p id="e872" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上图描绘了散点图上两个变量的数据。<br/>第一步:找到数据的中心。为此，我们将沿两个可变轴取所有观测值的平均值。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/ff91d65814e3d83c6f095d0fbf4ee024.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/0*SrWDRuC8IKef3jaP.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">寻找数据的中心。</figcaption></figure><p id="5b38" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">步骤2:一旦获得中心，我们将移动观察点，使中心与平面的原点重合。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jo"><img src="../Images/ead354f4b9ae3ba878cc1034d38ed7a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*jIOpa87KFoYgry1q.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">中心与平面的原点重合。</figcaption></figure><p id="5579" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">步骤3:我们必须找到第一个主成分(PC1)。我们必须为通过中心的数据点找到一条最佳拟合线。我们可以选择任意一条线，并将数据点投影到这条线上。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jp"><img src="../Images/d7d9a59e5406d0ae3a65f71627e7e67d.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/0*RtLRRnlB6pxN44rh.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">寻找最佳拟合线。</figcaption></figure><p id="74b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第四步:现在我们需要找到投影点到原点的距离，平方它们并最大化它们的和，如下所示。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jp"><img src="../Images/d43c5259bc8ade55b7f10abb6fd09899.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/0*w2TwX1Tx6tDWAcEu.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">求投影点到原点的距离。</figcaption></figure><p id="3c2a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，对于PCA算法，最佳拟合的<strong class="ig hi">线是所有投影点到原点的距离之和最大时的线。</strong></p><p id="948c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第五步:现在我们需要知道直线的斜率。假设我们的斜率为0.25，这意味着最佳拟合线由变量1的四部分和变量2的一部分组成。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ju"><img src="../Images/1d29bf9fadce7b76a08a734422faf05d.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/0*Ry-EY9WCkomJo56g.png"/></div></figure><p id="8430" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，B =4，C = 1。现在，借助毕达哥拉斯定理，我们可以很容易地找出A的值。<br/> PCA缩放这些值，因此A成为单位长度向量，这使得A = 1，这个单位向量A就是<strong class="ig hi">特征向量！</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jv"><img src="../Images/4260183c86e0e9cb70760ac0889d1263.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/0*QBiGnj5d2TWUYBey.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">将线缩小到1个单位。</figcaption></figure><p id="ad28" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">投影数据点到原点的平方距离之和(即d1、d2、d3、d4..)是<strong class="ig hi">的特征值！<br/> </strong>所以从上面的小例子我们可以明白，对于PC1来说，变量1的重要性几乎是变量2的四倍。</p><p id="ecba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第六步:我们几乎已经到达了目的地的一半，但是还有最后一站，那就是找到第二个主成分(PC2)。因为我们只使用了两个变量。由于主成分之间的相关性为0，我们的PC2将只是一个与我们刚刚发现的PC1正交的向量。<br/> PC2将只是一条穿过原点并与PC1正交的直线。因此，PC2将被计算为1份变量1必须与4份变量2混合。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jw"><img src="../Images/943b04df207a52ddcf6412fff54dd30a.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*2vzTJn5fBtwf_eNt.png"/></div></figure><p id="2d86" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">红线是PC2。</p><p id="b580" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，如果没有<strong class="ig hi">解释的差异，拥有这两个组件将没有任何用处。</strong></p><p id="4877" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">什么是解释差异？<br/> </strong>解释方差告诉我们数据中有多少方差是由各个主成分解释的。<br/>我们可以通过对两个主成分的平方距离求和，然后将这些值除以数据的样本大小，得到PC的各自方差。<br/>主成分按其解释的方差排序。如果总解释方差达到足够的值，我们选择顶部主成分。</p><p id="b155" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">利弊:<br/>利弊:<br/> </strong> 1)减少过度拟合<strong class="ig hi"> : </strong> PCA通过减少数据集中不需要的特征来帮助模型不过度拟合。<br/> 2)有助于可视化:高维(4D或更高维)的数据很难可视化，PCA通过降维来帮助可视化。<br/> 3)提高模型性能:拥有太多的特征将导致模型无法给出最佳和准确的结果，但由于PCA，它通过去除相关变量来加速机器学习算法。<br/> <strong class="ig hi">缺点:<br/> </strong> 1)在实现PCA之前，必须对数据集进行标准化，否则PCA将无法优化主成分。<br/> 2)如果我们不小心选择主成分，信息丢失的机会。输入变量变得越来越难以解释。</p><p id="817b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以就这样结束了！我希望我能够给我的读者这个美丽的算法的基本本质。<br/>我嵌入了一些我在学习这个算法时发现有用的重要资源:<br/>【https://www.youtube.com/watch?v=FgakZw6K1QQ】<br/><a class="ae jx" href="https://www.youtube.com/watch?v=OFyyWcw2cyM" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=OFyyWcw2cyM</a><br/><a class="ae jx" href="https://arxiv.org/pdf/1404.1100.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1404.1100.pdf</a></p><p id="892c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">祝大家学习愉快..:)</p></div></div>    
</body>
</html>