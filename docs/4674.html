<html>
<head>
<title>BERT — Pre-training + Fine-tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">BERT —预培训+微调</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bert-pre-training-fine-tuning-eb574be614f6?source=collection_archive---------0-----------------------#2021-12-26">https://medium.com/analytics-vidhya/bert-pre-training-fine-tuning-eb574be614f6?source=collection_archive---------0-----------------------#2021-12-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/e68d3a795e42500bf6d2dc5293fbb6de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hzPw6ngLghgnVdwy.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">来源—<a class="ae it" href="https://ruder.io/content/images/2021/02/fine-tuning_methods.png" rel="noopener ugc nofollow" target="_blank">https://ruder . io/content/images/2021/02/fine-tuning _ methods . png</a></figcaption></figure><p id="1931" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Huggingface.co通过他们的变形金刚API使得使用基于变形金刚的模型变得很方便。但是很多时候，仅仅微调是不起作用的。对未标记的数据进行预训练，然后进行微调，有助于模型达到预期的结果。Huggingface API也提供了预训练功能。在这篇博文中，我将解释如何进行预训练，然后微调一个基于变形金刚的模型。为此，我将使用BERT作为参考模型。</p><h1 id="5989" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">数据格式编排</h1><p id="6467" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">要执行预训练，数据必须采用特定的格式。它应该在一个文本文件(。txt格式)每行一句。此文本文件的目的是首先使用Word Piece tokenizer对数据进行标记，然后对数据执行预训练。</p><h1 id="5ed0" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">预训练模型</h1><h2 id="5fa1" class="kv jt hh bd ju kw kx ky jy kz la lb kc jf lc ld kg jj le lf kk jn lg lh ko li bi translated">在文本上训练分词器</h2><p id="dc68" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">将数据转换成所需的格式后，下一步是对输入数据训练标记器。这一步有助于创建数据的词汇表。下面的代码要点显示了如何使用单词片段标记器标记文本。要阅读更多关于单词片段分词器的内容，可以参考下面链接的第4.1节</p><blockquote class="lj"><p id="7301" class="lk ll hh bd lm ln lo lp lq lr ls jr dx translated">https://arxiv.org/pdf/1609.08144v2.pdf<a class="ae it" href="https://arxiv.org/pdf/1609.08144v2.pdf" rel="noopener ugc nofollow" target="_blank"/></p></blockquote><figure class="lt lu lv lw lx ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><h2 id="2fc8" class="kv jt hh bd ju kw kx ky jy kz la lb kc jf lc ld kg jj le lf kk jn lg lh ko li bi translated">训练伯特完成MLM任务</h2><p id="f5e2" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">下一步将是为掩蔽语言建模任务预先训练BERT。为此，我们将使用用于为此目的训练标记器的相同数据集。对于MLM任务，随机屏蔽15%的记号，然后训练模型来预测这些记号。该功能存在于Huggingface API中，在下面的代码中给出</p><figure class="ma mb mc md fd ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="f529" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">到目前为止，我们已经完成了预训练部分。让我们转到微调部分。</p></div><div class="ab cl me mf go mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ha hb hc hd he"><h1 id="3a38" class="js jt hh bd ju jv ml jx jy jz mm kb kc kd mn kf kg kh mo kj kk kl mp kn ko kp bi translated">微调模型</h1><h2 id="a8f9" class="kv jt hh bd ju kw kx ky jy kz la lb kc jf lc ld kg jj le lf kk jn lg lh ko li bi translated">数据准备</h2><p id="d96c" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">对于微调部分，数据的格式必须不同于我们在预训练部分使用的格式。伯特接受三个输入，即。—输入标识，注意掩码，令牌类型标识。我不会详细讨论它们是什么。你可以参考伯特论文中的内容。在这里，我将解释如何从Huggingface API计算它们。在这里，我将使用BERT模型进行分类。人们可以根据自己的方便对代码进行修改。</p><figure class="ma mb mc md fd ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="c05a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在上面的代码中，我使用了来自<strong class="iw hi"> torch.utils </strong>的<strong class="iw hi">数据集</strong>类和<strong class="iw hi"> BERT的</strong> <strong class="iw hi">标记器</strong>将数据转换成所需的格式。下一步，我将创建一个DataLoader类，用于培训和测试。</p><h2 id="324c" class="kv jt hh bd ju kw kx ky jy kz la lb kc jf lc ld kg jj le lf kk jn lg lh ko li bi translated">模型定义</h2><p id="fe0a" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">出于微调的目的，现在让我们从模型构建部分开始。出于分类目的，我将在BERT的顶部添加两个线性层，其中<strong class="iw hi"> dropout = 0。</strong> 1和<strong class="iw hi"> ReLU </strong>作为激活功能。人们也可以尝试不同的配置。我已经定义了PyTorch类来构建下面代码中的模型</p><figure class="ma mb mc md fd ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><h2 id="5901" class="kv jt hh bd ju kw kx ky jy kz la lb kc jf lc ld kg jj le lf kk jn lg lh ko li bi translated">训练和验证功能</h2><p id="a108" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">最后一步是定义训练和验证函数来执行微调。这将是PyTorch中每个人都会用到的一个常用函数。以下代码描述了这一点:-</p><figure class="ma mb mc md fd ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="0fa2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">瞧，现在你完成了实现目标所需的所有步骤。但是可以尝试如上所述的不同配置。此外，你可以尝试不同于上述分类的任务。如果你想完成代码，你可以访问下面的链接</p><div class="mq mr ez fb ms mt"><a href="https://github.com/DhavalTaunk08/NLP_scripts" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hi fi z dy my ea eb mz ed ef hg bi translated">GitHub-dhavaltaunk 08/NLP _ scripts:包含与各种基于变压器的模型相关的笔记本…</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">github.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh in mt"/></div></div></a></div></div><div class="ab cl me mf go mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ha hb hc hd he"><p id="ce92" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这次都是我这边的。如果你想阅读更多关于ML/DL的内容，请访问下面的链接，如果你愿意，请鼓掌。</p><div class="mq mr ez fb ms mt"><a rel="noopener follow" target="_blank" href="/@taunkdhaval08"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hi fi z dy my ea eb mz ed ef hg bi translated">Dhaval Taunk培养基</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">阅读达瓦尔·陶克在媒介上的作品。数据科学家@ Yes Bank |前ML实习生@ Haptik |前实习生@ IIT…</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">medium.com</p></div></div><div class="nc l"><div class="ni l ne nf ng nc nh in mt"/></div></div></a></div></div></div>    
</body>
</html>