<html>
<head>
<title>Logistic Regression : All You wanna Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归:你想知道的一切</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-all-you-wanna-know-e7938f402663?source=collection_archive---------19-----------------------#2021-01-15">https://medium.com/analytics-vidhya/logistic-regression-all-you-wanna-know-e7938f402663?source=collection_archive---------19-----------------------#2021-01-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="504f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从头开始完成实施</p><figure class="jc jd je jf fd jg er es paragraph-image"><div class="ab fe cl jh"><img src="../Images/3372b65ec4487ec9c7b87f5a2e9b9b44.png" data-original-src="https://miro.medium.com/v2/format:webp/0*3TunSOJZ__L70jxN.jpg"/></div></figure><p id="2d4a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">逻辑回归用于模拟某个类别的概率，以便可以将其赋值为0或1。这是一个统计和监督学习模型。这是二元分类的常用方法。</p><blockquote class="jk jl jm"><p id="83df" class="ie if jn ig b ih ii ij ik il im in io jo iq ir is jp iu iv iw jq iy iz ja jb ha bi translated">比如你想预测一个人是否有糖尿病！<strong class="ig hi"> <em class="hh">或</em> </strong>假设你想从任意数据集<strong class="ig hi"> <em class="hh">或</em> </strong>中知道存活率任意邮件是垃圾邮件(1)还是不是(0)。</p></blockquote><blockquote class="jr"><p id="31b9" class="js jt hh bd ju jv jw jx jy jz ka jb dx translated">所以基本上这是一种分类技术。这里我们将看到二进制分类。</p></blockquote><p id="4d48" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">我们在逻辑回归中使用<strong class="ig hi"> Sigmoid函数</strong>，因为我们希望我们的值介于0和1之间，如下图所示。</p><figure class="jc jd je jf fd jg er es paragraph-image"><div class="er es kg"><img src="../Images/9b3ab94ee78b2d9907fb77e3ea272b0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/0*GyAwcc6g0iQUHBQY.png"/></div></figure><h1 id="2815" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">假设函数</h1><figure class="jc jd je jf fd jg er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lf"><img src="../Images/6a418ae82b77dad1a705b498c0831b9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YU7k-oQVnWfvtpMl.png"/></div></div></figure><p id="8c0f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将两者结合起来会给我们→</p><figure class="jc jd je jf fd jg er es paragraph-image"><div class="er es lk"><img src="../Images/43398a8e9afd09e6f7be11fff3053b3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/0*FH33F0nbUQNAb_Ai.png"/></div></figure><p id="e1c7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个函数总是给我们0到1之间的值，告诉我们这个点的概率。当我们接近预测线时，我们的预测变得不那么有把握(因为它们是混乱的)。我们产生一个判定边界，即如果值大于0.5，则它是1，如果小于0.5，则它是0类。</p><figure class="jc jd je jf fd jg er es paragraph-image"><div class="er es ll"><img src="../Images/f002b7272806ed5c5dfe627e147f4ccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*OeYndw2_Hh_DiWxi.png"/></div></figure><h1 id="be58" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">对数损失(二元交叉熵)</h1><blockquote class="jk jl jm"><p id="a595" class="ie if jn ig b ih ii ij ik il im in io jo iq ir is jp iu iv iw jq iy iz ja jb ha bi translated">逻辑回归损失</p></blockquote><figure class="jc jd je jf fd jg er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lm"><img src="../Images/003f04023e1ea4dfefca32e7e81173af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_V2sc6VSwi5aXpBY.png"/></div></div></figure><p id="47bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这里，您可以看到正确值正在乘以预测值。公式的第一部分解释了何时标签为1乘以点的置信度为正。类似地，当标签为0乘以其为负的置信度时，第二部分成立。</p><blockquote class="jk jl jm"><p id="a108" class="ie if jn ig b ih ii ij ik il im in io jo iq ir is jp iu iv iw jq iy iz ja jb ha bi translated">我们必须最小化这个损失函数，为此我们将使用线性回归中的梯度下降法。</p></blockquote><p id="b9ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">线性回归博客链接</p><div class="ln lo ez fb lp lq"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/linear-regression-c6625caf9e8e"><div class="lr ab dw"><div class="ls ab lt cl cj lu"><h2 class="bd hi fi z dy lv ea eb lw ed ef hg bi translated">线性回归</h2><div class="lx l"><h3 class="bd b fi z dy lv ea eb lw ed ef dx translated">单个和多个因变量</h3></div><div class="ly l"><p class="bd b fp z dy lv ea eb lw ed ef dx translated">medium.com</p></div></div><div class="lz l"><div class="ma l mb mc md lz me ji lq"/></div></div></a></div><h1 id="b324" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">梯度下降</h1><figure class="jc jd je jf fd jg er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mf"><img src="../Images/1558d161cf7f50c25b42c2d3c6b15cb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gkOAExxW-kQ0feZ-.png"/></div></div></figure><p id="b1b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在计算上面照片中的导数时，我们没有使用负号，这是线性回归和逻辑回归的梯度下降公式之间的唯一区别。</p><p id="c274" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi mg translated"><span class="l mh mi mj bm mk ml mm mn mo di"> N </span>现在我们都准备好开始实施了→</p><pre class="jc jd je jf fd mp mq mr ms aw mt bi"><span id="f177" class="mu ki hh mq b fi mv mw l mx my">def <strong class="mq hi">sigmoid</strong>(x):<br/>    return 1.0/(1.0 + np.exp(-x))</span><span id="489e" class="mu ki hh mq b fi mz mw l mx my">def <strong class="mq hi">hypothesis</strong>(X,theta):<br/>    <em class="jn"># X - entire array (m,n+1)<br/>    # theta - np.array(n+1,1)</em><br/>    return sigmoid(np.dot(X,theta))</span><span id="082f" class="mu ki hh mq b fi mz mw l mx my">def <strong class="mq hi">error</strong>(X,y,theta):<br/>    <em class="jn">"""<br/>    parameters:<br/>    X - (m,n+1)<br/>    Y - (m,1)<br/>    theta - (n+1,1)<br/>    return scalar value of loss<br/>    """</em><br/>    hi = hypothesis(X,theta)<br/>    e = -1*np.mean(y*np.log(hi)+(1-y)*np.log(1-hi))<br/>    return e</span><span id="ba14" class="mu ki hh mq b fi mz mw l mx my">def <strong class="mq hi">gradient</strong>(X,y,theta):<br/>    <em class="jn">"""<br/>    parameters:<br/>    X - (m,n+1)<br/>    Y - (m,1)<br/>    theta - (n+1,1)<br/>    return vector<br/>    """</em><br/>    hi = hypothesis(X,theta)<br/>    grad = np.dot(X.T,(y-hi))<br/>    return grad/X.shape[0]</span><span id="be0f" class="mu ki hh mq b fi mz mw l mx my">def <strong class="mq hi">gradient_descent</strong>(X,y,lr=0.1,max_itr=500):<br/>    n = X.shape[1]<br/>    theta = np.zeros((n,1))<br/>    <br/>    error_list = []<br/>    for i in range(max_itr):<br/>        err = error(X,y,theta)<br/>        error_list.append(err)<br/>        <br/>        grad = gradient(X,y,theta)<br/>        <br/>        theta = theta + lr*grad<br/>    return theta ,error_list</span><span id="fc1a" class="mu ki hh mq b fi mz mw l mx my">def <strong class="mq hi">predict</strong>(X,theta):<br/>    h = hypothesis(X,theta)<br/>    output = np.zeros(h.shape)<br/>    output[h&gt;=0.5] = 1<br/>    output = output.astype('int')<br/>    return output</span></pre><p id="3683" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在只需将逻辑回归应用于任何二元模型，如糖尿病分类，或者您可以从sklearn导入乳腺癌数据集，然后应用它！！</p></div></div>    
</body>
</html>