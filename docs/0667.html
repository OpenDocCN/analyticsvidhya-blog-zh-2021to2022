<html>
<head>
<title>Understand T5 — Text-to-Text Transfer Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解T5 —文本到文本转换转换器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understand-t5-text-to-text-transfer-transformer-9bc1757989ab?source=collection_archive---------7-----------------------#2021-01-26">https://medium.com/analytics-vidhya/understand-t5-text-to-text-transfer-transformer-9bc1757989ab?source=collection_archive---------7-----------------------#2021-01-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1ed4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原文:<a class="ae jd" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">用统一的文本到文本转换器</a>探索迁移学习的极限。</p><h1 id="c3d6" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">概观</h1><p id="883a" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在我看来，这篇论文更多的是面向工程而不是面向方法。它没有提出新的模型架构或训练技术。然而这篇论文的贡献是巨大的。为了研究NLP中各种体系结构、训练目标、技术和训练数据集对迁移学习的确切贡献，作者进行了一系列系统的实验，并向我们展示了根据经验考虑的最佳和有前途的策略。之后，他们结合所有的发现，提出了一个预训练模型T5和数据集C4。另一个亮点是他们在研究中使用了统一的文本到文本框架，即把每个自然语言处理任务转换成文本到文本。</p><p id="e84d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据我的理解，使用统一框架的逻辑是，如果它可以在广泛的任务范围内取得良好的结果，那么它可能暗示着模型确实学到了自然语言中通用的东西。</p><h2 id="d9db" class="kh jf hi bd jg ki kj kk jk kl km kn jo iq ko kp js iu kq kr jw iy ks kt ka ku bi translated"><strong class="ak"> T5:文本到文本转换变压器</strong></h2><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es kv"><img src="../Images/d99084ea9f47102730df82abe641ee82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*voAo0zBPTCVrwcut1oHtEg.png"/></div></div></figure><p id="8247" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">框架中的架构是编码器-解码器，因此每个任务都应该转换成输入-输出格式，其中两者都是文本。为了帮助模型识别要执行的特定任务，任务名称被附加在输入的开头。T5的优点来自于关于多个方面的最优策略的组合，包括编码器-解码器架构、损坏跨度去噪目标、C4预训练数据集、多任务预训练+对下游任务的微调，以及在模型大小和训练时间方面的缩放。</p><h2 id="e988" class="kh jf hi bd jg ki kj kk jk kl km kn jo iq ko kp js iu kq kr jw iy ks kt ka ku bi translated"><strong class="ak">实验结果</strong></h2><p id="e62a" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated"><strong class="ih hj">架构</strong>:比较了三种类型:编码器-解码器、仅解码器语言模型、仅解码器前缀语言模型。控制参数的数量，编码器-解码器是所有下游任务中最好的。</p><p id="db5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">无监督目标</strong>:下面的流程图显示了他们的探索，表格通过一个例子说明了这些目标(标题包括实验的细节)。他们选择受损跨度去噪目标，因为计算成本可以降低，同时保持与基线相当的性能。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lh"><img src="../Images/4c72fa47226d80b25d3dfffc3dd797f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2S3QSwGgiM2nXVpE39khA.png"/></div></div></figure><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es li"><img src="../Images/98f76f99bc75358fdfde7c6457adfa3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*stHl6bn-fFxq8WoULiKYTQ.png"/></div></div></figure><p id="ab0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该目标的详细说明如下。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lj"><img src="../Images/cdfd2d099bb434ec8511923e2a351bdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PIVSYpFCjtnnj9ymgpkN2g.png"/></div></div></figure><p id="f74b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">预训练数据集</strong>:他们发现对域内未标记数据进行预训练可以提高下游任务上的性能，例如基于维基百科的SQuAD。此外，关于数据集的大小，他们发现一些重复的预训练数据可能是无害的。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lk"><img src="../Images/79065d9f0c20fb07186e88168143eafe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A2pW5kEWrtpnn3QQ3-8i6Q.png"/></div></div></figure><p id="0861" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">训练策略</strong>:他们实验了三种微调策略，发现微调所有参数是最好的。他们还考虑了多任务学习，其中他们将无监督的任务与其他任务混合，并探索了三种设置来自每个任务的数据比例的策略。结果表明，首先在无监督任务上进行预训练，然后在下游任务上进行微调优于多任务训练策略。然后，他们将多任务学习与微调相结合，并比较不同的策略，如下所示。由于多任务预训练+微调与无监督预训练+微调具有相似的性能，并且它使我们能够在整个训练期间监控“下游”性能，而不仅仅是微调，因此作者在他们最终的T5模型中使用了这一策略。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es ll"><img src="../Images/ceb53ba310ddebe4a5e4b34007f1e760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uRV_Rs3-hk9fUl_lkdfPGg.png"/></div></div></figure><p id="9029" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">扩展</strong>:他们专注于解决这个问题:“你刚刚获得了4倍的计算能力。应该怎么用？”他们发现，一般来说，与单独增加训练时间或批量相比，增加模型大小会导致性能的进一步提升。集成学习提供了一种通过扩展来提高性能的正交且有效的方法。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lj"><img src="../Images/e0b3f855cf98bbd7ffb91a40cfd9de0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HJPhFDPHyh5RznHrJp7NZA.png"/></div></div></figure><p id="3960" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最终，他们将所有的发现整合在一起，结合最佳设置，得出了T5，这是一个经过预先训练的模型，在许多排行榜上有着最先进的表现。</p><h1 id="0847" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">相关主题阅读</strong></h1><ul class=""><li id="a349" class="lm ln hi ih b ii kc im kd iq lo iu lp iy lq jc lr ls lt lu bi translated">多任务学习:<a class="ae jd" href="https://ruder.io/multi-task/" rel="noopener ugc nofollow" target="_blank">深度神经网络中多任务学习概述</a></li><li id="6c9e" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated">UnifiedQA :将文本到文本框架应用于问答的论文</li></ul><h1 id="959c" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">参考文献</strong></h1><ol class=""><li id="8c49" class="lm ln hi ih b ii kc im kd iq lo iu lp iy lq jc ma ls lt lu bi translated">Raffel，c .，Shazeer，n .，Roberts，a .，Lee，k .，Narang，s .，Matena，m .，Zhou，y .，Li，w .，，Liu，P. J. (2020)。用统一的文本到文本转换器探索迁移学习的局限性。<em class="mb">机器学习研究杂志</em>，<em class="mb"> 21 </em> (140)，1–67。</li></ol></div></div>    
</body>
</html>