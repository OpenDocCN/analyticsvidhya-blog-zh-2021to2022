<html>
<head>
<title>Named Entity Recognition using Deep Learning(ELMo Embedding+ Bi-LSTM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习的命名实体识别(ELMo嵌入+双LSTM)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/named-entity-recognition-using-deep-learning-elmo-embedding-bi-lstm-48295bc66cab?source=collection_archive---------0-----------------------#2021-06-13">https://medium.com/analytics-vidhya/named-entity-recognition-using-deep-learning-elmo-embedding-bi-lstm-48295bc66cab?source=collection_archive---------0-----------------------#2021-06-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/a7d824463e23a26ff566204e9ab1db8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*z9Ao3iQlgsvwqScKJq-3KQ.png"/></div></figure><h1 id="312a" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><em class="jk">简介:</em></h1><p id="294a" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><a class="ae kj" href="https://en.wikipedia.org/wiki/Named-entity_recognition" rel="noopener ugc nofollow" target="_blank"> <strong class="jn hj"> <em class="kk">命名-实体识别</em></strong><em class="kk">(</em><strong class="jn hj"><em class="kk"/></strong><em class="kk">)</em></a><em class="kk">(也称</em> <strong class="jn hj"> <em class="kk">实体识别</em> </strong> <em class="kk">，</em> <strong class="jn hj"> <em class="kk">实体分块</em></strong><em class="kk"/><strong class="jn hj"><em class="kk">实体提取</em> </strong> <em class="kk">)是一</em></p><p id="7d69" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><em class="kk">它为你的内容增加了丰富的语义知识，帮助你迅速理解任何给定文本的主题。</em></p><h1 id="6744" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">应用:</h1><p id="a786" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><em class="kk"/><strong class="jn hj"><em class="kk"/></strong><em class="kk">的几个应用包括:从</em> <strong class="jn hj"> <em class="kk">法律、金融、医疗文档中提取重要的命名实体，</em> </strong> <em class="kk">为</em> <strong class="jn hj"> <em class="kk">新闻提供商分类内容，</em> </strong> <em class="kk">改进</em><strong class="jn hj"><em class="kk"/></strong><em class="kk">等。</em></p><h1 id="2c78" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">解决这个问题的方法:</h1><ol class=""><li id="1ad4" class="kq kr hi jn b jo jp js jt jw ks ka kt ke ku ki kv kw kx ky bi translated"><strong class="jn hj">机器学习方法:</strong> <em class="kk">把问题当作一个</em> <strong class="jn hj"> <em class="kk">多类分类</em> </strong> <em class="kk">带有命名实体的是我们的标签。这里的问题是</em> <strong class="jn hj"> <em class="kk">对于较长的句子</em> </strong> <em class="kk"> </em> <strong class="jn hj"> <em class="kk">识别和标注命名实体需要对句子的上下文和其中的单词标注的顺序有透彻的理解，</em> </strong> <em class="kk">这种方法忽略了并且不能抓住整个句子的本质。</em></li><li id="5f44" class="kq kr hi jn b jo kz js la jw lb ka lc ke ld ki kv kw kx ky bi translated"><strong class="jn hj">深度学习方法:</strong> <em class="kk">可以解决这个问题的最佳可能模型是</em> <strong class="jn hj">【长短时记忆】</strong> <em class="kk">模型</em><strong class="jn hj"/><em class="kk">具体来说，我们将使用</em> <strong class="jn hj">双向LSTM </strong> <em class="kk">进行设置。</em> <strong class="jn hj">双向LSTM是两个LSTM的组合——一个从“从右到左”向前，一个从“从左到右”向后，从而捕捉句子的整个本质/上下文。<em class="kk"> </em> </strong> <em class="kk">对于</em><strong class="jn hj"><em class="kk">【NER】</em></strong><em class="kk">，由于上下文在一个序列中涵盖了过去和未来的标签，所以我们需要把过去和未来的信息都考虑进去。</em></li></ol><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es le"><img src="../Images/50fb6533cdc0c9e85a9fa3321f0510d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*N78Lm_Fn5LkF-G9Rb3PzMA.jpeg"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">双LSTM</figcaption></figure><blockquote class="ln lo lp"><p id="b69b" class="jl jm kk jn b jo kl jq jr js km ju jv lq kn jy jz lr ko kc kd ls kp kg kh ki hb bi translated"><strong class="jn hj">嵌入层:</strong><a class="ae kj" href="https://arxiv.org/pdf/1802.05365.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jn hj">ELMo</strong></a>(<strong class="jn hj"><em class="hi">嵌入来自语言模型</em></strong>):<strong class="jn hj"><em class="hi">ELMo</em></strong><em class="hi">是一个深度语境化的词表示，模型既，</em></p></blockquote><p id="dcd2" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><em class="kk"> (1)用词的复杂特点(如</em> <strong class="jn hj"> <em class="kk">【句法语义】</em> </strong> <em class="kk">)和</em></p><p id="bd61" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><em class="kk"> (2)这些用法如何在语言语境中变化(即</em> <strong class="jn hj"> <em class="kk">来模拟一词多义</em> </strong> <em class="kk">)。</em> <strong class="jn hj"> <em class="kk">举例</em> </strong> <em class="kk">:虽然'</em> <strong class="jn hj"> <em class="kk">苹果</em> </strong> <em class="kk">术语很常见，但是</em><strong class="jn hj"><em class="kk">ELMo</em></strong><em class="kk">会给两者不同的嵌入(</em> <strong class="jn hj"> <em class="kk">果实和组织</em> </strong> <em class="kk">)</em></p><p id="280d" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj">示例</strong> : <em class="kk">我们也不需要担心训练数据的词汇外(OOV)标记，因为ELMo也会为此生成字符嵌入。</em></p><p id="cd6d" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><em class="kk">这些词向量是深度双向语言模型(</em><strong class="jn hj"><em class="kk">biLM</em></strong><em class="kk">)内部状态的学习函数，在大型文本语料库上进行预训练。它们可以很容易地添加到现有的模型中，并在广泛的具有挑战性的NLP问题中显著提高技术水平，包括</em> <strong class="jn hj"> <em class="kk">问题回答、文本蕴涵和情感分析</em> </strong> <em class="kk">。</em></p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/f828346e080e06598db840c91053f8ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*VZW4IAamlxS6DjwwI4aOuQ.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">工程与后勤管理局</figcaption></figure><h1 id="d134" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">让我们看看如何解决这个问题:</h1><ol class=""><li id="875e" class="kq kr hi jn b jo jp js jt jw ks ka kt ke ku ki kv kw kx ky bi translated"><strong class="jn hj">数据</strong> <strong class="jn hj">采集</strong> : <em class="kk">我们要用一个来自Kaggle的</em> <a class="ae kj" href="https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/home" rel="noopener ugc nofollow" target="_blank"> <strong class="jn hj"> <em class="kk">数据集</em> </strong> <em class="kk"> </em> </a> <em class="kk">。请浏览数据以了解更多关于不同标签的信息。</em></li></ol><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/fca189c03a964558a31a9f86274611b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*abzDlmHVfAgxkp1uA0aC-Q.png"/></div></figure><p id="af37" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><em class="kk">我们的数据集中有</em><strong class="jn hj"><em class="kk">47958</em></strong><em class="kk">句子，</em><strong class="jn hj"><em class="kk">35179</em></strong><em class="kk">不同的</em> <strong class="jn hj"> <em class="kk">单词</em> </strong> <em class="kk">，</em> <strong class="jn hj"> <em class="kk"> 42 </em> </strong> <em class="kk">不同的</em><strong class="jn hj"><em class="kk">POS</em></strong><em class="kk"/></p><p id="a90b" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><em class="kk">在本文中我们将构建</em> <strong class="jn hj"> <em class="kk"> 2 </em> </strong> <em class="kk">不同的模型来分别预测</em> <strong class="jn hj"> <em class="kk">标签</em> </strong> <em class="kk">和</em><strong class="jn hj"><em class="kk">POS</em></strong><em class="kk">。</em></p><p id="cb29" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"> <em class="kk"> 2 </em> </strong>。<em class="kk">接下来我们将使用一个</em> <strong class="jn hj"> <em class="kk">类</em> </strong> <em class="kk">，它将把每一个带有命名实体(标签)的句子转换成一个元组列表[(单词，命名实体)，…] </em></p><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lv"><img src="../Images/0efe725c8d1a7a8aa040ed552cead908.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B5z8oEMK4Fdh2v81mjWkFw.png"/></div></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/fa7f9f1059bbdfd084fceefc5cf9eec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*TxZ8b20XfsqCBc2JAFnI1Q.png"/></div></figure><p id="1ac6" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"> <em class="kk"> 3。</em> </strong> <em class="kk">我们来看看</em> <strong class="jn hj"> <em class="kk">句子长度</em> </strong> <em class="kk">在数据集中的分布。所以最长的句子有</em><strong class="jn hj"><em class="kk"/></strong><em class="kk">140个单词，我们可以看到几乎所有的句子都有不到60个单词的</em><strong class="jn hj"><em class="kk"/></strong><em class="kk">。但是由于硬件不足，我们将使用更小的长度，即</em> <strong class="jn hj"> <em class="kk"> 50个字</em> </strong> <em class="kk">，这很容易处理。</em></p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/16e9a01c34c01e400fa1850fc020390f.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*KygMp8drMO2-eZpflWSNOg.png"/></div></figure><p id="e482" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"> <em class="kk"> 4。</em> </strong> <em class="kk">让我们创建</em> <strong class="jn hj"> <em class="kk">单词到索引和索引到单词</em></strong><em class="kk"/><strong class="jn hj"><em class="kk">映射</em> </strong> <em class="kk">，这是训练前和预测后单词转换所必需的。</em></p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/f683400bc101b57929f96e993c393eb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*RS2luMKJ8n0g90agq8oeoQ.png"/></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es md"><img src="../Images/5b0567a2353632fc66af3520df417298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bhKeHoENJG60cci2Go2yvQ.png"/></div></div></figure><p id="df0e" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"> 5。</strong> <em class="kk">从前面生成的元组列表中，现在我们将构建自变量和因变量结构。</em></p><ul class=""><li id="e3cb" class="kq kr hi jn b jo kl js km jw me ka mf ke mg ki mh kw kx ky bi translated"><strong class="jn hj"> <em class="kk">自变量/词语料库:</em> </strong></li></ul><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mi"><img src="../Images/7e2c2d12958cd5c761500bf7293dcbc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*wf9up0ZKnVQ5C_yxm7I71Q.png"/></div></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/452363332712cc78218599f74ed27d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/1*0_ZL7TTpMFQinoTHkiFRgw.png"/></div></figure><ul class=""><li id="86a2" class="kq kr hi jn b jo kl js km jw me ka mf ke mg ki mh kw kx ky bi translated"><strong class="jn hj"> <em class="kk">这同样适用于命名实体，但我们需要将我们的标签映射到数字上</em> </strong></li></ul><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mk"><img src="../Images/4af71ae4a8bcf97b003f744de2bddfde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7WM6FPUfHyKsJo_oawP1wg.png"/></div></div></figure><p id="b9d9" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"> <em class="kk"> 6 </em> </strong>。<strong class="jn hj"> <em class="kk">训练—测试分割(90:10): </em> </strong></p><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es ml"><img src="../Images/01f6b4330b49408fbcdcabb5d2e0b3a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kaLGnOWiz2xLkwS_fCHD5g.png"/></div></div></figure><p id="1904" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"> <em class="kk"> 7。批量训练:S </em> </strong> <em class="kk">由于我们把</em><strong class="jn hj"><em class="kk"/></strong><em class="kk">32作为</em> <strong class="jn hj"> <em class="kk">的批量</em> </strong> <em class="kk">，投料网络必须成块，都是</em> <strong class="jn hj"> <em class="kk"> 32的倍数。</em> </strong></p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/980cfe7c96c7bef3316288fd56cd9c9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*m2gL9_iKX9SB5EdhzhYIHA.png"/></div></figure><p id="11fb" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"><em class="kk">8</em></strong>T38】。 <strong class="jn hj"> <em class="kk">加载ELMo嵌入层:</em> </strong> <em class="kk">我们将导入</em><strong class="jn hj"><em class="kk">tensor flow Hub</em></strong><em class="kk">(一个用于机器学习模型的可重用部分的发布、发现和消费的库)来加载</em> <strong class="jn hj"> <em class="kk"> ELMo嵌入</em> </strong> <em class="kk">特性并创建一个函数，以便我们可以以</em> <strong class="jn hj"> <em class="kk">层的形式加载它</em></strong></p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/b45db6d0e7f174f74b6052827f50230b.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*twLhJAkgjmWYOrcRbXgGow.png"/></div></figure><p id="43aa" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"> <em class="kk">请降级您的Tensorflow套餐，以使用此代码。如果想在TF 2或更高版本中执行同样的操作，必须使用hub.load(url)，然后创建一个KerasLayer(…，其中trainable = True)。</em>T73】</strong></p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/5a14776f4bd9cbf3dc4b4524312f1d67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*djZDwvrZehMt4EHtH8G5Vg.png"/></div></figure><p id="7615" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj">9<em class="kk">。设计我们的神经网络:</em>T77】</strong></p><ul class=""><li id="8830" class="kq kr hi jn b jo kl js km jw me ka mf ke mg ki mh kw kx ky bi translated"><strong class="jn hj"><em class="kk">【ELMo】</em></strong><em class="kk">:我们将指定填充序列的最大长度(</em> <strong class="jn hj"> <em class="kk"> 50 </em> </strong> <em class="kk">)。在网络被训练之后，嵌入层将把每个令牌转换成n维向量。</em></li><li id="f127" class="kq kr hi jn b jo kz js la jw lb ka lc ke ld ki mh kw kx ky bi translated"><strong class="jn hj"> <em class="kk">双向LSTM </em> </strong> <em class="kk">:双向LSTM以一个递归层(如第一个LSTM层)为自变量。这一层从先前的嵌入层获得输出。</em></li><li id="bf90" class="kq kr hi jn b jo kz js la jw lb ka lc ke ld ki mh kw kx ky bi translated"><strong class="jn hj"> <em class="kk">我们将使用2个Bi LSTM层和残差连接到第一个BiLSTM </em> </strong></li><li id="c77f" class="kq kr hi jn b jo kz js la jw lb ka lc ke ld ki mh kw kx ky bi translated"><strong class="jn hj"><em class="kk">time distributed Layer</em></strong><em class="kk">:我们正在处理</em> <strong class="jn hj"> <em class="kk">多对多RNN </em> </strong> <em class="kk">架构，我们期望从每个输入序列得到输出。这里有一个例子，在序列(a1 →b1，a2 →b2…an →bn)中，a和b是每个序列的输入和输出。TimeDistributeDense层允许在每个时间步长的每个输出上进行密集(全连接)操作。现在使用这一层将导致一个最终输出。</em></li></ul><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/86563ac34d04f533b92baaa3150531cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*oqibNNv8LMQ-tHnt-jl5-Q.png"/></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/f210000fadca2f8ec00a6bc5706b0bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*6zKVf_7UsXJYGmvQS3vxDA.png"/></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mr"><img src="../Images/02b82c6e0ae321291a351b0e06528a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mZokcocmXaqpd5WtetQl8g.png"/></div></div></figure><p id="61db" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"> <em class="kk"> 10。训练</em> </strong> : <em class="kk">只运行了1个纪元，因为它花费了很多时间。但是结果很棒。</em></p><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es ms"><img src="../Images/60ced1a5c5b8b2d2e4cefb25de93da6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8toreywDV2YMFAkKQbHEZA.png"/></div></div></figure><p id="4184" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"> <em class="kk"> 11。批量预测</em> </strong> <em class="kk">并使用</em><strong class="jn hj"><em class="kk">index-to-tag</em></strong><em class="kk">将预测的索引转换回word格式</em> <strong class="jn hj"> <em class="kk">。</em> </strong></p><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mt"><img src="../Images/57cf9d120ab00fb2f63e740b27764d33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xmgLnNP6k4Ig52EouZC04A.png"/></div></div></figure><p id="037d" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"> <em class="kk"> 12。评估指标</em> </strong> : <em class="kk">在</em><strong class="jn hj"><em class="kk">NER</em></strong><em class="kk">的情况下，我们可能会处理重要的金融、医疗或法律文档，这些文档中命名实体的精确识别决定了模型的成功。换句话说，</em> <strong class="jn hj"> <em class="kk">误报和漏报在NER任务中是有业务成本的。</em> </strong> <em class="kk">因此，我们评估模型的主要指标将是</em> <strong class="jn hj"> <em class="kk"> F1得分</em> </strong> <em class="kk">因为我们需要在精度和召回率</em>之间取得平衡。</p><ul class=""><li id="d800" class="kq kr hi jn b jo kl js km jw me ka mf ke mg ki mh kw kx ky bi translated"><strong class="jn hj"> <em class="kk">我们能够获得81.2%的F1分数，这相当不错，如果你看看F1的微观、宏观和平均分数，它们也相当不错。如果你训练更多的时期，你肯定会得到更好的结果。</em>T49】</strong></li></ul><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/e7998dfca55fc2b519e3e1c547acbfa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*d6s4PJS82pY-uEnmgq6KKw.png"/></div></figure><p id="4b7e" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj">13<em class="kk">。将我们的结果与SPACY </em> </strong> : <em class="kk">进行比较，我们可以看到我们的模型甚至能够在单个时期内正确地检测到每个标签。</em></p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/95cba7d9fe5fc3f245565ba809e52304.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*MohE_uAOcwG2nrwlDtiP3A.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">我们的模型结果</figcaption></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mw"><img src="../Images/eadc25f6d2d76589c56e679b5307ea50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yagSRu-Hvf6hsPam-3XNMQ.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">宽大的</figcaption></figure><p id="30c6" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj">14<em class="kk">。</em> </strong> <em class="kk"> </em> <strong class="jn hj"> <em class="kk">词性标注/预测</em> </strong> <em class="kk">:因为，我们的数据集中也有</em> <strong class="jn hj"> <em class="kk">【词性】</em> </strong> <em class="kk">词性，我们也可以建立类似的模型来预测。我也实现了它，并且训练了一个时期，结果还是很棒。</em>T77】</p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/15408b2e994a26675913003e2c9cc707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*A1WX-lFpDwpx5mbS0B46TA.png"/></div></figure><ul class=""><li id="e86c" class="kq kr hi jn b jo kl js km jw me ka mf ke mg ki mh kw kx ky bi translated">我们能够获得97.1%的F1分数，这已经很不错了，如果你看看F1的微观、宏观和平均分数，它们也很不错。T81】</li></ul><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es my"><img src="../Images/cb0bb98c10620f3d0f499f971e728e86.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*VuXUEKWSdnuOAVssWyfC0w.png"/></div></figure><p id="fdc4" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"> <em class="kk">将我们的结果与SPACY </em> </strong> : <em class="kk">我们可以看到，我们的模型甚至能够在单个时期内正确地检测每个标签。</em></p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/b81893af1938a607e28a381c997ef71a.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*tUt14LPEswtnciIrG6pwdg.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">我们的模型结果</figcaption></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/d857629140360ee951a7b78f38ffde54.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*cIYcs2rqtQa91eor4PZ4sg.png"/></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es na"><img src="../Images/a469aea70923f69b959a90019dcc1e8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZuANVNuI2KQe5hG0Gz7i5A.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">宽大的</figcaption></figure><p id="4ec3" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"> <em class="kk">感谢阅读本博客。如果你喜欢它，请鼓掌，关注并分享。</em> </strong></p><h1 id="97a5" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">在哪里可以找到我的代码？</h1><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es nb"><img src="../Images/46dd106ae1df55f827598f2da97992f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VGW-SfChCxe7t4aS.jpeg"/></div></div></figure><p id="bb21" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj">Github</strong>:<a class="ae kj" href="https://github.com/SubhamIO/Named-Entity-Recognition-using-ELMo-BiLSTM" rel="noopener ugc nofollow" target="_blank">https://Github . com/SubhamIO/Named-Entity-Recognition-using-ELMo-BiLSTM</a></p><p id="70db" class="pw-post-body-paragraph jl jm hi jn b jo kl jq jr js km ju jv jw kn jy jz ka ko kc kd ke kp kg kh ki hb bi translated"><strong class="jn hj"> <em class="kk">参考文献:</em> </strong></p><ol class=""><li id="4204" class="kq kr hi jn b jo kl js km jw me ka mf ke mg ki kv kw kx ky bi translated">【https://jalammar.github.io/illustrated-bert/】T5<a class="ae kj" href="https://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">T6</a></li><li id="cbca" class="kq kr hi jn b jo kz js la jw lb ka lc ke ld ki kv kw kx ky bi translated"><a class="ae kj" href="https://arxiv.org/pdf/1802.05365.pdf" rel="noopener ugc nofollow" target="_blank"><em class="kk">https://arxiv.org/pdf/1802.05365.pdf</em></a></li><li id="0a66" class="kq kr hi jn b jo kz js la jw lb ka lc ke ld ki kv kw kx ky bi translated"><a class="ae kj" href="https://en.wikipedia.org/wiki/Named-entity_recognition" rel="noopener ugc nofollow" target="_blank"><em class="kk">https://en.wikipedia.org/wiki/Named-entity_recognition</em></a></li><li id="30f9" class="kq kr hi jn b jo kz js la jw lb ka lc ke ld ki kv kw kx ky bi translated"><a class="ae kj" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank"><em class="kk">https://allennlp.org/elmo</em></a></li><li id="b189" class="kq kr hi jn b jo kz js la jw lb ka lc ke ld ki kv kw kx ky bi translated"><a class="ae kj" href="https://sunjackson.github.io/2018/12/11/1ef8909353df3395a36f3f4d3336269b/" rel="noopener ugc nofollow" target="_blank"><em class="kk">https://sun Jackson . github . io/2018/12/11/1 ef 8909353 df 3395 a 36 F3 F4 d 3336269 b/</em></a></li></ol></div></div>    
</body>
</html>