<html>
<head>
<title>DECISION BOUNDARY FOR CLASSIFIERS: AN INTRODUCTION</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类器的决策边界:介绍</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-boundary-for-classifiers-an-introduction-cc67c6d3da0e?source=collection_archive---------0-----------------------#2021-09-07">https://medium.com/analytics-vidhya/decision-boundary-for-classifiers-an-introduction-cc67c6d3da0e?source=collection_archive---------0-----------------------#2021-09-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ed7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于如何确定最佳分类器有许多争论。测量性能指标得分、获得ROC下的面积是少数方法，但是从可视化决策边界中可以收集到大量有用的信息，这些信息将使我们直观地掌握学习模型。</p><p id="a2ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，在本文中，我们将了解以下内容:</p><ul class=""><li id="5110" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">什么是决策边界</li><li id="027d" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">决策边界的重要性</li><li id="ad7f" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">决策边界的类型</li><li id="75c8" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">不同分类器的决策边界。</li><li id="aefc" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">Python代码的用例</li><li id="048f" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">高维数据的决策边界</li><li id="5da0" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">结论</li></ul><p id="8038" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，让我们开始吧</p><h1 id="f9cf" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">什么是决策边界？</h1><p id="b1b9" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在数据集上训练分类器时，使用特定的分类算法，需要定义一组超平面，称为决策边界，将数据点分成特定的类，其中算法从一个类切换到另一个类。在决策边界的一侧，A数据点更可能被称为A类，而在边界的另一侧，它更可能被称为b类。</p><p id="a0cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们举一个逻辑回归的例子。</p><p id="bb19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归的目标是找出一些分割数据点的方法，以便使用特征中存在的信息对给定观察值的类别进行准确预测。</p><p id="70e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们定义了一条描述决策边界的线。因此，在边界一侧的所有点应该具有属于A类的所有数据点，并且在边界一侧的所有点应该具有属于b类的所有数据点。</p><p id="0c77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> S(z)=1/(1+e^-z) </strong></p><ul class=""><li id="2030" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">s(z)= 0和1之间的输出(概率估计值)</li><li id="15f6" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">z =函数的输入(z= mx + b)</li><li id="73c3" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">e =自然对数的基数</li></ul><p id="5d7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们当前的预测函数返回0到1之间的概率分数。为了将其映射到离散类(A/B ),我们选择一个阈值或临界点，高于该阈值或临界点时，我们将值分类为A类，低于该阈值或临界点时，我们将值分类为B类。</p><p id="72ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> p &gt; =0.5，class=A </strong></p><p id="6ef4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> p &lt; =0.5，class=B </strong></p><p id="2a49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们的阈值是0.5，而我们的预测函数返回0.7，我们会将该观察结果分类为a类。如果我们的预测是0.2，我们会将该观察结果分类为b类。</p><p id="da55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">所以，用0.5的线称为决定边界。</em>T9】</strong></p><p id="7bdf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了将预测值映射到概率，我们使用Sigmoid函数。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es kv"><img src="../Images/b21d4832eb1e77458d821398b957a2d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NM7JbD9nt2OEVwbx"/></div></div></figure><h1 id="7e20" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">决策边界的重要性</h1><p id="8326" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">判定边界是分离属于不同类别标签的数据点的表面。决策边界不仅仅局限于我们提供的数据点，还跨越了我们训练的整个特征空间。该模型可以预测我们的特征空间中任何可能的输入组合的值。如果我们训练的数据不是“多样化的”，模型的整体拓扑将<em class="ku">很难推广到新的实例</em>。因此，在将模型用于生产之前，分析最适合“多样化”数据集的所有模型非常重要。</p><p id="ce21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">检查决策边界是了解我们选择的训练数据如何影响我们的模型的性能和泛化能力的一个好方法。决策边界的可视化可以说明模型对每个数据集的敏感程度，这是理解特定算法如何工作以及它们对特定数据集的限制的一种很好的方式。</p><h1 id="19c1" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">用例</h1><p id="9916" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">目的:为各种分类器算法建立决策边界，并决定哪种算法是数据集的最佳算法。</p><p id="6435" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集在这里<a class="ae lh" href="https://www.kaggle.com/rakeshrau/social-network-ads" rel="noopener ugc nofollow" target="_blank">可用。</a></p><p id="d3ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集描述:数据集包含用户的信息，应该基于这些信息建立最佳模型来预测用户是否会购买汽车。</p><p id="bc61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">独立变量:</p><ul class=""><li id="f190" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">年龄:用户的年龄</li><li id="45cc" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">预计薪资</strong>:用户的薪资。</li></ul><p id="79d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因变量:“已购买”，如果用户购买汽车，则为1，否则为0。</p><p id="f188" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤1:导入所有需要的库</p><p id="1ba5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大蟒</p><pre class="kw kx ky kz fd li lj lk ll aw lm bi"><span id="c149" class="ln js hi lj b fi lo lp l lq lr"># Package imports</span><span id="c8b6" class="ln js hi lj b fi ls lp l lq lr">import matplotlib.pyplot as plt</span><span id="1118" class="ln js hi lj b fi ls lp l lq lr">import numpy as np</span><span id="e082" class="ln js hi lj b fi ls lp l lq lr">import sklearn</span><span id="a540" class="ln js hi lj b fi ls lp l lq lr">import sklearn.datasets</span><span id="1094" class="ln js hi lj b fi ls lp l lq lr">import sklearn.linear_model</span><span id="e6e6" class="ln js hi lj b fi ls lp l lq lr">import matplotlib</span><span id="5b3b" class="ln js hi lj b fi ls lp l lq lr">import pandas as pd</span></pre><p id="99a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤2:导入数据集</p><p id="0a26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大蟒</p><pre class="kw kx ky kz fd li lj lk ll aw lm bi"><span id="ae81" class="ln js hi lj b fi lo lp l lq lr">from google.colab import files</span><span id="7022" class="ln js hi lj b fi ls lp l lq lr">uploaded=files.upload()</span><span id="7705" class="ln js hi lj b fi ls lp l lq lr">import io</span><span id="0a38" class="ln js hi lj b fi ls lp l lq lr">df2=pd.read_csv(io.BytesIO(uploaded['Social_Network_Ads.csv']))</span></pre><p id="29de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤3:将StandardScaler应用于数据集。变量“薪金”和“年龄”不在同一个范围内。所以，这些应该被缩放。否则，模型不能预测一个好的结果。标准缩放还有助于加速算法中的计算。</p><p id="358d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大蟒</p><pre class="kw kx ky kz fd li lj lk ll aw lm bi"><span id="d450" class="ln js hi lj b fi lo lp l lq lr">X = df2.iloc[:, :-1].values</span><span id="6622" class="ln js hi lj b fi ls lp l lq lr">y = df2.iloc[:, -1].values</span><span id="88fe" class="ln js hi lj b fi ls lp l lq lr">from sklearn.preprocessing import StandardScaler</span><span id="8557" class="ln js hi lj b fi ls lp l lq lr">sc = StandardScaler()</span><span id="30c3" class="ln js hi lj b fi ls lp l lq lr">X = sc.fit_transform(X)</span></pre><p id="6135" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤4:为分类器导入sklearn库</p><p id="9a24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大蟒</p><pre class="kw kx ky kz fd li lj lk ll aw lm bi"><span id="f32b" class="ln js hi lj b fi lo lp l lq lr">from sklearn.linear_model import LogisticRegression</span><span id="89a1" class="ln js hi lj b fi ls lp l lq lr">from sklearn.naive_bayes import GaussianNB</span><span id="f32f" class="ln js hi lj b fi ls lp l lq lr">from sklearn.ensemble import RandomForestClassifier</span><span id="a5ac" class="ln js hi lj b fi ls lp l lq lr">from sklearn.svm import SVC</span></pre><p id="4bd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤5:获取数据集的维度。</p><p id="3f07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤6:建立逻辑回归模型并显示逻辑回归的决策边界。决策边界可以通过网格密集采样来可视化。但是，如果网格分辨率不够，边界就会显得不准确。<code class="du lt lu lv lj b">meshgrid </code>的目的是用一个x值数组和一个y值数组创建一个矩形网格。我们可以从<a class="ae lh" href="https://stackoverflow.com/questions/36013063/what-is-the-purpose-of-meshgrid-in-python-numpy/49439331" rel="noopener ugc nofollow" target="_blank">这里</a>得到关于如何绘制网格的完整解释。</p><p id="6301" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Meshgrid中，我们将制作一个图像，其中每个像素代表2D特征空间中的一个网格单元。该图像定义了2D特征空间上的网格。然后使用分类器对图像的像素进行分类，该分类器将为每个网格单元分配一个类别标签。然后，分类后的图像将用作散点图的背景，散点图将显示每个类别的数据点。</p><p id="5134" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">优点:它对2D特征空间中的网格点进行分类。</p><p id="8e74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">缺点:制作非常精细的决策边界图的计算成本，因为我们必须使网格越来越精细。</p><p id="4988" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大蟒</p><pre class="kw kx ky kz fd li lj lk ll aw lm bi"><span id="761a" class="ln js hi lj b fi lo lp l lq lr"># Display plots inline and change default figure size</span><span id="07c7" class="ln js hi lj b fi ls lp l lq lr">%matplotlib inline</span><span id="8d70" class="ln js hi lj b fi ls lp l lq lr">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</span><span id="2be7" class="ln js hi lj b fi ls lp l lq lr"># Train the logistic rgeression classifier</span><span id="2d80" class="ln js hi lj b fi ls lp l lq lr">clf = sklearn.linear_model.LogisticRegressionCV()</span><span id="1845" class="ln js hi lj b fi ls lp l lq lr">clf.fit(X, y)</span><span id="7ab3" class="ln js hi lj b fi ls lp l lq lr">​</span><span id="ea55" class="ln js hi lj b fi ls lp l lq lr"># Helper function to plot a decision boundary.</span><span id="3f41" class="ln js hi lj b fi ls lp l lq lr">def plot_decision_boundary(pred_func):</span><span id="4bb6" class="ln js hi lj b fi ls lp l lq lr"># Set min and max values and give it some padding</span><span id="3385" class="ln js hi lj b fi ls lp l lq lr">x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5</span><span id="76e1" class="ln js hi lj b fi ls lp l lq lr">y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5</span><span id="6157" class="ln js hi lj b fi ls lp l lq lr">h = 0.01</span><span id="5907" class="ln js hi lj b fi ls lp l lq lr"># Generate a grid of points with distance h between them</span><span id="6d49" class="ln js hi lj b fi ls lp l lq lr">xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><span id="3fb3" class="ln js hi lj b fi ls lp l lq lr"># Predict the function value for the whole gid</span><span id="73f5" class="ln js hi lj b fi ls lp l lq lr">Z = pred_func(np.c_[xx.ravel(), yy.ravel()])</span><span id="6890" class="ln js hi lj b fi ls lp l lq lr">Z = Z.reshape(xx.shape)</span><span id="550a" class="ln js hi lj b fi ls lp l lq lr"># Plot the contour and training examples</span><span id="a98d" class="ln js hi lj b fi ls lp l lq lr">plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><span id="a597" class="ln js hi lj b fi ls lp l lq lr">plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral</span><span id="28f2" class="ln js hi lj b fi ls lp l lq lr"># Plot the decision boundary</span><span id="c241" class="ln js hi lj b fi ls lp l lq lr">plot_decision_boundary(lambda x: clf.predict(x))</span><span id="15b1" class="ln js hi lj b fi ls lp l lq lr">plt.title("Logistic Regression")</span></pre><p id="8004" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在逻辑回归中，决策边界是一条线性线，它将A类和B类分开。来自A类的一些点也进入了B类的区域，因为在线性模型中，很难获得分隔这两个类的精确边界线。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lw"><img src="../Images/f6e9807bb6b2d86a00b2cd6b852792f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qJlZO-eVvJDCiOHk"/></div></div></figure><p id="07a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤7:建立随机森林模型，并绘制决策边界。作为一个基于树的模型，它有许多树，并且这个情节试图捕捉所有相关的类。它是一个非线性分类器。</p><p id="9b9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大蟒</p><pre class="kw kx ky kz fd li lj lk ll aw lm bi"><span id="2886" class="ln js hi lj b fi lo lp l lq lr"># Display plots inline and change default figure size</span><span id="1663" class="ln js hi lj b fi ls lp l lq lr">%matplotlib inline</span><span id="18e3" class="ln js hi lj b fi ls lp l lq lr">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</span><span id="a454" class="ln js hi lj b fi ls lp l lq lr"># Train the RandomForestClassifier</span><span id="9a22" class="ln js hi lj b fi ls lp l lq lr">clf1 = RandomForestClassifier(random_state=1, n_estimators=100)</span><span id="f2fa" class="ln js hi lj b fi ls lp l lq lr">clf1.fit(X, y)</span><span id="863b" class="ln js hi lj b fi ls lp l lq lr"># Plot the decision boundary</span><span id="96b5" class="ln js hi lj b fi ls lp l lq lr">plot_decision_boundary(lambda x: clf1.predict(x))</span><span id="ea24" class="ln js hi lj b fi ls lp l lq lr">plt.title("Random Forest")</span></pre><p id="ab31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树和随机森林的决策面非常复杂。决策树是迄今为止最敏感的，仅显示受单点严重影响的极端分类概率。随机森林的敏感度较低，孤立点的极端分类概率要低得多。SVM是最不敏感的，因为它有一个非常平滑的决策边界。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lx"><img src="../Images/d3ec881446da0fbb591d4d35e7c925d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*78n5Dex_lQUjmaND"/></div></div></figure><p id="e845" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第八步:建立支持向量机模型，绘制决策边界</p><p id="bd4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大蟒</p><pre class="kw kx ky kz fd li lj lk ll aw lm bi"><span id="b2f7" class="ln js hi lj b fi lo lp l lq lr"># Display plots inline and change default figure size</span><span id="f260" class="ln js hi lj b fi ls lp l lq lr">%matplotlib inline</span><span id="6b75" class="ln js hi lj b fi ls lp l lq lr">from sklearn.svm import SVC</span><span id="8f5d" class="ln js hi lj b fi ls lp l lq lr">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</span><span id="0629" class="ln js hi lj b fi ls lp l lq lr"># Train the Support Vector Machine classifier</span><span id="8f0b" class="ln js hi lj b fi ls lp l lq lr">clf3 = SVC(gamma='auto')</span><span id="9dc8" class="ln js hi lj b fi ls lp l lq lr">clf3.fit(X, y)</span><span id="cb65" class="ln js hi lj b fi ls lp l lq lr"># Plot the decision boundary</span><span id="5b9f" class="ln js hi lj b fi ls lp l lq lr">plot_decision_boundary(lambda x: clf3.predict(x))</span><span id="1a7d" class="ln js hi lj b fi ls lp l lq lr">plt.title("Support Vector Machine")</span></pre><p id="b936" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">支持向量机寻找一个超平面，该超平面将特征空间分成具有最大间隔的两类。如果问题最初不是线性可分的，则使用核技巧，通过增加维数，将它变成线性可分的问题。这样，一个小维空间中的一般超曲面就变成了一个更高维空间中的超平面。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es ly"><img src="../Images/76bd433b598662eed2347054a9ca61ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CQXWzCM3uLFArwhL"/></div></div></figure><p id="89ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤9:建立决策树模型并绘制决策边界</p><p id="4d0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大蟒</p><pre class="kw kx ky kz fd li lj lk ll aw lm bi"><span id="bb58" class="ln js hi lj b fi lo lp l lq lr"># Display plots inline and change default figure size</span><span id="5d71" class="ln js hi lj b fi ls lp l lq lr">%matplotlib inline</span><span id="7928" class="ln js hi lj b fi ls lp l lq lr">from sklearn.tree import DecisionTreeClassifier</span><span id="b6ce" class="ln js hi lj b fi ls lp l lq lr">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</span><span id="2492" class="ln js hi lj b fi ls lp l lq lr">​</span><span id="8f55" class="ln js hi lj b fi ls lp l lq lr"># Train the Decision Tree classifier</span><span id="5446" class="ln js hi lj b fi ls lp l lq lr">clf4 = DecisionTreeClassifier()</span><span id="b1c1" class="ln js hi lj b fi ls lp l lq lr">clf4.fit(X, y)</span><span id="e913" class="ln js hi lj b fi ls lp l lq lr">​</span><span id="eb6b" class="ln js hi lj b fi ls lp l lq lr"># Plot the decision boundary</span><span id="1afa" class="ln js hi lj b fi ls lp l lq lr">plot_decision_boundary(lambda x: clf4.predict(x))</span><span id="6911" class="ln js hi lj b fi ls lp l lq lr">plt.title("Decision Tree")</span></pre><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lz"><img src="../Images/87911a25fb21f2d60199bd5bec1fb2ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ksit05P9E78lo3Ri"/></div></div></figure><p id="82e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤10:建立高斯朴素贝叶斯模型并绘制决策边界</p><p id="e4ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大蟒</p><pre class="kw kx ky kz fd li lj lk ll aw lm bi"><span id="8c58" class="ln js hi lj b fi lo lp l lq lr"># Display plots inline and change default figure size</span><span id="3776" class="ln js hi lj b fi ls lp l lq lr">%matplotlib inline</span><span id="0866" class="ln js hi lj b fi ls lp l lq lr">from sklearn.naive_bayes import GaussianNB</span><span id="f72c" class="ln js hi lj b fi ls lp l lq lr">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</span><span id="9482" class="ln js hi lj b fi ls lp l lq lr"># Train the Gaussian NaiveBayes classifier</span><span id="4195" class="ln js hi lj b fi ls lp l lq lr">clf5 = GaussianNB()</span><span id="f76d" class="ln js hi lj b fi ls lp l lq lr">clf5.fit(X, y)</span><span id="ba7c" class="ln js hi lj b fi ls lp l lq lr">​</span><span id="b94f" class="ln js hi lj b fi ls lp l lq lr"># Plot the decision boundary</span><span id="a831" class="ln js hi lj b fi ls lp l lq lr">plot_decision_boundary(lambda x: clf5.predict(x))</span><span id="d6cb" class="ln js hi lj b fi ls lp l lq lr">plt.title("GaussianNB Classifier")</span></pre><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es ma"><img src="../Images/2b9ce14ee82ba9cd72311650207cc85e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wy2IyFRzWJIFgCno"/></div></div></figure><p id="d581" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">高斯朴素贝叶斯也表现很好，具有平滑的曲线边界线。</p><h1 id="bc76" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">高维数据的决策边界</h1><p id="1957" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">对于2D和3D数据集，决策边界很容易可视化。超越3D的概括在可视化方面形成了挑战，其中我们必须将多维度中存在的边界转换到较低的维度，这可以被专家显示和理解是困难的。</p><p id="f1ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，可以使用tSNE绘制决策边界，其中数据的维度可以分几步降低。例如:如果我的数据的维度是150，那么首先应该减少到50，然后应该减少到2个维度。</p><p id="0781" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">来自sklearn.manifold的库TSNE和来自sklearn.decomposition的TruncatedSVD用于此目的。</p><p id="f124" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里发表了一篇非常好的研究论文<a class="ae lh" href="https://www.researchgate.net/publication/271658381_Visualizing_multi-dimensional_decision_boundaries_in_2D" rel="noopener ugc nofollow" target="_blank">，描述了为高维数据绘制决策边界。</a></p><h1 id="d74c" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">结论:</h1><p id="139a" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在本文中，我们学习了决策边界在确定分类器模型中的作用，构建了几个分类器模型并绘制了它们各自的决策边界以选择最佳模型，还知道绘制高维数据的决策边界是一项复杂的任务，可以使用tSNE绘制，其中数据的维度可以通过几个步骤来降低。</p><p id="de93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，你的下一步是什么？请提出几个关于为高维数据绘制决策边界的建议方法的要点，如在这里发表的<a class="ae lh" href="https://www.researchgate.net/publication/271658381_Visualizing_multi-dimensional_decision_boundaries_in_2D" rel="noopener ugc nofollow" target="_blank">的研究论文中所发现的。</a></p><p id="af1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的下一篇文章中再见…在那之前请保持关注，祝学习愉快！</p></div></div>    
</body>
</html>