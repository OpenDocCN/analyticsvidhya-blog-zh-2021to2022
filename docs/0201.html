<html>
<head>
<title>Blog Post Explained- Creating Images from Text using DALL·E</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">博文解释——使用DALL E从文本创建图像</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/blog-post-explained-creating-images-from-text-using-dall-e-4613376bbf10?source=collection_archive---------6-----------------------#2021-01-08">https://medium.com/analytics-vidhya/blog-post-explained-creating-images-from-text-using-dall-e-4613376bbf10?source=collection_archive---------6-----------------------#2021-01-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b7a6ab1388e4ccd4357f791cbc9f54d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XDv0tZRUbiB8p4wcbyd6JQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">插图截图摘自<a class="ae iu" href="https://openai.com/blog/dall-e/" rel="noopener ugc nofollow" target="_blank"> Open AIs的官方博客帖子</a>。访问网站，了解更多疯狂的事情！！！</figcaption></figure><h1 id="e63a" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">简介和概述</h1><p id="28dc" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">老实说，我的心脏刚刚漏跳了一拍，这个神经网络的力量真是难以置信的好。这些图像太酷了，这里的重点是这些图像不是PS的或人类创造的，它们是由人工智能生成的，这种新模型叫做<strong class="jv hj"> DALL E </strong>。它可以做的是，它可以获取一段文本，然后输出一张与该文本匹配的图片。<strong class="jv hj">最令人震惊的是这些图像的质量，更令人震惊的是这款机型拥有的功能范围</strong>。DALL E是一个单一的模型，它可以获取一段文本和一部分图像，也可以不获取图像，它将输出图像，或者继续您已经给定的图像的一部分，或者自己生成图像。</p><p id="5689" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"> DALL E是</strong> <a class="ae iu" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj"> GPT-3 </strong> </a>的120亿参数版本，被训练成从文本描述生成图像，<strong class="jv hj">使用文本-图像对的数据集</strong>。作者没有透露他们是如何得到/制作这个数据集的。作者发现它具有多种多样的功能，包括创建拟人化版本的动物和物体，以看似合理的方式组合不相关的概念，渲染文本，以及对现有图像进行转换。</p><h1 id="afbe" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">与GPT-3的比较</h1><p id="4dc1" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">GPT-3是开放人工智能著名的文本生成模型。我对GPT-3的假设是，它巧妙地混合了训练数据，而不是记住训练数据，同样的假设对DALL E也有点重复。像GPT-3一样，DALL E是一个转换语言模型。它将文本和图像作为包含多达1280个标记的单个数据流接收，并使用最大似然估计进行训练，以一个接一个地生成所有标记。这个训练过程不仅允许DALL E从头生成一个图像，还允许它重新生成一个现有图像中延伸到右下角的任何矩形区域，这种方式与文本提示是一样的。</p><p id="6740" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">为了理解这个模型是如何工作的，我们需要稍微回过头来看看它所借鉴的模型，即<strong class="jv hj">矢量量化-变分自动编码器(VQ-VAE) </strong>和<strong class="jv hj"> GPT-3。</strong></p><h1 id="86c1" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">VQ-VAE</h1><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/36b2bf4116d8381df5ac5cd3ac0be51d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XFJ2gXfwPi5G0umXeWMEOw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">VQ-VAE建筑模型。最右边的部分是最近邻计算。该快照摘自这篇<a class="ae iu" href="https://arxiv.org/pdf/1711.00937.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的第4页。</figcaption></figure><p id="2876" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">VQ-VAE模型获取图像，将其编码到潜在空间(VQ)，再次将其重建/解码为同一图像(VAE)，然后中间的任何东西都是该图像的可感知潜在表示。如果你能训练这个模型，你会在中间得到某种描述图像的表示，否则，你就不能再现图像。事实证明，经典的自动编码器工作不太好，但这个模型的工作相当<strong class="jv hj">可怕。</strong>因此，我们将在两者之间有一个嵌入空间，也称为码本(词汇或潜在编码)。总的来说，码本向量包含8192个向量。</p><p id="c44c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这里要注意的重要一点是，模型不能只输出任何潜在编码，编码器输出是连续向量，但这些连续向量应该是潜在编码本身中存在的向量之一。如果模型编码的矢量<strong class="jv hj">不在潜在空间的</strong>矢量中，这是不可避免的(因为神经网络中存在随机性)，模型将在我们的码本(词汇)中找到最近的一个，并选择最近的矢量作为该矢量的编码。因此编码器只能命中这些码本向量中的一个。</p><p id="926a" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">然后，该模型将这些码本向量馈送到解码器，解码器只从这些码本向量中解码这些向量，这比简单地连续执行自动编码器要好得多。想象一下，这个密码本词汇有点像图像描述的词汇</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lb"><img src="../Images/4d804f2e8ddfc70ee3043a9c34e38adb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bv40K-IjlfIcYwBnoZjHbw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">VQ-VAE的复杂概述，即VQ-VAE-2。它类似于VQ-VAE，但是在VQ-VAE-2中，编码器-解码器机制是多级机制。底层代表高分辨率图像，顶层代表低分辨率图像。这张快照摘自这篇<a class="ae iu" href="https://arxiv.org/pdf/1906.00446.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的第4页。</figcaption></figure><p id="3a18" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">VQ-VAE算法是这样的，你拍摄一张图像，把图像分割成32×32大小的网格(没有像素那么精细，相当大)。<strong class="jv hj">每幅图像由1024个记号(32×32) </strong>描述，然后你做一个编码器，使得当这个网格通过编码器时，图像的低分辨率描述中的每个块对应一个码本向量，每个区域由码本向量本身描述。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lc"><img src="../Images/a3f11c270687a42f33fc9d4d649a2c15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xioS1D1FNg3WsiMQ-rzFKw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">从具有三个潜在地图(上、中、下)的分级VQ-VAE重建。最右边的图像是原始图像。每个潜在图都为重建增加了额外的细节。这些潜像分别比原始图像小大约3072倍、768倍、192倍。这张快照摘自VQ-瓦埃2 <a class="ae iu" href="https://arxiv.org/pdf/1906.00446.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的第4页。</figcaption></figure><p id="8442" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">有8192个码本向量，而图像仅由1024个记号组成，因此同一记号可能不会被重复使用。图像的低分辨率描述实际上不是下采样图像，它是一种描述，因为码本向量本身包含大量信息。编码器和解码器都用直通估计器同时训练，因为最近邻计算不是精确可微的，它们还训练码本以匹配编码器的输出，这就是VQ-VAE。</p><h1 id="f04e" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">VQ-VAE与GPT-3</h1><p id="0c20" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">令牌是离散词汇表中的任何符号；对人类来说，每个英文字母都是26个字母字母表中的一个标记。DALL E的词汇表既有文本概念也有图像概念。具体地说，每个图像标题用最大256字节成对编码的标记表示，词汇大小为16384，图像用1024个标记表示，词汇大小为8192。在训练期间，图像被预处理为256x256分辨率。与VQVAE类似，每个图像都使用离散VAE压缩为32×32的离散潜在代码网格，我们使用连续松弛对其进行了预训练。作者发现，使用这种放松模式避免了对大词汇量的需求。那么，持续放松会是什么样子呢？</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/dca0999be8b84cdfdb1d693907521eb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hnXECGt7xXY6tIc3ADemEg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">采样图的可视化。白色运算是确定性的，蓝色是随机的，四舍五入是连续的，平方是离散的。顶部节点是示例状态；亮度表示[0，1]中的一个值。这张快照摘自这篇<a class="ae iu" href="https://arxiv.org/pdf/1611.00712.pdf" rel="noopener ugc nofollow" target="_blank">文章的第4页</a>。</figcaption></figure><p id="2285" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">如果你有一个操作，比如一个离散的随机变量，你需要对它取一个argmax，你会得到某种形式的逻辑，在取了argmax之后，你会得到一个趋向于正标度值的分布。这与我们在VQ-VAE中所做的操作相同，我们将编码器的每个输出分配给最近的码本向量。VQ-VAE没有将编码器输出硬分配给码本向量，而是软分配。这有点类似于K近邻和高斯朴素贝叶斯算法之间的区别。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/912ea2c8a30af7e9ddb69416d6ce1cc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rm27YznByWnOzNsyfFnrFA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">我关于DALL E的假设，快照摘自<a class="ae iu" href="https://openai.com/blog/dall-e" rel="noopener ugc nofollow" target="_blank">的博文</a>。</figcaption></figure><p id="6319" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这里要注意的有趣的事情是，模型输出不是精心挑选的。所以他们在这里有一个小技巧，模型只输出512张图像，然后使用Open AI发布的一个<a class="ae iu" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj"> CLIP </strong> </a> <strong class="jv hj"> </strong>模型对这些图像进行重新排序。这个剪辑模型是一个非常好的重排序器，你给它一段文字和一个图像，这个模型会告诉你它们有多适合在一起。所以你在<a class="ae iu" href="https://openai.com/blog/dall-e/#summary" rel="noopener ugc nofollow" target="_blank">博客</a>帖子上看到的输出绝对是该模型的最佳输出。它不是由人类精心挑选的，而是由一个非常优秀的模特精心挑选的。第二点是，文字提示绝对是精挑细选的。这清楚地表明，该模型在如何准确表达文本提示方面非常脆弱，我们已经从GPT-3中了解到这一点，输入可能对人类来说是相同的，只是在某些情况下表达不同，然而该模型将输出完全不同的东西。很多GPT -3的例子都是根据输入提示构建的。这个模型可以很好地表现颜色、形状和纹理，但是它不擅长计数。样式和纹理是这些基于图像的模型的领域。</p><h1 id="3fbe" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">摘要</h1><p id="de33" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">架构和训练程序将在下一篇文章中详细介绍。本文只是对DALL E如何工作以及它使用什么基本概念来获得这样的SOTA性能的概述。DALL E是一个简单的仅支持解码器的转换器，它将文本和图像作为一个包含1280个标记的流接收，其中256个标记用于文本，1024个标记用于图像，并对所有标记进行自回归建模。在它的64个自我关注层的每一层的关注遮罩允许每个图像标记关注所有文本标记。DALL E对文本标记使用标准因果掩码，对图像标记使用稀疏注意，根据层的不同，使用行、列或卷积注意模式。</p><h1 id="f49f" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">参考</h1><ol class=""><li id="dfd3" class="lf lg hi jv b jw jx ka kb ke lh ki li km lj kq lk ll lm ln bi translated"><a class="ae iu" href="https://arxiv.org/pdf/1711.00937.pdf" rel="noopener ugc nofollow" target="_blank">神经离散表示学习(VQ-VAE)。</a></li><li id="d514" class="lf lg hi jv b jw lo ka lp ke lq ki lr km ls kq lk ll lm ln bi translated"><a class="ae iu" href="https://arxiv.org/pdf/1906.00446.pdf" rel="noopener ugc nofollow" target="_blank">用VQ-VAE-2生成多样的高保真图像。</a></li><li id="8197" class="lf lg hi jv b jw lo ka lp ke lq ki lr km ls kq lk ll lm ln bi translated"><a class="ae iu" href="https://arxiv.org/pdf/1611.00712.pdf" rel="noopener ugc nofollow" target="_blank">具体分布:离散随机变量的连续松弛。</a></li><li id="7397" class="lf lg hi jv b jw lo ka lp ke lq ki lr km ls kq lk ll lm ln bi translated"><a class="ae iu" href="https://openai.com/blog/dall-e/" rel="noopener ugc nofollow" target="_blank">打开艾的DALL E博文。</a></li><li id="1b96" class="lf lg hi jv b jw lo ka lp ke lq ki lr km ls kq lk ll lm ln bi translated"><a class="ae iu" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank">打开艾的剪辑博文。</a></li></ol><p id="2a0c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">如果你喜欢这个帖子，请一定要鼓掌👏。💬连接？再来说社交:<a class="ae iu" href="http://myurls.co/nakshatrasinghh" rel="noopener ugc nofollow" target="_blank"><strong class="jv hj"/></a><strong class="jv hj">。</strong></p></div></div>    
</body>
</html>