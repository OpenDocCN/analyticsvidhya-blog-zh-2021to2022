<html>
<head>
<title>Multi-Agent Reinforcement Learning: OpenAI’s MADDPG</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多智能体强化学习:OpenAI的MADDPG</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multi-agent-reinforcement-learning-openais-maddpg-a741c6cf3869?source=collection_archive---------10-----------------------#2021-05-12">https://medium.com/analytics-vidhya/multi-agent-reinforcement-learning-openais-maddpg-a741c6cf3869?source=collection_archive---------10-----------------------#2021-05-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="7081" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">探索OpeanAI的MADDPG算法解决多智能体环境问题。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/c4493f6be00757ad4f8cbef40ec49153.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/0*EB1hBAn6EBR8MZ-H"/></div></figure><p id="0ff6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者安东尼奥·利斯</p><h1 id="3b2c" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">介绍</h1><p id="e8e1" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">大家好，在通过经典的深度学习应用程序进行了一点有趣的练习后，我们将回到解决强化学习环境。</p><p id="c99a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">今天我们将使用OpenAI的MADDPG算法来解决一个有多个代理的环境。这将是不同的，同时与我们目前所看到的相似。对于多个代理，我们增加了复杂性和计算要求，但是，正如我们将看到的，我们将使用我们所学的知识来解决单代理环境。</p><p id="715b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">和往常一样，我们将使用Tensorflow 2从头开始实现一切。但是考虑到MADDPG是DDPG的多代理版本，我们将在帖子中使用大量关于<a class="ae kn" href="https://antonai.blog/?p=260#more-260" rel="noopener ugc nofollow" target="_blank"> DDPG </a>的代码。所以如果你没有读过，我建议你先读一读。</p><h1 id="064d" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">多代理环境</h1><p id="a3ad" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">强化学习领域的大多数发展都是在单代理领域。但是有许多重要的用例，其中多个代理同时学习协调它们的动作。</p><p id="745d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在多智能体环境中，由于智能体的决策策略不断变化，存在一个主要的挑战，即从任何单个智能体的角度来看，环境的非平稳性。</p><p id="a1e6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">标准的RL算法并不真正适合多代理环境。研究人员提出了不同的方法来处理这种非平稳性，可以分为集中式和分散式架构。</p><h1 id="2ea0" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">集中式和分散式架构</h1><p id="e897" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">在分散式学习中，代理人彼此分开训练。每个代理都使用本地观察作为输入来训练其策略网络。为了处理非平稳性，通常使用某种形式的自我游戏，其中代理与他们当前或以前的版本进行游戏，以学习最佳策略。</p><p id="ffa3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在集中学习中，代理以某种方式集体建模。<a class="ae kn" href="https://arxiv.org/pdf/1706.02275.pdf" rel="noopener ugc nofollow" target="_blank"> Lowe et al. (2017) </a>提出了MADDPG，一种使用确定性策略梯度算法的多代理集中式架构。该算法基于行动者-批评家体系结构，有一个集中的批评家和一个分散的行动者。集中的评论家建议如何通过查看全局信息来更新行动者策略。由于每个代理都可以通过评论家访问所有其他代理的观察和动作，所以策略梯度估计以其他代理的策略为条件，因此解决了非平稳性。在测试过程中，集中的批评家被移除，只留下参与者、他们的策略和本地观察。</p><h1 id="f308" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">MADDPG</h1><p id="d2be" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">来自OpenAI、加州大学伯克利分校和麦吉尔大学的研究人员在Lowe等人(2017) 的论文<a class="ae kn" href="https://arxiv.org/pdf/1706.02275.pdf" rel="noopener ugc nofollow" target="_blank">中提出了MADDPG。MADDPG是基于行动者-批评家框架的深度确定性策略梯度算法(DDPG)的多代理对等体。</a></p><p id="9b9b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在DDPG，我们只有一个代理人。在这里，我们有多个代理人，他们有自己的演员和评论家网络。actor-network将代理的本地观察作为输入，并输出推荐的动作，因此与DDPG actor-network相比并无新意。相反，批评家网络将环境的全局观察作为输入，因此它不仅将环境的状态和代理的动作作为输入，还将局部观察和其他代理的动作作为输入。critic网络的输出仍然是q值估计。所以评论家网络只在训练部分使用，给演员一个全局视野，以适应全局环境和其他代理的行动。通过这种方式，参与者可以学习合作或竞争策略。此外，由于每个智能体的训练是以所有其他智能体的观察和动作为条件的，所以每个智能体都将环境视为静止的。</p><p id="e106" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">像在DDPG一样，为了提高学习稳定性，我们也将为每个代理使用一个目标行动者和评论家网络，具有相同的软更新机制。像在DDPG一样，我们也将使用一些噪音来改善探索。与我们在<a class="ae kn" href="https://antonai.blog/?p=260#more-260" rel="noopener ugc nofollow" target="_blank"> DDPG </a>的帖子中实现的不同，我们不打算实现一个奥恩斯坦-乌伦贝克过程，但是我们将使用一个简单的随机噪声。</p><h1 id="6615" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">多智能体粒子环境</h1><p id="86cd" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">我们将解决论文中使用的相同环境<a class="ae kn" href="https://arxiv.org/pdf/1706.02275.pdf" rel="noopener ugc nofollow" target="_blank">混合合作-竞争环境</a>的多主体行动者-批评家。你可以在这里找到python包<a class="ae kn" href="https://github.com/openai/multiagent-particle-envs" rel="noopener ugc nofollow" target="_blank">，里面有关于如何安装和使用它的所有说明。</a></p><p id="827b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以开始探索一个简单的环境来理解库是如何工作的:</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="42ec" class="kt jl hh kp b fi ku kv l kw kx"><em class="ky">def</em> make_env(scenario_name, benchmark=False):<br/>    '''<br/>    Creates a MultiAgentEnv object as env. This can be used similar to a gym<br/>    environment by calling env.reset() and env.step().<br/>    Use env.render() to view the environment on the screen.<br/>    Input:<br/>        scenario_name   :   name of the scenario from ./scenarios/ to be Returns<br/>                            (without the .py extension)<br/>        benchmark       :   whether you want to produce benchmarking data<br/>                            (usually only done during evaluation)<br/>    Some useful env properties (see environment.py):<br/>        .observation_space  :   Returns the observation space for each agent<br/>        .action_space       :   Returns the action space for each agent<br/>        .n                  :   Returns the number of Agents<br/>    '''<br/>    from multiagent.environment import MultiAgentEnv<br/>    import multiagent.scenarios as scenarios<br/><br/>    # load scenario from script<br/>    scenario = scenarios.load(scenario_name + ".py").Scenario()<br/>    # create world<br/>    world = scenario.make_world()<br/>    # create multiagent environment<br/>    if benchmark:        <br/>        env = MultiAgentEnv(world, scenario.reset_world,     scenario.reward, scenario.observation, scenario.benchmark_data)<br/>    else:<br/>        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation)<br/>    return env<br/><br/><br/>env = make_env("simple_adversary")<br/>print(env.action_space)</span></pre><p id="cada" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ky">【离散(5)，离散(5)，离散(5)】</em></p><p id="ade4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用了make_env函数，您可以在库的Github库中找到它。如您所见，在简单的对手环境中，我们有三个动作空间，因此我们有三个代理需要输出长度为5的动作向量。我们还可以看到三个代理的输入:</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="31eb" class="kt jl hh kp b fi ku kv l kw kx">print(env.observation_space)</span></pre><p id="8dc3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ky">【Box(-inf，inf，(8，)float32)，Box(-inf，inf，(10，)float32)，Box(-inf，INF，(10，)float 32)】</em></p><p id="8ca9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，对于第一个代理，我们有一个(8，0)连续输入向量，对于另外两个代理，我们有一个(10，)连续输入向量。</p><p id="b86a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将使用简单的对手作为开发环境，然后我们也将解决简单的标签，以查看我们的算法如何执行解决不同的环境。</p><p id="5be8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从文档中:</p><ul class=""><li id="24c6" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb le lf lg lh bi translated">简单对手:1个对手(红色)，N个好特工(绿色)，N个地标(通常N=2)。所有特工观察路标和其他特工的位置。一个标志是“目标标志”(绿色)。优秀的特工根据他们中的一个与目标地标的接近程度得到奖励，但是如果对手接近目标地标，则得到负面奖励。对手的奖励基于它与目标的距离，但它不知道哪个地标是目标地标。所以优秀的特工必须学会“分头行动”,覆盖所有的地标来欺骗对手。</li><li id="56ed" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">简单的标签:捕食者-猎物环境。优秀的特工(绿色)速度更快，想要避免被对手击中(红色)。对手比较慢，想打好代理。障碍物(黑色大圆圈)挡住了去路。</li></ul><h1 id="c446" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">重放缓冲器</h1><p id="fb8e" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">我们从实现重放缓冲区开始，在这里我们保存状态、代理的动作、奖励等等。该代码与为DDPG实现的代码非常相似，但我们需要考虑到我们有多个代理的事实，所以有点不同:</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="09e9" class="kt jl hh kp b fi ku kv l kw kx"><em class="ky">class</em> ReplayBuffer():<br/>    <em class="ky">def</em> __init__(self, env, buffer_capacity=BUFFER_CAPACITY, batch_size=BATCH_SIZE, min_size_buffer=MIN_SIZE_BUFFER):<br/>        self.buffer_capacity = buffer_capacity<br/>        self.batch_size = batch_size<br/>        self.min_size_buffer = min_size_buffer<br/>        self.buffer_counter = 0<br/>        self.n_games = 0<br/>        self.n_agents = env.n<br/>        self.list_actors_dimension = [env.observation_space[index].shape[0] for index in <em class="ky">range</em>(self.n_agents)]<br/>        self.critic_dimension = <em class="ky">sum</em>(self.list_actors_dimension)        <br/>        self.list_actor_n_actions = [env.action_space[index].n for index in <em class="ky">range</em>(self.n_agents)]<br/>        <br/>        self.states = np.zeros((self.buffer_capacity, self.critic_dimension))<br/>        self.rewards = np.zeros((self.buffer_capacity, self.n_agents))<br/>        self.next_states = np.zeros((self.buffer_capacity, self.critic_dimension))<br/>        self.dones = np.zeros((self.buffer_capacity, self.n_agents), dtype=<em class="ky">bool</em>)<br/><br/>        self.list_actors_states = []<br/>        self.list_actors_next_states = []<br/>        self.list_actors_actions = []<br/>        <br/>        for n in <em class="ky">range</em>(self.n_agents):<br/>            self.list_actors_states.append(np.zeros((self.buffer_capacity, self.list_actors_dimension[n])))<br/>            self.list_actors_next_states.append(np.zeros((self.buffer_capacity, self.list_actors_dimension[n])))<br/>            self.list_actors_actions.append(np.zeros((self.buffer_capacity, self.list_actor_n_actions[n])))</span></pre><p id="46c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们正在初始化NumPy向量列表来存储多个代理的所有状态和动作，同时我们使用单个NumPy向量来存储全局状态和奖励。</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="2818" class="kt jl hh kp b fi ku kv l kw kx"><em class="ky">    def</em> __len__(self):<br/>        return self.buffer_counter<br/>        <br/>    <em class="ky">def</em> check_buffer_size(self):<br/>        return self.buffer_counter &gt;= self.batch_size and self.buffer_counter &gt;= self.min_size_buffer<br/>    <br/>    <em class="ky">def</em> update_n_games(self):<br/>        self.n_games += 1<br/>          <br/>    <em class="ky">def</em> add_record(self, actor_states, actor_next_states, actions, state, next_state, reward, done):<br/>        <br/>        index = self.buffer_counter % self.buffer_capacity<br/><br/>        for agent_index in <em class="ky">range</em>(self.n_agents):<br/>            self.list_actors_states[agent_index][index] = actor_states[agent_index]<br/>            self.list_actors_next_states[agent_index][index] = actor_next_states[agent_index]<br/>            self.list_actors_actions[agent_index][index] = actions[agent_index]<br/><br/>        self.states[index] = state<br/>        self.next_states[index] = next_state<br/>        self.rewards[index] = reward<br/>        self.dones[index] = done<br/>            <br/>        self.buffer_counter += 1</span></pre><p id="c10a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">add_record函数需要考虑到我们有多个代理的事实，所以我们使用for-cycle来更新之前初始化的列表中的状态和动作。在多智能体粒子环境中，智能体由数字标识，所以我们可以只使用列表和索引。在其他多代理环境中，每个代理都有一个名字，在这种情况下，我们应该使用一些字典，而不是使用名字作为关键字的列表。</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="5820" class="kt jl hh kp b fi ku kv l kw kx"><em class="ky">    def</em> get_minibatch(self):<br/>        # If the counter is less than the capacity we don't want to take zeros records, <br/>        # if the cunter is higher we don't access the record using the counter <br/>        # because older records are deleted to make space for new one<br/>        buffer_range = <em class="ky">min</em>(self.buffer_counter, self.buffer_capacity)<br/><br/>        batch_index = np.random.choice(buffer_range, self.batch_size, replace=False)<br/><br/>        # Take indices<br/>        state = self.states[batch_index]<br/>        reward = self.rewards[batch_index]<br/>        next_state = self.next_states[batch_index]<br/>        done = self.dones[batch_index]<br/>            <br/>        actors_state = [self.list_actors_states[index][batch_index] for index in <em class="ky">range</em>(self.n_agents)]<br/>        actors_next_state = [self.list_actors_next_states[index][batch_index] for index in <em class="ky">range</em>(self.n_agents)]<br/>        actors_action = [self.list_actors_actions[index][batch_index] for index in <em class="ky">range</em>(self.n_agents)]<br/><br/>        return state, reward, next_state, done, actors_state, actors_next_state, actors_action<br/>    <br/>    <em class="ky">def</em> save(self, folder_path):<br/>        """<br/>        Save the replay buffer<br/>        """<br/>        if not os.path.isdir(folder_path):<br/>            os.mkdir(folder_path)<br/>        <br/>        np.save(folder_path + '/states.npy', self.states)<br/>        np.save(folder_path + '/rewards.npy', self.rewards)<br/>        np.save(folder_path + '/next_states.npy', self.next_states)<br/>        np.save(folder_path + '/dones.npy', self.dones)<br/>        <br/>        for index in <em class="ky">range</em>(self.n_agents):<br/>            np.save(folder_path + '/states_actor_{}.npy'.<em class="ky">format</em>(index), self.list_actors_states[index])<br/>            np.save(folder_path + '/next_states_actor_{}.npy'.<em class="ky">format</em>(index), self.list_actors_next_states[index])<br/>            np.save(folder_path + '/actions_actor_{}.npy'.<em class="ky">format</em>(index), self.list_actors_actions[index])<br/>            <br/>        dict_info = {"buffer_counter": self.buffer_counter, "n_games": self.n_games}<br/>        <br/>        with <em class="ky">open</em>(folder_path + '/dict_info.json', 'w') as f:<br/>            json.dump(dict_info, f)<br/>            <br/>    <em class="ky">def</em> load(self, folder_path):<br/>        self.states = np.load(folder_path + '/states.npy')<br/>        self.rewards = np.load(folder_path + '/rewards.npy')<br/>        self.next_states = np.load(folder_path + '/next_states.npy')<br/>        self.dones = np.load(folder_path + '/dones.npy')<br/>        <br/>        self.list_actors_states = [np.load(folder_path + '/states_actor_{}.npy'.<em class="ky">format</em>(index)) for index in <em class="ky">range</em>(self.n_agents)]<br/>        self.list_actors_next_states = [np.load(folder_path + '/next_states_actor_{}.npy'.<em class="ky">format</em>(index)) for index in <em class="ky">range</em>(self.n_agents)]<br/>        self.list_actors_actions = [np.load(folder_path + '/actions_actor_{}.npy'.<em class="ky">format</em>(index)) for index in <em class="ky">range</em>(self.n_agents)]<br/>        <br/>        with <em class="ky">open</em>(folder_path + '/dict_info.json', 'r') as f:<br/>            dict_info = json.load(f)<br/>        self.buffer_counter = dict_info["buffer_counter"]<br/>        self.n_games = dict_info["n_games"]</span></pre><p id="ff82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后我们有get_minibatch和save/load函数，它们非常类似于单代理版本，但是我们总是使用列表和索引来获取和保存/加载关于代理的观察结果。</p><h1 id="22de" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">网络</h1><p id="91f9" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">我们需要为每个代理实现一个Actor和一个Critic，这里我们可以使用为DDPG实现的相同代码，逻辑是相同的，但是Critic的状态输入与单代理中的Actor不同:</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="8f82" class="kt jl hh kp b fi ku kv l kw kx"><em class="ky">class</em> Critic(tf.keras.Model):<br/>    <em class="ky">def</em> __init__(self, name, hidden_0=CRITIC_HIDDEN_0, hidden_1=CRITIC_HIDDEN_1):<br/>            <br/>        <em class="ky">super</em>(Critic, self).__init__()<br/>        <br/>        self.hidden_0 = hidden_0<br/>        self.hidden_1 = hidden_1<br/><br/>        self.net_name = name<br/><br/>        self.dense_0 = Dense(self.hidden_0, activation='relu')<br/>        self.dense_1 = Dense(self.hidden_1, activation='relu')<br/>        self.q_value = Dense(1, activation=None)<br/>    <br/>    <em class="ky">def</em> call(self, state, actors_actions):<br/>        state_action_value = self.dense_0(tf.concat([state, actors_actions], axis=1)) # multiple actions<br/>        state_action_value = self.dense_1(state_action_value)<br/><br/>        q_value = self.q_value(state_action_value)<br/><br/>        return q_value<br/><br/><em class="ky">class</em> Actor(tf.keras.Model):<br/>    <em class="ky">def</em> __init__(self, name, actions_dim, hidden_0=ACTOR_HIDDEN_0, hidden_1=ACTOR_HIDDEN_1):<br/>        <em class="ky">super</em>(Actor, self).__init__()<br/>        self.hidden_0 = hidden_0<br/>        self.hidden_1 = hidden_1<br/>        self.actions_dim = actions_dim<br/>        <br/>        self.net_name = name<br/><br/>        self.dense_0 = Dense(self.hidden_0, activation='relu')<br/>        self.dense_1 = Dense(self.hidden_1, activation='relu')<br/>        self.policy = Dense(self.actions_dim, activation='sigmoid') # we want something beetween zero and one<br/><br/>    <em class="ky">def</em> call(self, state):<br/>        x = self.dense_0(state)<br/>        policy = self.dense_1(x)<br/>        policy = self.policy(policy)<br/>        return policy</span></pre><h1 id="6a22" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">代理人</h1><p id="cdc9" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">我们现在可以定义单个代理的逻辑。我们在这里不打算像对单个对手那样定义损失函数，因为我们需要对环境有一个全局的看法来为评论家定义损失函数:</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="0adc" class="kt jl hh kp b fi ku kv l kw kx"><em class="ky">class</em> Agent:<br/>    <em class="ky">def</em> __init__(self, env, n_agent, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR, gamma=GAMMA, tau=TAU):<br/>        <br/>        self.gamma = gamma<br/>        self.tau = tau<br/>        self.actor_lr = actor_lr<br/>        self.critic_lr = critic_lr<br/>        <br/>        self.actor_dims = env.observation_space[n_agent].shape[0]<br/>        self.n_actions = env.action_space[n_agent].n<br/>        <br/>        self.agent_name = "agent_number_{}".<em class="ky">format</em>(n_agent)<br/>        <br/>        self.actor = Actor("actor_" + self.agent_name, self.n_actions)<br/>        self.critic = Critic("critic_" + self.agent_name)<br/>        self.target_actor = Actor("target_actor_" + self.agent_name, self.n_actions)<br/>        self.target_critic = Critic("critic_" + self.agent_name)<br/>        <br/>        self.actor.<em class="ky">compile</em>(optimizer=opt.Adam(learning_rate=actor_lr))<br/>        self.critic.<em class="ky">compile</em>(optimizer=opt.Adam(learning_rate=critic_lr))<br/>        self.target_actor.<em class="ky">compile</em>(optimizer=opt.Adam(learning_rate=actor_lr))<br/>        self.target_critic.<em class="ky">compile</em>(optimizer=opt.Adam(learning_rate=critic_lr))<br/>        <br/>        actor_weights = self.actor.get_weights()<br/>        critic_weights = self.critic.get_weights()<br/>        <br/>        self.target_actor.set_weights(actor_weights)<br/>        self.target_critic.set_weights(critic_weights)<br/>        <br/>    <em class="ky">def</em> update_target_networks(self, tau):<br/>        actor_weights = self.actor.weights<br/>        target_actor_weights = self.target_actor.weights<br/>        for index in <em class="ky">range</em>(<em class="ky">len</em>(actor_weights)):<br/>            target_actor_weights[index] = tau * actor_weights[index] + (1 - tau) * target_actor_weights[index]<br/><br/>        self.target_actor.set_weights(target_actor_weights)<br/>        <br/>        critic_weights = self.critic.weights<br/>        target_critic_weights = self.target_critic.weights<br/>    <br/>        for index in <em class="ky">range</em>(<em class="ky">len</em>(critic_weights)):<br/>            target_critic_weights[index] = tau * critic_weights[index] + (1 - tau) * target_critic_weights[index]<br/><br/>        self.target_critic.set_weights(target_critic_weights)<br/>        <br/>    <em class="ky">def</em> get_actions(self, actor_states):<br/>        noise = tf.random.uniform(shape=[self.n_actions])<br/>        actions = self.actor(actor_states)<br/>        actions = actions + noise<br/><br/>        return actions.numpy()[0]<br/>    <br/>    <em class="ky">def</em> save(self, path_save):<br/>        self.actor.save_weights(f"{path_save}/{self.actor.net_name}.h5")<br/>        self.target_actor.save_weights(f"{path_save}/{self.target_actor.net_name}.h5")<br/>        self.critic.save_weights(f"{path_save}/{self.critic.net_name}.h5")<br/>        self.target_critic.save_weights(f"{path_save}/{self.target_critic.net_name}.h5")<br/>        <br/>    <em class="ky">def</em> load(self, path_load):<br/>        self.actor.load_weights(f"{path_load}/{self.actor.net_name}.h5")<br/>        self.target_actor.load_weights(f"{path_load}/{self.target_actor.net_name}.h5")<br/>        self.critic.load_weights(f"{path_load}/{self.critic.net_name}.h5")<br/>        self.target_critic.load_weights(f"{path_load}/{self.target_critic.net_name}.h5")</span></pre><p id="aa5c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">都很标准。与DDPG相比，更大的区别是缺少奥恩斯坦-乌伦贝克噪声，为了简单起见，用简单的均匀噪声代替。至于DDPG，我们也使用目标评论家和演员来稳定训练。</p><h1 id="6d8c" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">超级代理</h1><p id="67e4" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">我们现在要定义引导代理行动的全局逻辑。我把这个模块叫做<em class="ky">“超级代理人”</em>，但是实际上并没有一个超级代理人，而是一群<em class="ky">“超级评论家”</em>帮助单个演员改进他们的政策:</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="4b13" class="kt jl hh kp b fi ku kv l kw kx"><em class="ky">class</em> SuperAgent:<br/>    <em class="ky">def</em> __init__(self, env, path_save=PATH_SAVE_MODEL, path_load=PATH_LOAD_FOLDER):<br/>        self.path_save = path_save<br/>        self.path_load = path_load<br/>        self.replay_buffer = ReplayBuffer(env)<br/>        self.n_agents = <em class="ky">len</em>(env.agents)<br/>        self.agents = [Agent(env, agent) for agent in <em class="ky">range</em>(self.n_agents)]<br/>        <br/>    <em class="ky">def</em> get_actions(self, agents_states):<br/>        list_actions = [self.agents[index].get_actions(agents_states[index]) for index in <em class="ky">range</em>(self.n_agents)]<br/>        return list_actions<br/>    <br/>    <em class="ky">def</em> save(self):<br/>        date_now = time.strftime("%Y%m%d%H%M")<br/>        full_path = f"{self.path_save}/save_agent_{date_now}"<br/>        if not os.path.isdir(full_path):<br/>            os.makedirs(full_path)<br/>        <br/>        for agent in self.agents:<br/>            agent.save(full_path)<br/>            <br/>        self.replay_buffer.save(full_path)<br/>    <br/>    <em class="ky">def</em> load(self):<br/>        full_path = self.path_load<br/>        for agent in self.agents:<br/>            agent.load(full_path)<br/>            <br/>        self.replay_buffer.load(full_path)</span></pre><p id="4812" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们首先定义重放缓冲区、代理和get_actions函数，该函数只调用单个参与者的get_actions函数。相同的逻辑用于仅调用单个代理的保存和加载的保存和加载功能，相同的功能用于重放缓冲区。</p><p id="5c0d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们进入有趣的部分，即代理的培训:</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="1c00" class="kt jl hh kp b fi ku kv l kw kx"><em class="ky">def</em> train(self):<br/>        if self.replay_buffer.check_buffer_size() == False:<br/>            return<br/>        <br/>        state, reward, next_state, done, actors_state, actors_next_state, actors_action = self.replay_buffer.get_minibatch()<br/>        <br/>        states = tf.convert_to_tensor(state, dtype=tf.float32)<br/>        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)<br/>        next_states = tf.convert_to_tensor(next_state, dtype=tf.float32)<br/>        <br/>        actors_states = [tf.convert_to_tensor(s, dtype=tf.float32) for s in actors_state]<br/>        actors_next_states = [tf.convert_to_tensor(s, dtype=tf.float32) for s in actors_next_state]<br/>        actors_actions = [tf.convert_to_tensor(s, dtype=tf.float32) for s in actors_action]<br/>        <br/>        with tf.GradientTape(persistent=True) as tape:<br/>            target_actions = [self.agents[index].target_actor(actors_next_states[index]) for index in <em class="ky">range</em>(self.n_agents)]<br/>            policy_actions = [self.agents[index].actor(actors_states[index]) for index in <em class="ky">range</em>(self.n_agents)]<br/>            <br/>            concat_target_actions = tf.concat(target_actions, axis=1)<br/>            concat_policy_actions = tf.concat(policy_actions, axis=1)<br/>            concat_actors_action = tf.concat(actors_actions, axis=1)<br/>            <br/>            target_critic_values = [tf.squeeze(self.agents[index].target_critic(next_states, concat_target_actions), 1) for index in <em class="ky">range</em>(self.n_agents)]<br/>            critic_values = [tf.squeeze(self.agents[index].critic(states, concat_actors_action), 1) for index in <em class="ky">range</em>(self.n_agents)]<br/>            targets = [rewards[:, index] + self.agents[index].gamma * target_critic_values[index] * (1-done[:, index]) for index in <em class="ky">range</em>(self.n_agents)]<br/>            critic_losses = [tf.keras.losses.MSE(targets[index], critic_values[index]) for index in <em class="ky">range</em>(self.n_agents)]<br/>            <br/>            actor_losses = [-self.agents[index].critic(states, concat_policy_actions) for index in <em class="ky">range</em>(self.n_agents)]<br/>            actor_losses = [tf.math.reduce_mean(actor_losses[index]) for index in <em class="ky">range</em>(self.n_agents)]<br/>        <br/>        critic_gradients = [tape.gradient(critic_losses[index], self.agents[index].critic.trainable_variables) for index in <em class="ky">range</em>(self.n_agents)]<br/>        actor_gradients = [tape.gradient(actor_losses[index], self.agents[index].actor.trainable_variables) for index in <em class="ky">range</em>(self.n_agents)]<br/>        <br/>        for index in <em class="ky">range</em>(self.n_agents):<br/>            self.agents[index].critic.optimizer.apply_gradients(<em class="ky">zip</em>(critic_gradients[index], self.agents[index].critic.trainable_variables))<br/>            self.agents[index].actor.optimizer.apply_gradients(<em class="ky">zip</em>(actor_gradients[index], self.agents[index].actor.trainable_variables))<br/>            self.agents[index].update_target_networks(self.agents[index].tau)</span></pre><p id="0c5e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们从调用get_minibatch函数从重放缓冲区获取观察值开始。然后我们把所有的NumPy向量转换成张量流向量。然后，我们从目标参与者和标准参与者获得目标和策略操作。我们在它们上创建全局视图，其中还包括连接所有代理的所采取的动作、策略和目标动作的所采取的动作。因此，我们可以从批评者那里获得价值，向他们提供关于状态和行为的全局观点，并始终使用贝尔曼方程来定义损失函数。</p><p id="f8bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">给定q值，我们可以定义演员损失，对评论家和演员应用梯度，并软更新目标网络。</p><h1 id="30d7" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">Wandb初始化和训练循环</h1><p id="3476" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">我们现在可以初始化wandb配置:</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="950e" class="kt jl hh kp b fi ku kv l kw kx">config = <em class="ky">dict</em>(<br/>  learning_rate_actor = ACTOR_LR,<br/>  learning_rate_critic = CRITIC_LR,<br/>  batch_size = BATCH_SIZE,<br/>  architecture = "MADDPG",<br/>  infra = "Colab",<br/>  env = ENV_NAME<br/>)<br/><br/>wandb.init(<br/>  project=f"tensorflow2_madddpg_{ENV_NAME.lower()}",<br/>  tags=["MADDPG", "RL"],<br/>  config=config,<br/>)</span></pre><p id="ed89" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，如果我们要重新开始培训会话，我们将初始化环境并加载一个经过培训的模型:</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="93c4" class="kt jl hh kp b fi ku kv l kw kx">env = make_env(ENV_NAME)<br/>super_agent = SuperAgent(env)<br/><br/>scores = []<br/><br/>if PATH_LOAD_FOLDER is not None:<br/>    print("loading weights")<br/>    actors_state = env.reset()<br/>    actors_action = super_agent.get_actions([actors_state[index][None, :] for index in <em class="ky">range</em>(super_agent.n_agents)])<br/>    [super_agent.agents[index].target_actor(actors_state[index][None, :]) for index in <em class="ky">range</em>(super_agent.n_agents)]<br/>    state = np.concatenate(actors_state)<br/>    actors_action = np.concatenate(actors_action)<br/>    [super_agent.agents[index].critic(state[None, :], actors_action[None, :]) for index in <em class="ky">range</em>(super_agent.n_agents)]<br/>    [super_agent.agents[index].target_critic(state[None, :], actors_action[None, :]) for index in <em class="ky">range</em>(super_agent.n_agents)]<br/>    super_agent.load()<br/><br/>    print(super_agent.replay_buffer.buffer_counter)<br/>    print(super_agent.replay_buffer.n_games</span></pre><p id="da7c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们实现了训练循环:</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="a959" class="kt jl hh kp b fi ku kv l kw kx">for n_game in tqdm(<em class="ky">range</em>(MAX_GAMES)):<br/>    start_time = time.time()<br/>    actors_state = env.reset()<br/>    done = [False for index in <em class="ky">range</em>(super_agent.n_agents)]<br/>    score = 0<br/>    step = 0<br/>    <br/>    if (super_agent.replay_buffer.n_games + 1) &gt; 5000:<br/>        MAX_STEPS = <em class="ky">int</em>((super_agent.replay_buffer.n_games + 1) / 200)<br/><br/>    while not <em class="ky">any</em>(done):<br/>        actors_action = super_agent.get_actions([actors_state[index][None, :] for index in <em class="ky">range</em>(super_agent.n_agents)])<br/>        actors_next_state, reward, done, info = env.step(actors_action)<br/>        <br/>        state = np.concatenate(actors_state)<br/>        next_state = np.concatenate(actors_next_state)<br/>        <br/>        super_agent.replay_buffer.add_record(actors_state, actors_next_state, actors_action, state, next_state, reward, done)<br/>        <br/>        actors_state = actors_next_state<br/>        <br/>        score += <em class="ky">sum</em>(reward)<br/>        step += 1<br/>        if step &gt;= MAX_STEPS:<br/>            break<br/>    <br/>    if super_agent.replay_buffer.check_buffer_size():<br/>        super_agent.train()<br/>        <br/>    super_agent.replay_buffer.update_n_games()<br/>    <br/>    scores.append(score)<br/>    <br/>    wandb.log({'Game number': super_agent.replay_buffer.n_games, '# Episodes': super_agent.replay_buffer.buffer_counter, <br/>               "Average reward": <em class="ky">round</em>(np.mean(scores[-10:]), 2), \<br/>                      "Time taken": <em class="ky">round</em>(time.time() - start_time, 2), 'Max steps': MAX_STEPS})<br/>    <br/>    if (n_game + 1) % EVALUATION_FREQUENCY == 0 and super_agent.replay_buffer.check_buffer_size():<br/>        actors_state = env.reset()<br/>        done = [False for index in <em class="ky">range</em>(super_agent.n_agents)]<br/>        score = 0<br/>        step = 0<br/>        while not <em class="ky">any</em>(done):<br/>            actors_action = super_agent.get_actions([actors_state[index][None, :] for index in <em class="ky">range</em>(super_agent.n_agents)])<br/>            actors_next_state, reward, done, info = env.step(actors_action)<br/>            state = np.concatenate(actors_state)<br/>            next_state = np.concatenate(actors_next_state)<br/>            actors_state = actors_next_state<br/>            score += <em class="ky">sum</em>(reward)<br/>            step += 1<br/>            if step &gt;= MAX_STEPS:<br/>                break<br/>        wandb.log({'Game number': super_agent.replay_buffer.n_games, <br/>                   '# Episodes': super_agent.replay_buffer.buffer_counter, <br/>                   'Evaluation score': score, 'Max steps': MAX_STEPS})<br/>            <br/>    if (n_game + 1) % SAVE_FREQUENCY == 0 and super_agent.replay_buffer.check_buffer_size():<br/>        print("saving weights and replay buffer...")<br/>        super_agent.save()<br/>        print("saved")</span></pre><p id="2f45" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">都很标准。唯一具体的是MAX_STEPS参数，我们随着玩游戏的次数而增加。这是因为粒子环境并没有真正结束，所以我们需要一个参数来摆脱循环。</p><h1 id="91e0" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">结果</h1><p id="3897" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">我们现在可以看到结果。我们从简单的对手开始:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ln"><img src="../Images/945c84ab02efd2a66cf897c5a34f306b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jTcQPTJBj4N3WphF"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ln"><img src="../Images/4fd699fc7fcbebcac985216054953019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hnkKTtIi1eAFxsem"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ln"><img src="../Images/f5ff22799e5c52a1143e7cfaf4b22d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EbwT8EddSDzWysSz"/></div></div></figure><p id="4616" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在第一幅图中，我们可以看到如何增加MAX_STEPS参数。在第二个例子中，我们可以看到训练中的平均回报是如何变化的。正如你所看到的，有很大的可变性，大部分是负面的结果，大部分是正面的结果。这是典型的DDPG，我们也看到了它在单一代理对应。在最后一个情节中，我们可以看到评价分数如何随着时间的推移而增加，直到我们停止它，但代理们正在学习。</p><p id="bd53" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用简单的对手环境来开发一切，但我们也在另一个环境simple tag上测试它，以确保它能够工作:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ln"><img src="../Images/e29e25b96d3273540850e07038309c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HThxb3wEoLYbN68M"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ln"><img src="../Images/e6b35141b38056f6c838700d686f5b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*a7e1IzV534BlWX4A"/></div></div></figure><p id="e73b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从这两个图中，我们可以看到代理正在学习，仍然有很高的可变性，在这种情况下，10k游戏的性能在变好之前变得最差。</p><h1 id="a2c5" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">结论</h1><p id="7b2a" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">因此，在这篇文章中，我们展示了如何实现和使用多智能体MADDPG来解决OpenAI提供的玩具环境，复制了Lowe等人(2017) 在论文<a class="ae kn" href="https://arxiv.org/pdf/1706.02275.pdf" rel="noopener ugc nofollow" target="_blank">混合合作竞争环境的多智能体演员-评论家中所取得的成就。</a></p><p id="aa2d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以在我的<a class="ae kn" href="https://github.com/antonai91/reinforcement_learning/tree/master/maddpg" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到所有代码。有任何问题，你可以通过<a class="ae kn" href="https://www.linkedin.com/in/lisiantonio/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我。</p><p id="999c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你喜欢这篇文章，分享给你的朋友和同事吧！我会在下一篇文章中看到你。与此同时，要小心，保持安全，记住<em class="ky">不要成为另一块墙砖</em>。</p><p id="8aae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Anton.ai</p></div><div class="ab cl ls lt go lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ha hb hc hd he"><p id="bf87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ky">原载于2021年5月12日</em><a class="ae kn" href="https://antonai.blog/multi-agent-reinforcement-learning-openais-maddpg/" rel="noopener ugc nofollow" target="_blank"><em class="ky">https://antonai . blog</em></a><em class="ky">。</em></p></div></div>    
</body>
</html>