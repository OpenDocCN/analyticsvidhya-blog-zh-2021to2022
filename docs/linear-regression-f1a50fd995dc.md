# 线性回归

> 原文：<https://medium.com/analytics-vidhya/linear-regression-f1a50fd995dc?source=collection_archive---------6----------------------->

线性回归是一种试图以线性方式预测因变量 Y 和自变量 X 之间关系的回归模型。

y=mx+c

它是一个回归模型，这意味着它用于预测连续值。

**例如:**股票预测、保险金额预测、贷款金额预测、房价预测等..

**使用 SKlearn 实现线性回归。**

**标准标量**以均值=0，标准差=1 的方式转换数据，这是标准正态分布。

标准标量如何影响测试数据？

**使用 RFECV(递归特征消除和交叉验证)的特征选择**

以递归方式消除不重要的特征。

1.  模型-您想要训练的机器学习算法。
2.  步长-确定我们想要在 onw 迭代中消除多少特征，步长为 1 是一个不错的选择。
3.  Min_features_to_select 当 RFECV 递归迭代时，我们需要强制停止它。

a.如果我们至少一次排除了所有的特征。

b.如果我们达到了最小特征数阈值。

**RFECV 有助于特性排名**

根据各要素与主得分的偏差对其进行排名

1.  如果得分大幅下降:重要特征
2.  如果得分下降较低:不太重要的功能
3.  如果分数没有下降:不重要的特征(没有影响)

得分低是由于预测列中值的分布和偏斜。

**K 线交叉折叠验证**

我们将我们的数据分成 k 个折叠，其中 k 一般在 5 到 10 之间。1 折用于测试，其余(k-1)折用于培训。不断重复这个过程，直到我们至少使用了一次每个折叠进行测试。

**缺点**

我们在每次迭代中重新创建训练和测试分割，并从头开始训练我们的模型。

a.如果 k=5，我们需要训练模型 5 次。

乙。当数据太大或 k 值很高时，计算开销很大。

从 sklearn.model_selection 导入 Kfold

**列车试运行的缺点**

1.  20%的测试数据太小，无法对预测充满信心。
2.  我们不会验证剩余 80%的数据，因为我们已经对其进行了培训..

a.也许我们可以用这 80%来提高我们对预测的信心。

**分析回归模型的性能。**

为了理解回归模型的性能，我们需要理解损失函数的概念。

**损失函数**

1.  外部模型在训练阶段试图最小化的目标函数

a.当模型出错时，通过惩罚它来帮助模型做出更好的预测。

2.消极导向，越低越好。

a.当损失较高时，模型被严重扣分。

**回归模型的两个主要损失函数是**

1.  **MAE-平均绝对误差-** 与原始值相比，预测误差的平均幅度
2.  **RMSE-均方根误差-** 原始值和预测值之差的均方根。

哪个更好？

1.  MAE 和 RMSE 的范围都是从 0 到无穷大，但是 RMSE 的值总是大于或等于 MAE。

a.当所有数据点的原始值和预测值之间的差异相等时，相等条件达到。

2.误差在被平均之前被平方，RMSE 对较大的误差给予相对较高的权重

a.当模型做出错误的预测时，RMSE 比 MAE 更有效地惩罚了模型。

**R2 得分和调整后的 R2 得分直觉**

r 平方用于确定特征和目标之间的相关强度。

a.它让我们知道与平均值相比，回归模型有多好。

**公式**为(R2)R 平方= 1-SSR/SST

分析 R2

1.  如果 SSR=0，R2 将为 1，即..Y=Y-hat，这意味着预测值与实际值完全相同，

a.这永远不会发生，因为精确预测连续值是不可能的。

2.如果 SSR=SSt，Rsquare 将为 0，即..y_hat=y_bar

a.如果我们的预测不比平均值好，那么我们的模型表现很差。

3.R2 也可以是负值，这发生在 SSR>SST 时，

a.也就是说，我们的预测并没有遵循价值的趋势，而是遵循相反的趋势。

**R 方的缺点**

1.  通过增加更多的特征数，R 平方不断增加，这个性质被称为“R 平方膨胀”或“R 平方增加的性质”。
2.  调整后的 R 平方能够随着不太重要的变量的增加而减小，从而产生更可靠和准确的评估。
3.  调整后的 R 平方是更好的模型评估器。

**注-**

**主成分分析(PCA)** 是一种统计程序，使用正交变换将一组相关变量转换为一组不相关变量。PCA 是探索性数据分析和预测模型的机器学习中最广泛使用的工具。此外，PCA 是一种无监督的统计技术，用于检查一组变量之间的相互关系。它也被称为一般因素分析，其中回归确定最佳拟合线。

**决定系数-** 这是一种统计测量，在预测给定事件的结果时，检验一个变量的差异如何用第二个变量的差异来解释。

**外推法**是指通过假设现有趋势将继续或当前方法将继续适用来估计或推断某事的行为。

**使用案例**

*   非常强大的统计技术，用于洞察消费者行为，了解业务和影响盈利能力的因素。
*   用于评估趋势并做出预测或估计
*   用于分析产品销售的营销效果、定价和促销。
*   在银行业中，线性回归用于根据年龄、以前的贷款、资产、申请的贷款额、职位等来预测特定人员的贷款额

—个人属性的风险越大，提供给他们的贷款就越少。

**让我们总结一下**

**线性回归**用于预测连续值。

**RFECV** —以递归方式删除不重要的特征

*   模型
*   步骤
*   最少功能

**交叉验证**

**绩效指标**

*   R2 评分，调整后的 R2 评分，梅和 RMSE