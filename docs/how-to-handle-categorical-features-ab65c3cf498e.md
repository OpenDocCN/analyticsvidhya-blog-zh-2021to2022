# 如何处理分类特征

> 原文：<https://medium.com/analytics-vidhya/how-to-handle-categorical-features-ab65c3cf498e?source=collection_archive---------0----------------------->

![](img/6519561796f86aff4de0bcfc0a3c088b.png)

大家好！！！欢迎来到我的博客。如果你在处理数据，我 110%确定你已经看到了按性别(男性、女性)或教育程度(博士、硕士、学士)分类的数据。因为我们正在处理机器学习中的数学模型，所以在利用它来训练我们的模型之前，我们可以将这个类别转换成数字是很重要的。

在这篇博客中，我们将看看什么是分类变量和它们的各种类型，以及用代码样本处理分类数据的不同方法。

所有的代码样本和数据集都可以在[这里](https://github.com/ashutoshsahu2015/How-to-handle-Categoriacal-Featues)获得。

**分类数据及其类型**

分类变量或离散变量是指有两个或更多类别(值)的变量。有两种不同类型的分类变量:

**名义上的**

一个名义变量对其类别没有内在的排序。例如，性别是一个分类变量，有两个类别(男性和女性)，它们之间没有内在的顺序。另一个例子是国家(印度、澳大利亚、美国等等)。

**序数**

一个序数变量在其范畴内有一个清晰的顺序。例如，将温度视为具有三个不同(但相关)类别(低、中、高)的变量。另一个例子是教育学位(博士、硕士或学士)。

**处理分类数据的不同方法**

一个热编码

一个热编码有多个类别

序数编码

计数或频率编码

目标引导顺序编码

平均序数编码

概率比编码

**一个热编码**

这种技术适用于名词分类特征。

在一种热编码方法中，每个类别值都被转换成一个新列，并为该列赋值 1 或 0。

这将使用 pandas get_dummies()函数来完成，然后我们将删除第一列以避免虚拟变量陷阱。

优势:

易于使用，非常适合类别较少的数据。

缺点:

较高类别的高基数将增加特征空间，导致维数灾难。

**一个热编码多类别**

这是从 KDD 橙杯比赛中挑选出来的合奏技巧之一。在这种技术中，作者对一种热门的编码技术进行了轻微的修改，即不是为每个类别创建新列，而是限制为 10 个最常见的类别创建新列。听起来像个行话！！！！我也是😊

让我们看看下面的代码来更好地理解它:

优势:

易于实施

不大量扩展特征空间

缺点:

不跟踪被忽略的类别值。

**序数编码**

顾名思义，这种技术用于序数分类特征。

在这种技术中，每个唯一的类别值都被赋予一个整数值。例如，“红色”等于 1，“绿色”等于 2，“蓝色”等于 3。

域信息可用于确定整数值的顺序。例如，我们喜欢星期六和星期天，最讨厌星期一。在这个场景中，工作日的映射是“星期一”是 1，“星期二”是 2，“星期三”是 3，“星期四”是 4，“星期五”是 5，“星期六”是 6，“星期日”是 7。

为了更好地理解，请看下面的代码:

优势:

实施起来简单明了

广泛用于调查和研究数据编码。

缺点:

没有标准化的区间尺度。

**计数或频率编码**

顾名思义，在这种技术中，我们将通过在数据集中显示类别的观察计数来替换类别。

举个例子。如果印度在国家列中出现了 56 次，而美国出现了 49 次，那么我们在国家列中用 56 代替印度，用 49 代替美国。

优势:

易于实施

特征空间不会增加。

与基于树的算法配合良好。

缺点:

如果频率相同，它将不会提供相同的权重。

**目标引导顺序编码**

在这项技术中，我们将通过与目标或输出变量进行比较来转换我们的分类变量。

**步骤**:

1)选择一个分类变量。

2)取分类变量的聚合平均值，并将其应用于目标变量。

3)将较高的整数值或较高的等级分配给具有最高平均值的类别。

优势:

在变量和目标之间建立单调关系。

有助于加快学习

缺点:

由于与目标变量的密切关系，常常会导致过度拟合。

**平均序数编码**

这是目标导向顺序编码的一种变体，在数据科学家中广泛传播。我们用获得的平均值代替类别，而不是给它分配整数值。

优势:

提高分类模型的效率。

快速获取信息

缺点:

导致过度拟合

如果两个类别具有相同的含义，可能会导致价值损失

**概率比编码**

这种技术仅适用于目标变量为二进制(1 或 0 或真或假)的分类问题。

在这种技术中，我们将用概率比即 P(1)/P(0)来代替类别值。

步骤:

1)使用分类变量，评估目标变量的概率(其中输出为真或 1)。

2)计算目标变量输出为假或 0 的概率。

3)计算概率比，即 P(真或 1) / P(假或 0)。

4)用概率比替换类别。

优势:

不要扩展特征空间。

从类别中捕获信息，从而产生更多预测功能。

缺点:

分母为 0 时未定义。

有时会导致过度拟合。

让我们通过讨论哪种技术最适合问题陈述或我们的模型来结束这个中介。

老实说，对于任何数据集或问题陈述，都没有放之四海而皆准的解决方案。我们需要测试几个案例，看看哪一个能产生最好的结果。

**参考文献**:

https://www.youtube.com/watch?v=uWD-r7GZppg

[https://towards data science . com/all-about-category-variable-encoding-305 f 3361 FD 02](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02)

感谢你花时间阅读这篇文章。如果你喜欢这篇文章，点击👏按钮并与他人分享。如果你有任何问题，请在评论区留言，我会尽力回复。

你可以在 [LinkedIn](https://www.linkedin.com/in/ashutoshsahu2015/) 、[脸书](https://www.facebook.com/ashutosh.sahu.9699)、 [Instagram](https://www.instagram.com/_ashutosh_sahu/) 上和我联系。

下次见，*再见，朋友！！！！！*