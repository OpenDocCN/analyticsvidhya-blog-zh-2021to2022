<html>
<head>
<title>Tokenising rants — The Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">表征咆哮——数据</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/tokenising-rants-d61b1a0def80?source=collection_archive---------20-----------------------#2021-03-29">https://medium.com/analytics-vidhya/tokenising-rants-d61b1a0def80?source=collection_archive---------20-----------------------#2021-03-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/e1c3fb15c5e5e33485028b7780604895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xZbE0z3duEmE3LMd.jpg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图片来自<a class="ae it" href="https://www.myneworleans.com/10-ways-to-alleviate-stress/" rel="noopener ugc nofollow" target="_blank">我的新奥尔良</a></figcaption></figure><p id="3779" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">疫情和随之而来的隔离切断了我们与许多线下互动的联系，这些交流曾经是我们许多人的安全出口。随着经济下滑和死亡率上升，我们人类作为适应性种族，走上网络平台来摆脱我们的负面情绪。</p><p id="d3c6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">无论是推特上的咆哮或趋势标签，还是媒体上令人心碎的故事，或者心理健康论坛上经常相关的帖子，都有大量数据可供我们分析，所以我开始了。</p><p id="e300" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在本文中，我们将讨论分析所需的数据收集(通过网络搜集)。这是精神健康数据符号化和分析系列文章的第1部分。</p><h1 id="9155" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">第一次尝试:Tweepy和社交网络</h1><p id="be15" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">虽然很多网站不允许你从他们的页面上抓取数据，幸运的是，Twitter有一个流行的API让你的生活更容易，python有了Tweepy包让这变得更容易。然而，要使用Tweepy，你需要在Twitter上创建一个<a class="ae it" href="https://developer.twitter.com/en/apply-for-access" rel="noopener ugc nofollow" target="_blank">开发者账户</a>。</p><p id="6a0f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一旦你的账户设置好了，你将拥有自己独特的令牌来使用Twitter API，并且你将会在使用Tweepy抓取Tweepy时需要它们。</p><p id="f853" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们先获得授权。还记得您在上创建的所有唯一令牌创建了一个开发者帐户吗？我们在这里使用它们。</p><blockquote class="kv kw kx"><p id="99e2" class="iu iv ky iw b ix iy iz ja jb jc jd je kz jg jh ji la jk jl jm lb jo jp jq jr ha bi translated">一个<!-- -->建议:不要和任何人分享这些代币，除非你想因为某人滥用你的账户而被起诉。出于某种原因，这些令牌是秘密且唯一的。</p></blockquote><pre class="lc ld le lf fd lg lh li lj aw lk bi"><span id="c6f6" class="ll jt hh lh b fi lm ln l lo lp">from tweepy import OAuthHandler</span><span id="1a1c" class="ll jt hh lh b fi lq ln l lo lp">from tweepy.streaming import StreamListener<br/>import tweepy<br/>import json<br/>import pandas as pd<br/>import time</span><span id="4afd" class="ll jt hh lh b fi lq ln l lo lp">access_token = "your_access_token_here"<br/>access_token_secret = "your_access_token_secret_here"<br/>api_key = "your_api_key_here"<br/>api_secret_key = "your_api_key_secret_here"</span><span id="c998" class="ll jt hh lh b fi lq ln l lo lp">auth = OAuthHandler(api_key, api_secret_key)<br/>auth.set_access_token(access_token, access_token_secret)<br/>api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify= True)</span></pre><p id="94c8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后是刮削，</p><pre class="lc ld le lf fd lg lh li lj aw lk bi"><span id="96da" class="ll jt hh lh b fi lm ln l lo lp">tweets_list = []</span><span id="b144" class="ll jt hh lh b fi lq ln l lo lp">query_list = [<br/>    "anxiety", "anxious", "depression", "depressed", "suicidal", <br/>    "suicide", "ptsd", "trauma", "mental ilness", "triggered", <br/>    "bipolar", "mental health", "mental illness", "therapist", "therapy"<br/>]</span><span id="4e94" class="ll jt hh lh b fi lq ln l lo lp">countries = [<br/>     "USA", "UK", "Finland", "India", "Canada", "Switzerland"<br/>]</span><span id="7451" class="ll jt hh lh b fi lq ln l lo lp">date_since = "2018-01-01"</span><span id="7350" class="ll jt hh lh b fi lq ln l lo lp">for query in query_list:<br/>    for country in countries:<br/>        places = api.geo_search(query=country, granularity="country") # Acquiring geocodes for selected countries to filter the twitter query results<br/>        place_id = places[0].id<br/>        query_string = query + f" place:{place_id}"<br/>        try:<br/>            tweets = tweepy.Cursor(api.search, q = query_string , lang="en", tweet_mode='extended', since=date_since).items(200)</span><span id="d6ca" class="ll jt hh lh b fi lq ln l lo lp">for tweet in tweets:<br/>                tweets_list.append([tweet.full_text, tweet.user.location, query])<br/>            print(f"Query: {query} for country: {country} done!")<br/>#             time.sleep(15*60)<br/>        except:<br/>            print(f"Something went wrong with query: {query}")<br/>            continue</span></pre><p id="0b31" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然而，这个决定适得其反。在社交媒体上，用户倾向于在非常轻松的背景下使用抑郁和焦虑等沉重的词汇。这并不意味着推文不是用户可能遭受某种精神痛苦的迹象，但鉴于幽默已经成为当代人非常常见的应对机制，收集的数据很快变得非常复杂。</p><p id="38a2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">另一个问题是Tweepy的刮极限。对于一个免费开发者的帐户，Twitter的API有一个非常低的限制，将导致你的刮刀需要相当长的时间，直到你可以获得一个很好的数据块进行分析。</p><p id="92d5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然而，已经进行了研究来分析使用这些社交媒体平台的用户的精神状态。然而，这需要更多的深入研究，我将在以后的文章中深入探讨。现在，让我们专注于一个更加小众的平台。</p><h1 id="b198" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">第二次尝试:BeautifulSoup和BeyondBlue</h1><p id="03f4" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">专门的论坛总是为数据收集提供更合适的来源，这个项目也不例外。在谷歌上快速搜索会出现几个专门讨论精神健康的论坛，而BeyondBlue 被证明是一个极好的起点。</p><p id="0e06" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">BeautifulSoup是PYTHON上非常流行的网络抓取库。如果你熟悉网页结构和CSS选择器的基础知识，从网上抓取数据(只要允许)是一件非常简单的事情！最上面的樱桃就是刮不限量。使用BeautifulSoup，您可以抓取网页上静态可用的所有数据。</p><blockquote class="kv kw kx"><p id="4dec" class="iu iv ky iw b ix iy iz ja jb jc jd je kz jg jh ji la jk jl jm lb jo jp jq jr ha bi translated">然而，BeautifulSoup的这一特性是一把双刃剑——虽然它给了你快速收集数据的自由，但你在选择选择器时也必须非常小心，否则你的数据集将很快被大量不必要的元数据堵塞。</p></blockquote><p id="9ee1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这个项目中，我坚持三个常见的精神健康话题:</p><ol class=""><li id="440d" class="lr ls hh iw b ix iy jb jc jf lt jj lu jn lv jr lw lx ly lz bi translated">焦虑</li><li id="4d13" class="lr ls hh iw b ix ma jb mb jf mc jj md jn me jr lw lx ly lz bi translated">抑郁</li><li id="5e2b" class="lr ls hh iw b ix ma jb mb jf mc jj md jn me jr lw lx ly lz bi translated">悲伤和损失</li></ol><p id="2ff5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">幸运的是，在BeyondBlue.org，上述每个话题都有专门的论坛主题。因此，我们所要做的就是在基本URL后面添加相应线程的扩展，然后让我们的scraper施展它的魔法。</p><p id="dfdb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在我们进入代码之前，这里有一个抓取过程的快速纲要:</p><ol class=""><li id="aaff" class="lr ls hh iw b ix iy jb jc jf lt jj lu jn lv jr lw lx ly lz bi translated">我们将从每个线程中只抓取原始张贴者的内容。在检查网页结构时，可以清楚地看到，在每个论坛的主页上，<strong class="iw hi">每个线程都被包裹在一个锚标签中，标签的类名为“sfforumThreadTitle”</strong>，所以这就是我们的scraper要搜索的目标。</li><li id="f3bf" class="lr ls hh iw b ix ma jb mb jf mc jj md jn me jr lw lx ly lz bi translated">对每个线程的进一步检查显示<strong class="iw hi">线程下的每个帖子都被包装在一个div标签中，标签的类为“postAndSig”</strong>。所以我们遍历线程页面中的每个div，找到第一个post (OP的post ),然后跳出循环，进入下一个线程。</li><li id="3de5" class="lr ls hh iw b ix ma jb mb jf mc jj md jn me jr lw lx ly lz bi translated">要移动到论坛中的下一页，我们只需更改URL中的页码:<strong class="iw hi">base _ URL+'/&lt;MH _ topic&gt;/page/'&lt;page _ number&gt;</strong></li></ol><pre class="lc ld le lf fd lg lh li lj aw lk bi"><span id="35c3" class="ll jt hh lh b fi lm ln l lo lp"><strong class="lh hi">## importing libraries</strong></span><span id="f094" class="ll jt hh lh b fi lq ln l lo lp">import requests<br/>import csv<br/>import time<br/>from bs4 import BeautifulSoup as BS</span><span id="415d" class="ll jt hh lh b fi lq ln l lo lp">data = []<br/>base_url = '<a class="ae it" href="https://healthyfamilies.beyondblue.org.au/seeking-support/helping-yourself-and-others/online-forums'" rel="noopener ugc nofollow" target="_blank">https://healthyfamilies.beyondblue.org.au/seeking-support/helping-yourself-and-others/online-forums'</a><br/>headers = {'User-Agent': 'Mozilla/5.0'}</span><span id="963b" class="ll jt hh lh b fi lq ln l lo lp"><strong class="lh hi">## Anxiety</strong></span><span id="bcde" class="ll jt hh lh b fi lq ln l lo lp">anxiety_threads = []<br/><strong class="lh hi">url = base_url + '/anxiety'</strong><br/>for i in range(1,10):<br/>    print("========================================")<br/>    print(f"Scraping page#{i} ( url: {url} )........")<br/>    page = requests.get(url, headers=headers)<br/>    soup = BS(page.text, 'html.parser')</span><span id="dd9d" class="ll jt hh lh b fi lq ln l lo lp">   <strong class="lh hi"> # Step 1</strong><br/>    links = soup.find_all('a', class_="sfforumThreadTitle") <br/>    for link in links:<br/>        thread_url = base_url + '/anxiety/' +(link.attrs['href'].split("/"))[1]<br/>        thread_page = requests.get(thread_url, headers=headers)<br/>        thread_soup = BS(thread_page.text, 'html.parser')</span><span id="e40e" class="ll jt hh lh b fi lq ln l lo lp">       <strong class="lh hi"> # Step 2</strong><br/>        divs = thread_soup.find_all("div")<br/>        for div in divs:<br/>            if(div.attrs.get("class")==["postAndSig"]):<br/>                anxiety_threads.append([div.text, 'Anxiety'])<br/>                print(f"Scraped {link.text}")<br/>                break<br/>    print("========================================")</span><span id="ef26" class="ll jt hh lh b fi lq ln l lo lp">    <strong class="lh hi"># Step 3 </strong><br/>    url = base_url+'/anxiety/page/'+str(i+1)<br/>    time.sleep(60)</span><span id="c0e5" class="ll jt hh lh b fi lq ln l lo lp"><strong class="lh hi">## Depression</strong></span><span id="b6ee" class="ll jt hh lh b fi lq ln l lo lp">depression_threads = []<br/>url = base_url + '/depression'<br/>for i in range(1,10):<br/>    print("========================================")<br/>    print(f"Scraping page#{i} ( url: {url} )........")<br/>    page = requests.get(url, headers=headers)<br/>    soup = BS(page.text, 'html.parser')</span><span id="6745" class="ll jt hh lh b fi lq ln l lo lp"><strong class="lh hi">    # Step 1</strong><br/>    links = soup.find_all('a', class_="sfforumThreadTitle")<br/>    for link in links:<br/>        thread_url = base_url + '/depression/'+ (link.attrs['href'].split("/"))[1]<br/>        thread_page = requests.get(thread_url, headers=headers)<br/>        thread_soup = BS(thread_page.text, 'html.parser')</span><span id="e3b7" class="ll jt hh lh b fi lq ln l lo lp"><strong class="lh hi">        # Step 2</strong><br/>        divs = thread_soup.find_all("div")<br/>        for div in divs:<br/>            if(div.attrs.get("class")==["postAndSig"]):<br/>                depression_threads.append([div.text, 'Depression'])<br/>                print(f"Scraped {link.text}")<br/>                break<br/>    print("========================================")</span><span id="acb5" class="ll jt hh lh b fi lq ln l lo lp">    <strong class="lh hi"># Step 3</strong><br/>    url = base_url+'/depression/page/'+str(i+1)<br/>    time.sleep(60)</span><span id="f619" class="ll jt hh lh b fi lq ln l lo lp"><strong class="lh hi">## Grief and Loss</strong></span><span id="3deb" class="ll jt hh lh b fi lq ln l lo lp">grief_threads = []<br/>url = base_url + '/grief-and-loss'<br/>for i in range(1,10):<br/>    print("========================================")<br/>    print(f"Scraping page#{i} (url: {url})........")<br/>    page = requests.get(url, headers=headers)<br/>    soup = BS(page.text, 'html.parser')</span><span id="82e4" class="ll jt hh lh b fi lq ln l lo lp"><strong class="lh hi">    # Step 1</strong><br/>    links = soup.find_all('a', class_="sfforumThreadTitle")<br/>    for link in links:<br/>        thread_url = base_url + '/grief-and-loss/'+ (link.attrs['href'].split("/"))[1]<br/>        thread_page = requests.get(thread_url, headers=headers)<br/>        thread_soup = BS(thread_page.text, 'html.parser')</span><span id="0c17" class="ll jt hh lh b fi lq ln l lo lp"><strong class="lh hi">        # Step 2</strong><br/>        divs = thread_soup.find_all("div")<br/>        for div in divs:<br/>            if(div.attrs.get("class")==["postAndSig"]):<br/>                grief_threads.append([div.text, 'Grief'])<br/>                print(f"Scraped {link.text}")<br/>                break<br/>    print("========================================")</span><span id="c5b0" class="ll jt hh lh b fi lq ln l lo lp">    <strong class="lh hi"># Step 3</strong><br/>    url = base_url+'/grief-and-loss/page/'+str(i+1)<br/>    time.sleep(60)</span></pre><p id="3204" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，我们将收集到的所有数据放入一个Pandas数据框，并将其存储在一个CSV文件中以供进一步分析。为了区别，我甚至保存了特定文本的论坛类别。</p><pre class="lc ld le lf fd lg lh li lj aw lk bi"><span id="45fb" class="ll jt hh lh b fi lm ln l lo lp">data = anxiety_threads + depression_threads + grief_threads<br/>print(f"Posts collected: {len(data)}")</span><span id="9a50" class="ll jt hh lh b fi lq ln l lo lp">import pandas as pd</span><span id="eca4" class="ll jt hh lh b fi lq ln l lo lp">data_df = pd.DataFrame(data = data, columns=['text', 'category'])<br/>data_df.to_csv("BeyondBlue_Data.csv", index=False)</span></pre><p id="921a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们完了。</p><p id="49ae" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">也就是数据收集。接下来是数据预处理部分，所有NLP爱好者(或者可能只有我)都对这个过程又爱又恨。</p><p id="5a46" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">尽管如此，NLP管道的一个非常重要的方面，我将在下一篇文章中介绍，敬请关注。</p><p id="b5b2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在那之前，快乐的(合法的)网络搜集！</p></div></div>    
</body>
</html>