<html>
<head>
<title>The Curse of Dimensionality and its Cure</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">维数灾难及其解决方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-curse-of-dimensionality-and-its-cure-f9891ab72e5c?source=collection_archive---------1-----------------------#2021-04-25">https://medium.com/analytics-vidhya/the-curse-of-dimensionality-and-its-cure-f9891ab72e5c?source=collection_archive---------1-----------------------#2021-04-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/3f936385068fd99ef91cb3409ae2599d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rYJSJP0xogZin1zz6-2NRQ.jpeg"/></div></div></figure><p id="b010" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Avada Kedavra (又名杀戮诅咒)可能是所有诅咒中最致命的，但是在这个<strong class="ir hi">麻瓜世界</strong>，我们不得不应对比这更糟糕的事情。</p><p id="3a89" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">维度的诅咒</strong></p><p id="dc42" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有一种特殊的麻瓜也被称为数据科学家，他们在日常工作中不得不处理这个诅咒。</p><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="f85c" class="jw jx hh js b fi jy jz l ka kb">(Okay, so enough with the harry potter references and let's get right into business.)</span></pre><p id="fbb9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">维数灾难</strong>是当我们处理大量具有多种特征的数据时出现的一个问题，或者我们可以说它是高维数据。数据的维度意味着数据集中的特征或列的数量。</p><p id="b403" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们在处理高维数据时面临的问题是，在分析和可视化数据的同时识别有意义的模式变得非常具有挑战性，并且它还降低了机器学习模型的准确性，同时降低了计算速度，即随着维度的增加，训练模型将变得更慢。</p><p id="c1dd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">随着维数的增加，多重共线性出现的机会也越来越多。</p><figure class="jn jo jp jq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es et"><img src="../Images/0c99ac37c729103e697dd8c5cd925ea3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*n2mKkdpAArHDQ_qg.png"/></div></div></figure><p id="3ebf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设我们在一个维度上有4个数据点(数据集中只有一个特征)。因此，它可以很容易地用一条线来表示，维度空间等于4。</p><p id="a7c9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，如果我们再添加一个特性，那么这将导致维度空间增加到4*4 =16。同样，如果我们再添加一个特征，维度空间将增加到4*4*4 = 64，以此类推(4个维度(4*4*4*4=256)等等。).所以随着维度的不断增加，维度空间呈指数增长。</p><p id="3e7c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">休斯现象</strong></p><p id="5322" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这种现象表明“在训练样本数量固定的情况下，分类器或回归器的平均(预期)预测能力首先随着所用维度或特征数量的增加而增加，但超过某个维度后，它开始恶化，而不是稳步提高”。</p><figure class="jn jo jp jq fd ii er es paragraph-image"><div class="er es kc"><img src="../Images/a8266360adb8ccc20a2bc19ae696033d.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/0*iogC18To3T5GVQBb.png"/></div></figure><p id="7420" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，随着维数的增加，计算观察值之间的距离也变得非常麻烦，因此，所有依赖于计算观察值之间的距离的机器学习算法发现处理高维数据非常麻烦，例如像KNN、K-Means等分割和聚类算法。</p><p id="b498" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">正如我们现在所知道的，高维数据是造成维数灾难的原因，但是为什么我们的数据中会有如此巨大的维数呢？</p><ul class=""><li id="38fb" class="kd ke hh ir b is it iw ix ja kf je kg ji kh jm ki kj kk kl bi translated">通常，在数据收集或提取阶段，我们从所有可能的地方收集数据并进行组合，这导致我们的数据集中有大量的要素。因此，为了解决现实世界的业务问题，我们通常需要大量的信息，因此数据中存在更多的特征。</li><li id="6150" class="kd ke hh ir b is km iw kn ja ko je kp ji kq jm ki kj kk kl bi translated">我们的数据集中通常也有分类特征，但许多机器学习模型并不适合分类数据。因此，我们所做的是使用特征编码技术(将分类变量转换为数字特征)，如一键编码，这为每个类别创建了许多虚拟变量，即增加了维数。</li></ul><p id="8f64" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="kr">我们知道</em> <strong class="ir hi"> <em class="kr">阿瓦达</em> </strong> <em class="kr"> </em> <strong class="ir hi">凯达弗拉</strong> <em class="kr">是一个瞬间杀死的诅咒，唯一能把你从诅咒中解救出来的是复活石，所以在麻瓜世界里，数据科学家也想出了一个治愈维度诅咒的方法。</em></p><p id="0b61" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">(好吧，我保证，这是本文中最后一次提到哈利波特)</p><p id="bd7f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">治愈</strong></p><figure class="jn jo jp jq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ks"><img src="../Images/fc15faa307490caf2bc662c44395e239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7lsaO0cPb4aAwuUy-U7fLw.jpeg"/></div></div></figure><p id="5193" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">降维</strong></p><p id="3a3c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">降维是将数据从高维空间转换到低维空间，以便低维表示保留原始数据的一些有意义的属性，理想情况下接近其自然维度，或者简单地说，这意味着<strong class="ir hi">降低我们数据集的维度</strong>。(<a class="ae kt" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiJgdGD0JjwAhULzDgGHV_ZAkcQFjAAegQIBBAD&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FDimensionality_reduction&amp;usg=AOvVaw3LPYfrqd7rNyW_pqgFqj31" rel="noopener ugc nofollow" target="_blank">维基百科</a>参考)</p><p id="8012" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">降维的优势:</p><ol class=""><li id="9233" class="kd ke hh ir b is it iw ix ja kf je kg ji kh jm ku kj kk kl bi translated">它减少了数据集的维度，因此减少了存储空间。</li><li id="dd65" class="kd ke hh ir b is km iw kn ja ko je kp ji kq jm ku kj kk kl bi translated">它减少了计算时间，因为更少的维数意味着需要更少的计算，这意味着算法比以前训练得更快。</li><li id="dd78" class="kd ke hh ir b is km iw kn ja ko je kp ji kq jm ku kj kk kl bi translated">多重共线性减少。</li><li id="66a7" class="kd ke hh ir b is km iw kn ja ko je kp ji kq jm ku kj kk kl bi translated">模型的准确性也可能提高。</li><li id="59e4" class="kd ke hh ir b is km iw kn ja ko je kp ji kq jm ku kj kk kl bi translated">它有助于更容易地可视化数据并在我们的数据集中识别有意义的模式，因为在1维、2维或3维空间中可视化比在更多维度中更容易。</li></ol><p id="21b2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">降维分为两种，<strong class="ir hi">特征选择</strong>和<strong class="ir hi">特征提取</strong>。</p><p id="4161" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们简要探讨一些降维技术。</p><p id="fe71" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先，特征选择是只选择重要的特征或从数据集中剔除不太有用的特征。</p><p id="6548" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一些特征选择方法是</p><ul class=""><li id="af08" class="kd ke hh ir b is it iw ix ja kf je kg ji kh jm ki kj kk kl bi translated"><strong class="ir hi">删除那些具有大量缺失值</strong>(超过50–60%)的特征，因为它将没有足够的信息提供给我们。我们可以设置一个特定的阈值，如果丢失的值超过这个阈值，那么我们将删除所有这些特征。</li><li id="9cfe" class="kd ke hh ir b is km iw kn ja ko je kp ji kq jm ki kj kk kl bi translated"><strong class="ir hi">去除相关性高的特征。</strong>两个变量之间的高度相关性意味着两者都携带相似的信息，放弃其中任何一个都不会对我们的结果产生太大影响。我们可以使用pandas的corr()函数找到我们的数据帧的相关性。我们可以设置一个阈值，如果相关系数超过这个阈值，我们就可以放弃它。</li><li id="9c47" class="kd ke hh ir b is km iw kn ja ko je kp ji kq jm ki kj kk kl bi translated"><strong class="ir hi">使用随机福里斯特的特征选择</strong> —随机福里斯特具有提供特征重要性度量的内置特征，但在此之前，我们需要将我们的数据转换成数字形式，因为随机福里斯特只接受数字输入。(注意——删除ID变量和目标变量)</li></ul><figure class="jn jo jp jq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kv"><img src="../Images/5a594cb61fa1c8a343a85955a6ebdbd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pRcAhjED33Nr8cMA.PNG"/></div></div></figure><p id="60bd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将从scikit-learn库中导入Random Forrest回归器。然后，我们将数据框放入模型中。然后，在“feature_importances_”属性的帮助下，我们可以找到特性的重要性，并将它们绘制出来以便可视化。</p><figure class="jn jo jp jq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kw"><img src="../Images/5bdd5bb8d5894b5810b116a34d9b3889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zpfF3c5pXkCIxLBy.PNG"/></div></div></figure><p id="2c08" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们绘制了所有特性中最重要的15个特性。</p><p id="e523" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后，我们可以删除那些不太重要的特征。</p><p id="c88d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">(<em class="kr">注意——在进行特征选择时，请牢记领域知识，因为这些技术容易丢失信息。</em>)</p><p id="daa4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们将研究一些<strong class="ir hi">特征提取</strong>方法</p><p id="acf9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 1。PCA —主成分分析</strong></p><p id="e82b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">PCA是一种<strong class="ir hi">线性降维算法</strong>，它帮助我们从现有的一大组变量中提取一组新的变量，这些变量被称为<strong class="ir hi">主成分</strong>。</p><p id="114a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">以这样的方式提取主成分:第一个主成分解释数据集中的最大方差，然后第二个主成分试图解释数据集中的剩余方差，并且与第一个主成分不相关，然后第三个主成分试图解释前两个主成分没有解释的方差，依此类推。</p><p id="d46b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以基本上，主成分分析所做的是，找到原始变量的最佳线性组合，使新变量的方差或分布最大。</p><figure class="jn jo jp jq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kx"><img src="../Images/18dea8840f5f4555037e1f2618589032.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*b_KyaI0jBaqRaygb.PNG"/></div></div></figure><p id="9cf5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上面的曲线量化了总方差的多少包含在前N个分量中。上面，我们可以看到，数字的前10个组成部分包含了大约75%的总方差，而我们需要大约20-25个组成部分来描述接近90%的方差。</p><p id="cd44" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了观察组件的分布，让我们使用sklearn库中的iris数据集。然后，我们将从sklearn库中导入PCA，让我们选择3个组件，然后拟合和转换数据集。之后，我们将使用matplotlib的散点图绘制组件。</p><figure class="jn jo jp jq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ky"><img src="../Images/f3dbc4d6069cfa8ba77ceb076063556f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SKdYyuRzkB7DQHYz.PNG"/></div></div></figure><p id="9530" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 2。</strong> <strong class="ir hi"> (t-SNE) t分布随机邻居嵌入</strong></p><p id="9782" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">它是一种用于探索高维数据的<strong class="ir hi">非线性降维算法</strong>。它根据数据点与特征的相似性来发现数据中的模式。点的相似性被计算为点A选择点B作为其邻居的条件概率。</p><p id="1ab2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后，它试图最小化高维空间和低维空间中这些条件概率之间的差异，以便在低维空间中完美地表示数据点。</p><p id="a82a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，为了可视化组件，让我们从sklearn库中导入TSNE并拟合数据。与其他方法相比，TSNE有其自身的优势，其中超参数是最重要的方法之一。我们可以对其进行微调，以获得最大收益。</p><figure class="jn jo jp jq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kx"><img src="../Images/f65883bcb3a69da6407b10461e987832.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z1CEF_t7YMF-9FPW.PNG"/></div></div></figure><p id="d3fb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">正如我们可以观察到的，t-SNE输出提供了比PCA和其他线性降维模型更好的结果。这是因为线性方法(如经典缩放)不擅长模拟弯曲流形。</p><p id="cf2d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">降维的一些缺点是，它可能会导致一些数据丢失，有时结果并不总是可视化的最佳结果，这使得很难解释。</p><p id="6ccc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们已经讨论了维数灾难的概念和一些降维方法。</p><p id="01b2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢阅读，继续学习。</p><p id="72e9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你觉得这篇文章很有帮助，那么请在LinkedIn上关注我。</p><p id="bd88" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">埃文斯科</strong></p></div></div>    
</body>
</html>