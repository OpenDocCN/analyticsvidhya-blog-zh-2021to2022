<html>
<head>
<title>Regularized Regression: When your Linear Regression model overfits.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正则化回归:当你的线性回归模型过度拟合时。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/regularized-regression-when-your-linear-regression-model-overfits-e717e23549d5?source=collection_archive---------15-----------------------#2021-04-11">https://medium.com/analytics-vidhya/regularized-regression-when-your-linear-regression-model-overfits-e717e23549d5?source=collection_archive---------15-----------------------#2021-04-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="af35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们深入研究什么是正则化回归之前，让我们理解一下高偏差和过度拟合意味着什么。</p><p id="f080" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们举一个简单的例子:如果我们请Sachin Tendulkar(印度板球运动员)解决一个数据科学问题，会发生什么？他将无法做好这件事？这是因为他太了解板球了，因此无法理解或解决不属于他的领域的问题。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/e7e6bab9c622635af7ac417fc1725906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UoA5wPD2HxBRqqrciSEGSw.jpeg"/></div></div></figure><p id="f386" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们用这个例子来简单地理解过度拟合。</p><p id="6e74" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jo">过度拟合是指当模型对数据理解得如此之好、如此之近，以至于在向其提供另一个数据集时，它也拾取了噪声，因此无法拟合，或者预测未来的效率低下。</em>T3】</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jp"><img src="../Images/c787ded6ffe2d23cd216078c62eb0b97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FjBuHirmlKUihtBEei0BrQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">左边的图展示了一个完美拟合的模型，右边的图展示了该模型如何通过穿过数据集中的每一点来很好地拾取数据</figcaption></figure><p id="bce0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们来理解高偏差:当有太多过于复杂的模型参数，并且一些参数比其他参数更重要时，就会出现高偏差。</p><p id="49cb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jo">高偏差导致过度拟合，从而降低精度。</em> </strong></p><p id="69c7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们知道了什么是过度拟合:让我们来看看用于改善它的回归分析方法。</p><h1 id="78ce" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">什么是正规化？</h1><p id="51ff" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">这是一种对回归系数/参数应用约束的方法，方法是通过减小回归系数/参数的大小并使其向零收缩，以防止过度拟合。</p><p id="af0a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">回归有两种类型:</p><ol class=""><li id="e665" class="kx ky hh ig b ih ii il im ip kz it la ix lb jb lc ld le lf bi translated"><strong class="ig hi"> <em class="jo">拉索回归</em> </strong></li><li id="1a5c" class="kx ky hh ig b ih lg il lh ip li it lj ix lk jb lc ld le lf bi translated"><strong class="ig hi"> <em class="jo">脊回归</em> </strong></li></ol><p id="48d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了解释这些，我将使用数据集:CarPrice</p><h1 id="3855" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">1.套索回归:</h1><p id="29c0" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">这也被称为L1范数。套索回归的一大优势不仅仅是一种对参数进行惩罚的回归技术，还可以将参数缩小到零，从而有助于理解重要的特征。套索回归会通过移除过多的要素使模型变得过于简单。</p><p id="ceb8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从数学上来说，这可以最小化:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ll"><img src="../Images/5ebf002d42ad24db456399431d33ebed.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*l6x8uhdnyXFRfq4O6HCIsA.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lm"><img src="../Images/575bafdd5aa382241b0f80fc2d48493f.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*rKu4ndRErRS_LhGzwIue3A.png"/></div></figure><p id="5151" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jo">对于lasso，这个区域是一个菱形，因为它约束了系数的绝对值。因为约束在每个轴上都有角，所以椭圆通常会在某个轴上与约束区域相交。</em></p><p id="a4f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看看如何做到这一点:</p><pre class="jd je jf jg fd ln lo lp lq aw lr bi"><span id="6ec4" class="ls jv hh lo b fi lt lu l lv lw">#import libray<br/>from sklearn.linear_model import Lasso</span><span id="eda0" class="ls jv hh lo b fi lx lu l lv lw">#Make the instance<br/>lasso  = Lasso(alpha=0.1 , max_iter= 3000)</span><span id="5c3f" class="ls jv hh lo b fi lx lu l lv lw">#Fit the model<br/>lasso.fit(X_train, y_train)</span></pre><p id="6f26" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，我假设超参数α为0.1，这可以通过强力网格搜索交叉验证或贝叶斯模型来选择。</p><pre class="jd je jf jg fd ln lo lp lq aw lr bi"><span id="9541" class="ls jv hh lo b fi lt lu l lv lw">#Taking the predicted dependent variable<br/>y_pred_l = lasso.predict(X_test)</span><span id="763d" class="ls jv hh lo b fi lx lu l lv lw">#Plotting against the actual value<br/>plt.plot(10**(y_pred_l))<br/>plt.plot(np.array(10**(y_test)))<br/>plt.legend(["Predicted","Actual"])<br/>plt.show()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ly"><img src="../Images/4991fa0550916d6daa1a06582d070fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*qnH0E26nDzDEMqPPaIKIEA.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">实际和预测之间有很大的差异。</figcaption></figure><p id="4f4c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">查看系数将使我们更好地理解特征选择。</p><pre class="jd je jf jg fd ln lo lp lq aw lr bi"><span id="087f" class="ls jv hh lo b fi lt lu l lv lw">lasso.coef_</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lz"><img src="../Images/e6f1e9d61fe75dfc1f48848b50b3ff66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MEMq8vZ-kEH2Z3mgaV68sw.png"/></div></div></figure><p id="97c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">看看大多数系数是如何变成0的，除了少数几个。这给了我们一个好主意，哪些是最重要的参数，从而最直接地与结果预测相关。</p><h1 id="8f2b" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">2.岭回归:</h1><p id="3be1" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">这被称为L2规范。这种回归缩小了系数，并使它们恶化，但不是恶化到0。当存在共线特征时，它给出比L1范数更稳定的性能。因此，它给出了优于L1范数的性能。</p><p id="9528" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从数学上讲，RSS术语可以写成:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ma"><img src="../Images/aa32d93b60736715eef8efed07b1c972.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*DUn5KJzfQQ_F7mK_t2te_w.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mb"><img src="../Images/aa54f03defff727c3197c568388db942.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*KbLeGVNTPM_VlqchWwltTQ.png"/></div></figure><p id="61d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于山脊，这个区域是一个圆，因为它限制了系数的平方。因此，相交一般不会出现在轴上，因此系数估计通常是非零的。</p><pre class="jd je jf jg fd ln lo lp lq aw lr bi"><span id="5f9f" class="ls jv hh lo b fi lt lu l lv lw">#import library<br/>from sklearn.linear_model import Ridge</span><span id="4bd8" class="ls jv hh lo b fi lx lu l lv lw">#Create instance<br/>ridge = Ridge(alpha=0.1 , max_iter= 3000)</span><span id="349e" class="ls jv hh lo b fi lx lu l lv lw">#Fit the model<br/>ridge.fit(X_train, y_train)</span></pre><p id="78d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们绘制实际值与预测值的对比图:</p><pre class="jd je jf jg fd ln lo lp lq aw lr bi"><span id="9d4d" class="ls jv hh lo b fi lt lu l lv lw">#Predict the y value <br/>y_pred_r= ridge.predict(X_test)</span><span id="37ee" class="ls jv hh lo b fi lx lu l lv lw">#Plot it against actual<br/>plt.plot(10**(y_pred_r))<br/>plt.plot(np.array(10**(y_test)))<br/>plt.legend(["Predicted","Actual"])<br/>plt.show()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mc"><img src="../Images/927578b91b95098dbd46383dfd5d96fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*rL8rPa8WjM7gSeA5HPUl0g.png"/></div></figure><p id="7bd2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">看看除了两点之外，大多数预测值都非常接近实际值。我们可能需要研究这些数据，以了解为什么这些数据点在预测和实际中存在差异。</p><p id="17d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看看系数，以便更好地理解</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es md"><img src="../Images/15da40717566f2ef093a86dc22bde588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7DN5QONcESgchxUC2jN5Qg.png"/></div></div></figure><p id="bd2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">看看系数是如何如此接近于0，而不是0。这就是惩罚的方式</p><p id="36a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么如何知道什么时候用什么呢？</p><ol class=""><li id="bfbe" class="kx ky hh ig b ih ii il im ip kz it la ix lb jb lc ld le lf bi translated">当我们想删除一堆我们认为不重要的特征时，使用套索。</li><li id="7f12" class="kx ky hh ig b ih lg il lh ip li it lj ix lk jb lc ld le lf bi translated">当我们只关注性能时，Ridge要稳定得多。</li><li id="a1c6" class="kx ky hh ig b ih lg il lh ip li it lj ix lk jb lc ld le lf bi translated">当相关系数具有相似值时，Ridge性能更好，而当相关系数具有一个大值而其他值很小时，Lasso性能更好。</li><li id="747a" class="kx ky hh ig b ih lg il lh ip li it lj ix lk jb lc ld le lf bi translated">我们也可以使用弹性网回归，它是套索和脊的结合。现在让我们来看看</li></ol><h1 id="38b8" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">3.弹性净回归；</h1><p id="6c03" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">弹性网是套索和脊的结合，我们结合了两者的缺点，以达到两全其美。</p></div></div>    
</body>
</html>