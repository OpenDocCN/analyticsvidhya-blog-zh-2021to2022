<html>
<head>
<title>Boosting Image Captioning Model Performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">提升图像字幕模型性能</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/boosting-image-captioning-model-performance-df57132cad05?source=collection_archive---------10-----------------------#2021-04-17">https://medium.com/analytics-vidhya/boosting-image-captioning-model-performance-df57132cad05?source=collection_archive---------10-----------------------#2021-04-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="5f08" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如何修改您的编码器、注意机制和解码器，以增加模型容量并产生更好的结果。还包括关于数据集大小和其他超参数的附加讨论。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/bbfbf54d8a75af3f2239ed6913344976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6OZT6YC36eFFc-_BiL3bzg.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">变形记II [1]</figcaption></figure><h1 id="4c18" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">回顾以前的工作</h1><p id="622b" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">在过去的两周里，Ethan Huang和我一直在努力研究、重建和改进当代CNN+LSTM图像字幕模型，以实现最终的建筑设计，使生命科学研究人员能够有效地查找以前记录的分子化合物。目前，许多旧论文包含以所谓的骨架式表示的分子图，但是这些表示不能够被计算机很好地解释，并且如果不花费过多的精力和时间，很难通过在线资源找到。随着我们过渡到一个更加数字化的社会，开发新方法来整合新旧方法变得越来越迫切。这促使科学家在21世纪初开发了国际化学标识(InChI)标签。这些独特的识别标签允许每个已知的化合物由特定的、不同长度的文本串来表示。这种分子结构的一个例子，InChI标签配对可以在下图中看到。通过以这种方式将可视图表转换为字符串，机器现在能够解析大量旧论文，并提取其中包含的有意义的化学物质，这将为当前的研究人员节省大量时间，并防止以前发表的结果消失在晦涩中。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kv"><img src="../Images/2e4fc92c430227733d0d17b147ddb349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FumwxSURWZOff1U6GekydQ.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">咖啡因的骨架配方和相应的英制标签[2]</figcaption></figure><p id="96b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">大型化工和制药公司对成功开发一种能够实现预期转化结果的模型也很感兴趣，因为它有可能大大加快R&amp;D的速度和效率，并产生更高的投资回报。出于这个原因，百时美施贵宝发布了一个数据集，其中包含240万张记录的分子结构及其相应的InChI标签的训练图像，希望有人能帮助他们产生一个解决这个问题的可靠解决方案。正是基于这些数据，我们一直在训练和建立我们的模型，并且在衡量我们的模型的成功时关注的主要评估度量是Levenshtein距离，该距离计算将预测标签转换为真实标签需要多少编辑(插入、删除或交换)。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kw"><img src="../Images/b78ccf692633b08f3c5de35dc3547ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*GQVXqwL-Xgmg_2KEbDypDw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">Levenshtein距离演示[3]</figcaption></figure><p id="73bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们之前的努力和现阶段的进展已经在我们之前的两篇博客文章(<a class="ae kx" rel="noopener" href="/analytics-vidhya/deep-learning-for-molecular-translation-b032b90750bc">第一篇</a> &amp; <a class="ae kx" rel="noopener" href="/analytics-vidhya/exploring-and-implementing-a-contemporary-cnn-lstm-image-captioning-model-17ca8e01a2b9">第二篇</a>)中有详细的记录，但是为了方便起见，我在这里简单总结一下。</p><p id="c677" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们最近的更新中，我们已经最终构建了一个基线、概念验证模型，该模型能够接受我们干净且经过处理的固定尺寸图像(256x448)并输出相应的预测字符串。我们选择检查的模型以及我们的项目所基于的模型可以在Mark Wijkhuizen的<a class="ae kx" href="https://www.kaggle.com/markwijkhuizen/tensorflow-tpu-training-baseline-lb-16-92" rel="noopener ugc nofollow" target="_blank">笔记本</a>中找到，并且具有下面描述的一般架构。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ky"><img src="../Images/b9c17893d8cd114ea976c73648a74b99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*670U1iBhcbd9A91Z-B57LA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">自动图像字幕的通用架构[4]</figcaption></figure><p id="f0f2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">截至上周，我们只能以极其精简的方式训练该模型，因为我们在管理如此大规模的数据集时遇到了一些严重的困难，并且只能在这个初始的截断训练过程中纳入非常有限的数据集。在我们的上一篇文章后不久，我们还发现，我们努力在任何有意义大小的图像集上训练我们的模型的原因之一是，我们第一次需要访问TPU而不是GPU。这需要我们将整个笔记本从Colab Pro转移到Kaggle，并更好地熟悉编码环境。虽然我们仍在努力解决管理如此规模的数据集的一些复杂性，但我们的模型能够成功地产生一个可行的(尽管不是特别明智或有意义的)预测，正如您在下面的可视化中所看到的。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kz"><img src="../Images/1e671cae532d552e2b17ac12d7195d61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-aUFnbi2zvGMDVOuo0q9Rg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">样本图像及其相应的预测和实际标签</figcaption></figure><p id="597e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">检查我们的模型在这一点上产生的输出，很明显，虽然预测字符串的一些小的子序列似乎与实际字符串匹配得相当好，但模型通常缺乏有意义的预测能力，并且陷入了一次又一次重复相同的3-gram“PBb”的循环中。虽然我们的模型在这一点上没有产生特别一致的InChI标签，但这是意料之中的，因为它还没有经过适当的训练。我们觉得这是一个非常好的中间点，因为我们有一个经过验证的模型，可以训练和执行推理，并且知道我们需要采取什么具体步骤来达到我们想要的水平。我们需要完成的第一件事是访问清理图像的完整数据集，在此基础上训练我们的模型，一旦完成，根据我们在课堂上学到的理论和概念修改模型的结构，以便对模型的功能进行重大改进。我们计划特别关注的模型部分是编码器CNN、注意力机制和与隐藏状态、嵌入维度相关的超参数。这些开发将是本文的重点，正如您将在最后看到的，对它们进行适当的修改将会极大地提高模型的容量和功能。</p></div><div class="ab cl la lb go lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ha hb hc hd he"><h1 id="0569" class="js jt hh bd ju jv lh jx jy jz li kb kc kd lj kf kg kh lk kj kk kl ll kn ko kp bi translated">完整数据集</h1><p id="5ce1" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">正如我们在总结中所讨论的，我们改进过程的第一步是弄清楚如何处理完整的数据集。尽管我们的中途模型证明了我们的管道完全完好无损，但我们希望在对架构进行任何更改之前，确保我们的模型能够在更大范围内产生合理的结果。</p><p id="b0f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此时，我们只对大约500张图像训练了模型，这只是我们所能处理的240万张图像中的一小部分。由于我们一直在使用Google Colab，对更多数据进行模型训练需要我们将图像上传到Google Drive，这一过程每100，000张图像需要大约1小时。</p><p id="ccad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们最终意识到使用Colab完全没有必要，因为我们的整个模型管道已经在Kaggle环境中了。在Kaggle中，任何笔记本都可以链接到比赛数据，这也允许更无缝的数据加载过程。事实上，这就是我们最终尝试的。用Pandas读取数据的代码如下所示(如果我们正在调试，我们选择只读取前1000个条目)。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es et"><img src="../Images/f2db94fd7e8c8020f2a7b17608129133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YyiBG24kx_osVk1nhV6ZxA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">加载训练数据</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lm"><img src="../Images/5e84da908c763a434a1b818b6102c765.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*6C9wP7P2SDq1ptGE6I9mqw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">前3行训练数据</figcaption></figure><p id="b891" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于数据按照图像ID、InChI标签和InChI标签长度整齐地组织起来，将图像转换为TFRecords格式是一个相对简单的过程，这要归功于Mark神奇的<a class="ae kx" href="https://www.kaggle.com/markwijkhuizen/advanced-image-cleaning-and-tfrecord-generation" rel="noopener ugc nofollow" target="_blank">探索性数据分析笔记本</a>。我们不会在这篇博文中详细介绍这个转换过程，但是我们已经在GitHub资源库中提供了我们所有的代码，在适当的地方引用了Mark的笔记本，这个资源库可以在文章的结尾找到。</p><p id="a54a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在所有240万张图像(230万次训练，10万次验证)被转换为TFRecords(大约4小时的过程)后，我们将它们上传到谷歌云存储(GCS)，以便数据可以与TPU的使用兼容。我们选择将图像分成40，000个图像的批次，这样我们总共有60个批次(57个训练和3个验证)。至关重要的是，我们公开了这个数据集，这样我们就可以从Kaggle环境中访问它。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ln"><img src="../Images/06fd02e34d1b4251eab424b2283fdcdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eGB_cCfLvYCS2eUV1guJHg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">TFRecords格式的10批训练图像</figcaption></figure><p id="d23c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦数据上传到GCS，我们所要做的就是把它加载到我们的Kaggle笔记本上。在认证了我们的Google Cloud凭证之后，我们创建了一个数据路径，并使用Glob提取图像。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lo"><img src="../Images/065bcc5a48019965b923c290455f1358.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*oyHWGnaR18RhmDBE_j543g.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lp"><img src="../Images/eebac7e0e5c7b119b1cec41f5e860bc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5z-uZ7LBpqnliStJtHqXjg.png"/></div></div></figure><p id="e0f5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们希望这一部分对任何希望在未来使用Kaggle的免费TPU来训练模型的人都有用(同样，我们所有的代码都可以在GitHub库中找到，以便仔细检查)。虽然我们花了几天时间来熟悉这种新的数据加载方法，但学习如何使用Kaggle环境和Google云存储最终被证明是非常有价值的。如果没有这些工具，我们就无法正确管理我们拥有的大量数据。</p></div><div class="ab cl la lb go lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ha hb hc hd he"><h1 id="43b4" class="js jt hh bd ju jv lh jx jy jz li kb kc kd lj kf kg kh lk kj kk kl ll kn ko kp bi translated">CNN编码器</h1><p id="6032" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">在我们构建的基线模型中，编码器简单地将我们选择的导入CNN应用于固定维度的图像(256x448 ),并使用每个通道的展平特征图输出作为嵌入的序列数据。本质上，CNN本身充当编码器，并且嵌入空间的维度是展平特征图的大小。对于我们的原始模型，我们使用EfficientNetB0(带“嘈杂学生”初始化),因为它的Imagenet精度与参数数量之比很好，但是，从下图中可以看出，该模型的后续版本实现了更高的精度，而不需要过多的参数。在我们之前的项目中，我们对木薯叶图像进行分类，我们使用ResNet50作为我们的基础模型，但是，正如你可以清楚地看到的，该模型比EfficientNetB4有更多的参数，与近83%的准确率相比，只能达到大约76%的准确率。因此，在决定新的编码器CNN时，我们选择坚持使用EfficientNet系列分类器。在网上做了一些研究，阅读了其他人试图实现不同CNN的评论，并观察了下面的图表后，我们决定将EfficientNetB4作为我们模型的最佳选择。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/d7e2f17665a2989b85a90080da20c563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*B1obmQ22sRJkYAmlTF2gTw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">不同型号在Imagenet上的性能与其尺寸的关系图[5]</figcaption></figure><p id="b512" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正确地对我们的原始模型进行这种修改需要我们完全理解和确认编码器和解码器中隐藏状态的维度的兼容性，以及它们之间的交互是如何通过注意力来调节的(这将在下一节中进一步讨论)，但是，一旦它被成功实现，我们就可以看到模型的性能有了相当大的改善，这可以通过验证集上的Levenshtein距离(val_lsd)来衡量。这可以从下面的输出中看出，我们实现了11.3的val_lsd。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lr"><img src="../Images/5e0955e7ea7803f38c76de379e456ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Zvr-ZV0wyCvjLIVUyRs3w.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">高效的模型性能网络4</figcaption></figure><p id="3b73" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，这些结果是在经过20个时期而不是10个时期的训练后测得的，但在训练过程中，这些结果比以前更加稳定，而且，正如您可以从持续稳定下降的损失分数中看出的那样，该模型非常有希望继续建立。</p></div><div class="ab cl la lb go lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ha hb hc hd he"><h1 id="5cfa" class="js jt hh bd ju jv lh jx jy jz li kb kc kd lj kf kg kh lk kj kk kl ll kn ko kp bi translated">注意机制</h1><p id="77b4" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">在修改了我们对图像分类器的选择后，我们决定实验的下一个组件是注意力机制。我们的中途模型展示了下图中常用的Bahdanau注意力模式。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ls"><img src="../Images/a798484f1733c90347ddca7395f5f484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NrkI96B2wQ1JXcJroLMAvw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">Bahdanau注意用于中途模式</figcaption></figure><p id="f8d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了提供一个高层次的概述，注意力的目标是找出编码器的每个隐藏状态对解码器输出的影响程度。为此，我们计算一个分数来量化编码器的隐藏状态和解码器的最新输出之间的关系。理想情况下，我们希望在进行预测时，只考虑对输出有明显影响的隐藏状态。</p><p id="2e10" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦我们有了编码器的每个隐藏状态的分数，我们就可以对它们进行软最大化(确保这些分量加起来等于1，较大的分量对应于与解码器的输出更相关的贡献者)来确定注意力权重。然后，通过将注意力权重应用于解码器的隐藏状态，获得告知解码器如何考虑先前信息的上下文向量。穆拉特·卡拉卡亚的精彩笔记详细展示了所有这些，并总结如下:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lt"><img src="../Images/2365188a6512a50636db13e2d7ff8dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*vIr2mkDkKQ2Xbt1RXS6xOw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">注意力是如何工作的[6]</figcaption></figure><p id="912b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以想象，有很多方法可以计算出决定注意力权重的分数。我们决定对Luong注意力进行实验，看看它与Bahdanau注意力相比如何。这两种机制之间的唯一区别是Luong通常使用某种形式的点积来计算分数(因此是乘法风格)，并且上下文向量在嵌入到隐藏状态之前与解码器输入相结合，而在Bahdanau的情况下，上下文向量直接连接到嵌入的解码器输入。所有这些如下图所示:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lu"><img src="../Images/c6579959dd271771e92cb7af5a54c554.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*8YSMa63F4-5sc9twUMRW5A.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><a class="ae kx" href="https://www.kaggle.com/kmkarakaya/encoder-decoder-with-bahdanau-luong-attention" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/kmkarakaya/encoder-decoder-with-bahda nau-Luong-attention</a></figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lv"><img src="../Images/6084b6a9400d864bc6908b24aea25c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A-JDYSx1W850a8ioTmdalA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">Luong和Bahdanau注意机制图[7]</figcaption></figure><p id="6e2b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不幸的是，Luong注意机制最终降低了我们模型的容量。虽然训练过程是稳定的，但我们的验证指标在第10个纪元之前就开始停滞不前了。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lw"><img src="../Images/6f5a461f2e4f5127391e92484bb0b376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uEOK-D9a1iWdpcyL0TelxQ.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">Luong训练指标:从第4时段到第5时段验证损失增加</figcaption></figure><p id="0776" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最终，Bahdanau注意力的结构方式使得它比Luong注意力有更多可学习的参数，这对于我们使用LSTM解码器的模型是至关重要的(我们将在结论中讨论为什么)。</p></div><div class="ab cl la lb go lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ha hb hc hd he"><h1 id="25f1" class="js jt hh bd ju jv lh jx jy jz li kb kc kd lj kf kg kh lk kj kk kl ll kn ko kp bi translated">超参数调谐</h1><p id="59e8" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">最后，我们决定研究控制训练过程的一些最重要的超参数:注意单元的数量和解码器的维数。我们怀疑增加这两个量会让我们的模型以一种有意义的方式提高它的能力，但是我们担心增加它们可能会导致额外的训练时间。</p><p id="93c8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">幸运的是，由于我们的自定义训练循环的设置方式和TPU提供的效率，同时增加这两个数量导致平均验证Levenshtein距离的明显改善，而总训练时间仅略有增加。具体来说，我们发现增加注意单元的数量会导致稍好的表现，而解码器维度所扮演的角色要重要得多。正如我们在上面看到的，将解码器尺寸减小2倍会导致更大的Levenshtein距离，正如我们将在下一节看到的，将解码器尺寸增加2倍会产生最佳结果。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lx"><img src="../Images/234aa0f22252b21e3cfa60a6a2df76f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bB6xVRJm3LBFsAKinnZZ1A.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">将解码器尺寸从512减小到256</figcaption></figure></div><div class="ab cl la lb go lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ha hb hc hd he"><h1 id="eaa3" class="js jt hh bd ju jv lh jx jy jz li kb kc kd lj kf kg kh lk kj kk kl ll kn ko kp bi translated">总结和最终表现</h1><p id="4ffd" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">概括地说，我们从研究和复制一个<a class="ae kx" href="https://www.kaggle.com/markwijkhuizen/tensorflow-tpu-training-baseline-lb-16-92" rel="noopener ugc nofollow" target="_blank"> public Kaggle笔记本</a>开始，它实现了22.2的平均Levenshtein距离。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ly"><img src="../Images/effb54d8a5f901c12ee8fad4b3b255f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w5aFgxSjEyqZ-vj_6d-zNA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><a class="ae kx" href="https://www.kaggle.com/markwijkhuizen/tensorflow-tpu-training-baseline-lb-16-92" rel="noopener ugc nofollow" target="_blank">马克的成绩</a></figcaption></figure><p id="e9b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在充分了解我们复制的基础模型的各个组件并对包括但不限于新图像分类器、注意力机制和超参数调整的领域进行实验后，我们最终能够在验证数据上实现7.4的平均Levenshtein距离，这是一个巨大的成功！</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lz"><img src="../Images/7cac6e77559ac3e800c142cb55dda670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CPRwVNQZdlG3SelCABIGQQ.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">决赛成绩</figcaption></figure><p id="53e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们观察我们对最终模型进行训练的大约30个时期的训练损失和训练准确性的图，似乎我们的模型非常接近达到其全部潜力。验证损失和Levenshtein距离讲述了一个类似的故事，并表明我们可能已经从我们的模型中获得了一些额外的性能，因为验证指标仍在改进(尽管在这一点上只是略有改进)。</p><p id="e1d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ma">请注意，我们试图用40个小时来训练最终模型，但由于Kaggle的最大连续运行时间为9个小时，我们只能完成大约30个时代。</em></p><div class="jd je jf jg fd ab cb"><figure class="mb jh mc md me mf mg paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><img src="../Images/948d2ee57323ab89140846cd33759b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*k1QqKkqFG-nDEzr5zlA9XA.jpeg"/></div></figure><figure class="mb jh mh md me mf mg paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><img src="../Images/800381d4e9f9f977ed55d0a71bc99f6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*X0BwYaq5En_zvFJUkVydpg.jpeg"/></div><figcaption class="jo jp et er es jq jr bd b be z dx mi di mj mk translated">最终模型的培训指标</figcaption></figure></div><p id="8b02" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，让我们来看一个预测的例子:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ml"><img src="../Images/1724fc2a98e7e64e3145f2caaa3b4139.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jN1iLZK7iccPiiyx-g5iUQ.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">预测示例，Levenshtein距离为17</figcaption></figure><p id="f369" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们所看到的，我们的模型在避免n-gram重复和产生与实际InChI标签长度一致的输出方面做得更好。由于标签的长度，这个特定的例子具有相对较高的Levenshtein延迟(与平均值7.4相比)，但是仍然展示了我们的最终模型如何能够很好地模拟长标签的一般结构，并且在此过程中有一些错误。</p></div><div class="ab cl la lb go lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ha hb hc hd he"><h1 id="a46c" class="js jt hh bd ju jv lh jx jy jz li kb kc kd lj kf kg kh lk kj kk kl ll kn ko kp bi translated">结束语</h1><p id="c19a" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">虽然我们很高兴达到了10以下的Levenshtein距离，但我们仍然看到了很大的改进空间。迄今为止，Kaggle上的最高公开表演仅低于2，因此，鉴于我们距离比赛结束还有一个多月的时间，我们相信我们将能够继续向这样的分数迈进。不幸的是，即使我们能够对我们为这个项目考虑的当前基线模型进行有意义的改进，看起来我们可能会达到当前框架的容量和预测能力的极限。由于编码器对于像我们这样具有简单编码器/解码器框架的模型来说至关重要，我们可以继续试验不同的CNN模型来用作编码器，并观察它们的表现，但我们认为，为了看到性能的下一次大幅提升，我们需要重新配置整个序列数据分析过程，并更新模型，以使用更现代的东西，如转换器，甚至可以直接从嵌入式编码器隐藏状态到预测输出的BERT架构。我们目前的模型有一些稍微过时的组件，如LSTM细胞和直接的Bahdanau注意力，因此，为了与顶级条目竞争，我们需要升级到最复杂的技术。鉴于我们面临的时间限制，这不是我们能够在此时进行调查并成功实施的事情，但我们对在下个月进一步探索这一想法的可能性感到兴奋。</p><p id="8406" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，我们遇到了一些有趣的想法，即可能有一个单独的模型尝试学习“InChI的语法”，然后使用它来调节来自我们另一个模型的预测输出，以确保产生的所有字符串都与我们试图预测的标签的底层语法结构兼容。进一步考虑所有这些事情会很有趣，我们期待在今年夏天开始时进一步开发我们的模型。</p><p id="af0a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">非常感谢您关注我们的进展！</p><p id="9591" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个项目的代码可以在这里找到。</p></div><div class="ab cl la lb go lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ha hb hc hd he"><p id="8a8d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">关于作者:Ethan Huang和我(Griffin McCauley)都是布朗大学应用数学专业的大三学生。此外，我是男子大学越野队和田径队的成员，也是经济系的助教，伊森是NFL平台Starting Eleven的主席和学术导师。</p><p id="955b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">鸣谢:该项目基于百时美施贵宝-分子翻译Kaggle竞赛(<a class="ae kx" href="https://www.kaggle.com/c/bms-molecular-translation" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/bms-molecular-translation</a>)，是数据科学倡议硕士项目中布朗大学数据2040课程(数据科学中的深度学习和专题)的一部分。</p><p id="8631" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">参考资料:</p><ol class=""><li id="343c" class="mm mn hh ig b ih ii il im ip mo it mp ix mq jb mr ms mt mu bi translated"><a class="ae kx" href="https://uploads6.wikiart.org/images/m-c-escher/metamorphosis-ii.jpg" rel="noopener ugc nofollow" target="_blank">https://uploads 6 . wiki art . org/images/m-c-escher/变态-ii.jpg </a></li><li id="da80" class="mm mn hh ig b ih mv il mw ip mx it my ix mz jb mr ms mt mu bi translated">【https://iupac.org/100/stories/what-on-earth-is-inchi/ T4】</li><li id="322d" class="mm mn hh ig b ih mv il mw ip mx it my ix mz jb mr ms mt mu bi translated"><a class="ae kx" href="https://www.cuelogic.com/blog/the-levenshtein-algorithm" rel="noopener ugc nofollow" target="_blank">https://www.cuelogic.com/blog/the-levenshtein-algorithm</a></li><li id="d763" class="mm mn hh ig b ih mv il mw ip mx it my ix mz jb mr ms mt mu bi translated"><a class="ae kx" href="https://www.analyticsvidhya.com/blog/2018/04/solving-an-image-captioning-task-using-deep-learning/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/04/solving-an-image-captioning-task-using-deep-learning/</a></li><li id="0399" class="mm mn hh ig b ih mv il mw ip mx it my ix mz jb mr ms mt mu bi translated"><a class="ae kx" href="https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2019/05/efficient net-improving-accuracy-and . html</a></li><li id="3cc5" class="mm mn hh ig b ih mv il mw ip mx it my ix mz jb mr ms mt mu bi translated"><a class="ae kx" href="https://www.kaggle.com/kmkarakaya/encoder-decoder-with-bahdanau-luong-attention" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/kmkarakaya/encoder-decoder-with-bahda nau-Luong-attention</a></li><li id="b8e4" class="mm mn hh ig b ih mv il mw ip mx it my ix mz jb mr ms mt mu bi translated"><a class="ae kx" href="http://cnyah.com/2017/08/01/attention-variants/" rel="noopener ugc nofollow" target="_blank">http://cnyah.com/2017/08/01/attention-variants/</a></li></ol></div></div>    
</body>
</html>