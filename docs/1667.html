<html>
<head>
<title>Subword Techniques for Neural Machine Translation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经机器翻译的子词技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/subword-techniques-for-neural-machine-translation-f55e4506a728?source=collection_archive---------10-----------------------#2021-03-12">https://medium.com/analytics-vidhya/subword-techniques-for-neural-machine-translation-f55e4506a728?source=collection_archive---------10-----------------------#2021-03-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/c560b1176d180ddd250b65ed935343d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*ALFCq4JyyqGJzeH7JjOKww.png"/></div></figure><p id="48c0" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">神经机器翻译(NMT)是当前最先进的机器翻译技术，可以产生流畅的翻译。然而，NMT模型受超词汇(OOV)和生僻字问题的影响，降低了翻译质量。OOV词是没有在语料库中出现的词，稀有词是在语料库中出现很少的词。当翻译这样的未知单词时，这些单词被替换为UNK标记。因此，翻译变得更糟，因为这些无意义的标记通过破坏句子结构增加了歧义。</p><p id="5ce2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">字符分割是机器翻译中使用的一种技术，用于避免单词级翻译的缺点。字符分割的主要优点是它可以模拟任何字符组合，从而能够更好地模拟罕见的形态变体。然而，由于缺少重要信息，改进可能不太显著，因为字符级别更细粒度。</p><p id="8cc8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">为了缓解这些问题，Sennrich等人(2016)引入了通过提供更有意义的表示将单词分割成子单词单元序列的概念。作为子词分割的例子，考虑单词“looked”。这个词可以拆分为“看”和“ed”。换句话说，用两个向量来表示“看了”。因此，即使这个单词是一个未知单词，该模型仍然可以通过将它视为一系列子单词单元来准确地翻译该单词。</p><p id="4a26" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">随着自然语言处理的发展，各种子词切分算法被提出。本文全面描述了以下子字技术。</p><ol class=""><li id="8496" class="jj jk hh in b io ip is it iw jl ja jm je jn ji jo jp jq jr bi translated">字节对编码(BPE)</li><li id="57bb" class="jj jk hh in b io js is jt iw ju ja jv je jw ji jo jp jq jr bi translated">单语法语言模型</li><li id="5dc9" class="jj jk hh in b io js is jt iw ju ja jv je jw ji jo jp jq jr bi translated">子字采样</li><li id="2478" class="jj jk hh in b io js is jt iw ju ja jv je jw ji jo jp jq jr bi translated">BPE辍学者</li></ol><h1 id="8d88" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">字节对编码(BPE)</h1><p id="955f" class="pw-post-body-paragraph il im hh in b io kv iq ir is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji ha bi translated">Sennrich等人(2016)提出了这种基于字节对编码压缩算法的分词技术。这是使NMT模式能够翻译生僻字的有效途径。它将单词拆分成字符序列，并迭代地将最常见的字符对组合成一个。</p><p id="f2da" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">下面是BPE算法获得子字的步骤。</p><p id="bb8b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="la">第一步:</em> <em class="la">初始化词汇</em></p><p id="9218" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="la">第二步:对于词汇表中的每个单词，附加单词结束标记&lt; /w &gt; </em></p><p id="6c7a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="la">第三步:将单词拆分成字符</em></p><p id="de01" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="la">步骤4:在每次迭代中获取最频繁出现的字符对，并将它们合并为一个标记&amp;将这个新标记添加到词汇表中</em></p><p id="70d1" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="la">步骤5:重复步骤4，直到完成期望数量的合并操作或者达到期望的词汇大小</em></p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lb"><img src="../Images/7dd198992f8e1fbcf13bcdc1a3336021.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*zQ-hg9NcFpSBoUmjWtz9Xg.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">学习BPE操作(森里奇等人，2016年)</figcaption></figure><h1 id="6cba" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">单语法语言模型</h1><p id="75cd" class="pw-post-body-paragraph il im hh in b io kv iq ir is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji ha bi translated">Kudo (2018)提出了基于单字语言模型的子词分割算法，该算法输出多个子词分割及其概率。该模型假设每个子词独立出现。子词序列x=(x1，…，xM)的概率是通过乘以子词出现概率p(xi)获得的。</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lk"><img src="../Images/a9f86b4cdb635bb3185142e2566be218.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/0*CKrQb9q7cDoIpTPk"/></div></figure><p id="74a7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这里，V是预先确定的词汇。句子X的最可能的分段x*由下式给出，</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es ll"><img src="../Images/c1cf97fbff832a87ac7b394f5dfee577.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/0*qEbgANKDKX0-Qalw"/></div></figure><p id="78c8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">S(X)是使用句子X获得的一组分割候选。x*是使用<a class="ae lm" href="https://en.wikipedia.org/wiki/Viterbi_algorithm" rel="noopener ugc nofollow" target="_blank">维特比算法</a>获得的。</p><p id="f43f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">子词出现概率p(xi)是使用<a class="ae lm" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">期望最大化(EM)算法</a>通过最大化下面的似然性l来估计的</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/4953b8f75c8e646c3c273a1e53c66bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/0*jEgW2ZvM2boBKliL"/></div></figure><p id="578b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">以下步骤描述了获得具有期望大小的词汇V的过程。</p><p id="1a68" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="la">第一步:初始化一个相当大的种子词汇表。</em></p><p id="3f6a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">第二步:定义想要的词汇量。</p><p id="a3bd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="la">步骤3:通过固定词汇表，使用EM算法优化子词出现概率。</em></p><p id="be40" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="la">步骤4:计算每个子字的损失。子词的丢失描述了当该子词从词汇表中移除时，上述似然性L的减少。</em></p><p id="1d7a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="la">步骤5:按损失排序子词，保留前n%的子词。保持子词只有一个字符，以避免超出词汇表的问题。</em></p><p id="3f6e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="la">第六步:重复第三步到第五步，直到达到第二步定义的所需词汇量。</em></p><p id="c1d8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">准备种子词汇表最常见的方法是使用语料库中最常见的子字符串和字符。这种基于单字语言模型的子词分割由字符、子词和词组成。</p><h1 id="9b54" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">子字采样</h1><p id="5b4d" class="pw-post-body-paragraph il im hh in b io kv iq ir is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji ha bi translated">在这种技术中，基于单语法语言模型用多个子词分段来训练模型，并且在训练期间对这些子词进行概率采样。最佳分割是一种可用于近似采样的方法。首先，获得l-最佳分割，并且在执行l-最佳搜索之后，采样一个分割。</p><p id="f3c1" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">子字正则化有两个超参数，即采样候选的大小(l)和平滑常数(α)。理论上，设置l→∞意味着考虑所有可能的分段。但是这是不可行的，因为字符的数量会随着句子的长度成指数增长。因此，前向滤波和后向采样算法被用于采样。此外，如果α小，分布更均匀，如果α大，它倾向于维特比分割。</p><h1 id="07a7" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">BPE辍学者</h1><p id="b181" class="pw-post-body-paragraph il im hh in b io kv iq ir is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji ha bi translated">BPE-dropout是一种有效的基于BPE的子词正则化方法，它能够对特定的词进行多次分割。这使得BPE词汇表和合并表保持不变，同时改变了分段过程。这里，在每个合并步骤中，以概率p随机移除一些合并，从而为同一单词给出多个分段。以下算法描述了该过程。</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/7e57ccfa17946007953c460f7f65f922.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/0*egKNnL-aZyOLDSEP"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">BPE辍学算法(Provilkov等人，2020年)</figcaption></figure><p id="9da5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果概率为零，则子词分段等于原始BPE。如果概率为1，则子词分割等于字符分割。如果概率从0到1变化，它给出具有不同粒度的多个分割。由于该方法将模型暴露给各种子词分割，因此它提供了更好地理解词和子词的能力。BPE-辍学是一个简单的程序，因为训练可以在没有训练除了BPE和推理使用标准的BPE之外的任何分段的情况下完成。</p></div><div class="ab cl lp lq go lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ha hb hc hd he"><p id="fca2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">本文探讨了各种子词技术，以改善神经机器翻译。基于转换器架构的NMT模型的示例实现可以在<a class="ae lm" href="https://github.com/aaivu/aaivu-machine-trans-eng-sin/tree/master/src/Subword-segmentation" rel="noopener ugc nofollow" target="_blank">这里</a>找到，它使用<a class="ae lm" href="https://github.com/google/sentencepiece" rel="noopener ugc nofollow" target="_blank">例句库</a>应用基于BPE和unigram语言模型的子字采样。</p><h2 id="e031" class="lw jy hh bd jz lx ly lz kd ma mb mc kh iw md me kl ja mf mg kp je mh mi kt mj bi translated">参考</h2><p id="254b" class="pw-post-body-paragraph il im hh in b io kv iq ir is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji ha bi translated">[1] R. Sennrich，B. Haddow，A. Birch，<a class="ae lm" href="https://www.aclweb.org/anthology/P16-1162.pdf" rel="noopener ugc nofollow" target="_blank">带子词单元的生僻字的神经机器翻译</a> (2016)，计算语言学协会第54届年会</p><p id="373b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">[2] T. Kudo，<a class="ae lm" href="https://www.aclweb.org/anthology/P18-1007.pdf" rel="noopener ugc nofollow" target="_blank">子词正则化:用多个候选子词改进神经网络翻译模型</a> (2018)，计算语言学协会第56届年会</p><p id="5b4a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">[3] I. Provilkov，D. Emelianenko和E. Voita，<a class="ae lm" href="https://www.aclweb.org/anthology/2020.acl-main.170.pdf" rel="noopener ugc nofollow" target="_blank">BPE-辍学:简单有效的子词正则化</a> (2020)，计算语言学协会第58届年会</p><p id="f60e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">[4] T. Kudo和J. Richardson，<a class="ae lm" href="https://www.aclweb.org/anthology/D18-2012.pdf" rel="noopener ugc nofollow" target="_blank"> SentencePiece:用于神经文本处理的简单且语言独立的子词分词器和去分词器</a> (2018)，自然语言处理中的经验方法会议(系统演示)</p></div></div>    
</body>
</html>