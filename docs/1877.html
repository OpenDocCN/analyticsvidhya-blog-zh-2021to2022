<html>
<head>
<title>ACTIVATION FUNCTIONS IN NEURAL NETWORK</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的激活函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-in-neural-network-mathematical-intuition-with-graphs-c789401e21c7?source=collection_archive---------16-----------------------#2021-03-23">https://medium.com/analytics-vidhya/activation-functions-in-neural-network-mathematical-intuition-with-graphs-c789401e21c7?source=collection_archive---------16-----------------------#2021-03-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="f0af" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">图形的数学直觉</h2></div><p id="737b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在神经网络中使用激活函数是为了以更完美的方式映射输入和输出，并且大多数时间(大约99%的时间)我们使用非线性激活函数而不是线性激活函数，因为非线性图在映射方面比线性图做得更好。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/4fb53f4db2b76e26699e17b62f9d9ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*otSSVzCGySZ6mAN7q9N9gQ.jpeg"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated"><a class="ae kj" href="https://ibelieveai.github.io/images/Activationfuns/Linear-NonLinear.JPG" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="dc17" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">乙状结肠:</h1><p id="d438" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">Sigmoid激活函数也称为挤压函数，映射范围(0，1)内的任何输入值，因此它们可以被解释为概率，并在最终层中使用，以便更容易解释结果。Sigmoid函数也是逻辑回归和分类模型的重要组成部分。逻辑回归是对两类分类的修改，它将一个或多个实值输入转换为概率，例如客户将购买产品的概率。</p><p id="0dd1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">sigmoid函数的数学公式:</p><p id="4a6b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">f(x) = 1/(1+e^(-x))</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lh"><img src="../Images/420db6495c2d69239c43bbbf1cf41a83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_N5aWv3Fjnl5wWYp7qLoNQ.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">从德斯莫斯在线图形计算器</figcaption></figure><p id="f189" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你观察这个图，你可以看出这是一条S形曲线，它在所有点上都是连续可微的，但是如果我们观察这个函数的导数图</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lh"><img src="../Images/537130c51cbf61b4d2c80b078ee65168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wXmQi-bbXaLUCVfYxJes5g.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">从德斯莫斯在线图形计算器</figcaption></figure><p id="13a5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">梯度值对于范围-3和3是显著的，但是图形在其他区域变得平坦得多。这意味着对于大于3或小于-3的值，将具有非常小的梯度。当梯度值接近零时，网络并没有真正学习。</p><p id="bc10" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，Sigmoid函数不是围绕零对称的。所以所有神经元的输出符号相同。</p><h1 id="e50a" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">RELU:</h1><p id="2934" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">ReLU代表整流线性单元。与其他激活函数相比，使用ReLU函数的主要优点是它不会同时激活所有的神经元。</p><p id="7cd2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">f(x) = max(0，x)(即对于大于0的任何x值，它返回相同的值，而对于小于0的任何x值，它返回0)</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es li"><img src="../Images/4fac162ee5d283cf458c9a76a3036eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*9UvwWFQtlQsc9gpEKxh9_g.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">从德斯莫斯图形计算器</figcaption></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lj"><img src="../Images/a2d012ddbcb2cb6ef27a507a9cc6df20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*T3j6hrP3Pq4ScGZut8P8fQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">从德斯莫斯图形计算器</figcaption></figure><p id="205a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ReLU激活函数的问题是，如果你看梯度图的负侧，你会注意到梯度值为零。由于这个原因，在反向传播过程中，一些神经元的权重和偏差没有更新。这会产生永远不会被激活的死亡神经元。</p><h1 id="957d" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">漏水的RELU:</h1><p id="e05a" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">Leaky ReLU在ReLU激活函数中引入了一点修改不是给x的负值赋0，我们把它定义为x的一个极小的线性分量。</p><p id="ed2d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当x &lt;0 and f(x) = x when x≥0</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lk"><img src="../Images/6c901a858948b1ef70fff569760117b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FW70FldR3FIfVxl2KBWr0Q.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated"><a class="ae kj" href="https://ml-cheatsheet.readthedocs.io/en/latest/_images/leakyrelu.png" rel="noopener ugc nofollow" target="_blank">来源</a>时f(x)= 0.01x</figcaption></figure><p id="485c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当x&lt;0时，f'(x) = 0.01，当x≥0时，f'(x) = 1</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lk"><img src="../Images/183d3c96fd9cf69ac2e6ee9a08f3145e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N1v8rYiHwyPymIJqDb02hA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated"><a class="ae kj" href="https://ml-cheatsheet.readthedocs.io/en/latest/_images/leakyrelu_prime.png" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="5465" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过这个小的修改，x的负值的梯度变成一个非零值。因此，我们在那个区域不会再遇到死亡的神经元。</p><h1 id="06c6" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">指数线性单位:</h1><p id="9971" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">指数线性单位或简称ELU也是修正线性单位(r ELU)的变体，它修改函数负部分的斜率。ELU使用对数曲线来定义负值。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ll"><img src="../Images/268b23d16ef17b66921dc087c4aff397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*H7xnJP7L4JgfsgzVeIGePQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated"><a class="ae kj" href="https://ml-cheatsheet.readthedocs.io/en/latest/_images/elu_prime.png" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="0e02" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">Tanh:</h1><p id="7c13" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">双曲正切函数非常类似于sigmoid函数。唯一不同的是，它是关于原点对称的。在这种情况下，值的范围是从-1到1。因此，产出将不仅仅是积极的。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lm"><img src="../Images/6a261f5695f069f1a219083a33a64372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ThDkzJlyFUkmER6_bjbI6w.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">从德斯莫斯图形计算器</figcaption></figure><p id="4725" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">除了值域之外，双曲正切函数的所有其他性质都与sigmoid函数相同。与sigmoid类似，双曲正切函数在所有点上都是连续且可微的。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ln"><img src="../Images/cb1d9c7937af413b0aeebe1214e3bf5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zTEeV0cRqSBhTMSyHWkNWQ.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">从德斯莫斯图形计算器</figcaption></figure><p id="d741" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">与sigmoid函数相比，tanh函数的梯度更陡，通常tanh比sigmoid函数更优选，因为它以零为中心，并且梯度不限于在某个方向上移动。</p><h1 id="44e1" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">唰:</h1><p id="db7e" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">Swish的计算效率与ReLU一样高，并且在更深层次的模型上表现出比ReLU更好的性能。swish的值范围从负无穷大到正无穷大。</p><p id="ec37" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">f(x) = x*sigmoid(x)</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lo"><img src="../Images/367fa174512d08051daafe39ee9ae03b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OmOyLU1WiXjnl9Ba1Jr1gg.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">从德斯莫斯图形计算器</figcaption></figure><p id="d64c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">f'(x) = f(x) + sigmoid(x)*(1-f(x))</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lp"><img src="../Images/864e181981e2f8acf2ebbea1ee7bf2eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*swl1HpuH51HEqjJ6xESMXg.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">从德斯莫斯图形计算器</figcaption></figure><p id="ba53" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">函数的曲线是光滑的，函数在所有点上都是可微的。这在模型优化过程中是有帮助的，并且被认为是swish优于ReLU的原因之一。</p><h1 id="4adb" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">SOFTMAX:</h1><p id="0835" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">Softmax是在最终输出层中使用的激活函数，主要用于多分类问题，因为它将报告每个类的“置信度得分”。因为我们在这里处理的是概率，所以softmax函数返回的分数总和将为1。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lq"><img src="../Images/bf42614e39f76f2f3f67131bade6897e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t2FbP10uDFZrufDCYELWFA.jpeg"/></div></div></figure><p id="7032" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">任何建议和更正都是受欢迎的。</p><p id="8359" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢您阅读这篇冗长的文章。</p></div></div>    
</body>
</html>