<html>
<head>
<title>Understanding Semantic Search — (Part 2: Machine Reading Comprehension at Scale)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解语义搜索—(第二部分:大规模机器阅读理解)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/open-domain-question-answering-series-part-2-machine-reading-comprehension-at-scale-7ca0b75dbd3a?source=collection_archive---------6-----------------------#2021-08-10">https://medium.com/analytics-vidhya/open-domain-question-answering-series-part-2-machine-reading-comprehension-at-scale-7ca0b75dbd3a?source=collection_archive---------6-----------------------#2021-08-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/dfa2364c70d32597b16cd78c1848736e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63snZJv5pisF2iGXmJ6mjw.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">作者照片:从华盛顿山看到的匹兹堡市区全景(2021年7月)</figcaption></figure><p id="7e62" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在<a class="ae jr" href="https://kaushikshakkari.medium.com/open-domain-question-answering-series-part-1-introduction-to-reading-comprehension-question-1898c8c9560e" rel="noopener">之前的文章</a>中，我介绍了使用SQuAD数据集、语言模型和迁移学习的机器阅读理解模型。在这篇文章中，我将讨论如何扩展长文档的机器阅读理解模型。</p><p id="c78c" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">机器阅读理解模型的最大限制是模型的输入规模。模型不能将整个文档作为输入，因为输入大小限于某些字符。例如，使用Google BERT语言模型的机器阅读理解模型的输入大小被限制为512个字符。因此，机器阅读理解模型不能直接应用于超过512个字符的长文档。</p><p id="2513" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在本文中，我将介绍<strong class="iv hi"> <em class="js">阅读器和检索器</em> </strong>架构，它有助于克服机器阅读理解模型的上述局限性。这种架构最初是在<a class="ae jr" href="https://arxiv.org/abs/1704.00051" rel="noopener ugc nofollow" target="_blank">阅读维基百科回答开放领域问题</a>的论文中介绍的。文档被分解成文本块。一段文本不一定是一个段落或一个句子，它是文档的子集。在本文的其余部分，我将把这些文本块称为段落。检索者的工作是预测与给定问题相关的顶部段落。读者的工作是在检索者预测的相关段落上运行机器阅读理解模型。在前一篇文章中，我们看到了阅读器(BERT)是如何工作的。在本文中，我们将讨论不同的检索器算法。</p><figure class="ju jv jw jx fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es jt"><img src="../Images/a15d8ceaa134c78631dd7c9fe29fb192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z32STZ-2Ra-_ASqr_5SVJA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">来自作者的截图</figcaption></figure><p id="8b34" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">检索器算法:</strong></p><p id="cfe9" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">搜索引擎通常使用检索器算法来基于网页与用户搜索查询的相关性对网页进行排序。在我们的例子中，我们使用检索器算法对给定问题的所有文档中的段落进行排序。检索器算法将一个问题和一段文字作为输入，并计算相关性分数。</p><p id="29ba" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">检索器算法可以分为两类:</p><ol class=""><li id="5cef" class="jy jz hh iv b iw ix ja jb je ka ji kb jm kc jq kd ke kf kg bi translated">经典或稀疏检索算法(TF-IDF和BM25)</li><li id="409d" class="jy jz hh iv b iw kh ja ki je kj ji kk jm kl jq kd ke kf kg bi translated">神经或密集检索算法(DPR和斯贝特)</li></ol><p id="c615" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">经典或稀疏检索算法:</strong></p><p id="90db" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">经典的检索算法有一个假设，即一个单词在一段文本中出现的频率越高，这段文本与该单词相关的可能性就越高。因此，如果一段文本包含问题中出现频率很高的单词，则该算法会假设它可能与问题相关。这些算法简单易懂，计算速度快。然而，在预测相关性分数时，它们没有捕捉词序、词义和语法结构。</p><p id="4659" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi"> TF-IDF: </strong></p><p id="acf5" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">TF-IDF是一种经典而著名的信息检索算法。TF-IDF得分通过TF(术语频率)和IDF(逆文档频率)相乘来计算。</p><p id="f52a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">词频是问题中的一个词在文章中出现的次数。逆文档频率惩罚常见或停用词，如“the，and，in ”,同时增加实际关键字的权重。在我们的例子中，来自包含来自具有高TF-IDF分数的问题的单词的文档的段落是给定问题的相关段落。</p><blockquote class="km kn ko"><p id="3577" class="it iu js iv b iw ix iy iz ja jb jc jd kp jf jg jh kq jj jk jl kr jn jo jp jq ha bi translated"><em class="hh"> TF =(文章中一个问题中一个单词的重复次数)/(文章中的单词数)</em></p><p id="5872" class="it iu js iv b iw ix iy iz ja jb jc jd kp jf jg jh kq jj jk jl kr jn jo jp jq ha bi translated"><em class="hh"> IDF =log((总段落数)/(包含该单词的段落数)】</em></p></blockquote><p id="1897" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">请注意，如果该单词非常常见，并且出现在所有段落中，则<strong class="iv hi"><em class="js"/></strong>的段落总数将等于包含该单词的<strong class="iv hi"> <em class="js">的段落数。</em> </strong>因此IDF = log 1 = 0，使得该单词的TF-IDF为零。(像and、the、a等非常常见的词，TF-IDF得分通常为0。)</p><p id="957f" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi"> BM25: </strong></p><p id="e319" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">BM25是TF-IDF的优化版本。让我们考虑一个文档中出现单词pizza 10次的段落和另一个文档中出现单词pizza 20次的段落。如果pizza是一个关键字，那么TFIDF给第二段的相关性分数是第一段的两倍。在大多数应用中，仅仅因为相关术语出现的频率是两倍，就给予两次或显著的相关性分数是不公平的。此外，较长的段落比较短的段落有优势，因为较长的段落比较短的段落有更多的单词，并且重复相关关键词的概率很高。</p><p id="e723" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">BM25修改了TF公式，使得在该单词在文章中出现一定次数后，它使TF饱和。它还修改了IDF公式，使得具有不同单词长度的段落被标准化，并且当它们具有来自问题的相同单词频率时，较短的段落比较长的段落具有更大的优先权。</p><figure class="ju jv jw jx fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ks"><img src="../Images/afe7764ca390f08bf49253d9f76f260b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hyhp-dxF7WwJOjuE0y5iNA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">截图来自作者:TF-IDF和BM25相关性评分示例</figcaption></figure><p id="2c66" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在上面的图中，我们可以看到BM25相关性曲线比TF-IDF曲线增长得快得多，但它后来变得饱和了。例如，BM25对于频率为20的单词的相关性分数略高于频率10，但是TF-IDF对于频率20的相关性分数是频率10的两倍。</p><blockquote class="km kn ko"><p id="973d" class="it iu js iv b iw ix iy iz ja jb jc jd kp jf jg jh kq jj jk jl kr jn jo jp jq ha bi translated"><a class="ae jr" href="https://www.elastic.co/blog/found-similarity-in-elasticsearch#how-lucene-does-bm25" rel="noopener ugc nofollow" target="_blank">关于BM25和配方的更多信息</a></p></blockquote><p id="6fd6" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">神经或密集检索算法:</strong></p><p id="9564" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">神经或密集检索算法使用密集表示来计算相关性分数。密集检索器相对于稀疏检索器的主要优势是捕捉语义相似性(领导、队长、头等。，都是类似的词)和区分野心的词(河<strong class="iv hi">银行</strong>和钱<strong class="iv hi">银行</strong>)。然而，与TF-IDF和BM25这样的备用猎犬相比，密集猎犬速度较慢。</p><p id="1164" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi"> SBERT: </strong></p><p id="61f3" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">SBERT通过在问题向量和所有段落的向量之间执行<a class="ae jr" href="http://datahacker.rs/dot-product-inner-product/" rel="noopener ugc nofollow" target="_blank">点积</a>或余弦相似性来生成相关性分数(向量之间的角度越小- &gt;点积越高- &gt;相关性分数越高)。SBERT使用<a class="ae jr" href="https://en.wikipedia.org/wiki/BERT_%28language_model%29" rel="noopener ugc nofollow" target="_blank"> BERT编码器</a>生成给定问题的问题向量和所有段落的段落向量。</p><figure class="ju jv jw jx fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kt"><img src="../Images/c9298bc6f22b4d7cf56acee9ccbe3085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AH8I4QdyzxHDOkhFJ1BgZw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">来自作者的截图</figcaption></figure><p id="2af5" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在上面的图中，我们可以看到向量A代表文章X，向量B代表文章Y，向量Q代表一个问题。向量B和向量Q之间的角度α小于向量A和向量Q之间的角度θ。因此，向量B和向量Q之间的点积更高，这意味着Y段比X段与问题更相关。</p><p id="1e95" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi"> DPR:(密度通道检索器)</strong></p><p id="38d8" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">DPR与SBERT类似，使用点积来计算一个问题和多段文章之间的相关性分数。然而，DPR使用两种不同的<a class="ae jr" href="https://en.wikipedia.org/wiki/BERT_%28language_model%29" rel="noopener ugc nofollow" target="_blank">伯特编码器</a>型号。第一编码器模型为问题生成问题向量，第二编码器模型为多个段落生成不同的向量。DPR适用于问题的单词长度与段落长度相比非常小的问题。因此，对问题和段落使用不同的编码器是一个好主意。</p><p id="38fd" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">结论:</strong></p><p id="b472" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">人们可以通过实验和超参数调整来选择合适的检索算法。读者的表现高度依赖于检索者的表现，因为最相关的答案来自最相关的段落。</p><p id="74f9" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">恭喜你，你大规模完成了机器阅读理解入门。下一篇文章将介绍知识图以及如何使用它们构建问答模型。</p><p id="54bc" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">敬请关注理解语义搜索系列的更多文章！(点击了解本系列其他文章的更多信息<a class="ae jr" rel="noopener" href="/@kaushikshakkari/list/9df1566193b0"/></p></div><div class="ab cl ku kv go kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="ha hb hc hd he"><h1 id="87fa" class="lb lc hh bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">在<a class="ae jr" href="https://www.linkedin.com/in/kaushik-shakkari/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上加我。谢谢大家！</h1></div></div>    
</body>
</html>