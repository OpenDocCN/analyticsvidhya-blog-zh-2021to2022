<html>
<head>
<title>Applying Linear Regression on a Weather Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对天气数据集应用线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/applying-linear-regression-on-a-weather-data-set-e84901120f88?source=collection_archive---------2-----------------------#2021-07-16">https://medium.com/analytics-vidhya/applying-linear-regression-on-a-weather-data-set-e84901120f88?source=collection_archive---------2-----------------------#2021-07-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/812169aaf495bcfde65c0485d18f0b05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aONZ_Ydzslt8CfynRNrvHw.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图片由<a class="ae it" href="https://pixabay.com/users/mylene2401-10328767/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=5298312" rel="noopener ugc nofollow" target="_blank"> Mylene2401 </a>来自<a class="ae it" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=5298312" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></figcaption></figure><p id="0489" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">本文将讨论如何应用预处理步骤、降维以及为真实数据集构建线性模型。数据集选自<a class="ae it" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>，包含2006年至2016年匈牙利塞格德<a class="ae it" href="https://en.wikipedia.org/wiki/Szeged?oldformat=true" rel="noopener ugc nofollow" target="_blank">地区每小时/每天的真实历史天气数据汇总。</a></p><div class="js jt ez fb ju jv"><a href="https://www.kaggle.com/budincsevity/szeged-weather?select=weatherHistory.csv" rel="noopener  ugc nofollow" target="_blank"><div class="jw ab dw"><div class="jx ab jy cl cj jz"><h2 class="bd hi fi z dy ka ea eb kb ed ef hg bi translated">塞格德2006-2016年的天气</h2><div class="kc l"><h3 class="bd b fi z dy ka ea eb kb ed ef dx translated">每小时/每天汇总温度、压力、风速等信息</h3></div><div class="kd l"><p class="bd b fp z dy ka ea eb kb ed ef dx translated">www.kaggle.com</p></div></div><div class="ke l"><div class="kf l kg kh ki ke kj in jv"/></div></div></a></div><p id="f1c9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">用例</strong>:湿度和温度有关系吗？湿度和表观温度之间呢？给定湿度，你能预测表观温度吗？</p><p id="a3ab" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将首先使用数据集中最相关的特征来预测表观温度。然后，我们将尝试在只给定湿度的情况下预测表观温度，并计算模型的准确性。</p><p id="5a84" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将要遵循的步骤如下所述。</p><ul class=""><li id="5b93" class="kk kl hh iw b ix iy jb jc jf km jj kn jn ko jr kp kq kr ks bi translated">处理缺失值和异常值</li><li id="dded" class="kk kl hh iw b ix kt jb ku jf kv jj kw jn kx jr kp kq kr ks bi translated">转换</li><li id="efd7" class="kk kl hh iw b ix kt jb ku jf kv jj kw jn kx jr kp kq kr ks bi translated">特征编码</li><li id="e39c" class="kk kl hh iw b ix kt jb ku jf kv jj kw jn kx jr kp kq kr ks bi translated">标准化</li><li id="210e" class="kk kl hh iw b ix kt jb ku jf kv jj kw jn kx jr kp kq kr ks bi translated">维度缩减(使用PCA)</li><li id="d909" class="kk kl hh iw b ix kt jb ku jf kv jj kw jn kx jr kp kq kr ks bi translated">模特培训</li></ul><p id="3e8d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">将使用Jupyter Labs </strong>，代码将基于它。让我们先添加几个必需的导入。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="2b51" class="lh li hh ld b fi lj lk l ll lm">import pandas as pd<br/>from matplotlib import pyplot as plt<br/>import scipy.stats as stats<br/>import numpy as np<br/>import seaborn as sns</span></pre><p id="a2fa" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">是时候导入和理解数据集了。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="9f43" class="lh li hh ld b fi lj lk l ll lm">df = pd.read_csv('weatherHistory.csv', parse_dates=['Formatted Date'])<br/>print(df.shape)</span><span id="f3bb" class="lh li hh ld b fi ln lk l ll lm">print(df.info())<br/>print(df.describe(include='all'))</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/9f088d283e64b9e4d3816b4e12ffe057.png" data-original-src="https://miro.medium.com/v2/resize:fit:220/format:webp/1*eia-kwlHWN94TwQRySm2zQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">df.shape</figcaption></figure><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/67209c049f7dca214ad65419c708e58a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*CGB6oyHw-b9HRM_BslljVQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">df.info()</figcaption></figure><p id="f4ec" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以看到总共有12列，包括8个数字列和4个对象类型列。并且96453个数据记录是可用的。</p><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lq"><img src="../Images/ba32fd11c6e798da9ff2f49f49f28460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zNhLMKL038mVH8YITakMCA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">df.describe(include='all ')</figcaption></figure><h1 id="7f18" class="lr li hh bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">处理缺失值和异常值</h1><h2 id="932b" class="lh li hh bd ls mo mp mq lw mr ms mt ma jf mu mv me jj mw mx mi jn my mz mm na bi translated">删除重复项</h2><p id="e2bd" class="pw-post-body-paragraph iu iv hh iw b ix nb iz ja jb nc jd je jf nd jh ji jj ne jl jm jn nf jp jq jr ha bi translated">我们可以看到“格式化日期”列显示了一些重复的日期值。具体时间不可能有两个记录。可以删除这些重复项。我们将保留第一条记录，删除其他记录，删除每个重复的数据行。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="f6ec" class="lh li hh ld b fi lj lk l ll lm">df = df.drop_duplicates(['Formatted Date'], keep='first')<br/>print(df.shape)</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/94dff28fa0bb1ffa3e65b3e35fb8e941.png" data-original-src="https://miro.medium.com/v2/resize:fit:220/format:webp/1*V2E3gZYcGU-GezFWRAQIUA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">df.shape</figcaption></figure><p id="92b6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">24条数据记录因重复而被删除。我们将按“格式化日期”对数据框进行排序，并重置索引。</p><p id="7ae4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="ng">注意:我们重置了索引，因为删除数据行也会删除相应的索引值。缺少索引时，会对稍后执行的数据框连接操作产生负面影响。</em></p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="6944" class="lh li hh ld b fi lj lk l ll lm">df.sort_values(by=['Formatted Date'], inplace=True)<br/>df.reset_index(inplace=True, drop=True)</span></pre><h2 id="0028" class="lh li hh bd ls mo mp mq lw mr ms mt ma jf mu mv me jj mw mx mi jn my mz mm na bi translated">处理缺失值</h2><p id="06b9" class="pw-post-body-paragraph iu iv hh iw b ix nb iz ja jb nc jd je jf nd jh ji jj ne jl jm jn nf jp jq jr ha bi translated">在数据框描述中，我们看到“大声覆盖”列下的所有记录都是0值。因此，可以删除该列。因为我们不打算执行时间序列分析，所以“格式化日期”列不再有用。然后让我们计算每列的空值百分比。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="ff03" class="lh li hh ld b fi lj lk l ll lm">df = df.drop(columns=['Loud Cover', 'Formatted Date'])<br/>print(df.isnull().sum(axis=0) * 100/len(df))</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es nh"><img src="../Images/df02b4a63fc1407af0f16b8ca0b945ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*Irkvl8nVgdxVEblsyC1nwQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">df.isnull()。总和(轴= 0)* 100/长度(df)</figcaption></figure><p id="0555" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">只有0.54%的“Precip Type”值为空。因为这是极少量的总数据记录，所以最好删除它们。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="e5d7" class="lh li hh ld b fi lj lk l ll lm">df = df[df['Precip Type'].notna()]</span></pre><p id="9366" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有关缺失值处理的更多详细信息，请参考此<a class="ae it" href="https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4" rel="noopener" target="_blank">文章</a>。</p><h2 id="3475" class="lh li hh bd ls mo mp mq lw mr ms mt ma jf mu mv me jj mw mx mi jn my mz mm na bi translated">处理异常值</h2><p id="6f60" class="pw-post-body-paragraph iu iv hh iw b ix nb iz ja jb nc jd je jf nd jh ji jj ne jl jm jn nf jp jq jr ha bi translated">识别异常值的最佳方法是绘制箱线图并进行分析。我们将使用“seaborn”库并绘制箱线图。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="567e" class="lh li hh ld b fi lj lk l ll lm">sns.set(rc={'figure.figsize':(20,8)}, style='whitegrid')<br/>sns.boxplot(data=df)</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ni"><img src="../Images/824b66ad53cd5727ba20433a9728d0e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uvqpi0W6657qaPP5i3zvnA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">sns.boxplot(数据=df)</figcaption></figure><p id="efea" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在单独分析每列的箱线图后，可以在名为“湿度”、“风速”和“压力”的三列中识别异常值。其他列可以被认为是倾斜的。</p><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nj"><img src="../Images/a7f6207cea97912d3575c1ee05122310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v6s4tehhfBByENlZNCozLA.png"/></div></div></figure><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nk"><img src="../Images/1ddd098f9cca0fbf654c6529fe072830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ULkTH596vWAURLlGaRP75Q.png"/></div></div></figure><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nk"><img src="../Images/9ccd317d69897c4c175ed012f93deeef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JSTHSibm2WaREnysdT9SWg.png"/></div></div></figure><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="4c03" class="lh li hh ld b fi lj lk l ll lm">df = df[df['Humidity'] != 0.0]<br/>df = df[df['Wind Speed (km/h)'] &lt;= 60]<br/>df = df[df['Pressure (millibars)'] &gt; 0]<br/>df.reset_index(inplace=True, drop=True)<br/>print(df.shape)</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es nl"><img src="../Images/2b83cf16b05a0dde342e28b8d7d6a24f.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/1*WF5Y3eR7u9-O3VsQsGYtnw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">df.shape</figcaption></figure><p id="2026" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所有识别出的异常值都将被丢弃，数据框索引将被重置。现在数据集中剩下10列和94601个数据行。</p><p id="9b67" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这篇<a class="ae it" rel="noopener" href="/towards-artificial-intelligence/handlingoutliers- in-machine-learning-f842d8f4c1dc">文章</a>中阅读更多关于机器学习中异常值的内容。</p><h1 id="1f9c" class="lr li hh bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">转换</h1><p id="f251" class="pw-post-body-paragraph iu iv hh iw b ix nb iz ja jb nc jd je jf nd jh ji jj ne jl jm jn nf jp jq jr ha bi translated"><strong class="iw hi"> Q-Q图</strong>和<strong class="iw hi">直方图</strong>可用于识别数据集的任何偏斜度。让我们为所有数字列绘制Q-Q图和直方图。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="33ca" class="lh li hh ld b fi lj lk l ll lm">fig, axes = plt.subplots(3,3)<br/>plt.subplots_adjust(hspace = 0.4)<br/>for i, column in enumerate(columns[2:9]):<br/>    stats.probplot(df[column], dist="norm", plot=axes[int(i/3)][i%3])<br/>    axes[int(i/3)][i%3].set_title(column)<br/>plt.show()</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nm"><img src="../Images/55245774cb74b08b20ec6f700bc20fbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JZ4SDfOEeyeA7AMHtVxrdQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Q-Q图</figcaption></figure><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="09aa" class="lh li hh ld b fi lj lk l ll lm">fig, axes = plt.subplots(3,3)<br/>for i, column in enumerate(columns[2:9]):<br/>    temp_df = pd.DataFrame(df[column], columns=[column])<br/>    temp_df.hist(ax=axes[int(i/3)][i%3])<br/>plt.show()</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nn"><img src="../Images/1329a13aa16f854810a4138324963f7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iZ3mbICmAkg9X7eJHFA78Q.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">直方图</figcaption></figure><p id="cc20" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">从这些图表中我们可以看出，</p><ul class=""><li id="f2d7" class="kk kl hh iw b ix iy jb jc jf km jj kn jn ko jr kp kq kr ks bi translated">“湿度”数据集向左倾斜。</li><li id="2b34" class="kk kl hh iw b ix kt jb ku jf kv jj kw jn kx jr kp kq kr ks bi translated">“风速”数据集是右偏的。</li></ul><p id="abba" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们对这两列进行适当的转换。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="05d1" class="lh li hh ld b fi lj lk l ll lm">from sklearn.preprocessing import FunctionTransformer<br/><br/># squre root transformation for 'Wind Speed'<br/>sqrt_transformer = FunctionTransformer(np.sqrt, validate=True)<br/>data_transformed = sqrt_transformer.transform(df[['Wind Speed (km/h)']])<br/>df[feature_name] = data_transformed</span><span id="ed38" class="lh li hh ld b fi ln lk l ll lm"># power transformation with power of 2 for 'Humidity'<br/>power_transformer = FunctionTransformer(lambda x: x**2, validate=True)<br/>data_transformed = power_transformer.transform(df[['Humidity']])<br/>df[feature_name] = data_transformed</span></pre><p id="a85f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="ng">注:由于“风速”包含0值，我们应用了</em> <strong class="iw hi"> <em class="ng">平方根变换</em> </strong> <em class="ng">。为了平衡，2次方的</em> <strong class="iw hi"> <em class="ng">次方变换</em> </strong> <em class="ng">已应用于‘湿度’。阅读这篇</em> <a class="ae it" href="https://heartbeat.fritz.ai/hands-on-with-feature-engineering-techniques-transforming-variables-acea03472e24" rel="noopener ugc nofollow" target="_blank"> <em class="ng">文章</em> </a> <em class="ng">可以更好地理解转换技巧。</em></p><h1 id="584a" class="lr li hh bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">特征编码</h1><p id="330f" class="pw-post-body-paragraph iu iv hh iw b ix nb iz ja jb nc jd je jf nd jh ji jj ne jl jm jn nf jp jq jr ha bi translated">我们可以在数据集中看到三个分类列，分别是“Precip Type”、“Summary”、“Daily Summary”。让我们使用一键编码对这些列进行编码，因为我们正在训练一个线性模型。</p><p id="6316" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="ng">注意:对于线性模型，建议使用一热编码，而不是标签/整数编码。这背后的原因是标签编码导致在数据集中发现不相关的模式。这是因为当分类值编码为1、2等时，模型给予值2的权重大于值1。</em></p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="4cb5" class="lh li hh ld b fi lj lk l ll lm">df['Precip Type'] = df['Precip Type'].astype('category').cat.codes<br/>encoded_summary = pd.get_dummies(df['Summary'])<br/>encoded_daily_summary = pd.get_dummies(df['Daily Summary'])</span><span id="6c59" class="lh li hh ld b fi ln lk l ll lm">df = df.join(encoded_summary)<br/>df = df.join(encoded_daily_summary)<br/>df = df.drop(columns=['Daily Summary', 'Summary'])</span></pre><p id="2a21" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">“Precip Type”有两个唯一的值，即rain和snow，它们已被编码为0和1。“Summary”列有26个唯一值，在执行一次热编码后，已将其转换为26个附加行。“每日摘要”列有214个唯一值，在执行一次性编码后，该列已被转换为214个额外的行。</p><p id="5796" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里有一篇很棒的关于分类数据编码的文章。</p><h1 id="a1e0" class="lr li hh bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">将数据拆分为训练和测试</h1><p id="0785" class="pw-post-body-paragraph iu iv hh iw b ix nb iz ja jb nc jd je jf nd jh ji jj ne jl jm jn nf jp jq jr ha bi translated">为了避免数据泄露，我们将对训练和测试数据集分别执行标准化和PCA。我们将把数据集分成0.8比0.2的训练和测试。(“表观温度”已被选为预测目标。)</p><p id="69f0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="ng">注意:如果我们同时考虑训练和测试数据集并进行标准化，它也可以通过揭示更多与测试数据集相关的信息来帮助模型。</em></p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="bb92" class="lh li hh ld b fi lj lk l ll lm">df_data = df.drop('Apparent Temperature (C)', axis=1)<br/>df_target = pd.DataFrame(df['Apparent Temperature (C)'], columns=['Apparent Temperature (C)'])<br/>x_train, x_test, y_train, y_test = train_test_split(df_data, df_target, test_size=0.2, random_state=42)</span></pre><h1 id="e9a8" class="lr li hh bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">数据缩放/标准化</h1><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="1a77" class="lh li hh ld b fi lj lk l ll lm">std_columns = ['Temperature (C)',<br/>               'Humidity',<br/>               'Wind Speed (km/h)',<br/>               'Wind Bearing (degrees)',<br/>               'Visibility (km)',<br/>               'Pressure (millibars)']</span></pre><p id="2e4d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">执行标准化时，应该忽略所有编码的列。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="eac9" class="lh li hh ld b fi lj lk l ll lm">from sklearn.preprocessing import StandardScaler</span><span id="60b8" class="lh li hh ld b fi ln lk l ll lm"># for feature data<br/>scaler_train = StandardScaler()<br/>scaler_train.fit(x_train[std_columns])<br/>x_train_scaled = scaler_train.transform(x_train[std_columns])<br/>x_test_scaled = scaler_train.transform(x_test[std_columns])<br/>df_x_train_standardized = pd.DataFrame(x_train_scaled, columns = std_columns)<br/>df_x_test_standardized = pd.DataFrame(x_test_scaled, columns = std_columns)</span><span id="5afb" class="lh li hh ld b fi ln lk l ll lm"># for target data<br/>scaler_target = StandardScaler()<br/>scaler_target.fit(y_train)<br/>y_train_scaled = scaler_target.transform(y_train)<br/>y_test_scaled = scaler_target.transform(y_test)<br/>df_y_train_standardized = pd.DataFrame(y_train_scaled, columns = ['Apparent Temperature (C)'])<br/>df_y_test_standardized = pd.DataFrame(y_test_scaled, columns = ['Apparent Temperature (C)'])</span></pre><p id="93c3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们绘制训练数据集的相关矩阵，并尝试识别重要特征。<br/> <em class="ng">注:我们为了容易而忽略了范畴特征。</em></p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="a11f" class="lh li hh ld b fi lj lk l ll lm">sns.set(rc={'figure.figsize':(6,6)})<br/>sns.heatmap(df_x_train_standardized.corr(), annot=True)</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es no"><img src="../Images/33c07d32802f7d9bd217d4701f55dc60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*ZqqV8PLuSqQkI6b2YfBDnA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">热图—标准化</figcaption></figure><p id="e412" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们看不出任何特征之间有任何显著的相关性。</p><h1 id="a137" class="lr li hh bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">使用主成分分析(PCA)进行降维</h1><p id="e27b" class="pw-post-body-paragraph iu iv hh iw b ix nb iz ja jb nc jd je jf nd jh ji jj ne jl jm jn nf jp jq jr ha bi translated">让我们将训练数据放入PCA中，并确定最有影响的组件数。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="18ff" class="lh li hh ld b fi lj lk l ll lm">from sklearn.decomposition import PCA<br/><br/>pca = PCA()<br/>pca.fit(x_train)<br/>pca.explained_variance_ratio_</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es np"><img src="../Images/ebcff067572faacec21be87ddca8c37c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*UW5HIdZ_ephcARNqoLFweA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">pca.explained_variance_ratio_</figcaption></figure><p id="5bb4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过分析上面的输出，我们可以保留多达<strong class="iw hi"> 11个组件</strong>。现在，让我们将训练和测试数据放入PCA中，并将其减少到11个组件。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="567b" class="lh li hh ld b fi lj lk l ll lm">pca = PCA(n_components=11)<br/>pca.fit(x_train)<br/>pca_x_train = pca.transform(x_train)<br/>pca_x_test = pca.transform(x_test)</span></pre><h1 id="ed34" class="lr li hh bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">模型训练—线性模型</h1><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="a221" class="lh li hh ld b fi lj lk l ll lm">from sklearn import linear_model<br/><br/>lm = linear_model.LinearRegression()<br/>model = lm.fit(pca_x_train, y_train)</span></pre><p id="395a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们已经将训练特征数据和目标数据拟合到线性模型。我们可以说我们现在已经训练好了模型。让我们使用训练特征数据来预测训练目标值，并计算训练准确度。</p><h2 id="9c01" class="lh li hh bd ls mo mp mq lw mr ms mt ma jf mu mv me jj mw mx mi jn my mz mm na bi translated">模型评估</h2><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="ddd1" class="lh li hh ld b fi lj lk l ll lm">predictions = lm.predict(pca_x_train)<br/>y_train_pred = pd.DataFrame(predictions, columns=['Predicted Apparent Temperature (C)'])</span></pre><p id="2550" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">预测的均方误差(MSE)、均方根误差(RMSE)和解释方差</strong>被计算为模型的精度测量值。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="7192" class="lh li hh ld b fi lj lk l ll lm">from sklearn.metrics import mean_squared_error<br/>from math import sqrt<br/><br/># Mean Squred Error<br/>mse = mean_squared_error(y_train, y_train_pred)<br/>print('Mean squared error(Train) :', round(mse, 4))<br/><br/># Root Mean Squared Error<br/>rmsq = sqrt(mean_squared_error(y_train, y_train_pred))<br/>print('Root mean squared error(Train) :', round(rmsq, 4))<br/><br/># Accuracy<br/>score = lm.score(pca_x_train, y_train)<br/>print('Explained variance of the predictions(Train) :', round(score * 100, 4))</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es nq"><img src="../Images/6c0215b7852832c3a80295f0c2473da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*MAkQAfUo6JdDKJFUmp2i4Q.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">训练准确性</figcaption></figure><p id="ced5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们已经达到了<strong class="iw hi"> 98.83%的训练准确率</strong>。现在让我们使用测试特征数据预测测试目标值，并计算<br/>测试精度。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="650f" class="lh li hh ld b fi lj lk l ll lm">predictions = lm.predict(pca_x_test)<br/>y_pred = pd.DataFrame(predictions, columns=['Predicted Apparent Temperature (C)'])</span><span id="e283" class="lh li hh ld b fi ln lk l ll lm"># Mean Squred Error<br/>mse = mean_squared_error(y_test, y_pred)<br/>print('Mean squared error(Testing) :', round(mse, 4))<br/><br/># Root Mean Squared Error<br/>rmsq = sqrt(mean_squared_error(y_test, y_pred))<br/>print('Root mean squared error(Testing) :', round(rmsq, 4))<br/><br/># Accuracy<br/>score = lm.score(pca_x_test, y_test)<br/>print('Explained variance of the predictions(Testing) :', round(score * 100, 4))</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es nr"><img src="../Images/568b5f15370c8510e844541bfd8109b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*Y4BUeUhus4h0H6A-nkPSNQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">测试准确度</figcaption></figure><p id="2e04" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们已经达到了<strong class="iw hi"> 98.81%的测试精度</strong>。让我们画出预测的表观温度与实际的表观温度相差多少。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="db02" class="lh li hh ld b fi lj lk l ll lm">sns.set(rc={'figure.figsize':(20,10)})<br/>plt.plot(y_pred[:200], label='Predicted', color='blue')<br/>plt.plot(y_test[:200], label='Actual', color='red')<br/>plt.xlabel('x - axis')<br/>plt.ylabel('y - axis')<br/>plt.title('Predicitons vs Actual')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ns"><img src="../Images/c5f991c3cf0bfcbb744c27c42b427e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mM6C_b4dD-nNG6UerIwIjA.png"/></div></div></figure><p id="2b86" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里只考虑前200条记录。我们可以在这里和那里看到一些偏差。</p><h1 id="ea8b" class="lr li hh bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">结论</h1><p id="9119" class="pw-post-body-paragraph iu iv hh iw b ix nb iz ja jb nc jd je jf nd jh ji jj ne jl jm jn nf jp jq jr ha bi translated">对于我们的线性模型来说，98.81%是一个相当不错的精度水平。湿度与温度和表观温度呈低负相关关系。如果我们试图只给出湿度来预测表观温度，那就不太准确了。</p><p id="1e94" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由于温度与表观温度具有非常高的正相关性，我们已经达到了非常高的模型精度。我们可以在不考虑温度的情况下尝试预测表观温度。</p><h1 id="2d43" class="lr li hh bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">未来的工作</h1><ul class=""><li id="4b83" class="kk kl hh iw b ix nb jb nc jf nt jj nu jn nv jr kp kq kr ks bi translated">仅给定湿度，预测表观温度。</li><li id="5603" class="kk kl hh iw b ix kt jb ku jf kv jj kw jn kx jr kp kq kr ks bi translated">在不考虑温度的情况下预测表观温度。</li><li id="ba56" class="kk kl hh iw b ix kt jb ku jf kv jj kw jn kx jr kp kq kr ks bi translated">用奇异值分解(SVD)测试模型精度。</li></ul><h1 id="c379" class="lr li hh bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">资源</h1><ul class=""><li id="27ca" class="kk kl hh iw b ix nb jb nc jf nt jj nu jn nv jr kp kq kr ks bi translated">完整的Jupyter笔记本解决方案</li></ul><div class="js jt ez fb ju jv"><a href="https://github.com/JayaShakthi97/machine-learning/blob/main/Solution_174119U.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="jw ab dw"><div class="jx ab jy cl cj jz"><h2 class="bd hi fi z dy ka ea eb kb ed ef hg bi translated">jayashakthi 97/机器学习</h2><div class="kc l"><h3 class="bd b fi z dy ka ea eb kb ed ef dx translated">这是理解带有预处理技术的线性回归的笔记本。…</h3></div><div class="kd l"><p class="bd b fp z dy ka ea eb kb ed ef dx translated">github.com</p></div></div><div class="ke l"><div class="nw l kg kh ki ke kj in jv"/></div></div></a></div><ul class=""><li id="2e4f" class="kk kl hh iw b ix iy jb jc jf km jj kn jn ko jr kp kq kr ks bi translated">天气数据集— Kaggle</li></ul><div class="js jt ez fb ju jv"><a href="https://www.kaggle.com/budincsevity/szeged-weather?select=weatherHistory.csv" rel="noopener  ugc nofollow" target="_blank"><div class="jw ab dw"><div class="jx ab jy cl cj jz"><h2 class="bd hi fi z dy ka ea eb kb ed ef hg bi translated">塞格德2006-2016年的天气</h2><div class="kc l"><h3 class="bd b fi z dy ka ea eb kb ed ef dx translated">每小时/每天汇总温度、压力、风速等信息</h3></div><div class="kd l"><p class="bd b fp z dy ka ea eb kb ed ef dx translated">www.kaggle.com</p></div></div><div class="ke l"><div class="kf l kg kh ki ke kj in jv"/></div></div></a></div><p id="1098" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">特别感谢Subha Fernando博士(Moratuwa大学高级讲师)对本文创作的指导和激励。</p></div></div>    
</body>
</html>