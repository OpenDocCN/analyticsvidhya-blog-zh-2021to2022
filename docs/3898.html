<html>
<head>
<title>Nuclei Detection using UNet and HRNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用UNet和HRNet的核探测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nuclei-detection-using-unet-and-hrnet-1f586560a582?source=collection_archive---------0-----------------------#2021-08-07">https://medium.com/analytics-vidhya/nuclei-detection-using-unet-and-hrnet-1f586560a582?source=collection_archive---------0-----------------------#2021-08-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/87779b0887e6d2d5ed59b6f80e40b1fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OEQAtHFJAR5-19A0CmuG8Q.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">细胞核检测。<a class="ae it" href="https://www.kaggle.com/c/data-science-bowl-2018" rel="noopener ugc nofollow" target="_blank">信用</a></figcaption></figure><p id="18b0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">目录:</strong></p><ul class=""><li id="6acf" class="js jt hh iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">介绍</li><li id="3fb0" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">商业问题</li><li id="d074" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">问题定式化</li><li id="137d" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">数据管道</li><li id="b99f" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">自定义性能指标</li><li id="75a8" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">UNet模型</li><li id="a841" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">HRNet模型</li><li id="2700" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">推理</li><li id="815f" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">模型比较</li><li id="6516" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">模型量化</li><li id="01bc" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">结论</li><li id="96b7" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">未来的工作</li><li id="2c2b" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">链接</li><li id="b19b" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">参考</li></ul><h1 id="3990" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h1><p id="bed4" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">计算机视觉是一个专注于帮助计算机看东西的研究领域。它是一个多学科领域，可以广泛地称为人工智能和机器学习的子领域，可能涉及使用专门的方法和利用通用的学习算法。计算机视觉的目标是理解数字图像的内容。典型地，这包括开发试图复制人类视觉能力的方法。理解数字图像的内容可能涉及从图像中提取描述，该描述可以是对象、文本描述、三维模型等等。许多流行的计算机视觉应用包括尝试识别照片中的事物；例如:</p><ul class=""><li id="58f6" class="js jt hh iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated"><strong class="iw hi">物体分类:</strong>这张照片中的物体属于哪一大类？</li><li id="c5d3" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hi">物体识别:</strong>这张照片中给定物体的类型是什么？</li><li id="f58a" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hi">物体验证:</strong>物体在照片里吗？</li><li id="9dd5" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hi">物体检测:</strong>照片中的物体在哪里？</li><li id="2bd4" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hi">物体地标检测:</strong>照片中物体的关键点是什么？</li><li id="1063" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hi">物体分割:</strong>图像中哪些像素属于物体？</li><li id="33a5" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hi">物体识别:</strong>这张照片里有什么物体，它们在哪里？</li></ul><p id="d5e8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在数字图像处理和计算机视觉中，图像分割是将数字图像分割成多个片段(像素组，也称为图像对象)的过程。分割的目标是简化和/或改变图像的表示，使其更有意义，更易于分析。图像分割通常用于定位对象和边界(直线、曲线等)。)在图像中。更准确地说，图像分割是给图像中的每个像素分配标签的过程，使得具有相同标签的像素共享某些特征。有两种类型的分割-语义分割和实例分割。如果在一幅图像中有5个人，语义分割将集中于将所有人分类为单个实例。另一方面，实例分段。会逐一确认这些人的身份。医学图像分割是分割医学图像中感兴趣对象的任务。图像分割被认为是最基本的医学成像过程，因为它通过半自动或自动过程提取感兴趣区域(ROI)。它根据指定的描述将图像划分为多个区域，例如在边界检测、肿瘤检测/分割和肿块检测的医学应用中分割身体器官/组织。因为分割将图像分割成连贯的区域，所以可以通过提取图像的全局特征将聚类过程应用于分割，以专业地将ROI从背景中分离出来。</p><h1 id="1f5a" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">商业问题</h1><ul class=""><li id="29ca" class="js jt hh iw b ix le jb lf jf lj jj lk jn ll jr jx jy jz ka bi translated"><strong class="iw hi">问题陈述:<br/>识别细胞图像中的细胞核。但是为什么呢？<br/> </strong>识别细胞核是大多数分析的起点，因为人体30万亿个细胞中的大多数都含有一个充满DNA的细胞核，DNA是为每个细胞编程的遗传密码。识别细胞核使研究人员能够识别样本中的每个细胞，通过测量细胞对各种处理的反应，研究人员可以了解潜在的生物过程。</li><li id="8503" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hi">数据来源:<br/></strong>https://www.kaggle.com/c/data-science-bowl-2018</li><li id="3c8c" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hi">现实世界/业务约束:<br/> * </strong>没有这样的延迟要求。但是这个模型也不应该花几个小时来分割。<br/> *不正确分割的成本很高，因为它可能无法正确识别细胞核，这将在进一步的任务中产生进一步的后果。<br/> *模型要有良好的泛化能力，不能过拟合。</li></ul><h1 id="378a" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">问题定式化</h1><ul class=""><li id="337d" class="js jt hh iw b ix le jb lf jf lj jj lk jn ll jr jx jy jz ka bi translated"><strong class="iw hi">数据</strong> <br/> *来源:<a class="ae it" href="https://www.kaggle.com/c/data-science-bowl-2018/overview" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/data-science-bowl-2018/overview</a><br/>*该数据集包含大量分割的细胞核图像。</li><li id="7038" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">每个图像由一个关联的ImageId表示。属于图像的文件包含在具有此ImageId的文件夹中。该文件夹中有两个子文件夹:<br/>*<em class="lm">‘图像’</em>—包含图像文件。<br/>*<em class="lm">‘遮罩’</em>—包含每个细胞核的分段遮罩。此文件夹仅包含在训练集中。每个掩模包含一个核。遮罩不允许重叠(没有像素属于两个遮罩)。</li><li id="8f8f" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hi">深度学习问题类型:图像分割</strong> <br/>我们要识别细胞图像中存在的每一个细胞核。</li><li id="3902" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hi">性能指标:<br/> </strong>由于这是一个图像分割任务，因此有两个最常用的指标:</li></ul><ol class=""><li id="7cd8" class="js jt hh iw b ix iy jb jc jf ju jj jv jn jw jr ln jy jz ka bi translated"><strong class="iw hi">交集超过并集:</strong> IoU是预测分割和基础事实之间的重叠面积除以预测分割和基础事实之间的并集面积。该指标的范围为0–1(0–100%)，0表示没有重叠，1表示完全重叠的分段。</li></ol><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lo"><img src="../Images/81b6da0a0672a2d1cd187f2fb17f8008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xy-QqXf1gX832y8Ja9l95A.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">欠条评分。<a class="ae it" href="https://jinglescode.github.io/2019/11/07/biomedical-image-segmentation-u-net/" rel="noopener ugc nofollow" target="_blank">信用</a></figcaption></figure><p id="5be3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 2。Dice系数:</strong> Dice系数是2 *重叠面积除以两幅图像的总像素数。骰子系数与借据非常相似。它们是正相关的，这意味着如果一个人说模型A在分割图像方面比模型B好，那么另一个人也会这么说。像欠条一样，它们的范围都是从0到1，1表示预测和真实之间的最大相似度。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lt"><img src="../Images/8791cd3eb4358b5f8f9b3e28f4c97d43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yxx3IDFssiHThye8TgaHmg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">骰子系数。<a class="ae it" href="https://jinglescode.github.io/2019/11/07/biomedical-image-segmentation-u-net/" rel="noopener ugc nofollow" target="_blank">信用</a></figcaption></figure><h1 id="c63d" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">数据管道</h1><p id="d08f" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">对于这个项目，我用以下方式创建了输入数据管道:</p><ol class=""><li id="067f" class="js jt hh iw b ix iy jb jc jf ju jj jv jn jw jr ln jy jz ka bi translated">首先，我解压缩提供的训练和测试数据集，并将内容存储到“训练”和“测试”文件夹/目录中:</li></ol><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="038f" class="lz kh hh lv b fi ma mb l mc md">#Unzipping the training and testing folders into directories</span><span id="0259" class="lz kh hh lv b fi me mb l mc md">print('Unzipping stage1_train.zip')<br/>!unzip -q "../input/data-science-bowl-2018/stage1_train.zip" -d train/<br/>print('Unzipped stage1_train.zip')</span><span id="4209" class="lz kh hh lv b fi me mb l mc md">print('Unzipping stage1_test.zip')<br/>!unzip -q "../input/data-science-bowl-2018/stage1_test.zip" -d test/<br/>print('Unzipped stage1_test.zip')</span></pre><p id="4ede" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2.之后，我创建了一个数据帧，文件名作为行，对应于数据集中的每个样本点。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="ed2a" class="lz kh hh lv b fi ma mb l mc md"># Function to create a dataframe of files which will be used for further processing</span><span id="4962" class="lz kh hh lv b fi me mb l mc md">def files_df(root_dir):<br/>  subdir = os.listdir(root_dir)<br/>  files = []<br/>  df = pd.DataFrame()<br/>  for dir in subdir:<br/>    files.append(os.path.join(root_dir,dir))<br/>  df['files'] = files <br/>  return df</span><span id="cf87" class="lz kh hh lv b fi me mb l mc md"># Root directories for training and testing<br/>TRAIN_ROOT = './train'<br/>TEST_ROOT = './test'</span><span id="8f0b" class="lz kh hh lv b fi me mb l mc md">train_df = files_df(TRAIN_ROOT)<br/>test_df = files_df(TEST_ROOT)</span></pre><p id="a864" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">3.然后，我再次创建了一个数据帧，它将从前面的数据帧中获取文件名，并为图像创建图像路径和掩码路径，以及与每个文件名相对应的掩码。以下函数不仅会创建一个数据帧，还会将对应于每个单元图像的各种掩码组合成一个掩码。这样做的原因是对于特定的细胞图像有各种掩模，其中每个掩模识别同一细胞图像中的不同细胞核:</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="3ee8" class="lz kh hh lv b fi ma mb l mc md"># Hyperparameters<br/>IMG_WIDTH = 256<br/>IMG_HEIGHT = 256<br/>IMG_CHANNELS = 3<br/>CLASSES = 1<br/>BATCH_SIZE = 8</span><span id="db38" class="lz kh hh lv b fi me mb l mc md"># Function which will create a dataframe of image paths and mask paths along with creating a single mask with multiple masks</span><span id="fe3a" class="lz kh hh lv b fi me mb l mc md">def image_df(filenames):<br/>  image_paths = []<br/>  mask_paths = []<br/>  df = pd.DataFrame()</span><span id="9b92" class="lz kh hh lv b fi me mb l mc md">  for filename in tqdm(filenames):<br/>    file_path = os.path.join(filename,'images')<br/>    image_path = os.path.join(file_path,os.listdir(file_path)[0])<br/>    image_paths.append(image_path)</span><span id="1fb2" class="lz kh hh lv b fi me mb l mc md">    mask = np.zeros((IMG_WIDTH,IMG_HEIGHT,1))<br/>    mask_dir = file_path.replace("images", "masks")<br/>    masks = os.listdir(mask_dir)</span><span id="e29d" class="lz kh hh lv b fi me mb l mc md">    for m in masks:<br/>      mask_path = os.path.join(mask_dir,m)<br/>      mask_  = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)<br/>      mask_  = cv2.resize(mask_,(IMG_WIDTH,IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)<br/>      mask_  = np.expand_dims(mask_, axis = -1)<br/>      mask = np.maximum(mask,mask_)</span><span id="a650" class="lz kh hh lv b fi me mb l mc md">    newmask_dir = mask_dir.replace("masks", "masks_")</span><span id="c459" class="lz kh hh lv b fi me mb l mc md">    if not os.path.isdir(newmask_dir):<br/>      os.mkdir(newmask_dir)</span><span id="cb2c" class="lz kh hh lv b fi me mb l mc md">    newmask_path = image_path.replace("images", "masks_")<br/>    mask_paths.append(newmask_path)<br/>    cv2.imwrite(newmask_path, mask)</span><span id="c66b" class="lz kh hh lv b fi me mb l mc md">  df['images'] = image_paths<br/>  df['masks'] = mask_paths<br/>  return df</span><span id="1776" class="lz kh hh lv b fi me mb l mc md"><br/># Training dataframe<br/>train_filenames = train_df['files']<br/>train = image_df(train_filenames)</span></pre><p id="b9dc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">4.之后，我将训练数据分为训练集和验证集:</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="f4f8" class="lz kh hh lv b fi ma mb l mc md">X_train, X_val = train_test_split(train, test_size=0.1, random_state=42)</span></pre><p id="2d9c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">5.然后我做了一个数据预处理函数:</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="85b0" class="lz kh hh lv b fi ma mb l mc md"># Function to parse image and mask file path and convert them into image and mask</span><span id="de1f" class="lz kh hh lv b fi me mb l mc md">def parse_function(image_path, mask_path):<br/>  <br/>  image_string = tf.io.read_file(image_path)<br/>  image = tf.image.decode_png(image_string, channels=IMG_CHANNELS)#<br/>  image = tf.image.convert_image_dtype(image, tf.float32)<br/>  image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])</span><span id="e460" class="lz kh hh lv b fi me mb l mc md">  mask_string = tf.io.read_file(mask_path)<br/>  mask = tf.image.decode_png(mask_string, channels=IMG_CHANNELS)#<br/>  mask = tf.image.convert_image_dtype(mask, tf.float32)<br/>  mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH])<br/>  return image, mask</span></pre><p id="e90f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">6.然后是使用tf.data创建训练和验证数据集:</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="261b" class="lz kh hh lv b fi ma mb l mc md"># Training dataset</span><span id="b847" class="lz kh hh lv b fi me mb l mc md">train_ds = tf.data.Dataset.from_tensor_slices((X_train['images'], X_train['masks']))</span><span id="27f2" class="lz kh hh lv b fi me mb l mc md">train_ds = train_ds.shuffle(X_train.shape[0])</span><span id="bdd4" class="lz kh hh lv b fi me mb l mc md">train_ds = train_ds.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)</span><span id="24d9" class="lz kh hh lv b fi me mb l mc md">train_ds = train_ds.map(train_preprocess, num_parallel_calls=tf.data.AUTOTUNE)</span><span id="6344" class="lz kh hh lv b fi me mb l mc md">train_ds = train_ds.batch(BATCH_SIZE)</span><span id="dbf7" class="lz kh hh lv b fi me mb l mc md">train_ds = train_ds.prefetch(1)</span><span id="453f" class="lz kh hh lv b fi me mb l mc md"># Validation dataset</span><span id="5940" class="lz kh hh lv b fi me mb l mc md">val_ds = tf.data.Dataset.from_tensor_slices((X_val['images'], X_val['masks']))</span><span id="6fe3" class="lz kh hh lv b fi me mb l mc md">val_ds = val_ds.shuffle(X_val.shape[0])</span><span id="ab2f" class="lz kh hh lv b fi me mb l mc md">val_ds = val_ds.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)</span><span id="fb31" class="lz kh hh lv b fi me mb l mc md">val_ds = val_ds.batch(BATCH_SIZE)</span><span id="4f47" class="lz kh hh lv b fi me mb l mc md">val_ds = val_ds.prefetch(1)</span></pre><p id="2d48" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">7.训练数据集示例:</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/89431efc2345ddc6d3bc0093c8921208.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*rM6-wC8bjthCag19_0orqg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">训练数据集示例-左侧为原始图像，右侧为地面实况。</figcaption></figure><p id="60e2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">8.验证数据集示例:</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/c3de1a0778ce462f326633d6f22426b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*NQntjUFtz71dcH8Io-2-yQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">验证数据集的示例-左侧为原始图像，右侧为地面实况。</figcaption></figure><h1 id="4065" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">自定义性能指标</h1><p id="e569" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">为了这项工作，我定制了张量流的平均IoU度量。这样做是为了结合一个阈值，用于将预测概率转换成布尔张量，布尔张量是可变的。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="5c49" class="lz kh hh lv b fi ma mb l mc md"># Custom MeanIoU Metric function</span><span id="a820" class="lz kh hh lv b fi me mb l mc md">class MeanIoU(tf.keras.metrics.Metric):<br/>  def __init__(self, num_classes, thres=0.5, name='mean_iou', dtype=None):<br/>    super(MeanIoU, self).__init__(name=name, dtype=dtype)<br/>    self.num_classes = num_classes<br/>    self.thres = thres<br/>    self.total_cm = self.add_weight('total_confusion_matrix',<br/>                                   shape=(num_classes, num_classes),<br/>                                 initializer=tf.zeros_initializer())<br/>  def update_state(self, y_true, y_pred, sample_weight=None):<br/>    y_true = tf.cast(y_true, self._dtype)<br/>    y_pred = tf.cast(y_pred, self._dtype)<br/>    if y_pred.shape.ndims &gt; 1:<br/>      y_pred = tf.reshape(y_pred, [-1])<br/>    if y_true.shape.ndims &gt; 1:<br/>      y_true = tf.reshape(y_true, [-1])</span><span id="c4ac" class="lz kh hh lv b fi me mb l mc md">    y_pred = tf.where(y_pred &gt; self.thres, 1.0, 0.0)</span><span id="166f" class="lz kh hh lv b fi me mb l mc md">    if sample_weight is not None:<br/>      sample_weight = tf.cast(sample_weight, self._dtype)<br/>      if sample_weight.shape.ndims &gt; 1:<br/>        sample_weight = tf.reshape(sample_weight, [-1])</span><span id="6fde" class="lz kh hh lv b fi me mb l mc md">    current_cm = tf.math.confusion_matrix(y_true,<br/>                                          y_pred,<br/>                                          self.num_classes,<br/>                                          weights=sample_weight,<br/>                                          dtype=self._dtype)</span><span id="bd66" class="lz kh hh lv b fi me mb l mc md">    return self.total_cm.assign_add(current_cm)<br/>  <br/>  def result(self):<br/>    sum_over_row = tf.cast(tf.reduce_sum(self.total_cm, axis=0), dtype=self._dtype)</span><span id="2d0b" class="lz kh hh lv b fi me mb l mc md">    sum_over_col = tf.cast(tf.reduce_sum(self.total_cm, axis=1), dtype=self._dtype)</span><span id="408d" class="lz kh hh lv b fi me mb l mc md">    true_positives = tf.cast(tf.linalg.tensor_diag_part(self.total_cm), dtype=self._dtype)</span><span id="4376" class="lz kh hh lv b fi me mb l mc md">    denominator = sum_over_row + sum_over_col - true_positives</span><span id="5e4b" class="lz kh hh lv b fi me mb l mc md">    num_valid_entries = tf.reduce_sum(tf.cast(tf.math.not_equal(denominator, 0), dtype=self._dtype))</span><span id="aa40" class="lz kh hh lv b fi me mb l mc md">    iou = tf.math.divide_no_nan(true_positives, denominator)</span><span id="4ab1" class="lz kh hh lv b fi me mb l mc md">    return tf.math.divide_no_nan(tf.reduce_sum(iou, name='mean_iou'), num_valid_entries)</span><span id="340c" class="lz kh hh lv b fi me mb l mc md">  def reset_states(self):<br/>    # The state of the metric will be reset at the start of each epoch.<br/>    tf.keras.backend.set_value(self.total_cm, np.zeros((self.num_classes, self.num_classes)))</span><span id="e157" class="lz kh hh lv b fi me mb l mc md">  def get_config(self):<br/>    config = {'num_classes': self.num_classes}<br/>    base_config = super(MeanIoU, self).get_config()<br/>    return dict(list(base_config.items()) + list(config.items()))</span></pre><h1 id="652c" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">UNet模型</h1><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mh"><img src="../Images/936d33d2089d1e6a4c7ab53965e50ad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pIgyDKEl580V4mwe9t6MQw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">UNet模型的体系结构。<a class="ae it" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank">信用</a></figcaption></figure><p id="e4c1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">UNet <em class="lm"> </em>由Olaf Ronneberger等人开发，用于生物医学图像分割。该架构包含两条路径。第一条路径是收缩路径(也称为编码器)，用于捕获图像中的上下文或提取图像中的因素。编码器以堆叠方式由卷积层和最大池层组成。</p><p id="016a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">第二条路径是对称扩展路径(也称为解码器)，用于使用转置卷积实现精确定位。因此，它是一个端到端的完全卷积网络(FCN)，即它只包含卷积层，不包含任何密集层，因此它可以接受任何大小的图像。要了解更多关于UNet的信息，请阅读研究论文— <a class="ae it" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> U-Net:生物医学图像分割的卷积网络</a>。</p><h2 id="0023" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">体系结构</h2><p id="ff49" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">在下面的代码中，我们可以看到使用Tensorflow实现的UNet架构的实现。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="038b" class="lz kh hh lv b fi ma mb l mc md">mean_iou = MeanIoU(2, 0.4)</span><span id="e445" class="lz kh hh lv b fi me mb l mc md"># Input Layer<br/># Input shape 256X256X3<br/>inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))</span><span id="b469" class="lz kh hh lv b fi me mb l mc md"><br/># Left Side/Downsampling Side<br/># 256 -&gt; 128<br/>conv1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)</span><span id="21ef" class="lz kh hh lv b fi me mb l mc md">conv1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv1)</span><span id="ea1f" class="lz kh hh lv b fi me mb l mc md">pool1 = MaxPool2D((2, 2))(conv1)<br/>pool1 = Dropout(0.25)(pool1)</span><span id="d084" class="lz kh hh lv b fi me mb l mc md"># 128 -&gt; 64<br/>conv2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool1)</span><span id="8f65" class="lz kh hh lv b fi me mb l mc md">conv2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv2)</span><span id="1766" class="lz kh hh lv b fi me mb l mc md">pool2 = MaxPool2D((2, 2))(conv2)<br/>pool2 = Dropout(0.5)(pool2)</span><span id="aea1" class="lz kh hh lv b fi me mb l mc md"># 64 -&gt; 32<br/>conv3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool2)</span><span id="eaa2" class="lz kh hh lv b fi me mb l mc md">conv3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv3)</span><span id="10cb" class="lz kh hh lv b fi me mb l mc md">pool3 = MaxPool2D((2, 2))(conv3)<br/>pool3 = Dropout(0.5)(pool3)</span><span id="33a4" class="lz kh hh lv b fi me mb l mc md"># 32 -&gt; 16<br/>conv4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool3)</span><span id="5bbd" class="lz kh hh lv b fi me mb l mc md">conv4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv4)</span><span id="a930" class="lz kh hh lv b fi me mb l mc md">pool4 = MaxPool2D((2, 2))(conv4)<br/>pool4 = Dropout(0.5)(pool4)</span><span id="ea55" class="lz kh hh lv b fi me mb l mc md"># Middle Part<br/># 16 -&gt; 16<br/>convm = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool4)</span><span id="05b1" class="lz kh hh lv b fi me mb l mc md">convm = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(convm)</span><span id="377c" class="lz kh hh lv b fi me mb l mc md"># Right Side/ Upsampling Side<br/># 16 -&gt; 32<br/>uconv4 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(convm)</span><span id="fa92" class="lz kh hh lv b fi me mb l mc md">uconv4 = Concatenate()([uconv4, conv4])<br/>uconv4 = Dropout(0.5)(uconv4)</span><span id="3230" class="lz kh hh lv b fi me mb l mc md">uconv4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(uconv4)</span><span id="041e" class="lz kh hh lv b fi me mb l mc md">uconv4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(uconv4)</span><span id="ad5b" class="lz kh hh lv b fi me mb l mc md"># 32 -&gt; 64<br/>uconv3 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(uconv4)</span><span id="bb53" class="lz kh hh lv b fi me mb l mc md">uconv3 = Concatenate()([uconv3, conv3])<br/>uconv3 = Dropout(0.5)(uconv3)</span><span id="9fdf" class="lz kh hh lv b fi me mb l mc md">uconv3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(uconv3)</span><span id="ba6a" class="lz kh hh lv b fi me mb l mc md">uconv3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(uconv3)</span><span id="a39e" class="lz kh hh lv b fi me mb l mc md"># 64 -&gt; 128<br/>uconv2 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(uconv3)</span><span id="875a" class="lz kh hh lv b fi me mb l mc md">uconv2 = Concatenate()([uconv2, conv2])<br/>uconv2 = Dropout(0.5)(uconv2)</span><span id="88f8" class="lz kh hh lv b fi me mb l mc md">uconv2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(uconv2)</span><span id="6e19" class="lz kh hh lv b fi me mb l mc md">uconv2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(uconv2)</span><span id="337f" class="lz kh hh lv b fi me mb l mc md"># 128 -&gt; 256<br/>uconv1 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(uconv2)</span><span id="db7b" class="lz kh hh lv b fi me mb l mc md">uconv1 = Concatenate()([uconv1, conv1])<br/>uconv1 = Dropout(0.5)(uconv1)</span><span id="de35" class="lz kh hh lv b fi me mb l mc md">uconv1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(uconv1)</span><span id="cf67" class="lz kh hh lv b fi me mb l mc md">uconv1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(uconv1)</span><span id="6251" class="lz kh hh lv b fi me mb l mc md"># Output Layer<br/># Output shape 256X256X1<br/>outputs = Conv2D(CLASSES, (1, 1), activation='sigmoid')(uconv1)</span><span id="464f" class="lz kh hh lv b fi me mb l mc md">model = Model(inputs=[inputs], outputs=[outputs])<br/>model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[mean_iou])#</span><span id="da19" class="lz kh hh lv b fi me mb l mc md">model.summary()</span></pre><h2 id="4011" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">复试</h2><p id="dc79" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">模型检查点回调用于保存最佳性能模型。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="52b0" class="lz kh hh lv b fi ma mb l mc md">!rm -rf ./unet_save/</span><span id="7e9c" class="lz kh hh lv b fi me mb l mc md">if not os.path.exists('unet_save'):<br/>  os.makedirs('unet_save')</span><span id="8e3b" class="lz kh hh lv b fi me mb l mc md">filepath="unet_save/weights-{epoch:04d}.hdf5"<br/>checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,<br/>                                                save_best_only=True,<br/>                                                mode='max',<br/>                                             monitor='val_mean_iou')</span></pre><h2 id="91a9" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">培养</h2><p id="3f75" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">模型第一次训练30个周期，然后再训练60个周期。第二次训练从第一轮表现最好的时期开始。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="55f7" class="lz kh hh lv b fi ma mb l mc md">callbacks_list = [checkpoint]<br/># First round of training<br/>history = unet_model.fit(train_ds,<br/>                         initial_epoch = 0,<br/>                         epochs=30,<br/>                         callbacks=callbacks_list,<br/>                         validation_data=val_ds)</span><span id="e4d4" class="lz kh hh lv b fi me mb l mc md"># Second round of training<br/>initial_epoch = int(sorted(os.listdir('unet_save'))[-1].split('.')[0].split('-')[-1])</span><span id="5af8" class="lz kh hh lv b fi me mb l mc md">history = unet_model.fit(train_ds,<br/>                         initial_epoch = initial_epoch,<br/>                         epochs=60,<br/>                         callbacks=callbacks_list,<br/>                         validation_data=val_ds)</span><span id="1754" class="lz kh hh lv b fi me mb l mc md"># Best model<br/>unet_model = tf.keras.models.load_model('./unet_save/'+sorted(os.listdir('unet_save'))[-1])</span><span id="07fc" class="lz kh hh lv b fi me mb l mc md"># save model<br/>unet_model.save("unet_model.h5")<br/>unet_model = tf.keras.models.load_model("unet_model.h5")</span></pre><h1 id="c331" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">HRNet模型</h1><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es mv"><img src="../Images/b523f2401a780d4d843d725377efe012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*EjZuJbD0PM3lUNSj_NTbiw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">HRNet模型的体系结构。<a class="ae it" href="https://arxiv.org/abs/1908.07919" rel="noopener ugc nofollow" target="_blank">信用</a></figcaption></figure><p id="7060" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">HRNet模型是京东王等人开发的。艾尔。用于解决先前图像分割模型中缺乏高级分辨率的问题。他们的架构能够在整个过程中保持高分辨率的表现。该架构的第一级是一个高分辨率子网，然后逐渐将高到低分辨率子网逐一添加，形成更多级，并将多分辨率子网并联。(I)这种方法并联连接高到低分辨率子网，而不是像大多数现有解决方案那样串联连接。因此，可以保持高分辨率，而不是通过由低到高的过程来恢复分辨率。(ii)大多数现有的融合方案集合了低级和高级表示。相反，在该架构中，在相同深度和相似级别的低分辨率表示的帮助下，执行重复的多尺度融合来增强高分辨率表示，反之亦然，从而导致高分辨率表示更加丰富。要了解更多关于HRNet的信息，请阅读研究论文— <a class="ae it" href="https://arxiv.org/abs/1908.07919" rel="noopener ugc nofollow" target="_blank">视觉识别的深度高分辨率表征学习</a>。</p><h2 id="8f75" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">体系结构</h2><p id="f85d" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">在下面的代码中，我们可以看到使用Tensorflow实现的HRNet架构的实现。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="c42c" class="lz kh hh lv b fi ma mb l mc md"># Hyperparameters<br/>BN_MOMENTUM = 0.1<br/>BN_EPSILON = 1e-5<br/>INITIALIZER = 'he_normal'</span><span id="51b5" class="lz kh hh lv b fi me mb l mc md"># Functions to build layers</span><span id="9913" class="lz kh hh lv b fi me mb l mc md">def conv(x, outsize, kernel_size, strides_=1, padding_='same', activation=None):<br/>  return Conv2D(outsize, <br/>                kernel_size, <br/>                strides=strides_, <br/>                padding=padding_,<br/>                kernel_initializer=INITIALIZER, <br/>                use_bias=False,<br/>                activation=activation)(x)</span><span id="b0fc" class="lz kh hh lv b fi me mb l mc md"><br/>def BasicBlock(x, size, downsampe=False):<br/>  residual = x</span><span id="a367" class="lz kh hh lv b fi me mb l mc md">  out = conv(x, size, 3)<br/>  out = BatchNormalization(epsilon=BN_EPSILON,<br/>                           momentum=BN_MOMENTUM)(out)<br/>  out = Activation('relu')(out)</span><span id="8a8c" class="lz kh hh lv b fi me mb l mc md">  out = conv(out, size, 3)<br/>  out = BatchNormalization(epsilon=BN_EPSILON,<br/>                           momentum=BN_MOMENTUM)(out)</span><span id="c812" class="lz kh hh lv b fi me mb l mc md">  if downsampe:<br/>    residual = conv(x, size, 1, padding_='valid')<br/>    residual = BatchNormalization(epsilon=BN_EPSILON,<br/>                                  momentum=BN_MOMENTUM)(residual)</span><span id="cdec" class="lz kh hh lv b fi me mb l mc md">  out = Add()([out, residual])<br/>  out = Activation('relu')(out)</span><span id="ac7e" class="lz kh hh lv b fi me mb l mc md">  return out</span><span id="10d0" class="lz kh hh lv b fi me mb l mc md"><br/>def BottleNeckBlock(x, size, downsampe=False):<br/>  residual = x<br/>  out = conv(x, size, 1, padding_='valid')<br/>  out = BatchNormalization(epsilon=BN_EPSILON,<br/>                           momentum=BN_MOMENTUM)(out)<br/>  out = Activation('relu')(out)</span><span id="48c3" class="lz kh hh lv b fi me mb l mc md">  out = conv(out, size, 3)<br/>  out = BatchNormalization(epsilon=BN_EPSILON,<br/>                           momentum=BN_MOMENTUM)(out)<br/>  out = Activation('relu')(out)</span><span id="f44f" class="lz kh hh lv b fi me mb l mc md">  out = conv(out, size * 4, 1, padding_='valid')<br/>  out = BatchNormalization(epsilon=BN_EPSILON,<br/>                           momentum=BN_MOMENTUM)(out)</span><span id="55d5" class="lz kh hh lv b fi me mb l mc md">  if downsampe:<br/>    residual = conv(x, size * 4, 1, padding_='valid')<br/>    residual = BatchNormalization(epsilon=BN_EPSILON,<br/>                                  momentum=BN_MOMENTUM)(residual)<br/>  <br/>  out = Add()([out, residual])<br/>  out = Activation('relu')(out)</span><span id="ad52" class="lz kh hh lv b fi me mb l mc md">  return out</span><span id="33a1" class="lz kh hh lv b fi me mb l mc md"><br/>def layer1(x):<br/>  x = BottleNeckBlock(x, 64, downsampe=True)<br/>  x = BottleNeckBlock(x, 64)<br/>  x = BottleNeckBlock(x, 64)<br/>  x = BottleNeckBlock(x, 64)<br/> <br/>  return x</span><span id="0752" class="lz kh hh lv b fi me mb l mc md"><br/>def transition_layer(x, in_channels, out_channels):<br/>  num_in = len(in_channels)<br/>  num_out = len(out_channels)<br/>  out = []</span><span id="00c0" class="lz kh hh lv b fi me mb l mc md">  for i in range(num_out):<br/>    if i &lt; num_in:<br/>      if in_channels[i] != out_channels[i]:<br/>        residual = conv(x[i], out_channels[i], 3)<br/>        residual = BatchNormalization(epsilon=BN_EPSILON,<br/>                                     momentum=BN_MOMENTUM)(residual)<br/>        residual = Activation('relu')(residual)<br/>        out.append(residual)<br/>      else:<br/>        out.append(x[i])<br/>    else:<br/>      residual = conv(x[-1], out_channels[i], 3, strides_=2)<br/>      residual = BatchNormalization(epsilon=BN_EPSILON,<br/>                                    momentum=BN_MOMENTUM)(residual)<br/>      residual = Activation('relu')(residual)<br/>      out.append(residual)</span><span id="f8b2" class="lz kh hh lv b fi me mb l mc md">  return out</span><span id="5986" class="lz kh hh lv b fi me mb l mc md"><br/>def branches(x, block_num, channels):<br/>  out = []<br/>  for i in range(len(channels)):<br/>    residual = x[i]<br/>    for j in range(block_num):<br/>      residual = BasicBlock(residual, channels[i])<br/>    out.append(residual)</span><span id="fe2b" class="lz kh hh lv b fi me mb l mc md">  return out</span><span id="7821" class="lz kh hh lv b fi me mb l mc md"><br/>def fuse_layers(x, channels, multi_scale_output=True):<br/>  out = []<br/>  for i in range(len(channels) if multi_scale_output else 1):<br/>    residual = x[i]<br/>    for j in range(len(channels)):<br/>      if j &gt; i:<br/>        y = conv(x[j], channels[i], 1, padding_='valid')<br/>        y = BatchNormalization(epsilon=BN_EPSILON,<br/>                               momentum=BN_MOMENTUM)(y)<br/>        y = UpSampling2D(size=2 ** (j - i))(y)<br/>        residual = Add()([residual, y])<br/>      elif j &lt; i:<br/>        y = x[j]<br/>        for k in range(i - j):<br/>          if k == i - j - 1:<br/>            y = conv(y, channels[i], 3, strides_=2)<br/>            y = BatchNormalization(epsilon=BN_EPSILON,<br/>                                   momentum=BN_MOMENTUM)(y)<br/>          else:<br/>            y = conv(y, channels[j], 3, strides_=2)<br/>            y = BatchNormalization(epsilon=BN_EPSILON,<br/>                                   momentum=BN_MOMENTUM)(y)<br/>            y = Activation('relu')(y)<br/>        residual = Add()([residual, y])<br/>    residual = Activation('relu')(residual)<br/>    out.append(residual)<br/>  <br/>  return out</span><span id="8737" class="lz kh hh lv b fi me mb l mc md"><br/># Functions to create model</span><span id="1000" class="lz kh hh lv b fi me mb l mc md"><br/>def HighResolutionModule(x, channels, multi_scale_output=True):<br/>  residual = branches(x, 4, channels)<br/>  out = fuse_layers(residual, channels,<br/>                    multi_scale_output=multi_scale_output)<br/>  return out</span><span id="27ed" class="lz kh hh lv b fi me mb l mc md"><br/>def stage(x, num_modules, channels, multi_scale_output=True):<br/>  out = x<br/>  for i in range(num_modules):<br/>    if i == num_modules - 1 and multi_scale_output == False:<br/>      out = HighResolutionModule(out, channels,<br/>                                 multi_scale_output=False)<br/>    else:<br/>      out = HighResolutionModule(out, channels)<br/>  return out</span><span id="299c" class="lz kh hh lv b fi me mb l mc md"><br/>def hrnet_keras(input_size=(256, 256, 3)):<br/>  channels_2 = [32, 64]<br/>  channels_3 = [32, 64, 128]<br/>  channels_4 = [32, 64, 128, 256]<br/>  num_modules_2 = 1<br/>  num_modules_3 = 4<br/>  num_modules_4 = 3</span><span id="7f6d" class="lz kh hh lv b fi me mb l mc md">  inputs = Input(input_size)<br/>  x = conv(inputs, 64, 3, strides_=2)<br/>  x = BatchNormalization(epsilon=BN_EPSILON,momentum=BN_MOMENTUM)(x)<br/>  x = conv(x, 64, 3, strides_=2)<br/>  x = BatchNormalization(epsilon=BN_EPSILON,momentum=BN_MOMENTUM)(x)<br/>  x = Activation('relu')(x)</span><span id="0f2d" class="lz kh hh lv b fi me mb l mc md">  la1 = layer1(x)<br/>  tr1 = transition_layer([la1], [256], channels_2)<br/>  st2 = stage(tr1, num_modules_2, channels_2)<br/>  tr2 = transition_layer(st2, channels_2, channels_3)<br/>  st3 = stage(tr2, num_modules_3, channels_3)<br/>  tr3 = transition_layer(st3, channels_3, channels_4)<br/>  st4 = stage(tr3, num_modules_4, channels_4,<br/>              multi_scale_output=False)<br/>  up1 = UpSampling2D()(st4[0])<br/>  up1 = conv(up1, 32, 3)<br/>  up1 = BatchNormalization(epsilon=BN_EPSILON,<br/>                           momentum=BN_MOMENTUM)(up1)<br/>  up1 = Activation('relu')(up1)<br/>  up2 = UpSampling2D()(up1)<br/>  up2 = conv(up2, 32, 3)<br/>  up2 = BatchNormalization(epsilon=BN_EPSILON,<br/>                           momentum=BN_MOMENTUM)(up2)<br/>  up2 = Activation('relu')(up2)<br/>  final = conv(up2, 1, 1, padding_='valid', activation='sigmoid')</span><span id="95c8" class="lz kh hh lv b fi me mb l mc md">  model = Model(inputs=inputs, outputs=final)<br/>  <br/>  return model</span><span id="8e6e" class="lz kh hh lv b fi me mb l mc md"><br/>hrnet_model = hrnet_keras()<br/>hrnet_model.summary()<br/>hrnet_model.compile(optimizer='adam', <br/>                    loss='binary_crossentropy', <br/>                    metrics=[mean_iou])</span></pre><h2 id="32ed" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">复试</h2><p id="30bc" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">模型检查点回调用于保存最佳性能模型。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="71a6" class="lz kh hh lv b fi ma mb l mc md">!rm -rf ./hrnet_save/</span><span id="77b8" class="lz kh hh lv b fi me mb l mc md">if not os.path.exists('hrnet_save'):<br/>  os.makedirs('hrnet_save')<br/>filepath="hrnet_save/weights-{epoch:04d}.hdf5"<br/>checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,<br/>                                                save_best_only=True,<br/>                                                mode='max',<br/>                                             monitor='val_mean_iou')</span></pre><h2 id="1faa" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">培养</h2><p id="6777" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">模型第一次训练30个周期，然后再训练60个周期。第二次训练从第一轮表现最好的时期开始。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="0174" class="lz kh hh lv b fi ma mb l mc md">callbacks_list = [checkpoint]<br/># First round of training<br/>history = hrnet_model.fit(train_ds,<br/>                          initial_epoch = 0,<br/>                          epochs=30,<br/>                          callbacks=callbacks_list,<br/>                          validation_data=val_ds)</span><span id="dc80" class="lz kh hh lv b fi me mb l mc md"># Second round of training<br/>initial_epoch = int(sorted(os.listdir('hrnet_save'))[-1].split('.')[0].split('-')[-1])</span><span id="3cd2" class="lz kh hh lv b fi me mb l mc md">history = hrnet_model.fit(train_ds,<br/>                          initial_epoch = initial_epoch,<br/>                          epochs=60,<br/>                          callbacks=callbacks_list,<br/>                          validation_data=val_ds)</span><span id="aa5d" class="lz kh hh lv b fi me mb l mc md"># Best model<br/>hrnet_model = tf.keras.models.load_model('./hrnet_save/'+sorted(os.listdir('hrnet_save'))[-1])</span><span id="2f91" class="lz kh hh lv b fi me mb l mc md"># save model<br/>hrnet_model.save("hrnet_model.h5")<br/>hrnet_model = tf.keras.models.load_model("hrnet_model.h5")</span></pre><h1 id="b9ec" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">推理</h1><h2 id="a09c" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">对验证数据集的推断:</h2><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="6c6c" class="lz kh hh lv b fi ma mb l mc md">for image, mask in val_ds.take(1):<br/>  for i in range(BATCH_SIZE):<br/>    pred_mask_u  = unet_model.predict(image[i][np.newaxis,:,:,:])<br/>    pred_mask_h  = hrnet_model.predict(image[i][np.newaxis,:,:,:])</span><span id="c041" class="lz kh hh lv b fi me mb l mc md">    fig = plt.figure(figsize=(14,10))</span><span id="2d0e" class="lz kh hh lv b fi me mb l mc md">    ax1 = fig.add_subplot(141)<br/>    ax1.title.set_text('Original Image')<br/>    ax1.imshow(image[i])</span><span id="a69e" class="lz kh hh lv b fi me mb l mc md">    ax2 = fig.add_subplot(142)<br/>    ax2.title.set_text('Ground Truth')<br/>    ax2.imshow(mask[i][:,:,0], cmap='gray')</span><span id="bd31" class="lz kh hh lv b fi me mb l mc md">    ax3 = fig.add_subplot(143)<br/>    ax3.title.set_text('UNet Prediction')<br/>    ax3.imshow(pred_mask_u[0,:,:,0], cmap='gray')<br/>    <br/>    ax4 = fig.add_subplot(144)<br/>    ax4.title.set_text('HRNet Prediction')<br/>    ax4.imshow(pred_mask_h[0,:,:,0], cmap='gray')<br/>   <br/>    plt.show()</span></pre><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es mw"><img src="../Images/74eb0b25d4b8b995f24a9bafc2996286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*OLF-0kdRRgmZ9fcVzqmigw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">对验证数据集的推断。</figcaption></figure><h2 id="0551" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">对测试数据集的推断:</h2><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="6bc7" class="lz kh hh lv b fi ma mb l mc md">test_filenames = test_df['files']<br/>for filename in test_filenames[:5]:<br/>  file_path = os.path.join(filename,'images')<br/>  image_path = os.path.join(file_path,os.listdir(file_path)[0])<br/>  image_string = tf.io.read_file(image_path)<br/>  image = tf.image.decode_png(image_string, channels=IMG_CHANNELS)#<br/>  image = tf.image.convert_image_dtype(image, tf.float32)<br/>  image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])</span><span id="9d09" class="lz kh hh lv b fi me mb l mc md">  pred_mask_u  = unet_model.predict(image[np.newaxis,:,:,:])<br/>  pred_mask_h  = hrnet_model.predict(image[np.newaxis,:,:,:])</span><span id="5131" class="lz kh hh lv b fi me mb l mc md">  fig = plt.figure(figsize=(10,6))</span><span id="b27e" class="lz kh hh lv b fi me mb l mc md">  ax1 = fig.add_subplot(131)<br/>  ax1.title.set_text('Original Image')<br/>  ax1.imshow(image)</span><span id="80b2" class="lz kh hh lv b fi me mb l mc md">  ax2 = fig.add_subplot(132)<br/>  ax2.title.set_text('UNet Prediction')<br/>  ax2.imshow(pred_mask_u[0,:,:,0], cmap='gray')</span><span id="1a11" class="lz kh hh lv b fi me mb l mc md">  ax3 = fig.add_subplot(133)<br/>  ax3.title.set_text('HRNet Prediction')<br/>  ax3.imshow(pred_mask_h[0,:,:,0], cmap='gray')<br/>  plt.show()</span></pre><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es mx"><img src="../Images/820fdb82088db4f315f308afee8464d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*AgfXmFZOQcEnGwaoE3Wy8w.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">测试数据集上的推断。</figcaption></figure><h1 id="ce92" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">模型比较</h1><p id="3811" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">在这一节中，我们将看到我们的两个模型的表现。首先，我们将制作一个数据框架，其中包含每个数据点的每个模型的IoU得分。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="bc7d" class="lz kh hh lv b fi ma mb l mc md"># Function to create a dataframe with iou_scores for each model and image and mask paths.</span><span id="7e48" class="lz kh hh lv b fi me mb l mc md">def metric_df(data):<br/>  unet_iou_scores = []<br/>  hrnet_iou_scores = []<br/>  m = MeanIoU(2, 0.4)<br/>  for i in tqdm(range(len(data))):<br/>    image_path = data['images'].iloc[i]<br/>    mask_path = data['masks'].iloc[i]</span><span id="a45d" class="lz kh hh lv b fi me mb l mc md">    image_string = tf.io.read_file(image_path)<br/>    image = tf.image.decode_png(image_string,channels=IMG_CHANNELS)<br/>    image = tf.image.convert_image_dtype(image, tf.float32)<br/>    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])</span><span id="cf09" class="lz kh hh lv b fi me mb l mc md">    mask_string = tf.io.read_file(mask_path)<br/>    mask = tf.image.decode_png(mask_string, channels=CLASSES)<br/>    mask = tf.image.convert_image_dtype(mask, tf.float32)<br/>    mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH])</span><span id="f48f" class="lz kh hh lv b fi me mb l mc md">    pred_mask_u = unet_model.predict(image[np.newaxis,:,:,:])<br/>    m.update_state(mask, pred_mask_u)<br/>    u_iou_score = m.result().numpy()<br/>    unet_iou_scores.append(round(u_iou_score,4))</span><span id="65de" class="lz kh hh lv b fi me mb l mc md">    pred_mask_h = hrnet_model.predict(image[np.newaxis,:,:,:])<br/>    m.update_state(mask, pred_mask_h)<br/>    h_iou_score = m.result().numpy()<br/>    hrnet_iou_scores.append(round(h_iou_score,4))</span><span id="58c2" class="lz kh hh lv b fi me mb l mc md">  data['unet_iou_scores'] = unet_iou_scores<br/>  data['hrnet_iou_scores'] = hrnet_iou_scores<br/>  return data</span><span id="6e37" class="lz kh hh lv b fi me mb l mc md"><br/>df = train.copy()<br/>df = metric_df(df)<br/>df = df.sort_values(by=['hrnet_iou_scores','unet_iou_scores'])</span></pre><h2 id="d714" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">最佳输出样本</h2><p id="e342" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">这些是获得最佳IoU分数的样品。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="bc62" class="lz kh hh lv b fi ma mb l mc md">d1 = df.tail()<br/>for i in range(5):<br/>  image_path = d1['images'].iloc[i]<br/>  mask_path = d1['masks'].iloc[i]</span><span id="68d5" class="lz kh hh lv b fi me mb l mc md">  image_string = tf.io.read_file(image_path)<br/>  image = tf.image.decode_png(image_string, channels=IMG_CHANNELS)<br/>  image = tf.image.convert_image_dtype(image, tf.float32)<br/>  image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])</span><span id="3701" class="lz kh hh lv b fi me mb l mc md">  mask_string = tf.io.read_file(mask_path)<br/>  mask = tf.image.decode_png(mask_string, channels=CLASSES)<br/>  mask = tf.image.convert_image_dtype(mask, tf.float32)<br/>  mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH])</span><span id="ea63" class="lz kh hh lv b fi me mb l mc md">  pred_mask_u  = unet_model.predict(image[np.newaxis,:,:,:])<br/>  pred_mask_h  = hrnet_model.predict(image[np.newaxis,:,:,:])</span><span id="7219" class="lz kh hh lv b fi me mb l mc md">  fig = plt.figure(figsize=(14,10))</span><span id="65ef" class="lz kh hh lv b fi me mb l mc md">  ax1 = fig.add_subplot(141)<br/>  ax1.title.set_text('Original Image')<br/>  ax1.imshow(image)</span><span id="fb00" class="lz kh hh lv b fi me mb l mc md">  ax2 = fig.add_subplot(142)<br/>  ax2.title.set_text('Ground Truth')<br/>  ax2.imshow(mask[:,:,0], cmap='gray')</span><span id="bfd4" class="lz kh hh lv b fi me mb l mc md">  ax3 = fig.add_subplot(143)<br/>  ax3.title.set_text('UNet: ' +str(round(d1['unet_iou_scores'].iloc[i],4)))<br/>  ax3.imshow(pred_mask_u[0,:,:,0], cmap='gray')</span><span id="994c" class="lz kh hh lv b fi me mb l mc md">  ax4 = fig.add_subplot(144)<br/>  ax4.title.set_text('HRNet: ' +str(round(d1['hrnet_iou_scores'].iloc[i],4)))<br/>  ax4.imshow(pred_mask_h[0,:,:,0], cmap='gray')</span><span id="36f5" class="lz kh hh lv b fi me mb l mc md">  plt.show()</span></pre><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es my"><img src="../Images/35918e2b98c7893ab68ac53a406d1359.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*Mzc1_vNG5mVjNEJhcu589Q.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">得分最高的样本。</figcaption></figure><h2 id="d9c2" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">最差输出样本</h2><p id="6c68" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">这些是IoU得分最低的样本。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="5c6a" class="lz kh hh lv b fi ma mb l mc md">d2 = df.head()<br/>for i in range(5):<br/>  image_path = d2['images'].iloc[i]<br/>  mask_path = d2['masks'].iloc[i]</span><span id="ab2c" class="lz kh hh lv b fi me mb l mc md">  image_string = tf.io.read_file(image_path)<br/>  image = tf.image.decode_png(image_string, channels=IMG_CHANNELS)<br/>  image = tf.image.convert_image_dtype(image, tf.float32)<br/>  image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])</span><span id="717a" class="lz kh hh lv b fi me mb l mc md">  mask_string = tf.io.read_file(mask_path)<br/>  mask = tf.image.decode_png(mask_string, channels=CLASSES)<br/>  mask = tf.image.convert_image_dtype(mask, tf.float32)<br/>  mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH])<br/>  <br/>  pred_mask_u  = unet_model.predict(image[np.newaxis,:,:,:])<br/>  pred_mask_h  = hrnet_model.predict(image[np.newaxis,:,:,:])</span><span id="99db" class="lz kh hh lv b fi me mb l mc md">  fig = plt.figure(figsize=(14,10))</span><span id="f89a" class="lz kh hh lv b fi me mb l mc md">  ax1 = fig.add_subplot(141)<br/>  ax1.title.set_text('Original Image')<br/>  ax1.imshow(image)</span><span id="f1c6" class="lz kh hh lv b fi me mb l mc md">  ax2 = fig.add_subplot(142)<br/>  ax2.title.set_text('Ground Truth')<br/>  ax2.imshow(mask[:,:,0], cmap='gray')</span><span id="dab7" class="lz kh hh lv b fi me mb l mc md">  ax3 = fig.add_subplot(143)<br/>  ax3.title.set_text('UNet: '+ str(round(d2['unet_iou_scores'].iloc[i],4)))<br/>  ax3.imshow(pred_mask_u[0,:,:,0], cmap='gray')</span><span id="b3a8" class="lz kh hh lv b fi me mb l mc md">  ax4 = fig.add_subplot(144)<br/>  ax4.title.set_text('HRNet: '+ str(round(d2['hrnet_iou_scores'].iloc[i],4)))<br/>  ax4.imshow(pred_mask_h[0,:,:,0], cmap='gray')</span><span id="b3fd" class="lz kh hh lv b fi me mb l mc md">  plt.show()</span></pre><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es mz"><img src="../Images/4fcf223cda99dea082ae02b670352fe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*kI3EQKjSpcpWFeTEhqeI9g.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">分数最差的样本。</figcaption></figure><h2 id="eab7" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">模型的IoU分数分布</h2><ol class=""><li id="a171" class="js jt hh iw b ix le jb lf jf lj jj lk jn ll jr ln jy jz ka bi translated"><strong class="iw hi">范围在0到1之间的借据分数分布:<br/> * </strong>下图没有提供太多信息。<br/> *我们将它放大。</li></ol><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es na"><img src="../Images/0f11e090b839ef3fae101b7dcede74ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*WRu1NdpdMefe2My4jRIZyg.png"/></div></figure><p id="b9da" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 2。范围在0.91和0.96之间时的IoU分数分布:<br/> * </strong>下面两个图看起来几乎相同。<br/> *就HRNet而言，iou得分大于0.93的分数似乎略高于UNet。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es nb"><img src="../Images/d9c7f9e321f9ebd8d7a475051b793d7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*zPhffGrf1pVS6rEDY2DPSQ.png"/></div></figure><p id="dabf" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 3。范围在0.92和0.935之间时的IoU分数分布:<br/> </strong> *从下图中我们可以看到，HRNet的情况下0.93之后的分数比UNet多。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es nb"><img src="../Images/99a6040ea1d5b824a7be0621d9b63422.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*9eAYQOGMF-DO-_BMuQVjiQ.png"/></div></figure><h2 id="db72" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">两种模型的IoU分数散点图</h2><ol class=""><li id="a21e" class="js jt hh iw b ix le jb lf jf lj jj lk jn ll jr ln jy jz ka bi translated"><strong class="iw hi">介于0和1之间时得分散点图:<br/> * </strong>两个模型的iou得分散点图。<br/> *绿色表示HRNet比UNet给出更好的iou分数。<br/> *红色表示UNet给出的iou分数优于HRNet的分数。</li></ol><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es my"><img src="../Images/4fdb4c6663586f0aa4b88e957a748b0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*SsRbDJkmiJQuFNHVz7vbIA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">介于0和1之间的分数散点图。</figcaption></figure><p id="ff6d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 2。范围在0.92到0.94之间时分数之间的散点图:<br/> * </strong>放大散点图我们可以看到绿点比红点多。<br/> *这表明HRNet的表现优于UNet。<br/> *我们可以看到，UNet表现更好的点遍布整个系列。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es nc"><img src="../Images/b29307d854d72c0cbf64a101521487f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*qeDlSOkQlrEstqf3ckYEbw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">介于0.92和0.94之间的分数散点图。</figcaption></figure><p id="1a0c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 3。范围在0.925和0.9275之间时分数之间的散点图:</strong> <br/> *在该图中我们可以看到，虽然HRNet的表现优于UNet，但差异非常小。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es nd"><img src="../Images/5904e5f45111c32df4a07ad75125600f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*4O77WBB36MQ2LbAJcr6lyQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">介于0.925和0.9275之间的分数散点图。</figcaption></figure><h1 id="b6e9" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">模型量化</h1><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="7df5" class="lz kh hh lv b fi ma mb l mc md">tflite_models_dir = pathlib.Path("/content/drive/MyDrive/CaseStudy2/")<br/>tflite_models_dir.mkdir(exist_ok=True, parents=True)</span></pre><h2 id="29a6" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">UNet模型的训练后量化:</h2><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="3b21" class="lz kh hh lv b fi ma mb l mc md"># UNet Model<br/>unet_model = tf.keras.models.load_model("/content/drive/MyDrive/CaseStudy2/unet_model.h5")</span><span id="ae07" class="lz kh hh lv b fi me mb l mc md"># Post Training quantized UNet model<br/>converter = tf.lite.TFLiteConverter.from_keras_model(unet_model)<br/>converter.optimizations = [tf.lite.Optimize.DEFAULT]<br/>quant_unet_model = converter.convert()</span><span id="5c0d" class="lz kh hh lv b fi me mb l mc md"># Save the quantized UNet model:<br/>quant_unet_file = tflite_models_dir/"quant_unet_model.tflite"<br/>quant_unet_file.write_bytes(quant_unet_model)</span></pre><h2 id="13f7" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">人力资源网培训后量化；</h2><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="342b" class="lz kh hh lv b fi ma mb l mc md"># HRNet Model<br/>hrnet_model = tf.keras.models.load_model("/content/drive/MyDrive/CaseStudy2/hrnet_model.h5")</span><span id="8f41" class="lz kh hh lv b fi me mb l mc md"># Post Training quantized HRNet model<br/>converter = tf.lite.TFLiteConverter.from_keras_model(hrnet_model)<br/>converter.optimizations = [tf.lite.Optimize.DEFAULT]<br/>quant_hrnet_model = converter.convert()</span><span id="6f10" class="lz kh hh lv b fi me mb l mc md"># Save the quantized HRNet model:<br/>quant_hrnet_file = tflite_models_dir/"quant_hrnet_model.tflite"<br/>quant_hrnet_file.write_bytes(quant_hrnet_model)</span></pre><h2 id="69ad" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">量化前后模型的大小:</h2><p id="7f2c" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">*我们可以看到UNet和HRNet模型的文件大小都有相当大的减少。<br/> *当我们希望在小型设备上部署我们的模型时，这种缩减是很好的。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="b47d" class="lz kh hh lv b fi ma mb l mc md">print("UNet model in Mb:", os.path.getsize("/content/drive/MyDrive/CaseStudy2/unet_model.h5") / float(2**20))</span><span id="e349" class="lz kh hh lv b fi me mb l mc md">print("Quantized UNet in Mb:", os.path.getsize("/content/drive/MyDrive/CaseStudy2/quant_unet_model.tflite") / float(2**20))</span><span id="95a5" class="lz kh hh lv b fi me mb l mc md">print("Float HRNet in Mb:", os.path.getsize("/content/drive/MyDrive/CaseStudy2/hrnet_model.h5") / float(2**20))</span><span id="cf70" class="lz kh hh lv b fi me mb l mc md">print("Quantized HRNet in Mb:", os.path.getsize("/content/drive/MyDrive/CaseStudy2/quant_hrnet_model.tflite") / float(2**20))</span></pre><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es ne"><img src="../Images/45c70e7369272468ed7fe7a92e2c398d.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*-9KjBuPsKkcbk4Lbt3LbzQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">所有型号的大小，以MB为单位。</figcaption></figure><h2 id="41ef" class="lz kh hh bd ki mi mj mk km ml mm mn kq jf mo mp ku jj mq mr ky jn ms mt lc mu bi translated">未量化和训练后量化模型的评估；</h2><ul class=""><li id="be55" class="js jt hh iw b ix le jb lf jf lj jj lk jn ll jr jx jy jz ka bi translated"><strong class="iw hi">量化的UNet: </strong></li></ul><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="1eeb" class="lz kh hh lv b fi ma mb l mc md"># Importing Quantized UNet model<br/>u_interpreter = tf.lite.Interpreter(model_path="/content/drive/MyDrive/CaseStudy2/quant_unet_model.tflite")</span><span id="3066" class="lz kh hh lv b fi me mb l mc md"># Function to predict segments using quantized UNet model<br/>def lite_unet_model(images):<br/>  u_interpreter.allocate_tensors()<br/>  u_interpreter.set_tensor(u_interpreter.get_input_details()[0]['index'], images)<br/>  u_interpreter.invoke()<br/>  return u_interpreter.get_tensor(u_interpreter.get_output_details()[0]['index'])</span></pre><ul class=""><li id="1eaa" class="js jt hh iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated"><strong class="iw hi">量化HRNet: </strong></li></ul><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="8dfd" class="lz kh hh lv b fi ma mb l mc md"># Importing Quantized HRNet model<br/>h_interpreter = tf.lite.Interpreter(model_path="/content/drive/MyDrive/CaseStudy2/quant_hrnet_model.tflite")</span><span id="c1ee" class="lz kh hh lv b fi me mb l mc md"># Function to predict segments using quantized HRNet model<br/>def lite_hrnet_model(images):<br/>  h_interpreter.allocate_tensors()<br/>  h_interpreter.set_tensor(h_interpreter.get_input_details()[0]['index'], images)<br/>  h_interpreter.invoke()<br/>  return h_interpreter.get_tensor(h_interpreter.get_output_details()[0]['index'])</span></pre><ul class=""><li id="08d2" class="js jt hh iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated"><strong class="iw hi">所有模型的样本预测</strong> : <br/> *在某些点上，UNet的表现优于HRNet，在某些点上，HRNet的表现优于UNet。<br/> *还可以看出，对于那些UNet表现优于HRNet的数据点，其量化版本显示了相同的行为。<br/> *总体来看，四款机型的IoU评分差异非常小。</li></ul><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="939e" class="lz kh hh lv b fi ma mb l mc md">df = train.sample(n=8, random_state=1)<br/>m = MeanIoU(2, 0.4)<br/>for i in range(len(df)):<br/>  image_path = df['images'].iloc[i]<br/>  mask_path = df['masks'].iloc[i]<br/>  <br/>  image_string = tf.io.read_file(image_path)<br/>  image = tf.image.decode_png(image_string, channels=IMG_CHANNELS)#<br/>  image = tf.image.convert_image_dtype(image, tf.float32)<br/>  image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])</span><span id="fc65" class="lz kh hh lv b fi me mb l mc md">  mask_string = tf.io.read_file(mask_path)<br/>  mask = tf.image.decode_png(mask_string, channels=CLASSES)<br/>  mask = tf.image.convert_image_dtype(mask, tf.float32)<br/>  mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH])</span><span id="e02e" class="lz kh hh lv b fi me mb l mc md">  pred_mask_u = unet_model.predict(image[np.newaxis,:,:,:])<br/>  m.update_state(mask, pred_mask_u)<br/>  u_iou_score = m.result().numpy()</span><span id="8865" class="lz kh hh lv b fi me mb l mc md">  pred_mask_qu = lite_unet_model(image[np.newaxis,:,:,:])[0]<br/>  m.update_state(mask, pred_mask_qu)<br/>  qu_iou_score = m.result().numpy()</span><span id="570e" class="lz kh hh lv b fi me mb l mc md">  pred_mask_h = hrnet_model.predict(image[np.newaxis,:,:,:])<br/>  m.update_state(mask, pred_mask_h)<br/>  h_iou_score = m.result().numpy()</span><span id="e45b" class="lz kh hh lv b fi me mb l mc md">  pred_mask_qh = lite_hrnet_model(image[np.newaxis,:,:,:])[0]<br/>  m.update_state(mask, pred_mask_qh)<br/>  qh_iou_score = m.result().numpy()</span><span id="3ba5" class="lz kh hh lv b fi me mb l mc md">  fig = plt.figure(figsize=(16,14))</span><span id="60e7" class="lz kh hh lv b fi me mb l mc md">  ax1 = fig.add_subplot(161)<br/>  ax1.title.set_text('Original Image')<br/>  ax1.imshow(image)</span><span id="2cce" class="lz kh hh lv b fi me mb l mc md">  ax2 = fig.add_subplot(162)<br/>  ax2.title.set_text('Ground Truth')<br/>  ax2.imshow(mask[:,:,0], cmap='gray')</span><span id="d34c" class="lz kh hh lv b fi me mb l mc md">  ax3 = fig.add_subplot(163)<br/>  ax3.title.set_text('UNet: '+ str(round(u_iou_score,4)))<br/>  ax3.imshow(pred_mask_u[0,:,:,0], cmap='gray')</span><span id="5876" class="lz kh hh lv b fi me mb l mc md">  ax4 = fig.add_subplot(164)<br/>  ax4.title.set_text('Quant_UNet: '+ str(round(qu_iou_score,4)))<br/>  ax4.imshow(pred_mask_qu[:,:,0], cmap='gray')</span><span id="e942" class="lz kh hh lv b fi me mb l mc md">  ax5 = fig.add_subplot(165)<br/>  ax5.title.set_text('HRNet: '+ str(round(h_iou_score,4)))<br/>  ax5.imshow(pred_mask_h[0,:,:,0], cmap='gray')</span><span id="8320" class="lz kh hh lv b fi me mb l mc md">  ax6 = fig.add_subplot(166)<br/>  ax6.title.set_text('Quant_HRNet: '+ str(round(qh_iou_score,4)))<br/>  ax6.imshow(pred_mask_qh[:,:,0], cmap='gray')<br/> <br/>  plt.show()</span></pre><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/e35aa466fd79c50c9731e689d5bac85d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*uIB-XwhyQFgg4IN-WygVug.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">具有相应IoU分数的所有模型的样本预测。</figcaption></figure><ul class=""><li id="f351" class="js jt hh iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated"><strong class="iw hi">30个图像样本的平均IoU得分:<br/> * </strong>我们可以得出结论，在UNet和HRNet中，HRNet具有更好的平均IoU得分。类似地，量化后HRNet具有更好的平均IoU分数。<br/> *同样对于这个样本大小，量化HRNet比浮点HRNet具有更好的平均IoU分数。如果我们想在更小的设备上部署模型，那么量化HRNet将是一个更好的选择。</li></ul><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="22c9" class="lz kh hh lv b fi ma mb l mc md">df = train.sample(n=30, random_state=1)<br/>unet_iou_scores = []<br/>quant_unet_iou_scores = []<br/>hrnet_iou_scores = []<br/>quant_hrnet_iou_scores = []<br/>m = MeanIoU(2, 0.4)<br/>for i in range(len(df)):<br/>  image_path = df['images'].iloc[i]<br/>  mask_path = df['masks'].iloc[i]</span><span id="a589" class="lz kh hh lv b fi me mb l mc md">  image_string = tf.io.read_file(image_path)<br/>  image = tf.image.decode_png(image_string, channels=IMG_CHANNELS)<br/>  image = tf.image.convert_image_dtype(image, tf.float32)<br/>  image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])</span><span id="d618" class="lz kh hh lv b fi me mb l mc md">  mask_string = tf.io.read_file(mask_path)<br/>  mask = tf.image.decode_png(mask_string, channels=CLASSES)<br/>  mask = tf.image.convert_image_dtype(mask, tf.float32)<br/>  mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH])</span><span id="0a54" class="lz kh hh lv b fi me mb l mc md">  pred_mask_u = unet_model.predict(image[np.newaxis,:,:,:])<br/>  m.update_state(mask, pred_mask_u)<br/>  u_iou_score = m.result().numpy()<br/>  unet_iou_scores.append(round(u_iou_score,4))</span><span id="0ccf" class="lz kh hh lv b fi me mb l mc md">  pred_mask_qu = lite_unet_model(image[np.newaxis,:,:,:])[0]<br/>  m.update_state(mask, pred_mask_qu)<br/>  qu_iou_score = m.result().numpy()<br/>  quant_unet_iou_scores.append(round(qu_iou_score,4))</span><span id="d55c" class="lz kh hh lv b fi me mb l mc md">  pred_mask_h = hrnet_model.predict(image[np.newaxis,:,:,:])<br/>  m.update_state(mask, pred_mask_h)<br/>  h_iou_score = m.result().numpy()<br/>  hrnet_iou_scores.append(round(h_iou_score,4))</span><span id="6358" class="lz kh hh lv b fi me mb l mc md">  pred_mask_qh = lite_hrnet_model(image[np.newaxis,:,:,:])[0]<br/>  m.update_state(mask, pred_mask_qh)<br/>  qh_iou_score = m.result().numpy()<br/>  quant_hrnet_iou_scores.append(round(qh_iou_score,4))</span><span id="3803" class="lz kh hh lv b fi me mb l mc md">print('The average IoU Score for UNet model: ', np.mean(np.array(unet_iou_scores)))</span><span id="2650" class="lz kh hh lv b fi me mb l mc md">print('The average IoU Score for Quantized UNet model: ', np.mean(np.array(quant_unet_iou_scores)))</span><span id="2433" class="lz kh hh lv b fi me mb l mc md">print('The average IoU Score for HRNet model: ', np.mean(np.array(hrnet_iou_scores)))</span><span id="b2f2" class="lz kh hh lv b fi me mb l mc md">print('The average IoU Score for Quantized HRNet model: ', np.mean(np.array(quant_hrnet_iou_scores)))</span></pre><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es ng"><img src="../Images/fa8ac9a25c1cd75f7ca8bb369be3a426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*x1HdK88dqVTjk00UY6U1Gg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">样本量为30的模型的平均IoU分数。</figcaption></figure><h1 id="ba95" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h1><ul class=""><li id="9439" class="js jt hh iw b ix le jb lf jf lj jj lk jn ll jr jx jy jz ka bi translated">总的来说，如果我们看到HRNet比UNet表现更好，虽然差异非常小。</li><li id="84b2" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">如果我们想在智能手机或raspberry pi等小型设备上部署这项工作，那么使用量化模型会更好。</li><li id="21e9" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">量化模型尺寸非常小，性能几乎没有下降。</li><li id="6f98" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">在这种情况下，量化的模型表现得和未量化的模型一样好。</li><li id="3496" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">量化后的模型大多稍逊于未量化的模型。但是在某些情况下，量化模型给出的结果比未量化的模型稍好。</li><li id="683d" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">对于UNet比HRNet表现更好的情况，它们的量化版本显示了相同的行为，反之亦然。</li></ul><h1 id="2385" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">未来的工作</h1><p id="5360" class="pw-post-body-paragraph iu iv hh iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">这项工作将扩展到探索最新分段体系结构。此外，还可以探索进一步提高本工作中使用的模型的性能。</p><h1 id="46a0" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">链接</h1><ul class=""><li id="305c" class="js jt hh iw b ix le jb lf jf lj jj lk jn ll jr jx jy jz ka bi translated"><strong class="iw hi"> Github链接:<br/></strong><a class="ae it" href="https://github.com/sahu-mak/Nuclei_Detection_CaseStudy" rel="noopener ugc nofollow" target="_blank">https://github.com/sahu-mak/Nuclei_Detection_CaseStudy</a></li><li id="3567" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hi"> Linkedin链接:</strong><br/><a class="ae it" href="https://www.linkedin.com/in/sahumayank/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/sahumayank/</a></li><li id="925b" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hi">细流app视频链接:</strong><br/><a class="ae it" href="https://youtu.be/7F6eKNpPep0" rel="noopener ugc nofollow" target="_blank">https://youtu.be/7F6eKNpPep0</a></li></ul><h1 id="ede1" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">参考</h1><ul class=""><li id="918a" class="js jt hh iw b ix le jb lf jf lj jj lk jn ll jr jx jy jz ka bi translated"><a class="ae it" href="https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2" rel="noopener" target="_blank">https://towards data science . com/metrics-to-evaluate-your-semantic-segmentation-model-6 BCB 99639 aa 2</a></li><li id="e169" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><a class="ae it" href="https://cs230.stanford.edu/blog/datapipeline/" rel="noopener ugc nofollow" target="_blank">https://cs230.stanford.edu/blog/datapipeline/</a></li><li id="61bc" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><a class="ae it" href="https://www.tensorflow.org/guide/data#basic_mechanics" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/guide/data#basic_mechanics</a></li><li id="74f1" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">https://arxiv.org/abs/1505.04597</li><li id="9001" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">人力资源网—<a class="ae it" href="https://arxiv.org/abs/1908.07919" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1908.07919</a></li><li id="4374" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><a class="ae it" href="https://github.com/1044197988/TF.Keras-Commonly-used-models/blob/master/%E5%B8%B8%E7%94%A8%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B/HRNet.py" rel="noopener ugc nofollow" target="_blank">https://github.com/1044197988/TF.keras-常用-模型/blob/master/% E5 % B8 % B8 % E7 % 94% A8 % E5 % 88% 86% E5 % 89% B2 % E6 % A8 % A1 % E5 % 9E % 8B/HR net . py</a></li><li id="3c7c" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><a class="ae it" href="https://www.tensorflow.org/lite/performance/post_training_quant" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/lite/performance/post _ training _ quant</a></li><li id="40ec" class="js jt hh iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><a class="ae it" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com</a></li></ul></div></div>    
</body>
</html>