<html>
<head>
<title>Understanding the BERT Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解伯特模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-the-bert-model-a04e1c7933a9?source=collection_archive---------0-----------------------#2021-09-05">https://medium.com/analytics-vidhya/understanding-the-bert-model-a04e1c7933a9?source=collection_archive---------0-----------------------#2021-09-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/71676ab151917d667cf43ba111124347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ezaSmCMRQgW1L37Z"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">苏阿德·卡玛丁在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><blockquote class="iv"><p id="2381" class="iw ix hi bd iy iz ja jb jc jd je jf dx translated"><em class="jg"> Bert是最常用的文本嵌入模型之一。它彻底改变了NLP任务的世界。在这个博客中，我们将开始什么是Bert模型，它与其他嵌入模型有何不同。然后，我们将详细研究Bert的工作及其配置。</em></p></blockquote><blockquote class="jh ji jj"><p id="6312" class="jk jl jm jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh jf hb bi translated"><strong class="jn hj"> <em class="hi">涉及主题</em> </strong> <em class="hi"> : </em></p></blockquote><ul class=""><li id="e6bc" class="ki kj hi jn b jo kk js kl km kn ko kp kq kr jf ks kt ku kv bi translated">伯特的基本思想</li><li id="2924" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf ks kt ku kv bi translated">伯特的工作</li><li id="abab" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf ks kt ku kv bi translated">Bert的配置</li><li id="0de5" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf ks kt ku kv bi translated">预先训练Bert模型</li><li id="fed0" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf ks kt ku kv bi translated">训练前程序</li><li id="d0d3" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf ks kt ku kv bi translated">子词标记化算法</li></ul><blockquote class="iv"><p id="231b" class="iw ix hi bd iy iz ja jb jc jd je jf dx translated">伯特的基本思想</p></blockquote><p id="d2cf" class="pw-post-body-paragraph jk jl hi jn b jo jp jq jr js jt ju jv km jx jy jz ko kb kc kd kq kf kg kh jf hb bi translated">Bert代表<strong class="jn hj"> <em class="jm">双向编码器表示转换器</em>。</strong>它在自然语言处理领域取得了重大突破，为许多自然语言处理任务提供了更好的结果，如问答、文本生成、句子分类等等。它成功的主要原因之一是它是一个基于上下文的嵌入模型，不像任何流行的嵌入模型，如word2vec，它是一个上下文无关的。</p><p id="270b" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">首先让我们理解上下文无关和基于上下文的模型之间的区别。考虑下面的句子</p><p id="3aba" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">句子A:他被蟒蛇咬了。</em>T19】</strong></p><p id="339d" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"><em class="jm">B句:Python是我最喜欢的编程语言。</em> </strong></p><p id="4cd7" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">通过阅读这两个句子，我们可以理解单词<strong class="jn hj"><em class="jm">【Python】</em></strong>在这两个句子中的意思是不同的。在<strong class="jn hj"> <em class="jm">句A </em> </strong>中的单词<strong class="jn hj"><em class="jm">【Python】</em></strong>指代一条蛇，而在<strong class="jn hj"> <em class="jm">句B中的</em> </strong>指代一种编程语言。</p><p id="026e" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">现在，如果使用word2vec这样的嵌入模型来嵌入单词<strong class="jn hj"><em class="jm">【Python】</em></strong>，我们将为两个句子获得相同的嵌入，因此将在两个句子中呈现单词的含义。这是因为word2vec是一个上下文无关的模型，它会忽略上下文，对单词<strong class="jn hj"><em class="jm">【Python】</em></strong>给予相同的嵌入，而不考虑上下文。</p><p id="ea05" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">另一方面，Bert是一个基于上下文的模型。它将理解上下文，然后基于上下文生成单词的嵌入。所以，对于前面两个单词，它会给单词<strong class="jn hj"><em class="jm">“Python”不同的嵌入。</em>T51】</strong></p><p id="cb42" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">但是这是怎么做到的呢？伯特是如何理解语境的？</em>T55】</strong></p><p id="d1bb" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">让我们以句子A为例，在这种情况下，Bert将句子中的每个单词与句子中的所有单词联系起来，以获得每个单词的上下文含义。通过这样做，伯特可以理解，单词<strong class="jn hj"> <em class="jm">【蟒蛇】</em> </strong> <em class="jm">表示蛇。</em>同样，在句子B中，Bert理解单词<strong class="jn hj"><em class="jm">“Python”</em></strong><em class="jm">表示一种编程语言。</em></p><div class="le lf lg lh fd ab cb"><figure class="li ij lj lk ll lm ln paragraph-image"><img src="../Images/f33f9ee2160aa392271f969f1f282da7.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*m0BA7DAarEs5_IiTxkkKIQ.png"/></figure><figure class="li ij lo lk ll lm ln paragraph-image"><img src="../Images/5bc5496ecea6b5866e831694eeb71d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*lcIy1ffLYqKsBRhNYMeYsQ.png"/></figure></div><p id="f419" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">现在的问题是伯特到底是如何工作的？它是如何理解上下文的？</em>T15】</strong></p><blockquote class="iv"><p id="b1f9" class="iw ix hi bd iy iz lp lq lr ls lt jf dx translated">伯特的工作</p></blockquote><p id="e1fa" class="pw-post-body-paragraph jk jl hi jn b jo jp jq jr js jt ju jv km jx jy jz ko kb kc kd kq kf kg kh jf hb bi translated">顾名思义，Bert是基于变压器模型的。要简单了解什么是变压器，请参考我的<a class="ae iu" rel="noopener" href="/analytics-vidhya/the-ultimate-transformer-and-the-attention-you-need-1dafaf8cb6c7"> <strong class="jn hj"> <em class="jm">以前的博客</em> </strong> </a>关于变压器及其工作原理。</p><p id="93a0" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">在Transformer中，我们将句子作为输入提供给转换器的编码器，它将句子中每个单词的表示作为输出返回。这就是Bert的确切含义— <strong class="jn hj"> <em class="jm">变压器的编码器表示</em> </strong>但是<strong class="jn hj"> <em class="jm">双向，因为变压器的编码器是双向的。</em>T29】</strong></p><p id="1ffb" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">一旦我们将句子作为输入馈送给编码器，编码器就会使用多头注意力机制来理解上下文。</em> </strong></p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/7750f298009dd8f947c5b1b53abca066.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*OcWONCAB8c1AeQNo1A6xDQ.png"/></div></figure><blockquote class="iv"><p id="200e" class="iw ix hi bd iy iz ja jb jc jd je jf dx translated">Bert的配置</p></blockquote><p id="ffbb" class="pw-post-body-paragraph jk jl hi jn b jo jp jq jr js jt ju jv km jx jy jz ko kb kc kd kq kf kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">研究者们提出了伯特的两种主要构型</em> </strong></p><ul class=""><li id="830c" class="ki kj hi jn b jo kk js kl km kn ko kp kq kr jf ks kt ku kv bi translated"><strong class="jn hj"><em class="jm"/></strong></li><li id="434f" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf ks kt ku kv bi translated"><strong class="jn hj">T43【伯特-大号】T44</strong></li></ul><p id="5752" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm"> Bert-base </em> </strong> —有<strong class="jn hj"> <em class="jm"> 12个编码器层</em> </strong>堆叠在另一层的顶部，<strong class="jn hj"> <em class="jm"> 12个注意头</em> </strong>和由<strong class="jn hj"> <em class="jm"> 768个隐藏单元</em> </strong>组成。参数Bert-base总数为<strong class="jn hj"><em class="jm">1.1亿</em> </strong>。</p><p id="612c" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"><em class="jm">Bert-large</em></strong>——有<strong class="jn hj"> <em class="jm"> 24个编码器</em> </strong>层层叠放，<strong class="jn hj"> <em class="jm"> 16个注意</em> </strong>头和由<strong class="jn hj"> <em class="jm"> 1024个隐藏单元</em> </strong>。Bert-large的参数总数为<strong class="jn hj"> <em class="jm"> 3400万</em> </strong>。</p><p id="577e" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">除了两种标准配置之外，还有其他Bert配置，例如Bert-mini、Bert-tiny、Bert-medium等。</p><p id="7a70" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">我们可以在计算资源有限的环境中使用较小的Bert配置。然而，该标准给出了更准确的结果，因为它们被广泛使用。</p><blockquote class="iv"><p id="97e0" class="iw ix hi bd iy iz lp lq lr ls lt jf dx translated">预先训练Bert模型</p></blockquote><p id="a1ec" class="pw-post-body-paragraph jk jl hi jn b jo jp jq jr js jt ju jv km jx jy jz ko kb kc kd kq kf kg kh jf hb bi translated">预训练一个模型意味着用一个巨大的数据集为一个特定的任务训练一个模型，并保存训练好的模型。现在对于一个新的任务，我们将初始化已经训练好的模型的权重，而不是用随机权重初始化一个新的模型。由于模型是在一个巨大的数据集上训练的，而不是为一个新任务从头开始训练模型，我们使用预训练的模型，并根据新任务调整(微调)其权重。这是一种迁移学习。</p><p id="b456" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">Bert模型使用两个有趣的任务在巨大的语料库上进行预训练，这两个任务被称为掩蔽语言建模和下一句预测。对于一个新的任务，比方说问答，我们使用预训练的Bert并微调其权重。</p><blockquote class="jh ji jj"><p id="5eae" class="jk jl jm jn b jo kk jq jr js kl ju jv jw lb jy jz ka lc kc kd ke ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="hi">输入数据表示</em> </strong></p></blockquote><p id="4082" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">在将输入馈送给Bert之前，我们使用3个嵌入层将输入转换成嵌入</p><ul class=""><li id="f9c2" class="ki kj hi jn b jo kk js kl km kn ko kp kq kr jf ks kt ku kv bi translated">令牌嵌入</li><li id="bca3" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf ks kt ku kv bi translated">片段嵌入</li><li id="0dc5" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf ks kt ku kv bi translated">位置嵌入</li></ul><p id="5b31" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">令牌嵌入</em> </strong></p><p id="e8ae" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">我们举个例子来理解这一点。考虑下面两句话</p><p id="5107" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">巴黎是一座美丽的城市。</p><p id="719a" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">句子B:我爱巴黎。</p><p id="17b6" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">首先，我们对这两个句子进行了标记，我们的输出如下</p><p id="90aa" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm"> |tokens =【巴黎，是，一个，美丽，城市，我，爱，巴黎】</em> </strong></p><p id="09ce" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">然后，我们在令牌的开头添加一个名为[cls]的新令牌</p><p id="af72" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm"> |tokens =[[cls]，巴黎，是，一个，美丽，城市，我，爱，巴黎] </em> </strong></p><p id="1ec3" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">然后我们在每个句子的末尾加上[sep]标记</p><p id="3857" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm"> |tokens =[[cls]，巴黎，是，a，美丽，城市，[sep]，我，爱，巴黎] </em> </strong></p><p id="ea78" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"><em class="jm">【cls】</em></strong>记号用于分类任务，而<strong class="jn hj"><em class="jm">【sep】</em></strong>用于表示每个句子的结束。现在，在将令牌提供给Bert之前，我们使用称为令牌嵌入层的嵌入层将令牌转换为嵌入。注意，嵌入的值将在训练期间学习。</p><p id="a1f8" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">段嵌入</em> </strong></p><p id="f845" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">片段嵌入用于区分两个给定的句子。</p><p id="1c14" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">让我们再次考虑我们之前的例子。</p><p id="4caa" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm"> |tokens =[[cls]，巴黎，是，a，美丽，城市，[sep]，我，爱，巴黎] </em> </strong></p><p id="b7c6" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">现在，除了<strong class="jn hj"><em class="jm">【sep】</em></strong>之外，我们还必须给我们的模型一些指示符，以区分这两个句子。为此，我们将输入令牌提供给片段嵌入层。</p><p id="e759" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">片段嵌入层仅返回两个嵌入e A(句子A的嵌入)或e B(句子B的嵌入)中的一个，即如果输入标记属于句子A，则EA否则EB用于句子B</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/b985ef3f5c67ace6f8574675dd0b14ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0QTiIoJ4Smt6UP3Dd8dbAw.png"/></div></div></figure><p id="33a9" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">位置嵌入</em> </strong></p><p id="9e99" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">因为我们知道转换器不使用任何递归机制，而是并行处理所有单词，所以我们需要提供一些与单词顺序相关的信息，所以我们使用了位置编码。</p><p id="e987" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">我们知道Bert本质上是转换器的编码器，所以我们需要在将单词直接输入到我们的Bert之前给出单词在句子中的位置信息。</p><p id="1d05" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">最终表现</em> </strong></p><p id="b0a9" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">现在让我们看看输入数据的最终表示</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/7662b68fceb30b6029265c6b29798e4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u086OuB_xgqsSTZqMHOblw.png"/></div></div></figure><p id="8060" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">分词器</em> </strong></p><p id="376f" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">Bert使用一种特殊类型的记号赋予器，称为<strong class="jn hj"> <em class="jm">单词块记号赋予器</em> </strong>。单词标记器遵循子单词标记器方案。让我们理解单词块标记器，考虑一个句子</p><p id="53fe" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">“让我们开始预训练模型”</em> </strong></p><p id="32ea" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">现在，如果我们使用单词块来标记句子，那么将获得</p><p id="8dfd" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm"> |token = [let，us，start，pre，###train，##ing，the，model] </em> </strong></p><p id="dd8b" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">在对句子进行分词时，我们的单词预处理被分成3个部分，这是因为我们的单词片段分词器首先检查单词是否在我们的词汇表中。如果这个单词存在，那么它将被用作一个标记，如果不存在，那么我们的单词将被递归地分割成子单词，直到在我们的语料库中找到这些子单词。这个过程有效地处理了<strong class="jn hj"> <em class="jm">中的不在词汇表中的单词。</em> </strong></p><blockquote class="iv"><p id="8701" class="iw ix hi bd iy iz lp lq lr ls lt jf dx translated">培训前策略</p></blockquote><p id="c7e3" class="pw-post-body-paragraph jk jl hi jn b jo jp jq jr js jt ju jv km jx jy jz ko kb kc kd kq kf kg kh jf hb bi translated">Bert模型针对以下两项任务进行了预训练:</p><ol class=""><li id="9a3b" class="ki kj hi jn b jo kk js kl km kn ko kp kq kr jf lx kt ku kv bi translated">掩蔽语言建模</li><li id="5710" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf lx kt ku kv bi translated">下一句预测</li></ol><p id="d02a" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">在直接进入这两个模型之前，让我们先了解一下语言建模。 </p><blockquote class="jh ji jj"><p id="eeec" class="jk jl jm jn b jo kk jq jr js kl ju jv jw lb jy jz ka lc kc kd ke ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="hi">语言建模</em> </strong></p></blockquote><p id="4351" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">在语言建模任务中，我们训练模型来预测给定单词序列的下一个单词。我们可以将语言建模分为两个方面:</p><ol class=""><li id="f7a9" class="ki kj hi jn b jo kk js kl km kn ko kp kq kr jf lx kt ku kv bi translated"><strong class="jn hj"> <em class="jm">自回归语言建模</em> </strong></li><li id="ca54" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf lx kt ku kv bi translated"><strong class="jn hj"> <em class="jm">自动编码语言建模</em> </strong></li></ol><p id="4b20" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">自回归语言建模</em> </strong></p><p id="3f56" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">我们可以将自回归语言建模分类如下</p><ul class=""><li id="673c" class="ki kj hi jn b jo kk js kl km kn ko kp kq kr jf ks kt ku kv bi translated">向前(从左到右)预测</li><li id="0a1e" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf ks kt ku kv bi translated">向后(从右到左)预测</li></ul><p id="e169" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">现在考虑我们之前的例子<strong class="jn hj"> <em class="jm">“巴黎是个美丽的城市。我爱巴黎”。</em> </strong>我们把城市这个词去掉，加个空格。现在，我们的模型必须预测空白。如果我们使用前向预测，那么我们的模型从左到右读取空白处的所有单词，以便进行预测。</p><p id="537c" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">巴黎是个美丽的__。</em>T51】</strong></p><p id="6a69" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">但是如果我们使用反向预测，那么我们的模型从右到左读取所有的单词，以便进行预测</p><p id="2e3a" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm"> __。我爱巴黎。</em>T3】</strong></p><p id="5154" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">因此，自回归模型是单向的，这意味着它们只能从一个方向解读句子。</p><p id="49d7" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">自动编码语言建模</em> </strong></p><p id="a55d" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">自动编码语言建模利用了前向和后向预测，因此我们可以说自动编码模型本质上是双向的。从两个方向阅读句子会使句子更加清晰，因此会得到更好的结果。Bert是一种自动编码语言模型。</p><blockquote class="jh ji jj"><p id="4e9c" class="jk jl jm jn b jo kk jq jr js kl ju jv jw lb jy jz ka lc kc kd ke ld kg kh jf hb bi translated"><strong class="jn hj">蒙版语言建模</strong></p></blockquote><p id="3921" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">在给定输入的屏蔽语言建模任务中，我们随机屏蔽15%的单词，并训练网络来预测屏蔽单词。为了预测屏蔽词，我们的模型从两个方向读取。</p><p id="43ac" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">让我们来了解一下掩码语言建模是如何工作的。</p><p id="1878" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm"> |tokens =[[cls]，Paris，is，a，漂亮，[Mask]，[sep]，I，love，Paris ] </em> </strong></p><p id="08b9" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">在前面的例子中，我们用<strong class="jn hj"><em class="jm">【Mask】</em></strong>token替换单词city。</p><p id="9eea" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">以这种方式屏蔽令牌将在预训练和微调之间产生差异，这意味着我们通过预测<strong class="jn hj"> <em class="jm">【屏蔽】</em> </strong>令牌来训练Bert。在训练之后，我们可以对预训练的Bert进行微调，以用于下游任务，例如情感分析。但是在微调期间，我们不会在输入中有任何<strong class="jn hj"><em class="jm">【Mask】</em></strong>标记，这将导致Bert的预训练方式和它用于微调的方式之间的不匹配。</p><p id="8b19" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">为了解决这个问题，我们玩80-10-10%规则。我们知道我们随机屏蔽了句子的15%,现在对于这15 %,我们做如下处理:</p><ul class=""><li id="e7df" class="ki kj hi jn b jo kk js kl km kn ko kp kq kr jf ks kt ku kv bi translated">80 %的时间我们用<strong class="jn hj"> <em class="jm">【面具】</em> </strong>令牌替换单词。</li><li id="a3e2" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf ks kt ku kv bi translated">对于10 %的时间，我们用随机令牌替换令牌，这样我们的输入将如下</li></ul><p id="3276" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"><em class="jm">【【cls】，巴黎，是，a，美，[sep]，爱，我】</em> </strong></p><ul class=""><li id="bb03" class="ki kj hi jn b jo kk js kl km kn ko kp kq kr jf ks kt ku kv bi translated">10 %的时间我们不做任何改变。</li></ul><p id="07de" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">在标记化和屏蔽之后，我们将输入标记馈送到标记、分段和位置嵌入层，并获得输入嵌入。</p><p id="0017" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">现在我们将输入嵌入到Bert中。Bert接受输入并返回每个令牌的表示作为输出</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/0f424d2a510c9f7aaca7652a495f83a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7TdPa1j3HenGRMNSzlIheg.png"/></div></div></figure><p id="1642" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">为了预测屏蔽令牌，我们将由Bert返回的屏蔽令牌R[masked]的表示馈送给具有SoftMax激活函数的前馈。现在，前馈网络将R[masked]作为输入，并返回我们词汇表中的单词成为我们的单词的概率。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/201b1bc58a1d9504ebe139a3aa28a712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u8Z8EJw3AXlLkMfwkKw50Q.png"/></div></div></figure><p id="d30d" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">掩蔽语言建模任务也称为完形填空任务。屏蔽输入标记时，我们也可以使用稍微不同的方法，称为全词屏蔽。</p><blockquote class="jh ji jj"><p id="e8be" class="jk jl jm jn b jo kk jq jr js kl ju jv jw lb jy jz ka lc kc kd ke ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="hi">全字掩蔽</em> </strong></p></blockquote><p id="501f" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">考虑句子<strong class="jn hj"> <em class="jm">“让我们开始预训练模型”，</em> </strong>在使用分词器后我们将得到</p><p id="e1ca" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm"> |token =【让，合，开始，预，# # #列车，###ing，the，model】</em></strong></p><p id="8af0" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">接下来我们将添加<strong class="jn hj"><em class="jm">【Cls】</em></strong>令牌并屏蔽单词的15 %</p><p id="d868" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm"> |token = [[CLS]，[掩码]，us，start，pre，[掩码]，###ing，the，model] </em> </strong></p><p id="2c37" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">正如我们可以看到的，我们已经按照单词预处理的部分屏蔽了一个子单词。在整个单词屏蔽中，子单词被屏蔽，然后我们屏蔽对应于该子单词的所有单词，保留我们的屏蔽率，即15%。</p><p id="290f" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm"> |token = [[CLS]，let，us，start，[面具]，[面具]，[面具]，the，model] </em> </strong></p><blockquote class="jh ji jj"><p id="4d28" class="jk jl jm jn b jo kk jq jr js kl ju jv jw lb jy jz ka lc kc kd ke ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="hi">下一句预测(NSP) </em> </strong></p></blockquote><p id="212d" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">NSP是一个二元分类任务，我们给伯特两个句子，他必须预测第二个句子是否是第一个句子的后续。通过执行NSP任务，我们的模型可以理解这两个句子之间的关系。理解这两个句子之间的关系在诸如问答和文本生成的下游任务的情况下是有用的。</p><p id="fc31" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">为了执行分类，我们简单地采用<strong class="jn hj"><em class="jm">【CLS】</em></strong>记号<strong class="jn hj"> <em class="jm"> </em> </strong>的表示，并用SoftMax函数将它馈送给前馈网络，该网络然后返回句子对是isNext或notNext的概率。<strong class="jn hj"><em class="jm">【CLS】</em></strong>的嵌入基本上持有所有令牌的聚合表示。</p><blockquote class="iv"><p id="8940" class="iw ix hi bd iy iz lp lq lr ls lt jf dx translated">训练前程序</p></blockquote><p id="5046" class="pw-post-body-paragraph jk jl hi jn b jo jp jq jr js jt ju jv km jx jy jz ko kb kc kd kq kf kg kh jf hb bi translated">使用多伦多图书语料库和维基百科数据集对Bert进行预训练。我们知道，伯特是使用掩蔽语言建模和NSP任务进行预训练的。现在，我们如何使用这两项任务为经过训练的Bert准备数据集。</p><p id="4633" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">让我们考虑两个句子</p><p id="db6a" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">我们喜欢这场比赛。 </p><p id="2adc" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">句子B:调了收音机</em> </strong></p><p id="55e8" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"> <em class="jm">注意——两个句子的记号总数之和必须小于等于512。在对两个文本进行抽样时，50%的时间我们将句子B视为句子A的后续，而50%的时间则视为不是后续。</em>T47】</strong></p><p id="6d5f" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">在对句子进行标记化并在每句话尾添加[CLS]和[SEP]标记后，我们得到</p><p id="5c55" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"><em class="jm">|令牌-【【CLS】，我们，乐此不疲，游戏，[九月]，打开收音机】</em> </strong></p><p id="940b" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">接下来，根据80–10–10%规则随机屏蔽15%的令牌。</p><p id="5c30" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated"><strong class="jn hj"><em class="jm">|令牌-【【CLS】，我们，乐此不疲，【面具】，【九月】，调收音机】</em> </strong></p><p id="fe0a" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">现在，我们将通过蒙版建模和NSP任务来训练伯特。</p><blockquote class="iv"><p id="2052" class="iw ix hi bd iy iz lp lq lr ls lt jf dx translated">子词标记化算法</p></blockquote><blockquote class="jh ji jj"><p id="cefc" class="jk jl jm jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh jf hb bi translated"><strong class="jn hj">字节对编码(BPE) </strong></p></blockquote><p id="0a30" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">它包括以下步骤</p><ol class=""><li id="0868" class="ki kj hi jn b jo kk js kl km kn ko kp kq kr jf lx kt ku kv bi translated">从给定的数据集中提取单词及其计数</li><li id="37cb" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf lx kt ku kv bi translated">定义词汇量。</li><li id="e56f" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf lx kt ku kv bi translated">将单词拆分成字符序列。</li><li id="6b29" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf lx kt ku kv bi translated">将我们字符序列中的所有独特字符添加到词汇表中。</li><li id="4499" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf lx kt ku kv bi translated">选择并合并具有高频率的符号对。</li><li id="cca7" class="ki kj hi jn b jo kw js kx km ky ko kz kq la jf lx kt ku kv bi translated">重复步骤5，直到达到词汇量。</li></ol><blockquote class="jh ji jj"><p id="686b" class="jk jl jm jn b jo kk jq jr js kl ju jv jw lb jy jz ka lc kc kd ke ld kg kh jf hb bi translated"><strong class="jn hj">字节级字节对编码(BBPE) </strong></p></blockquote><p id="a871" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">它类似于BPE，但不是将单词拆分成字符序列，而是将其拆分成字节码序列。它在处理OOV单词方面非常有效，在跨多种语言共享词汇方面也非常棒。</p><blockquote class="jh ji jj"><p id="a078" class="jk jl jm jn b jo kk jq jr js kl ju jv jw lb jy jz ka lc kc kd ke ld kg kh jf hb bi translated"><strong class="jn hj">文字片</strong></p></blockquote><p id="38ce" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">像BPE一样，我们在WordPiece中做了同样的事情，但是有一点不同，这里我们不基于频率合并符号对。相反，我们基于可能性合并符号对。</p><p id="80cd" class="pw-post-body-paragraph jk jl hi jn b jo kk jq jr js kl ju jv km lb jy jz ko lc kc kd kq ld kg kh jf hb bi translated">谢谢你..</p></div></div>    
</body>
</html>