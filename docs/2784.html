<html>
<head>
<title>Fine Grained Image Classification using Bilinear Convolutional Neural Networks - Tensorflow V2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用双线性卷积神经网络-张量流V2的细粒度图像分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fine-grained-image-classification-using-bilinear-convolutional-neural-networks-tensorflow-v2-4baf875d45e1?source=collection_archive---------7-----------------------#2021-05-17">https://medium.com/analytics-vidhya/fine-grained-image-classification-using-bilinear-convolutional-neural-networks-tensorflow-v2-4baf875d45e1?source=collection_archive---------7-----------------------#2021-05-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="c81e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章是发表在<a class="ae jc" href="https://arxiv.org/abs/1504.07889" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1504.07889</a>的双线性CNN的TF实现。请务必阅读这篇论文，以便更好地理解。</p><p id="83a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">先决条件:- Python，CNN的，Keras，TF</p><p id="728c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">代码托管在</strong><a class="ae jc" href="https://github.com/tommarvoloriddle/Bilinear-CNN-Tensorflow2.4-implementation" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">https://github . com/tommarvoloriddle/双线性-CNN-tensor flow 2.4-实现</strong> </a></p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="7733" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">想法</h1><p id="f20d" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">诸如VGG、RNN的CNN已经被发现对于细粒度图像识别执行得不太好，如果被训练的图像数据集不包含与Imagenet相似的图像，则性能会更差。因此，这个想法是创造一个新的架构，而不失去以前的线性SOTA的功能</p><h1 id="6a02" class="jk jl hh bd jm jn kn jp jq jr ko jt ju jv kp jx jy jz kq kb kc kd kr kf kg kh bi translated">直觉</h1><p id="696d" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">双线性CNN背后的直觉可以理解为简单的并行CNN，每个CNN试图识别同一图像的不同特征。粗略地说，在识别特定的鸟类物种时，可以使用两个平行的CNN，一个识别喙，另一个识别尾巴。这个例子只是提供一种直觉，实际上多个CNN识别不同的特征，但是这些特征可以是非常小的边缘，不像尾巴和喙那样明显。对输入进行热图分析将有助于更好地理解B-CNN。</p><h1 id="0438" class="jk jl hh bd jm jn kn jp jq jr ko jt ju jv kp jx jy jz kq kb kc kd kr kf kg kh bi translated">使用</h1><p id="ce57" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">B-CNN克服了线性CNN的一些问题，例如改进了精细分类，例如将鸟类分类为200种(<a class="ae jc" href="http://www.vision.caltech.edu/visipedia/CUB-200.html" rel="noopener ugc nofollow" target="_blank">http://www.vision.caltech.edu/visipedia/CUB-200.html</a>)，对零售店产品进行分类。</p><h1 id="fc65" class="jk jl hh bd jm jn kn jp jq jr ko jt ju jv kp jx jy jz kq kb kc kd kr kf kg kh bi translated">双线性卷积神经网络</h1><p id="087e" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">双线性CNN是使用矩阵外积(<a class="ae jc" href="https://en.wikipedia.org/wiki/Outer_product" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Outer_product</a>)组合的简单并行CNN。CNN的输出在FC层之前获取。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ks"><img src="../Images/0a94807579ece191655966ca11a46469.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gr4snhggh9jV1F3mnHeErg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">图1 : B-CNN架构。</figcaption></figure><p id="8697" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于整体架构是有向非循环图，因此可以通过反向传播分类损失的梯度(例如，交叉熵)来训练参数。双线性形式简化了梯度计算。如果两个网络的输出分别是大小为L × M和L × N的矩阵A和B，则双线性特征为x = A(T)B(A * B的转置),大小为M × N。设dl/dx为损失函数L相对于x的梯度，则根据梯度链规则，我们得到</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es li"><img src="../Images/9405c2d607fca2dd88b8ff054fa0cc33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*fYc_t9zzaezKa8mQBBmyOg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">图2:梯度的链式法则。</figcaption></figure><h2 id="3bfa" class="lj jl hh bd jm lk ll lm jq ln lo lp ju ip lq lr jy it ls lt kc ix lu lv kg lw bi translated">反向传播中的梯度流。</h2><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es lx"><img src="../Images/0e90b722ea679317c71323439145fcb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LhA58dy7uE_rhQqGlugaYg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">图3:梯度流。</figcaption></figure><h1 id="ce09" class="jk jl hh bd jm jn kn jp jq jr ko jt ju jv kp jx jy jz kq kb kc kd kr kf kg kh bi translated">履行</h1><p id="a061" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">我们必须定义我们的外积、l2归一化和平方根函数，以实现如图3所示的流程。</p><p id="0609" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">点积函数假设输入张量的大小相同，但也可以是不同的大小，如图3所示，点积需要一些变化。</p><pre class="kt ku kv kw fd ly lz ma mb aw mc bi"><span id="c1bf" class="lj jl hh lz b fi md me l mf mg"><em class="mh">"""</em><br/><em class="mh">Calculates dot product of x[0] and x[1] for mini_batch </em><br/><br/><em class="mh">Assuming both have same size and shape</em><br/><br/><em class="mh">@param</em><br/><em class="mh">x -&gt; [ (size_minibatch, total_pixels, size_filter), (size_minibatch, total_pixels, size_filter) ]</em><br/><br/><em class="mh">"""</em><br/><strong class="lz hi">def</strong> dot_product(x):<br/><br/>    <strong class="lz hi">return</strong> keras.backend.batch_dot(x[0], x[1], axes=[1,1]) / x[0].get_shape().as_list()[1] <br/><br/><em class="mh">"""</em><br/><em class="mh">Calculate signed square root</em><br/><br/><em class="mh">@param</em><br/><em class="mh">x -&gt; a tensor</em><br/><br/><em class="mh">"""</em><br/><br/><strong class="lz hi">def</strong> signed_sqrt(x):<br/><br/>    <strong class="lz hi">return</strong> keras.backend.sign(x) * keras.backend.sqrt(keras.backend.abs(x) + 1e-9)<br/><br/><em class="mh">"""</em><br/><em class="mh">Calculate L2-norm</em><br/><br/><em class="mh">@param</em><br/><em class="mh">x -&gt; a tensor</em><br/><br/><em class="mh">"""</em><br/><br/><strong class="lz hi">def</strong> L2_norm(x, axis=-1):<br/><br/>    <strong class="lz hi">return</strong> keras.backend.l2_normalize(x, axis=axis)</span></pre><h2 id="c5d1" class="lj jl hh bd jm lk ll lm jq ln lo lp ju ip lq lr jy it ls lt kc ix lu lv kg lw bi translated">建筑模型</h2><p id="3756" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">构建模型函数将返回给我们双线性模型。在这种情况下，我们从最后一层获取2个VGG16网络的输出。要自定义CNN，请检查repo中的代码(<a class="ae jc" href="https://github.com/tommarvoloriddle/Bilinear-CNN-Tensorflow2.4-implementation/blob/main/BILINEAR-Custom.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/tommarvoloriddle/Bilinear-CNN-tensor flow 2.4-implementation/blob/main/Bilinear-custom . ipynb</a>)。</p><p id="271c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">输出被重新整形以匹配张量形状，这在构建定制CNN时很有用。</p><p id="7a54" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">外积、L2范数、sqrt作为Lamda层被添加到模型中，也将不具有任何可训练的权重。</p><pre class="kt ku kv kw fd ly lz ma mb aw mc bi"><span id="6dfa" class="lj jl hh lz b fi md me l mf mg"><em class="mh">'''</em><br/><br/><em class="mh">    Take outputs of last layer of VGG and load it into Lambda layer which calculates outer product.</em><br/><em class="mh">    </em><br/><em class="mh">    Here both bi-linear branches have same shape.</em><br/><em class="mh">    </em><br/><em class="mh">    z -&gt; output shape tuple</em><br/><em class="mh">    x -&gt; outpur og VGG tensor</em><br/><em class="mh">    y -&gt; copy of x as we modify x, we use x, y for outer product.</em><br/><em class="mh">    </em><br/><em class="mh">'''</em><br/><br/><strong class="lz hi">def</strong> build_model():<br/>    tensor_input = keras.layers.Input(shape=[150,150,3])<br/><br/><em class="mh">#   load pre-trained model</em><br/>    tensor_input = keras.layers.Input(shape=[150,150,3])<br/>    <br/><br/>    <br/>    model_detector = keras.applications.vgg16.VGG16(<br/>                            input_tensor=tensor_input, <br/>                            include_top=<strong class="lz hi">False</strong>,<br/>                            weights='imagenet')<br/>    <br/>    model_detector2 = keras.applications.vgg16.VGG16(<br/>                            input_tensor=tensor_input, <br/>                            include_top=<strong class="lz hi">False</strong>,<br/>                            weights='imagenet')<br/>    <br/>    <br/>    model_detector2 = keras.models.Sequential(layers=model_detector2.layers)<br/>  <br/>    <strong class="lz hi">for</strong> i, layer <strong class="lz hi">in</strong> enumerate(model_detector2.layers):<br/>        layer._name = layer.name  +"_second"<br/><br/>    model2 = keras.models.Model(inputs=[tensor_input], outputs = [model_detector2.layers[-1].output])<br/>                       <br/>    x = model_detector.layers[17].output<br/>    z = model_detector.layers[17].output_shape<br/>    y = model2.layers[17].output<br/>    <br/>    print(model_detector.summary())<br/>    <br/>    print(model2.summary())<br/><em class="mh">#   rehape to (batch_size, total_pixels, filter_size)</em><br/>    x = keras.layers.Reshape([z[1] * z[2] , z[-1]])(x)<br/>        <br/>    y = keras.layers.Reshape([z[1] * z[2] , z[-1]])(y)<br/>    <br/><em class="mh">#   outer products of x, y</em><br/>    x = keras.layers.Lambda(dot_product)([x, y])<br/>    <br/><em class="mh">#   rehape to (batch_size, filter_size_vgg_last_layer*filter_vgg_last_layer)</em><br/>    x = keras.layers.Reshape([z[-1]*z[-1]])(x)<br/>        <br/><em class="mh">#   signed_sqrt</em><br/>    x = keras.layers.Lambda(signed_sqrt)(x)<br/>        <br/><em class="mh">#   L2_norm</em><br/>    x = keras.layers.Lambda(L2_norm)(x)<br/><br/><em class="mh">#   FC-Layer</em><br/><br/>    initializer = tf.keras.initializers.GlorotNormal()<br/>            <br/>    x = keras.layers.Dense(units=258, <br/>                           kernel_regularizer=keras.regularizers.l2(0.0),<br/>                           kernel_initializer=initializer)(x)<br/><br/>    tensor_prediction = keras.layers.Activation("softmax")(x)<br/><br/>    model_bilinear = keras.models.Model(inputs=[tensor_input],<br/>                                        outputs=[tensor_prediction])<br/>    <br/>    <br/><em class="mh">#   Freeze VGG layers</em><br/>    <strong class="lz hi">for</strong> layer <strong class="lz hi">in</strong> model_detector.layers:<br/>        layer.trainable = <strong class="lz hi">False</strong><br/>        <br/><br/>    sgd = keras.optimizers.SGD(lr=1.0, <br/>                               decay=0.0,<br/>                               momentum=0.9)<br/><br/>    model_bilinear.compile(loss="categorical_crossentropy", <br/>                           optimizer=sgd,<br/>                           metrics=["categorical_accuracy"])<br/><br/>    model_bilinear.summary()<br/>    <br/>    <strong class="lz hi">return</strong> model_bilinear</span></pre><h2 id="5d33" class="lj jl hh bd jm lk ll lm jq ln lo lp ju ip lq lr jy it ls lt kc ix lu lv kg lw bi translated">模型摘要</h2><p id="5922" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">自定义B-CNN的摘要，因为带有2个VGG16的会占用很多页面，会令人困惑。</p><p id="9cff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在此总结中，我们可以看到，101和104的输入是相同的，这是我们的起点。我们将103和106的输出送到lambda 18(外积)。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mi"><img src="../Images/08c9c79af344ebaae3d7eb94ef9f3e12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*691rvWpIoeDsc0-CBnx1Bg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">图4:定制B-CNN的模型总结</figcaption></figure><p id="f84c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其他通用方法如模型拟合和预测在本文中被跳过，因为它已经很长了，但是可以在<a class="ae jc" href="https://github.com/tommarvoloriddle/Bilinear-CNN-Tensorflow2.4-implementation" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">https://github . com/tommarvoloriddle/Bilinear-CNN-tensor flow 2.4-implementation</strong></a><strong class="ig hi">上找到。</strong></p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><p id="d05e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[1]聪-林玉等人的用于细粒度视觉识别的双线性CNN(<a class="ae jc" href="https://arxiv.org/pdf/1504.07889.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1504.07889.pdf</a>)</p><p id="5895" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2] <a class="ae jc" href="https://github.com/ryanfwy" rel="noopener ugc nofollow" target="_blank">瑞恩</a>-(<a class="ae jc" href="https://github.com/ryanfwy/BCNN-keras-clean" rel="noopener ugc nofollow" target="_blank">https://github.com/ryanfwy/BCNN-keras-clean</a>)</p></div></div>    
</body>
</html>