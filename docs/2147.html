<html>
<head>
<title>ACTIVATION FUNCTIONS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">激活功能</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-67cd5f5d599c?source=collection_archive---------12-----------------------#2021-04-07">https://medium.com/analytics-vidhya/activation-functions-67cd5f5d599c?source=collection_archive---------12-----------------------#2021-04-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/2adcdc8fdf6013439a7a62b9b1000727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OArNSRf6sS5y0Fr6r8VqXg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">非线性的艺术</figcaption></figure><p id="d4b7" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">本文是对以下部分的简要讨论</p><ul class=""><li id="f4ee" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated"><strong class="iv hi">激活功能</strong></li><li id="9c29" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">各种激活功能</strong></li><li id="f4fa" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">需要激活功能</strong></li></ul><h1 id="354a" class="kf kg hh bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">激活功能</h1><p id="aadb" class="pw-post-body-paragraph it iu hh iv b iw ld iy iz ja le jc jd je lf jg jh ji lg jk jl jm lh jo jp jq ha bi translated">激活函数是决定神经网络输出的方程。激活函数的主要目的是向神经网络引入非线性。</p><ul class=""><li id="adc5" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated">激活函数将神经元线性输入转换成非线性输出。</li><li id="5eaa" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">它有助于使每个神经元的输出在-1到1的范围内正常化</li></ul><h1 id="9db2" class="kf kg hh bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">不同的激活功能</h1><ul class=""><li id="d81c" class="jr js hh iv b iw ld ja le je li ji lj jm lk jq jw jx jy jz bi translated">Sigmoid函数</li><li id="7f46" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">Softmax函数</li><li id="40a8" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">双曲正切函数</li><li id="c90c" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">校正线性单位函数</li><li id="2765" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">指数线性单位函数(ELU)</li><li id="421c" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">漏整流线性单位函数</li></ul><h2 id="17c0" class="ll kg hh bd kh lm ln lo kl lp lq lr kp je ls lt kt ji lu lv kx jm lw lx lb ly bi translated">Sigmoid函数</h2><ul class=""><li id="e034" class="jr js hh iv b iw ld ja le je li ji lj jm lk jq jw jx jy jz bi translated">Sigmoid是一个可微函数</li><li id="6952" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">它有固定的输出范围。[介于0和1之间]</li><li id="dba3" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">它通常用于二元分类</li></ul><figure class="ma mb mc md fd ii er es paragraph-image"><div class="er es lz"><img src="../Images/6e14c4384057f6809be2de2e244ce4d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*ph34n0tXy7lYl2YQxhUthw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">乙状结肠的</figcaption></figure><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="3ef1" class="ll kg hh mf b fi mj mk l ml mm">def sigmoid(x):<br/>    return 1/(1+np.exp(-x))</span></pre><h2 id="24aa" class="ll kg hh bd kh lm ln lo kl lp lq lr kp je ls lt kt ji lu lv kx jm lw lx lb ly bi translated">Softmax函数</h2><ul class=""><li id="1fae" class="jr js hh iv b iw ld ja le je li ji lj jm lk jq jw jx jy jz bi translated">Softmax函数是根据神经元的输入值计算概率分布的激活函数</li><li id="4ae2" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">它有固定的输出范围。[介于0和1之间]</li><li id="196d" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">它用在多类模型中，返回每个类的概率，目标类的概率最高。</li><li id="e7fd" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">它生成范围在0和1之间的输出，并且所有类别的概率之和等于1。</li></ul><figure class="ma mb mc md fd ii er es paragraph-image"><div class="er es mn"><img src="../Images/28f9aa15840ef592ef0e8597feca6b3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*Ntskn_qAdLY1pUDXNncc4g.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">softmax</figcaption></figure><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="19f0" class="ll kg hh mf b fi mj mk l ml mm">def softmax(x):<br/>    return np.exp(x)/(np.sum(np.exp(x)))</span></pre><h2 id="5601" class="ll kg hh bd kh lm ln lo kl lp lq lr kp je ls lt kt ji lu lv kx jm lw lx lb ly bi translated">双曲正切函数</h2><ul class=""><li id="6086" class="jr js hh iv b iw ld ja le je li ji lj jm lk jq jw jx jy jz bi translated">Tanh是一个以零为中心的函数</li><li id="c4bd" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">它有一个固定的输出范围，[在-1和1之间]</li><li id="4dcb" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">它用于多类神经网络模型</li></ul><figure class="ma mb mc md fd ii er es paragraph-image"><div class="er es mo"><img src="../Images/776714ca89795bcacbbdbdf72563a12a.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*yUtX8REEAUzCQOglQVQHeA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">双曲正切</figcaption></figure><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="8c21" class="ll kg hh mf b fi mj mk l ml mm">def tanh(x):<br/>    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))</span></pre><h2 id="cf28" class="ll kg hh bd kh lm ln lo kl lp lq lr kp je ls lt kt ji lu lv kx jm lw lx lb ly bi translated">校正线性单位函数</h2><ul class=""><li id="df1e" class="jr js hh iv b iw ld ja le je li ji lj jm lk jq jw jx jy jz bi translated">ReLU确认小于零的输入值，并将其设置为零</li><li id="c637" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">它没有固定的范围，[从0到inf]</li><li id="e8b2" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">计算成本更低且速度更快</li><li id="2c8c" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">它只能在神经网络模型的隐藏层中使用</li></ul><figure class="ma mb mc md fd ii er es paragraph-image"><div class="er es mp"><img src="../Images/79dbc47a0e081562280c4b50990923bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*SvLKVn7fISAJB17tXy08RQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">热卢</figcaption></figure><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="70b7" class="ll kg hh mf b fi mj mk l ml mm">def ReLU(x):<br/>    return np.array([i if i &gt; 0 else 0 for i in x])</span></pre><h2 id="94f2" class="ll kg hh bd kh lm ln lo kl lp lq lr kp je ls lt kt ji lu lv kx jm lw lx lb ly bi translated">指数线性单位函数(ELU)</h2><ul class=""><li id="2fa5" class="jr js hh iv b iw ld ja le je li ji lj jm lk jq jw jx jy jz bi translated">ELU是ReLU的替代品</li><li id="5cc9" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">当输入值小于0时，ELU使用参数化的[α]指数函数</li></ul><figure class="ma mb mc md fd ii er es paragraph-image"><div class="er es mq"><img src="../Images/85c76cb9b300da11dd8411428785b87d.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*sLigTLpeXQgvEQjlTwwg-Q.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">ELU</figcaption></figure><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="11a6" class="ll kg hh mf b fi mj mk l ml mm">def ELU(x, alpha=0.01):<br/>    return np.array(<br/>        [i if i &gt; 0 else alpha*(np.exp(i) - 1) for i in x]<br/>    )</span></pre><h2 id="833d" class="ll kg hh bd kh lm ln lo kl lp lq lr kp je ls lt kt ji lu lv kx jm lw lx lb ly bi translated">漏整流线性单位函数</h2><ul class=""><li id="f8d8" class="jr js hh iv b iw ld ja le je li ji lj jm lk jq jw jx jy jz bi translated">LeakyReLu是ReLu的另一个变体</li><li id="430f" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">对于小于0的输入值，它允许非常小的非零恒定梯度α (α=0.01)</li></ul><figure class="ma mb mc md fd ii er es paragraph-image"><div class="er es mr"><img src="../Images/1402b5a2293e8a8c4c43a5744308bc6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*vJplXHERBZiuYYAYV96eAg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">泄漏ReLU</figcaption></figure><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="eb2a" class="ll kg hh mf b fi mj mk l ml mm">def LeakyReLU(x, alpha=0.01):<br/>    return np.array(<br/>        [i if i &gt; 0 else alpha*i for i in x]<br/>    )</span></pre><h1 id="4af3" class="kf kg hh bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">需要激活功能</strong></h1><ul class=""><li id="9427" class="jr js hh iv b iw ld ja le je li ji lj jm lk jq jw jx jy jz bi translated">如果没有应用激活函数，神经网络的输出将是一个<strong class="iv hi">线性函数</strong></li><li id="08f0" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">激活功能将在模型中引入一些非线性<strong class="iv hi"/>。这种增加的非线性使得神经网络不同于简单的线性函数或简单的线性回归模型。</li><li id="e49b" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">激活函数赋予神经网络理解不同特征之间复杂关系的能力。</li><li id="20fc" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">在激活函数的帮助下，不同特征之间的<strong class="iv hi">非线性关系可以被神经网络模型理解</strong></li></ul></div></div>    
</body>
</html>