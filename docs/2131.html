<html>
<head>
<title>Optimization of Support Vector Machine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机的优化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/optimization-of-support-vector-machine-709090570dbb?source=collection_archive---------6-----------------------#2021-04-06">https://medium.com/analytics-vidhya/optimization-of-support-vector-machine-709090570dbb?source=collection_archive---------6-----------------------#2021-04-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="2f6b" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">用二次规划优化线性可分分类器的数学解释。</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es iw"><img src="../Images/4c0392686d21e682e4f768f1e102bb46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*QYfHwDbRagXa2G_TX7DWxw.jpeg"/></div><figcaption class="je jf et er es jg jh bd b be z dx translated">来源:<a class="ae ji" href="https://cdn.jvejournals.com/articles/18120/xml/img2.jpg" rel="noopener ugc nofollow" target="_blank">图片</a></figcaption></figure><p id="378b" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">对于线性可分数据，支持向量机(SVM)的目标函数定义为:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es kf"><img src="../Images/ebd035a0bf31386b03b1c2f0508f7884.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/0*re94VQVjlP1qqzHo"/></div><figcaption class="je jf et er es jg jh bd b be z dx translated">SVM的目标函数</figcaption></figure><p id="af04" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">如果你想知道更多关于我们是如何制定它的，请阅读<a class="ae ji" href="https://ajinkya14jadhav.medium.com/support-vector-machine-intuition-validate-with-maths-e10fb8cc7f57" rel="noopener"> <strong class="jl hi">第一部分</strong> </a>。</p><p id="f6fe" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">这是一个带有线性约束的<strong class="jl hi">二次</strong>和<strong class="jl hi">凸</strong>目标函数。因此，为了优化这个目标函数，我们需要引入拉格朗日乘数α；不等式约束中各一个。这是一个叫做<strong class="jl hi">二次规划问题的非线性规划问题。</strong></p><p id="534b" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated"><a class="ae ji" href="https://en.wikipedia.org/wiki/Lagrange_multiplier" rel="noopener ugc nofollow" target="_blank">朗格乘子法</a>处理寻找服从<strong class="jl hi">等式约束</strong>的函数的局部最小值或局部最大值。但是在目标函数中，我们有一个不等式约束，所以我们需要使用一个<em class="kg">广义版本的拉格朗日乘数法，同时考虑到</em> <strong class="jl hi"> <em class="kg">不等式约束</em> </strong>。所以，<strong class="jl hi">卡鲁什-库恩-塔克(KKT)条件</strong>考虑不等式约束的广义拉格朗日乘子法为；</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es kh"><img src="../Images/878e083ac3549a126897b79be6d6607a.png" data-original-src="https://miro.medium.com/v2/resize:fit:132/0*eUGMYsdW0JsBSuk0"/></div></figure><p id="f0a2" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">现在目标函数的拉格朗日(L)变成了；</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ki"><img src="../Images/6f8ba3ea83c2c846c69030ca97436ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/0*Nd2WLSt__vD_K9BL"/></div></figure><p id="c02d" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">其中α是拉格朗日乘数。因为线性约束大于或等于类型；我们使它小于或等于乘以-1，以满足KKT条件的拉格朗日乘子法。关于(w.r.t.) <strong class="jl hi"> w </strong>和<strong class="jl hi"/>b<strong class="jl hi"/>l必须<strong class="jl hi">最小化</strong>而对于<strong class="jl hi">α</strong>s<strong class="jl hi">最大化</strong>，这意味着我们需要找到一个鞍点。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es kj"><img src="../Images/5ccb32f022477708185d79694fc777e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*xVnhPVwx0VJ7_q4StIbMXg.jpeg"/></div><figcaption class="je jf et er es jg jh bd b be z dx translated"><a class="ae ji" href="https://i.stack.imgur.com/VQhmi.png" rel="noopener ugc nofollow" target="_blank">形象信用</a></figcaption></figure><p id="4400" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">在上图中，你可以看到X是一个鞍点，沿着曲线AB我们最小化函数，沿着曲线CD最大化它的对偶。</p><p id="14e0" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated"><strong class="jl hi">对偶定理</strong>陈述了在<em class="kg">中，一个凸优化问题和一组线性约束，如果原问题有最优解，那么对偶问题也有最优解。对应的值相等，我们称之为鞍点。</em>这里的主要问题是最小化关于<strong class="jl hi"> W </strong>和b的L，而对偶问题是最大化关于<strong class="jl hi"> α </strong>的L</p><p id="5dc8" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">如果违反了线性约束，那么可以通过增加相应的<strong class="jl hi"> α </strong>来增加L，但是然后<strong class="jl hi"> W </strong>和b必须改变，使得L减小。这是当数据点位于裕量内并且必须改变<strong class="jl hi"> W </strong>和b以调整裕量时的情况。对于所有不满足的约束，等式意味着大于0，则相应的<strong class="jl hi"> α </strong>必须为0，以使l最大化。</p><p id="81fb" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">因为我们需要最小化L w.r.t. <strong class="jl hi"> W </strong>和b，所以我们对其求导并使其等于零。如；</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es kk"><img src="../Images/a8ed9ea8d55d1e98cf1bbe72ce040597.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/0*mn6YLhi0xnEnb_rN"/></div></figure><p id="340d" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">对L w r t<strong class="jl hi">w</strong>和b求导后我们得到:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es kl"><img src="../Images/488251016f04ddd01eca17cf54c0d777.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/0*CyrOH96PtycSNL50"/></div></figure><p id="3d9a" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">重新排列L的项，替换上面得到的值，然后</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es km"><img src="../Images/6e4c73fe0fc67f64292fb022c51ae23a.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/0*stIrZmv6zt0g9MBL"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es kn"><img src="../Images/d12172a44b14378cd25c309e460d222e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/0*iLW2aGHGxycovUu3"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ko"><img src="../Images/366ce0e422a84703d44741a0b71fe7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/0*myALCszCuYhlPG13"/></div></figure><p id="14f2" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">现在，这是一个对偶问题，我们需要最大化w.r.t. α，约束条件为:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es kp"><img src="../Images/2ef71f82e3672d183f58b541406c96f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/0*3h7hERVcHSX_Ib93"/></div></figure><p id="2f31" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">这是使用二次优化的数值方法解决的，该方法通过最大化对偶问题l给出每个训练点的最佳α值。最佳α值大于0的训练点称为<strong class="jl hi">支持向量。</strong>对于其他训练点，最优α的值为零，位于边缘超平面的那一侧，使得；</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es kq"><img src="../Images/77dffebf8a134df777bbb01cb3e765b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/0*FyNd0pA81ABSBPt7"/></div></figure><p id="ca07" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">这个不等式严格成立。</p><p id="d03f" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">正如你可以注意到的，在上面的L的对偶公式中，两个维数为d的数据点的内积被采用。正如我们所知，内积提供了某种程度的相似性，或者说它表明了它们之间的距离。如果它们<strong class="jl hi">相似</strong>则<strong class="jl hi">内积为1 </strong>或者如果它们<strong class="jl hi">不相似</strong>则为<strong class="jl hi"> 0 </strong>。</p></div><div class="ab cl kr ks go kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ha hb hc hd he"><h2 id="f811" class="ky kz hh bd la lb lc ld le lf lg lh li js lj lk ll jw lm ln lo ka lp lq lr ls bi translated">关于L的内部产品的见解:</h2><p id="9c43" class="pw-post-body-paragraph jj jk hh jl b jm lt ii jo jp lu il jr js lv ju jv jw lw jy jz ka lx kc kd ke ha bi translated">所以，让我们在内积的背景下理解L的对偶。正如我们所说，如果α不为零，那么函数将最大化。这些是帮助最大化利润的支持向量。</p><p id="b62c" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">请注意约束中所有的α都是正的。让我们来探讨一些随之而来的案例:</p><p id="0811" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">情况1:如果两个特征相似，那么它们的内积是1。然后有两个子类，例如:</p><p id="81dd" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">a)如果两个特征向量对y的输出值做出相反的预测，即一个是+1或者另一个是-1，那么；</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ly"><img src="../Images/66754cfda5a7c33d959d970f9613de07.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/0*MpAszZ9Nodrm6rau"/></div></figure><p id="e6bc" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">在函数的第二项变成负的，我们要减去它，所以这加到和上，使它最大化。这些是我们正在寻找的例子；区分这两个阶层的关键因素。</p><p id="45c8" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">b)如果两个特征向量对y的输出值做出相似的预测，即两者都是+1或-1，那么；</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ly"><img src="../Images/32f9b7188ef9992685ace9960939215f.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/0*4_7sMNGsi11Vt2Q5"/></div></figure><p id="6ebb" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">在函数的第二项变为正值，并且我们正在减去它，这降低了l的值。因此，该算法降低了做出相同预测的相似特征向量的等级。</p><p id="7e14" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">情况2:如果两个特征向量不相似，那么它们的内积是0。所以，这个术语；</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ly"><img src="../Images/317f23c19ed6d9f36b5d360e7446a4ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/0*ivOWeFeXkYBdtvxE"/></div></figure><p id="a0fa" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">变为零。所以他们对l没有贡献。</p></div><div class="ab cl kr ks go kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ha hb hc hd he"><p id="19ac" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">使用α的最佳值，我们计算最佳权重，使得</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lz"><img src="../Images/c904a2556bd5e18e192c5e8e9e938835.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/0*mcSmCyeA-86V1Z6u"/></div></figure><p id="1f1b" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">现在最佳分离超平面由下式给出</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ma"><img src="../Images/9eb33cdcc42d8c946a253b12b2f8434f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/0*rzvKuUSdclXrerHj"/></div><figcaption class="je jf et er es jg jh bd b be z dx translated">超平面的最优方程</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mb"><img src="../Images/0b51da23349122178d60a99aa01b2c22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*q-iVLZxL7qF2BQ71pCeu4w.png"/></div><figcaption class="je jf et er es jg jh bd b be z dx translated">来源:<a class="ae ji" href="https://d3i71xaburhd42.cloudfront.net/6c41c29257597af6b7da10fbb335cd2c2f9bde75/7-Figure2-1.png" rel="noopener ugc nofollow" target="_blank">霍夫曼的研究论文</a></figcaption></figure></div><div class="ab cl kr ks go kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ha hb hc hd he"><p id="f6d2" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated"><strong class="jl hi">总结:</strong>简言之，<strong class="jl hi">点(内)积</strong>在优化中决定哪些数据点有助于得到最优超平面。这些数据点被称为<strong class="jl hi">支持向量</strong>，用于确定超平面的方向。</p><p id="6467" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated"><strong class="jl hi">参考文献:</strong></p><p id="53fe" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">[1]:<a class="ae ji" href="https://www.youtube.com/watch?v=YOsrYl1JRrc&amp;ab_channel=MachineLearning-SudeshnaSarkar" rel="noopener ugc nofollow" target="_blank">sude shna Sarkar的演讲</a></p><p id="5434" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">[2]:支持向量机—内核和内核技巧—马丁·霍夫曼</p><p id="fefa" class="pw-post-body-paragraph jj jk hh jl b jm jn ii jo jp jq il jr js jt ju jv jw jx jy jz ka kb kc kd ke ha bi translated">我希望这篇博客有助于理解SVM使用二次规划的最优化。如果您有任何想法、反馈或建议，请随时在下面发表评论。谢谢大家！</p></div></div>    
</body>
</html>