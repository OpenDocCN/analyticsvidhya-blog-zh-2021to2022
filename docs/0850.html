<html>
<head>
<title>NLP Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP变压器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-seq2seq-sequence-to-sequence-models-improved-into-transformers-using-attention-mechanism-9905fb88d5ef?source=collection_archive---------3-----------------------#2021-02-03">https://medium.com/analytics-vidhya/how-seq2seq-sequence-to-sequence-models-improved-into-transformers-using-attention-mechanism-9905fb88d5ef?source=collection_archive---------3-----------------------#2021-02-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7548b5224d1f61265c5b3c1a0f82f89d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UA5p4Ed1S6AXEWQPw7-nOA.png"/></div></div></figure><p id="6d3a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">自然语言处理(NLP)是人工智能的一个分支，帮助机器理解人类的自然语言。基本上，人类主要通过语音或文本进行交流。那么，NLP是如何工作的呢？嗯，我们可以在机器学习中处理的一种数据类型是序列数据，即一系列数据(例如，文本、语音)。NLP可以构建这样的系统，它将一个数据序列作为输入，对其进行处理，并产生另一个数据序列。这个字母被称为序列模型或Seq2seq(序列(输入)到序列(输出))。</p><p id="b509" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本帖中，我们将探讨什么是顺序模型以及从seq2seq到transformer的旅程。我将假设输入是文本/句子。请注意，我没有深入探讨我们正在探索的模型的架构细节。然而，我会解释高层次的想法，以及变压器是如何介绍的。</p><h2 id="22f7" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">Seq2Seq</h2><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kj"><img src="../Images/ed28b85d60f061d28155a5209a456781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zVI8zviAJNYSbK2eYWpdwQ.png"/></div></div></figure><p id="764a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">seq2seq是一个基于编码器-解码器的模型，它将一个句子(单词序列)作为输入，并将另一个句子作为输出。<strong class="is hj">编码器</strong>是一个可以堆叠在一起的多RNN(递归神经网络)单元。RNN按顺序读取输入，对句子进行逐字编码，并产生输出。这个输出就是最终的<strong class="is hj">隐藏状态</strong>(也称为上下文向量)，是输入的一个封装信息。这个向量将是解码器的输入。<strong class="is hj">解码器</strong>也是一堆RNN单元，但旨在解码隐藏状态并将其转换为预测输出(单词)。</p><p id="54eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你需要更深入地阅读seq2seq，这里有“<a class="ae ko" href="https://arxiv.org/pdf/1409.3215.pdf" rel="noopener ugc nofollow" target="_blank">序列学习</a>”[1]，这是Google发表的一篇研究论文，供进一步阅读。</p><p id="a4c3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这种类型的模型用于机器翻译、语音和实体识别、情感分类等任务。然而，隐藏状态向量是固定大小的长度，因此编码器很难封装/压缩句子中的所有信息。如果输入很长，就会遭遇所谓的信息瓶颈。<strong class="is hj">注意机制</strong>是针对这一限制提出的解决方案。</p><h2 id="4aae" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">注意机制</h2><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/2856c39df488127163ffe45e33a35259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*CeYx-0Npi_ecwqRYxO4EGw.png"/></div></figure><p id="775d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在英语中，注意力意味着将注意力集中在某件事情上的状态，在深度学习中也是如此。Dzmitry Bahdanau等人在他们的<a class="ae ko" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> [2]中介绍了注意机制。它被添加到编码器-解码器模型中，以帮助该模型，特别是解码器关注句子/输入序列的相关单词/部分。特别是，它使编码器能够封装来自输入的所有信息，并传递所有产生的隐藏状态，而不是像seq2seq中那样只传递最后一个状态。这项技术证明并显示了更好的结果。</p><p id="990d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们想让模型更快呢？引入transformer是为了加快模型的运行时间。</p><h2 id="49a3" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">变压器</h2><p id="36bf" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated">变形金刚是由谷歌在“<a class="ae ko" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的一切</a>”[3]的论文中提出的。Transformer是一个深度学习模型，也是一个基于编码器-解码器的架构，它使用注意机制，但不依赖于RNNs来加速模型。它由一个编码器和相同数量的解码器组成，编码器可以被认为是一堆编码器/编码层(本文中有六层)。</p><p id="bd77" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每个编码器/编码层的组件是两个子层，包括关注层和前馈神经网络(FFNN ),而解码器块由三个子层组成，包括两个关注层和一个FFNN层。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es kv"><img src="../Images/e268dcd8913cf17a564d198804e5276a.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*InsTuWpZTYm0kwi8ovIMAQ.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">变压器—模型架构[3]。</figcaption></figure><p id="f232" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">作为进一步的阅读，一篇“<a class="ae ko" href="https://arxiv.org/pdf/1904.02874.pdf" rel="noopener ugc nofollow" target="_blank">注意力模型的注意力调查</a>”[4]论文通过提供一个注意力分类法，提供了一个注意力建模发展的结构化和全面的概述。</p><p id="2b58" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，您可以在这里找到“<a class="ae ko" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">带注释的Transformer </a>”，这是“注意力是您所需要的全部”论文的“注释”版本，由哈佛NLP团队以逐行实现的形式呈现。</p><h2 id="c313" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">结论</h2><p id="9afa" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated">在这篇文章中，我们探讨了序列模型是如何在NLP中得到改进的，以及变压器是如何被引入的。</p><p id="2382" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">快乐学习！！</p><h2 id="cd2a" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">参考</h2><p id="74bd" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated">[1] I. Sutskever，<a class="ae ko" href="https://arxiv.org/pdf/1409.3215.pdf" rel="noopener ugc nofollow" target="_blank">用神经网络进行序列对序列学习</a> (2014)，Google</p><p id="455c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[2] D. Bahdanau，<a class="ae ko" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">通过联合学习对齐和翻译的神经机器翻译</a> (2015)，ICLR</p><p id="0750" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[3] A .瓦斯瓦尼，<a class="ae ko" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">关注是你所需要的一切</a> (2017)，谷歌</p><p id="63d0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[4] S. Chaudhari，<a class="ae ko" href="https://arxiv.org/pdf/1904.02874.pdf" rel="noopener ugc nofollow" target="_blank">注意力模型的仔细调查</a> (2020)，《智能系统与技术汇刊》(TIST)</p></div></div>    
</body>
</html>