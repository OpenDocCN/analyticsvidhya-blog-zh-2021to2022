<html>
<head>
<title>WHAT ATTENTION? WHY ATTENTION</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么关注？为什么关注</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-attention-why-attention-ad38589ef67d?source=collection_archive---------8-----------------------#2021-05-08">https://medium.com/analytics-vidhya/what-attention-why-attention-ad38589ef67d?source=collection_archive---------8-----------------------#2021-05-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="1dfc" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="79c0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">想象一下，你在一家超市想买一些谷类食品，你会怎么做？你会一次看完店里所有的商品吗？不，你没有，你找到了谷物货架，只看着它们，而忽略了商店里的其他商品换句话说，你关注了谷物货架注意力网络在NLP的深度学习的帮助下做了类似的事情</p><p id="99d1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这个概念是在关于神经机器翻译的<a class="ae kf" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中介绍的，在详细介绍它之前，让我们先简短地看一下在此之前使用了什么</p><h1 id="a058" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">编码器-解码器架构</h1><p id="823a" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">NMT最初基于seq2seq编码器-解码器架构(本文介绍)，该架构有3个重要部分:编码器、上下文向量和解码器。编码器是一个RNN(或LSTM/RNN ),接收输入序列并将其转换为上下文向量。上下文向量传递给解码器，解码器对上下文向量进行解码以给出输出序列</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/c02baa048fc05db5be82c183e4f50b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RwupeVik3pUSwNiQSAGRhw.png"/></div></div></figure><h2 id="1d20" class="ks if hh bd ig kt ku kv ik kw kx ky io jn kz la is jr lb lc iw jv ld le ja lf bi translated">编码器</h2><p id="f688" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">几个RNN(或LSTM/GRU，以获得更好的性能)的堆栈从输入和前一个隐藏状态中接受单个元素，从输入中收集信息，并将隐藏状态传递给下一个LSTM。隐藏状态的计算如下</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lg"><img src="../Images/af0e02217e87376e8f06870c9fabc67c.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/0*mbw577y51AiQYFiG"/></div></figure><p id="641e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">W_hh是与先前输入状态相关联的权重<br/> W_hx是与当前输入序列相关联的权重</p><h2 id="d627" class="ks if hh bd ig kt ku kv ik kw kx ky io jn kz la is jr lb lc iw jv ld le ja lf bi translated">解码器</h2><p id="6267" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">几个RNN(或LSTM/GRU)单元的堆栈，它接受编码器的最后一个隐藏状态作为上下文向量，最后一个编码器单元的单元状态作为初始值，并预测输出序列</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lh"><img src="../Images/53d64cd47ca8187bc5d32c559e06a09b.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/0*2cIzcFtG-BE8_JjI"/></div></figure><h1 id="4ffa" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">seq2seq模型的缺点</h1><ol class=""><li id="ad51" class="li lj hh je b jf jg jj jk jn lk jr ll jv lm jz ln lo lp lq bi translated">编码器-解码器网络需要将来自源句子的所有信息压缩到单个模糊长度向量中。这可能在长句和比训练语料库中的句子更大的句子中产生问题</li><li id="48e5" class="li lj hh je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated">它没有考虑单词的个体贡献量，另一方面，注意力理解在个体时间步骤中关注哪些单词</li></ol><h1 id="c032" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">注意机制</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lw"><img src="../Images/d266112cc467b43e7f6de72dbd5e8cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*A42KBeJMxOlk16rstvIcYQ.png"/></div></div></figure><p id="8527" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">(巴哈马收件人)</p><p id="73e7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为了解决上述问题，在<a class="ae kf" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">论文</a>中引入了注意力机制，它保持相同的RNN编码器，但是对于每个时间步长，它计算每个标记的隐藏表示的注意力分数。设输入为x，输出为y</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lx"><img src="../Images/27901618e1dbd0ad183de2eb61d17af5.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/0*h2LAG5Yx3d0fhmsO"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lx"><img src="../Images/91945697f300fd7262ea06ede0fd2bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/0*dBh6Fd2K68VCdZ8y"/></div></figure><p id="5a9f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们将使用双向编码器，从左到右以及从右到左阅读句子，这包括一个单词注释中的前面和后面的单词</p><p id="c148" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">向前隐藏状态=</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ly"><img src="../Images/94c66408c679c5aec80cab7c3ff247fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:40/0*oydd6H2Gx55oNQVp"/></div></figure><p id="e754" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">向后隐藏状态=</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ly"><img src="../Images/ccaa92a44992cd663515fa0fabe1d821.png" data-original-src="https://miro.medium.com/v2/resize:fit:40/0*95ZEN98pTsasai5T"/></div></figure><p id="1fff" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">源隐藏状态=</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lz"><img src="../Images/3b373183ba0a2dd4b301f435635107db.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/0*gRX5iawi9VN00qUp"/></div></figure><p id="94ae" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在这个模型中，条件概率定义如下</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ma"><img src="../Images/f4ab28dc3d462060bdbc918404e7b852.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/0*Vu-UEECqHifwNFZ-"/></div></figure><p id="e02b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这里g是具有非线性激活的全连接层，并且将所有显示输入作为级联，s_i是时间步长I的解码器隐藏状态，并且计算如下</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mb"><img src="../Images/707759f7529689d87c71d3987ca156f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/0*5aNGl4BvSnuvubeA"/></div></figure><p id="eb34" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">(f是RNN/LSTM函数)</p><p id="15d5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">c_i是使用注意力分数为每个时间步长计算的上下文向量，注意力分数是使用对齐模型计算的，以对位置I处的输入和位置j处的输出的匹配程度进行评分</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mc"><img src="../Images/3f46be74d9720cdec0f411cf38003b18.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/0*to13VYL7IkAse7bO"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es md"><img src="../Images/aea8ae64e15eb1fa84acc3885d5271b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/0*xTvECxv4LkonaVH6"/></div></figure><p id="1f92" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">(分数标准化的softmax)</p><p id="cd77" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在Bahdanau的论文中，对齐分数a()由具有单个隐藏层的前馈网络来参数化，并且该网络与模型的其他部分联合训练</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es me"><img src="../Images/a655619d05ce54390fa69c0371a152e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/0*FR2eKH6-df6LOjlw"/></div></figure><p id="c23c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">(全连接层，输入是双向编码器的级联隐藏状态和解码器的最后隐藏状态)</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mf"><img src="../Images/2c5d007e3e0005f5ef9f0ac0f62c0894.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/0*0VS77PfGiIMqn3-m"/></div></figure><p id="bdd9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">c_t(上下文向量)是源句子中元素的加权平均值，它表示关于当前元素h_j(out)和相似性得分e_ij的句子表示，然后上下文向量与当前隐藏状态和最后的目标标记y_t-1相结合以生成当前标记y_j</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mg"><img src="../Images/ec51a41c37aaae36e876480c5d5223fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/0*SbJ1kRN3kB-PiZGD"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mh"><img src="../Images/1a04c00e432b2f8e0bc0c692cb4b2063.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/0*eZMqrHh8qlElE1WX"/></div></figure><p id="0801" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">对每个标记y_t重复这个过程，直到输出序列结束</p><p id="8b57" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">注意力得分可以用多种方法计算，这里有一些流行的注意力得分机制</p><h1 id="1d5e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">全球关注</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mi"><img src="../Images/5c201a564cfb97088ed79e1ecaa7e087.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*im28v71I0aLPZxT-zk-fBw.png"/></div></figure><p id="07e5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">全局注意力模型的思想是在导出上下文向量c_t时考虑编码器的所有隐藏状态，这里通过将当前隐藏状态与每个源隐藏状态进行比较来确定分数</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mj"><img src="../Images/419a72a83537bd3afa8d1bf21d416651.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/0*hV6iYJzUUb7RXcmM"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mk"><img src="../Images/51e0be91626030df8eab80bd631e02cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:220/0*OdPaaK2FE8KFxhrA"/></div></figure><p id="cb03" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">(分数标准化的softmax)</p><p id="4f8d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">有三种计算分数的方法</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ml"><img src="../Images/bdd649cad85228ddb1d09dd6bc60e8f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/0*VhyW5C8zeQOSH5n-"/></div></figure><p id="e32b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">(圆点)</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mm"><img src="../Images/6151bf23b99a152870a9d362e1b23a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/0*joqN2nXP3O43LYFH"/></div></figure><p id="3d0b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">(常规)</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mn"><img src="../Images/275605d3ee9a57807072e6a325fd5137.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/0*ZttrGrez5gXSGS_k"/></div></figure><p id="9ba7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">(串联)</p><p id="0c6d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在精神上，全局注意力类似于我们之前讨论过的bahda nu注意力，但两者之间存在某些差异，而bahda nu使用前向和后向编码器状态与目标序列的先前隐藏状态的串联，全局注意力只是使用编码器和解码器LSTM的顶部隐藏状态。</p><h1 id="5a2e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">当地的关注</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mo"><img src="../Images/ffcaf270491de1e79d7cbcac7ecf175c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*g4uG5A6zSz-TP4XVWJ5llg.png"/></div></figure><p id="9d7f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">它选择性地聚焦于小的上下文窗口，并且是可微分的，首先，该模型在时间t为每个目标单词生成“对齐位置”p_t，然后，上下文向量c_t被导出为窗口内的一组编码器隐藏状态的加权平均值(p _ t-D，p_t + D ),其中D是根据经验选择的，如果窗口跨越句子边界，则我们简单地忽略外部部分，并且聚焦于窗口内的单词</p><p id="50d7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">局部注意模型有两种变体</p><p id="220b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><em class="mp">单调对齐</em> —这里位置向量p_t设置为p_t = t，假设源句子和目标句子是单调对齐的，对齐计算如下</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mq"><img src="../Images/f9671cb2677da36370b7448a39a826fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/0*P3EUy14gCs_M3tuk"/></div></figure><p id="98be" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">单调对齐几乎与全局注意相同，除了向量α_it是固定长度和更短</p><p id="45a1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><em class="mp">预测比对</em> —与单调比对不同，这里我们不假设p_t的值，我们使用以下等式进行预测</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mo"><img src="../Images/208032c585aa151821a090c83661e7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*erg2Lukp2iBW9n-8"/></div></figure><p id="1247" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">W_p和v_p是与模型的其余部分一起训练的模型参数。S是源序列的长度，因为我们已经添加了sigmoid the的范围是[0，S]</p><p id="0e07" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">对齐计算如下</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mr"><img src="../Images/003411e34235fd25a3fa353f730f8661.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/0*Z7X_WM6iJOnlGJLE"/></div></figure><p id="f6b0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为了有利于p_t附近的点，高斯分布以p_t为中心</p><h1 id="ee79" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">分层注意</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ms"><img src="../Images/c942639b15c419da2db3f499fc1206d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*VRCY-z__UUz2WaPEHMMVvw.jpeg"/></div></figure><p id="97c3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">该模型主要用于诸如文档分类的应用，它具有反映该模型的分层结构的分层结构，该模型具有在单词级和句子级应用的两级注意机制，使得它能够在构建文档表示时更多或更少地考虑重要内容。如果我们想到一个文档，它有一个如下的嵌套结构</p><p id="3f35" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">字符&gt;单词&gt;句子&gt;文档</p><p id="6c80" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">相应地，层次结构是从doc到char(自顶向下)或者相反(自底向上)构建的。网络通过建立句子的表示，然后将这些集合成一个文档表示来构建文档表示。通过对句子中的单词进行编码并对它们应用注意机制来构建句子表示，从而产生句子表示。以相同的方式构建文档表示，但是它仅接收句子向量作为输入。</p><p id="9cc3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在单词级，我们使用双向GRU作为RNN单元，这为我们提供了单词注释，该注释总结了来自向后和向前方向的信息，产生了变量h_it</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mt"><img src="../Images/e96698f7c429ec12ca266af8933bd4e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/0*gkJP3jbuTYeUz4sZ"/></div></figure><p id="b876" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">然后我们像以前一样应用注意力机制</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mu"><img src="../Images/1cdfa14b71ff7aed1bf2422a0f7e887b.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/0*yEUvAz2N4BL52YCE"/></div></figure><p id="3ecb" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们使用tanh来保持值在[-1，1]之间</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ma"><img src="../Images/56b4ded9878da4bb77498bb2bfa42a2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/0*-NeoO4iDcMz1rOZB"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mc"><img src="../Images/864b8d56969f3b121b0a3fb457fe0b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/0*60BbglL3mung2tU1"/></div></figure><p id="1ab4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在句子层面，我们重复同样的过程，但是s_i作为双向GRU细胞的输入</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mv"><img src="../Images/e02a3ad50e59ab54ffc4a88ed2a99eb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:248/0*aIyYFuSmrVMV2foi"/></div></figure><p id="464f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">可训练的权重和偏差被随机初始化，并且在训练过程中被共同学习。最终输出是文档向量v，它可以用作文档分类的特征。</p><h1 id="4b1e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考</h1><p id="ab7d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><a class="ae kf" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">序列对序列学习用神经网络</a> <br/> <a class="ae kf" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#definition" rel="noopener ugc nofollow" target="_blank">注意？立正！</a> <br/> <a class="ae kf" href="https://arxiv.org/abs/1811.05544" rel="noopener ugc nofollow" target="_blank">介绍自然语言处理问题中的注意机制</a> <br/> <a class="ae kf" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">联合学习对齐和翻译的神经机器翻译</a> <br/> <a class="ae kf" href="https://arxiv.org/pdf/1508.04025.pdf" rel="noopener ugc nofollow" target="_blank">基于注意的神经机器翻译的有效方法</a> <br/> <a class="ae kf" href="https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf" rel="noopener ugc nofollow" target="_blank">用于文档分类的分层注意网络</a> <br/> <a class="ae kf" href="https://humboldt-wi.github.io/blog/research/information_systems_1819/group5_han/" rel="noopener ugc nofollow" target="_blank">利用分层注意网络进行文本分类</a></p></div></div>    
</body>
</html>