<html>
<head>
<title>Review of FutureGAN: Predict future video frames using Generative Adversarial Networks(GANs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">《未来:用生成对抗网络预测未来视频帧》综述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/review-of-futuregan-predict-future-video-frames-using-generative-adversarial-networks-gans-3120d90d54e0?source=collection_archive---------2-----------------------#2021-08-13">https://medium.com/analytics-vidhya/review-of-futuregan-predict-future-video-frames-using-generative-adversarial-networks-gans-3120d90d54e0?source=collection_archive---------2-----------------------#2021-08-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="78f8" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用GANs的未来视频帧预测</h2></div><p id="71b8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由Aigner等人提出的<a class="ae jt" href="https://arxiv.org/abs/1810.01325" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj"> FutureGAN </strong> </a>是一个基于GAN的框架，用于预测未来的视频帧。视频预测是基于先前帧序列的上下文来预测未来视频帧的能力。与静态图像不同，视频在时间维度上提供了复杂的变换和运动模式。因此，为了准确预测未来的视频帧，该模型需要考虑时间和空间分量。典型地，递归神经网络用于模拟时间动态。然而，FutureGAN的作者提出以渐进增长的方式使用<strong class="iz hj">时空3d卷积</strong>来预测未来的视频帧。</p><h2 id="efb8" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">FutureGAN论文的主要观点:</h2><ol class=""><li id="0a59" class="kp kq hi iz b ja kr jd ks jg kt jk ku jo kv js kw kx ky kz bi translated">FutureGAN使用编码器-解码器GAN模型来预测视频序列的未来帧，条件是过去帧的序列。</li><li id="9db8" class="kp kq hi iz b ja la jd lb jg lc jk ld jo le js kw kx ky kz bi translated">为了捕获视频序列的空间和时间分量，在所有编码器和解码器模块中使用时空3d卷积。</li><li id="9a85" class="kp kq hi iz b ja la jd lb jg lc jk ld jo le js kw kx ky kz bi translated">它使用了现有的渐进生长GAN (ProGAN ),可在生成高分辨率单一图像时获得高质量的结果。</li><li id="da27" class="kp kq hi iz b ja la jd lb jg lc jk ld jo le js kw kx ky kz bi translated">FutureGAN框架适用于各种不同的数据集，无需额外改动，性能稳定。</li></ol><h2 id="ccb6" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">为什么是时空3d卷积？</h2><p id="3450" class="pw-post-body-paragraph ix iy hi iz b ja kr ij jc jd ks im jf jg lf ji jj jk lg jm jn jo lh jq jr js hb bi translated">问题是为什么使用时空3d卷积而不是CNN + RNNs来处理空间和时间域。</p><ol class=""><li id="8a9a" class="kp kq hi iz b ja jb jd je jg li jk lj jo lk js kw kx ky kz bi translated">我们知道，应用于图像的2D卷积会输出图像，应用于多个图像的2D卷积(将它们视为不同的通道)也会产生图像。因此，2D卷积运算会在每次卷积运算后立即丢失输入信号的时间信息。</li><li id="4042" class="kp kq hi iz b ja la jd lb jg lc jk ld jo le js kw kx ky kz bi translated">然而，只有3D卷积保留了产生输出体积的输入信号的时间信息。同样的现象也适用于2D和3D投票。</li></ol><h1 id="bee0" class="ll jv hi bd jw lm ln lo ka lp lq lr ke io ls ip kh ir lt is kk iu lu iv kn lv bi translated">未来模型</h1><h2 id="4722" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">发电机网络:</h2><ol class=""><li id="7b38" class="kp kq hi iz b ja kr jd ks jg kt jk ku jo kv js kw kx ky kz bi translated">FutureGAN发电机由编码器和解码器部分组成。作者在他们的编码器和解码器中使用了渐进增长(ProGAN)方法。</li><li id="4886" class="kp kq hi iz b ja la jd lb jg lc jk ld jo le js kw kx ky kz bi translated">编码器学习输入的潜在表示。解码器使用该潜在表示来生成预测。</li><li id="8c7d" class="kp kq hi iz b ja la jd lb jg lc jk ld jo le js kw kx ky kz bi translated">所有卷积层都使用3d卷积。这允许发生器适当地编码和解码输入序列的空间和时间分量。</li><li id="8893" class="kp kq hi iz b ja la jd lb jg lc jk ld jo le js kw kx ky kz bi translated">具有不对称核大小和跨度的3d卷积被用于下采样，而具有不对称核大小和跨度的转置3d卷积被用于上采样。</li></ol><h2 id="f8f4" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">鉴别器网络:</h2><ol class=""><li id="9070" class="kp kq hi iz b ja kr jd ks jg kt jk ku jo kv js kw kx ky kz bi translated">我们的FutureGAN模型的鉴别器被设计用来区分真实和虚假的序列。</li><li id="ee0a" class="kp kq hi iz b ja la jd lb jg lc jk ld jo le js kw kx ky kz bi translated">鉴别器网络从训练集中获取代表地面实况序列的帧和由生成器创建的帧作为输入。</li><li id="00dd" class="kp kq hi iz b ja la jd lb jg lc jk ld jo le js kw kx ky kz bi translated">除了瓶颈层，FutureGAN鉴别器密切模仿发电机网络的编码器组件。鉴别器中没有逐像素的特征向量归一化层，这是一个显著的区别。</li></ol><h2 id="cdde" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">FutureGAN培训:</h2><p id="c5b8" class="pw-post-body-paragraph ix iy hi iz b ja kr ij jc jd ks im jf jg lf ji jj jk lg jm jn jo lh jq jr js hb bi translated">作者首先对模型进行编程，以获取4x4px分辨率帧的集合，并输出相同分辨率的帧，类似于ProGAN。经过一定数量的轮次后，逐渐增加图层以使分辨率加倍。输入帧的分辨率始终与网络当前条件下的分辨率相同。</p><p id="6c8e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">稳定训练和防止模式崩溃的一些其他关键特征包括:</p><ol class=""><li id="cd84" class="kp kq hi iz b ja jb jd je jg li jk lj jo lk js kw kx ky kz bi translated">重量比例。</li><li id="f608" class="kp kq hi iz b ja la jd lb jg lc jk ld jo le js kw kx ky kz bi translated">生成器中的特征规范化。</li><li id="91fb" class="kp kq hi iz b ja la jd lb jg lc jk ld jo le js kw kx ky kz bi translated">使用WGAN-GP损失和ε损失。</li></ol><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es lw"><img src="../Images/348bd14c76ed0483aeee7a0682f1c73d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D-4HitlwKROYQJkVjuUyJw.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">带ε损失的WGAN-GP损失</figcaption></figure><p id="5597" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">损失函数由Wasserstein GAN梯度惩罚(WGAN-GP)损失组成，带有ε惩罚项，防止损失漂移。作者选择WGAN-GP损失来训练他们的模型，因为它提高了生成的帧的质量。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mm"><img src="../Images/d34eb7df10c0e2264641991e42731683.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Elqv-e2auiyVMPYOcR4N3g.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">培训期间的FutureGAN发电机</figcaption></figure><h2 id="3024" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">结果:</h2><p id="1a34" class="pw-post-body-paragraph ix iy hi iz b ja kr ij jc jd ks im jf jg lf ji jj jk lg jm jn jo lh jq jr js hb bi translated">为了评估他们的模型，作者在三个越来越复杂的数据集上进行了实验，其中包括MovingMNIST，KTH行动数据集和Cityscapes数据集。作者提供了地面真实和预测帧序列之间的均方误差(MSE)、峰值信噪比(PSNR)和结构相似性指数(SSIM)的值，以定量评估模型。结果如下所示:</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mn"><img src="../Images/e185441b590563106e8ad0a5a3fb6fbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-96YTCHhP1KEVNrLpf2uYw.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">未来结果</figcaption></figure><p id="0050" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从表中可以看出，对于MovingMNIST数据集，FutureGAN模型的性能优于fRNN模型。然而，MCNet模型对KTH行动数据集表现更好。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mo"><img src="../Images/b31686b2b406d40c4b94fc77eedb8d5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjPAOKGt-HdORfP4VViu0g.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">MovingMNIST数据集的未来结果。a:输入，b:地面实况，c: FutureGAN，d: fRNN</figcaption></figure><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mo"><img src="../Images/ff381ad5ca67fd1709f40448c54370da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U4gEUoc0LOzWruH4sh4iXg.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">KTH动作测试分割的FutureGAN结果。a:输入，b:地面实况，c: FutureGAN，d: fRNN，e: MCNet</figcaption></figure><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mp"><img src="../Images/c2fd3c7522fcb6fd6e47b1be7736cfd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WBVQRRF61c3Hks1Xufj_MQ.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">Cityscapes数据集的FutureGAN结果。a:输入，b:地面实况，c:未来感</figcaption></figure><h2 id="a5fc" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">结论:</h2><p id="3771" class="pw-post-body-paragraph ix iy hi iz b ja kr ij jc jd ks im jf jg lf ji jj jk lg jm jn jo lh jq jr js hb bi translated">通过FutureGAN论文，作者展示了在不使用RNNs的情况下，使用基于GAN的框架进行未来视频帧预测。作者表明，ProGAN模型可以用来生成逼真的未来帧。作者表示，FutureGAN是一个高度灵活的模型，可以很容易地在不同分辨率的各种数据集上进行训练，而无需事先了解数据。</p><h2 id="9f62" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">参考:</h2><p id="3e52" class="pw-post-body-paragraph ix iy hi iz b ja kr ij jc jd ks im jf jg lf ji jj jk lg jm jn jo lh jq jr js hb bi translated"><a class="ae jt" href="https://arxiv.org/abs/1810.01325" rel="noopener ugc nofollow" target="_blank"> FutureGAN:在渐进增长的GAN中使用时空3d卷积预测视频序列的未来帧</a></p></div></div>    
</body>
</html>