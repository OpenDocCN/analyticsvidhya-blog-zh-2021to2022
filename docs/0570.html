<html>
<head>
<title>Optimization algorithms in machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的优化算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/optimization-algorithms-in-machine-learning-6493c7badb6e?source=collection_archive---------9-----------------------#2021-01-22">https://medium.com/analytics-vidhya/optimization-algorithms-in-machine-learning-6493c7badb6e?source=collection_archive---------9-----------------------#2021-01-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="cc02" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated"><strong class="ak">柯西vs牛顿</strong></h2></div><p id="5d7e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这篇文章将集中在应用于机器学习问题的优化算法上。与流行的梯度下降算法相比，牛顿型算法是最高级的优化算法。但是在机器学习中广泛使用的是梯度算法的变体。事实是梯度算法属于所谓的<strong class="iy hi">柯西型</strong>算法，只有一个<strong class="iy hi">线性收敛速度</strong>【1】。奥古斯丁·柯西(Augustin Cauchy)提出了梯度下降算法的第一个变体之一——最速下降法(又名柯西法)。它使用迭代过程，其中使用一维线性优化来估计第α<strong class="iy hi">k</strong>步长，而第<strong class="iy hi"> s(。)</strong>基于第<strong class="iy hi"> k </strong>点的梯度估计选择方向。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es js"><img src="../Images/12e53f46be0c4e12216b1891d7386bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*ENpK0fKICkqlKmlNFwVFGA.jpeg"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">所有梯度算法的一般迭代过程</figcaption></figure><p id="a059" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">具有二次收敛速度的更有效和更快速的优化算法，例如<strong class="iy hi">牛顿</strong>方法或高斯-牛顿方法，在理论上早已为人所知。然而，它们在ML中的应用存在一些问题。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es ke"><img src="../Images/237165bd47360250545cd489089e58a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*waVaWO78TbYvfsjxtBfarg.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">左边是奥古斯丁·路易斯·柯西，右边是艾萨克·牛顿(g .科内尔的肖像，1689年)</figcaption></figure><p id="c7ac" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为简单起见，让我们考虑一种具有固定步长的梯度算法变体，它演示了线性优化。该算法是任何神经网络操作的基础。其优点在于逐步优化的可靠性和保证收敛到次优点。这种相当简单但鲁棒的算法的使用表明，我们仍处于将优化理论应用于机器学习的早期阶段。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kf"><img src="../Images/111bd1713f56ddd495884abddc76bc73.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*uHaHMYcNxy_6IZj79wS8qQ.jpeg"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">梯度算法，或称柯西最速下降算法，广泛应用于人工智能和最大似然法</figcaption></figure><p id="7fb2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">事实是梯度法应用了关于一阶导数的知识，并参考了<strong class="iy hi">一阶</strong>方法。与此同时，在最优化理论中，寻找最优解的更有效的方法早已被开发并存在。训练诸如递归(RNN)、具有多个隐藏层的网络(深度神经网络CNN)或卷积网络(CNN)的神经网络的任务被简化为找到它们的权重系数。通常，梯度算法的一个变体充当优化算法。选项如图所示。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kg"><img src="../Images/9d4ff9d966e4aab1f04d7390f6ef9d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q2ZY8OlRP1ggzxemcz86Ng.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">机器学习中梯度下降的进化</figcaption></figure><p id="0435" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，可以认为，所有现代机器学习系统都是基于一系列梯度算法，采用逐步优化或逐步线性解决方案搜索。奇怪的是，我们目前没有使用具有非线性收敛速度的快速搜索过程。</p><p id="a4f0" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">梯度算法的一种替代方法是牛顿法或其选项——改进的牛顿法[1]。这些方法是<strong class="iy hi">二阶方法</strong>，具有二次收敛性，使用二阶导数。这提高了收敛速度，实现了更通用的搜索策略，并加速了机器学习。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kl"><img src="../Images/6b20f5111e6aafd9945c356cf5b04d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/1*Dx-QFYKVnqULsgD2m9r_uw.gif"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">牛顿方法</figcaption></figure><blockquote class="km kn ko"><p id="895e" class="iw ix kp iy b iz ja ii jb jc jd il je kq jg jh ji kr jk jl jm ks jo jp jq jr ha bi translated">我们可以在依赖于两个参数<strong class="iy hi">【x，y】的函数的最简单优化问题上比较这些算法的收敛速度和稳定性。</strong></p></blockquote><p id="5038" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们考虑一个具有几个局部最小值的多峰函数的例子，由下面的表达式给出:</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kt"><img src="../Images/62fc5ad562188a5d5cc456573fad5e84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RNoyWDvSmEW4zk3j6YpBeA.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">多模态Himmelblau函数</figcaption></figure><p id="0d30" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是最优化理论中众所周知的Himelblau函数。它有四个相邻的局部最小值，一个中心局部最大值和四个鞍点。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es ku"><img src="../Images/14327435e325f49910f0e6e1ecc35f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*KowWEHFk_nHhh0vrfw9hIg.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">希梅尔布劳函数</figcaption></figure><p id="4c4f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">从寻找最佳点和测试训练或优化算法的角度来看，该函数很有趣。绘制Himmelblau函数的 <strong class="iy hi">的<strong class="iy hi">代码如下:</strong></strong></p><figure class="jt ju jv jw fd jx"><div class="bz dy l di"><div class="kv kw l"/></div></figure><p id="7ec4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为神经网络寻找最优系数(目标函数的最优解)的问题是<strong class="iy hi">现代问题具有<strong class="iy hi"> <em class="kp"> N &gt; 1000的大维数</em></strong>。 </strong>情况因异构、“原始”训练数据而变得复杂，这导致目标函数表面上的噪声。例如，将参数<strong class="iy hi"> <em class="kp"> M=0，sigma =20 </em> </strong>的正态分布噪波添加到Himmelblau函数中，会导致曲面的显著变形。这是数值算法故障和神经网络系数估计中出现误差的原因。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es ku"><img src="../Images/1f1704205ccc8f3cad631b61d2d926a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*fRYD6uzTKak7wChGcOYGGw.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">嘈杂的Himmelblau函数</figcaption></figure><p id="3856" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">实践表明，具有自适应步长调整、约束、罚函数和正则化的梯度算法的现代版本允许相当精确地调整网络权重系数。</p><p id="002c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们理解为什么梯度法的简单版本不能有效地工作，并且需要如此多的复杂性。对于简化的Himmelblau函数，我们绘制了梯度算法和牛顿法的收敛图。</p><p id="a933" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">带搜索记录的<em class="kp">梯度算法</em> 的<strong class="iy hi">代码如下。</strong></p><figure class="jt ju jv jw fd jx"><div class="bz dy l di"><div class="kv kw l"/></div></figure><p id="829e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> W_history </strong>变量是函数值，<strong class="iy hi"> f_history </strong>是函数值，<strong class="iy hi"> eps_history_f </strong>是函数值之间的范数距离，<strong class="iy hi"> eps_history_xy </strong>是变量值之间的范数距离。这些变量集合使用<strong class="iy hi"> np存储在堆栈中。堆栈()。</strong>它们允许分析梯度算法收敛到Himmelblau函数<strong class="iy hi"> f(x，y)的最优解<strong class="iy hi"> f (x*，y*) </strong>。</strong>收敛到最优解可以用极限来表示。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kx"><img src="../Images/da453d3fcfcf6a13ca3693f6a16c6d18.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*sdcZqdOW5-yNgO45vytBzA.jpeg"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">收敛到最优解</figcaption></figure><blockquote class="km kn ko"><p id="cce9" class="iw ix kp iy b iz ja ii jb jc jd il je kq jg jh ji kr jk jl jm ks jo jp jq jr ha bi translated">类似地，在机器学习中，梯度算法最小化目标误差函数<strong class="iy hi"> J(。)</strong>用同样的方法从矢量中提取θ参数。它将<strong class="iy hi"> h </strong>假设与<strong class="iy hi">数据</strong>训练数据相匹配。</p></blockquote><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es ky"><img src="../Images/54adb0fbb5bfa793cf6172447cb0268e.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*yblx6P8a2-iNqmr-3_OUgg.jpeg"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">最小化神经网络的目标函数</figcaption></figure><p id="7413" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们呈现为<strong class="iy hi"> <em class="kp"> f (x，y) </em> </strong>函数和<strong class="iy hi"><em class="kp">【x，y】</em></strong>自变量获得的收敛图。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kz"><img src="../Images/a3aa594f57a6ec24b120394589d07acc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jxxcFaJo-8vUGI3HhBYyxA.png"/></div></div></figure><p id="6976" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">图表显示了梯度算法在<strong class="iy hi"> ~200 </strong>次迭代中的收敛性。作为比较，牛顿法仅在<strong class="iy hi"> ~20 </strong>次迭代中收敛。<strong class="iy hi">牛顿法如下所示。</strong></p><figure class="jt ju jv jw fd jx"><div class="bz dy l di"><div class="kv kw l"/></div></figure><blockquote class="km kn ko"><p id="d6bd" class="iw ix kp iy b iz ja ii jb jc jd il je kq jg jh ji kr jk jl jm ks jo jp jq jr ha bi translated">梯度法慢慢滑动到坐标为[3，2]的一个最优点。相反，牛顿法沿着目标函数的曲面向相同的最优方向跳跃。</p></blockquote><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es la"><img src="../Images/5ee6166a2c3390c82970dd42842591a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HLfuj_IZ4PFs1tUlkunyDg.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">最优解的收敛性[x，y]</figcaption></figure><p id="dc2f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">机器学习为什么不用二阶方法？毕竟，它们允许应用更通用的搜索策略。事实是，尽管有效，二阶方法不太稳定。他们给出了适应过程中的一些异常值。当在真实世界的情况下使用有噪声的目标函数时，神经网络将被训练有更多的异常值。</p><p id="2ab5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下面可以观察到有噪声的Himmelblau函数的优化，其中函数值和第一梯度的值都被具有参数<strong class="iy hi"> <em class="kp"> (M=0，sigma =5) </em> </strong>的高斯白噪声所噪声化。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es la"><img src="../Images/746cd76eb77e916d70aa30bccdc153ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yXj_T0L16DWkn7j0h77xVg.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">有噪声最优解的收敛性</figcaption></figure><blockquote class="km kn ko"><p id="27b1" class="iw ix kp iy b iz ja ii jb jc jd il je kq jg jh ji kr jk jl jm ks jo jp jq jr ha bi translated">对于噪声函数收敛到次优点的图形表明，慢梯度算法看起来比快速“牛顿”算法更可取！</p></blockquote><p id="609b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">牛顿法有时间适应函数的噪声抖动，这会对[x，y]最优值的估计产生负面影响(下图中的绿色图形)。同时，梯度法给出了自变量的相当准确的平均估计值(下面[x，y]的红色图形)。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kz"><img src="../Images/b29bbcb1c2170ef451ee2d343bee517e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VDv2pZEyE4kfjfnpe8Q2Pg.png"/></div></div></figure><p id="a6af" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，当优化神经网络的成本函数<strong class="iy hi"> J </strong>时，很难确定更可靠和更快速的算法。工程师们通常选择一种缓慢但可靠的逐步梯度算法，并对其进行了大量修改，称为Adam [2]或其第二个版本Nadam [3]。由于真实数据噪声很大且不均匀，梯度算法的缓慢收敛特性非常有用。<strong class="iy hi">带有注释和许多参数的完整代码</strong>用于比较柯西算法(基本梯度下降算法)和牛顿法，可在以下网址找到:</p><div class="lb lc ez fb ld le"><a href="https://github.com/AlexTitovWork/Optimator" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab dw"><div class="lg ab lh cl cj li"><h2 class="bd hi fi z dy lj ea eb lk ed ef hg bi translated">AlexTitovWork/光学校准仪</h2><div class="ll l"><h3 class="bd b fi z dy lj ea eb lk ed ef dx translated">玩优化算法。关于代码。在这个代码研究梯度或柯西方法-一阶方法与…</h3></div><div class="lm l"><p class="bd b fp z dy lj ea eb lk ed ef dx translated">github.com</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls jy le"/></div></div></a></div><h2 id="4dfd" class="lt lu hh bd lv lw lx ly lz ma mb mc md jf me mf mg jj mh mi mj jn mk ml mm mn bi translated">链接</h2><ol class=""><li id="9ef8" class="mo mp hh iy b iz mq jc mr jf ms jj mt jn mu jr mv mw mx my bi translated">雷克雷蒂斯g .，雷文德兰a .，雷格斯代尔K. (1986年)。设备的优化。在两本书里。第一册。从英语翻译过来。M.: Mir(俄语)。</li><li id="a379" class="mo mp hh iy b iz mz jc na jf nb jj nc jn nd jr mv mw mx my bi translated"><a class="ae ne" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">https://machinelingmastery . com/Adam-optimization-algorithm-for-deep-learning/</a></li><li id="5402" class="mo mp hh iy b iz mz jc na jf nb jj nc jn nd jr mv mw mx my bi translated"><a class="ae ne" rel="noopener" href="/konvergen/modifying-adam-to-use-nesterov-accelerated-gradients-nesterov-accelerated-adaptive-moment-67154177e1fd">https://medium . com/konvergen/modificing-Adam-to-use-nesterov-accelerated-gradients-nesterov-accelerated-adaptive-moment-67154177 e1fd</a></li></ol></div></div>    
</body>
</html>