<html>
<head>
<title>How does ReLU activation work? (part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ReLU激活是如何工作的？(第二部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-does-relu-activation-work-part-2-8bb4feeb3b42?source=collection_archive---------8-----------------------#2021-02-19">https://medium.com/analytics-vidhya/how-does-relu-activation-work-part-2-8bb4feeb3b42?source=collection_archive---------8-----------------------#2021-02-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/17c34ce390074253f0935ef98c3285a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U-w1-go_LhEmXhjJZxzqZQ.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">由<a class="ae it" href="https://unsplash.com/@alexwende?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">亚历山大·文德</a>在<a class="ae it" href="https://unsplash.com/s/photos/dimension?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="9a9a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是理解ReLU激活如何工作的旅程的第二部分。如果您还没有阅读第一篇文章，请在继续阅读之前阅读它。</p><p id="e910" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在上一篇文章中，我们已经看到了一个简单的激活函数是如何与一个线性函数相结合并使其变得复杂的。但是我们还没有讨论好的学习功能如何与实际功能相比较。</p><p id="6df1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在此之前，我们必须更新我们对学习功能的理解。到目前为止，我们认为学习函数只是输入的函数。实际上，作为一个学习函数，它必须是一个可以使用参数修改的<strong class="iw hi">函数。然后，我们可以使用“学习函数”作为改变这些参数的各种组合的输出的函数。对于这些不同的值，我们定义了一个<strong class="iw hi">损失值。</strong>当我们以理想的方式配置学习函数时，损失值应该很低。</strong></p><p id="7872" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，注意损失函数对于不同的参数组合给出不同的损失值；我们的<strong class="iw hi">目标</strong>是找到最小化损失的“最佳点-参数值”。因此，我们将此定义为我们的“目标函数”；它输出一个必须最小化的值。我们最终将我们的“模型”视为<em class="js">“损失总量的优化”</em></p><figure class="ju jv jw jx fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es jt"><img src="../Images/8d1298f52012a221d2f66b46e5aa39cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MyFzeLPS36V1TAt9oMN16A.png"/></div></div></figure><p id="877d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在反向传播步骤中，我们计算每个参数应该改变多少，以降低给定<code class="du jy jz ka kb b">X</code>值处的损失函数值[如果我们在每个数据点反向传播]。</p><p id="6ca3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在我们知道了学习函数，目标函数，以及一点点学习函数是如何变化的。现在让我们来解决我们最大的问题，“ReLU激活如何用一个线性函数以非常高的精度逼近一个复杂的函数？”。为了说明这一点，我们试图建立一个对数函数的回归模型。</p><p id="1325" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">假设我们想要“学习”二维输入的以下函数:</p><figure class="ju jv jw jx fd ii er es paragraph-image"><div class="er es kc"><img src="../Images/2e1e109c0b49871fb1c7e6a13768d435.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*Xb4QQQVGKlbRu5x6bjFIsg.png"/></div></figure><figure class="ju jv jw jx fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kd"><img src="../Images/45240da0c4d9bac45b463dafb1210fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CHMRpWds-YVej6TxYNeOdQ.png"/></div></div></figure><p id="e754" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了方便我们创建最终的图形，这里我们建立一个4层的神经网络，有2个输入和1个输出。你会看到除了<strong class="iw hi"> logits层</strong>【输出神经元】之外，每个神经元后面都有ReLU激活函数。在潜入3D空间之前，需要了解一件小事；即使我们的输入是2D，ReLU也只接受向量空间[1D输入],正如我们之前看到的。但这并不意味着我们的学习功能只接受一个输入。</p><figure class="ju jv jw jx fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ke"><img src="../Images/74b7d56959327dbc92dfa42690a29238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jsDCAoBVQSXE0so2St9PLQ.png"/></div></div></figure><figure class="ju jv jw jx fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kf"><img src="../Images/f0180ea1afc7037b83f2933dd46e4cb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KiDvhQRMM2dPq5wRZ0d3hQ.png"/></div></div></figure><p id="9f9f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">第一步:<em class="js">在2D空间中重新折叠图形</em></p><p id="c8c7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们希望神经网络做的是找到一个函数，该函数(如果按照我们的要求训练的话)给出与上面的对数函数的平滑曲线接近的形状。</p><p id="99f2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在想象一下:</p><p id="74cb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">想一个由2D函数值构成的曲面。想象它是x-y平面上方的一个可拉伸、可折叠的表面。现在，改变x-y平面上方的曲面形状相当于改变输入-输出关系:也就是说，每个不同的形状都是不同的函数。飞机越扭曲折叠，功能越复杂。</p><p id="0bfd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在来做一些“精神折纸”:</p><p id="185a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">简单的F(x，y)= WX + b在做平面。</p><p id="dbfc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">应用一个Relu意味着“折叠”穿过切割x-y平面的线，如步骤1所示。</p><p id="ee22" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">请注意，我们的神经网络的第2层有12个节点，每个节点给出简单1折曲面的不同版本。</p><figure class="ju jv jw jx fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kg"><img src="../Images/c6be0cb0f5917f37af318bfb2de382b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2kmGR9kZUxYGoiLi6Axppg.jpeg"/></div></div></figure><p id="83c5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">步骤2:前一层中功能的集合。(我们对第2层中的输入做了什么)</p><p id="504f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们的神经网络的第二个隐藏层的每个节点将4个这样的1折曲面作为输入，然后将它们相加(将4个曲面叠加在一起)。这创造了一个很好的“漏斗形状”。基于我们的线性权重矩阵w，我们得到长/高漏斗、平/开口漏斗和许多其他形状。</p><figure class="ju jv jw jx fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kg"><img src="../Images/70ebdbb576a8cb0a50dcfcee89ab25e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gWVpF-Ex83lqgefBE6m8gw.jpeg"/></div></div></figure><figure class="ju jv jw jx fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kd"><img src="../Images/e5fed601b74a28e6d65772bd72bcf137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i5NRsbRpvbOZwoJXgl2UJA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">层2的Relu现在使x-y平面下的表面变平，以给出最终的漏斗形状。</figcaption></figure><p id="f036" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">步骤3:集合函数空间的集合函数。(第3层的输出)</p><figure class="ju jv jw jx fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kh"><img src="../Images/a73e9c222bc6fb2211d62b8139fd6e25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F4g1BfTiXl7DCZs7PYhErQ.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">现在，我们每个节点有三个漏斗的集合，这些集合构成了一个更复杂的漏斗。</figcaption></figure><p id="ab4a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">第四步:创建最终学习函数。(第4层-没有ReLU，只是一个聚合)</p><figure class="ju jv jw jx fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kd"><img src="../Images/e3554d63423118bdfe79ffbdf932ccbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nBp7-N_qFZ8KNS1thy70ww.png"/></div></div></figure><p id="a0b2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">第五步:模型建立的实际学习功能</p><figure class="ju jv jw jx fd ii"><div class="bz dy l di"><div class="ki kj l"/></div></figure><figure class="ju jv jw jx fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kk"><img src="../Images/5be7ca1bbcf577125264d8dc998b8dd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5hkKXKxl_yCtKRYYXMB0EA.png"/></div></div></figure><div class="kl km ez fb kn ko"><a href="https://drive.google.com/drive/folders/1jMvEqEx0Xi1lwy8IerYphouAAQAcjiJ0?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="kp ab dw"><div class="kq ab kr cl cj ks"><h2 class="bd hi fi z dy kt ea eb ku ed ef hg bi translated">geogebra-文件- Google Drive</h2><div class="kv l"><h3 class="bd b fi z dy kt ea eb ku ed ef dx translated">编辑描述</h3></div><div class="kw l"><p class="bd b fp z dy kt ea eb ku ed ef dx translated">drive.google.com</p></div></div><div class="kx l"><div class="ky l kz la lb kx lc in ko"/></div></div></a></div><p id="9c5b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，唯一剩下的问题是为什么ReLU激活在深度学习中受欢迎。</p><ul class=""><li id="9a74" class="ld le hh iw b ix iy jb jc jf lf jj lg jn lh jr li lj lk ll bi translated"><strong class="iw hi"><em class="js">“ReLU赋予模型函数流形空间”。</em> </strong> A <strong class="iw hi">流形</strong>是一个“局部”类似欧几里得空间的拓扑空间。这仅仅意味着我们可以用一个流形在一个相对低维的空间中表示高维空间。这里<strong class="iw hi"> </strong>我们通过2D无穷小面片【平面】建立到三维空间的流形。因此，我们可以像在梯度下降中使用<strong class="iw hi">等高线</strong>图一样使用这个空间。</li><li id="be6d" class="ld le hh iw b ix lm jb ln jf lo jj lp jn lq jr li lj lk ll bi translated">还有，最后的学习函数是一个“n重<strong class="iw hi">折纸空间”</strong>。这意味着通过折叠空间来增加维度，就像通过折叠2D纸来构建3D折纸一样。<strong class="iw hi"> N折</strong>的意思是，折纸空间中所有的折线都是恒定梯度的【直线】。我们可以利用这些特性，使用线性规划(如单纯形算法)来优化学习函数。</li><li id="a8a6" class="ld le hh iw b ix lm jb ln jf lo jj lp jn lq jr li lj lk ll bi translated">ReLU是一个计算非常简单但功能非常复杂的函数。数学上，这是输入函数和单位阶跃函数之间的卷积。这意味着我们的模型更复杂，计算负担更少</li><li id="207f" class="ld le hh iw b ix lm jb ln jf lo jj lp jn lq jr li lj lk ll bi translated">使用ReLU时，自动签名比使用其他激活功能相对容易。这意味着使用ReLU激活不会改变学习函数的导数，因此ReLU激活不会在每次反向传递时增加计算梯度的额外负担。</li></ul><p id="e39e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，我们已经讨论了ReLU激活函数是什么<strong class="iw hi">，</strong>它对学习函数有什么作用，它如何构建精确而复杂的学习函数，以及为什么ReLU在深度学习中如此有前途。我们对ReLU激活函数的讨论到此结束。</p><p id="9edc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">谢谢大家！</p></div></div>    
</body>
</html>