<html>
<head>
<title>3D Reconstruction News — AAAI 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">三维重建新闻— AAAI 2021</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/3d-reconstruction-news-aaai-2021-5f435cba6718?source=collection_archive---------8-----------------------#2021-04-16">https://medium.com/analytics-vidhya/3d-reconstruction-news-aaai-2021-5f435cba6718?source=collection_archive---------8-----------------------#2021-04-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="dba4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">【免责声明:3D重建新闻是一系列博客文章，我在其中回顾了在某次会议上对深度估计、MVS、SfM、VO、VSLAM和其他与从图像进行大规模户外3D重建相关的领域的最佳贡献】</em></p><p id="9891" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里是今年的另一个主要会议，即<a class="ae jd" href="https://aaai.org/Conferences/AAAI-21/" rel="noopener ugc nofollow" target="_blank"> AAAI 2021 </a>(你可以在这里找到我之前在WACV 2021 <a class="ae jd" rel="noopener" href="/analytics-vidhya/3d-reconstruction-news-wacv-2021-4a8e504aea19">上的帖子)。3D重建领域有什么新进展？以下是我通常的三个要点:</a></p><ul class=""><li id="5179" class="je jf hh ig b ih ii il im ip jg it jh ix ji jb jj jk jl jm bi translated">无人监管的多视图立体(MVS)终于成为相关的东西。</li><li id="50ff" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">立体匹配网络非常愚蠢，对对抗性攻击不够稳健。</li><li id="476d" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">随着网络能够处理遮挡、动态场景、高分辨率输入等等，单目深度估计正在经历它的黄金时代。</li></ul></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h2 id="3caf" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">通过有效的共同分割和数据增强的自我监督的多视图立体</h2><p id="169e" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">虽然无监督学习已经充斥了单目深度估计领域，但是大多数多视图立体(MVS)方法仍然需要地面真实深度图进行训练，从而严重限制了它们在现实世界中的应用。继最近关于自我监督的MVS的其他很酷的工作之后，本文[1]展示了如何从现有的MVS基线中移除监督，例如MVSNet [2]。他们基于以下观察:当前无监督的MVS网络依赖于来自不同视图的点之间的颜色恒常性，这导致了模糊的监督。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es kz"><img src="../Images/c17d21b25511b39f94e16aff3ee363b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SQFjNMliwwt1uu0gPXOq3A.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">现有无监督MVS网络中模糊监督的例子。图片来自[1]。</figcaption></figure><p id="e09f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了缓解这个问题，他们提出用两个先验来指导自我监督:</p><ul class=""><li id="db8c" class="je jf hh ig b ih ii il im ip jg it jh ix ji jb jj jk jl jm bi translated">多视图中的对应点应该语义一致。他们使用基于非负矩阵分解的无监督共同分割方法来实现这一想法，但在更具体的场景中(如自动驾驶)，来自预训练网络的语义地图将做得很好。语义损失项仅仅是给定分割图和扭曲分割图之间的每像素交叉熵损失。</li><li id="0ee2" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">该网络应该足够健壮，以便从具有不同颜色、照明和模糊的数据中学习。为此，对输入图像应用随机变换并将其投影到源视图，然后计算原始深度图和受损深度图之间的差异。这是一个正则项。</li></ul><p id="cb4c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这两项有助于最终的损失函数，以及来自单目深度估计文献的常见光度损失和平滑损失。这个框架的有趣的方面是，原则上任何MVS网络都可以用作主干，并且结果变得可以与监督方法竞争，同时显著优于现有的非监督方法。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h2 id="a595" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">立体认知:用对抗性扰动愚弄立体网络</h2><p id="4811" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">深度网络的鲁棒性是一个关键问题，因为它们将被部署在现实世界的场景中，例如自动驾驶汽车的立体深度估计。加州大学洛杉矶分校的这项新工作[3]表明，当前的立体声网络对随机噪声足够鲁棒，但对精心制作的敌对扰动不够鲁棒，这可能导致彻底失败。这是令人惊讶的，因为实际上不需要学习立体，因为立体对和底层3D场景之间的关系完全以闭合形式确定，至少对于具有足够好的视觉条件的点(即，共同可见的点位于具有恒定照明的光滑朗伯表面上)是如此。在立体匹配中使用深度学习的原因本质上是为了在不满足这些条件的区域中正则化解决方案，因此网络对这些攻击的脆弱性表明它们实际上被迫忽略证据。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lp"><img src="../Images/f3accdae4f9c9eda2e37cf79f479174d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OpAm1Sp-J1HeFVL52m6HCA.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">敌对攻击的灾难性后果的例子。图片来自[3]。</figcaption></figure><p id="0c34" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另一个有趣的见解是，不同的架构显示出不同程度的鲁棒性，基于特征的方法(如PSMNet [4])最不鲁棒，基于匹配的方法(如AANet [5])最鲁棒。最后，他们表明，这些对抗性扰动可以在训练中用作数据增强技术，在不损害整体准确性的情况下增加鲁棒性。这与图像分类网络形成对比，在图像分类网络中，对攻击的鲁棒性和准确性成反比。</p><p id="f43b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意:这些攻击可以通过完全访问网络来实现，并且需要大量时间来构建。还有，你的新自动驾驶特斯拉配备了很多冗余的传感器，你可以放心让它开。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h2 id="737f" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">HR-DEPTH:高分辨率自我监督单目深度估计</h2><p id="a207" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">从单目视频进行无监督深度估计的最新研究方向是建立能够处理和利用高分辨率输入的模型。HRDepth [6]的作者对这一目标做出了新的贡献，他们表明现有高分辨率网络(如PackNet [7]或SuperDepth [8])的准确性并不比低分辨率网络好多少。但是为什么呢？他们的主要观察结果是，大多数误差都集中在对象边界，用于上采样的双线性插值模块不可避免地会引起这种误差。下图显示了当地面实况为大的图像梯度时，上采样的低分辨率估计比高分辨率估计产生更高的预测误差。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lq"><img src="../Images/2bddcbcce2fb7a2983b952c601537bb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lqC7JPhqYCopb9GcjhdzvQ.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">上采样在对象边界引入误差的原因。图片来自[6]。</figcaption></figure><p id="82b9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了解决这个问题，提出了两种架构改进:</p><ul class=""><li id="2cbd" class="je jf hh ig b ih ii il im ip jg it jh ix ji jb jj jk jl jm bi translated">首先，U-Net通常的跳过连接不足以正确融合特征。想法是添加几个中间结点来聚合要素，并通过包含这些中间结点来增加跳过连接的密度。这些新模块的设计只是一个卷积，后面跟着一个激活函数。然而，它们之间的相互联系才是最重要的。</li><li id="2e34" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">其次，编码器由简单的残差块构成，解码器由特征压缩和激励模块构成，减少了网络参数，提高了融合质量。这些块首先使用全局平均池来挤压特征图，然后使用完全连接的层来衡量每个特征的重要性，最后使用1x1卷积来融合它们。</li></ul><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lr"><img src="../Images/c2dc409e8168fa7889cba1ebf49e4f35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zRcalDP68vQwJBXqvTl_Uw.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">HRDepth架构。图片来自[6]。</figcaption></figure><p id="9db0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种方法实现了最先进的性能，同时具有更少的参数和处理更高的输入图像。这项工作的另一个有趣的贡献是提出了同一网络的精简版本，通过将更大网络的输出视为地面事实，以“受监督”的方式对其进行训练:向将这些模型部署到便携式设备迈出了一步。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h2 id="0b56" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">用于单目深度估计的逐块注意网络</h2><p id="a31b" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">注意力无疑是现在机器学习中最热门的话题。在本系列的上一集中，我们看到它被应用于MVS网络，而在这里，一个新的注意力模块已经被设计用于监督单目深度估计[9]。他们旨在解决的具体问题是，以前的研究没有足够关注场景局部区域中相邻像素之间的关系。为此，基本思想是设计逐块注意模块，该模块将来自解码器的最后上采样层的局部上下文特征和具有预定义块尺寸的全局上下文特征有效地组合。由此产生的繁琐架构如下所示，它代表解码器的最后一步，紧接着是深度预测之前的卷积层。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es ls"><img src="../Images/b028f74335c47994785218f96aa6d0f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hKEW9rQfH-EZwyCaH2bAbg.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">[9]中提出的注意模块的整体架构。</figcaption></figure><p id="1cfc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">整个模块可以分为两部分:</p><ul class=""><li id="6e13" class="je jf hh ig b ih ii il im ip jg it jh ix ji jb jj jk jl jm bi translated">通道注意部分首先将全局上下文特征与局部特征的平均池和最大池版本连接起来。该体积然后通过卷积和全连接层，然后再次与本地上下文输入融合以形成信道细化特征。</li><li id="9453" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">空间注意部分将先前的输出与内插的全局输入连接起来，然后处理该体积以产生空间细化特征。最后，这个中间输出被添加到局部和全局上下文特征，以给出输出特征。</li></ul><p id="9344" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">局部和全局特征之间的这种紧密互连允许通过显式地对相邻像素之间的局部关系进行建模来胜过现有技术的方法。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h2 id="3a3e" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">一种全局遮挡感知的自监督单目视觉里程计方法</h2><p id="6a20" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">[10]提出了另一种基于注意力的方法，在这种情况下，用于学习更好的全局上下文，并使用它来产生更精确的遮挡图。处理深度估计中的遮挡的经典方法是计算二进制逐像素掩模，并在优化损失函数时忽略被掩模的像素。然而，这种方法不能在遮挡遮罩中编码全局信息，注意力应该解决这个问题。具体而言，本文还提出了两个关注级别:</p><ul class=""><li id="66c0" class="je jf hh ig b ih ii il im ip jg it jh ix ji jb jj jk jl jm bi translated">级内注意模块被放置在每个解码器级之后，以便适当地融合来自解码器和编码器的特征映射(具有跳跃连接，如通常在U-Net类架构中那样)。</li><li id="a067" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">级联跨级注意用于合并来自不同级内注意模块的信息。该块在每个比例下的输出通过卷积层产生多比例遮挡图，如下所示。</li></ul><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lt"><img src="../Images/ebadd73b77d76e0b2a4c156332b1abcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OTa0fK2FKRBHVgVIYfKfwQ.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">上面输入图片，下面对应遮挡图。相机在左图像和中间图像之间是静态的，因此只有动态对象被标记为遮挡。图片来自[10]。</figcaption></figure><p id="4547" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文的第二个主要贡献是一个对抗性学习方案，其中训练一个鉴别器来发现真实图像和通过视图合成获得的重建图像之间的差异。对于最终的自监督物镜，来自鉴别器的损失函数与更经典的光度和平滑度损失相结合。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h2 id="8995" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">通过实例感知投影一致性学习动态场景中的单目深度</h2><p id="7d9b" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">AAAI 2021对3D重建世界的最后一个有趣贡献是处理自动驾驶中高度动态场景的问题[11]。这个特殊的问题通常由联合估计光流、实例分割或其他任务的复杂框架来解决。这种方法当然也不例外，因为它通过对每个实例进行估计来明确处理对象运动。该框架需要两个图像作为输入，每个图像与一个背景遮罩和一个实例遮罩相关联。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lu"><img src="../Images/2448c5dd29b8db23edef0d9a0b0624f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G6KpW0fmXz6xOTNfqNUHZQ.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">[11]中提出的总体框架。</figcaption></figure><p id="e251" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上面的方案可以看出，三个网络被联合训练:</p><ul class=""><li id="e42f" class="je jf hh ig b ih ii il im ip jg it jh ix ji jb jj jk jl jm bi translated">传统的深度预测器简单地输出两幅图像的深度。</li><li id="dafc" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">姿态网络只接收背景图像(即输入图像乘以背景遮罩)并产生相对姿态。</li><li id="2cc5" class="je jf hh ig b ih jn il jo ip jp it jq ix jr jb jj jk jl jm bi translated">设计了一种新颖的对象网络来预测输入图像中每个对象的运动。该网络的输入由一个模块给出，该模块根据估计的姿态、深度和分割掩模向前和向后投射每个对象，以确保每个对象之间的实例几何一致性。</li></ul><p id="bd0b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我个人想知道是否会有更简单的方法来完成这项任务，因为这种框架的参数数量和整体复杂性使得它们很难与实时要求兼容。此外，为什么不首先使用惊人的<a class="ae jd" href="https://bertabescos.github.io/EmptyCities/" rel="noopener ugc nofollow" target="_blank"> EmptyCities </a>项目来移除动态对象呢？</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h2 id="7fc9" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">参考</h2><p id="fceb" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">[1]徐等.<em class="jc">基于有效共分割和数据增强的自监督多视点立体视觉</em>，2021</p><p id="177d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2]姚等，<em class="jc"> MVSNet:面向非结构化多视点立体的深度推理</em>，2018</p><p id="547b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3] Wong等，<em class="jc">立体认知:用对抗性扰动愚弄立体网络</em>，AAAI 2021</p><p id="4d80" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4]常等，金字塔立体匹配网络，2018</p><p id="7139" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[5]徐等，<em class="jc"> AANet:自适应聚合网络高效立体匹配</em>，2020</p><p id="7f39" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[6] Lyu等人，<em class="jc"> HR-Depth:高分辨率自监督单目深度估计</em>，AAAI 2021</p><p id="2910" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[7] Vasilijevic等人，<em class="jc">用于自监督单目深度估计的3D打包</em>，CVPR 2020</p><p id="843b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[8] Pillai等人，<em class="jc">超深度:自监督超分辨单目深度估计</em>，ICRA 2019</p><p id="ba3a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[9] Lee等，<em class="jc">用于单目深度估计的逐片注意网络</em>，AAAI 2021</p><p id="48b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[10]陆等，<em class="jc">一种全局遮挡感知的自监督单目视觉里程计方法</em>，2021</p><p id="ab78" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[11] Lee等，<em class="jc">通过实例感知投影一致性学习动态场景中的单目深度</em>，AAAI 2021</p></div></div>    
</body>
</html>