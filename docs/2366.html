<html>
<head>
<title>Towards GPU-accelerated image classification on low-end hardware</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向低端硬件的GPU加速图像分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/towards-gpu-accelerated-image-classification-on-low-end-hardware-ec592e125ad9?source=collection_archive---------7-----------------------#2021-04-20">https://medium.com/analytics-vidhya/towards-gpu-accelerated-image-classification-on-low-end-hardware-ec592e125ad9?source=collection_archive---------7-----------------------#2021-04-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e4926bfdd8ca2d544a5a7e40075feec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n5UbbUWXcgPDqIbi9HEJOQ.jpeg"/></div></div></figure><p id="f5e5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi jo translated">深度学习不再是强大的桌面GPU的专属特权。关于神经网络的推断，这是千真万确的:例如，移动GPU正在极大地处理这一点。但是，说到树莓派，它的GPU却莫名其妙地被忽略了。让它作为推理的硬件加速器的选择很少甚至没有。</p><p id="e82d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">拥有<em class="jx"> VideoCore VI </em>图形的Raspberry Pi 4成为一个例外:它更强大，并被<a class="ae jy" href="https://www.khronos.org/conformance/adopters/conformant-products#submission_530" rel="noopener ugc nofollow" target="_blank">认证支持Vulkan </a>，这是计算机图形和GPU通用计算的最新标准，我们开始看到库使用它作为神经网络推理的计算后端，如<a class="ae jy" href="https://github.com/Tencent/ncnn" rel="noopener ugc nofollow" target="_blank"> ncnn </a>。</p><p id="62eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是之前的Pi机型配这个GPU的上一个版本(<em class="jx"> VideoCore IV </em>)呢？由于一些技术原因(下面将讨论其中的一些)，没有框架可以在这个硬件上运行通用神经网络的推理。</p><p id="172b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在当前的文章中，我们通过<strong class="is hj">在10美元的Raspberry Pi Zero W </strong>上启用图像分类器的GPU加速推理来挑战这一点。我们使用GLSL着色器对GPU进行编程，实现了在此硬件上无法实现的吞吐量，而无需借助外部加速器。</p><ul class=""><li id="5701" class="jz ka hi is b it iu ix iy jb kb jf kc jj kd jn ke kf kg kh bi translated">我们首先重用一些MobileNet、ShuffleNet和ResNeXt设计模式来构建一个网络架构，该架构符合在Raspberry Pi上使用着色器所施加的限制。</li><li id="fc9e" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn ke kf kg kh bi translated">我们训练该模型，并由于GLSL的广泛采用，将其推理部署在各种硬件上，从Raspberry Pi开始，继续到桌面GPU，最后到Android智能手机。</li></ul><p id="cca8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在包含狗和猫的图片的ImageNet的具有挑战性的120类子集上训练，我们的225K参数模型在Pi Zero W上达到了<strong class="is hj"> 72% </strong>顶级单作物验证精度，并且由于使用了GPU，吞吐量为每秒670 millons乘加。</p><h1 id="4996" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">构建模型</h1><h2 id="1fa6" class="ll ko hi bd kp lm ln lo kt lp lq lr kx jb ls lt lb jf lu lv lf jj lw lx lj ly bi translated">为什么？</h2><p id="3599" class="pw-post-body-paragraph iq ir hi is b it lz iv iw ix ma iz ja jb mb jd je jf mc jh ji jj md jl jm jn hb bi translated">图像分类可能是视觉中研究得最多的问题。那么，我们为什么不采用现有的训练分类模型，并将其部署在Raspberry Pi上呢？</p><p id="a7b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">很少有人尝试将VideoCore IV GPU用作计算设备的实际原因可能与其性能无关(它可能比CPU更强大，特别是在Pi Zero W上)。这是由它有限的可编程性造成的。例如，它没有官方的OpenCL支持。让我们看一下技术细节，因为我们需要从符合<em class="jx"> OpenGL ES </em>标准的角度进行推理，OpenGL似乎是在较老的树莓上编程GPU的唯一“官方”方式。</p><p id="bb1b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">OpenGL是一种被广泛采用的计算机图形标准。它的不同版本对兼容的GPU应该具备的能力提出了要求，并定义了一个通用接口来访问GPU功能，而不管供应商、平台等。今天，<em class="jx">任何</em> GPU都兼容一个体面版本的OpenGL。</p><p id="0dd7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">也就是说，我们的Pi Zero W GPU仅符合OpenGL ES 2.0，而其继任者VideoCore VI以及任何现代移动GPU都符合OpenGL ES 3.1。这是什么意思？</p><p id="77d3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">OpenGL ES标准通过定义<em class="jx">着色器</em>，小程序并行运行，实现了GPU的可编程性。3.1版本引入了<em class="jx">计算着色器</em>，适用于GPU上的通用计算，并允许浮点输入和输出，而OpenGL ES 2.0仅限于计算机图形中常见的顶点和片段着色器，仅支持定点<em class="jx">纹理</em>。然后，使用计算着色器在每一层的基础上实现神经网络的推理是相当容易的(即使需要<a class="ae jy" href="https://arxiv.org/pdf/1907.01989.pdf" rel="noopener ugc nofollow" target="_blank">细化实现</a>以获得最高性能)。然而，当使用OpenGL ES 2.0时，事情变得困难得多。</p><p id="ae3c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这项工作中，我们遵循我们在<a class="ae jy" rel="noopener" href="/analytics-vidhya/inferring-a-super-resolution-neural-network-on-raspberry-pi-gpu-89b5456d21ef">上一篇文章</a>中建立的关于如何使用GLSL实现神经网络推理的指导方针。我们还详细讨论了OpenGL ES 2.0及其Raspberry Pi实现对神经网络构成的技术约束，以便在Raspberry Pi上运行。这些可以归结为以下几点:</p><ul class=""><li id="6606" class="jz ka hi is b it iu ix iy jb kb jf kc jj kd jn ke kf kg kh bi translated">激活信号存储在RGBA纹理中，颜色通道代表特征图。纹理只能使用每个值8位来存储0…1范围内的值，因此激活值被量化并限制在范围内。</li><li id="b03d" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn ke kf kg kh bi translated">从不同输入获取的样本数量有限，无法产生单一输出值。实际上，我们不能从一个4通道纹理中采样超过25个纹理元素，也不能将超过8个纹理绑定到一个着色器。这限制了2D卷积，卷积神经网络中的主要算子。</li></ul><p id="f9c2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">没有一个主流的分类架构符合这些约束，所以我们必须自己设计一个合适的架构。</p><h2 id="92c9" class="ll ko hi bd kp lm ln lo kt lp lq lr kx jb ls lt lb jf lu lv lf jj lw lx lj ly bi translated">怎么会？</h2><p id="6f36" class="pw-post-body-paragraph iq ir hi is b it lz iv iw ix ma iz ja jb mb jd je jf mc jh ji jj md jl jm jn hb bi translated">可以说，基于梯度下降的优化最棒的事情之一是，可以以任意方式将计算工作流约束在一个模型中，它仍然会学习，希望不会执行得太差。</p><ul class=""><li id="85c7" class="jz ka hi is b it iu ix iy jb kb jf kc jj kd jn ke kf kg kh bi translated">我们使用3×3和1×1(逐点)<em class="jx">组卷积</em>为我们的模型设计了一个<em class="jx">主构建模块</em>，每个输出样本需要很少的输入样本。我们还使用混洗操作来确保激活值在各功能频道间有效共享。然后，构建块实例在模型中重复出现，并与缩减像素采样层相互连接。</li><li id="284e" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn ke kf kg kh bi translated">当把激活信号存储到纹理中时，为了使它们在0…1范围内，我们在隐藏层中应用了一个合适的激活函数(拜拜！)此外，我们使用16位定点表示在线性(密集)层的两个纹理通道中对单个输出值进行编码。</li><li id="d654" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn ke kf kg kh bi translated">我们实际上并没有对激活信号的8位量化做什么特别的事情，而正确的方式可能是求助于<a class="ae jy" href="https://www.tensorflow.org/model_optimization/guide/quantization/training" rel="noopener ugc nofollow" target="_blank">量化感知训练技术</a>。我们根据经验发现，当我们在目标GPU上部署经过训练的网络时，验证准确性不会下降太多。</li></ul><p id="a3e0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们来分析一下。</p><h2 id="4461" class="ll ko hi bd kp lm ln lo kt lp lq lr kx jb ls lt lb jf lu lv lf jj lw lx lj ly bi translated">模型概述</h2><p id="ed18" class="pw-post-body-paragraph iq ir hi is b it lz iv iw ix ma iz ja jb mb jd je jf mc jh ji jj md jl jm jn hb bi translated">在一个120级问题中使用的模型的形状如下。</p><pre class="me mf mg mh fd mi mj mk ml aw mm bi"><span id="aa2c" class="ll ko hi mj b fi mn mo l mp mq">     ╔═══════╦═══════════════════════╦══════════════╦═════════╗<br/>     ║ Stage ║ Layers                ║ Output shape ║ #Params ║<br/>     ║       ║                       ║ (H,W,C)      ║         ║<br/>     ╠═══════╬═══════════════════════╬══════════════╬═════════╣<br/>     ║     1 ║ 5x5 conv + BN + act   ║ 191, 191, 32 ║    2528 ║<br/>     ║       ║ Main block            ║ 191, 191, 32 ║    2432 ║<br/>     ║       ║ Main block            ║ 191, 191, 32 ║    2432 ║<br/>     ║───────║───────────────────────║──────────────║─────────║<br/>     ║     2 ║ 3x3 conv + BN + act   ║   95, 95, 32 ║    2432 ║<br/>     ║       ║ Main block w/ shuffle ║   95, 95, 32 ║    2432 ║<br/>     ║       ║ Main block w/ shuffle ║   95, 95, 32 ║    2432 ║<br/>     ║       ║ Main block w/ shuffle ║   95, 95, 32 ║    2432 ║<br/>     ║───────║───────────────────────║──────────────║─────────║<br/>     ║     3 ║ 3x3 conv + BN + act   ║   47, 47, 64 ║    4864 ║<br/>     ║       ║ Main block w/ shuffle ║   47, 47, 64 ║    4864 ║<br/>     ║       ║ Main block w/ shuffle ║   47, 47, 64 ║    4864 ║<br/>     ║       ║ Main block w/ shuffle ║   47, 47, 64 ║    4864 ║<br/>     ║───────║───────────────────────║──────────────║─────────║<br/>     ║     4 ║ 3x3 conv + BN + act   ║  23, 23, 128 ║    9728 ║<br/>     ║       ║ Main block w/ shuffle ║  23, 23, 128 ║    9728 ║<br/>     ║       ║ Main block w/ shuffle ║  23, 23, 128 ║    9728 ║<br/>     ║       ║ Main block w/ shuffle ║  23, 23, 128 ║    9728 ║<br/>     ║       ║ Main block w/ shuffle ║  23, 23, 128 ║    9728 ║<br/>     ║───────║───────────────────────║──────────────║─────────║<br/>     ║     5 ║ 3x3 conv + BN + act   ║  11, 11, 192 ║   14592 ║<br/>     ║       ║ Main block w/ shuffle ║  11, 11, 192 ║   14592 ║<br/>     ║       ║ Main block w/ shuffle ║  11, 11, 192 ║   14592 ║<br/>     ║       ║ Main block w/ shuffle ║  11, 11, 192 ║   14592 ║<br/>     ║       ║ Main block w/ shuffle ║  11, 11, 192 ║   14592 ║<br/>     ║       ║ Main block w/ shuffle ║  11, 11, 192 ║   14592 ║<br/>     ║───────║───────────────────────║──────────────║─────────║<br/>     ║ Final ║ 3x3 conv + BN + act   ║    5, 5, 192 ║   14592 ║<br/>     ║       ║ 3x3 conv + BN + act   ║    3, 3, 192 ║   14592 ║<br/>     ║       ║ Average pooling       ║    1, 1, 192 ║       0 ║<br/>     ║       ║ Dense + softmax       ║          120 ║   23160 ║<br/>     ╚═══════╩═══════════════════════╩══════════════║─────────║<br/>                                                    ║  225112 ║<br/>                                                    ╚═════════╝</span></pre><p id="4661" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">前五个阶段由下采样块和下面详述的主块的若干次重复组成。下采样块是一个步长为3×3的卷积，后面是一个批处理归一化和一个激活函数，与最初的MobileNet论文<a class="ae jy" href="https://arxiv.org/pdf/1704.04861.pdf" rel="noopener ugc nofollow" target="_blank">中的非常相似。最后阶段使分类器头增加几个卷积/批量范数/激活模块、平均池、密集和softmax层。</a></p><p id="8a68" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">主构建模块是一个<em class="jx">剩余模块</em>变体，其特征是一个<em class="jx">可分离卷积</em>，类似于MobileNet 第一版<a class="ae jy" href="https://arxiv.org/pdf/1704.04861.pdf" rel="noopener ugc nofollow" target="_blank">中使用的主构建模块。尽管如此，还是有一些重要的区别。</a></p><figure class="me mf mg mh fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/e78e7c553f8f0f48be2327c10509d488.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*YXxvk9-2uiHNw-Oy3G90aw.png"/></div><figcaption class="ms mt et er es mu mv bd b be z dx translated">我们的模型主要由受MobileNet v1块启发的这种模式的实例组成。</figcaption></figure><h2 id="e0f0" class="ll ko hi bd kp lm ln lo kt lp lq lr kx jb ls lt lb jf lu lv lf jj lw lx lj ly bi translated">群组卷积和频道洗牌</h2><p id="368f" class="pw-post-body-paragraph iq ir hi is b it lz iv iw ix ma iz ja jb mb jd je jf mc jh ji jj md jl jm jn hb bi translated">当设计主块时，采样约束立即发挥作用:在<em class="jx">宽的</em>块中，即那些处理许多特征图的块中，我们不能使用常规的1x1卷积，因为它们要求每个输出样本有太多的输入样本，因此不能使用着色器有效地实现。一般来说，我们正在设计的计算后端不是点态卷积友好的。为此，我们不使用在MobileNet v2中执行的<em class="jx">反向剩余瓶颈阻塞模式</em>。</p><p id="4ea3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们用<em class="jx">组卷积</em>代替:输入张量在特定数量的<em class="jx">组</em>中的通道维度上切片，每个组计算实际的卷积运算，结果沿着通道轴连接回来。</p><figure class="me mf mg mh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/52454d6d7716635721f0d98f78f246ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*upY-S_wgZjPlgbDI2pU55A.png"/></div></div><figcaption class="ms mt et er es mu mv bd b be z dx translated">常规2D卷积(上图)与2组卷积(下图)的比较。图片来自<a class="ae jy" href="https://arxiv.org/pdf/2006.09791.pdf" rel="noopener ugc nofollow" target="_blank">本文</a>。</figcaption></figure><blockquote class="mx my mz"><p id="70b8" class="iq ir jx is b it iu iv iw ix iy iz ja na jc jd je nb jg jh ji nc jk jl jm jn hb bi translated">分组卷积可以追溯到<a class="ae jy" href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet </a>。后来，<a class="ae jy" href="https://arxiv.org/pdf/1611.05431.pdf" rel="noopener ugc nofollow" target="_blank"> ResNeXt论文</a>表明，虽然AlexNet使用组卷积“作为一种工程折衷”，但与残差块中的常规卷积相比，它们表现出更强的表示能力。最后，<a class="ae jy" href="https://arxiv.org/pdf/1707.01083.pdf" rel="noopener ugc nofollow" target="_blank"> ShuffleNet论文</a>论证了分组可以用信道混洗操作来补充，以确保信息在网络内部的后续块中跨不同组传播。</p></blockquote><p id="17d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这验证了我们构建模块的设计。特别是，在一个着色器中最多同时使用8个四通道纹理采样器，我们最多可以采样32个特征图。因此，在以超过32个特征映射通道操作的块中，我们以4个通道为一组来混洗特征映射。我们在推理中免费得到这种洗牌:没有内存副本，我们只是在着色器程序中将纹理绑定到采样器时改变了顺序。</p><p id="5d00" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们使用的混洗模式是确定性的，并试图最大化32通道块的跨块互连。以下是128通道特征图在第4阶段如何洗牌的示例:</p><pre class="me mf mg mh fd mi mj mk ml aw mm bi"><span id="44f5" class="ll ko hi mj b fi mn mo l mp mq">INPUT       -&gt;&gt; Shuffling -&gt;&gt;      OUTPUT<br/>CHANNEL        (128 to 128)       CHANNEL<br/> 0  ┐ ───────────────┬────────────── ┌  0<br/> 1  │ Texture        │       Texture │  1<br/> 2  │ #0             │            #0 │  2<br/> 3  ┘                │               └  3       Textures order table<br/>                     │                          for 3 iterations<br/> 4  ┐                │               ┌ 32       of 128-chan. shuffle<br/> 5  │ Texture        │       Texture │ 33       ────────────────────<br/> 6  │ #1             │            #8 │ 34       In    #1    #2    #3<br/> 7  ┘                │               └ 35        0     0     0     0<br/>                     │                           1     8     2    16<br/> 8  ┐                │               ┌ 64        2    16     4     1<br/> 9  │ Texture        │       Texture │ 65        3    24     6    17<br/>10  │ #2             │           #16 │ 66        4     1     8     2<br/>11  ┘                │               └ 67        5     9    10    18<br/>                     │                           6    17    12     3<br/>12  ┐                │               ┌ 96        7    25    14    19<br/>13  │ Texture        │       Texture │ 97        8     2    16     4<br/>14  │ #3      ───────┴───────    #24 │ 98        9    10    18    20<br/>15  ┘            accessed            └ 99       10    18    20     5<br/>                in the same                     11    26    22    21<br/>16  ┐         1x1 conv shader        ┌  4       12     3    24     6<br/>17  │ Texture ───────┬─────── Texture│  5       13    11    26    22<br/>18  │ #4             │            #1 │  6       14    19    28     7<br/>19  ┘                │               └  7       15    27    30    23<br/>                     │                          16     4     1     8<br/>20  ┐                │               ┌ 36       17    12     3    24<br/>21  │ Texture        │       Texture │ 37       18    20     5     9<br/>22  │ #5             │            #9 │ 38       19    28     7    25<br/>23  ┘                │               └ 39       20     5     9    10<br/>                     │                          21    13    11    26<br/>24  ┐                │               ┌ 68       22    21    13    11<br/>25  │ Texture        │       Texture │ 69       23    29    15    27<br/>26  │ #6             │           #17 │ 70       24     6    17    12<br/>27  ┘                │               └ 71       25    14    19    28<br/>                     │                          26    22    21    13<br/>28  ┐                │               ┌100       27    30    23    29<br/>29  │ Texture        │       Texture │101       28     7    25    14<br/>30  │ #7             │           #25 │102       29    15    27    30<br/>31  ┘ ───────────────┴────────────── └103       30    23    29    15<br/>                                                31    31    31    31<br/>32  ┐                                ┌  8<br/>33  │ Texture                Texture │  9<br/>34  │ #8                          #2 │ 10<br/>35  ┘                                └ 11<br/> . . .                              . . .</span></pre><p id="101f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，我们在下采样卷积和最后一级卷积中使用8个信道组(与使用深度方向卷积的MobileNet的另一个区别)。背后的基本原理是进一步改善跨渠道维度的信息共享。</p><h2 id="7b8f" class="ll ko hi bd kp lm ln lo kt lp lq lr kx jb ls lt lb jf lu lv lf jj lw lx lj ly bi translated">激活功能</h2><p id="96d8" class="pw-post-body-paragraph iq ir hi is b it lz iv iw ix ma iz ja jb mb jd je jf mc jh ji jj md jl jm jn hb bi translated">为了使用纹理来存储激活信号，我们求助于一个合适的激活函数，将值“压缩”到0…1的范围内。我们测试了几种选择。</p><figure class="me mf mg mh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nd"><img src="../Images/0840d2e244574f36c5bad7aba5f4d9ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OuH5DKHSfT5emFNfkjEJGQ.png"/></div></div><figcaption class="ms mt et er es mu mv bd b be z dx translated">一些经过测试的合适的激活功能具有0…1的输出范围。</figcaption></figure><p id="3c00" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最简单的是ReLU削波到0…1范围。这个<a class="ae jy" rel="noopener" href="/analytics-vidhya/inferring-a-super-resolution-neural-network-on-raspberry-pi-gpu-89b5456d21ef">以前很好地服务于我们，</a>但是在足够深的分类网络中使用时失败了。该模型就是不学习，卡在相对较高的损失函数值。可能的原因是梯度没有很好地通过这个激活函数反向传播，只要它的梯度在0…1范围之外为零。</p><p id="a2de" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另一个选项是MobileNet v2非线性，<em class="jx"> ReLU6 </em>。它的输出范围是0…6，但我们可以很容易地将输出扩展到0…1范围，然后写出到纹理，并在读取时将其缩小。通过这种激活，模型学习得很好，但是在不太广泛的实验中，最终验证准确度达不到70%的验证准确度。</p><p id="f0eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">就最终验证精度而言，性能最好的选项是sigmoid的分段线性近似。我们尝试了一些手动设计的近似器，保留了上图中最好的一个(更好的方法是以一种可学习的方式参数化它)。与原始的sigmoid相比，这种近似在前向和后向计算上都非常有效。此外，其梯度在相对较大的范围内(从-6.5到6.5)是分段恒定的，并且与原始sigmoid函数相比，向其边界“提升”，这消除了该范围内的梯度消失问题。</p><p id="22a3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每次将张量写出到纹理时，即在每次卷积操作之后，批量归一化被融合到卷积核中，并且在可选残差连接被添加到卷积层输出之前，应用激活。</p><h2 id="c367" class="ll ko hi bd kp lm ln lo kt lp lq lr kx jb ls lt lb jf lu lv lf jj lw lx lj ly bi translated">密集(线性)层实现</h2><p id="9b62" class="pw-post-body-paragraph iq ir hi is b it lz iv iw ix ma iz ja jb mb jd je jf mc jh ji jj md jl jm jn hb bi translated">我们的分类器最终得到一个线性层。很常见。</p><p id="4c83" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在推论中，密集层只不过是惯用的矩阵与向量的乘积加上一个恒定的偏置向量，<em class="jx"> Ax+b </em>。我们在GLSL按照多级分块GEMV模式实现了这一点:</p><ul class=""><li id="9af2" class="jz ka hi is b it iu ix iy jb kb jf kc jj kd jn ke kf kg kh bi translated">输入特征向量<strong class="is hj"> <em class="jx"> x </em> </strong>被分割成8个值的块，这些值乘以权重矩阵<strong class="is hj"><em class="jx"/></strong>(在第一<em class="jx">乘法阶段着色器程序</em>中每个线程一个子矩阵)的8*8个子矩阵，</li><li id="f883" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn ke kf kg kh bi translated">然后在随后的<em class="jx">缩减阶段着色器程序</em>中对部分结果求和。最后一个这样的程序也采样并相加以输出常量向量<strong class="is hj"><em class="jx"/></strong><em class="jx">。</em></li></ul><p id="78c5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们没有定点0…1范围的存储(不幸的是，我们有),这种方法实现起来相对简单。</p><p id="d923" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用激活功能将输入信号压缩到0…1范围内实际上不是解决这个问题的唯一选择。另一种方法是使用多个颜色通道来存储单个数值(这甚至是一种常见的做法<a class="ae jy" href="http://marcodiiga.github.io/encoding-normalized-floats-to-rgba8-vectors" rel="noopener ugc nofollow" target="_blank"/><a class="ae jy" href="http://marcodiiga.github.io/encoding-normalized-floats-to-rgba8-vectors" rel="noopener ugc nofollow" target="_blank">e</a><a class="ae jy" href="http://marcodiiga.github.io/encoding-normalized-floats-to-rgba8-vectors" rel="noopener ugc nofollow" target="_blank">或者</a>)。</p><p id="89ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了实现密集层，我们采用了一种使用2个纹理通道来表示单一特征图的技术。也就是说，我们以定点格式覆盖[-128，128]范围，以便一个纹理通道存储小数部分，另一个通道获取整数部分。</p><p id="5e9a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">定点表示的有限范围是它的主要缺点，如果中间算术运算产生的值超出这个范围，可能会导致失败。避免这种情况的替代方法是覆盖整个实轴的<em class="jx">浮点表示法</em>(例如<a class="ae jy" href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format" rel="noopener ugc nofollow" target="_blank"> bfloat16 </a>)。</p><p id="8f76" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，在矩阵乘积计算期间，定点算术不会在加法/乘法中引入任何数值误差(如果一切都在有效范围内)，而在浮点情况下，加法是一个常见的误差源，在归约求和期间会产生严重影响。使用类似bfloat16的数据类型和8位指数的矩阵产品的测试实现变得不太精确。第二，如果密集层在推断中遭受超范围误差，我们可以通过在训练期间调整其输入和权重来减轻这种情况。幸运的是，我们不需要这样做:我们根据经验验证了16位定点表示是可以的，我们基于着色器的分类器达到了非常接近于在桌面GPU上使用普通单精度浮点演算的参考实现的精度。</p><p id="b134" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">这完成了神经网络设计</strong>，考虑了在Raspberry Pi GPU上实现GLSL的可行性。网络消耗包含输入图像的RGB纹理，将其传递给一组执行不同层计算的着色器程序，并输出120长度的logits向量。除了在logits向量上应用的最后一个softmax映射之外，CPU不计算任何东西(即，计算它们的指数、求和并归一化以获得类概率)。虽然CPU有一些管理工作要做:它负责设置着色器程序，绑定纹理和启动光栅化过程。</p><h1 id="c4c9" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">训练模型</h1><p id="df3c" class="pw-post-body-paragraph iq ir hi is b it lz iv iw ix ma iz ja jb mb jd je jf mc jh ji jj md jl jm jn hb bi translated">对于实验，我们在ILSVRC 2012数据集的120类子集上训练模型。我们这样做并不是因为更大的网络不适合Raspberry Pi，而是因为用于训练和实验的GPU资源有限。一个完整的1000级模型很可能可以在Pi上运行:我们的120级模型推理需要不到6 Mb的纹理内存来存储激活信号(但可能需要更多的内存来存储编译后的着色器程序，这很难估计)。</p><p id="fed9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们选择了120个包含狗和猫的图像的类，作为与例如120个类的随机子集相比实际上有意义的事情。然而，这种选择使问题变得更加困难，因为不同类的实例看起来非常相似:对于一个人来说，区分西伯利亚哈士奇和雪橇犬可能比区分雪橇犬和浴缸要困难得多，因为这三者都是原始ImageNet集中的对象类。</p><figure class="me mf mg mh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ne"><img src="../Images/e99679de6e6fd0a494d505e0a791f5dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xdMG9qOKeqMiszwJDG0Qug.jpeg"/></div></div><figcaption class="ms mt et er es mu mv bd b be z dx translated">ILSVRC 2012验证集样本显示狗和为什么这个问题很难。最左边的三张图片不是一个档次的:西伯利亚哈士奇，雪橇犬，加拿大爱斯基摩犬。最右边的图片被误标为雪橇犬，偶然发现。这些图像是其各自版权所有者的财产。</figcaption></figure><p id="69d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该模型使用TensorFlow 2.3中的Keras进行训练。在多种训练方案中，表现最好的是在400个时期达到稳定时以0.01的学习率进行训练，然后每50个时期将学习率减半，直到600个时期。使用了Adam优化器。表现最好的批量标准化动量是0.75(我们处理64个图像的批量)。使用特斯拉T4 GPU进行训练，花费约30分钟完成一个纪元。为了加速这个过程，类sigmoid激活函数已经以TensorFlow 的<a class="ae jy" href="https://github.com/lnstadrum/sigmoid_like_tf_op" rel="noopener ugc nofollow" target="_blank"> C++扩展的形式实现。</a><a class="ae jy" href="https://github.com/lnstadrum/fastaugment" rel="noopener ugc nofollow" target="_blank">快速增强</a>是提高速度和更好的泛化能力的另一个重要因素。训练脚本可在<a class="ae jy" href="https://github.com/lnstadrum/imagenet_dogs_classifier" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><p id="6d90" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在该实验中，该模型实现了最高71.98%的top-1验证准确度。我们发现，平均来自3个检查点的模型权重，在接近全局最大值时达到最高验证精度，会进一步提高0.3%，因此，在被转换为GLSL着色器之前，训练模型<strong class="is hj"> </strong>获得的最大精度为<strong class="is hj"> 72.28% </strong>。</p><h1 id="0724" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">测试</h1><p id="3aa5" class="pw-post-body-paragraph iq ir hi is b it lz iv iw ix ma iz ja jb mb jd je jf mc jh ji jj md jl jm jn hb bi translated">推理实现是在C++中完成的，并使用OpenGL来运行GLSL计算代码。源代码可在<a class="ae jy" href="https://github.com/lnstadrum/beatmup" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><h2 id="59d4" class="ll ko hi bd kp lm ln lo kt lp lq lr kx jb ls lt lb jf lu lv lf jj lw lx lj ly bi translated">单板计算机和桌面GPU</h2><p id="1738" class="pw-post-body-paragraph iq ir hi is b it lz iv iw ix ma iz ja jb mb jd je jf mc jh ji jj md jl jm jn hb bi translated">我们首先在Raspberry Pi Zero W、Pi 3 Model B+、Pi 4、Nvidia Jetson Nano和各种桌面GPU上测试我们的模型。该测试是通过在120类验证集(6000个图像)上运行推理来完成的，采用大小调整为385*385的目标输入分辨率的图像的正方形中心裁剪。</p><figure class="me mf mg mh fd ij"><div class="bz dy l di"><div class="nf ng l"/></div><figcaption class="ms mt et er es mu mv bd b be z dx translated">不同硬件/操作系统上的推理时间和前1名验证准确性。</figcaption></figure><p id="a481" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在所有硬件上，最高验证精度与参考精度的匹配度高达0.2%。精度上的微小差异可以用OpenGL实现的差异来解释，例如片段着色器中对高精度浮点计算的支持。此外，在支持浮点纹理的设备上，我们不在密集层中使用16位定点打包，这恰好是除了两个树莓之外的每个人。</p><p id="ede2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">运行时间从Pi Zero W上的<strong class="is hj"> 595 ms </strong>到NVidia GeForce RTX 2070上的仅几毫秒不等。在测试的两个Raspberry Pi模型之间也有很大不同，即使它们有相同的GPU: Pi 3模型B+每张图像只需要<strong class="is hj"> 328 ms </strong>。这可以通过GPU时钟频率的差异(250 MHz与300 MHz)来解释，并且事实上，CPU反应能力对于保持高效地为GPU提供着色器程序仍然很重要:Pi Zero W运行1 GHz单核ARM v6，其功率只有Pi 3 Model B+板上1.5 GHz四核ARM v8的一小部分。</p><h2 id="7897" class="ll ko hi bd kp lm ln lo kt lp lq lr kx jb ls lt lb jf lu lv lf jj lw lx lj ly bi translated">安卓智能手机</h2><p id="6667" class="pw-post-body-paragraph iq ir hi is b it lz iv iw ix ma iz ja jb mb jd je jf mc jh ji jj md jl jm jn hb bi translated">我们使用相同的技术在Android智能手机上运行我们模型的推理。尽管我们在这里进行了一个更好的测试:我们将摄像机直接插入我们的模型的输入端，并实时预测它捕捉的图像的类别。Android中的相机图像可以作为OpenGL纹理直接访问，这使得我们的实现非常简单。</p><p id="edc3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们通过拍摄计算机屏幕上显示的来自ILSVRC 2012验证集的一些图像，并将预测的类别标签与地面真实标签进行比较，来评估网络性能。我们很少用这种方式测试不同代的Android设备。下面分享了一些显示当前帧的相机预览和预测类别概率的截图。</p><ul class=""><li id="4dc7" class="jz ka hi is b it iu ix iy jb kb jf kc jj kd jn ke kf kg kh bi translated">配备Mali-G72 MP3 GPU的三星Galaxy M31每秒可分类约8幅图像。</li><li id="faf7" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn ke kf kg kh bi translated">采用Mali-T830 MP2 GPU的华为P20 Lite每秒运行约5张图像。</li><li id="a71c" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn ke kf kg kh bi translated">追溯到2014年的搭载PowerVR G6430 GPU的华硕Fonepad 8平板每秒处理~2.3帧。</li></ul><figure class="me mf mg mh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nh"><img src="../Images/35157e66401c4adc9aadbe0e248bbb38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5tWzkl0qSXEhBTyksFA_Uw.jpeg"/></div></div><figcaption class="ms mt et er es mu mv bd b be z dx translated">三星Galaxy M31每秒可分类约8张图像。不幸的是，神经网络无法明确区分哈士奇、雪橇犬和爱斯基摩犬，但最右边的图片可能会被正确分类。</figcaption></figure><figure class="me mf mg mh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nh"><img src="../Images/3964a166d0d42b1de82b861e08e04965.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dcojNpOJgvsNCbwD29k0ZA.jpeg"/></div></div><figcaption class="ms mt et er es mu mv bd b be z dx translated">三星Galaxy M31每秒可分类约8张图像。这些图片上的所有图像都被正确分类:预测的类别与地面事实相符。</figcaption></figure><figure class="me mf mg mh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ni"><img src="../Images/a3c2c584fccfc8feee2fd836fa5a3477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DXq3dypmSYafAt6XNetYSA.jpeg"/></div></div><figcaption class="ms mt et er es mu mv bd b be z dx translated">华为P20 Lite每秒分类约5张图像。实验中使用的数据集实际上包含119类狗和一个“虎猫”类。只是因为手头有一只真猫的测试样本。</figcaption></figure><h1 id="05b0" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">包扎</h1><p id="7cb3" class="pw-post-body-paragraph iq ir hi is b it lz iv iw ix ma iz ja jb mb jd je jf mc jh ji jj md jl jm jn hb bi translated">我们描述了一种在低端设备上实现GPU加速图像分类的实验方法，这同样适用于更强大的硬件。</p><p id="d4a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了让它发挥作用，我们从主流分类神经网络中挑选了一些技巧来设计一种架构，以处理已确定的硬件约束。</p><ul class=""><li id="416c" class="jz ka hi is b it iu ix iy jb kb jf kc jj kd jn ke kf kg kh bi translated">这为我们提供了一个参数高效的架构，在仅具有约225K可训练参数的具有挑战性的120类ILSVRC2012子集上，达到了72%的顶级单作物验证准确率。</li><li id="e81b" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn ke kf kg kh bi translated">该模型执行约400M乘加操作来对385*385像素的图像进行分类；GLSL的实现产生了大约2800万个RGBA纹理元素。</li></ul><p id="e572" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">允许8位定点信号存储的组卷积、混洗和激活函数的使用减少了存储器带宽，使得所设计的架构适合于嵌入式设备。此外，所描述的推理实现技术适用于各种GPU，包括廉价的单板计算机、移动设备、来自不同厂商的集成和分立GPU。</p><p id="aff6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，GLSL着色器和OpenGL带来了巨大的开销，其速度是高效的CUDA或OpenCL实现在相同设备上无法比拟的。例如，次优平铺限制了2D卷积的性能:为了计算每四个输出通道，从输入张量读取整个相应的输入通道组(因此在非组卷积的情况下是整个张量)。这增加了全局内存流量，使实现严重受限于内存I/O，这可能是它的主要缺点。这可以通过计算着色器或多个OpenGL绘制缓冲区来解决，以便将更多的输出通道绑定到单个着色器，但在4 Model B之前的Raspberry Pi上不支持这些功能。由此产生的吞吐量相当适中:具有不错CPU的设备可能会在CPU上运行更大的网络，例如MobileNet，速度更快。但对于Pi Zero W来说，情况肯定不是这样，在Pi Zero W中，有必要以某种方式对GPU进行编程，或者使用外部加速器来运行神经网络的推理。</p></div></div>    
</body>
</html>