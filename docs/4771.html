<html>
<head>
<title>A comprehensive introduction to GNNs — Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GNNs综合介绍—第3部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-comprehensive-introduction-to-gnns-part-3-954f46b61c04?source=collection_archive---------2-----------------------#2022-02-09">https://medium.com/analytics-vidhya/a-comprehensive-introduction-to-gnns-part-3-954f46b61c04?source=collection_archive---------2-----------------------#2022-02-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="a3df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从香草GNN到图形注意力网络(GAT)</p><h1 id="1113" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">在你开始阅读之前</h1><p id="fcdc" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">嗨，亲爱的读者！如果你刚刚开始阅读GNNs的介绍，我鼓励你看看已经发行的第一部<a class="ae kf" rel="noopener" href="/analytics-vidhya/a-comprehensive-introduction-to-gnns-part-1-c07396fa3b91">和第二部<a class="ae kf" rel="noopener" href="/analytics-vidhya/a-comprehensive-introduction-to-gnns-part-2-8d8941a39b44">的第一部</a>和第二部</a>部分，充分享受这份激动人心的新出版物。</p><h1 id="214f" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">接下来是什么？</h1><p id="01f9" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">该系列出版物旨在涵盖以下主题:</p><ul class=""><li id="f12b" class="kg kh hh ig b ih ii il im ip ki it kj ix kk jb kl km kn ko bi translated">图表</li><li id="48a5" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">有向图</li><li id="7af4" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">信息网络</li><li id="a952" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">GNNs背后的动机</li><li id="f3c0" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated"><a class="ae kf" rel="noopener" href="/p/954f46b61c04#1cf6">节点嵌入</a></li><li id="7ce9" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated"><a class="ae kf" rel="noopener" href="/p/954f46b61c04#a0bb">香草GNN </a></li><li id="9c0f" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated"><a class="ae kf" rel="noopener" href="/p/954f46b61c04#3f83">图卷积网络</a></li><li id="165b" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated"><a class="ae kf" rel="noopener" href="/p/954f46b61c04#437c">图形注意力网络(GATs) </a></li><li id="3dd7" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">异构图上的学习</li></ul><p id="6a93" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本部分将从<strong class="ig hi">节点嵌入</strong>的概念开始，以<strong class="ig hi"> GATs </strong>的介绍结束。</p><h1 id="1cf6" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">节点嵌入</h1><p id="d60d" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">上一次我们谈到gnn时，我描述了它们的架构，使用了下图所示的功能组合。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/21410dc74a0f63186d73a1e34c976e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FP9bKLIZtGIZ4nT-gz3vsw.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">被视为功能组合的GNN建筑。</figcaption></figure><p id="a4d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我快速提醒你一下他们的目的！让我们想象一下，我们给一个图结构数据集作为节点分类环境中已经训练好的GNN<strong class="ig hi">的输入。换句话说，我们有一个数据集形成了一个<strong class="ig hi">信息网络</strong>，其中每个节点都有与之相关的特征(一个向量表示),我们希望预测图中一个或几个节点的类别目标。</strong></p><p id="0090" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是图结构数据集通过经过<strong class="ig hi">训练的</strong> GNN时发生的步骤:</p><ul class=""><li id="f739" class="kg kh hh ig b ih ii il im ip ki it kj ix kk jb kl km kn ko bi translated">(<strong class="ig hi"> q </strong>)图中的每个节点通过聚集来自其自身特征以及其邻居特征的信息来更新其向量表示。在这一点上，我们将节点的矢量表示称为<strong class="ig hi">嵌入</strong> ( <strong class="ig hi"> z </strong> ) <strong class="ig hi"> </strong>，因为它们嵌入了关于其周围环境的丰富信息。</li><li id="e611" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">(<strong class="ig hi">f</strong>)<strong class="ig hi">嵌入</strong> ( <strong class="ig hi"> z </strong>)然后被独立地转换，以获得<strong class="ig hi">编码</strong>，这仅仅是嵌入本身的更有意义的(即提炼的)版本。你可以把嵌入比作一个成分列表，把编码比作一个更复杂的配方。既然我们知道GNN <strong class="ig hi">已经被训练，</strong>函数f应该已经仔细学习了如何组合配料，以便得到一个帮助函数g进行良好分类的配方。<strong class="ig hi">注意，这部分是可选的，嵌入可以直接用作下一步的编码。</strong></li><li id="6186" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">(<strong class="ig hi">g</strong>)<strong class="ig hi">编码</strong>最终通过一个函数(例如softmax)得到类预测。</li></ul><p id="3b9c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第一步，即<strong class="ig hi">节点嵌入部分</strong> ( <strong class="ig hi"> q </strong>)，是GNN区别于任何其他前馈神经网络的真正原因。但是节点嵌入实际上是如何创建的呢？我用很少的数字来帮你理解“如何”的部分！</p><p id="1e5f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们考虑下图中的图形，并假设对于每个节点，我们想要创建一个嵌入，该嵌入从2跳距离范围内的邻居捕获信息。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/50450f18c97e4a2d166242548f059daa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UNHzg_WjC2DDRjEGdi4XxA.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">节点嵌入创建概念。</figcaption></figure><p id="2731" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这个例子中，如果我们观察节点1，我们将期望它在节点嵌入过程结束时从图中的所有节点接收信息，因为所有节点都不超过2跳。然而，我们也希望以一种使直接邻居(<strong class="ig hi"> 2 &amp; 5 </strong>)在新节点表示的创建中更有影响力的方式进行。</p><p id="4ee1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，实际进行的方式是通过<strong class="ig hi">将它们自己的向量表示与它们的直接邻居的向量表示聚合</strong>来迭代更新节点状态。在每一步中，我们将节点的矢量表示称为它们的<strong class="ig hi">隐藏状态</strong> ( <strong class="ig hi"> h </strong>)。对于每个节点，初始隐藏状态(<strong class="ig hi"> h(0) </strong>)是具有原始特征的向量，而最后隐藏状态是其嵌入(<strong class="ig hi"> h(K) = z </strong>)。在这种特殊情况下，<strong class="ig hi"> K = 2 </strong>，因为我们希望每个节点都考虑2跳邻域中的邻居。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/72da49e243ae4922430de74f104815ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1D3HI02pnJr0CuVOYdUQfw.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">迭代隐藏状态更新，K=2。</figcaption></figure><p id="1f05" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">按照这个想法，在隐藏状态更新期间，GNN架构将主要不同于它们的聚集方法。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/ec8f37b5fcfe54ff122bdf140693e5a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0HkgILb1DI3x1klfscNX7g.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">被视为黑盒的聚合方法。</figcaption></figure><h1 id="a0bb" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">香草GNN</h1><p id="d14e" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">各位，做好准备，这可能是最棘手的部分。如果你没有掌握这一节的所有内容，这完全没关系。我认为这里提出的每一个概念都是非常通用的，在图卷积网络(GCNs)一节中会更加清晰。</p><p id="3631" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">GNN的概念来自于将图像中常见的<strong class="ig hi">卷积</strong>操作推广到结构化数据的想法。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/a8f12016d59d328f01e34668c4112a8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FBaek0HefENuzmhh2mcLvg.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">应用于图像和图形的卷积的比较。</figcaption></figure><p id="2f1a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它首先引入了函数的概念，带有<strong class="ig hi">可学习参数</strong>，由所有节点共享，以允许它们更新它们的隐藏状态。为了更新节点的隐藏状态，这个被称为<strong class="ig hi">局部转移函数(或消息传递函数)</strong>的函数需要节点自身的当前隐藏状态以及其直接邻居的当前隐藏状态。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/08e0d3095556c86108cde76b369a3fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dafFLn9PgSoMSYIvV3sKBQ.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">局部转移函数的定义。</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/523b659e14069cbfaa488914d3ef31d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D-VdhIgoNStc8B1ZGDURTQ.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">局部转移函数的黑盒可视化。</figcaption></figure><p id="f8b3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">尽管如此，<em class="lk"> Scarselli等人</em>增加了一个标准，即<strong class="ig hi">局部转移函数</strong>必须是一个<strong class="ig hi">收缩</strong>，意味着每次更新任意一对节点的隐藏状态时，它们之间的距离应该更短或相等。这个准则保证了某些收敛性质显示得更远一点。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/d55350005881aad97ad1027a67f46f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PIz6T4kUOoGMkZ2XfpCTgA.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">局部转移函数的收缩准则。</figcaption></figure><p id="882d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第二个引入的概念是一个叫做<strong class="ig hi">全局转换函数</strong>的函数。它是一个带有<strong class="ig hi">可学习参数</strong>的函数，可以一次更新一个图中所有节点的隐藏状态。为此，<strong class="ig hi">全局转换</strong> <strong class="ig hi">函数</strong>需要所有当前隐藏状态和图的<strong class="ig hi">邻接矩阵</strong>。在神经网络的词汇中，我们可以说<strong class="ig hi">全局转移函数</strong>是GNN的<strong class="ig hi">层</strong>。你可以在下图中看到，这基本上是一种在单个步骤中将<strong class="ig hi">局部转换函数</strong>应用到所有节点的有效方法。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/3c2d0d808dcf12424719ec8933fe4fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2OFsyhbyBaw6P4Y9AH0o3g.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">全局转移函数的定义。</figcaption></figure><p id="36f7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于我们之前假设<strong class="ig hi">局部转移函数</strong>是一个收缩，如果我们重复应用<strong class="ig hi">全局转移函数</strong>,<strong class="ig hi">Banach不动点定理</strong>(见下图来源)确保每个隐藏状态都应该收敛到一个固定值。目标是使用这些稳定的隐藏状态作为节点嵌入。然而，在实践中，我们将通过对预定义的迭代次数(<strong class="ig hi"> K </strong>)应用<strong class="ig hi">全局转移函数</strong>来使用这些稳定隐藏状态的估计。注意，应用函数<strong class="ig hi"> K </strong>次将使每个节点嵌入在一个<strong class="ig hi"> K </strong>跳邻域内携带信息。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/9c9ce074d4ade7cde19324eaf75cfe86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M2m1Yd3Qq2kn7HYBeuf4Sw.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">全局转移函数的收敛性。</figcaption></figure><p id="4881" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从<strong class="ig hi">全局转移函数</strong>的这种迭代应用中得到的函数组合将表示普通GNN框架中的函数<strong class="ig hi"> q </strong>。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/1e2c276e6897fb7923ed59dad42c6138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rqzz-Ntt-SekSnalDRU2Eg.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">香草GNN框架。</figcaption></figure><h1 id="3f83" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">图形卷积网络</h1><p id="7ec6" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">在香草GNN的想法之后，图卷积网络(GCNs)提出了两个巨大的变化:</p><ul class=""><li id="8d50" class="kg kh hh ig b ih ii il im ip ki it kj ix kk jb kl km kn ko bi translated">仅执行少量的隐藏状态更新(2–3)，以便节点嵌入仅携带来自邻近区域的信息。</li><li id="6d15" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">对每一层使用不同的参数(<strong class="ig hi"> W </strong>)，以允许各层之间隐藏状态大小的变化。</li></ul><p id="a2bf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下图中，我们可以看到一种更新节点隐藏状态的简单方法。它包括对节点的当前隐藏状态和其直接邻居的当前隐藏状态取一个平均<strong class="ig hi"/>，执行该平均与权重参数矩阵的矩阵乘法(<strong class="ig hi"> W </strong>)，并对结果应用非线性激活函数(<strong class="ig hi">∑</strong>)。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/9b80d643584e25fa31a9cfa54b7c1ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q_ujM4PO9Ajijs-WFBBStQ.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">平均更新隐藏状态的描述。</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/378872c571d2042ce6a690d6ff8eadc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j9rFTOByw5JWVCMbhgZxew.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">局部转移函数的标识。</figcaption></figure><p id="86a7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下图中，我们可以看到示例图中节点1的隐藏状态更新背后的计算步骤。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/f6f8144365975f889bc2be06b4219ca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ClGwZ-oK2VUd_RTFcADS5A.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">用神经网络结构表示的隐藏状态计算。</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/5017f833a4f819b63203e13cecfd536c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mzBjGTT2xyNa-7dDQXcSTA.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">隐藏状态更新的黑盒表示。</figcaption></figure><p id="4a07" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在您可能想知道如何一次有效地更新所有这些隐藏状态。让我马上用详细的插图给你看！下图定义了与我们刚刚探索的简单的<strong class="ig hi">局部转移函数</strong>相关的<strong class="ig hi">全局转移函数</strong>。可能会有一堆令人困惑的数学符号，但我会在下面的<strong class="ig hi"> 4 </strong>图中总结整个概念。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/79ef96884ca74a1f84fbd382ed84bee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0QF_vRjH6O8Ujy2iATuQpA.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">用基本方法定义整体转移函数。</figcaption></figure><p id="edc2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了高效地计算图中节点的隐藏状态，您需要做的第一件事是稍微修改一下原始图！准确地说，您需要添加一条从每个节点到其自身的有向边。这个动作包括在原始邻接矩阵(<strong class="ig hi"> A </strong>)的对角线上添加<strong class="ig hi"> 1 </strong> s，以便得到修改后的邻接矩阵(<strong class="ig hi"> A~ </strong>)。从这个矩阵中，我们可以计算入度矩阵的逆矩阵(<strong class="ig hi"> D~ </strong>)。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/84b500dc45fff184e4c194b0d990536e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dph1HRR7YIoB3tF-sO9tlA.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">向图中添加自连接。</figcaption></figure><p id="d510" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">完成这两个步骤后，我们现在可以考虑下面显示的简单权重矩阵(<strong class="ig hi"> W </strong>)来执行单个隐藏状态更新。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/87637985249a390476d11144a1b929c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLVM8A2A1bdD-EcIt4f1-Q.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">权重矩阵的初始化。</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/f83eaccb6339f495cec8f5e833402116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MJ6VzWr1W2xEX7igrfoAdA.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">括号中的矩阵乘法。</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/a6666e6a5ca4ab5ff3a4636f7ed08378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-hXoXAuK4PMzzU7yO9Q6Jw.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">将ReLU应用于结果。</figcaption></figure><p id="11e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">是不是又好看又轻松？使用修改的邻接矩阵及其关联的入度矩阵将我们从低效的“<strong class="ig hi"> for循环”中解救出来！</strong></p><p id="7b37" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意，还有许多其他简单的方法来聚集节点的隐藏状态。例如<em class="lk"> Kipf和Welling【2017】</em>提出了一种加权平均值，它降低了直接邻居的重要性，而直接邻居本身就有许多直接邻居！</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/124658475b167632120bbcf27fcf55ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j2xcelOdBOHDjS_5swzkGQ.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">GCN主建筑的局部过渡功能。</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/6fa5b10f95d9141930a21a0f6c414f0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1CsCO48m-QVwmMKIiIYXsQ.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">GCN主建筑的整体转换功能。</figcaption></figure><h1 id="437c" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">图形注意网络</h1><p id="9917" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">现在我已经向你展示了GCNs的基础，你知道我们如何修改它们，以便在没有太多额外参数的情况下获得更高的分数吗？</p><p id="a241" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于大多数聚集函数基于隐藏状态的加权平均，图注意力网络(GATs)提出每个节点应该能够学习如何将权重(即重要性)分配给它们的直接邻居。</p><p id="3f21" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如您在下图中所注意到的，我们将根据系数为<strong class="ig hi"> alpha-ij的节点<strong class="ig hi"> i </strong>来记录节点<strong class="ig hi"> j </strong>的重要性。</strong></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/40a7f75fee41ba5e6154aa68cd1ea7cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tGfKDg8U3zrhxlMLOlZwNA.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">注意系数的定义。</figcaption></figure><p id="62e8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了给每个节点提供学习邻居权重的能力，GATs在每一层都使用了注意机制。但是注意力机制实际上是如何工作的呢？</p><p id="e2f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">嗯，让我们假设一个节点<strong class="ig hi"> i </strong>想要了解它的每个邻居的重要性。为此，我们首先需要对邻域中的每个节点<strong class="ig hi"> j </strong>(包括节点<strong class="ig hi"> i </strong>本身)执行以下步骤:</p><ul class=""><li id="62fe" class="kg kh hh ig b ih ii il im ip ki it kj ix kk jb kl km kn ko bi translated">对节点<strong class="ig hi"> i </strong>和节点<strong class="ig hi"> j </strong>隐藏状态应用相同的线性变换。</li><li id="81f8" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">连接两个结果。</li><li id="cfc0" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">用层的<strong class="ig hi">注意力向量</strong> ( <strong class="ig hi"> a </strong>)和连接的结果执行点积。</li><li id="01ea" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">涂抹泄漏的润滑油。</li><li id="e4c1" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">计算结果的指数。</li></ul><p id="cdfb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们将所有系数除以它们的总和，以确保它们都在范围[0，1]内，总和为1。这些系数(alpha-ij)中的每一个现在表示根据节点<strong class="ig hi"> i </strong>的节点<strong class="ig hi"> j </strong>的权重。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/e9f8b692c5ca1f63cf873629544a053d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*emnTRyJGsYMfsVC_35u_Ow.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">用注意机制定义GAT层。</figcaption></figure><p id="d58b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在接下来的图中，您可以看到一个简单示例中所需的计算，其中节点1的隐藏状态被更新。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/f6376d912d8ab944d8fbc951ea2ef33c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gEfCjl3Q3bCeK53mlNbikg.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">注意力机制的可视化(LR = Leaky ReLU)。</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/3dc94a205d2425c9bd2df5ed8dcec59d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GOBkHmr8OoIa7YFVssPrzA.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">隐藏状态更新的可视化。</figcaption></figure><h1 id="fb00" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">在你走之前</h1><p id="18c8" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">感谢您的阅读！欢迎访问我的<a class="ae kf" href="https://www.linkedin.com/in/nicolas-raymond-002950b6/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>页面。</p><h1 id="5d8c" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">特别感谢</h1><p id="94a6" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">我想特别提一下斯坦福大学的<a class="ae kf" href="https://www.youtube.com/watch?v=JAB_plj2rbA" rel="noopener ugc nofollow" target="_blank">cs 224 w:带图的机器学习讲座</a>可以在网上免费获得。他们帮助我理解GNNs，同时也启发我设计一些图形。</p><h1 id="66bb" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">参考</h1><h2 id="539c" class="ll jd hh bd je lm ln lo ji lp lq lr jm ip ls lt jq it lu lv ju ix lw lx jy ly bi translated">文章</h2><ul class=""><li id="0385" class="kg kh hh ig b ih ka il kb ip lz it ma ix mb jb kl km kn ko bi translated">基普夫，托马斯n，和马克斯韦林。"图卷积网络的半监督分类."<em class="lk">ArXiv:1609.02907【Cs，Stat】</em>2017年2月<a class="ae kf" href="http://arxiv.org/abs/1609.02907." rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1609.02907.</a></li><li id="4a04" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">《图形神经网络模型》<em class="lk"> IEEE神经网络汇刊</em>，第20卷第1期，2009年1月，第61–80页，doi:10.1109/TNN.2008.2005605。</li><li id="1ef6" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">一个广义的巴拿赫不动点定理。<em class="lk">马来西亚数学科学学会公报</em>，第39卷第4期，2016年10月，第1529–39页，doi:10.1007/s 40840–015–0255–5。</li><li id="fc20" class="kg kh hh ig b ih kp il kq ip kr it ks ix kt jb kl km kn ko bi translated">《图形注意网络》<em class="lk">ArXiv:1710.10903【Cs，Stat】</em>2018年2月<a class="ae kf" href="http://arxiv.org/abs/1710.10903." rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1710.10903.</a></li></ul><h2 id="9ea1" class="ll jd hh bd je lm ln lo ji lp lq lr jm ip ls lt jq it lu lv ju ix lw lx jy ly bi translated">网站</h2><ul class=""><li id="9e35" class="kg kh hh ig b ih ka il kb ip lz it ma ix mb jb kl km kn ko bi translated"><a class="ae kf" href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" rel="noopener ugc nofollow" target="_blank">* http://deep learning . net/software/the ano/tutorial/conv _算术. html </a></li></ul></div></div>    
</body>
</html>