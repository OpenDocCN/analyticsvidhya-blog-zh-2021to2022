<html>
<head>
<title>Aspect based sentiment analysis on financial news data using classical machine learning algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用经典机器学习算法对财经新闻数据进行基于方面的情感分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/aspect-based-sentiment-analysis-on-financial-news-data-using-classical-machine-learning-algorithms-1ec66447b4cf?source=collection_archive---------1-----------------------#2021-07-28">https://medium.com/analytics-vidhya/aspect-based-sentiment-analysis-on-financial-news-data-using-classical-machine-learning-algorithms-1ec66447b4cf?source=collection_archive---------1-----------------------#2021-07-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="ce4e" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">LinkedIn <a class="ae iw" href="https://www.linkedin.com/in/ankit-shah-643ab3217" rel="noopener ugc nofollow" target="_blank">简介</a> &amp; GitHub <a class="ae iw" href="https://github.com/ankitshah-dev/aspect_based_sentiment_analysis" rel="noopener ugc nofollow" target="_blank">简介</a></h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/6ac7a2a7a96ec4dcf0783dd698094aea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hMvv10n8_H0iN0smmktc0w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">基于方面的情感分析的例子</figcaption></figure><p id="f0c5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在本案例研究中，我们将浏览以下部分:</p><ol class=""><li id="5407" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated">问题的介绍和概述</li><li id="d0d3" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">数据概述</li><li id="0ad0" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">将现实世界的问题映射到最大似然问题</li><li id="29f0" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">电子设计自动化(Electronic Design Automation)</li><li id="d0a1" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">数据预处理</li><li id="4938" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">特征工程</li><li id="ef5a" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">机器学习建模</li></ol></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="1599" class="le lf hh bd lg lh li lj lk ll lm ln lo in lp io lq iq lr ir ls it lt iu lu lv bi translated">1.问题的介绍和概述</h1><h2 id="921f" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">1.1.什么是基于方面的情感分析？</h2><p id="718d" class="pw-post-body-paragraph jn jo hh jp b jq mk ii js jt ml il jv jw mm jy jz ka mn kc kd ke mo kg kh ki ha bi translated">情感分析是自然语言处理中非常流行的技术。我们可以看到它被应用于获取社交网络帖子、电影评论甚至书籍的极性。</p><p id="d2ae" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">然而，基本的情感分析可能会受到限制，因为我们在诱发主题方面缺乏精确性。</strong></p><p id="aa9d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们以一台电脑的评论为例:我们如何知道什么是好的/坏的？是键盘，屏幕，处理器？</p><p id="74aa" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">基于方面的情感分析方法直接解决了这个限制。在2014年SemEval年度比赛期间推出，ABSA旨在寻找术语提到的方面，并给出相关的情感分数。</p><p id="6fcd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">回到我们的计算机例子，在下面的评论中:</p><ul class=""><li id="2866" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki mp kp kq kr bi translated"><em class="mq">“我绝对喜欢这款明亮的视网膜屏幕”</em></li><li id="474c" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated"><em class="mq">“蝴蝶键盘骗人！”</em></li></ul><p id="7fe5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">有了ABSA，我们会对屏幕产生积极的情绪，对键盘产生消极的情绪。从商业角度来看，这更可行。</p><h2 id="0db2" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">1.2.问题陈述</h2><p id="7be7" class="pw-post-body-paragraph jn jo hh jp b jq mk ii js jt ml il jv jw mm jy jz ka mn kc kd ke mo kg kh ki ha bi translated">本帖将在财经新闻标题数据集上用python实现<strong class="jp hi"> ABSA任务。</strong></p><h2 id="9c80" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">1.3.概观</h2><p id="2ce6" class="pw-post-body-paragraph jn jo hh jp b jq mk ii js jt ml il jv jw mm jy jz ka mn kc kd ke mo kg kh ki ha bi translated">对于<em class="mq">域外</em>用例，ABSA模型可以分解为三个不同的主要流程:</p><ul class=""><li id="5917" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki mp kp kq kr bi translated">学习方面类别(<code class="du mr ms mt mu b">SCREEN$Quality</code>、<code class="du mr ms mt mu b">PROCESSOR$Performance</code>等)。)</li><li id="e400" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated">在句子中寻找体词(<code class="du mr ms mt mu b">retina screen</code>、<code class="du mr ms mt mu b">butterfly keyboard</code>等)。)并分类到方面类别</li><li id="103c" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated">计算每个条目中每个方面的极性</li></ul><p id="0d17" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">然而，我们的财务用例是我们所谓的<em class="mq">域内</em>。也就是说，我们已经定义了与财经新闻相关的方面类别。然后我们有一个由两个不同的模型组成的模型:方面类别分类器和情感回归模型。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mv"><img src="../Images/83ff787c46c8c875e0127bd3060b9c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E78Ofdl7IC7Eaok8rI_UbA.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">ABSA高层工作流</figcaption></figure></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="c14f" class="le lf hh bd lg lh li lj lk ll lm ln lo in lp io lq iq lr ir ls it lt iu lu lv bi translated">2.数据概述</h1><p id="4cc9" class="pw-post-body-paragraph jn jo hh jp b jq mk ii js jt ml il jv jw mm jy jz ka mn kc kd ke mo kg kh ki ha bi translated"><a class="ae iw" href="https://drive.google.com/file/d/1icRTdnu8UcWyDIXtzpsYc2Hm6ACHT-Ch/view" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hi"> <em class="mq"> FiQA任务1数据集【18】</em></strong></a>包含关于基于方面的情感分析信息，该信息关于从金融领域网页如Wikinews、Stocktwits和Reddit提取的帖子和新闻标题。有435条带注释的标题和675条带注释的财经推文，向每个目标提供了特征和情感分数</p><h2 id="7aac" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">2.1.示例数据点</h2><p id="b18e" class="pw-post-body-paragraph jn jo hh jp b jq mk ii js jt ml il jv jw mm jy jz ka mn kc kd ke mo kg kh ki ha bi translated"><em class="mq"> "55": { "sentence": "Tesco放弃在Blinkbox销售中的视频流野心"，" info": [ { "snippets": "['视频流野心']"，" target": "Blinkbox "，" opinion _ score ":"-0.195 "，" aspects": "['企业/战略']" }，{ "snippets": "['Tesco放弃视频流野心']，" target": "Tesco "，" opinion _ score ":":"-0.335 "，" aspects": "['企业/战略']]</em></p><h2 id="dd15" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">2.2.数据字段</h2><ul class=""><li id="37ec" class="kj kk hh jp b jq mk jt ml jw mw ka mx ke my ki mp kp kq kr bi translated"><strong class="jp hi"> id: </strong>帖子id</li><li id="4cef" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated"><strong class="jp hi">句子:</strong>帖子正文</li><li id="b892" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated"><strong class="jp hi">目标:</strong>接收意见的实体</li><li id="6c6e" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated"><strong class="jp hi">情感得分:</strong>标题的真实情感得分(范围从-1到1)</li><li id="e108" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated"><strong class="jp hi"> aspect_category(1，2，3): </strong>财务方面由不同的节点级别表示。对于不同的目标，同一个句子可以有一个或多个方面。想法是将第二层节点信息表示为方面输出，以训练/测试模型</li><li id="ea9a" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated"><strong class="jp hi"> aspect_snippet(1，2和3): </strong> aspect文本片段，它提供了更多的信息来识别aspect类别。同一个句子可以有一个或多个针对不同目标的片段，也可以没有</li></ul></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="093f" class="le lf hh bd lg lh li lj lk ll lm ln lo in lp io lq iq lr ir ls it lt iu lu lv bi translated">3.将现实世界的问题映射到最大似然问题</h1><h2 id="0ec8" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">3.1.机器学习问题的类型</h2><p id="b4c6" class="pw-post-body-paragraph jn jo hh jp b jq mk ii js jt ml il jv jw mm jy jz ka mn kc kd ke mo kg kh ki ha bi translated">这个问题有三个方面</p><ol class=""><li id="d68a" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated">二元分类</li><li id="f8e9" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">多类分类</li><li id="c2f4" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">回归</li></ol><p id="ca9f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">一旦我们到达ML建模部分，我们将会看到每一个</p><h2 id="ca4e" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">3.2.评估指标</h2><ol class=""><li id="5ce4" class="kj kk hh jp b jq mk jt ml jw mw ka mx ke my ki ko kp kq kr bi translated">二元分类:<strong class="jp hi"> <em class="mq"> roc_auc，混淆矩阵</em> </strong></li><li id="01ea" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">多类分类:<strong class="jp hi">微观f1得分</strong></li><li id="378a" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">回归:<strong class="jp hi">均方误差</strong></li></ol><p id="515f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我将避免详细讨论上面的每一个评估指标，否则这篇文章会太长而无法阅读。</p></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="213c" class="le lf hh bd lg lh li lj lk ll lm ln lo in lp io lq iq lr ir ls it lt iu lu lv bi translated">4.探索性数据分析</h1><p id="22dc" class="pw-post-body-paragraph jn jo hh jp b jq mk ii js jt ml il jv jw mm jy jz ka mn kc kd ke mo kg kh ki ha bi translated">我刚刚在这里添加了所需的代码片段。您应该可以找到我的GitHub repo的链接，了解详细的代码实现。</p><h2 id="d0c5" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">4.1.首先让我们导入所有必要的库</h2><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="d537" class="lw lf hh mu b fi nd ne l nf ng"># importing basic libraries</span><span id="95dc" class="lw lf hh mu b fi nh ne l nf ng">import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>import json<br/>import re<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>import os<br/>import csv<br/>from datetime import datetime<br/>from wordcloud import WordCloud<br/>from nltk.corpus import stopwords</span></pre><h2 id="6448" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">4.2.为简单起见，将所有内容放入熊猫数据框中</h2><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="02ad" class="lw lf hh mu b fi nd ne l nf ng"># putting everything in a dataframe<br/>df = pd.DataFrame({"aspects":aspects_li, <br/>                      "sentiments":sentiments_li, <br/>                      "snippets":snippets_li, <br/>                      "target":target_li, <br/>                      "sentence":sentence_li})</span><span id="ad7c" class="lw lf hh mu b fi nh ne l nf ng">print(df.shape) #(1111,5)<br/>df.head()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ni"><img src="../Images/2c8f75c2c54c36befec69fa138616c4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_7igLyQxw036OUQfXMMxnQ.png"/></div></div></figure><h2 id="491f" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">4.3.方面的分布</h2><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="f33f" class="lw lf hh mu b fi nd ne l nf ng"># using CountVectorizer to get unique aspects<br/>vectorizer = CountVectorizer()<br/>aspects_dtm = vectorizer.fit_transform(df.aspects)</span><span id="5489" class="lw lf hh mu b fi nh ne l nf ng">print("Number of data points :", aspects_dtm.shape[0])<br/>print("Number of unique aspects :", aspects_dtm.shape[1])</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nj"><img src="../Images/bf86d5cab0958f6df714adb6f242a57f.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*EZwOyy3yTw2I-Ml5W4lqhw.png"/></div></figure><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="3117" class="lw lf hh mu b fi nd ne l nf ng">#'get_feature_name()' gives us the vocabulary.<br/>aspects = vectorizer.get_feature_names()</span><span id="1427" class="lw lf hh mu b fi nh ne l nf ng">#Lets look at the tags we have.<br/>print("Some of the aspects we have :", aspects[:10])</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nk"><img src="../Images/6292676dddca14fe4f1799f9f3b7e841.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kquhFdMigqSlcNG2oTPtNA.png"/></div></div></figure><h2 id="16c8" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">4.4.每个方面出现的次数</h2><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="74d0" class="lw lf hh mu b fi nd ne l nf ng">#Lets now store the document term matrix in a dictionary.<br/>freqs = aspects_dtm.sum(axis=0).A1<br/>result = dict(zip(aspects, freqs))</span><span id="9353" class="lw lf hh mu b fi nh ne l nf ng">#Saving this dictionary to csv files.<br/>if not os.path.isfile('../01. Data/aspect_counts_dict_dtm.csv'):<br/>    with open('aspect_counts_dict_dtm.csv', 'w') as csv_file:<br/>        writer = csv.writer(csv_file)<br/>        for key, value in result.items():<br/>            writer.writerow([key, value])<br/>aspects_df = pd.read_csv("aspect_counts_dict_dtm.csv", names=['aspects', 'Counts'])<br/>aspects_df.head(2)</span><span id="eccc" class="lw lf hh mu b fi nh ne l nf ng">#Sorting aspects in descending order of their counts<br/>aspects_df_sorted = aspects_df.sort_values(['Counts'], ascending=False)<br/>aspects_counts = aspects_df_sorted['Counts'].values</span><span id="4da4" class="lw lf hh mu b fi nh ne l nf ng">#Plotting the aspects<br/>plt.plot(aspects_counts)<br/>plt.title("Distribution of number of times aspects appeared")<br/>plt.grid()<br/>plt.xlabel("aspects number")<br/>plt.ylabel("Number of times aspects appeared")<br/>plt.show()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nl"><img src="../Images/a04ce3f0fe82730737615e816fc48144.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g49lxNCkjM9TRlDzRkTjcQ.png"/></div></div></figure><p id="d00b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">正如我们在上面的图中看到的，分布是高度偏斜的</strong></p><ol class=""><li id="7ec1" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated">索引为0的一个方面在1111行中出现了600次以上</li><li id="fe23" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">大多数方面只出现几次</li><li id="fb77" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">由于一些标签比其他标签出现得更频繁，因此微平均F1分数是解决此问题的合适指标</li></ol><h2 id="f3cc" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">4.5.每个项目的方面(行)</h2><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="6447" class="lw lf hh mu b fi nd ne l nf ng">#Storing the count of aspects in each question in list 'aspects_count'<br/>aspects_quest_count = aspects_dtm.sum(axis=1).tolist()</span><span id="2a87" class="lw lf hh mu b fi nh ne l nf ng">#Converting list of lists into single list, we will get [[3], [4], [2], [2], [3]] and we are converting this to [3, 4, 2, 2, 3]<br/>aspects_quest_count=[int(j) for i in aspects_quest_count for j in i]<br/>print ('We have total {}  datapoints.'.format(len(aspects_quest_count)))</span><span id="b5cf" class="lw lf hh mu b fi nh ne l nf ng">print(aspects_quest_count[:5])</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nm"><img src="../Images/275b3ef862a803b1716582974017f5f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*IEZvkyXIWFvoJnv111_9TQ.png"/></div></figure><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="0ac1" class="lw lf hh mu b fi nd ne l nf ng">print( "Maximum number of aspects per question: %d"%max(aspects_quest_count))<br/>print( "Minimum number of aspects per question: %d"%min(aspects_quest_count))<br/>print( "Avg. number of aspects per question: %f"% ((sum(aspects_quest_count)*1.0)/len(aspects_quest_count)))</span><span id="fedf" class="lw lf hh mu b fi nh ne l nf ng"># plotting the counts<br/>sns.countplot(aspects_quest_count, palette='gist_rainbow')<br/>plt.title("Number of aspects in the items/rows ")<br/>plt.xlabel("Number of aspects")<br/>plt.ylabel("Number of items")<br/>plt.show()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nn"><img src="../Images/6071e865217e3c26c7fd194cd39f6c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*BBP5mFG0nUzk5y944lzlyg.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es no"><img src="../Images/8362cbe027ce575ab63008a9e0b1ee68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YiMTzxrXwfw8XcysrP7oQw.png"/></div></div></figure><p id="8198" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">对上述图的观察:</strong></p><ol class=""><li id="70d1" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated">每个问题的最大方面数:8</li><li id="4f81" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">每个问题的最小方面数:1</li><li id="7c55" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">平均值。每个问题的方面数:3.649865</li><li id="7457" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">根据上述计数图，大多数行数/数据点具有3或4个方面</li></ol><h2 id="f484" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">4.6.让我们看看情感专栏</h2><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="7cc3" class="lw lf hh mu b fi nd ne l nf ng">#distribution of sentiments<br/>sns.distplot(df.sentiments)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es np"><img src="../Images/eefe1b92eb830ae6b4873e0ab5b711b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*qP7DbHO4iEA2zR_boE56bw.png"/></div></figure><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="14a7" class="lw lf hh mu b fi nd ne l nf ng"># general sentiment metrics<br/>print("max sentiment: {} | min sentiment: {} | average sentiment: {}".format(np.max(df.sentiments), np.min(df.sentiments), np.mean(df.sentiments)))</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nq"><img src="../Images/61d8e625b076bebcac4595b770a0636f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*arOVJjLUysbSp9S1kRrWsg.png"/></div></figure><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="e209" class="lw lf hh mu b fi nd ne l nf ng"># lets bucket the sentiments to +ve, -ve and neutral<br/>sentiments_bucket = []<br/>error_bucket = []<br/>for elem in np.array(df.sentiments):<br/>    try:<br/>        if elem &gt; 0:<br/>            sentiments_bucket.append("positive")<br/>        elif elem &lt; 0:<br/>            sentiments_bucket.append("negative")<br/>        else:<br/>            sentiments_bucket.append("neutral")<br/>    except:<br/>        error_bucket.append(elem)</span><span id="847b" class="lw lf hh mu b fi nh ne l nf ng"># lets get the counts for these<br/>sns.countplot(sentiments_bucket, palette='hls')<br/>plt.title("Sentiment counts ")<br/>plt.xlabel("sentiments")<br/>plt.ylabel("counts")<br/>plt.show()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nr"><img src="../Images/1f8dc56e62175b68708e96c7278ec7af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*tqcjqDKZ8pY05umK8vB_9w.png"/></div></figure><p id="51a2" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">关于EDA的最终想法:</strong></p><ol class=""><li id="cb37" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated">由于每个项目都有多个方面，我们将继续选择2级方面，这样每个项目就有一个方面，这样更容易分类。</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ns"><img src="../Images/b6d16b2be4f95ee75a8c9e4990fd936b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*swxI8FQ3c3ZmJ-NkMXGUcg.png"/></div></figure><p id="39e9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">2.为简单起见，我们将从较大的数据框架中抽取2列<strong class="jp hi">(“句子”、“目标”)</strong>用于我们的<strong class="jp hi">二进制和多类分类问题</strong>以及<strong class="jp hi">(“句子”、“目标”、“情感”)用于我们的回归问题</strong></p></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="00c2" class="le lf hh bd lg lh li lj lk ll lm ln lo in lp io lq iq lr ir ls it lt iu lu lv bi translated">5.数据预处理</h1><h2 id="6983" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">5.1.清理数据集:“句子”列</h2><p id="6141" class="pw-post-body-paragraph jn jo hh jp b jq mk ii js jt ml il jv jw mm jy jz ka mn kc kd ke mo kg kh ki ha bi translated">在这里，我们将删除标点符号，html标签，@提及，#标签，停用词，数字和不需要的空格</p><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="a345" class="lw lf hh mu b fi nd ne l nf ng">def remove_punctuation(s):<br/>    list_punctuation = list(punctuation)<br/>    for i in list_punctuation:<br/>        s = s.replace(i,' ')<br/>    return s</span><span id="abfe" class="lw lf hh mu b fi nh ne l nf ng">def clean_sentence(sentence):<br/>    sentence = sentence.lower()<br/>    #remove multiple repeat non num-aplha char !!!!!!!!!--&gt;!<br/>    sentence = re.sub(r'(\W)\1{2,}', r'\1', sentence) <br/>    #removes alpha char repeating more than twice aaaa-&gt;aa<br/>    sentence = re.sub(r'(\w)\1{2,}', r'\1\1', sentence)<br/>    #removes links<br/>    sentence = re.sub(r'(?P&lt;url&gt;https?://[^\s]+)', r'', sentence)<br/>    # remove <a class="ae iw" href="http://twitter.com/usernames" rel="noopener ugc nofollow" target="_blank">@usernames</a><br/>    sentence = re.sub(r"\@(\w+)", "", sentence)<br/>    #remove # from #tags<br/>    sentence = sentence.replace('#','')<br/>    sentence = sentence.replace("'s",'')<br/>    sentence = sentence.replace("-",' ')<br/>    # split into tokens by white space<br/>    tokens = sentence.split()<br/>    # remove punctuation from each token<br/>    tokens = [remove_punctuation(w) for w in tokens]<br/>    # filter out stop words<br/>    stop_words = set(stopwords.words('english'))<br/>    tokens = [w for w in tokens if not w in stop_words]<br/>    remove_digits = str.maketrans('', '', digits)<br/>    tokens = [w.translate(remove_digits) for w in tokens]<br/>    tokens = [w.strip() for w in tokens]<br/>    tokens = [w for w in tokens if w!=""]<br/>    tokens = ' '.join(tokens)<br/>    return tokens</span><span id="11e2" class="lw lf hh mu b fi nh ne l nf ng">#Cleaned sentence column<br/>df["cleaned_sentence"] = [clean_sentence(x) for x in df.sentence]<br/>df.drop(["sentence"], axis=1, inplace=True)<br/>df.head(2)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nt"><img src="../Images/36c71fb5064efc80c33bab883c9e263d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*S7ZAqeK0SEMTDRAc7KCm2w.png"/></div></figure><h2 id="f482" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">5.2.为分类方面创建分类数据集</h2><p id="ff8e" class="pw-post-body-paragraph jn jo hh jp b jq mk ii js jt ml il jv jw mm jy jz ka mn kc kd ke mo kg kh ki ha bi translated">如上所述，我们将使用二元分类和多元分类。</p><p id="f7a0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">原因是职业高度不均衡，如EDA部分所示。一个类“价格行为”通过出现约60%的次数来支配数据集，因此为了更好的结果进行了分割。</p><p id="6356" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">您可以应用上采样技术来平衡数据集，但这里的问题只是针对一个类，而不是其他类。因此，分类问题有两个模型</p><ul class=""><li id="bded" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki mp kp kq kr bi translated">二元分类—针对最大的类别(价格行为)与其他类别</li><li id="536b" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated">多类分类—其他</li></ul><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="9808" class="lw lf hh mu b fi nd ne l nf ng"># binary dataset<br/>df_binary = df.copy(deep=True)<br/>df_binary["target_binary"] = [1 if x == "Price Action" else 0 for x in df_binary.target]<br/>df_binary.drop(["target"], axis=1, inplace=True)<br/>df_binary.head(2)</span><span id="2edf" class="lw lf hh mu b fi nh ne l nf ng"># multiclass dataset<br/>df_multi = df[df.target != "Price Action"]<br/>df_multi.head(2)</span></pre><div class="iy iz ja jb fd ab cb"><figure class="nu jc nv nw nx ny nz paragraph-image"><img src="../Images/59eb6fa939713d57095ec2620dd75899.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*FooTxyk62eSTVkraAVTMOQ.png"/></figure><figure class="nu jc oa nw nx ny nz paragraph-image"><img src="../Images/a8eb5690787f8da527d50ba93ede87ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*J70Kuhm8FNWTSrJJ4Lct7Q.png"/><figcaption class="jj jk et er es jl jm bd b be z dx ob di oc od translated">左:二元分类数据集，右:多类分类数据集</figcaption></figure></div><h2 id="3e12" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">5.3.创建用于预测情绪的回归数据集</h2><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="33f7" class="lw lf hh mu b fi nd ne l nf ng"># read train data<br/>df = pd.read_csv("/content/drive/MyDrive/datasets/case study1: aspect based sentiment analysis/01.data/train_final.csv")<br/>print(df.shape)<br/>df.drop(["snippets", "combined", "news"], axis=1, inplace=True)</span><span id="2d9f" class="lw lf hh mu b fi nh ne l nf ng"># read train data<br/>df_val = pd.read_csv("/content/drive/MyDrive/datasets/case study1: aspect based sentiment analysis/01.data/val.csv")<br/>df_sentiment = pd.concat([df, df_val])<br/>df_sentiment.head()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es no"><img src="../Images/e84885700a162a5864358fe68af3fdc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CNtqZgykYGlEnyUfRNEqog.png"/></div></div></figure><h2 id="1d1f" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">5.4.列车测试分离</h2><p id="a52b" class="pw-post-body-paragraph jn jo hh jp b jq mk ii js jt ml il jv jw mm jy jz ka mn kc kd ke mo kg kh ki ha bi translated">我们将以80:20的比例分割数据。80%用于培训，其余20%用于测试</p><blockquote class="oe of og"><p id="a2d1" class="jn jo mq jp b jq jr ii js jt ju il jv oh jx jy jz oi kb kc kd oj kf kg kh ki ha bi translated">以下表示法适用于二元分类数据集，我们也将对多类分类和回归问题进行同样的处理</p></blockquote><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="230f" class="lw lf hh mu b fi nd ne l nf ng"># train test split<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)<br/>print(X_train.shape, X_test.shape)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ok"><img src="../Images/8ec378479b2ac0a1a74484363f6dfce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*BhviCt_Ykvu3jFPRkuGmCg.png"/></div></figure></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="2f4a" class="le lf hh bd lg lh li lj lk ll lm ln lo in lp io lq iq lr ir ls it lt iu lu lv bi translated">6.特征工程</h1><h2 id="8d05" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">6.1.使用通用句子编码器从“句子”栏生成512维特征</h2><p id="d332" class="pw-post-body-paragraph jn jo hh jp b jq mk ii js jt ml il jv jw mm jy jz ka mn kc kd ke mo kg kh ki ha bi translated">简而言之:通用句子编码器(USE)接受文本作为输入，并生成512维向量，这些向量可进一步用于我们的下游任务，如分类。</p><p id="5520" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">详细的使用文档:<a class="ae iw" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/universal-sentence-encoder/4</a></p><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="7ae5" class="lw lf hh mu b fi nd ne l nf ng"># encoding using Universal sentence encoder<br/># using universal sentence encoder for text embedding<br/>import tensorflow_hub as hub<br/>embed = hub.load("<a class="ae iw" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/universal-sentence-encoder/4</a>")</span><span id="20e9" class="lw lf hh mu b fi nh ne l nf ng">embeddings_train = embed(X_train.reshape(1,-1)[0])<br/>embeddings_test = embed(X_test.reshape(1,-1)[0])<br/>embeddings_train.shape, embeddings_test.shape</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ol"><img src="../Images/40751d1499dbce46981296b0600c1a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*hyv9ZmlUIFA00Sdt1DCDUw.png"/></div></figure></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="dc39" class="le lf hh bd lg lh li lj lk ll lm ln lo in lp io lq iq lr ir ls it lt iu lu lv bi translated">7.机器学习建模</h1><p id="6da8" class="pw-post-body-paragraph jn jo hh jp b jq mk ii js jt ml il jv jw mm jy jz ka mn kc kd ke mo kg kh ki ha bi translated">你可以查看我的GitHub repo的所有模型实验。在这里，我将展示二元分类、多类分类和回归问题的最佳模型结果</p><h2 id="38b6" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">7.1.二元分类模型</h2><h2 id="a499" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">7.1.1.过采样以平衡类别</h2><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="eb1d" class="lw lf hh mu b fi nd ne l nf ng"># oversampling<br/>from imblearn.over_sampling import RandomOverSampler<br/># define oversampling strategy<br/>oversample = RandomOverSampler(sampling_strategy=0.8)</span><span id="ac18" class="lw lf hh mu b fi nh ne l nf ng"># fit and apply the transform<br/>X, y = oversample.fit_resample(df_binary.drop(["target_binary"], 1), df_binary.drop(["cleaned_sentence"], 1))</span><span id="2a24" class="lw lf hh mu b fi nh ne l nf ng"># plotting to check the difference<br/>sns.countplot(y)<br/>plt.title("Balanced data")<br/>plt.show()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es om"><img src="../Images/a2ea40e6cadc6417f694146d1ffef68c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qnhu5BWuwnfp7xQUoV9_3Q.png"/></div></div></figure><h2 id="d7de" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">7.1.2.具有通用语句编码器特征的KNN</h2><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="d128" class="lw lf hh mu b fi nd ne l nf ng"># <a class="ae iw" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve</a><br/>from sklearn.metrics import roc_curve, auc<br/>embeddings_train = np.array(embeddings_train)<br/>embeddings_test = np.array(embeddings_test)</span><span id="145a" class="lw lf hh mu b fi nh ne l nf ng"># we will use the above parameters and add regularization to avoid overfitting<br/>mnb = clf.best_estimator_<br/>mnb.fit(embeddings_train, y_train)</span><span id="7827" class="lw lf hh mu b fi nh ne l nf ng"># roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class<br/># not the predicted outputs</span><span id="7af2" class="lw lf hh mu b fi nh ne l nf ng">y_train_pred = mnb.predict_proba(embeddings_train) <br/>y_test_pred = mnb.predict_proba(embeddings_test)</span><span id="a662" class="lw lf hh mu b fi nh ne l nf ng">train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred[:, 1])<br/>test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred[:, 1])</span><span id="0db6" class="lw lf hh mu b fi nh ne l nf ng">plt.plot(train_fpr, train_tpr, label="train AUC ="+str(auc(train_fpr, train_tpr)))<br/>plt.plot(test_fpr, test_tpr, label="test AUC ="+str(auc(test_fpr, test_tpr)))<br/>plt.legend()<br/>plt.xlabel("FPR")<br/>plt.ylabel("TPR")<br/>plt.title("ROC Curve")<br/>plt.grid()<br/>plt.show()</span><span id="a542" class="lw lf hh mu b fi nh ne l nf ng"># confusion matrix<br/>from sklearn.metrics import confusion_matrix<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt   <br/>cm = confusion_matrix(y_test, mnb.predict(embeddings_test))<br/>sns.heatmap(cm, annot=True,fmt="d",cmap='Blues')<br/>plt.xlabel("Predicted")<br/>plt.ylabel("Actual")<br/>plt.title("Confusion Matrix")</span></pre><div class="iy iz ja jb fd ab cb"><figure class="nu jc on nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/d22ec6d225aa07ca6f7a8fa4197af384.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*8MzsK8cdH9QBYNBR1OCn3w.png"/></div></figure><figure class="nu jc oo nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/c477a15a2d259ba58e144b41bf374d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*Mw-s_EVBMLN_E-2l-5ym4w.png"/></div></figure></div><h2 id="d7c2" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">7.2.多类分类模型</h2><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="2021" class="lw lf hh mu b fi nd ne l nf ng">from sklearn.metrics import f1_score, log_loss, confusion_matrix<br/>from sklearn.calibration import CalibratedClassifierCV</span><span id="ac1c" class="lw lf hh mu b fi nh ne l nf ng">alpha = [10 ** x for x in range(-5, 4)]<br/>cv_log_error_array=[]<br/>for i in alpha:<br/>    logisticR=LogisticRegression(penalty='l2',C=i,)<br/>    logisticR.fit(X_train,y_train)<br/>    sig_clf = CalibratedClassifierCV(logisticR, method="sigmoid")<br/>    sig_clf.fit(X_train, y_train)<br/>    predict_y = sig_clf.predict_proba(X_cv)<br/>    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=logisticR.classes_, eps=1e-15))<br/>    <br/>for i in range(len(cv_log_error_array)):<br/>    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])</span><span id="7c0c" class="lw lf hh mu b fi nh ne l nf ng">best_alpha = np.argmin(cv_log_error_array)<br/>    <br/>fig, ax = plt.subplots()<br/>ax.plot(alpha, cv_log_error_array,c='g')<br/>for i, txt in enumerate(np.round(cv_log_error_array,3)):<br/>    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))<br/>plt.grid()<br/>plt.title("Cross Validation Error for each alpha")<br/>plt.xlabel("Alpha i's")<br/>plt.ylabel("Error measure")<br/>plt.show()</span><span id="c752" class="lw lf hh mu b fi nh ne l nf ng">logisticR=LogisticRegression(penalty='l2',C=alpha[best_alpha],)<br/>logisticR.fit(X_train,y_train)<br/>sig_clf = CalibratedClassifierCV(logisticR, method="sigmoid")<br/>sig_clf.fit(X_train, y_train)<br/>pred_y=sig_clf.predict(X_test)</span><span id="6325" class="lw lf hh mu b fi nh ne l nf ng">predict_y = sig_clf.predict_proba(X_train)<br/>print ('log loss for train data',log_loss(y_train, predict_y, labels=logisticR.classes_, eps=1e-15))<br/>predict_y = sig_clf.predict_proba(X_cv)<br/>print ('log loss for cv data',log_loss(y_cv, predict_y, labels=logisticR.classes_, eps=1e-15))<br/>predict_y = sig_clf.predict_proba(X_test)<br/>print ('log loss for test data',log_loss(y_test, predict_y, labels=logisticR.classes_, eps=1e-15))<br/>print("Micro F1 score: {}".format(f1_score(y_test, pred_y, labels=logisticR.classes_, average="micro")))<br/>print("Macro F1 score: {}".format(f1_score(y_test, pred_y, labels=logisticR.classes_, average="macro")))</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es op"><img src="../Images/19aa22bf5870579954de132798accc3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*Na6OU8Oiz7WNjZUwyIM6JQ.png"/></div></figure><h2 id="320c" class="lw lf hh bd lg lx ly lz lk ma mb mc lo jw md me lq ka mf mg ls ke mh mi lu mj bi translated">7.3.回归模型</h2><pre class="iy iz ja jb fd mz mu na nb aw nc bi"><span id="2cd9" class="lw lf hh mu b fi nd ne l nf ng"># Randomforest<br/>from sklearn.ensemble import RandomForestRegressor<br/>rf = RandomForestRegressor()<br/>distributions = dict(n_estimators = [10,50,100,300,500,1000])<br/>clf = RandomizedSearchCV(rf, distributions,n_jobs=-1, cv=5)<br/>search = clf.fit(x_train, y_train)<br/>print("Best Params :{}".format(search.best_params_))<br/>rf = clf.best_estimator_<br/>rf.fit(x_train, y_train)<br/>pred_y = rf.predict(x_train)<br/>prediction = rf.predict(x_test)<br/>print("training mse: {} and r2 score: {}".format(mean_squared_error(y_train, pred_y), r2_score(y_train, pred_y)))<br/>print("testing mse: {} and r2 score: {}".format(mean_squared_error(y_test, prediction), r2_score(y_test, prediction)))</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es oq"><img src="../Images/847b3cccce71bf65ee632cf81ca407ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*FmpIh8D_ikU207Upvx63gQ.png"/></div></figure><h1 id="d31c" class="le lf hh bd lg lh or lj lk ll os ln lo in ot io lq iq ou ir ls it ov iu lu lv bi translated">进一步的改进</h1><ol class=""><li id="7d4f" class="kj kk hh jp b jq mk jt ml jw mw ka mx ke my ki ko kp kq kr bi translated">我们可以微调BERT以获得更好的分类结果</li><li id="fe31" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">如果我们设法获得更多的训练数据，我们可以简化分类的多模型架构</li></ol><h1 id="113f" class="le lf hh bd lg lh or lj lk ll os ln lo in ot io lq iq ou ir ls it ov iu lu lv bi translated">参考</h1><p id="72e1" class="pw-post-body-paragraph jn jo hh jp b jq mk ii js jt ml il jv jw mm jy jz ka mn kc kd ke mo kg kh ki ha bi translated"><a class="ae iw" href="https://remicnrd.github.io/Aspect-based-sentiment-analysis/" rel="noopener ugc nofollow" target="_blank">https://remicnrd.github.io/Aspect-based-sentiment-analysis</a></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ow ox l"/></div></figure></div></div>    
</body>
</html>