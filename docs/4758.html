<html>
<head>
<title>Machine Learning #4 — Unsupervised Learning, Clustering, K-Means Algorithm, Dimensional Reduction, Principal Components Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习#4 —无监督学习、聚类、K均值算法、降维、主成分分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-4-unsupervised-learning-clustering-k-means-algorithm-dimensional-reduction-238a54e8cfcd?source=collection_archive---------1-----------------------#2022-02-04">https://medium.com/analytics-vidhya/machine-learning-4-unsupervised-learning-clustering-k-means-algorithm-dimensional-reduction-238a54e8cfcd?source=collection_archive---------1-----------------------#2022-02-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/edf38a6c6a900e2ccd4d40ca26fccae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3rRW73wuojJ9B1d-yYoLxQ.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">捕捉自<a class="ae it" href="https://adimadimgurme.com/2020/09/20/sante-wine-more/" rel="noopener ugc nofollow" target="_blank">https://adimadimgurme.com/2020/09/20/sante-wine-more/</a></figcaption></figure><p id="4a6b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js">这篇文章是我的机器学习系列的第四篇文章，我写它是为了巩固和分享我所学到的东西。对于发表在Analytics Vidhya上的系列文章的第三篇:</em> <a class="ae it" rel="noopener" href="/analytics-vidhya/machine-learning-3-linear-regression-ridge-lasso-functions-2fa7fda624a0"> <em class="js">链接</em> </a></p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><p id="97e3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你对这篇文章的土耳其语不感兴趣，可以跳过这一段。</p><p id="ac35" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js"> Bu yaz，orendklerimi peki TIR MEK ve pay lamak I in kale me ALD m机器学习yaz dizimin rdnüyaz SDR . serin in yaynlananüüncy as I in:</em><a class="ae it" rel="noopener" href="/software-development-turkey/makine-öğrenmesi-3-lineer-regresyon-ridge-lasso-fonksiyonları-9dbf6036e29e"><em class="js">链接</em> </a></p><p id="ebbc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">bu yazūn n türke si isi in:<a class="ae it" href="https://gokerguner.medium.com/machine-learning-4-gözetimsiz-öğrenme-kümeleme-k-means-algoritması-boyut-azaltma-temel-44598c53ee0c" rel="noopener">链接</a></p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><p id="1271" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你好，在我们机器学习系列的第四集中，</p><ul class=""><li id="a5ae" class="ka kb hh iw b ix iy jb jc jf kc jj kd jn ke jr kf kg kh ki bi translated">检查无监督学习的概念，</li><li id="a0ea" class="ka kb hh iw b ix kj jb kk jf kl jj km jn kn jr kf kg kh ki bi translated">将学习聚类方法和包含该方法的K-Means算法，</li><li id="12de" class="ka kb hh iw b ix kj jb kk jf kl jj km jn kn jr kf kg kh ki bi translated">我们将触及降维和主成分分析(PCA)的概念，它们是聚类方法的关键问题。</li></ul><p id="2d40" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将在本系列的第二篇文章中使用葡萄酒质量数据集，但这一次我们将使用无监督学习技术检查数据集，而不进行标记。</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="2931" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这篇文章中，我们将讨论一个不同于该系列前几篇文章的学习模型，<strong class="iw hi">无监督学习</strong>。<strong class="iw hi">无监督学习</strong>与监督学习不同，它是学习数据中存在的关系和结构，而不会将数据标记为因果或输入输出。</p><p id="bd0b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">与监督学习不同，我们不需要训练数据。无监督学习算法解释和分组数据。然后它猜测新数据属于哪个组。在进行这种估计时，它试图找出其特征与哪些组更相似，但无法对这个样本进行评论，因为数据没有被标记。虽然这种解释留给了实现者，但是它非常有用，因为它很容易可视化。</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="c0b8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于聚类分析算法，重要的是所有值都是非空的，并且不包含任何字符串。我们所有的值都必须是数字，因为聚类将根据分析空间中的距离计算来完成。</p><h2 id="e3b4" class="ku kv hh bd kw kx ky kz la lb lc ld le jf lf lg lh jj li lj lk jn ll lm ln lo bi translated"><strong class="ak"> K-Means聚类算法</strong></h2><p id="d0f0" class="pw-post-body-paragraph iu iv hh iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr ha bi translated">K-means聚类方法是将数据集划分为K个作为输入参数给出的聚类。目的是确保在划分过程结束时获得的聚类在聚类内具有最大相似性，而在聚类之间具有最小相似性。</p><p id="72f3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">根据K-means算法的工作机制，随机选取K个对象来代表每个聚类的中心点或均值。考虑到它们与聚类平均值的距离，剩余的对象被包括在与它们最相似的聚类中。然后，通过计算每个聚类的平均值，确定新的聚类中心，并再次检查对象到中心的距离。该算法继续重复，直到没有变化。</p><p id="b4a6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">该算法基本上由4个阶段组成:</p><ol class=""><li id="8954" class="ka kb hh iw b ix iy jb jc jf kc jj kd jn ke jr lu kg kh ki bi translated"><strong class="iw hi">聚类中心的确定</strong></li><li id="3e9c" class="ka kb hh iw b ix kj jb kk jf kl jj km jn kn jr lu kg kh ki bi translated"><strong class="iw hi">根据数据的距离将数据聚集在中心之外</strong></li><li id="5d60" class="ka kb hh iw b ix kj jb kk jf kl jj km jn kn jr lu kg kh ki bi translated"><strong class="iw hi">根据聚类确定新中心(或将旧中心转移到新中心)</strong></li><li id="5b9a" class="ka kb hh iw b ix kj jb kk jf kl jj km jn kn jr lu kg kh ki bi translated"><strong class="iw hi">重复步骤2和3，直到稳定。</strong></li></ol><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="f3ac" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在我们使用该数据集的上一篇文章中，我们说过，通过使用<em class="js"> df.describe() </em>方法并解释表中的数值，我们数据集的大多数质量属性都具有诸如5和6这样的值。为了直观地表现这一点，我们可以用seaborn库来做这样一个可视化。</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="10ec" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们在本系列的前几篇文章中已经提到了相关矩阵的概念。这一次，我们用不同的方法来画它，没有上半部分。</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="ae97" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了更直观地表达相关性，我选择了相关性更强的变量，而不是我们的目标变量“质量”。从相关矩阵可以看出，<strong class="iw hi">固定酸度</strong>特征与<strong class="iw hi">密度</strong>呈强正(<strong class="iw hi"> 0.67 </strong>)相关，与pH呈强负(<strong class="iw hi"> -0.68 </strong>)相关。</p><h2 id="200d" class="ku kv hh bd kw kx ky kz la lb lc ld le jf lf lg lh jj li lj lk jn ll lm ln lo bi translated">质心和惯性概念</h2><p id="8f8a" class="pw-post-body-paragraph iu iv hh iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr ha bi translated">K-均值聚类算法的目的只是尽可能好地选择K个聚类的质心。这些重心叫做质心。尽可能选择最佳时，损失函数(即惯性值)最小。</p><p id="ec76" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">注:</strong>当然，我们的最终目的不仅仅是尽可能的收缩惯性。如果我们将聚类或质心的数量设置为等于数据集元素的数量，自然惯性将为零。但结果是，我们的数据没有被聚类。</p><p id="d464" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">理想的集群数量可以通过<strong class="iw hi">肘法</strong>找到。</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="78a6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们从数据集中的样本属性中提取“质量”属性，然后将这些值分配给一个<strong class="iw hi"> X </strong>变量，并通过<strong class="iw hi">缩放</strong>来消除标量的影响，从而准备好我们的算法将用来理解数据的输入集。</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="ba47" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">注意:</strong>在确定最佳聚类数时，我们没有使用缩放后的X值，因为这种方法是一种数值修正，我们将对其进行修正以提高模型性能。但是首先，我们要检查数据本身，而不是被篡改的版本，以了解我们要将这些数据分成多少个簇。</p><p id="c75d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">似乎我们可以将我们的聚类数设置为3，因为在这个数之后，惯性值的下降率减小。</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="f4c1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们的K-Means模型将使用我们为训练准备的数据集的前1000个样本，并根据它从前1000个样本中做出的推断对其他500个样本进行分类。</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="397e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">您可以在图上看到以正方形表示的簇的质心。虽然我们能够正确地聚类一些清晰的样本，但颜色看起来有点混合。我们如何帮助我们的算法获得更好的性能？</p><h2 id="9c33" class="ku kv hh bd kw kx ky kz la lb lc ld le jf lf lg lh jj li lj lk jn ll lm ln lo bi translated">降维与主成分分析</h2><p id="e845" class="pw-post-body-paragraph iu iv hh iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr ha bi translated">降维发现数据中的模式，并以“压缩”格式重新表述它们。因此，它可以更有效地计算时间和大小。在处理大得多的数据集时，这不仅对准确性很重要，对时间也很重要。</p><p id="95b0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">它最重要的功能是从数据集中移除包含相对较少信息的要素。降维功能中使用的最基本的技术是PCA(主成分分析)。</p><p id="a6d9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将尝试做的主要事情是找到包含数据重要信息的“关键组件”的数量，并将数据集中的要素减少到该数量。</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="7644" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在某些资料中，这些成分可能会被视为“内在维度”。对于这个例子，我们可以说我们的数据集实际上是一个多维数据集，而“本质上”它实际上是二维的。</p><p id="fc15" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这里，我们看到相当一部分特征不包含信息。如果我们将组件计数设置为2，看起来我们可以保留对我们有用的大部分信息。那么，我们到底保存了多少这种“信息”？</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="5569" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当我们将分量数设置为2时，我们可以保留大约99.8%的总方差，也就是几乎所有的方差。</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="2969" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">质心和簇看起来都比我们第一次尝试时更好。</p><h2 id="d52a" class="ku kv hh bd kw kx ky kz la lb lc ld le jf lf lg lh jj li lj lk jn ll lm ln lo bi translated">分层聚类</h2><p id="d6ce" class="pw-post-body-paragraph iu iv hh iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr ha bi translated">分层聚类在从小到大的聚类层次结构中组织样本，对于非技术人员来说，这是一种可视化数据的好方法。就像我们在高中生物中看到的对生活世界的分类一样，从最小(或最大)的共同特征开始，按分类继续。</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="371e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在完全链接(在上面的链接方法中称为“完全”)中，簇之间的距离是簇的最远点之间的距离。这被称为凝聚法。在这种方法的开始，所有的对象都是相互分离的。也就是说，每个样本本身就是一个集合。随后，将具有相似属性的聚类放在一起以获得单个聚类。这种方法的另一个名字是<strong class="iw hi">最远邻居方法</strong>。</p><p id="1aaa" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在单个链接中(在下面的链接方法中称为“单个”)，簇之间的距离是簇的最近点之间的距离。在分裂方法中，分裂策略占主导地位。在这种方法中，开始时只有一个集群。在每个阶段，根据距离/相似性矩阵将对象从主聚类中分离出来，并形成不同的子集。该过程的结果是，每个数据成为一个集合。这种方法的另一个名字是<strong class="iw hi">最近邻法</strong>。</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="4c84" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下一篇文章再见。</p><div class="lv lw ez fb lx ly"><a href="https://github.com/gokerguner" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hi fi z dy md ea eb me ed ef hg bi translated">gokerguner -概述</h2><div class="mf l"><p class="bd b fp z dy md ea eb me ed ef dx translated">随着时间的推移，这些领域将会丰富Python、数据科学和机器Learning.github.com的内容</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml in ly"/></div></div></a></div></div></div>    
</body>
</html>