<html>
<head>
<title>BERT Embedding for Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于分类的BERT嵌入</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bert-embedding-for-classification-7c51aead26d9?source=collection_archive---------0-----------------------#2021-05-16">https://medium.com/analytics-vidhya/bert-embedding-for-classification-7c51aead26d9?source=collection_archive---------0-----------------------#2021-05-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ce73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">机器学习的最新进展和不断增长的可用数据对自然语言处理领域产生了巨大影响。它们促进了新的神经架构的发展，并导致许多NLP任务的强大改进，如机器翻译或文本分类。一个特别重要的进步是建立高质量的、机器可读的词义表示模型的发展。这些表示通常被称为单词嵌入，是可以在处理文本数据的神经模型中用作特征的向量。</p><p id="9936" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">嵌入类型</strong></p><p id="10aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.静态单词嵌入:</p><p id="9ef9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">顾名思义，这些单词嵌入在本质上是静态的。这些包含了预先训练的单词值，我们可以在训练模型时使用这些值。</p><p id="8b3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，<strong class="ih hj"> Glove和word2vec </strong>就是静态单词嵌入的最好例子。这些嵌入大多我们可以使用预训练一次。但是在某些情况下，我们也可以训练这些嵌入。但它将包含高计算能力的GPU和大量的时间来从头训练这些嵌入。</p><p id="1bc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.上下文嵌入:</p><p id="aecc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">上下文嵌入</strong>(例如ELMo，BERT)，旨在为文档中的每个单词学习一个<strong class="ih hj">连续(向量)表示</strong>。连续表示可以用于下游的机器学习任务。</p><p id="a594" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是这些情境化的表现有多情境化呢？</p><p id="9ecc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑一下“鼠标”这个词。它有多个词义，一个指啮齿动物，另一个指一种装置。伯特是否有效地为每个词义创造了一个“鼠标”的表征(左图)？或者伯特创造了无限多的“鼠标”的表现形式，每一个都高度特定于它的上下文(右)？</p><div class="je jf jg jh fd ab cb"><figure class="ji jj jk jl jm jn jo paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><img src="../Images/f413328d68cafbf9e35f45bc53af4f9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/0*zfRWj4iKpmuEPxn4.png"/></div></figure><figure class="ji jj jv jl jm jn jo paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><img src="../Images/a1d32d18b2c43d733284d9fca1269f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/0*EskOnmnNyeomqRDk.png"/></div></figure></div><p id="8af7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，传统的嵌入只代表两种鼠标之间的区别，而从BERT中，我们也可以获得实时上下文。如图所示，喜欢奶酪显示了与啮齿动物的相似性，而对于另一个“点击”显示了相似性。</p><p id="443e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我们如何使用嵌入进行分类？</strong></p><p id="a99a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们没有太多的分类数据，在这种情况下，我们可以使用嵌入的方法，就像我们在暹罗网络中进行面部识别一样。</p><p id="0dc3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种方法基本上称为单次学习或零次学习，我们没有太多的数据来建立任何分类器。在这种情况下，我们可以计算现有标记数据的嵌入，然后根据其嵌入与其他嵌入的接近程度对新数据点进行分类。谁更接近，我们就把它归入那个特定的类别。为了嵌入的紧密性，我们使用余弦相似度函数。</p><p id="2257" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个任务中，我将使用拥抱面部库。(<a class="ae jw" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/transformers/</a></p><p id="12e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下面的用例中，我将展示长句(超过512个标记)的嵌入创建。因为我们已经知道使用超过512个令牌的Bert内存限制。</p><p id="c615" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解决这个问题，我在这里使用了滑动窗口技术，这样我们可以一次提取整个语料库的嵌入内容。</p><figure class="je jf jg jh fd jj er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jx"><img src="../Images/a3da86fda27c90e2db67bae660bc479e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qEq1SLQ5-StV7gZuY12K0Q.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">加载库</figcaption></figure><p id="d477" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用的是“bert-base-uncased”模型。总是设置<strong class="ih hj">output _ hidden _ States</strong>= True，否则在初始化模型时，模型不会吐出隐藏状态。</p><p id="f8bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在记号赋予器库的帮助下创建文本的记号。在设置<strong class="ih hj"> add_special_tokens= False </strong>时，特殊令牌(‘CLS’或‘SEP’)不包含在令牌本身中。</p><figure class="je jf jg jh fd jj er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es kc"><img src="../Images/63aded10f0a77ab43052d8bebadecbb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bDyeNyrUwxFp-mKWXaV17A.png"/></div></div></figure><p id="cc0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了克服512个令牌限制的问题，我们将使用滑动窗口技术。因此，我们将分割输入id和注意屏蔽id，使它们的总长度不超过512。</p><figure class="je jf jg jh fd jj er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es kd"><img src="../Images/24a23d3f02433a615afde3cdd7a8ae76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRDIgvltMYK3EeitYbGVBQ.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">将令牌分割成510个长度的块。剩下的两个是CLS和SEP代币</figcaption></figure><p id="6ec6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们可以迭代每一个块，并通过我们的模型传递它们，生成它们中每一个的嵌入。</p><figure class="je jf jg jh fd jj er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es ke"><img src="../Images/58e5a4fa225015166f2ee490247cb7ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IeloDbKx4yQLsEcYPPlfhw.png"/></div></div></figure><p id="47b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里torch.tensor([101])和([102])表示“CLS和SEP”记号。在将这些标记添加到输入块之后，我们还需要检查填充。大多数情况下，在最后一次迭代中，我们的输入长度会小于510。</p><p id="212a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的例子中，只在最后一次迭代中需要填充。下面我们准备输入标识和掩码标识，然后将它们传递到模型中。</p><figure class="je jf jg jh fd jj er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es kf"><img src="../Images/c9f94b83d77bc7ee949f76a157a431be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_8qv5-FjCgb1AoMchBL5Eg.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">在传递到模型之前转换为长张量和短张量</figcaption></figure><p id="29fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型将以元组格式显示输出和隐藏状态。Bert total将发送13层(也包括输入嵌入)。但是根据研究人员的说法，嵌入的最后一层包含了关于语料库上下文的大部分信息。这就是为什么我们只取最后9层来为我们的语料库创建嵌入。</p><figure class="je jf jg jh fd jj er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es kg"><img src="../Images/ee0c18baac5d73889b1313cd52c7ccaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kJnRW0M3yhdy7pbtigmqUA.png"/></div></div></figure><p id="4429" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的代码只得到4层隐藏状态，并取出它们的平均值。</p><p id="5f9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">隐藏层形状=[句子数量或批量大小、总标记、隐藏层数量、特征数量]</p><figure class="je jf jg jh fd jj er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es kh"><img src="../Images/2a5d1f74f30df58d53cc7a17d212f953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RpJ8JjEe7wOU4HxKsuzV5g.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">取9层的平均值。</figcaption></figure><p id="0789" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">toek_embedding[:，:，9:，:] →给出最后4个隐藏层以获得更好的上下文。</p><p id="e7bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这不是我们文档的最终嵌入，因为我们已经将文档分成了510个令牌的块。因此，我们将迭代每个块，并获得其最后4层嵌入的平均值。最后，我们也可以取所有标记嵌入的平均值，以得到文档的最终嵌入。</p><p id="9a2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们有了文档的最终嵌入，我们就可以很容易地借助余弦相似度找到它与其他文档的相似度。</p><p id="7d88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">什么是余弦相似度？</strong></p><p id="e90e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">余弦相似度是一种度量标准，用于衡量文档无论大小有多相似。在数学上，它测量的是在多维空间中投影的两个向量之间的角度余弦。余弦相似性是有利的，因为即使两个相似的文档相距欧几里德距离很远(由于文档的大小)，它们仍有可能更靠近在一起。角度越小，余弦相似度越高。</em> </strong></p><figure class="je jf jg jh fd jj er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es ki"><img src="../Images/aa5af7150effa46d6837b431b47d880d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qApkpYi66AsUdXocsl_gqg.png"/></div></div></figure><p id="8fc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">或者我们可以使用PyTorch的内置功能</p><p id="1886" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">cos = torch . nn . cosine similarity(dim = 0，eps=1e-6)</p><p id="cb31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相似度= cos(sent1嵌入，sent2嵌入)</p><p id="8db6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读！！</p><p id="1adb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><div class="kj kk ez fb kl km"><a href="http://ai.stanford.edu/blog/contextual/" rel="noopener  ugc nofollow" target="_blank"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hj fi z dy kr ea eb ks ed ef hh bi translated">伯特、埃尔莫和GPT-2:语境化的单词表征有多语境化？</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">将上下文结合到单词嵌入中——如伯特、埃尔莫和GPT-2所举的例子——已被证明是一个分水岭</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">ai.stanford.edu</p></div></div><div class="kv l"><div class="kw l kx ky kz kv la jt km"/></div></div></a></div></div></div>    
</body>
</html>