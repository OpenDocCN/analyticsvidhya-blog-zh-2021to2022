<html>
<head>
<title>Pytorch to Keras using ONNX</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用ONNX将Pytorch转换为Keras</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pytorch-to-keras-using-onnx-71d98258ad76?source=collection_archive---------0-----------------------#2021-09-24">https://medium.com/analytics-vidhya/pytorch-to-keras-using-onnx-71d98258ad76?source=collection_archive---------0-----------------------#2021-09-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/c34ca6b1af51fe0793dfcba8574b1ea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Qm_wAwcMPe-Q3TDKUaIqA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用ONNX将Pytorch转换为Keras</figcaption></figure><h1 id="2ff4" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated"><strong class="ak">模型部署</strong></h1><p id="f9e4" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">模型部署是一种方法，通过这种方法，您可以将机器学习模型集成到现有的生产环境中，以根据数据做出实际的业务决策。这是机器学习生命周期中的最后一个阶段，也可能是最繁琐的阶段之一。【<a class="ae kq" href="https://www.datarobot.com/wiki/machine-learning-model-deployment/" rel="noopener ugc nofollow" target="_blank">定义来源</a></p><p id="d8fb" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">模型部署可能是机器学习模型生命周期中最重要的部分，但仍然是研究最少的部分。ML/DL领域的大多数课程都教授如何探索数据、设计特征、训练模型和生成预测。但是他们忽略了最重要的部分:之后做什么？</p><p id="8dc3" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">除了为学习或Kaggle竞赛开发的模型之外，所有其他模型都是为了创收而构建的，如果你不将一个模型部署到生产中，那么就没有人使用它，因此没有收入。</p></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><h1 id="2f3b" class="iu iv hi bd iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn lh jp jq jr bi translated"><strong class="ak"> ONNX vs厂商锁定</strong></h1><p id="ffb8" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">有时你使用一个框架创建模型，例如,<a class="ae kq" href="https://developer.apple.com/documentation/coreml" rel="noopener ugc nofollow" target="_blank"> coreML </a>,但是你想把它部署到一个它不支持的系统中(例如，Android)。这种不可互操作性意味着您训练过的、测试过的模型没有任何用处。</p><p id="5bf6" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">一个可能且显而易见的解决方案是使用Android支持的格式重新训练您的模型。但是这个显而易见的解决方案可行吗？在有严格时间表的生产环境中，这样的解决方案不是最佳的。即使你有充足的时间，再培训也不是最好的解决方案。训练一个模型并使其达到最佳精度绝非易事。这涉及到长时间的训练和调试，非常麻烦。所以，当你已经有了一个最佳精度的模型，你就不想再从头开始重新训练它了。引用软件工程中的一句话:</p><blockquote class="li"><p id="d05d" class="lj lk hi bd ll lm ln lo lp lq lr kp dx translated">编程的第一条规则:如果有用，不要碰它。</p></blockquote><p id="75ad" class="pw-post-body-paragraph js jt hi ju b jv ls jx jy jz lt kb kc kd lu kf kg kh lv kj kk kl lw kn ko kp hb bi translated">一个更好的选择是将您的模型转换成一个框架，该框架受您想要部署模型的系统的支持。这就是<a class="ae kq" href="https://github.com/onnx/onnx" rel="noopener ugc nofollow" target="_blank"> <strong class="ju hj"> ONNX </strong> </a> <strong class="ju hj"> </strong>出现的原因。</p><p id="ad7c" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">ONNX代表<strong class="ju hj">开放式神经网络交换</strong>。ONNX是一个开源的人工智能生态系统，可用于交换深度学习模型。它承诺使深度学习模型可移植，从而防止供应商锁定。</p><p id="b396" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">2017年9月，脸书和微软推出了它，用于在PyTorch和Caffe2等机器学习框架之间切换。后来，IBM、华为、英特尔、AMD、ARM和高通宣布支持该计划。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/7b68228e481bb7510e9a3c5cdde42be3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OZA-la4uErLMTK--xGR6Tg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">ONNX引入了互操作性。<a class="ae kq" href="https://microsoft.github.io/ai-at-edge/docs/onnx/" rel="noopener ugc nofollow" target="_blank">图像来源</a></figcaption></figure></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><h1 id="1fa4" class="iu iv hi bd iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn lh jp jq jr bi translated"><strong class="ak">手头的任务</strong></h1><p id="b164" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">下面，我将解释使用ONNX将Pytorch模型转换为Keras模型的过程(类似的方法也可以用于其他类型模型之间的转换)。我们将使用以下用例进行演示。</p><h1 id="0c72" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">用例:手写数字分类</h1><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/9b1894c783f08855653f1d1208570152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*Mw20mFOOjS0fE1j2Tiz02A.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来自MNIST数据集的样本图像。<a class="ae kq" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank">图片来源</a></figcaption></figure><p id="cbb6" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated"><strong class="ju hj">用例简介</strong>:我们想要创建一个ML模型，它可以将图像作为输入，并成功地将其分类为0到9之间的数字。</p><p id="03da" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">我们将创建一个多层感知器(MLP)网络，用于构建手写数字分类器。我们将利用<em class="md">火炬视觉</em>包中包含的MNIST数据集。</p><p id="78dd" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">我们将使用PyTorch内提供的不同实用程序包(<em class="md"> NN </em>、<em class="md">autograted</em>、<em class="md"> optim </em>、<em class="md"> torchvision </em>、<em class="md"> torchtext </em>等)。)来建立和训练神经网络。为了便于使用和访问，我们将使用<a class="ae kq" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank">谷歌实验室</a>来运行我们的代码。</p><blockquote class="li"><p id="fb4e" class="lj lk hi bd ll lm ln lo lp lq lr kp dx translated">让我们把手弄脏。</p></blockquote><h1 id="783d" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf me jh ji jj mf jl jm jn mg jp jq jr bi translated">装置</h1><p id="ba36" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">安装默认情况下Google Colab不附带的必要软件包。如果您在Google Colab之外运行，那么您需要安装本文最后一部分提到的所有软件包。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="3fda" class="mm iv hi mi b fi mn mo l mp mq">! pip install onnx</span><span id="e7b2" class="mm iv hi mi b fi mr mo l mp mq">! pip install onnx2keras</span><span id="c5a8" class="mm iv hi mi b fi mr mo l mp mq">! pip install onnxruntime</span></pre><h1 id="439d" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">数据加载和处理</h1><p id="dad2" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">与您将要从事的任何ML项目一样，第一步是数据预处理。我们需要将原始数据集转换成张量，并在固定范围内对其进行归一化。torchvision包提供了一个名为transforms的工具，可以用来将不同的转换组合在一起。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="e7ae" class="mm iv hi mi b fi mn mo l mp mq"><strong class="mi hj">from</strong> <strong class="mi hj">torchvision</strong> <strong class="mi hj">import</strong> transforms</span><span id="c6d5" class="mm iv hi mi b fi mr mo l mp mq">_tasks = transforms.Compose([transforms.ToTensor(),<br/>  transforms.Normalize((0.5,), (0.5,))<br/>])</span></pre><p id="1362" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">第一个变换将原始数据转换为张量变量，第二个变换使用以下操作执行归一化:</p><p id="87ba" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">x_normalized = x均值/标准差</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="2442" class="mm iv hi mi b fi mn mo l mp mq"><strong class="mi hj">from</strong> <strong class="mi hj">torchvision.datasets</strong> <strong class="mi hj">import</strong> MNIST</span><span id="88a8" class="mm iv hi mi b fi mr mo l mp mq"><em class="md">## Load MNIST Dataset and apply transformations</em><br/>mnist = MNIST("data", download=<strong class="mi hj">True</strong>, train=<strong class="mi hj">True</strong>, transform=_tasks)</span></pre><h1 id="c1e7" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">数据可视化</h1><p id="58ed" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">现在让我们来看看数据集</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="4d14" class="mm iv hi mi b fi mn mo l mp mq"><strong class="mi hj">from</strong> <strong class="mi hj">torch.utils.data</strong> <strong class="mi hj">import</strong> DataLoader</span><span id="8bb5" class="mm iv hi mi b fi mr mo l mp mq">rawData = DataLoader(mnist, batch_size=10)</span><span id="50f7" class="mm iv hi mi b fi mr mo l mp mq">dataiter = iter(rawData)<br/>data, labels = dataiter.next()<br/>data = data.view(data.shape[0], -1)</span><span id="561b" class="mm iv hi mi b fi mr mo l mp mq">print("shape", data.shape)</span></pre><p id="f9c9" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">Out[ ]:</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="95d0" class="mm iv hi mi b fi mn mo l mp mq">shape torch.Size([10, 784])</span></pre><p id="eed3" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">该数据集包含手写数字的图像。每个图像(最初是一个28×28的矩阵)被压缩成一个1×784的矢量。我们可以通过将向量整形为28×28的矩阵来得到原始图像。</p><p id="1d52" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">这是第一个向量的200个起始值的样子:</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="b42d" class="mm iv hi mi b fi mn mo l mp mq">data[0][:200]</span></pre><p id="8c55" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">Out[ ]:</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="919f" class="mm iv hi mi b fi mn mo l mp mq">tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -0.9765, -0.8588, -0.8588, -0.8588, -0.0118,  0.0667,  0.3725, -0.7961,<br/>         0.3020,  1.0000,  0.9373, -0.0039, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,<br/>        -0.7647, -0.7176, -0.2627,  0.2078,  0.3333,  0.9843,  0.9843,  0.9843,<br/>         0.9843,  0.9843,  0.7647,  0.3490,  0.9843,  0.8980,  0.5294, -0.4980,<br/>        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000])</span></pre><p id="2e47" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">由于上面这样的数字不是很直观，让我们试着形象化一下。请注意，在下面的代码中，我们已经将vector调整为28×28的矩阵，以便绘制。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="2c97" class="mm iv hi mi b fi mn mo l mp mq"><strong class="mi hj">import</strong> <strong class="mi hj">matplotlib.pyplot</strong> <strong class="mi hj">as</strong> <strong class="mi hj">plt</strong><br/><br/>num = 10<br/>num_row = 2<br/>num_col = 5<br/><br/><em class="md"># plot images</em><br/>fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))<br/><strong class="mi hj">for</strong> i <strong class="mi hj">in</strong> range(num):<br/>    ax = axes[i//num_col, i%num_col]<br/>    pixels = data[i].numpy()<br/>    pixels = pixels.reshape((28,28))<br/>    ax.imshow(pixels, cmap='gray')<br/>plt.tight_layout()<br/>plt.show()</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/39460e798eba8593b0b7bbf37d761d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*Q2CNp9Qqvl2mPkjZNhGbdw.png"/></div></figure><p id="0bff" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">这就是我们的图像绘制时的样子。让我们朝着训练和推理前进。</p><h1 id="39f6" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">Pytorch模型培训</h1><p id="0836" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated"><em class="md"> PyTorch </em>的另一个优秀工具是<em class="md">数据加载器</em>迭代器，它提供了使用多处理工作器并行批处理、混洗和加载数据的能力。为了评估我们的模型，我们将把数据分成训练集和验证集。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="582b" class="mm iv hi mi b fi mn mo l mp mq"><strong class="mi hj">from</strong> <strong class="mi hj">torch.utils.data</strong> <strong class="mi hj">import</strong> DataLoader<br/><strong class="mi hj">from</strong> <strong class="mi hj">torch.utils.data.sampler</strong> <strong class="mi hj">import</strong> SubsetRandomSampler</span><span id="6686" class="mm iv hi mi b fi mr mo l mp mq"><em class="md">## create training and validation split </em><br/>split = int(0.8 * len(mnist))<br/>index_list = list(range(len(mnist)))<br/>train_idx, valid_idx = index_list[:split], index_list[split:]</span><span id="6e98" class="mm iv hi mi b fi mr mo l mp mq"><em class="md">## create sampler objects using SubsetRandomSampler</em><br/>tr_sampler = SubsetRandomSampler(train_idx)<br/>val_sampler = SubsetRandomSampler(valid_idx)</span><span id="e784" class="mm iv hi mi b fi mr mo l mp mq"><em class="md">## create iterator objects for train and valid datasets</em><br/>trainloader = DataLoader(mnist, batch_size=256, sampler=tr_sampler)<br/>validloader = DataLoader(mnist, batch_size=256, sampler=val_sampler)</span></pre><p id="6630" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">PyTorch中的神经网络架构可以在一个类中定义，该类继承了来自名为Module的<em class="md"> NN </em>包的基类的属性。这个继承自<em class="md"> nn。模块</em>类允许我们轻松地实现、访问和调用许多方法。我们可以定义类的构造函数中的所有层，以及forward函数中的前向传播步骤。</p><p id="1526" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">我们将定义一个具有以下层配置的网络:[784，128，10]。此配置表示输入层中有784个节点(28*28像素)，隐藏层中有128个节点，输出层中有10个节点。在forward函数内部，我们将使用隐藏层中的sigmoid激活函数(可以从<em class="md"> NN </em>模块访问)。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="f793" class="mm iv hi mi b fi mn mo l mp mq"><strong class="mi hj">import</strong> <strong class="mi hj">torch.nn.functional</strong> <strong class="mi hj">as</strong> <strong class="mi hj">F</strong><br/><strong class="mi hj">import</strong> <strong class="mi hj">torch.nn</strong> <strong class="mi hj">as</strong> <strong class="mi hj">nn</strong></span><span id="961f" class="mm iv hi mi b fi mr mo l mp mq"><strong class="mi hj">class</strong> <strong class="mi hj">Model</strong>(nn.Module):<br/>    <strong class="mi hj">def</strong> __init__(self):<br/>        super().__init__()<br/>        self.hidden = nn.Linear(784, 128)<br/>        self.output = nn.Linear(128, 10)<br/>        <br/>    <strong class="mi hj">def</strong> forward(self, x):<br/>        x = self.hidden(x)<br/>        x = F.sigmoid(x)<br/>        x = self.output(x)<br/>        <strong class="mi hj">return</strong> x</span><span id="587e" class="mm iv hi mi b fi mr mo l mp mq">model = Model()</span></pre><p id="316f" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">使用<em class="md"> NN </em>和<em class="md"> optim </em>包定义损失函数和优化器:</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="2e9e" class="mm iv hi mi b fi mn mo l mp mq"><strong class="mi hj">from</strong> <strong class="mi hj">torch</strong> <strong class="mi hj">import</strong> optim<br/><strong class="mi hj">import</strong> <strong class="mi hj">numpy</strong> <strong class="mi hj">as</strong> <strong class="mi hj">np</strong><br/><strong class="mi hj">import</strong> <strong class="mi hj">torch</strong></span><span id="998f" class="mm iv hi mi b fi mr mo l mp mq">loss_function = nn.CrossEntropyLoss()<br/>optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay= 1e-6, momentum = 0.9, nesterov = <strong class="mi hj">True</strong>)</span></pre><p id="0540" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">我们现在准备训练模型。核心步骤将是正向传播、损耗计算、反向传播和更新参数。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="54cb" class="mm iv hi mi b fi mn mo l mp mq"><strong class="mi hj">for</strong> epoch <strong class="mi hj">in</strong> range(1, 11): <em class="md">## run the model for 10 epochs</em><br/>    train_loss, valid_loss = [], []<br/>    <em class="md">## training part </em><br/>    model.train()<br/>    <strong class="mi hj">for</strong> data, target <strong class="mi hj">in</strong> trainloader:<br/>        optimizer.zero_grad()<br/>        <em class="md">## 1. forward propagation</em><br/>        data = data.view(data.shape[0], -1)<br/>        output = model(data)<br/>        <br/>        <em class="md">## 2. loss calculation</em><br/>        loss = loss_function(output, target)<br/>        <br/>        <em class="md">## 3. backward propagation</em><br/>        loss.backward()<br/>        <br/>        <em class="md">## 4. weight optimization</em><br/>        optimizer.step()<br/>        <br/>        train_loss.append(loss.item())<br/>        <br/>    <em class="md">## evaluation part </em><br/>    model.eval()<br/>    <strong class="mi hj">for</strong> data, target <strong class="mi hj">in</strong> validloader:<br/>        data = data.view(data.shape[0], -1)<br/>        output = model(data)<br/>        loss = loss_function(output, target)<br/>        valid_loss.append(loss.item())<br/>    print ("Epoch:", epoch, "Training Loss: ", np.mean(train_loss), "Valid Loss: ", np.mean(valid_loss))</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/facadce225411ceb96419cf2aa63fb41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*AJNozeuMxkPm99-1vYxPEQ.png"/></div></figure><p id="904a" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">在上面的代码片段中，我只使用了10个纪元，但是您可以根据准确性指标增加它们。</p><h1 id="8a7b" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">模型推理</h1><p id="ce01" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">模型定型后，对验证数据进行预测。<strong class="ju hj">我们将使用预测值来查看我们的<em class="md"> PyTorch </em>模型和转换后的Keras模型是否输出相同的值。</strong></p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="2285" class="mm iv hi mi b fi mn mo l mp mq"><em class="md">## dataloader for validation dataset </em><br/>dataiter = iter(validloader)<br/>data, labels = dataiter.next()<br/>data = data.view(data.shape[0], -1)<br/>output = model(data)</span><span id="2f72" class="mm iv hi mi b fi mr mo l mp mq">_, preds_tensor = torch.max(output, 1)<br/>pytorchPredictions = np.squeeze(preds_tensor.numpy())</span></pre><p id="0a9e" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">现在我们已经将预测保存在“pytorchPredictions”变量中，我们将把<em class="md"> PyTorch </em>模型转换成ONNX格式，然后再把它转换成<em class="md"> Keras </em>模型。</p><p id="469e" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">转换将是这样的:</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mu"><img src="../Images/e58b65bb921aec14c56b4d6a6f139799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8EqqdWszSLsnctShHKMmTg.png"/></div></div></figure></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><h1 id="de41" class="iu iv hi bd iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn lh jp jq jr bi translated">Pytorch呼叫ONNX</h1><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="5295" class="mm iv hi mi b fi mn mo l mp mq"><em class="md"># ONNX is natively supported by Pytorch so we just need </em><br/><em class="md"># these 2 lines to export Pytorch model to ONNX.</em></span><span id="23eb" class="mm iv hi mi b fi mr mo l mp mq"><em class="md"># while running inferences you will have to pass data of this shape only</em><br/>x = torch.randn(1, 1, 256, 784, requires_grad=<strong class="mi hj">True</strong>)<br/>torch.onnx.export(model, x, "torchToOnnx.onnx", verbose=<strong class="mi hj">True</strong>, input_names = ['input'], output_names = ['output'])</span></pre><p id="e953" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">运行上述代码后的ONNX模型以“torchToOnnx.onnx”的名称保存在当前目录中。</p><p id="15d3" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">如果你的目的只是推理，那么你可以直接使用<em class="md"> ONNX </em>对象来做。由于许多平台都支持ONNX，所以直接用ONNX进行推理是一个合适的选择。为此，我们将需要ONNX运行时。以下代码描述了同样的情况:</p><p id="eef3" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated"><strong class="ju hj">使用ONNX进行推理:</strong></p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="5996" class="mm iv hi mi b fi mn mo l mp mq"><strong class="mi hj">import</strong> <strong class="mi hj">onnxruntime</strong> <strong class="mi hj">as</strong> <strong class="mi hj">rt</strong><br/><strong class="mi hj">import</strong> <strong class="mi hj">numpy</strong><br/><br/>sess = rt.InferenceSession("/content/torchToOnnx.onnx")<br/>input_name = sess.get_inputs()[0].name<br/><br/><em class="md"># Note: The input must be of the same shape as the shape of x during # the model export part. i.e. second argument in this function call: torch.onnx.export()</em><br/>onnxPredictions = sess.run(<strong class="mi hj">None</strong>, {input_name: data.numpy().reshape(1,1,256,784)})[0]</span></pre><p id="2256" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">我们会在最后看看预测值。现在，让我们继续前进，将模型转换为Keras。</p></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><h1 id="9060" class="iu iv hi bd iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn lh jp jq jr bi translated">ONNX到Keras</h1><p id="5bb5" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">ONNX到Keras不是本地支持的，但是多亏了慷慨的Python社区，我们有<a class="ae kq" href="https://pypi.org/project/onnx2keras/" rel="noopener ugc nofollow" target="_blank"> onnx2keras </a>为我们做这些。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="1534" class="mm iv hi mi b fi mn mo l mp mq"><strong class="mi hj">import</strong> <strong class="mi hj">onnx</strong><br/><strong class="mi hj">from</strong> <strong class="mi hj">onnx2keras</strong> <strong class="mi hj">import</strong> onnx_to_keras</span><span id="382d" class="mm iv hi mi b fi mr mo l mp mq"><em class="md"># Load ONNX model</em><br/>onnx_model = onnx.load('/content/torchToOnnx.onnx')</span><span id="c88c" class="mm iv hi mi b fi mr mo l mp mq"><em class="md"># Call the converter (input will be equal to the input_names parameter that you defined during exporting)</em><br/>k_model = onnx_to_keras(onnx_model, ['input'])</span></pre><p id="303b" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">让我们看看Keras模型的概要，看看它是否被正确导入</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="23aa" class="mm iv hi mi b fi mn mo l mp mq">k_model.summary()</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/68de314f530ffa47d7bdeabdc37dea2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mxH9N0GfeUm05frYxdqH0w.png"/></div></figure><p id="5028" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">现在我们有了Keras模型，我们可以在同一个数据集上生成预测，并将其与Pytorch模型的输出进行比较，以验证转换。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="1db7" class="mm iv hi mi b fi mn mo l mp mq">KerasPredictions = []<br/><strong class="mi hj">for</strong> i <strong class="mi hj">in</strong> range(10):<br/>    inp = data[i].numpy()<br/>    out = k_model.predict(inp.reshape(1, 784))<br/>    KerasPredictions.append(np.argmax(out))</span></pre></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><h1 id="9e43" class="iu iv hi bd iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn lh jp jq jr bi translated">预测比较</h1><p id="79b2" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">让我们先看看原始图像</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="289a" class="mm iv hi mi b fi mn mo l mp mq">num = 10<br/>num_row = 2<br/>num_col = 5</span><span id="fa55" class="mm iv hi mi b fi mr mo l mp mq"><em class="md"># plot images</em><br/>fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))<br/><strong class="mi hj">for</strong> i <strong class="mi hj">in</strong> range(num):<br/>    ax = axes[i//num_col, i%num_col]<br/>    pixels = data[i].numpy()<br/>    pixels = pixels.reshape((28,28))<br/>    ax.imshow(pixels, cmap='gray')<br/>plt.tight_layout()<br/>plt.show()</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/fddcf56bb2869c2d51df3b5d2db1ae57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*DngVBkgdsk8GWt6TnLkxBw.png"/></div></figure><p id="e8a9" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated"><strong class="ju hj">图像的实际标签</strong></p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="151d" class="mm iv hi mi b fi mn mo l mp mq">labels[:10]</span></pre><p id="3960" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">Out[ ]:</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="f432" class="mm iv hi mi b fi mn mo l mp mq">tensor([7, 6, 5, 1, 7, 2, 0, 0, 6, 4])</span></pre><p id="1fbc" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated"><strong class="ju hj">使用原始Pytorch模型的预测</strong></p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="8b60" class="mm iv hi mi b fi mn mo l mp mq">pytorchPredictions[:10]</span></pre><p id="5f7e" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">Out[ ]:</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="62eb" class="mm iv hi mi b fi mn mo l mp mq">array([7, 6, 5, 1, 7, 2, 0, 0, 6, 4])</span></pre><p id="78df" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated"><strong class="ju hj">使用ONNX运行时的预测</strong></p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="d7a7" class="mm iv hi mi b fi mn mo l mp mq">np.argmax(onnxPredictions[0][0][:10], axis=1)</span></pre><p id="005b" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">Out[ ]:</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="370a" class="mm iv hi mi b fi mn mo l mp mq">array([7, 6, 5, 1, 7, 2, 0, 0, 6, 4])</span></pre><p id="91ac" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated"><strong class="ju hj">使用转换后的Keras模型进行预测</strong></p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="3ce7" class="mm iv hi mi b fi mn mo l mp mq">KerasPredictions</span></pre><p id="55d4" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">Out[ ]:</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="46d3" class="mm iv hi mi b fi mn mo l mp mq">[7, 6, 5, 1, 7, 2, 0, 0, 6, 4]</span></pre><p id="dd91" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">正如我们在最后三个输出单元中看到的，Pytorch、Keras和onnx模型的输出是相同的，因此我们的转换有效。</p></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><h1 id="822d" class="iu iv hi bd iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn lh jp jq jr bi translated"><strong class="ak">套餐:</strong></h1><p id="5d0e" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">以下是笔记本中使用的包列表:</p><ol class=""><li id="7463" class="mx my hi ju b jv kr jz ks kd mz kh na kl nb kp nc nd ne nf bi translated">火炬:1.6.0+cu101</li><li id="fdd9" class="mx my hi ju b jv ng jz nh kd ni kh nj kl nk kp nc nd ne nf bi translated">onnx: 1.7.0</li><li id="4c8d" class="mx my hi ju b jv ng jz nh kd ni kh nj kl nk kp nc nd ne nf bi translated">数字:1.18.5</li><li id="7589" class="mx my hi ju b jv ng jz nh kd ni kh nj kl nk kp nc nd ne nf bi translated">onnx2keras: 0.0.23</li><li id="0ab3" class="mx my hi ju b jv ng jz nh kd ni kh nj kl nk kp nc nd ne nf bi translated">火炬视觉:0.7.0+cu101</li><li id="a2c0" class="mx my hi ju b jv ng jz nh kd ni kh nj kl nk kp nc nd ne nf bi translated">matplotlib: 3.2.2</li><li id="091e" class="mx my hi ju b jv ng jz nh kd ni kh nj kl nk kp nc nd ne nf bi translated">onnx运行时:1.5.2</li></ol><h1 id="8104" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">参考资料:</h1><ol class=""><li id="0165" class="mx my hi ju b jv jw jz ka kd nl kh nm kl nn kp nc nd ne nf bi translated"><a class="ae kq" href="https://www.analyticsvidhya.com/blog/2019/01/guide-pytorch-neural-networks-case-studies/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2019/01/guide-py torch-neural-networks-case-studies/</a></li><li id="81bb" class="mx my hi ju b jv ng jz nh kd ni kh nj kl nk kp nc nd ne nf bi translated"><a class="ae kq" rel="noopener" href="/@mrdatascience/how-to-plot-mnist-digits-using-matplotlib-65a2e0cc068">https://medium . com/@ mrdata science/how-to-plot-Mn ist-digits-using-matplotlib-65 a2 E0 cc 068</a></li><li id="7f06" class="mx my hi ju b jv ng jz nh kd ni kh nj kl nk kp nc nd ne nf bi translated"><a class="ae kq" href="https://blog.paperspace.com/what-every-ml-ai-developer-should-know-about-onnx/" rel="noopener ugc nofollow" target="_blank">https://blog . paper space . com/what-every-ml-ai-developer-should-know-on NX/</a></li><li id="5cd4" class="mx my hi ju b jv ng jz nh kd ni kh nj kl nk kp nc nd ne nf bi translated"><a class="ae kq" href="https://microsoft.github.io/ai-at-edge/docs/onnx/" rel="noopener ugc nofollow" target="_blank">https://microsoft.github.io/ai-at-edge/docs/onnx/</a></li><li id="c51e" class="mx my hi ju b jv ng jz nh kd ni kh nj kl nk kp nc nd ne nf bi translated"><a class="ae kq" rel="noopener" href="/analytics-vidhya/how-to-convert-your-keras-model-to-onnx-8d8b092c4e4f">https://medium . com/analytics-vid hya/how-to-convert-your-keras-model-to-onnx-8d 8b 092 C4 e4f</a></li></ol></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><h1 id="b5fd" class="iu iv hi bd iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn lh jp jq jr bi translated">结束注释</h1><p id="453a" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">这就是你如何轻松获得ONNX平台及其背后慷慨的开源社区带来的互操作性好处。</p><p id="d9df" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">为了写这篇文章，我得到了开源社区免费提供的多种资源的帮助。因为我从很多地方得到了帮助，所以我不也不能声称对这篇文章有任何所有权。你想怎么用就怎么用。你愿意的话可以提一下这篇文章。</p><p id="d251" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">此外，即使这篇文章是我自己写的(就像我的上一篇<a class="ae kq" rel="noopener" href="/analytics-vidhya/save-and-load-a-tensorflow-estimator-model-for-predictions-233b798620a9">文章</a>，我自己写并开发了代码)，我也不会声称拥有任何所有权。</p><p id="f8fb" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">“结尾笔记”中的所有哲学不是为了吹嘘自己，而是为了激励你为开源社区做出贡献，以便每个人都可以学习，我们可以<a class="ae kq" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank">让神经网络再次变得不酷</a>。</p><p id="3353" class="pw-post-body-paragraph js jt hi ju b jv kr jx jy jz ks kb kc kd kt kf kg kh ku kj kk kl kv kn ko kp hb bi translated">如果你喜欢这篇文章，那么你可以想按多少次拍手按钮就按多少次。另外，你可以在<a class="ae kq" href="https://www.linkedin.com/in/singhajeet23/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我，或者在<a class="ae kq" href="https://github.com/AjeetSingh02" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上关注我。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es no"><img src="../Images/712ad07c6c03300c3e4e8cd021f01a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z0uC9eofQiqv7WgK-6riVw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">快乐学习</figcaption></figure></div></div>    
</body>
</html>