<html>
<head>
<title>ML19: The “Linear” in Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML19:线性回归中的“线性”</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ml19-4069a5f7a552?source=collection_archive---------22-----------------------#2021-01-06">https://medium.com/analytics-vidhya/ml19-4069a5f7a552?source=collection_archive---------22-----------------------#2021-01-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="ca61" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">这个“线性”是代表线性函数还是线性映射？</h2></div><p id="ad13" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">关键词</strong>:线性回归，线性函数，微积分，线性映射，线性变换，线性代数</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="d3a4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">具有<em class="jz">高次项(次数&gt; 1)、相互作用项、正则化、</em>和<em class="jz">逐步过程</em>的线性回归是<strong class="iy hi">廉价、省时、可解释且性能良好的</strong>。对于所有ML/DS项目来说，这是一个很好的起点<strong class="iy hi">和基准模型<strong class="iy hi">。</strong></strong></p><blockquote class="ka kb kc"><p id="f45c" class="iw ix jz iy b iz ja ii jb jc jd il je kd jg jh ji ke jk jl jm kf jo jp jq jr ha bi translated"><strong class="iy hi"> <em class="hh">大纲</em></strong><em class="hh"><br/>(1)</em><a class="ae kg" href="#249b" rel="noopener ugc nofollow"><em class="hh">简介</em></a><em class="hh"><br/>(2)</em><a class="ae kg" href="#4451" rel="noopener ugc nofollow"><em class="hh">关键证据</em></a><em class="hh"><br/>【3】</em><a class="ae kg" href="#d27b" rel="noopener ugc nofollow"><em class="hh">答案:线性映射</em></a><em class="hh"><br/>(4)</em><a class="ae kg" href="#c11e" rel="noopener ugc nofollow"><em class="hh">线性回归:一个便宜、省时的</em></a></p></blockquote></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="249b" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(1)引言</h1><h2 id="701c" class="kz ki hh bd kj la lb lc kn ld le lf kr jf lg lh kt jj li lj kv jn lk ll kx lm bi translated">1.线性函数</h2><ul class=""><li id="aa8e" class="ln lo hh iy b iz lp jc lq jf lr jj ls jn lt jr lu lv lw lx bi translated"><em class="jz">微积分</em>中的一个概念。</li><li id="7c0f" class="ln lo hh iy b iz ly jc lz jf ma jj mb jn mc jr lu lv lw lx bi translated">指1次或0次多项式，例如y = ax + b。</li><li id="a002" class="ln lo hh iy b iz ly jc lz jf ma jj mb jn mc jr lu lv lw lx bi translated">注意，在某些上下文中，一个<em class="jz">线性映射</em>也被称为<em class="jz">线性函数</em>【1】，尽管这实际上很少见。</li></ul><figure class="me mf mg mh fd mi er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es md"><img src="../Images/c647146d4ef5b9d7242e750a985b2597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P1iknstcmvL9_V85IbXIEA.png"/></div></div><figcaption class="mp mq et er es mr ms bd b be z dx translated">图1:来自维基百科的“线性函数”。[2]</figcaption></figure><h2 id="21d0" class="kz ki hh bd kj la lb lc kn ld le lf kr jf lg lh kt jj li lj kv jn lk ll kx lm bi translated">2.线性地图</h2><ul class=""><li id="c019" class="ln lo hh iy b iz lp jc lq jf lr jj ls jn lt jr lu lv lw lx bi translated"><em class="jz">线性代数</em>中的一个概念。</li><li id="278a" class="ln lo hh iy b iz ly jc lz jf ma jj mb jn mc jr lu lv lw lx bi translated"><em class="jz">线性映射</em>(也称为<em class="jz">线性映射</em>、<em class="jz">线性变换</em>或在某些上下文中的<em class="jz">线性函数</em>)是两个模块(例如，两个向量空间)之间的映射V → W，它保留加法和标量乘法的操作。如果一个<em class="jz">线性映射</em>是一个双射，那么它被称为线性同构。[1]</li><li id="40ad" class="ln lo hh iy b iz ly jc lz jf ma jj mb jn mc jr lu lv lw lx bi translated">我们从图1中看到,“线性”在数学中有双重含义。那么，线性回归中的“线性”代表什么？</li></ul><figure class="me mf mg mh fd mi er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mt"><img src="../Images/af06ac55081b3c06dc3dcbcb59b42bd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QMzChMFjEBSAlS0U84qH_w.png"/></div></div><figcaption class="mp mq et er es mr ms bd b be z dx translated">图2:来自维基百科的“线性地图”。[1]</figcaption></figure></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="4451" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(2)关键证据</h1><p id="c931" class="pw-post-body-paragraph iw ix hh iy b iz lp ii jb jc lq il je jf mu jh ji jj mv jl jm jn mw jp jq jr ha bi translated">让我们查一下世界各地统计系毕业生的著名教材——<strong class="iy hi"/><em class="jz"/><strong class="iy hi"><em class="jz"/></strong><em class="jz">(第4版。)</em>【3】—求解答。令人扫兴的是，就连这本教材也没有对线性回归中的“线性”是线性函数还是线性映射给出明确的解释；然而，我们可以在教科书中找到一些线索。</p><h2 id="39c8" class="kz ki hh bd kj la lb lc kn ld le lf kr jf lg lh kt jj li lj kv jn lk ll kx lm bi translated">1.证据1</h2><figure class="me mf mg mh fd mi er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mx"><img src="../Images/d2c910378d93a5320c0bc468e0c0efdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CcQEDhWXmBK7zvdoEcFfJQ.png"/></div></div><figcaption class="mp mq et er es mr ms bd b be z dx translated">图3:应用线性回归<strong class="bd kj"> </strong>(第4版。)，第vii页。[3]</figcaption></figure><p id="c79b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在目录中，第2章的标题是“简单线性回归”</p><h2 id="96ec" class="kz ki hh bd kj la lb lc kn ld le lf kr jf lg lh kt jj li lj kv jn lk ll kx lm bi translated">2.证据2</h2><figure class="me mf mg mh fd mi er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es my"><img src="../Images/705ce40ea7247a3aa35a5002de8cd505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GZ31JsElhRDAyHjO6rMJ3Q.png"/></div></div><figcaption class="mp mq et er es mr ms bd b be z dx translated">图4:应用线性回归(第4版。)，第51页。[3]</figcaption></figure><p id="2337" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里我们看到的“多元回归”实际上是“多元线性回归”注意在微积分中没有所谓的“多元线性函数”这个术语。</p><h2 id="e8e6" class="kz ki hh bd kj la lb lc kn ld le lf kr jf lg lh kt jj li lj kv jn lk ll kx lm bi translated">3.证据3</h2><figure class="me mf mg mh fd mi er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mz"><img src="../Images/0b6d6b80efcff3959e30c6928db82f61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UcS_QiXQL_zUJQyA8cvf2Q.png"/></div></div><figcaption class="mp mq et er es mr ms bd b be z dx translated">图5:应用线性回归<strong class="bd kj"> </strong>(第4版。)，第55页。[3]</figcaption></figure><p id="7f41" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里的描述和线性地图是完全一样的概念。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="d27b" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(3) <em class="na">答案:线性地图</em></h1><p id="053f" class="pw-post-body-paragraph iw ix hh iy b iz lp ii jb jc lq il je jf mu jh ji jj mv jl jm jn mw jp jq jr ha bi translated">因此，我们得到的答案是，线性回归中的“线性”正是线性代数中的线性映射！</p><p id="19e9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">此外，线性回归有两个分支——简单线性回归和多元线性回归。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="c11e" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(4) <em class="na">线性回归:一种廉价、省时、高效的模型</em></h1><h2 id="cd2c" class="kz ki hh bd kj la lb lc kn ld le lf kr jf lg lh kt jj li lj kv jn lk ll kx lm bi translated">1.含高阶项(阶数&gt; 1)和交互项的线性回归</h2><p id="44ae" class="pw-post-body-paragraph iw ix hh iy b iz lp ii jb jc lq il je jf mu jh ji jj mv jl jm jn mw jp jq jr ha bi translated">网上相当一部分ML/DS的书和文章误解线性回归，把它当成直线，即1次或0次的多项式；因此，他们错过了线性回归的力量。</p><p id="e7a0" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">事实上，线性回归<em class="jz">可以有更高阶的项(degree &gt; 1)和交互项</em>，这有助于比简单的直线更精确地拟合数据。</p><h2 id="c9bf" class="kz ki hh bd kj la lb lc kn ld le lf kr jf lg lh kt jj li lj kv jn lk ll kx lm bi translated">2.便宜、省时、可解释且性能良好</h2><p id="56c0" class="pw-post-body-paragraph iw ix hh iy b iz lp ii jb jc lq il je jf mu jh ji jj mv jl jm jn mw jp jq jr ha bi translated">具有高阶项(阶数&gt; 1)和交互项的线性回归是一个<em class="jz">便宜、省时、可解释且性能良好的模型</em>。线性回归是ML/DS项目中最基本也是最好的开始模型。</p><h2 id="b2c2" class="kz ki hh bd kj la lb lc kn ld le lf kr jf lg lh kt jj li lj kv jn lk ll kx lm bi translated">3.起点和基线模型</h2><p id="b528" class="pw-post-body-paragraph iw ix hh iy b iz lp ii jb jc lq il je jf mu jh ji jj mv jl jm jn mw jp jq jr ha bi translated">以线性回归为起点，我们可以<em class="jz">在建立更复杂的模型(例如SVM、RF、XGBT、ANN、CNN、RNN)之前，发现数据的特征并选择关键特征</em>，这可能比线性回归花费更多。</p><p id="4a94" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">此外，我们可以<em class="jz">以上面这个复杂的线性回归模型作为基线模型来评估每个复杂模型</em>(例如SVM、RF、XGBT、ANN、CNN、RNN)的性能。毕竟，根据奥卡姆剃刀定律(简约法则)，为什么要费心去使用复杂而耗时的模型，其精确度接近普通模型——线性回归？</p><p id="cbfe" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">另外，利用正则化(套索、脊、弹性网)可以帮助我们减轻线性回归的过度拟合，并产生更好的线性回归模型。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="891f" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(5)书籍误解了线性回归中的“线性”</h1><p id="8367" class="pw-post-body-paragraph iw ix hh iy b iz lp ii jb jc lq il je jf mu jh ji jj mv jl jm jn mw jp jq jr ha bi translated">可惜网上的ML/DS书籍和文章大多只讨论简单的线性回归。其中，有几本书“明确地”误解了线性回归(我高度怀疑许多作者只是简单地认为线性回归是一条直线，但我没有足够的证据)，它们的描述如下:</p><pre class="me mf mg mh fd nb nc nd ne aw nf bi"><span id="df99" class="kz ki hh nc b fi ng nh l ni nj">1. <strong class="nc hi">Kane, F. (2017). Hands-on Data Science and Python Machine Learning. Birmingham, UK: Packt Publishing.</strong><br/>"All it (linear regression) is, is fitting a straight line to a set of data points."</span><span id="3952" class="kz ki hh nc b fi nk nh l ni nj">2. <strong class="nc hi">Joshi, P. (2016). Python Machine Learning Cookbook. Birmingham, UK: Packt Publishing.</strong><br/>"You might say that there might be a curvy line out there that fits these points better, but linear regression doesn't allow this."</span></pre><p id="f1f6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">另一方面，有书“明确地”认识到线性回归的真正威力，提到线性回归中的高阶项(degree&gt;1)或交互项:</p><pre class="me mf mg mh fd nb nc nd ne aw nf bi"><span id="4077" class="kz ki hh nc b fi ng nh l ni nj">1. <strong class="nc hi">Albon, C. (2018). Machine Learning with Python Cookbook: Practical Solutions from Preprocessing to Deep Learning. California, CA: O’Reilly Media.</strong></span><span id="30b4" class="kz ki hh nc b fi nk nh l ni nj">2. <strong class="nc hi">VanderPlas, J. (2017). Python Data Science Handbook: Essential Tools for Working with Data.</strong> <strong class="nc hi">California, CA: O’Reilly Media.</strong></span><span id="8d07" class="kz ki hh nc b fi nk nh l ni nj">3. <strong class="nc hi">Hackeling, G. (2017). Mastering Machine Learning with scikit-learn (2nd ed.).  Birmingham, UK: Packt Publishing.</strong></span></pre></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="ab0d" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(6)结论</h1><ol class=""><li id="f2e0" class="ln lo hh iy b iz lp jc lq jf lr jj ls jn lt jr nl lv lw lx bi translated">线性回归中的“线性”是指<strong class="iy hi"> <em class="jz"> </em> </strong> <em class="jz">线性代数</em>中的<strong class="iy hi"> <em class="jz">线性映射</em> </strong>而不是<em class="jz">微积分</em>中的<em class="jz">线性函数</em>(1或0次多项式)。</li><li id="f810" class="ln lo hh iy b iz ly jc lz jf ma jj mb jn mc jr nl lv lw lx bi translated">具有<em class="jz">高次项(次数&gt; 1)、相互作用项、正则化、</em>和<em class="jz">逐步过程</em>的线性回归<strong class="iy hi">肯定优于</strong>线性回归，例如z = ax + by + c</li><li id="d25e" class="ln lo hh iy b iz ly jc lz jf ma jj mb jn mc jr nl lv lw lx bi translated">具有<em class="jz">高阶项(degree &gt; 1)、交互项、正则化、</em>和<em class="jz">逐步过程</em>的线性回归<strong class="iy hi">便宜、省时、可解释且性能良好</strong>。对于所有的ML/DS项目来说，在建立更复杂的模型(例如SVM、RF、XGBT、ANN、CNN、RNN)之前发现数据的特征并选择关键特征是一个很好的起点。</li><li id="daf5" class="ln lo hh iy b iz ly jc lz jf ma jj mb jn mc jr nl lv lw lx bi translated">而且，我们可以<em class="jz">以上面这个复杂的线性回归模型作为</em> <strong class="iy hi"> <em class="jz">基线模型</em> </strong> <em class="jz">来评估各个复杂模型</em>(如SVM、RF、XGBT、ANN、CNN、RNN)的表现。所谓的基线模型不应该是像z = ax + by + c或y= ax + b这样过于普通的模型。</li><li id="c803" class="ln lo hh iy b iz ly jc lz jf ma jj mb jn mc jr nl lv lw lx bi translated">读者可以查看ML20和ML21，分别使用R &amp; Python进行实际的线性回归实现。</li></ol><div class="nm nn ez fb no np"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/ml21-8fb43cc5082d"><div class="nq ab dw"><div class="nr ab ns cl cj nt"><h2 class="bd hi fi z dy nu ea eb nv ed ef hg bi translated">ML21:使用Python进行线性回归</h2><div class="nw l"><h3 class="bd b fi z dy nu ea eb nv ed ef dx translated">更高级的术语和交互</h3></div><div class="nx l"><p class="bd b fp z dy nu ea eb nv ed ef dx translated">medium.com</p></div></div><div class="ny l"><div class="nz l oa ob oc ny od mn np"/></div></div></a></div><div class="nm nn ez fb no np"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/ml20-abb54a435b3"><div class="nq ab dw"><div class="nr ab ns cl cj nt"><h2 class="bd hi fi z dy nu ea eb nv ed ef hg bi translated">ML20:带R的逐步线性回归</h2><div class="nw l"><h3 class="bd b fi z dy nu ea eb nv ed ef dx translated">更高级的术语和交互</h3></div><div class="nx l"><p class="bd b fp z dy nu ea eb nv ed ef dx translated">medium.com</p></div></div><div class="ny l"><div class="oe l oa ob oc ny od mn np"/></div></div></a></div></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="7ca5" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(7)参考文献</h1><p id="4b54" class="pw-post-body-paragraph iw ix hh iy b iz lp ii jb jc lq il je jf mu jh ji jj mv jl jm jn mw jp jq jr ha bi translated">[1]维基百科(身份不明)。线性地图。从https://en.wikipedia.org/wiki/Linear_map<a class="ae kg" href="https://en.wikipedia.org/wiki/Linear_map" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="c8cd" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[2]维基百科(身份不明)。线性函数。从https://en.wikipedia.org/wiki/Linear_function<a class="ae kg" href="https://en.wikipedia.org/wiki/Linear_function" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="2b01" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[3]韦斯伯格，S. (2014年)。应用线性回归(第4版。).新泽西州，约翰·威利父子公司。</p><p id="3ba1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[4]阿尔邦，C. (2018)。机器学习与Python食谱:从预处理到深度学习的实用解决方案。加利福尼亚州:奥赖利媒体。</p><p id="67fc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[5]范德普拉斯，J. (2017年)。Python数据科学手册:处理数据的基本工具。加利福尼亚州:奥赖利媒体。</p><p id="bebc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[6]哈克林，G. (2017)。用scikit-learn掌握机器学习(第2版。).英国伯明翰:Packt出版公司。</p><p id="ae08" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[7]凯恩，F. (2017)。动手数据科学和Python机器学习。英国伯明翰:Packt出版公司。</p><p id="d932" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[8]乔希，P. (2016)。Python机器学习食谱。英国伯明翰:Packt出版公司。</p></div></div>    
</body>
</html>