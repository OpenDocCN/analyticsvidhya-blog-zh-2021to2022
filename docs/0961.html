<html>
<head>
<title>A Deep Dive Into Vanilla Policy Gradients</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对普通政策梯度的深入探究</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-deep-dive-into-vanilla-policy-gradients-3a79a95f3334?source=collection_archive---------5-----------------------#2021-02-08">https://medium.com/analytics-vidhya/a-deep-dive-into-vanilla-policy-gradients-3a79a95f3334?source=collection_archive---------5-----------------------#2021-02-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/fc58f378b5c1fecaa9f32b85bd06d3c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*j0Ax8GuxbLiZdoKR"/></div></div></figure><p id="d865" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">电脑征服了人类——当然是在游戏中。</p><p id="929f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">很难相信计算机能像人类一样玩游戏，通过动作推理，并与游戏中的物体互动。但是电脑在游戏中的威力已经被证明了；AI已经掌握了像<a class="ae jo" href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" rel="noopener ugc nofollow" target="_blank">围棋</a>和<a class="ae jo" href="https://openai.com/projects/five/" rel="noopener ugc nofollow" target="_blank"> Dota 2 </a>这样的游戏，在这两个游戏中击败了顶级职业选手。</p><p id="f018" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">观察这些系统的工作留下了许多问题。这些系统实际上是如何运作的？他们是如何在没有任何人类帮助的情况下学会玩电子游戏的？要回答这样的问题，我们需要一头扎进<strong class="is hj">强化学习这个领域。</strong></p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jp"><img src="../Images/05183d9a9fdf5b0d71b5fb84f03e6f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aYr6LHYJWUjWcHwo"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated"><strong class="bd jy">强化学习</strong>是什么让这个AI把雅达利突围打得这么好！<a class="ae jo" href="https://gsurma.medium.com/atari-reinforcement-learning-in-depth-part-1-ddqn-ceaa762a546f" rel="noopener">来源</a></figcaption></figure><h1 id="904a" class="jz ka hi bd jy kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">什么是强化学习？</strong></h1><p id="255b" class="pw-post-body-paragraph iq ir hi is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated">强化学习允许计算机通过<em class="lb">强化</em>积极行为来自主学习执行任务。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/01aa77ebb467456c372c1482249fec1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ge2UKoI4TKK1GGap"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">通用强化学习框架。<a class="ae jo" href="https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="6737" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">强化学习包括两个项目，一个<strong class="is hj">代理</strong>和一个<strong class="is hj">环境。</strong>继续玩视频游戏的运行示例，代理是玩游戏的模型的一部分，而环境是游戏本身。</p><p id="9ecf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">代理通过<strong class="is hj">动作</strong>与环境交互。在游戏中向前移动角色或发射武器都是代理可以采取的可能行动。</p><p id="caa7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了决定采取什么行动，代理被给予环境的状态。我们的视频游戏代理从游戏中以单个帧的形式接收状态，分析它们以确定响应给定状态的理想动作。如果行动产生了积极的结果(如分数增加)，则给予代理<strong class="is hj">奖励</strong>，鼓励类似于产生奖励的行动。导致负面结果的行为通常会受到惩罚。</p><p id="73fc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">强化学习模型中的代理是一个被称为<strong class="is hj">策略网络</strong>的神经网络。一个状态被输入到一个策略网络中，该网络输出给定该状态下要采取的理想动作。</p><p id="24e7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lb">注意:如果你不熟悉神经网络，就把它们想象成一个数学函数，可以被训练来执行某些任务。在这里阅读更多关于他们的</em><a class="ae jo" href="https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9" rel="noopener" target="_blank"><em class="lb"/></a><em class="lb">。</em></p><h1 id="dc8c" class="jz ka hi bd jy kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">培训政策网络</strong></h1><p id="ca37" class="pw-post-body-paragraph iq ir hi is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated">像其他神经网络一样，策略网络必须经过训练才能决定适当的行动。</p><p id="36a1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">网络开始时是随机初始化的，这意味着模型没有经过训练。因为未经训练，网络产生随机动作。例如，一个未经训练的视频游戏代理可能漫无目的地移动它的角色而得不到分数。</p><p id="bd86" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，经过多次博弈后，代理人最终选择了一系列导致积极结果的行动。这些行动得到积极的强化，推动网络选择产生类似积极结果的行动。相反，导致负回报的行为会被消极地强化，阻止算法采取这样的行为。</p><p id="9e93" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这种强化框架引导网络执行更明智的行动，从而使策略获得更多回报。在不断重复这一过程后，政策获得了对什么样的行动序列导致高回报的理解。</p><p id="7e4a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个过程被称为<strong class="is hj">标准政策梯度</strong> <strong class="is hj">算法</strong>。该策略被训练来采取培养更高回报的行动，并避免减少回报的行动。</p><h1 id="db9b" class="jz ka hi bd jy kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">普通策略梯度算法内部</strong></h1><p id="70b2" class="pw-post-body-paragraph iq ir hi is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated">普通策略梯度算法背后的思想相当简单，但它需要许多数学组件才能正常工作。深入研究这些组件有助于理解算法。</p><p id="2ce2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">训练策略总是从代理在环境中运行并接收一系列状态开始，为每个状态决定适当的动作。这一系列的状态和动作存储在一个名为<strong class="is hj">轨迹</strong>的列表中。</p><p id="d910" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">轨迹中的信息是计算策略需要学习的其余组件的基础。其中一个组成部分是奖励函数。</p><h2 id="e5f8" class="ld ka hi bd jy le lf lg ke lh li lj ki jb lk ll km jf lm ln kq jj lo lp ku lq bi translated"><strong class="ak">奖励功能</strong></h2><p id="ecf4" class="pw-post-body-paragraph iq ir hi is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated"><strong class="is hj">奖励函数</strong>计算从轨迹中的动作获得的奖励。因为代理人的表现完全依赖于从其行为中获得的报酬，所以拥有尽可能好的报酬函数是必不可少的。</p><p id="7a2e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于可能的奖励函数，存在一些选择。</p><p id="19e8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">有限期限未贴现回报</strong>简单地总结了一系列行动的回报。因为它只是简单的奖励相加，所以实现起来非常简单和容易，但是替代的奖励函数可能更有效。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/1699c8c626500bf9f18d966177732225.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/0*0ApGyNKcHcEHxTRh"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">有限期未贴现回报函数公式。<a class="ae jo" href="https://spinningup.openai.com/en/latest/algorithms/vpg.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="6bd2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">无限期贴现回报</strong>与有限期未贴现回报非常相似，但有一个新的系数，称为<strong class="is hj">贴现因子。</strong>折扣系数对应于这些奖励在未来多长时间收集，是一个介于0和1之间的数字。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/03af8f7cb922a7bb04651b54eb775b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/0*t3vsLAY01iZ5S4wC"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">无限期贴现收益函数公式。<a class="ae jo" href="https://spinningup.openai.com/en/latest/algorithms/vpg.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="6eeb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这两种奖励函数，尽管差别很大，但计算的是同一件事——从一系列行动中获得的总奖励。</p><h2 id="17f9" class="ld ka hi bd jy le lf lg ke lh li lj ki jb lk ll km jf lm ln kq jj lo lp ku lq bi translated"><strong class="ak">价值函数</strong></h2><p id="4b3b" class="pw-post-body-paragraph iq ir hi is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated">算法使用<strong class="is hj">值函数</strong>来预测一组动作产生的奖励。</p><p id="123b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">普通策略梯度算法使用一个<strong class="is hj"> on-policy value函数</strong>，这实质上意味着使用从与代理的最新交互中收集的经验来更新策略网络。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/744ab6f2929a8776ee84c3fef8d45554.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/0*B5SKX0glT1lqX0fL"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">普通策略梯度算法中使用的主值函数。<a class="ae jo" href="https://spinningup.openai.com/en/latest/algorithms/vpg.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="be82" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“E”对应的是预期回报，括号内的“s”项对应的是开始状态。将这些放在一起，价值函数试图找到如果代理人从一个特定的状态(s0)开始并遵循一个给定的策略时的期望回报(E)。</p><p id="add3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">普通政策梯度还需要第二个函数来计算优势(我们很快会谈到这一点)。该功能被称为<strong class="is hj">政策动作值功能。</strong>类似于保单价值函数，它计算预期回报，但也利用其等式中的行动。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/60ec7df1f4a8ab9bd9ddcae5eb034d62.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/0*9mEQ7SpCB9hkqcwU"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">普通策略梯度算法使用的第二个Q函数。<a class="ae jo" href="https://spinningup.openai.com/en/latest/algorithms/vpg.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="19f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">同样,“E”对应于预期的奖励,“s0”对应于开始状态。然而,‘A0’项对应于一个随机的开始动作，从而使该函数能够使用来自动作的信息。</p><p id="2df3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这两个函数构成了普通保单梯度计算的必要值方程。</p><h2 id="0a69" class="ld ka hi bd jy le lf lg ke lh li lj ki jb lk ll km jf lm ln kq jj lo lp ku lq bi translated"><strong class="ak">优势功能</strong></h2><p id="e020" class="pw-post-body-paragraph iq ir hi is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated">出门买车，往往不是心血来潮做的决定。你比较不同的汽车，试图找出买一辆车比买另一辆车的优点。</p><p id="cabc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">强化学习利用了一个非常相似的概念，只是该算法不是汽车，而是比较不同的动作。为此，该算法采用了一个<strong class="is hj">优势函数。</strong></p><p id="a39e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">算法无法像人类一样比较和对比想法；它必须找到一种量化优势的方法。advantage函数使用一个相当简单的公式来实现这一点。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/96c6d234715b6e68ac15a84ed923a6b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/0*0HemY6wAX8NEajpR"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">优势函数公式。<a class="ae jo" href="https://spinningup.openai.com/en/latest/algorithms/vpg.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="5a9a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过从政策价值函数中减去政策行动价值函数，可以简单地计算出这种优势。</p><p id="6273" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">优势、价值和回报函数是理解普通策略梯度算法的内部机制所必需的主要拼图。让我们把这些碎片拼在一起。</p><h2 id="8c0b" class="ld ka hi bd jy le lf lg ke lh li lj ki jb lk ll km jf lm ln kq jj lo lp ku lq bi translated"><strong class="ak">拼起来</strong></h2><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/9fbf1d4c796575a8391618d069d13f7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YPFg1RoDB7hVhUh8"/></div></div></figure><p id="67ba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在使用前面讨论的功能之前，一些基本步骤是必要的。</p><p id="1e63" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">策略和值函数需要<strong class="is hj">参数</strong>才能起作用，这些参数以随机数开始。然而，随着策略和值被训练，这些参数被逐渐调整以允许两个网络更好地执行。</p><p id="5ee9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">定义参数后，算法开始训练。</p><p id="9582" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该策略在环境中运行，并收集一系列状态和动作，将其存储在轨迹中。奖励函数然后使用这个轨迹来计算奖励。advantage函数使用value和action-value函数来估计报酬。</p><p id="18bc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下一步计算策略梯度，然后在后续步骤中使用该梯度来调整策略网络的参数。只需将这两个步骤视为算法的“学习步骤”，策略网络在其中学习做出更好的决策。</p><p id="bf4b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后一步涉及更新价值函数的参数，允许它创建更好的回报预期。在该步骤之后，模型返回到收集轨迹的初始步骤，并且该过程再次开始，<strong class="is hj">不断重复，直到策略网络能够在其环境中做出好的决策。</strong></p><p id="6b80" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">普通策略梯度算法只是强化学习的广泛范围中的一种算法。其他算法如<a class="ae jo" href="https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank"> Q-Learning </a>和<a class="ae jo" rel="noopener" href="/ai³-theory-practice-business/reinforcement-learning-part-7-a-brief-introduction-to-deep-q-networks-aa45314a2ae"> Deep Q-Networks </a>也取得了显著的成就！然而，不管各种算法的特性如何，有一点是肯定的；强化学习将继续存在，并且随着它的改进，无疑将进一步改变人工智能领域。</p><p id="abff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lb">查看</em> <a class="ae jo" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> OpenAI的Spinning Up网站</em> </a> <em class="lb">了解更多关于数学函数及其推导的详细解释。这些信息大部分来自他们的网页；它提供了强化学习概念的详细解释，如果你想深入研究强化学习，我强烈推荐你访问他们的网站！</em></p></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="e25a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lb">谢谢你看完！别忘了留下👏也是！</em></p><p id="135e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lb">简单介绍一下我——我今年17岁，非常喜欢颠覆性技术，主要是人工智能。如果你喜欢这篇文章或者想谈论有趣的深度学习/机器学习项目、研究论文或想法，请通过我的</em><a class="ae jo" href="https://www.linkedin.com/in/ayan-nair-31388a1b7/" rel="noopener ugc nofollow" target="_blank"><em class="lb">LinkedIn</em></a><em class="lb">，</em><a class="ae jo" href="https://www.instagram.com/nair.030/" rel="noopener ugc nofollow" target="_blank"><em class="lb">insta gram</em></a><em class="lb">或电子邮件(ayan.aji.nair@gmail.com)给我发消息！你也可以通过我的每月简讯获得我的酷项目的更新和我写的新文章——在这里订阅</em><a class="ae jo" href="https://www.subscribepage.com/a8y3j1" rel="noopener ugc nofollow" target="_blank"><em class="lb"/></a><em class="lb">！</em></p></div></div>    
</body>
</html>