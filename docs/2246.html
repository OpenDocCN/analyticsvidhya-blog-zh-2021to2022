<html>
<head>
<title>Stochastic Gradient Descent in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的随机梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/stochastic-gradient-descent-in-deep-learning-b19c038ec025?source=collection_archive---------8-----------------------#2021-04-14">https://medium.com/analytics-vidhya/stochastic-gradient-descent-in-deep-learning-b19c038ec025?source=collection_archive---------8-----------------------#2021-04-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f2a9" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">神经网络通常由数百万个权重组成，我们需要找到这些权重的正确值。使用可用数据优化该网络需要仔细考虑要选择的优化器。在这篇文章中，我将讨论梯度下降和随机梯度下降(SGD ),以及为什么SGD在深度学习中是首选</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/98097e6b3249d751cc853f4cfc0decda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*flLZbs75V56F0Pv-jYfCNA.png"/></div></figure><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="e1ba" class="jr js hi jn b fi jt ju l jv jw">for k in [1, number_iterations]:<br/>    X(k+1) = X(k) - α ▽L(X(k))</span></pre><p id="2b65" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">这对于适当选择凸函数的α很有效。但是，深度神经网络最理想的特性之一是它们是通用逼近器。这意味着它们也应该能够覆盖非凸函数。非凸函数的问题是，您的初始猜测可能不在全局最小值附近，梯度下降可能会收敛到局部最小值。考虑以下情况:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jx"><img src="../Images/ead70e9dd7f83996b38f3691b14c0eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*yosWhXILiWL1nVcttPQyzA.png"/></div></figure><h2 id="a219" class="jr js hi bd jy jz ka kb kc kd ke kf kg ir kh ki kj iv kk kl km iz kn ko kp kq bi translated"><strong class="ak"> <em class="if">但是等等，真的是在寻找深度学习的全局极小值吗？</em>T3】</strong></h2><p id="369d" class="pw-post-body-paragraph ig ih hi ii b ij kr il im in ks ip iq ir kt it iu iv ku ix iy iz kv jb jc jd hb bi translated">我们正在优化损失或成本函数，该函数大部分时间代表实际产出和预测之间的一些距离。我们希望最小化损失函数，但同时我们不希望它太接近于零，因为这通常会导致过拟合。我们希望它能很好地处理看不见的数据，因此需要推广。<strong class="ii hj">我们在这里寻找的是平整度。</strong></p><p id="d394" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="kw">好的局部极小值对坏的局部极小值</em> <br/> </strong>我们不是期望全局极小值，而是寻找“好的局部极小值”而不是“坏的局部极小值”。好的局部极小值通常是指更平坦的极小值。</p><p id="c955" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="kw">你是怎么定义平坦度的？</em> <br/> </strong>嗯，光看曲线就能非常直观的理解2D或者3D。一般来说，平坦度与函数的Hessian 的较小特征值有关。特征值越大，曲率越大，临界点成为尖锐极小值的几率越大。</p><p id="7f8f" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="kw">随机梯度下降:</em> <br/> </strong>随机梯度下降(SGD)通过抽取随机样本并计算每次迭代的损失，取代了计算整个数据集平均损失的高成本操作。与梯度下降相比，这又改变了收敛行为。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="e9df" class="jr js hi jn b fi jt ju l jv jw">for k in [1, number_iterations]:<br/>    pick a random datapoint di;<br/>    calculate loss Li<br/>    X(k+1) = X(k) - α ▽Li(X(k))</span></pre><p id="16d9" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">在之前的工作中，Hinton &amp; Van Camp(1993)，Hochreiter &amp; Schmidhuber提出了一个论点，即平极小需要更少的信息来描述，因此应该比锐极小更好地概括。</p><p id="ef23" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">尖锐的极小值在训练数据上几乎没有不正确的预测。然而，网络参数的微小变化会使输出发生很大变化。对于分类任务，这意味着边界非常接近这些点。相比之下，在平坦最小值中，边界与点之间的距离是安全的，网络参数的微小变化不会导致精度下降，从而使其成为我们试图逼近的实际函数的更好概括。看看下面这张纸上的形象化:<a class="ae kx" href="https://arxiv.org/abs/1906.03291" rel="noopener ugc nofollow" target="_blank"><strong class="ii hj"/></a></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ky"><img src="../Images/5a30c0c53422ce2562d7dc63eb7e127d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GWa70nZYV06u6IUxTskO3A.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">左:平坦极小值上的分类器边界；右图:尖锐极小值上的分类器边界</figcaption></figure><p id="a475" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">在我们进入SGD收敛到平坦最小值之前，让我们理解下面的全局优化技术。</p><p id="3ca5" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="kw">模拟退火</em> <br/> </strong>这是一种全局优化技术。模拟退火通常采取与梯度方向不匹配的步骤。采取这些不正常的步骤依赖于一种叫做能量的东西。开始时有更大的能量，使它采取不正常的步骤。这些异常的步骤是非常必要的，因为它们可以很好地走出局部极小值，并探索全局极小值的前景。能量随着迭代次数而减少，并且算法趋向于采取较少的异常步骤，并且最终收敛到最小值。与梯度下降法相比，这种方法陷入局部极小值的可能性很小。</p><p id="7728" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">为了从混乱的实现中抽象出来(因为实际的实现更详细)，假设一架直升机被分配了寻找并降落在山脉最深的山谷的任务，并且被给予了有限的燃料。直升机一开始肯定会随意探索山脉，但随着时间的推移，它会尽量避免随意探索，并会尽量深入山谷，这是因为考虑到了燃料限制。最初的任意探索使直升机有可能走出局部极小值，并探索超出此范围的下降。</p><p id="7ef2" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">尽管SGD寻找平坦的最小值而不是全局最小值，但是SGD和模拟退火之间有一些相似之处，这使得它们可以避开局部最小值。</p><p id="7fde" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="kw">为什么SGD达到一个平坦的极小值？</em> <br/> </strong>在初始迭代处，SGD携带大量噪声(类似于模拟退火)，并且梯度在每次迭代时可以变化很大。这种行为让SGD算法像模拟退火一样。在初始步骤中非常随机的损失携带能量以很好地脱离局部最小值。平坦的极小值有更大的区域可以探索。<strong class="ii hj">想象一下直升机试图在巨大的山脉中寻找一个降落点。很明显，面积较大的山谷(因此平坦程度较高)有更好的机会容纳一个停靠点。</strong></p><p id="3cca" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">使用SGD还有其他一些优点:</p><ul class=""><li id="2eec" class="lh li hi ii b ij ik in io ir lj iv lk iz ll jd lm ln lo lp bi translated">当例子相似时，SGD收敛很快。如果有重复或相似的数据点，SGD可以收敛得更快，因为过一段时间后，它会优化它已经看到的点。</li><li id="f23e" class="lh li hi ii b ij lq in lr ir ls iv lt iz lu jd lm ln lo lp bi translated">SGD可以在飞行中训练。如果您的数据不容易获得和流入，可以使用SGD。</li><li id="00fd" class="lh li hi ii b ij lq in lr ir ls iv lt iz lu jd lm ln lo lp bi translated">SGD的一个重要好处是每次迭代只处理一个数据点。当大型数据集无法将所有数据保存到RAM <strong class="ii hj"> ( </strong>我没有使用过这样的数据集时，这真的很有帮助。可能我还没有处理过足够大的数据集<strong class="ii hj"> ) </strong>。</li></ul><p id="f813" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="kw">参考文献:</em> </strong></p><ol class=""><li id="ce95" class="lh li hi ii b ij ik in io ir lj iv lk iz ll jd lv ln lo lp bi translated">黄、、戈德布拉姆、福尔、特里、黄和戈德斯坦。通过可视化理解概括。2019</li><li id="2ae5" class="lh li hi ii b ij lq in lr ir ls iv lt iz lu jd lv ln lo lp bi translated">辛顿&amp;范·坎普(1993)</li><li id="b14e" class="lh li hi ii b ij lq in lr ir ls iv lt iz lu jd lv ln lo lp bi translated">施密特·胡伯。平极小1997</li></ol></div></div>    
</body>
</html>