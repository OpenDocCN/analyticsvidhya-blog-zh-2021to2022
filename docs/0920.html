<html>
<head>
<title>What is RESUNET — Idiot Developer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是RESUNET——白痴开发者</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-is-resunet-idiot-developer-9a28762f14b4?source=collection_archive---------11-----------------------#2021-02-06">https://medium.com/analytics-vidhya/what-is-resunet-idiot-developer-9a28762f14b4?source=collection_archive---------11-----------------------#2021-02-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="afbb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">RESUNET指深度残余UNET。它是由张等人开发的一种用于语义切分的编解码体系结构。它最初用于遥感图像分析领域的高分辨率航空图像的道路提取。后来，它被研究人员用于多种其他应用，如<a class="ae jc" href="https://idiotdeveloper.com/polyp-segmentation-using-unet-in-tensorflow-2" rel="noopener ugc nofollow" target="_blank">息肉分割</a>，脑肿瘤分割，人类图像分割，等等。</p><p id="b1c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">原文:<a class="ae jc" href="https://arxiv.org/pdf/1711.10684.pdf" rel="noopener ugc nofollow" target="_blank">用深度残留U网提取道路</a></p><p id="a26c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">RESUNET是一个完全卷积的神经网络，旨在用更少的参数获得高性能。这是对现有的<a class="ae jc" href="https://idiotdeveloper.com/what-is-unet/" rel="noopener ugc nofollow" target="_blank"> UNET </a>架构的改进。RESUNET利用了<a class="ae jc" href="https://idiotdeveloper.com/what-is-unet/" rel="noopener ugc nofollow" target="_blank"> UNET </a>架构和深度剩余学习的优势。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/4a84b2e3f15f4e58feeb43af42236158.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/0*Q9iM4_vhdCYDlTsO.png"/></div></figure><h1 id="3e30" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">RESUNET的优势</h1><ul class=""><li id="a216" class="kj kk hh ig b ih kl il km ip kn it ko ix kp jb kq kr ks kt bi translated">残余块的使用有助于建立更深的网络，而不用担心消失梯度或爆炸梯度的问题。这也有助于网络的简单训练。</li><li id="f30f" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated">RESUNET中丰富的跳跃连接有助于不同层之间更好的信息流动，这有助于训练时更好的梯度流动(反向传播)。</li></ul><h1 id="6e35" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">RESUNET的整体架构</h1><p id="4648" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip kz ir is it la iv iw ix lb iz ja jb ha bi translated">RESUNET由编码网络、解码网络和连接这两个网络的网桥组成，就像一个U-Net。U-Net使用两个3×3卷积，每个卷积后面都有一个ReLU激活函数。在RESUNET的情况下，这些层由预激活的残余块代替。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lc"><img src="../Images/ceebe3033dba09de1ecf4a38d31a0a83.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/0*buJseBTGbCpHkC8r.png"/></div></figure><h1 id="2217" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">编码器</h1><p id="21ab" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip kz ir is it la iv iw ix lb iz ja jb ha bi translated">编码器获取输入图像，并将其通过不同的编码器模块，这有助于网络学习抽象表示。编码器由三个编码器模块组成，使用预激活的残差模块构建。每个编码器块的输出充当相应解码器块的跳跃连接。</p><p id="f435" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了减少特征图的空间维度(高度和宽度)，第一个3×3卷积层在第二个和第三个编码器块中使用步长2。跨距值2将空间维度减少一半，即从256减少到128。</p><h1 id="f130" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">桥</h1><p id="1956" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip kz ir is it la iv iw ix lb iz ja jb ha bi translated">该桥还包括一个跨距值为2的预激活剩余块。</p><h1 id="8123" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">解码器</h1><p id="3e8d" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip kz ir is it la iv iw ix lb iz ja jb ha bi translated">解码器采用来自桥的特征图和来自不同编码器块的跳过连接，并学习更好的语义表示，用于生成分段掩码。</p><p id="9103" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">解码器由三个解码器块组成，并且在每个块之后，特征图的空间维度加倍，并且特征通道的数量减少。</p><p id="849a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个解码器模块从2×2上采样开始，这使得特征图的空间维度加倍。接下来，将这些特征映射与来自编码器模块的适当跳跃连接连接起来。这些跳跃连接有助于解码器模块获得编码器网络学习到的特征。此后，来自连接操作的特征映射通过预激活的残差块。</p><p id="1477" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后一个解码器的输出通过一个1×1的sigmoid激活卷积。sigmoid激活函数给出了表示逐像素分类的分割掩模。</p><h1 id="02d2" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">张量流(Keras)码</h1><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="ca5d" class="li jm hh le b fi lj lk l ll lm">from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate, Input<br/>from tensorflow.keras.models import Model<br/><br/>def batchnorm_relu(inputs):<br/>    """ Batch Normalization &amp; ReLU """<br/>    x = BatchNormalization()(inputs)<br/>    x = Activation("relu")(x)<br/>    return x<br/><br/>def residual_block(inputs, num_filters, strides=1):<br/>    """ Convolutional Layers """<br/>    x = batchnorm_relu(inputs)<br/>    x = Conv2D(num_filters, 3, padding="same", strides=strides)(x)<br/>    x = batchnorm_relu(x)<br/>    x = Conv2D(num_filters, 3, padding="same", strides=1)(x)<br/><br/>    """ Shortcut Connection (Identity Mapping) """<br/>    s = Conv2D(num_filters, 1, padding="same", strides=strides)(inputs)<br/><br/>    """ Addition """<br/>    x = x + s<br/>    return x<br/><br/>def decoder_block(inputs, skip_features, num_filters):<br/>    """ Decoder Block """<br/><br/>    x = UpSampling2D((2, 2))(inputs)<br/>    x = Concatenate()([x, skip_features])<br/>    x = residual_block(x, num_filters, strides=1)<br/>    return x<br/><br/>def build_resunet(input_shape):<br/>    """ RESUNET Architecture """<br/><br/>    inputs = Input(input_shape)<br/><br/>    """ Endoder 1 """<br/>    x = Conv2D(64, 3, padding="same", strides=1)(inputs)<br/>    x = batchnorm_relu(x)<br/>    x = Conv2D(64, 3, padding="same", strides=1)(x)<br/>    s = Conv2D(64, 1, padding="same")(inputs)<br/>    s1 = x + s<br/><br/>    """ Encoder 2, 3 """<br/>    s2 = residual_block(s1, 128, strides=2)<br/>    s3 = residual_block(s2, 256, strides=2)<br/><br/>    """ Bridge """<br/>    b = residual_block(s3, 512, strides=2)<br/><br/>    """ Decoder 1, 2, 3 """<br/>    x = decoder_block(b, s3, 256)<br/>    x = decoder_block(x, s2, 128)<br/>    x = decoder_block(x, s1, 64)<br/><br/>    """ Classifier """<br/>    outputs = Conv2D(1, 1, padding="same", activation="sigmoid")(x)<br/><br/>    """ Model """<br/>    model = Model(inputs, outputs, name="RESUNET")<br/><br/>    return model<br/><br/>if __name__ == "__main__":<br/>    shape = (224, 224, 3)<br/>    model = build_resunet(shape)<br/>    model.summary()</span></pre></div><div class="ab cl ln lo go lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ha hb hc hd he"><p id="a609" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lu">原载于2021年2月6日</em><a class="ae jc" href="https://idiotdeveloper.com/what-is-resunet/" rel="noopener ugc nofollow" target="_blank"><em class="lu">【https://idiotdeveloper.com】</em></a><em class="lu">。</em></p></div></div>    
</body>
</html>