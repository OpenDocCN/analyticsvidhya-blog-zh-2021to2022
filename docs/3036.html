<html>
<head>
<title>Linear Dimensionality Reduction — PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性降维—主成分分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-dimensionality-reduction-pca-7128e6e437ca?source=collection_archive---------13-----------------------#2021-05-30">https://medium.com/analytics-vidhya/linear-dimensionality-reduction-pca-7128e6e437ca?source=collection_archive---------13-----------------------#2021-05-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/1a4ccc43ec523f72393fbcc7b353a264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KysvstURlHMEofxcdQ2X9Q.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Haneul Kim摄</figcaption></figure><h1 id="3524" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">目录</h1><blockquote class="jr js jt"><p id="7454" class="ju jv jw jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ha bi translated">1.介绍</p><p id="8bde" class="ju jv jw jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ha bi translated">2.数学推导</p><p id="cd2d" class="ju jv jw jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ha bi translated">3.结论</p></blockquote><p id="9f03" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">前面简要介绍了各种DRT，本章我们将详细介绍一种线性降维技术的数学工作细节，这种技术可能是最流行的一种，叫做<strong class="jx hi">主成分分析</strong>。</p><p id="be84" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">通常，我倾向于以自下而上的方式工作，机器学习项目的简单性，因此无论是模型本身还是预处理步骤，我都从线性函数开始，除非我事先有关于底层数据结构的信息。</p><h1 id="a30c" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">介绍</h1><p id="afcc" class="pw-post-body-paragraph ju jv hh jx b jy kw ka kb kc kx ke kf kt ky ki kj ku kz km kn kv la kq kr ks ha bi translated">PCA的目标是找到保持原始数据最大方差的<strong class="jx hi">正交基(又称主成分/特征向量)</strong>。这是一种特征提取技术，它不是选择特征子集来降低维数，而是将数据投射到不同的基中。</p><p id="7126" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated"><strong class="jx hi">正交性</strong>简单来说就是指任何互相成90度的向量。</p><p id="2270" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated"><strong class="jx hi">基</strong>:向量空间的基是跨越整个空间的线性独立向量的集合。换句话说，当你去掉一个向量，它减少了维数，那么这个向量就是基向量。在欧几里得空间中，指向上方和右侧的单位向量是基向量，因为通过进行线性变换，我们可以到达2-d欧几里得空间中的任何点，并且移除一个基向量将维度降低到1-d。</p><p id="f676" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">出于演示目的，我们将创建非常简单的二维数据集。请记住，降维技术并不关心y值(目标)</p><figure class="lb lc ld le fd ii"><div class="bz dy l di"><div class="lf lg l"/></div></figure><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lh"><img src="../Images/c6cc9727741898883e9fe6ff2c64fc4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-npu2_TUwdA2fMPCMjNduw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">以上测向标绘</figcaption></figure><p id="f4f8" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">为了找到最大方差保持正交基，我们将遵循以下步骤:</p><ol class=""><li id="d475" class="li lj hh jx b jy jz kc kd kt lk ku ll kv lm ks ln lo lp lq bi translated">将质心移到原点。</li><li id="cb29" class="li lj hh jx b jy lr kc ls kt lt ku lu kv lv ks ln lo lp lq bi translated">计算协方差矩阵</li><li id="30a6" class="li lj hh jx b jy lr kc ls kt lt ku lu kv lv ks ln lo lp lq bi translated">计算特征向量和特征值。</li><li id="dc2f" class="li lj hh jx b jy lr kc ls kt lt ku lu kv lv ks ln lo lp lq bi translated">绘制特征向量(主成分)</li></ol></div><div class="ab cl lw lx go ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ha hb hc hd he"><h1 id="d6a0" class="it iu hh bd iv iw md iy iz ja me jc jd je mf jg jh ji mg jk jl jm mh jo jp jq bi translated">数学推导</h1><p id="6554" class="pw-post-body-paragraph ju jv hh jx b jy kw ka kb kc kx ke kf kt ky ki kj ku kz km kn kv la kq kr ks ha bi translated"><strong class="jx hi">步骤1 —将质心移动到原点</strong></p><figure class="lb lc ld le fd ii"><div class="bz dy l di"><div class="lf lg l"/></div></figure><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lh"><img src="../Images/05e836c34ff5c0387960c9633945e18c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TsHV_EYwkZlCGD8ROog3HQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">以原点为中心</figcaption></figure><p id="c880" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">请注意，我们的绘图中心已经转移到原点。这样做的原因是因为这将使我们在计算协方差矩阵时更容易。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es mi"><img src="../Images/cd0ddd0a372bb9f12ca27729626c232f.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*Qao3BC8rBEo8QRtQOXzc1g.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">协方差矩阵公式</figcaption></figure><p id="05cb" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">通过将中心居中到原点，请注意x_bar将变为零，从而将等式简化为:</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mj"><img src="../Images/1f3b98af80a1a7677605d57be9af0620.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*xpZm48FRItyPC0U_oqan0g.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">以原点为中心的协方差矩阵公式</figcaption></figure><p id="6abb" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated"><strong class="jx hi">步骤2 —计算协方差矩阵</strong></p><ul class=""><li id="8990" class="li lj hh jx b jy jz kc kd kt lk ku ll kv lm ks mk lo lp lq bi translated">协方差矩阵应该是<em class="jw"> dxd </em>矩阵，其中d是维数。</li><li id="bfbd" class="li lj hh jx b jy lr kc ls kt lt ku lu kv lv ks mk lo lp lq bi translated">每个元素代表两个特征之间的协方差(无论它们是正相关还是负相关，请注意，因为它是<strong class="jx hi">而不是相关</strong>数字较大并不意味着相关性较大)。</li><li id="b0d3" class="li lj hh jx b jy lr kc ls kt lt ku lu kv lv ks mk lo lp lq bi translated">对角线元素表示每个特征的方差。</li></ul><p id="121f" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">通常真实世界的数据应该如下所示，包含m个维度，但是在我们的例子中，请记住m = 2。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es ml"><img src="../Images/29ec77acb7685ca6a5574b8a732871ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*b8rvFC4ib3J2Nxx8KQvMVg.png"/></div></figure><p id="83ff" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">将我们的x1、x2表示为矩阵:</p><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es mm"><img src="../Images/1cd2b90f6aefd3f20f8a815b98fd44c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*6N5CRClJxyWZZmS2dQxHZw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">包含向量x1，x2的矩阵X</figcaption></figure><p id="26bc" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">将转置X乘以X，我们得到:</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mn"><img src="../Images/85f17cee6e8d54e7268c29c9d554dfb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JaH_85fZ1V8DXldXd9Vz3g.png"/></div></div></figure><p id="342d" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">可以简化成这样</p><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es mo"><img src="../Images/12c991849e444fa528f4d63bdc1147ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*KhFi0TzpxVoFf8rgPMIurg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">封面(X)</figcaption></figure><p id="52e9" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">我们可以看到对角线上是方差公式(当x_bar = 0时)。而所有其他元素对应于x1，x2的协方差(当它们的均值=0时)。</p><p id="9c0b" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">因此，我们计算了协方差矩阵，并证明对角线元素表示特征的方差，其他元素表示特征之间的协方差。</p><figure class="lb lc ld le fd ii"><div class="bz dy l di"><div class="lf lg l"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">在python中计算Cov(X)</figcaption></figure><p id="eea6" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated"><strong class="jx hi">步骤3——计算特征向量和特征值</strong></p><p id="a95c" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">特征向量决定了新投影空间的方向，特征值决定了它们的大小，这意味着它们解释了数据沿投影空间的变化。</p><p id="ba20" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">计算完成后，将特征向量按其特征值降序排列。当数据被投影到相应的特征向量时，较高的特征值承受原始数据的最大方差，而最低的特征值承受最小方差(我们应该丢弃的那些)。</p><figure class="lb lc ld le fd ii"><div class="bz dy l di"><div class="lf lg l"/></div></figure><pre class="lb lc ld le fd mp mq mr ms aw mt bi"><span id="c063" class="mu iu hh mq b fi mv mw l mx my">[(array([-0.31091055, -0.95043918]), 79.42349759589935),<br/> (array([-0.95043918,  0.31091055]), 0.30812852991522277)]</span></pre><p id="c54c" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">如果你想知道投影到第一个特征向量(PC1)保留了多少方差(也称为解释方差):</p><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es mz"><img src="../Images/473ec5ecee4a3c966d36317cb7dbcec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*1ZVVGAFMhh0SLuCr8jxY7g.png"/></div></figure><p id="436c" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">所以在我们的例子中，PC1将保持79.42/(79.42+0.31) = 0.996，99.6%的方差。</p><p id="09a9" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">没有规定你必须选择多少台电脑，因此通常的做法是绘制弯头图，并选择与图中弯头相对应的k。另一种选择是选择k，直到它保留你希望保留的方差。这在sklearn中很容易实现。PCA，不是给出整数值，而是给出0~1之间的值ex: PCA(n=0.95)的意思是保留保留95%方差的特征。</p><p id="a20d" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated"><strong class="jx hi">步骤4——绘制特征向量(主成分)</strong></p><p id="8673" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">投影矩阵只是k个选定的特征向量。让我们画出PC1和PC2，看看当我们将数据投射到它上面时，它的保存情况如何。</p><figure class="lb lc ld le fd ii"><div class="bz dy l di"><div class="lf lg l"/></div></figure><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es na"><img src="../Images/52ae81deeaaa406903eff6737c15fee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*NudWoI-XangVmfra8taWnw.png"/></div></figure><p id="5b34" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">黑线代表主要成分。较大的一个是PC1，它似乎穿过直线，如果数据投影到它上面，我们可以看到大部分方差将被保留。因此，我们可以将所有的点都投影到PC1上，然后去掉PC2，这样我们就将维度从二维降低到了一维！</p></div><div class="ab cl lw lx go ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ha hb hc hd he"><h1 id="7e69" class="it iu hh bd iv iw md iy iz ja me jc jd je mf jg jh ji mg jk jl jm mh jo jp jq bi translated">结论</h1><p id="d732" class="pw-post-body-paragraph ju jv hh jx b jy kw ka kb kc kx ke kf kt ky ki kj ku kz km kn kv la kq kr ks ha bi translated">本教程的主要目的是获得关于PCA的数学直觉，它如何在后台工作，因为理解它如何在后台运行将在调试ML模型时给你更多的洞察力。为了简单起见，使用2-D数据，但是相同的概念适用于任何数量的维度。下一篇教程将介绍为什么<strong class="jx hi"> PCA不适用于非线性可分离数据集</strong>，并介绍另一种称为ISOMAP的非线性维度技术。</p><p id="418f" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated">感谢您的阅读，如果有任何不正确的信息，请评论，我很乐意纠正我的误解:)</p></div><div class="ab cl lw lx go ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ha hb hc hd he"><p id="f0fc" class="pw-post-body-paragraph ju jv hh jx b jy jz ka kb kc kd ke kf kt kh ki kj ku kl km kn kv kp kq kr ks ha bi translated"><strong class="jx hi">参考文献:</strong></p><ul class=""><li id="ce1f" class="li lj hh jx b jy jz kc kd kt lk ku ll kv lm ks mk lo lp lq bi translated"><a class="ae nb" href="https://www.youtube.com/watch?v=k7RM-ot2NWY&amp;ab_channel=3Blue1Brown" rel="noopener ugc nofollow" target="_blank">跨度，基本矢量—3蓝色1棕色</a></li><li id="5d17" class="li lj hh jx b jy lr kc ls kt lt ku lu kv lv ks mk lo lp lq bi translated"><a class="ae nb" href="https://www.youtube.com/watch?v=G16c2ZODcg8&amp;ab_channel=BenLambert" rel="noopener ugc nofollow" target="_blank">协方差矩阵—本·兰伯特</a></li><li id="785f" class="li lj hh jx b jy lr kc ls kt lt ku lu kv lv ks mk lo lp lq bi translated"><a class="ae nb" href="https://www.youtube.com/watch?v=WBlnwvjfMtQ&amp;ab_channel=LuisSerrano" rel="noopener ugc nofollow" target="_blank">协方差矩阵——路易斯·塞拉诺</a></li><li id="54a4" class="li lj hh jx b jy lr kc ls kt lt ku lu kv lv ks mk lo lp lq bi translated"><a class="ae nb" href="https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643" rel="noopener" target="_blank">PCA背后的数学——走向数据科学</a></li><li id="5880" class="li lj hh jx b jy lr kc ls kt lt ku lu kv lv ks mk lo lp lq bi translated"><a class="ae nb" href="https://www.youtube.com/watch?v=bEX6WPMiLvo&amp;list=PLetSlH8YjIfWMdw9AuLR5ybkVvGcoG2EW&amp;index=5&amp;ab_channel=KoreaUnivDSBAKoreaUnivDSBA" rel="noopener ugc nofollow" target="_blank">降维PCA —韩国大学课程(韩语)</a></li><li id="efb2" class="li lj hh jx b jy lr kc ls kt lt ku lu kv lv ks mk lo lp lq bi translated"><a class="ae nb" href="https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#:~:text=The%20eigenvectors%20and%20eigenvalues%20of,the%20eigenvalues%20determine%20their%20magnitude." rel="noopener ugc nofollow" target="_blank">3个简单步骤中的主成分分析— Sebastian Raschka </a></li><li id="7f55" class="li lj hh jx b jy lr kc ls kt lt ku lu kv lv ks mk lo lp lq bi translated"><a class="ae nb" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html" rel="noopener ugc nofollow" target="_blank"> Python数据科学手册PCA — Jake VanderPlas </a></li></ul></div></div>    
</body>
</html>