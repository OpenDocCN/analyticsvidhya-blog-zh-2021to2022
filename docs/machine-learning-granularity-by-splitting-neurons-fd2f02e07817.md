# 第一部分通过分裂神经元的机器学习粒度

> 原文：<https://medium.com/analytics-vidhya/machine-learning-granularity-by-splitting-neurons-fd2f02e07817?source=collection_archive---------20----------------------->

# 问题是

1.  训练机器学习模型可能需要很长时间。
2.  存在死神经元的问题，其中模型可能开始时有太多的神经元和/或层，但没有有效的方法来提前知道这一点或稍后移除那些额外的参数。
3.  有时，一个模型开始稳定在我们想要的精度以下，如果不在一个更大的新模型上开始训练，就没有办法进一步提高精度。

# 解决方案

如果我们可以从一个较小的神经网络模型开始，在训练过程中将其增长到问题的大小，会怎么样？

假设我们可以从两个神经元的隐藏层开始，当它开始达到稳定的精确度时，保持两倍的大小。这就是我们今天要使用 TensorFlow 和 Keras 开始探索的内容。

我们首先认识一个简单的矩阵恒等式:

![](img/c484659cb78d7886f7bd3af70e9f1240.png)

我们在上面所做的就是在左边取 x₁和 y₁，在右边把它们分成两个元素。x₁被重复，而 y₁被重复**和**除以二。然而，这一变化使矩阵点积保持不变。

考虑神经网络中的一个神经元。如果我们能把那个神经元一分为二，同时保持输出不变，会怎么样？这将给予损失函数新的参数来调整反向传播。如果我们可以对每个神经元都这样做，从而使我们的网络翻倍，而不用从头开始，会怎么样？这正是这次行动的可能。让我们通过一个模型来演示如何在 TensorFlow 中实现这一点。

# 一个工作实例

让我们从调用我们的模块并加载 MNIST 数据集开始:

*注意:这段代码适用于 Tensorflow 2.4.0、Python 3.8 和 Numpy 1.18.5。*

我们将使用 *num_neurons* 来定义我们将从多少个神经元开始。在这个演示中，我们将只使用两个神经元。现在来看看我们的模型架构。

![](img/454f04aa0d4fc1a5cd5077d84dcffe93.png)

这只是一个简单的序列模型，有一个隐藏的密集层和 1600 个参数。现在让我们编译并拟合模型。

您可能会注意到我们的 fit 方法中的 [*提前停止*回调](https://keras.io/api/callbacks/early_stopping/)。这将确保一旦在两个连续时期内验证准确度的变化小于 1%,模型就停止。现在我们想从模型中得到权重和偏差。我们可以通过 Keras 内置的[*get _ weights*](https://keras.io/api/models/model_saving_apis/#get_weights-method)*方法来实现。*

*每层包含两个数组，一个用于权重，一个用于偏差。权重通过矩阵乘法相乘。偏差然后被添加到结果向量中。我们的模型包含一个密集层和一个输出层。权重和偏好的结构是这样的:*

*我们不想分割输出，因为我们仍然应该有 10 个类别。我们需要做以下工作:*

1.  *复制层 1 密集权重、层 1 偏差和输出层权重中的所有元素。*
2.  *将输出图层权重的元素除以 2。*
3.  *层 1 中的新神经元对将是相同的。所以损失函数最终会把它们调整得完全一样。因此，我们将希望在相反的方向上稍微推动它们的偏差，这样每个相同的神经元对都有一个主导神经元和一个被动神经元，它们可以在进一步的训练中继续多样化。*

*这为我们提供了一个新的数组列表:*

*我们可以看到上面的数组就是我们想要的形状。现在让我们用两倍的神经元创建第二个模型架构。*

*使用[*set _ weights*](https://keras.io/api/models/model_saving_apis/#set_weights-method)*方法将我们所有处理过的权重和偏差传递给新模型。让我们来总结一下:**

**![](img/5a46d30a0bdf4f358bf4662a744fe6db.png)**

**我们可以看到我们的新模型的参数增加了近一倍(1，600 到 3，190)。现在让我们继续使我们的参数适合新模型。**

**首先，让我们在较小的双神经元模型上进行训练。**

**![](img/21f1177d1e1be005f2fb3a43852cbeab.png)**

**我们可以看到，在由于回调而停止之前，我们的 2 个神经元达到了 68%的验证准确率。看起来是个不错的增长目标！让我们看看 4 个神经元是如何工作的:**

**![](img/e8b6421660d91940e2301eca26055544.png)**

**正如我们所希望的！新的更大的模型接受了分开的权重和偏差，开始时准确率约为 68%，损失量与小模型相同。到了第六纪元，在我们停止回调之前，整体准确率提高到了 87%。当然，您可以对更大的模型重复上述操作，直到模型达到期望的精度。任务完成！**

# **模块化生长神经网络**

**虽然上面演示了神经元确实可以被分裂以及如何进行分裂，但每次分裂所有神经元并不一定是最有效的方法。理想情况下，我们希望将损失最大的神经元作为分裂的目标。假设我们建立了一个阈值损失，其中任何高于特定损失的东西都会在反向传播后分裂其神经元。**

**我们之所以希望以损失最大的神经元为目标，是因为这些神经元在每一批中都经历了最大的变化，并且是增加粒度的理想目标。**

**根据正在处理的数据的类型以及有多少数据可用，您可能也希望这种情况只发生在这么多的时期之后。这样，在我们继续以神经元为分裂目标之前，模型有机会解决一些权重和偏差。**

**分裂神经元对于具有昂贵训练机制的大型模型来说尤其理想，例如生成式对抗网络(GANs)和自然语言处理器(NLP)。这可以让这些模型增长到数据，并大大减少死亡神经元的积累。**

**更新(5/9/21):下一篇文章将介绍一个为 Pytorch 开发的模块，该模块可以根据线性层和 Conv2D 层的每个神经元的活跃程度，在不同时期之间自动分割单个神经元。我们将运行一些测试，看看性能是否有所提高。**

# **最后**

**我希望你和我一样对此感兴趣！欢迎留下评论/问题，感谢您的阅读。**

**下面是 Github 上完整 python 脚本的链接:**

**[https://github.com/therealjjj77/Splitting-Neurons](https://github.com/therealjjj77/Splitting-Neurons)**