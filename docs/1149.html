<html>
<head>
<title>A primer for…Random Forests</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林入门</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-primer-for-random-forests-1019c1d02bcb?source=collection_archive---------16-----------------------#2021-02-16">https://medium.com/analytics-vidhya/a-primer-for-random-forests-1019c1d02bcb?source=collection_archive---------16-----------------------#2021-02-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="a799" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">第一章</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/ef4b60c49ea1293d56e18f4b94641f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Nv6u-_8S3rLgP4ac"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">那是随机的。戴维·科瓦连科在<a class="ae jm" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="4ca8" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这是(我希望会是)一系列初级读本中的第一篇，帮助您开始了解重要的数据科学概念和方法。我发现当我学习新东西的时候，阅读相关的方法，然后尝试自己复制它是我最好的方法。</p><p id="53f1" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我不知道对你来说最好的方法是什么，但是让我们试一试，你将会离找到答案更近一步。而且，你可能会学到一些关于递归的东西，这非常有趣！</p></div><div class="ab cl kj kk go kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="ha hb hc hd he"><h1 id="e17c" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">入门指南</h1><p id="a5b3" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">本指南的目标是向您介绍或再次介绍随机森林的基础知识。为了更好地理解本指南，你应该对python中的数据分析、统计和编程有一个基本的了解。你不会找到很多公式或者正式的算法定义。我希望你能对什么是随机森林、它是如何工作的、它为什么工作以及如何用python构建一个随机森林有一个简单直观的理解。我还会为那些有兴趣深入研究的人提供一些进一步阅读的链接。</p><p id="e872" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们将以一个30秒的随机森林概述开始，然后将其分解为基本的构建块。一旦我们熟悉了构建模块，我们将更详细地探讨每一个模块。最后，我们将通过在python中实现随机森林算法，把我们所学的一切放在一起。准备好了吗？让我们开始吧！</p></div><div class="ab cl kj kk go kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="ha hb hc hd he"><h1 id="31d2" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">30秒总结</h1><p id="22ae" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">随机森林由决策树组成。决策树包括将预测器空间分割成简单的区域。为了对给定的观察值进行预测，我们取其所属区域中训练观察值的平均值(用于回归)或模式(用于分类)。</p><p id="2a1e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">用于定义每个区域的规则集可以概括为一棵树，因此得名。与单个决策树相比，随机森林会生成许多这样的树，并采用这些树的平均值或预测模式来实现改进的预测性能。</p><p id="fc2e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果这还没有意义，不要担心，当我们在接下来的章节中更详细地介绍它时，它将变得有意义。今天我们将为回归生长一个随机森林，但是我们将学习的原则也适用于分类。唯一的区别是终端节点计算和成本函数选择。</p></div><div class="ab cl kj kk go kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="ha hb hc hd he"><h1 id="cf68" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">积木</h1><p id="0553" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated"><strong class="jp hi">确定森林的大小:</strong>好的，我们知道一个随机森林是由决策树组成的，我们对决策树有一个大致的概念。森林里有多少棵树？树的数量由用户决定。选择决策树的数量很重要，通常归结为一个简单的成本效益等式:计算更多树的成本与提高性能的可能收益。</p><p id="2f02" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> Bagging: </strong>为了从相同的训练数据中创建多个决策树，我们应用了Bagging。装袋是机器学习中的常见过程，并且不限于基于树的方法。本质上，bagging建立了多个模型，每个模型都基于训练数据的随机样本。很简单，对吧？</p><p id="733e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">自举:</strong>使用自举来生成这些样本。为了理解引导，假设您有一个玩具数据集:</p><pre class="ix iy iz ja fd ln lo lp lq aw lr bi"><span id="23fd" class="ls kr hh lo b fi lt lu l lv lw">import pandas as pd<br/>import numpy as np</span><span id="41bc" class="ls kr hh lo b fi lx lu l lv lw">np.random.seed(0)<br/>toy_data = pd.DataFrame({<br/>           'a' : np.random.choice(57, 10), <br/>           'c' : np.random.choice(11, 10),<br/>           'y' : np.random.choice(78, 10)<br/>           })</span><span id="d4f7" class="ls kr hh lo b fi lx lu l lv lw">display(toy_data)</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ly"><img src="../Images/0184ce9181e0f846c69edec874cb3b49.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/format:webp/1*64QlSVWCN6Q9HE7CyBTIog.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">我们的玩具数据。</figcaption></figure><p id="5443" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">要引导这个玩具数据，首先需要随机选择一行。选择的行是我们从<code class="du lz ma mb lo b">toy_data</code>采样的第一个观察结果。接下来，您对另一行进行采样，注意您首先采样的行保留在可能要选择的行池中。重复这个过程，直到你有你想要的观察数量。现在您有了一个引导示例！</p><p id="b281" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在生长随机林时，通过引导选择的行数通常等于定型数据中的行数。你需要的自举样本的数量等于你需要生长的决策树的数量。</p><p id="a550" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">总而言之，引导就是简单地从训练数据中进行替换采样。每个样本中的行数是训练数据中的行数，样本数是您的林所需的树的数量。简单。</p><p id="0b96" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们在实践中看到它。</p><pre class="ix iy iz ja fd ln lo lp lq aw lr bi"><span id="e487" class="ls kr hh lo b fi lt lu l lv lw">def bootstrap(df, random_seed):<br/>    return df.sample(len(df), replace = True, <br/>                              random_state = random_seed)</span><span id="21e9" class="ls kr hh lo b fi lx lu l lv lw">bootstrap(toy_data, 1)</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mc"><img src="../Images/3a0a63c3d23aa223d32d630461460c0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*Qic9HlSLAdUOmnRlp0wYsQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">我们的自举样本。</figcaption></figure><p id="f369" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">你可以看到我使用了<code class="du lz ma mb lo b">pandas.DataFrame.sample with replacement = True</code>,这就是全部内容。</p><p id="91cf" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">随机部分:</strong>因此，我们有许多决策树，它们是基于我们的训练数据的自举样本生成的。我们有随机森林了吗？不完全是。我们需要解决随机森林的随机部分！</p><p id="cc1a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在生长正常决策树的每个阶段，所有的预测器都被考虑来确定树中的最佳下一步。在随机森林树中，在评估每个决策步骤之前，对可能的预测值进行随机抽样。这就限制了每一步可以选择哪些预测值。为什么这很重要？</p><p id="1b05" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">想象一下，与用标准方法生长的三棵树相比，用这种改进的方法生长的三棵树。这三个随机森林树很可能彼此不太相似，因为它们都被迫考虑随机选择的一组预测因子。每一棵随机的森林树更有可能考虑其他树忽略的预测因子。</p><p id="48d3" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">将这与标准流程进行比较——这些树可能彼此非常相似。他们在每个阶段都考虑了相同的预测集，唯一的区别是他们作为训练输入接收的自举样本。<strong class="jp hi">随机森林树过程的结果是减少了树之间的相关性。</strong></p><p id="ccda" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">随机林中的预测只是来自林中决策树的预测的汇总。总结多棵树的目的是减少方差，对吗？对一组相关性较低的树进行汇总会进一步降低方差。带回家？</p><h2 id="7af8" class="ls kr hh bd ks md me mf kw mg mh mi la jw mj mk lc ka ml mm le ke mn mo lg mp bi translated">在看不见的测试数据上，随机森林通常比单个决策树或一组袋装决策树表现更好。</h2></div><div class="ab cl kj kk go kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="ha hb hc hd he"><p id="861b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在我们已经了解了基本的构建模块，我们准备开始第二章——生成决策树！即将推出…:-)</p><pre class="ix iy iz ja fd ln lo lp lq aw lr bi"><span id="ac29" class="ls kr hh lo b fi lt lu l lv lw"><strong class="lo hi">References</strong></span><span id="0ad0" class="ls kr hh lo b fi lx lu l lv lw">1. <em class="mq">Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. An Introduction to Statistical Learning : with Applications in R. New York :Springer, 2013.</em></span></pre></div></div>    
</body>
</html>