<html>
<head>
<title>Overfitting Issues and Optimization Techniques in Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的过拟合问题及优化技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-learning-overfitting-issu-optimization-techniques-68714400e2ca?source=collection_archive---------8-----------------------#2021-06-21">https://medium.com/analytics-vidhya/deep-learning-overfitting-issu-optimization-techniques-68714400e2ca?source=collection_archive---------8-----------------------#2021-06-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5029" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文将涵盖以下主题</p><ul class=""><li id="8c86" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">梯度下降的问题</li><li id="7c61" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">饱和函数的问题</li><li id="1199" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">方差偏差权衡</li><li id="9c1a" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">不同类型的优化器</li><li id="d620" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">正规化和交叉验证</li></ul><h2 id="f18b" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated">消失，爆炸梯度，死亡神经元</h2><p id="ee01" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">消失梯度下降出现在具有梯度下降学习方法和反向传播的神经网络中。梯度下降是一种迭代方法，在每个时期更新权重和偏差，以最小化损失。在反向传播期间，在每次迭代中，通过当前权重减去总误差相对于当前权重的偏导数来更新权重。在一些情况下，在一些迭代之后，层的权重将永远不会改变，梯度下降将永远不会收敛到最优解。</p><p id="3b59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有一些激活函数，如Sigmoid，双曲线Tan h，其范围在(0，1)和(-1，1)之间，两条曲线都是S形的，对于大的输入值，在一段时间后将达到饱和，因此Sigmoid函数输入的大变化将导致导数更接近0。这意味着梯度很小，权重和偏差没有正确更新，这将导致整个网络不准确，还可能导致神经元死亡。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kr"><img src="../Images/08b8f9326c036a04964a2c9d6ffb4849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*FXwGAhpSvvHzF4_935Y1Xw.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx translated">消失梯度下降</figcaption></figure><p id="aa34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">爆炸梯度下降</strong></p><p id="bbd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与网络中每个权重相关联的梯度等于许多数字的乘积。如果值的乘积大于1，可能会导致梯度变得非常大。假设在下面的例子中，它从初始值开始，突然均方误差呈指数增长，MSE再次下降。现在，当前的MSE高于初始值，因此，模型将是不稳定的，损失将非常高，并且算法发散并且永远不会达到好的解决方案</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kr"><img src="../Images/69549094b26d8c089d982f49da36cb5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*-nCpA9RsAaE5CrHT_stXKg.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx translated">爆炸梯度下降</figcaption></figure><p id="3967" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">梯度下降或香草梯度下降:</strong></p><p id="776d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降是线性回归中的一种迭代优化算法，用于寻找微分函数的局部最小值。梯度下降算法将在斜率开始时选择随机初始化，并截取和计算误差项。在每次迭代中，它将反向检查误差项，并反向传播和更新斜率和截距值，直到误差最小。</p><p id="174d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们在下图(1)中看到的，当学习率较高时，它会收敛得更快，但不会达到精确的全局最小值。在图(2)中，我们的学习率非常低。当数据集很大时，它会花费更长的时间来收敛，有时它可能会在局部最小值处饱和。图(3)显示了一个大小和方向正确的示例。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kr"><img src="../Images/405f8688f422e521b44667a32152f9fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*-skldmfrQTFZy4zh-Huqpw.png"/></div></figure><p id="ae92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优点:</strong></p><p id="8ed4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实现起来非常简单</p><p id="90b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点:</strong></p><p id="84d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为它需要整个数据集来计算反向传播中的导数并更新权重，所以它消耗更多的存储器并且计算量很大。由于这个原因，它可能以局部最小值结束，也没有达到全局最小值。</p><p id="3bc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">随机梯度下降:</strong></p><p id="5bcc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机梯度下降从整个数据集中分批提取较少数量的样本，并执行1个历元，因为样本数据集很小，所以内存消耗将很低，并且成本函数将在最初波动，并随着时间的推移而降低。在这两种情况下，我们必须手动选择学习率，这将是一个超参数</p><p id="b7ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优势:</strong></p><p id="a4be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">内存消耗低</p><p id="1785" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点:</strong></p><ul class=""><li id="4582" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">完成完整数据集的1个历元所需的时间高于梯度下降。</li><li id="7922" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">可能停留在局部最小值</li></ul><p id="e9a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ADAGRAD(自适应梯度算法)</strong></p><p id="9448" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学习率被称为是自适应的，因为它是基于每个参数中斜率的陡度来缩放的。当斜率较陡时，会降低学习率，使误差梯度向正确的方向移动以达到全局最小值。</p><p id="3b5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优点:</strong></p><p id="1f37" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">无需手动更新学习率</p><p id="5699" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点:</strong></p><p id="81e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于大量迭代，学习率将降低到非常小的数值，这可能导致收敛缓慢。</p><p id="8143" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Adam(自适应运动估计)</strong></p><p id="72ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Adam是Adam的升级版本，这种方法也计算每个参数的自适应学习率。但是它收敛更快，计算量更小，克服了Adagrad的缺点。</p><p id="1abf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是神经网络中广泛使用的优化器。</p><h1 id="8c8b" class="ld js hi bd jt le lf lg jx lh li lj kb lk ll lm ke ln lo lp kh lq lr ls kk lt bi translated"><strong class="ak">优化技术:</strong></h1><p id="0f18" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">1.正确初始化</p><p id="5ddc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.批量标准化</p><p id="3029" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.使用不饱和激活函数</p><p id="f9c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.渐变剪辑</p><p id="1bce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">正确初始化</strong></p><p id="6a02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑具有三个隐藏层的神经网络，并假设它们在每个神经元的输出的初始化期间具有恒定的权重和0偏差。在前向/反向传播期间，所有三个隐藏层的权重在整个过程中将是对称的，并且将防止神经元学习不同的东西。</p><p id="e841" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有一个随机初始化的权重，而不是常数，如果权重初始化得更高，那么这将导致爆炸梯度问题。</p><p id="3163" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了克服这一点，我们需要使用<strong class="ih hj"> Xavier或He初始化。</strong></p><p id="3da7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑一个线性神经元。</p><p id="45c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y = w1x1 + w2x2 + … + wNxN + b</p><p id="dc31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于每个经过的层，我们期望输入层的方差等于输出层的方差，</p><p id="74d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们计算y的方差。</p><p id="f4ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">var(y)= var(w1x 1+w2x 2+…+wNxN+b)</strong></p><p id="71f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们计算上式右边括号内各项的方差。</p><p id="1bcb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">var(wixi)= E(Xi)2 var(wi)+E(wi)2 var(Xi)+var(wi)var(Xi)</p><p id="6249" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里E()代表给定变量的期望值，基本代表平均值。我们假设输入和权重来自零均值的高斯分布。因此“E()”项消失了，我们得到:</p><p id="d816" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">var(wixi) = var(wi) var(xi)</p><p id="f1e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意‘b’是一个常数，方差为零，所以它会消失。让我们代入原始方程:</p><p id="24ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">var(y)= var(w1)var(x1)+…+var(wN)var(xN)</p><p id="a57d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为它们都是同分布的，所以我们可以写成:</p><p id="f749" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">var(y) = N * var(wi) * var(xi)</p><p id="dd4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以如果我们希望y的方差和x一样，那么“N *var(wi)”项应该等于1。因此:</p><p id="38b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">N * var(wi) = 1 <br/> var(wi) = 1/N</p><p id="63c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Navg= Nin +Nout/2</p><p id="2c82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Var(Wi)= 2/(Nin +Nout)</p><p id="24bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该值将是随机初始化的权重的倍数，以防止梯度爆炸。</p><p id="7cf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">批量标准化:</strong></p><p id="d3a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，归一化意味着，我们的输入特征将具有不同的尺度。我们将使用各种归一化技术，如标准缩放器、最小最大缩放器、鲁棒缩放器，以便在预处理步骤中使所有特征具有相同的尺度。</p><p id="df67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">即使我们在预处理中将所有输入特征转换为标准比例，当激活函数被添加到每个隐藏层的输出时，这也将导致数据中的小变化。</p><p id="2827" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，在激活函数的输出端，每一个隐层后都加入了批量归一化，对数据进行归一化处理。</p><p id="f341" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它简单地计算为(输入均值)/标准差。通过增加批量归一化，加快了模型的训练速度。</p><p id="3fdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">渐变裁剪:</strong></p><p id="8524" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度削波是一种方法，其中在通过网络的反向传播期间，将<strong class="ih hj">误差导数改变或削波到阈值</strong>，并使用削波的梯度来更新权重。</p><p id="26ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过重新调整误差导数，权重的更新也将被重新调整，从而显著降低梯度消失或爆炸的可能性。</p><p id="7ebd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">不饱和激活功能:</strong></p><p id="f0db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> RELU(整流线性单元):</strong></p><p id="d6aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是非线性的，通常值的范围在0到无穷大之间，斜率永远不会达到0，除非值为负。RELU在计算上比Sigmoid和Tan h更便宜</p><p id="9e08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">T12】A(x)= max(0，x)T14】。如果x是正数，它给出一个输出x，否则给出0</strong></p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kr"><img src="../Images/4fd5a531bc6eadec1d0b67ef7ebb5991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*FjMAnFS4eFXGUFNSm8yc4Q.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx translated">RELU</figcaption></figure><p id="9922" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">泄漏的Relu: </strong></p><p id="d408" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于负值，RELU函数将总是导致斜率为0，因此为了克服这种情况，引入了在负方向具有窄斜率的泄漏Relu。</p><p id="d7b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">LReLU(x)= { x if x&gt;0αx if x≤0 }</strong></p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kr"><img src="../Images/398f48a672213e139c97f991a1a08e64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*yUnXr_bHlny_BiGju2MEfQ.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx translated">泄漏Relu</figcaption></figure><p id="0bce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ELU(指数线性单位)</strong></p><p id="fa73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果输入值X&gt;0，那么这将与RELU相同，但是当x &lt;0 we get a value less than 0.This graph is similar to Leaky RELU but we induce a exponential function here. This will mitigate dying of Neurons.</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es lv"><img src="../Images/6dd266a36b183de4b21ddf84328aa033.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*fG8_3pLpc_YNyX7-O_di9A.png"/></div></figure><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kr"><img src="../Images/3a7ec4b8c6d288edc1cbad99851548e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*qbC_y5KA1e9awoDo7Ruztw.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx translated">ELU</figcaption></figure><p id="36a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">过拟合和方差偏差权衡</strong>时</p><p id="a001" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">偏差:</strong>偏差是您的预测模型捕捉输入数据集的实际趋势的程度，高偏差可能会导致训练数据集中的高误差，并可能导致过度拟合。</p><p id="5d8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">方差:</strong>模型的可变性是数据从实际数据点传播的程度。具有高方差的模型将对训练数据具有复杂的拟合，这将导致测试数据中的高方差，从而导致过度拟合。</p><p id="f9c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果模型非常简单，具有较少数量的特征，则它可能具有高偏差和低方差，从而导致拟合不足。它将不能正确地捕捉模式，所以我们需要在神经网络中添加更多的数据或模型层。</p><p id="977e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果模型非常复杂且基数很高，则它可能具有低偏差和高方差，这将导致过度拟合。当模型已经用训练数据集进行了异常训练，并且当新数据被引入到模型中时，它未能更好地执行时，将会产生这种结果。为了克服这个问题，我们可能需要减少层数/关闭神经元或使用正则化方法来克服这个问题。</p><p id="26de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，总是建议在偏差和方差之间进行适当的权衡，以获得模型的最佳性能</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kr"><img src="../Images/e269ef6f4f3904c69ddd28c581fb0c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*aGkCgUR5Gz8FLJxuVR9atQ.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx translated">方差偏差权衡</figcaption></figure><p id="58cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">减轻过度拟合:</strong></p><p id="dd5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.正规化</p><p id="153a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.交互效度分析</p><p id="1934" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.退出</p><p id="2443" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">正规化:</strong></p><p id="93da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最常见的正规化方法有L1、L2以及L1和L2的结合。</p><p id="b1f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">L2正则化也称为岭正则化，这将在损失函数中增加一个惩罚因子。由于增加了正则项，它降低了权重，因为它假设具有降低的权重的模型不太复杂。这将使权重更接近于零，但永远不会为零。</p><p id="3593" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">成本函数=损失+ λ ∑||w|| </strong></p><p id="183f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">L1正则化也被称为套索正则化，它惩罚权重的绝对值。权重有时会衰减到零。</p><p id="4343" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的<strong class="ih hj"> λ是控制相对权重的超级参数。</strong></p><p id="214f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">成本函数=损失+ λ ∑|w| </strong></p><p id="69e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">弹性网</p><p id="f7fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是L1和L2正规化的结合。</p><p id="7a6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">代价函数=损失+ λ ∑||w|| + λ ∑|w| </strong></p><p id="687a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">交叉验证:</strong></p><p id="a5c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">交叉验证是一种将训练数据集分为训练数据集和验证数据集，并执行不同的迭代折叠以从使用不同数据集组合执行的所有迭代中获得误差度量的平均值的方法。取平均值会给出一个广义误差项，我们可以用它来比较验证集和测试集，以检查它是否是一个过拟合模型。</p><p id="9547" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">退出:</strong></p><p id="c927" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">退出是一种有意关闭部分神经元以避免过度适应的方法。在每个训练步骤中，随机选择退出神经元和不同神经元。</p><p id="25a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">复杂模型多层感知器</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kr"><img src="../Images/5371848f1ccc185c3f477f1e2cd2a926.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Jv0ncq8Cqq9xn_ET54KRrw.png"/></div></figure><p id="9f02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在隐藏层中应用50%的下降后，拓扑将如下所示:</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kr"><img src="../Images/c1c17994a1beb902a190257133a72848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*lpnyVbW5eV3_z7Gm3ai6dg.png"/></div></figure><p id="759a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Keras中的超参数调谐:</p><p id="8817" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Github链接:<a class="ae lw" href="https://github.com/Jayachandran9283/Deep-Learning/blob/main/Deep%20Learning%20Hyperparameters.ipynb" rel="noopener ugc nofollow" target="_blank">深度学习/深度学习超参数. ipynb at main jayachandran 9283/深度学习(github.com)</a></p></div></div>    
</body>
</html>