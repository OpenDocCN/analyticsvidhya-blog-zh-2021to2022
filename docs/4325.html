<html>
<head>
<title>Laptop Price Prediction by Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于机器学习的笔记本电脑价格预测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/laptop-price-prediction-by-machine-learning-7e1211bb96d1?source=collection_archive---------1-----------------------#2021-09-22">https://medium.com/analytics-vidhya/laptop-price-prediction-by-machine-learning-7e1211bb96d1?source=collection_archive---------1-----------------------#2021-09-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/c4e9d81d036549aa6233cbb4296f0be8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XmAiY1Zac-OcaF6U"/></div></div></figure><div class=""/><p id="7437" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用<a class="ae jo" href="https://www.python.org/" rel="noopener ugc nofollow" target="_blank"> Python </a>，<a class="ae jo" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> Numpy </a>，<a class="ae jo" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank">熊猫</a> s，<a class="ae jo" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"> Matplotlib </a>，<a class="ae jo" href="https://plotly.com/python/" rel="noopener ugc nofollow" target="_blank"> Plotly </a>，<a class="ae jo" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>。</p><blockquote class="jp jq jr"><p id="f54b" class="iq ir js is b it iu iv iw ix iy iz ja jt jc jd je ju jg jh ji jv jk jl jm jn hb bi translated">关于计算机的好消息是它们会做你让它们做的事情。坏消息是他们按照你说的去做。—泰德·纳尔逊</p></blockquote><p id="ad8f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">膝上型电脑</strong>、<strong class="is hu">膝上型电脑</strong>或<strong class="is hu">笔记本电脑</strong>是一种小型便携式个人电脑<a class="ae jo" href="https://en.wikipedia.org/wiki/Personal_computer" rel="noopener ugc nofollow" target="_blank"/>(PC)，带有屏幕和字母数字键盘。这些通常有一个<a class="ae jo" href="https://en.wikipedia.org/wiki/Flip_(form)" rel="noopener ugc nofollow" target="_blank">翻盖式</a>外形，通常有安装在上盖内部的<a class="ae jo" href="https://en.wikipedia.org/wiki/Computer_screen" rel="noopener ugc nofollow" target="_blank">屏幕</a>和安装在下盖内部的<a class="ae jo" href="https://en.wikipedia.org/wiki/Alphanumeric_keyboard" rel="noopener ugc nofollow" target="_blank">键盘</a>，尽管带有可拆卸键盘的<a class="ae jo" href="https://en.wikipedia.org/wiki/2-in-1_PC" rel="noopener ugc nofollow" target="_blank"> 2合1电脑</a>通常作为笔记本电脑或具有笔记本电脑模式销售。笔记本电脑折叠起来便于运输，因此适合<a class="ae jo" href="https://en.wikipedia.org/wiki/Mobile_computing" rel="noopener ugc nofollow" target="_blank">移动使用</a>。它的名字来源于<a class="ae jo" href="https://en.wikipedia.org/wiki/Lap" rel="noopener ugc nofollow" target="_blank">大腿</a>，因为它被认为在使用时可以放在人的大腿上。今天，笔记本电脑被用于各种场合，如工作、教育、玩游戏、浏览网页、个人多媒体和普通家用电脑。</p><p id="dd3b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">截至2021年，<a class="ae jo" href="https://en.wikipedia.org/wiki/American_English" rel="noopener ugc nofollow" target="_blank">美式英语</a>中，<strong class="is hu">笔记本电脑</strong>和<strong class="is hu">笔记本电脑</strong>两个术语互换使用；在英语的其他方言中，这种或那种可能更受青睐。术语“笔记本电脑”或“笔记本”最初指的是特定尺寸的笔记本电脑(最初比当时的主流笔记本电脑更小更轻)，这些术语现在指的是同一种东西，而<strong class="is hu">笔记本</strong>不再指任何特定的尺寸。</p><h1 id="496d" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">导入库:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="1b3c" class="ld jx ht kz b fi le lf l lg lh">import numpy as np</span><span id="17e4" class="ld jx ht kz b fi li lf l lg lh">import pandas as pd</span><span id="fb99" class="ld jx ht kz b fi li lf l lg lh">import matplotlib.pyplot as plt</span><span id="e3b9" class="ld jx ht kz b fi li lf l lg lh">import seaborn as sns<br/>import plotly.express as px</span><span id="376f" class="ld jx ht kz b fi li lf l lg lh"># Configuring styles</span><span id="bd35" class="ld jx ht kz b fi li lf l lg lh">sns.set_style("darkgrid")</span><span id="0ca5" class="ld jx ht kz b fi li lf l lg lh">matplotlib.rcParams['font.size'] = 16</span><span id="e8cd" class="ld jx ht kz b fi li lf l lg lh">matplotlib.rcParams['figure.figsize'] = (12, 6)</span><span id="55a4" class="ld jx ht kz b fi li lf l lg lh">matplotlib.rcParams['figure.facecolor'] = '#00000000'</span></pre><h1 id="7e65" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">正在加载数据集:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="1c0a" class="ld jx ht kz b fi le lf l lg lh">import io</span><span id="6259" class="ld jx ht kz b fi li lf l lg lh">#df2 = pd.read_csv('laptop_price.csv')</span><span id="ec8d" class="ld jx ht kz b fi li lf l lg lh">df2=pd.read_csv('laptop_price.csv',encoding='latin-1')</span></pre><h1 id="2af3" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">显示数据:</h1><figure class="ku kv kw kx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lj"><img src="../Images/4dc16148a2f838b5c92dc6c9b88f74a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iqTkzemjQiEtDRzOSunRlw.png"/></div></div></figure><h1 id="0aa0" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">关于数据集中所有列的信息:</h1><figure class="ku kv kw kx fd hk er es paragraph-image"><div class="er es lk"><img src="../Images/e9e35894cb36eac2a39c069a9b8875ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*dAC_6TDAky_LHLIid9xlXw.png"/></div></figure><h1 id="4bb6" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">数据描述:</h1><figure class="ku kv kw kx fd hk er es paragraph-image"><div class="er es ll"><img src="../Images/57d2254c0492dc99ef1885980be1897e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*QbTA_4snZKrmw8M-Rv9IhA.png"/></div></figure><h1 id="e5b3" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">数据之间的相关性:</h1><figure class="ku kv kw kx fd hk er es paragraph-image"><div class="er es lm"><img src="../Images/dc1b7be9233e0a90a63a9a3cb41a5057.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*giJBwWchYOBA14Qy-MrcmQ.png"/></div></figure><h1 id="546b" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">数据集的形状:</h1><figure class="ku kv kw kx fd hk er es paragraph-image"><div class="er es ln"><img src="../Images/b54f8a76c56d3a39c51f11c1154fbd5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*RKvBA9cEYzINeH-RtwLj8g.png"/></div></figure><h1 id="f5fb" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">数据集的热图:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="0043" class="ld jx ht kz b fi le lf l lg lh">plt.figure(figsize=(16,10))</span><span id="e0c4" class="ld jx ht kz b fi li lf l lg lh">sns.heatmap(df2.corr(), annot=True,cmap ='RdYlGn')</span></pre><figure class="ku kv kw kx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lo"><img src="../Images/3bd0ff8b9f9cc459094c6cbf4d0f19cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4CCWdGgtTRJGwLcXsSpFXw.png"/></div></div></figure><h1 id="e5bf" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">数据可视化:</strong></h1><figure class="ku kv kw kx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/350677f0f0c16a0913cde79994114827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0HoLqxVVIGx8C3E6yf_DyQ.jpeg"/></div></div></figure><h1 id="48e3" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">笔记本电脑公司分布柱状图:</strong></h1><figure class="ku kv kw kx fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="abcf" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以看到，联想、戴尔、惠普笔记本电脑的分布都差不多。这意味着这些是目前销量最高的笔记本电脑。</p><h1 id="68dc" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">Ram与欧元价格的直方图:</strong></h1><figure class="ku kv kw kx fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><h1 id="585f" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">RAM散点图对比价格对比公司:</strong></h1><figure class="ku kv kw kx fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="a010" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">内存大小与欧元价格成反比，价格较高的笔记本电脑的销售率很低。</p><h1 id="c1d2" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">CPU与价格和公司的散点图:</h1><figure class="ku kv kw kx fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="0405" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">处理器的代与价格成正比。搭载英特尔、AMD处理器的笔记本电脑是这里销量最高的。</p><figure class="ku kv kw kx fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><h1 id="7547" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">笔记本电脑操作系统分布柱状图:</strong></h1><figure class="ku kv kw kx fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="0271" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以看到人们大多在使用windows 10。如今，几乎每台笔记本电脑都装有windows 10操作系统，但也很少有笔记本电脑装有windows 7操作系统。</p><h1 id="2bcf" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">公司对比操作系统对比价格饼状图:</strong></h1><figure class="ku kv kw kx fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><h1 id="7af6" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">公司vs Ram vs操作系统vs价格vs CPU vs内存散点图:</strong></h1><figure class="ku kv kw kx fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><h1 id="484a" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">数据预测:</h1><figure class="ku kv kw kx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lr"><img src="../Images/2852143a04d6777374ce1c6f541b2f94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RPsAXw56pLtMfeJRWrvlDA.jpeg"/></div></div></figure><h1 id="b968" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">线性回归:</h1><p id="4cc6" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">线性回归是一种基于监督学习的机器学习算法。它执行回归任务。回归基于独立变量对目标预测值进行建模。它主要用于找出变量和预测之间的关系。不同的回归模型基于因变量和自变量之间的关系类型、所考虑的因素以及所使用的自变量数量而有所不同。</p><p id="234a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性回归执行的任务是根据给定的自变量(x)预测因变量值(y)。因此，这种回归技术找出了x(输入)和y(输出)之间的线性关系。因此，其名称为线性回归。y=ax+c</p><figure class="ku kv kw kx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lx"><img src="../Images/47cea4b270e398694c57e5deec9df86c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OsEmBJIRKfZKRCmf.jpeg"/></div></div></figure><p id="3fc3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了获得更好的结果，我们必须从数据集中删除一些实体。</p><h1 id="5a02" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">从sklearn导入模块:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="69f2" class="ld jx ht kz b fi le lf l lg lh">import re</span><span id="f1af" class="ld jx ht kz b fi li lf l lg lh">import pandas as pd</span><span id="0732" class="ld jx ht kz b fi li lf l lg lh">import seaborn as sns</span><span id="72b2" class="ld jx ht kz b fi li lf l lg lh">import numpy as np</span><span id="5479" class="ld jx ht kz b fi li lf l lg lh">import matplotlib.pyplot as plt</span><span id="e5fe" class="ld jx ht kz b fi li lf l lg lh">%matplotlib inline</span><span id="12d8" class="ld jx ht kz b fi li lf l lg lh">from sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor,VotingRegressor</span><span id="a86a" class="ld jx ht kz b fi li lf l lg lh">from sklearn.linear_model import SGDRegressor,LinearRegression</span><span id="d0ff" class="ld jx ht kz b fi li lf l lg lh">from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score</span><span id="d0bf" class="ld jx ht kz b fi li lf l lg lh">from sklearn.model_selection import train_test_split</span><span id="c0cf" class="ld jx ht kz b fi li lf l lg lh">from sklearn.neighbors import KNeighborsRegressor</span><span id="f465" class="ld jx ht kz b fi li lf l lg lh">from sklearn.preprocessing import LabelEncoder</span><span id="581d" class="ld jx ht kz b fi li lf l lg lh">from sklearn.tree import DecisionTreeRegressor</span><span id="9dd0" class="ld jx ht kz b fi li lf l lg lh">from xgboost import XGBRegressor</span></pre><h1 id="3be8" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">标签编码:</h1><p id="7f0f" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">标签编码是指将标签转换成数字形式，从而将其转换成机器可读的形式。然后，机器学习算法可以以更好的方式决定这些标签必须如何操作。这是监督学习中结构化数据集的重要预处理步骤。</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="671b" class="ld jx ht kz b fi le lf l lg lh">le={}</span><span id="2bb6" class="ld jx ht kz b fi li lf l lg lh">for col in set(df.columns).difference({'Price_euros'}):</span><span id="04b9" class="ld jx ht kz b fi li lf l lg lh">le[col] = LabelEncoder()</span><span id="0fdd" class="ld jx ht kz b fi li lf l lg lh">df[col]  = le[col].fit_transform(df[col])</span><span id="e271" class="ld jx ht kz b fi li lf l lg lh">df</span></pre><figure class="ku kv kw kx fd hk er es paragraph-image"><div class="er es ly"><img src="../Images/9ca9f55374b58557e8818d0d15333856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*Ygq96VblximxIDwrMTZE-g.png"/></div></figure><h1 id="9e0b" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">特征缩放:</h1><p id="cfe7" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">特征缩放是一种用于归一化独立变量范围或数据特征的方法。在数据处理中，它也被称为数据规范化，通常在数据预处理步骤中执行。</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="9ccd" class="ld jx ht kz b fi le lf l lg lh">def normalize_col(col_name):</span><span id="17f1" class="ld jx ht kz b fi li lf l lg lh">      return (df[col_name] -           df[col_name].min())/(df[col_name].max()-df[col_name].min())</span><span id="c263" class="ld jx ht kz b fi li lf l lg lh">for col in ['Product','Price_euros']:</span><span id="ed91" class="ld jx ht kz b fi li lf l lg lh">     df[col]=normalize_col(col)</span><span id="edf8" class="ld jx ht kz b fi li lf l lg lh"> df</span></pre><figure class="ku kv kw kx fd hk er es paragraph-image"><div class="er es lz"><img src="../Images/f42256401c2abbcc7dd4984675300663.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*gplZVxeVXWQJUtbs137gUA.png"/></div></figure><h1 id="9292" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">分成测试集和训练集:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="2868" class="ld jx ht kz b fi le lf l lg lh">#x=df.drop('Price_euros',axis=1)</span><span id="d900" class="ld jx ht kz b fi li lf l lg lh">#y=df.Price_euros</span><span id="32c4" class="ld jx ht kz b fi li lf l lg lh">x = df.iloc[:, -4:-3].values</span><span id="5587" class="ld jx ht kz b fi li lf l lg lh">y = df.iloc[:, -1].values</span><span id="e945" class="ld jx ht kz b fi li lf l lg lh">x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.4,random_state=1)</span></pre><p id="a208" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，我们将数据分为训练集和测试集。</p><h1 id="f9c6" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">使用简单线性回归对训练集进行训练:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="6525" class="ld jx ht kz b fi le lf l lg lh">from sklearn.linear_model import LinearRegression</span><span id="24f2" class="ld jx ht kz b fi li lf l lg lh">regressor = LinearRegression()</span><span id="522f" class="ld jx ht kz b fi li lf l lg lh">regressor.fit(x_train, y_train)</span></pre><h1 id="34dd" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">对测试集的预测:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="8f41" class="ld jx ht kz b fi le lf l lg lh">y_pred = regressor.predict(x_test)</span><span id="9b16" class="ld jx ht kz b fi li lf l lg lh">y_pred</span></pre><h1 id="8403" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">可视化训练集:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="747c" class="ld jx ht kz b fi le lf l lg lh">plt.scatter(x_train, y_train, color = 'red')</span><span id="ce9e" class="ld jx ht kz b fi li lf l lg lh">plt.plot(x_train, regressor.predict(x_train), color = 'blue')</span><span id="efab" class="ld jx ht kz b fi li lf l lg lh">plt.title("Price vs Product Name of laptop")</span><span id="c770" class="ld jx ht kz b fi li lf l lg lh">plt.xlabel("Product Name")</span><span id="fd1d" class="ld jx ht kz b fi li lf l lg lh">plt.ylabel("Price")</span><span id="93a0" class="ld jx ht kz b fi li lf l lg lh">plt.show()</span></pre><figure class="ku kv kw kx fd hk er es paragraph-image"><div class="er es ma"><img src="../Images/4e893289e0ab35f815e28d785597a871.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*m2rvnqjriXxCjL-T_cq_9A.png"/></div></figure><p id="de90" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以看到数据点远离直线。证明了线性回归不容易拟合数据点。因此，现在我们必须寻找另一种方法来创建最佳拟合模型。</p><h1 id="c100" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">多元回归:</h1><p id="db26" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">多元线性回归试图通过将线性方程拟合到观察到的数据来模拟两个或多个特征与响应之间的关系。执行多元线性回归的步骤几乎与简单线性回归的步骤相似。区别在于评价。我们可以用它来找出哪个因素对预测产量的影响最大，现在不同的变量相互关联。</p><p id="3d4d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里:Y = B0+B1 * x1+B2 * x2+B3 * x3+……bn * xn Y =因变量而x1，x2，x3，…… xn =多个自变量。</p><figure class="ku kv kw kx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mb"><img src="../Images/816dcc14394b958783a4db4ebf5d0623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dUf0z_39Nkbvb0iJ.jpg"/></div></div></figure><h1 id="9be8" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">在训练集上训练多元线性回归模型:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="0b7c" class="ld jx ht kz b fi le lf l lg lh">from sklearn.linear_model import LinearRegression</span><span id="361f" class="ld jx ht kz b fi li lf l lg lh">regressor = LinearRegression()</span><span id="3872" class="ld jx ht kz b fi li lf l lg lh">regressor.fit(X_train, y_train)</span></pre><h1 id="9187" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">预测测试集结果:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="d14d" class="ld jx ht kz b fi le lf l lg lh">y_pred = regressor.predict(X_test)</span><span id="70b5" class="ld jx ht kz b fi li lf l lg lh">np.set_printoptions(precision= 3)</span><span id="e9af" class="ld jx ht kz b fi li lf l lg lh">print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test),1)), 1))</span></pre><h1 id="b264" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">可视化结果:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="3e5c" class="ld jx ht kz b fi le lf l lg lh">plt.plot(y_pred)</span><span id="1669" class="ld jx ht kz b fi li lf l lg lh">plt.plot(y_test)</span><span id="f3b6" class="ld jx ht kz b fi li lf l lg lh">plt.show()</span></pre><figure class="ku kv kw kx fd hk er es paragraph-image"><div class="er es mc"><img src="../Images/188d58a5db7b28b036874f75a2a2ad56.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*Y2KMfwth7Z8RaJpCRrTwRg.png"/></div></figure><p id="8bd1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，我们对数据集应用了多元线性回归模型。我们可以看到，在某些区域，橙色图形线比蓝色图形线更陡。这证明了多元回归模型比线性回归模型拟合得更好，但它仍然不是最佳拟合图，因为一些预测已经越过了测试值。</p><h1 id="d60d" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">随机森林:</h1><p id="9e39" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">随机森林或随机决策森林是一种用于分类、回归和其他任务的集成学习方法，它通过在训练时构建大量决策树来操作。对于分类任务，随机森林的输出是大多数树选择的类。对于回归任务，返回单个树的平均值或平均预测值。随机决策森林纠正了决策树过度适应其训练集的习惯587–588随机森林通常优于决策树，但其准确性低于梯度提升树。但是，数据特征会影响它们的性能。</p><figure class="ku kv kw kx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es md"><img src="../Images/d2cc1e1a72a3249690f77b32755d5f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pKTTkYeiG9V_C6f2.png"/></div></div></figure><h1 id="7c51" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">在训练集上训练随机森林模型:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="d4ec" class="ld jx ht kz b fi le lf l lg lh">from sklearn.ensemble import RandomForestRegressor</span><span id="df3c" class="ld jx ht kz b fi li lf l lg lh">regressor = RandomForestRegressor(n_estimators = 95, random_state = 78)</span><span id="65f3" class="ld jx ht kz b fi li lf l lg lh">regressor.fit(X, y)</span></pre><h1 id="8213" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">可视化结果:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="4cb2" class="ld jx ht kz b fi le lf l lg lh">X_grid = np.arange(min(X), max(X), 0.01)</span><span id="0004" class="ld jx ht kz b fi li lf l lg lh">X_grid = X_grid.reshape((len(X_grid), 1))</span><span id="74e7" class="ld jx ht kz b fi li lf l lg lh">plt.scatter(X, y, color = 'red')</span><span id="c64d" class="ld jx ht kz b fi li lf l lg lh">plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')</span><span id="db42" class="ld jx ht kz b fi li lf l lg lh">plt.title("Price vs Product Name of laptop")</span><span id="f67a" class="ld jx ht kz b fi li lf l lg lh">plt.xlabel("Product Name")</span><span id="610b" class="ld jx ht kz b fi li lf l lg lh">plt.ylabel("Price")</span><span id="1be8" class="ld jx ht kz b fi li lf l lg lh">plt.show()</span></pre><figure class="ku kv kw kx fd hk er es paragraph-image"><div class="er es ma"><img src="../Images/da48692f4ccfca5d8821097b737d7fd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*VZDYnDI6QEEy9lm-5H_j-Q.png"/></div></figure><p id="7004" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这种随机森林预测更加准确，并且覆盖了大多数数据点。这个模型的准确率是96%。所以这个模型远比前两个模型更符合。</p><h1 id="5b31" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">决策树:</h1><p id="6dd6" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">决策树是一种决策支持工具，它使用决策及其可能结果(包括偶然事件结果、资源成本和效用)的树状模型。这是显示只包含条件控制语句的算法的一种方式。</p><p id="74ba" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">决策树通常用于运筹学，特别是决策分析，以帮助确定最有可能达到目标的策略，但也是机器学习中的一种流行工具。</p><p id="a2de" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">决策树是一种类似流程图的结构，其中每个内部节点代表对一个属性的“测试”(例如，掷硬币是正面还是反面)，每个分支代表测试的结果，每个叶节点代表一个类标签(在计算所有属性后做出的决定)。从根到叶的路径代表分类规则。</p><figure class="ku kv kw kx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es me"><img src="../Images/0181f6e3e585a6f09f0a1491aa37485a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*M8Mv_zFbqJ-XLrfs.png"/></div></div></figure><h1 id="e254" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">在训练集上训练决策树模型:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="8409" class="ld jx ht kz b fi le lf l lg lh">from sklearn.tree import DecisionTreeRegressor</span><span id="45b5" class="ld jx ht kz b fi li lf l lg lh">regressor = DecisionTreeRegressor(random_state = 45)</span><span id="1761" class="ld jx ht kz b fi li lf l lg lh">regressor.fit(X,y)</span></pre><h1 id="4f73" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">可视化结果:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="0dfe" class="ld jx ht kz b fi le lf l lg lh">X_grid = np.arange(min(X), max(X), 0.01)</span><span id="c3c6" class="ld jx ht kz b fi li lf l lg lh">X_grid = X_grid.reshape((len(X_grid), 1))</span><span id="bab7" class="ld jx ht kz b fi li lf l lg lh">plt.scatter(X, y, color = 'red')</span><span id="8be1" class="ld jx ht kz b fi li lf l lg lh">plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')</span><span id="bf8d" class="ld jx ht kz b fi li lf l lg lh">plt.title("Price vs Product Name of laptop")</span><span id="5ff0" class="ld jx ht kz b fi li lf l lg lh">plt.xlabel("Product Name")</span><span id="0646" class="ld jx ht kz b fi li lf l lg lh">plt.ylabel("Price")</span><span id="c860" class="ld jx ht kz b fi li lf l lg lh">plt.show()</span></pre><figure class="ku kv kw kx fd hk er es paragraph-image"><div class="er es ma"><img src="../Images/ad7ca1eabb6f944c75546faa8b54061a.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*40MN1fUHta_QjOiSkI03bw.png"/></div></figure><p id="3237" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在决策树模型中，准确率要比线性和多元回归高得多。所以我们可以说这个模型也是最适合的模型之一。</p><h1 id="6709" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">支持向量回归模型:</h1><p id="2b04" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">在机器学习中，支持向量机(SVM，也称为支持向量网络)是具有相关学习算法的监督学习模型，这些算法分析数据以进行分类和回归分析。由Vladimir Vapnik及其同事(Boser等人，1992年，Guyon等人，1993年，Vapnik等人，1997年)在美国电话电报公司贝尔实验室开发的支持向量机是最稳健的预测方法之一，它基于Vapnik (1982年，1995年)和Chervonenkis (1974年)提出的统计学习框架或VC理论。给定一组训练样本，每个样本被标记为属于两个类别中的一个，SVM训练算法建立一个模型，将新样本分配给一个类别或另一个类别，使其成为非概率二元线性分类器(尽管存在诸如普拉特标度的方法，以在概率分类设置中使用SVM)。SVM将训练样本映射到空间中的点，以便最大化两个类别之间的差距。然后，新的例子被映射到相同的空间，并根据它们落在差距的哪一边来预测属于哪个类别。</p><p id="027a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">除了执行线性分类，支持向量机还可以使用所谓的核技巧有效地执行非线性分类，将它们的输入隐式映射到高维特征空间。</p><p id="fa77" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当数据未标记时，监督学习是不可能的，并且需要非监督学习方法，该方法试图找到数据到组的自然聚类，然后将新数据映射到这些形成的组。由Hava Siegelmann和Vladimir Vapnik创建的支持向量聚类算法应用在支持向量机算法中开发的支持向量的统计来对未标记的数据进行分类，并且是工业应用中最广泛使用的聚类算法之一。</p><figure class="ku kv kw kx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mf"><img src="../Images/80e64a94d51db34959ffcbd3ed553a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mBy2vZMo3UKygY2T.png"/></div></div></figure><h1 id="4f05" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">在训练集上训练支持向量回归模型:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="57d4" class="ld jx ht kz b fi le lf l lg lh">from sklearn.svm import SVR</span><span id="5aee" class="ld jx ht kz b fi li lf l lg lh">regressor = SVR(kernel = 'rbf')</span><span id="7f7d" class="ld jx ht kz b fi li lf l lg lh">regressor.fit(X, y)</span></pre><h1 id="558e" class="jw jx ht bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">可视化结果:</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="54ba" class="ld jx ht kz b fi le lf l lg lh">plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = 'red')</span><span id="3fb1" class="ld jx ht kz b fi li lf l lg lh">plt.plot(sc_X.inverse_transform(X), sc_y.inverse_transform(regressor.predict(X)), color = 'blue')</span><span id="79d9" class="ld jx ht kz b fi li lf l lg lh">plt.title("Price vs Product Name of laptop")</span><span id="9b1d" class="ld jx ht kz b fi li lf l lg lh">plt.xlabel("Product Name")</span><span id="72d4" class="ld jx ht kz b fi li lf l lg lh">plt.ylabel("Price")</span><span id="53f4" class="ld jx ht kz b fi li lf l lg lh">plt.show()</span></pre><figure class="ku kv kw kx fd hk er es paragraph-image"><div class="er es ma"><img src="../Images/72fc6e34e9065982060253a9038066b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*E3KeYD4p0tUpdv_-cW2rjg.png"/></div></figure><p id="63ca" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在SVM，我们将数据点分成两个平面。在中间，蓝色的平面被称为超平面。但不幸的是，这个模型并不是我们想象中最合适的模型。</p><figure class="ku kv kw kx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mg"><img src="../Images/a5fe018b1310d8fbf65be3ed0e19b582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lVE9jnTA45JRbSdYfmqjEA.jpeg"/></div></div></figure></div></div>    
</body>
</html>