<html>
<head>
<title>DECISION TREE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策图表</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-tree-fce5018f3278?source=collection_archive---------8-----------------------#2021-01-13">https://medium.com/analytics-vidhya/decision-tree-fce5018f3278?source=collection_archive---------8-----------------------#2021-01-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="92cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇博客中，我将写一个广泛使用的分类(机器学习)算法，即决策树。</p><p id="2b58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我将解释<em class="jd">什么是决策树，决策树算法的类型，如何创建决策树，决策树的应用，优点/缺点</em>，最后，我将提供一个链接，链接到我简单介绍的Jupyter笔记本，从零开始实现<em class="jd">决策树算法</em>。</p><p id="8413" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，没有任何进一步的到期让我们开始。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/7696ca6f36f01de843d22f187ff5db2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nG_paquGRzd_3nHD.jpg"/></div></div></figure><h1 id="5ed2" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">什么是决策树算法？</strong></h1><ul class=""><li id="628a" class="ko kp hi ih b ii kq im kr iq ks iu kt iy ku jc kv kw kx ky bi translated">决策树算法属于监督学习的范畴。它们可以用来解决回归和分类问题。</li><li id="514d" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">决策树使用树表示来解决每个叶节点对应一个类标签，属性在树的内部节点上表示的问题。</li><li id="3a54" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">我们可以使用决策树来表示离散属性上的任何布尔函数。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es le"><img src="../Images/112cb25ffef717cf6789ddb68ec06c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2fCUPLFeE2L-qQKs.png"/></div></div></figure><p id="dab9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">以下是我们在使用决策树时做出的一些假设:</strong></p><ul class=""><li id="4726" class="ko kp hi ih b ii ij im in iq lf iu lg iy lh jc kv kw kx ky bi translated">开始时，我们将整个训练集视为根。</li><li id="e0da" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">特征值最好是分类的。如果这些值是连续的，则在构建模型之前会将其离散化。</li><li id="0a5e" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">基于属性值，记录被递归地分布。</li><li id="b8c1" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">我们使用统计方法将属性排序为根或内部节点。</li></ul><h1 id="fb71" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">决策树的类型</h1><p id="c07b" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">根据目标变量，决策树分为两种类型。</p><ol class=""><li id="3249" class="ko kp hi ih b ii ij im in iq lf iu lg iy lh jc ll kw kx ky bi translated"><strong class="ih hj">分类变量决策树:</strong>这是算法有分类目标变量的地方。例如，假设要求您预测一台计算机的相对价格，分为三类:<em class="jd">低</em>、<em class="jd">中</em>或<em class="jd">高。</em>功能可能包括<em class="jd">显示器类型</em>、<em class="jd">扬声器质量</em>、<em class="jd"> RAM </em>和<em class="jd"> SSD </em>。决策树将从这些特征中学习，在通过每个节点传递每个数据点之后，它将在三个分类目标<em class="jd">低</em>、<em class="jd">中</em>或<em class="jd">高</em>之一的叶节点处结束。</li><li id="7a2a" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc ll kw kx ky bi translated"><strong class="ih hj">连续变量决策树:</strong>在这种情况下，输入到决策树的特征(如房屋质量)将用于预测连续输出(如房屋价格)。</li></ol><h1 id="7342" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">关键术语</h1><p id="7a61" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">让我们看看决策树是什么样子的，以及当给定一个新的预测输入时，它们是如何工作的。</p><p id="ffcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下图解释了决策树的基本结构。每棵树都有一个<strong class="ih hj">根节点</strong>，输入在这里传递。这个根节点被进一步划分成决策节点集，其中结果和观察是有条件的。将单个节点划分为多个节点的过程称为<strong class="ih hj">分裂</strong>。如果一个节点没有分裂成更多的节点，那么它被称为<strong class="ih hj">叶节点</strong>，或者<strong class="ih hj">终端节点</strong>。决策树的一个子部分称为<strong class="ih hj">分支</strong>或<strong class="ih hj">子树</strong>(例如在下图的方框中)。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lm"><img src="../Images/1c87c7c1cd751bba7d1906efe77157a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/0*WS7K78w34FyaRWY3"/></div></figure><p id="98c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还有一个概念与分裂完全相反。如果有可以消除的决策规则，我们就把它们从树上砍下来。这个过程被称为<strong class="ih hj">修剪</strong>，有助于最小化算法的复杂性。</p><h1 id="230b" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">如何创建决策树</h1><p id="b574" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">在这一节中，我们将讨论描述如何创建决策树的核心算法。这些算法完全依赖于目标变量，然而，这些算法不同于用于分类和回归树的算法。</p><p id="0135" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有几种技术可以用来决定如何分割给定的数据。决策树的主要目标是在节点之间进行最佳分割，从而以最佳方式将数据划分到正确的类别中。为此，我们需要使用正确的决策规则。规则直接影响算法的性能。</p><p id="0a51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们开始之前，需要考虑一些假设:</p><ul class=""><li id="7954" class="ko kp hi ih b ii ij im in iq lf iu lg iy lh jc kv kw kx ky bi translated">开始时，整个数据被认为是根，此后，我们使用算法进行分裂或将根分成子树。</li><li id="1276" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">特征值被认为是分类的。如果这些值是连续的，则在构建模型之前会将它们分开。</li><li id="9a3d" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">记录是基于属性值递归分布的。</li><li id="f41f" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">属性作为树的根或内部节点的排序是使用统计方法来完成的。</li></ul><p id="fbfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们从常用的拆分技术开始，从而构建决策树。</p><blockquote class="ln lo lp"><p id="1ff5" class="if ig jd ih b ii ij ik il im in io ip lq ir is it lr iv iw ix ls iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="hi"> 1。信息增益</em> </strong></p></blockquote><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lt"><img src="../Images/8e7fc2e4a698b7d394c3540ff1a6a1f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/0*6SjSKL8l2IZUSQ-h"/></div></figure><p id="b6ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们使用决策树中的节点将训练实例划分成更小的子集时，熵会发生变化。信息增益是这种熵变化的量度。<br/> <strong class="ih hj"> <em class="jd">定义</em> </strong>:设S是一组实例，A是一个属性，Sv是S的子集且A = v，Values (A)是A的所有可能值的集合，那么</p><p id="2c9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">熵</strong> <br/>熵是随机变量不确定性的度量，它表征了任意样本集合的不纯度。熵越高，信息量越大。<br/> <strong class="ih hj"> <em class="jd">定义</em> </strong>:设S是一组实例，A是一个属性，Sv是S的子集A = v，Values (A)是A的所有可能值的集合，那么</p><p id="1433" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">示例:</p><pre class="jf jg jh ji fd lu lv lw lx aw ly bi"><span id="e34b" class="lz jr hi lv b fi ma mb l mc md">For the set X = {a,a,a,b,b,b,b,b}<br/>Total intances: 8<br/>Instances of b: 5<br/>Instances of a: 3</span><span id="123a" class="lz jr hi lv b fi me mb l mc md">= -[0.375 * (-1.415) + 0.625 * (-0.678)] <br/>              =-(-0.53-0.424) <br/>              = 0.954</span></pre><p id="3ae7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">利用信息增益构建决策树</strong> <br/> <strong class="ih hj">要领:</strong></p><ul class=""><li id="39b9" class="ko kp hi ih b ii ij im in iq lf iu lg iy lh jc kv kw kx ky bi translated">从与根节点关联的所有训练实例开始</li><li id="8071" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">使用信息增益选择标记每个节点的属性</li><li id="2390" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated"><em class="jd">注意:</em>根到叶的路径不应该包含两次相同的离散属性</li><li id="d4a2" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">递归地构造训练实例的子集上的每个子树，该训练实例将沿着树中的路径被分类。</li><li id="2448" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">如果所有正面或负面的训练实例都存在，则相应地将该节点标记为“是”或“否”</li><li id="cde1" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">如果没有剩余的属性，用在该节点上剩余的训练实例的多数投票来标记</li><li id="9fed" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">如果没有剩余的实例，用父实例的多数投票来标记</li></ul><p id="f14b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">举例:</strong> <br/>现在，让我们用信息增益为下面的数据画一个决策树。</p><p id="e141" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">训练集:3个特性和2个类</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mf"><img src="../Images/ade7cc6098b938cda09282da6b2038c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*QAa87d57TDzcQ_VOd2SdCg.png"/></div></figure><p id="8ece" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们有3个特性和2个输出类。<br/>利用信息增益建立决策树。我们将采用每个特征并计算每个特征的信息。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mg"><img src="../Images/0eafc0e972e9818c0d7a84c90d33d207.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/0*B-D07eY2PDCO22dW.png"/></div></figure><p id="88cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">在特征X上分割</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mh"><img src="../Images/e249a264e5087687fb72b9b123f0c2fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/0*h7u8AJbco4uxExTm.png"/></div></figure><p id="971b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">特征Y上的分割</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mi"><img src="../Images/ace3213c926bf5890aa1fb39edbd228a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/0*6kAT1auShwiOIBTr.png"/></div></figure><p id="563d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">在特征Z上分割</strong></p><p id="4d16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的图像中我们可以看到，当我们对特征Y进行分割时，信息增益最大。因此，对于根节点，最适合的特征是特征Y。现在我们可以看到，当按特征Y分割数据集时，子节点包含目标变量的纯子集。所以我们不需要进一步分割数据集。</p><p id="cb38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上述数据集的最终树将如下所示:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mj"><img src="../Images/aa66564889b927afc9c3a6aa92d4abe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/0*08P_O2KRepouwVc4.png"/></div></figure><p id="4075" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd"> 2。基尼指数</em> </strong></p><ul class=""><li id="fee3" class="ko kp hi ih b ii ij im in iq lf iu lg iy lh jc kv kw kx ky bi translated">基尼指数是一种衡量随机选择的元素被错误识别的频率的指标。</li><li id="19e9" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">这意味着基尼系数越低的属性越好。</li><li id="a090" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">Sklearn支持基尼指数的“基尼”标准，默认情况下，它采用“基尼”值。</li><li id="ed78" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">计算基尼指数的公式如下。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mk"><img src="../Images/8c498533088a75b691355f05e17870e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*oXzL3R_Od6QFrjFcQsiqJw.png"/></div></div></figure><p id="4894" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">示例:</strong> <br/>让我们考虑下图中的数据集，用基尼指数画一个决策树。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ml"><img src="../Images/d3de73236705f1ff59c19b6c40ca56af.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*RCg2MkyAA5g_DM5Pxcynqg.png"/></div><figcaption class="mm mn et er es mo mp bd b be z dx translated">GFG全图</figcaption></figure><p id="6379" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的数据集中，有5个属性，其中属性E是包含2个(正和负)类的预测特征。我们两个班的比例相等。<br/>在基尼指数中，我们要选择一些随机值来对每个属性进行分类。该数据集的这些值是:</p><pre class="jf jg jh ji fd lu lv lw lx aw ly bi"><span id="2276" class="lz jr hi lv b fi ma mb l mc md">A       B        C         D<br/>  &gt;= 5     &gt;= 3.0      &gt;= 4.2    &gt;= 1.4<br/>   &lt; 5      &lt; 3.0       &lt; 4.2     &lt; 1.4</span></pre><p id="cc7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">计算基尼指数为Var A: </strong> <br/> <strong class="ih hj">值&gt; = 5: 12 <br/> </strong>属性A &gt; = 5 &amp;类=正:5/12 <br/>属性A &gt; = 5 &amp;类=负:7/12 <br/>基尼(5，7) = 1 — [ (5/12) + (7/12) ] = 0.4860</p><p id="dc67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">值&lt; 5: 4 <br/> </strong>属性A &lt; 5 &amp; class =正:3/4 <br/>属性A &lt; 5 &amp; class =负:1/4 <br/> Gini(3，1)= 1-[(3/4)+(1/4)]= 0.375</p><p id="2f1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将每个基尼系数加权求和:<br/>基尼系数(目标值，A)=(12/16)*(0.486)+(4/16)*(0.375)= 0.45825</p><p id="2fad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">计算Var B的基尼指数:</strong> <br/> <strong class="ih hj">值&gt; = 3: 12 <br/> </strong>属性B &gt; = 3 &amp; class =正:8/12 <br/>属性B &gt; = 3 &amp; class =负:4/12 <br/>基尼(5，7) = 1 — [ (8/12) + (4/12) ] = 0.4460</p><p id="4233" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">值&lt; 3: 4 <br/> </strong>属性B &lt; 3 &amp;类=正:0/4 <br/>属性B &lt; 3 &amp;类=负:4/4 <br/>基尼(3，1)= 1-[(3/4)+(1/4)]= 0.375</p><p id="7d1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过对每个基尼指数加权求和:<br/>基尼(目标，B)=(12/16)*(0.446)+(0/16)*(0)= 0.3345</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mq"><img src="../Images/e5dbc7bad54f27fd27938fedaa120733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_oUq-pMGz16QdbbmSAL-Iw.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mr"><img src="../Images/85657cb6e571c0472572d1a6f7bb0fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/0*SdUaSYsztRAL4wJq.png"/></div></figure><h1 id="7cdc" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">决策树的应用</h1><p id="4592" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">决策树是机器学习领域中最基本也是最广泛使用的算法之一。它在分类和回归建模的不同领域得到了应用。由于其描绘可视化输出的能力，人们可以很容易地从建模过程流中获得洞察力。这里有几个可以使用决策树的例子，</p><ul class=""><li id="4103" class="ko kp hi ih b ii ij im in iq lf iu lg iy lh jc kv kw kx ky bi translated">企业管理</li><li id="ffc5" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">客户关系管理</li><li id="0a78" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">欺诈性声明检测</li><li id="ebbe" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">能耗</li><li id="7dff" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">医疗保健管理</li><li id="fef3" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">故障诊断</li></ul><h1 id="04de" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">优点和缺点</h1><p id="363b" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">决策树有一些优点和缺点。先说优点。与其他算法相比，决策树在处理数据时花费的时间非常少。可以跳过一些预处理步骤，如数据的标准化、转换和缩放。尽管数据集中存在缺失值，但模型的性能不会受到影响。决策树模型直观，易于向技术团队和利益相关者解释，并且可以跨多个组织实现。</p><p id="6168" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">缺点来了。在决策树中，数据的微小变化会导致决策树结构的巨大变化，从而导致不稳定。训练时间急剧增加，与数据集的大小成比例。在某些情况下，与其他传统算法相比，计算可能会变得复杂。</p><h1 id="58dd" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">从头开始构建决策树模型</strong></h1><p id="0f2d" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated"><strong class="ih hj">第一步:导入重要库并加载数据</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ms"><img src="../Images/7593526d83f49950c8161b9a73dabb89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G5zsx4KI8D6X9PxsJo5YRw.png"/></div></div></figure><p id="aed9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第二步:计算每个属性的熵，选择具有最大<em class="jd">信息增益</em>的属性作为根节点</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mt"><img src="../Images/de45b2df5561424100debd1373faa28c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cg7gkIQA2AvMjN-2AZcKrA.png"/></div></div></figure><p id="7caf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第三步:构建决策树</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mu"><img src="../Images/bb151ad54c3e3ca8b496bc569ddb10de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cW2ntjs1ZyYF6uDPd8yP_w.png"/></div></div></figure><p id="9203" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第四步:做预测</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mv"><img src="../Images/a0c6f0a6222cad06f9bbfa02884fc657.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yQchSWUg58qRXbUl4HOH2Q.png"/></div></div></figure><blockquote class="mw"><p id="3e4c" class="mx my hi bd mz na nb nc nd ne nf jc dx translated"><strong class="ak">完整的代码请访问下面的链接。那是我的笔记本。</strong></p></blockquote><div class="ng nh ni nj nk nl"><a href="https://github.com/Shag10/Machine-Learning/blob/master/Internity_Internship/Day-9/Decision_Tree.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab dw"><div class="nn ab no cl cj np"><h2 class="bd hj fi z dy nq ea eb nr ed ef hh bi translated">shag 10/机器学习</h2><div class="ns l"><h3 class="bd b fi z dy nq ea eb nr ed ef dx translated">这个库包含机器学习算法的基础。基于监督学习的算法…</h3></div><div class="nt l"><p class="bd b fp z dy nq ea eb nr ed ef dx translated">github.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz jo nl"/></div></div></a></div><p id="14c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我试图提供所有关于决策树及其实现入门的重要信息。希望你能在这里找到有用的东西。谢谢你一直读到最后。</strong></p></div><div class="ab cl oa ob gp oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="hb hc hd he hf"><h1 id="7a2e" class="jq jr hi bd js jt oh jv jw jx oi jz ka kb oj kd ke kf ok kh ki kj ol kl km kn bi translated"><strong class="ak">参考文献</strong></h1><div class="om on ez fb oo nl"><a href="https://www.geeksforgeeks.org/decision-tree-introduction-example/" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab dw"><div class="nn ab no cl cj np"><h2 class="bd hj fi z dy nq ea eb nr ed ef hh bi translated">决策树介绍及示例- GeeksforGeeks</h2><div class="ns l"><h3 class="bd b fi z dy nq ea eb nr ed ef dx translated">决策树算法属于监督学习的范畴。它们可以用来解决回归和…</h3></div><div class="nt l"><p class="bd b fp z dy nq ea eb nr ed ef dx translated">www.geeksforgeeks.org</p></div></div><div class="nu l"><div class="op l nw nx ny nu nz jo nl"/></div></div></a></div><div class="om on ez fb oo nl"><a href="https://blog.paperspace.com/decision-trees/" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab dw"><div class="nn ab no cl cj np"><h2 class="bd hj fi z dy nq ea eb nr ed ef hh bi translated">决策树完全指南| Paperspace博客</h2><div class="ns l"><h3 class="bd b fi z dy nq ea eb nr ed ef dx translated">决策树是许多经典机器学习算法的基础，如随机森林、Bagging和…</h3></div><div class="nt l"><p class="bd b fp z dy nq ea eb nr ed ef dx translated">blog.paperspace.com</p></div></div><div class="nu l"><div class="oq l nw nx ny nu nz jo nl"/></div></div></a></div></div></div>    
</body>
</html>