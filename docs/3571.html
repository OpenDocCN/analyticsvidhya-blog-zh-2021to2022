<html>
<head>
<title>A Mathematical dive into word embeddings (skip grams)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对单词嵌入的数学探究(跳格)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-mathematical-dive-into-word-embeddings-5c06f2fd9bbe?source=collection_archive---------4-----------------------#2021-07-10">https://medium.com/analytics-vidhya/a-mathematical-dive-into-word-embeddings-5c06f2fd9bbe?source=collection_archive---------4-----------------------#2021-07-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="1610" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">为什么是单词嵌入？</strong></h1><p id="d2f9" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">让我们考虑我们的NLP模型学习短语:</p><blockquote class="ka kb kc"><p id="ed69" class="jc jd kd je b jf ke jh ji jj kf jl jm kg kh jp jq ki kj jt ju kk kl jx jy jz ha bi translated">那是一幅美丽的画</p></blockquote><p id="af0c" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">它很难预测短语中的单词<em class="kd">【绘画】</em>——那是一个华丽的________。<br/>这是因为<em class="kd">漂亮</em>和<em class="kd">华丽这两个词之间没有直接关系。因此，下一个问题是，我们要为此责备谁呢？</em></p><h2 id="0997" class="km if hh bd ig kn ko kp ik kq kr ks io jn kt ku is jr kv kw iw jv kx ky ja kz bi translated"><strong class="ak">一个热矢量</strong></h2><p id="87bc" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">以前，每个单词都是由整个单词语料库(称为词汇表)的一个热点向量来描述的。因此，一百万个单词的语料库将导致每个单词有一百万维长。</p><h2 id="b2ba" class="km if hh bd ig kn ko kp ik kq kr ks io jn kt ku is jr kv kw iw jv kx ky ja kz bi translated"><strong class="ak">单词嵌入</strong></h2><p id="65e9" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">正如您可能已经想到的，主要目标是为单词构建一种编码，封装它们之间的某种关系。这些编码被称为单词嵌入。如果您正在寻找相同的直觉，我们的目标是创建嵌入，这样具有相似上下文的单词就聚集在一起(余弦相似性可以用来发现两个单词嵌入有多接近)。<br/>此外，这也有助于减少输入向量的维数。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es la"><img src="../Images/e7949b1245ded640cdb51ca7f4eee28a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*DEbs5tWie25uSNVqxxt8yg.gif"/></div></figure></div><div class="ab cl li lj go lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ha hb hc hd he"><h1 id="777e" class="ie if hh bd ig ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb bi translated"><strong class="ak">如何生成单词嵌入？</strong></h1><p id="bf54" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">希望现在你对我们要做的事情有个大概的了解。</p><p id="b2ec" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">现在是时候考虑我们实际上如何生成这些编码了！</p><p id="902e" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">先从<strong class="je hi"> <em class="kd">跳过克</em> </strong>说起吧。</p><h2 id="e0f5" class="km if hh bd ig kn ko kp ik kq kr ks io jn kt ku is jr kv kw iw jv kx ky ja kz bi translated"><strong class="ak">跳过克</strong></h2><p id="f882" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">skip-gram神经网络模型的最基本形式实际上非常简单。它包括训练一个有一个隐藏层的神经网络。然而，与架构同样重要和有趣的是我们如何使用它来生成单词嵌入。</p><p id="28fb" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">最初我们试图解决一个完全不同的问题。从一个大的英语文本语料库中，我们不断选择其长度取决于先前选择的窗口大小的短语。从这个窗口中，我们选择一个中心词，并将其称为目标。该短语中的其余单词称为上下文。我们建立一个模型，给定一个目标词，我们预测它的上下文。</p><blockquote class="ka kb kc"><p id="b8c2" class="jc jd kd je b jf ke jh ji jj kf jl jm kg kh jp jq ki kj jt ju kk kl jx jy jz ha bi translated">我想要两份意大利干酪比萨饼</p><p id="f6fc" class="jc jd kd je b jf ke jh ji jj kf jl jm kg kh jp jq ki kj jt ju kk kl jx jy jz ha bi translated">P(Wo|Wi) -&gt;给定输入单词Wi，得到上下文单词Wo的概率</p></blockquote><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es lu"><img src="../Images/dfc3f6bc416244110c2c510f4bcc93b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*DZxkGxGTEUXGr2iZ6lpYPQ.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">神经网络的目标—(等式1)</figcaption></figure><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es lz"><img src="../Images/0984e2072c2db7935fb327a29117bc2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*Za6uasyG__qovOJyExzmAQ.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">跳过gram模型架构—(图1)</figcaption></figure><pre class="lb lc ld le fd ma mb mc md aw me bi"><span id="d2d5" class="km if hh mb b fi mf mg l mh mi">Shape of W1 = (300, 10000)<br/>Shape of W2 = (10000, 300)</span><span id="6d41" class="km if hh mb b fi mj mg l mh mi">Hidden = W1.X (300,1)<br/>(Note that no activation function is applied here and neither is a bias term involved)</span></pre><p id="2af1" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">看看这个架构，很容易看出它是如何最大化获得与目标单词相对应的上下文单词的目标的。问题是，它如何帮助我们生成单词嵌入？</p><p id="1679" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">答案就在隐藏层权重矩阵W1中。</p><p id="72e5" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">当学习一个目标词的不同上下文时，我们在某种程度上开发了一个模型，它封装了目标词和上下文之间的关系。因此，我们可以得出结论，隐藏层实际上是给定输入单词的单词嵌入。然而，我们需要语料库中所有单词的单词嵌入。因此，我们转向权重矩阵W1。</p><p id="55e4" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">W1的形状为(300，10000)。与形状为(10000，1)的独热输入向量的矩阵乘法将呈现形状为(300，1)的向量。假设输入是一个独热编码向量，只有值为1的节点在矩阵乘法中起作用。其他节点为0，则不会有任何贡献。</p><p id="8fcd" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">因此，我们可以说，对应于独热编码向量中的第10个元素的字将具有W1矩阵的第10列作为其字嵌入。(稍微想想这个)</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es mk"><img src="../Images/381b5defea656bd8e7f769cefc71fbf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*pqR0VDcnphV9xmzYC_nffQ.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">(图二)</figcaption></figure></div><div class="ab cl li lj go lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ha hb hc hd he"><h2 id="db9f" class="km if hh bd ig kn ko kp ik kq kr ks io jn kt ku is jr kv kw iw jv kx ky ja kz bi translated">softmax层</h2><p id="54d9" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">关于输出softmax层的细节故意保持模糊。现在是时候揭开这背后的神秘面纱了。为了理解这一点，有必要理解我们实际上是在学习两个单词的嵌入；一个当单词是目标时，一个当单词是上下文时。前者由权重矩阵W1给出，后者由权重矩阵W2给出。</p><pre class="lb lc ld le fd ma mb mc md aw me bi"><span id="7688" class="km if hh mb b fi mf mg l mh mi">e1 = W1 . I      [I is the input vector]<br/>e2 = (W2)T . O   [O is the one hot vector of the softmax argmax]</span></pre><p id="9958" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">对于每个输入向量，隐藏层学习对应于权重矩阵W1的300个权重(Vwi)。<strong class="je hi"> <em class="kd"> Vwi </em> </strong>然后乘以权重矩阵W2，以便给出softmax输出。对于softmax输出中的每个节点，我们可以说存在来自W2矩阵的300 (Uwj)个参数(对于每个输出字j)。根据我们前面看到的，<br/> Vwi = e1和Uwj = e2。利用这一点，我们最终可以说点积:<br/> <strong class="je hi"> <em class="kd"> (Vwi)T . Uwj </em> </strong>在计算输出节点j处的小数概率中起作用。</p><p id="7651" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated"><strong class="je hi">注意:</strong>以上计算是针对一个输入向量Vwi</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es ml"><img src="../Images/46ade4b7b12b92aff686a54c2b2743cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*nblz2MoSK5kUrohnDShMlw.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">Vwi和Uwj可视化示意图(图3)</figcaption></figure><p id="9fce" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">利用这一点，我们可以说每个输入的softmax函数如下所示:</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es mm"><img src="../Images/df34c9baaf7efd60120023275f8b092d.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*dwE-9HUWCMtjYUUqp6GWiQ.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">(等式2)</figcaption></figure></div><div class="ab cl li lj go lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ha hb hc hd he"><h2 id="342d" class="km if hh bd ig kn ko kp ik kq kr ks io jn kt ku is jr kv kw iw jv kx ky ja kz bi translated"><strong class="ak">成本函数</strong></h2><p id="39ee" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">为了定义成本函数，我们必须回到我们实际定义目标的时候。我们的目标是在给定目标单词的情况下，最大化获得上下文单词的概率，如等式1所述。再多说一点吧。</p><p id="d722" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">首先，让我们来回答这个问题:<em class="kd">为什么是乘法运算？</em> <br/>和所有机器学习问题一样，我们假设东西。这里我们假设独立。也就是说，所有的上下文目标对都是相互独立的(就像抛硬币一样！)</p><p id="4282" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">这给了我们这样的东西:</p><blockquote class="ka kb kc"><p id="6944" class="jc jd kd je b jf ke jh ji jj kf jl jm kg kh jp jq ki kj jt ju kk kl jx jy jz ha bi translated">P(w1，w2，w3 | Wi) = P(w1|Wi)。P(w2|Wi)。P(w3|Wi)</p></blockquote><figure class="lb lc ld le fd lf er es paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="er es mn"><img src="../Images/7d5b4e338ad9ec6e570c7cef1ed45a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PaWHZFbv9f3JIkuZEwO3HQ.png"/></div></div></figure><p id="8e83" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">此外，你是否发现缺少了什么？</p><p id="c9b9" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">等式1仅考虑一个尺寸为2c+1的窗口。然而，我们需要考虑整个英语文本语料库中的所有窗口。因此，等式1必须扩展到我们遇到的所有T窗口。</p><p id="6eac" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">最后，为了便于微分和防止下溢，我们取目标函数的对数。这给我们留下了:</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es ms"><img src="../Images/742921203e71468e6bd06ed3239e747d.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*qY5-bD3tmzzX6QTVrMnr7A.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">我们除以T，使得成本函数不受语料库大小的影响—(等式3)</figcaption></figure><p id="0151" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">概率P可以使用等式2来计算</p></div><div class="ab cl li lj go lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ha hb hc hd he"><h2 id="b8b3" class="km if hh bd ig kn ko kp ik kq kr ks io jn kt ku is jr kv kw iw jv kx ky ja kz bi translated"><strong class="ak">计算问题</strong></h2><p id="c1dd" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">skip gram模型适用于较小的词汇量。然而，情况并非总是如此。词汇表的大小很容易扩大到数百万个单词，这对skip gram模型构成了严重威胁。</p><p id="513f" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">为了更好地理解这一点，我们必须回到等式2，看看分母中的求和。大约1000万的词汇量很容易使计算变得不可行。此外，必须对语料库中存在的每个上下文-目标对进行1000万次求和。这导致skip gram模型非常慢。</p><p id="0869" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">因此，下一个看似合理的问题是:<em class="kd">我们如何解决这个问题？</em></p><p id="3746" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">有几种方法可以降低这种复杂性。<br/> 1。子采样<br/> 2。噪声对比估计<br/> 3。负采样<br/> 4。分层软件最大值</p><p id="ea44" class="pw-post-body-paragraph jc jd hh je b jf ke jh ji jj kf jl jm jn kh jp jq jr kj jt ju jv kl jx jy jz ha bi translated">对上述解决方案的深入解释超出了这篇博文的范围。希望我能在不久的将来写下它们，并把它们链接到这里。</p></div><div class="ab cl li lj go lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ha hb hc hd he"><h1 id="3eff" class="ie if hh bd ig ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb bi translated"><strong class="ak">参考文献</strong></h1><ol class=""><li id="a8fd" class="mt mu hh je b jf jg jj jk jn mv jr mw jv mx jz my mz na nb bi translated"><a class="ae nc" href="https://www.youtube.com/watch?v=G5ah0QqkUgE&amp;list=PLxlkzujLkmQ8erDs5VSteKTmVg-y9Z1dR&amp;index=8" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=Co-YYk0731I&amp;list = plxlkzujlkmq 8 erds 5 vstektmvg-y9 z1dr&amp;index = 7</a></li><li id="db31" class="mt mu hh je b jf nd jj ne jn nf jr ng jv nh jz my mz na nb bi translated">Tomas Mikolov等人，向量空间中单词表示的有效估计，2013年</li><li id="b903" class="mt mu hh je b jf nd jj ne jn nf jr ng jv nh jz my mz na nb bi translated">Andrew NG(深度学习. ai系列课程5第2周)</li></ol></div></div>    
</body>
</html>