<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pytorch-tutorial-0-preliminaries-385e09bed825?source=collection_archive---------25-----------------------#2021-02-07">https://medium.com/analytics-vidhya/pytorch-tutorial-0-preliminaries-385e09bed825?source=collection_archive---------25-----------------------#2021-02-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><p id="a672" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">在这里，我们将快速浏览PyTorch的初级读本，它将与大多数ML问题相关。这包括一些非常基本的操作，如如何使用张量数据类型、函数梯度、与<code class="du ie if ig ih b">numpy</code>的互操作性以及一些基本的数学运算</p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="cb82" class="iq ir hh ih b fi is it l iu iv"># importing libraries<br/>from __future__ import print_function<br/>import torch<br/>import numpy as np</span></pre><p id="0e36" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">张量，你可能已经知道，是一个广义矩阵。这种数据结构是深度学习框架的基础，包括PyTorch和Tensorflow。</p><p id="63b1" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated"><strong class="hi iw"> 1。大小为(2，2)的零张量。或者具有2行2列的零矩阵。</strong></p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="16cf" class="iq ir hh ih b fi is it l iu iv">torch.zeros(2,2)</span><span id="18f5" class="iq ir hh ih b fi ix it l iu iv">tensor([[0., 0.],<br/>        [0., 0.]])</span></pre><p id="aee0" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated"><strong class="hi iw"> 2。</strong>大小为(3，2，2)的零张量。它可以被看作是三个2×2矩阵的列表。</p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="2a80" class="iq ir hh ih b fi is it l iu iv">torch.zeros(3,2,2)</span><span id="ec40" class="iq ir hh ih b fi ix it l iu iv">tensor([[[0., 0.],<br/>         [0., 0.]],<br/><br/>        [[0., 0.],<br/>         [0., 0.]],<br/><br/>        [[0., 0.],<br/>         [0., 0.]]])</span></pre><p id="bdee" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated"><strong class="hi iw"> 3。</strong>将numpy数组转换为torch张量</p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="34e1" class="iq ir hh ih b fi is it l iu iv">x = np.array([i for i in range(10)])<br/>t = torch.from_numpy(x)<br/>t</span><span id="bcc3" class="iq ir hh ih b fi ix it l iu iv">tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></pre><p id="9313" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated"><strong class="hi iw"> 4。</strong>生成包含随机数的形状为4，5的PyTorch张量。同样，来自正态分布的随机数</p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="47b0" class="iq ir hh ih b fi is it l iu iv"># tensor of shape (4,5) containing random numbers<br/>torch.rand(4, 5)</span><span id="29a8" class="iq ir hh ih b fi ix it l iu iv">tensor([[0.4474, 0.2355, 0.1144, 0.8174, 0.9252],<br/>        [0.9531, 0.8114, 0.9184, 0.8397, 0.2032],<br/>        [0.4815, 0.9815, 0.4809, 0.3215, 0.8845],<br/>        [0.1835, 0.0037, 0.6612, 0.0857, 0.7403]])</span><span id="1ef9" class="iq ir hh ih b fi ix it l iu iv"># tensor of shape (4,5) containing random numbers from normal distribution with mean=0 and std=1<br/>torch.randn(4, 5)</span><span id="ca05" class="iq ir hh ih b fi ix it l iu iv">tensor([[-1.6499, -1.5163, -0.6925, -1.6744,  1.7437],<br/>        [-0.2022,  0.0557,  1.2434,  0.2079, -1.2519],<br/>        [-0.2232,  0.0858, -0.9037, -0.0357, -0.4679],<br/>        [-1.8760,  1.0836,  0.4805, -2.1150, -1.2654]])</span></pre><p id="adfb" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated"><strong class="hi iw"> 5。</strong>按行和按列连接张量。</p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="2f7d" class="iq ir hh ih b fi is it l iu iv">zz = torch.rand(4,5)</span><span id="d60f" class="iq ir hh ih b fi ix it l iu iv"># concat row wise<br/>concat_row_wise = torch.cat((zz, zz), 0)<br/>concat_row_wise</span><span id="980c" class="iq ir hh ih b fi ix it l iu iv">tensor([[0.3640, 0.4250, 0.3018, 0.6409, 0.3439],<br/>        [0.6825, 0.6012, 0.2290, 0.3628, 0.5291],<br/>        [0.3116, 0.5469, 0.9689, 0.5661, 0.9783],<br/>        [0.1276, 0.7452, 0.8182, 0.2522, 0.0682],<br/>        [0.3640, 0.4250, 0.3018, 0.6409, 0.3439],<br/>        [0.6825, 0.6012, 0.2290, 0.3628, 0.5291],<br/>        [0.3116, 0.5469, 0.9689, 0.5661, 0.9783],<br/>        [0.1276, 0.7452, 0.8182, 0.2522, 0.0682]])</span><span id="3f72" class="iq ir hh ih b fi ix it l iu iv"># concat column wise<br/>concat_col_wise = torch.cat((zz, zz), 1)<br/>concat_col_wise</span><span id="727c" class="iq ir hh ih b fi ix it l iu iv">tensor([[0.3640, 0.4250, 0.3018, 0.6409, 0.3439, 0.3640, 0.4250, 0.3018, 0.6409,<br/>         0.3439],<br/>        [0.6825, 0.6012, 0.2290, 0.3628, 0.5291, 0.6825, 0.6012, 0.2290, 0.3628,<br/>         0.5291],<br/>        [0.3116, 0.5469, 0.9689, 0.5661, 0.9783, 0.3116, 0.5469, 0.9689, 0.5661,<br/>         0.9783],<br/>        [0.1276, 0.7452, 0.8182, 0.2522, 0.0682, 0.1276, 0.7452, 0.8182, 0.2522,<br/>         0.0682]])</span></pre><p id="9707" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated"><strong class="hi iw"> 6。</strong>改变张量类型</p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="fa2c" class="iq ir hh ih b fi is it l iu iv"># we will take the tensor, t, created in Question 3 above<br/>xx = np.array([i for i in range(10)])<br/>tt = torch.from_numpy(xx)<br/>print('Type of Tensor, tt before changing datatype', tt.dtype)<br/>tt = tt.type(torch.FloatTensor)<br/>print('Type of Tensor, tt after changing datatype', tt.dtype)</span><span id="064b" class="iq ir hh ih b fi ix it l iu iv">Type of Tensor, tt before changing datatype torch.int64<br/>Type of Tensor, tt after changing datatype torch.float32</span></pre><p id="2378" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated"><strong class="hi iw"> 7。关于张量的一些数学运算</strong></p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="1cd1" class="iq ir hh ih b fi is it l iu iv">torch.add(tt, 2.0), torch.mul(tt, 4.0), torch.sigmoid(tt), torch.exp(tt)</span><span id="348f" class="iq ir hh ih b fi ix it l iu iv">(tensor([ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.]),<br/> tensor([ 0.,  4.,  8., 12., 16., 20., 24., 28., 32., 36.]),<br/> tensor([0.5000, 0.7311, 0.8808, 0.9526, 0.9820, 0.9933, 0.9975, 0.9991, 0.9997,<br/>         0.9999]),<br/> tensor([1.0000e+00, 2.7183e+00, 7.3891e+00, 2.0086e+01, 5.4598e+01, 1.4841e+02,<br/>         4.0343e+02, 1.0966e+03, 2.9810e+03, 8.1031e+03]))</span></pre><p id="fb68" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated"><strong class="hi iw"> 8。</strong>矩阵运算</p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="c147" class="iq ir hh ih b fi is it l iu iv">A = torch.rand((4, 4))<br/>B = torch.rand((4, 2))<br/>y = torch.rand((4, ))<br/><br/>print('Matrix scalar addition', A + 0.1)<br/>print('Matrix scalar substraction', A - 0.1)<br/>print('Matrix dot product', torch.dot(y, y))<br/>print('Matrix-Vector product', torch.matmul(A, y))<br/>print('Matrix-Matrix product', torch.matmul(A, B))</span><span id="6ab4" class="iq ir hh ih b fi ix it l iu iv">Matrix scalar addition tensor([[0.9696, 0.8804, 0.6037, 0.7482],<br/>        [0.5646, 0.9997, 0.3943, 0.5602],<br/>        [0.6013, 0.1008, 0.8565, 0.9929],<br/>        [0.2433, 1.0106, 1.0618, 0.6003]])<br/>Matrix scalar substraction tensor([[ 0.7696,  0.6804,  0.4037,  0.5482],<br/>        [ 0.3646,  0.7997,  0.1943,  0.3602],<br/>        [ 0.4013, -0.0992,  0.6565,  0.7929],<br/>        [ 0.0433,  0.8106,  0.8618,  0.4003]])<br/>Matrix dot product tensor(0.9021)<br/>Matrix-Vector product tensor([1.1309, 0.7484, 0.6812, 0.6732])<br/>Matrix-Matrix product tensor([[0.9110, 1.1572],<br/>        [0.5398, 0.8662],<br/>        [0.6528, 1.0397],<br/>        [0.5384, 1.1976]])</span></pre><p id="b8b8" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated"><strong class="hi iw"> 9。</strong>查看操作或整形</p><p id="5c6a" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">注意:视图的工作方式类似于numpy中的整形。但是，如果更改视图张量中的值，原始张量数据也会更改。</p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="5e25" class="iq ir hh ih b fi is it l iu iv"># the following variable A is taken from above question..<br/># ..A has original dimension of 4x4. It is changed into 2x8.<br/>A.view(2, 8)</span><span id="d7e0" class="iq ir hh ih b fi ix it l iu iv">tensor([[8.6963e-01, 7.8036e-01, 5.0374e-01, 6.4817e-01, 4.6455e-01, 8.9967e-01,<br/>         2.9432e-01, 4.6023e-01],<br/>        [5.0126e-01, 8.1986e-04, 7.5655e-01, 8.9285e-01, 1.4329e-01, 9.1057e-01,<br/>         9.6179e-01, 5.0029e-01]])</span></pre><p id="f80f" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">10。松开并挤压</p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="7e37" class="iq ir hh ih b fi is it l iu iv"># `unsqueeze` inserts a singleton dimension at specified postion<br/>x = torch.tensor([1, 2, 3, 4])<br/>unsqueezed = torch.unsqueeze(x, 0)<br/>unsqueezed</span><span id="5ddf" class="iq ir hh ih b fi ix it l iu iv">tensor([[1, 2, 3, 4]])</span><span id="5a82" class="iq ir hh ih b fi ix it l iu iv"># `squeeze` remove all dimesnions of 1 in the tensor<br/><br/># Following from PyTorch documentation...<br/>#  if input is of shape: (A×1×B×C×1×D) then the out tensor will be of shape: (A×B×C×D) .<br/>#  When dim is given, a squeeze operation is done only in the given dimension. <br/># If input is of shape: (A×1×B) , squeeze(input, 0) leaves the tensor unchanged,..<br/># ..but squeeze(input, 1) will squeeze the tensor to the shape (A×B).<br/>    <br/>unsqueezed.squeeze()</span><span id="96ee" class="iq ir hh ih b fi ix it l iu iv">tensor([1, 2, 3, 4])</span></pre><p id="d8ad" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">11。功能<code class="du ie if ig ih b">as_tensor</code></p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="52f1" class="iq ir hh ih b fi is it l iu iv"># Perhaps very commonly used operation.<br/># You can convert a list, tuple, NumPy ndarray, scalar, and other types to torch tensor.<br/>a = [1, 2, 3]<br/>torch.as_tensor(a)</span><span id="b199" class="iq ir hh ih b fi ix it l iu iv">tensor([1, 2, 3])</span></pre><p id="38d0" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">12。 <code class="du ie if ig ih b">stack</code> vs <code class="du ie if ig ih b">cat</code></p><p id="c955" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">堆叠沿着全新的轴添加张量数据，而<code class="du ie if ig ih b">cat</code>在现有的轴中连接</p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="6f44" class="iq ir hh ih b fi is it l iu iv">t1 = torch.rand((3,4))<br/>t2 = torch.rand((3,4))</span></pre><p id="8ca3" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">如果我们沿着第一维堆叠2个以上的张量(在这种情况下，您可以将其视为行维，因为它是2维矩阵)，我们将总共有2个新行。然而，当我们连接时，在0轴或行上追加数据。</p><p id="105a" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">类似的逻辑可以用在列或第二轴上。</p><pre class="ii ij ik il fd im ih in io aw ip bi"><span id="d1fa" class="iq ir hh ih b fi is it l iu iv"># stack and concat on rows<br/>torch.stack((t1,t2), dim=0).shape, torch.cat((t1,t2), dim=0).shape</span><span id="4be2" class="iq ir hh ih b fi ix it l iu iv">(torch.Size([2, 3, 4]), torch.Size([6, 4]))</span><span id="5f45" class="iq ir hh ih b fi ix it l iu iv"># stack and concat on cols<br/>torch.stack((t1,t2), dim=1).shape, torch.cat((t1,t2), dim=1).shape</span><span id="ebba" class="iq ir hh ih b fi ix it l iu iv">(torch.Size([3, 2, 4]), torch.Size([3, 8]))</span></pre></div></div>    
</body>
</html>