<html>
<head>
<title>Vanishing/Exploding Gradients</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">消失/爆炸渐变</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/vanishing-exploding-gradients-b00b4735d70d?source=collection_archive---------0-----------------------#2022-02-16">https://medium.com/analytics-vidhya/vanishing-exploding-gradients-b00b4735d70d?source=collection_archive---------0-----------------------#2022-02-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="d7d8" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">窥视反向传播算法</h2></div><p id="7785" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">反向传播这个术语及其在神经网络中的一般用途是在Rumelhart，Hinton &amp; Williams ( <a class="ae js" href="https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769" rel="noopener ugc nofollow" target="_blank"> 1986a </a>)中介绍的，然后在Rumelhart，hint on&amp;Williams(<a class="ae js" href="https://archive.org/details/paralleldistribu00rume" rel="noopener ugc nofollow" target="_blank">1986 b</a>)中得到阐述和推广。但是这项技术在20世纪60年代被独立地重新发现了很多次。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/f28a038199e9714e783af5d543949421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JD9ELa182CpgDJvoyYw54Q.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">来源:<a class="ae js" href="https://www.researchgate.net/figure/Schematic-diagram-of-backpropagation-training-algorithm-and-typical-neuron-model_fig2_275721804" rel="noopener ugc nofollow" target="_blank">研究之门</a></figcaption></figure><p id="55fa" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当拟合到神经网络时，反向传播基于单个输入-输出样本的网络权重来计算损失函数的梯度，并且这样做是有效的，不像直接单独计算关于每个权重的梯度。基本上，它使用梯度以梯度下降步骤更新每个参数。</p><h2 id="4d69" class="kj kk hh bd kl km kn ko kp kq kr ks kt jf ku kv kw jj kx ky kz jn la lb lc ld bi translated">消失问题</h2><p id="321f" class="pw-post-body-paragraph iw ix hh iy b iz le ii jb jc lf il je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">然而，随着算法向下进行到较低层，梯度通常变得越来越小。因此，较低层的连接权重实际上没有变化。这就是所谓的<strong class="iy hi"> <em class="lj">消失渐变问题</em> </strong>。</p><h2 id="97af" class="kj kk hh bd kl km kn ko kp kq kr ks kt jf ku kv kw jj kx ky kz jn la lb lc ld bi translated">爆炸问题</h2><p id="fc28" class="pw-post-body-paragraph iw ix hh iy b iz le ii jb jc lf il je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">另一方面，在某些情况下，梯度可以变得越来越大。如此多的层得到疯狂的大权重更新。它导致算法发散。这叫做<strong class="iy hi"> <em class="lj">爆炸渐变问题</em> </strong>。</p><p id="ba78" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">好吧，你可能会问为什么它会消失或爆炸。</p><p id="51b1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在某些激活函数的输入和输出之间存在巨大的差异，例如逻辑(sigmoid)激活函数。每层输出的方差远大于其输入的方差。在网络中前进，方差在每一层之后不断增加，直到激活函数在顶层饱和。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lk"><img src="../Images/297ca42c120fc7b66b0f3fcf9f94f669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*RJ2k1KsoraEsEKeoO7r4hQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">来源:<a class="ae js" href="https://www.google.com/search?q=sigmoid+function&amp;tbm=isch&amp;ved=2ahUKEwja5KyO4Pr1AhVS0OAKHe7XAAwQ2-cCegQIABAA&amp;oq=sigmoid+&amp;gs_lcp=CgNpbWcQARgAMgQIABBDMgUIABCABDIECAAQQzIFCAAQgAQyBQgAEIAEMgQIABBDMgQIABBDMgUIABCABDIECAAQQzIECAAQQzoECAAQHjoGCAAQCBAeUJEMWLIVYMQmaABwAHgAgAF4iAHOB5IBAzEuOJgBAKABAaoBC2d3cy13aXotaW1nwAEB&amp;sclient=img&amp;ei=jPYHYtruJdKggwfur4Ng&amp;bih=937&amp;biw=1920&amp;rlz=1C1GCEA_enTR961TR961#imgrc=bgaPC4vKBrRtcM&amp;imgdii=_CGSSGgwGrwd5M" rel="noopener ugc nofollow" target="_blank">谷歌图片</a></figcaption></figure><p id="7b9b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在某些情况下，参数可能变得太大，以至于溢出并导致NaN值。</p><p id="7f73" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们如何解决这个问题？</p><p id="7462" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有几种方法可以解决这个问题。这是第一个:</p><h2 id="3d07" class="kj kk hh bd kl km kn ko kp kq kr ks kt jf ku kv kw jj kx ky kz jn la lb lc ld bi translated">Glorot和He初始化</h2><p id="cb60" class="pw-post-body-paragraph iw ix hh iy b iz le ii jb jc lf il je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">当正向预测和反向传播梯度时，我们希望信号流是适当的。我们不希望信号减弱或爆炸。</p><p id="604f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Glorot和Bengio认为，我们需要每层输出的方差等于输入的方差，我们还需要梯度在反向流过一个层之前和之后具有相等的方差。</p><p id="bb32" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">除非该层中的输入数量等于该层中的神经元数量(这些数量称为该层的扇入、扇出)，否则这两个条件都不能得到保证。然而，他们提出了一个在实践中证明非常有效的折衷方案:每层的连接权重必须随机初始化。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ll"><img src="../Images/05760abf8305eec957f3766e8f60dfb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*81wAEt7WYafAk9UV7CMLvg.png"/></div></figure><p id="dd62" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">此外，使用Glorot初始化可以大大加快训练速度。</p><p id="55d1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">默认情况下，Keras使用均匀分布的Glorot初始化。您可以通过设置kernel_initializer="he_uniform "或kernel_initializer="he_normal "来将其更改为He初始化</p><p id="40fc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">比如说；</p><blockquote class="lm ln lo"><p id="5eee" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi translated">keras.layer.Dense(25，activation = "relu "，kernel_initializer="he_normal ")</p></blockquote><h2 id="165d" class="kj kk hh bd kl km kn ko kp kq kr ks kt jf ku kv kw jj kx ky kz jn la lb lc ld bi translated">不饱和激活函数</h2><p id="2d0b" class="pw-post-body-paragraph iw ix hh iy b iz le ii jb jc lf il je jf lg jh ji jj lh jl jm jn li jp jq jr ha bi translated">在检查sigmoid激活函数时，我提到对于较大的输入，饱和度是梯度消失背后的主要原因，由于其性质，因此它不能用于网络的隐藏层。当然，有可能在深度神经网络中找到更好的激活函数！让我们看看ReLU。</p><p id="1ef3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">整流线性单元(ReLU) </strong></p><p id="8c79" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">它是优选的，因为它在正值时不饱和。该功能对激活和输入提供更高的灵敏度，并防止容易饱和。此外，它的计算速度相当快。</p><p id="fccf" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然而，像所有的好东西一样，这一个有一个问题:<em class="lj">垂死的ReLU </em> s</p><p id="8417" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一些神经元在训练期间停止输出除0以外的任何信号，它们实际上已经死亡。特别是如果你使用了一个大的学习率，你可以看到你的网络中有一半的神经元是死的。</p><p id="597f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">作为这个问题的解决方案，您可能想要使用ReLU的一个变体:<em class="lj"> leaky ReLU </em>。</p><p id="85ec" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">漏整流线性单元(</strong> <em class="lj"> </em> <strong class="iy hi"> <em class="lj">漏整流单元</em> ) </strong></p><p id="a868" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Leaky ReLU是一种基于ReLU的激活函数，但是它对于负值具有小的斜率，而不是平坦的斜率。斜率系数是在训练前确定的，也就是说，它不是在训练过程中学习的。泄漏量由一个超参数控制。泄漏的较小斜率确保由泄漏的ReLU供电的神经元永远不会死亡；虽然他们可能会在漫长的训练阶段陷入昏迷，但最后总有醒来的机会。</p><p id="3f29" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">随机泄漏整流线性单元(RReLU) </strong></p><p id="a2cf" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">他们还评估了RReLU，其中泄漏参数在训练期间在给定范围内随机选取，并在测试期间固定为平均值。它还表现得相当好，并作为一个正则化，即减少过度拟合训练集的风险。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ls"><img src="../Images/ea6b14b354da6c1acdc47795fa46bdfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N4zJLGo_F4SRDOYbKvckgA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">来源:<a class="ae js" href="https://www.google.com/search?q=leaky+relu&amp;tbm=isch&amp;ved=2ahUKEwi6ooP4mYT2AhVKyBQKHaBsBYcQ2-cCegQIABAA&amp;oq=leaky+relu&amp;gs_lcp=CgNpbWcQAzIFCAAQgAQyBAgAEB4yBAgAEB4yBAgAEB4yBAgAEB4yBAgAEB4yBAgAEB4yBAgAEB4yBAgAEB4yBAgAEB46BwgjEO8DECc6BggAEAcQHlDzBViADmCcFmgAcAB4AIABhwGIAYEHkgEDMC43mAEAoAEBqgELZ3dzLXdpei1pbWfAAQE&amp;sclient=img&amp;ei=JOsMYrqAK8qQU6DZlbgI&amp;bih=927&amp;biw=1920&amp;client=firefox-b-d#imgrc=p-ApAAMq3vlSMM" rel="noopener ugc nofollow" target="_blank">谷歌图片</a></figcaption></figure><p id="8002" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">另见:普雷卢，eLU。</p><p id="cd01" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用泄漏的ReLU</p><blockquote class="lm ln lo"><p id="ebcb" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi translated">leaky _ relu = keras . layers . leaky relu(alpha = 0.2)</p><p id="03de" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi translated">layer = keras.layers.Dense(10，activation = leaky_relu，内核初始化器="he_normal ")</p></blockquote><p id="e3ed" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">批量归一化</strong></p><p id="4089" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">即使我前面提到的方法在训练开始时减少了消失/爆炸，但它们并不保证在训练期间不会返回。</p><p id="545d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">它包括在每个隐藏层的激活功能之前或之后在模型中添加一个操作。该操作简单地对每个输入进行零中心化和归一化，然后使用每层两个新的参数向量对结果进行缩放和移位:一个用于缩放，另一个用于移位。换句话说，该操作让模型学习每个图层输入的最佳比例和平均值。</p><p id="f6c1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了对输入进行零中心化和归一化，算法需要估计每个输入的平均值和标准差。这是通过评估当前小批量中每个输入的平均值和标准偏差来实现的。</p><p id="17d8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然而，BN增加了模型的复杂性。此外，神经网络的预测速度较慢。</p><blockquote class="lm ln lo"><p id="5273" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi translated">model = keras . models . sequential([</p><p id="db66" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi translated">keras . layers . flatten(input _ shape =[28，28])，</p><p id="7efe" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi translated">keras . layers . batch normalization()，</p><p id="59b3" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi translated">keras.layers.Dense(300，activation="relu ")，</p><p id="5ea6" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi translated">keras . layers . batch normalization()，</p><p id="d25e" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi translated">keras.layers.Dense(100，activation="relu ")，</p><p id="b82d" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi translated">keras . layers . batch normalization()，</p><p id="dc14" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi translated">keras.layers.Dense(10，activation="softmax ")</p><p id="979b" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi">])</p></blockquote><p id="489f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">渐变裁剪</strong></p><p id="0ffb" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了减轻爆炸梯度问题，我们可以使用这种技术。我们在反向传播过程中剪切梯度，使它们不超过某些阈值。通常，这种技术用于递归神经网络。对于其他类型的网络，BN通常就足够了。</p><blockquote class="lm ln lo"><p id="13c4" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi translated">optimizer = tensor flow . keras . optimizer . SGD(clip value = 1.0)</p><p id="8586" class="iw ix lj iy b iz ja ii jb jc jd il je lp jg jh ji lq jk jl jm lr jo jp jq jr ha bi translated">model.compile(loss='mse '，optimizer=optimizer)</p></blockquote><p id="a739" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这段代码意味着，关于每个可训练参数，损失的所有偏导数将被限制在-1.0和1.0之间。</p><p id="ff86" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">要了解更多细节，你可以查阅我在参考资料中提到的奥雷连·盖伦的书。这本书真的是必读，我强烈推荐。</p><p id="346b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">还有一个比较短的推荐，我建议你看一下neptune.ai发表的文章<a class="ae js" href="https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing" rel="noopener ugc nofollow" target="_blank">《神经网络模型中的消失和爆炸梯度:调试、监控和修复》</a>，我可以说是一篇非常详细的研究，用代码丰富。</p></div><div class="ab cl lt lu go lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ha hb hc hd he"><p id="acf0" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">参考</p><ul class=""><li id="ae0e" class="ma mb hh iy b iz ja jc jd jf mc jj md jn me jr mf mg mh mi bi translated"><a class="ae js" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener ugc nofollow" target="_blank">https://www . oreilly . com/library/view/hands-on-machine-learning/9781492032632/</a></li><li id="e79d" class="ma mb hh iy b iz mj jc mk jf ml jj mm jn mn jr mf mg mh mi bi translated">【https://paperswithcode.com/method/leaky-relu T4】</li></ul></div></div>    
</body>
</html>