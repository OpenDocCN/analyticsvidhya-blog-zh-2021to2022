<html>
<head>
<title>Data augmentation with transformer models for named entity recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于命名实体识别的变压器模型的数据扩充</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/data-augmentation-with-transformer-models-for-named-entity-recognition-a662e834b524?source=collection_archive---------13-----------------------#2021-01-09">https://medium.com/analytics-vidhya/data-augmentation-with-transformer-models-for-named-entity-recognition-a662e834b524?source=collection_archive---------13-----------------------#2021-01-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/51be420b2d9d59f83d08efc37c39d330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*At2p6h80MY3SFZlcUEL30Q.png"/></div></div></figure><p id="809f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">基于语言模型的预训练模型，如<a class="ae jn" href="https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/" rel="noopener ugc nofollow" target="_blank">伯特</a>已经在不同的NLP任务中提供了显著的增益。对于许多<a class="ae jn" href="https://www.depends-on-the-definition.com/tags/nlp/" rel="noopener ugc nofollow" target="_blank"> NLP </a>任务来说，带标签的训练数据是稀缺的，并且获取它们是一项昂贵且要求高的任务。数据扩充可以通过人工扰动标记的训练样本来增加可用数据点的绝对数量，从而有助于提高数据效率。在NLP中，这通常是通过用基于词典的同义词替换单词或者<a class="ae jn" href="https://arxiv.org/abs/2005.05909" rel="noopener ugc nofollow" target="_blank">翻译成不同的语言然后返回</a>来实现的。</p><p id="736e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这篇文章探索了一种不同的方法。根据Kumar等人的建议，我们将从预训练的变压器中采样，以增强用于<a class="ae jn" href="https://www.depends-on-the-definition.com/tags/named-entity-recognition" rel="noopener ugc nofollow" target="_blank">命名实体识别</a>的小型、带标签的文本数据集。艾尔。他们提出使用transformer模型从文本数据生成增强版本。他们提出了以下算法:</p><figure class="jp jq jr js fd ii er es paragraph-image"><div class="er es jo"><img src="../Images/de6c2455231fb5920f8e8f5525a9eb3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*xLH3SLS2W3GM5xh6IRIOIQ.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated"><a class="ae jn" href="https://arxiv.org/abs/2003.02245" rel="noopener ugc nofollow" target="_blank">图片来源:库马尔等人。艾尔。</a></figcaption></figure><p id="5c91" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了简单起见，我们跳过第1行中的微调步骤，直接从预先训练的模型生成。让我们看看如何使用预训练的基于转换器的模型，如自动编码器模型，如BERT，用于使用pytorch进行命名实体识别的条件数据增强。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="9e7f" class="kc kd hh jy b fi ke kf l kg kh"><strong class="jy hi">import</strong> <strong class="jy hi">torch</strong><br/><strong class="jy hi">import</strong> <strong class="jy hi">torch.nn</strong> <strong class="jy hi">as</strong> <strong class="jy hi">nn</strong><br/><strong class="jy hi">import</strong> <strong class="jy hi">torch.nn.functional</strong> <strong class="jy hi">as</strong> <strong class="jy hi">F</strong><br/><strong class="jy hi">import</strong> <strong class="jy hi">torch.optim</strong> <strong class="jy hi">as</strong> <strong class="jy hi">optim</strong><br/><strong class="jy hi">from</strong> <strong class="jy hi">torch.utils.data</strong> <strong class="jy hi">import</strong> TensorDataset, DataLoader<br/><strong class="jy hi">from</strong> <strong class="jy hi">tqdm.notebook</strong> <strong class="jy hi">import</strong> tqdm<br/><br/>torch.manual_seed(2020)<br/><br/><strong class="jy hi">print</strong>(torch.cuda.get_device_name(torch.cuda.current_device()))<br/><strong class="jy hi">print</strong>(torch.cuda.is_available())<br/><strong class="jy hi">print</strong>(torch.__version__)</span></pre></div><div class="ab cl ki kj go kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="ha hb hc hd he"><h1 id="6326" class="kp kd hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">加载数据</h1><p id="afd6" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">在我们做任何事情之前，我们加载示例数据集。你可能从我在命名实体识别上的其他<a class="ae jn" href="https://www.depends-on-the-definition.com/tags/named-entity-recognition" rel="noopener ugc nofollow" target="_blank">帖子中了解到这一点。</a></p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="5094" class="kc kd hh jy b fi ke kf l kg kh"><strong class="jy hi">import</strong> <strong class="jy hi">pandas</strong> <strong class="jy hi">as</strong> <strong class="jy hi">pd</strong><br/><strong class="jy hi">import</strong> <strong class="jy hi">numpy</strong> <strong class="jy hi">as</strong> <strong class="jy hi">np</strong><br/><br/>data = pd.read_csv("ner_dataset.csv", encoding="latin1")<br/>data = data.fillna(method="ffill")</span><span id="f91b" class="kc kd hh jy b fi lr kf l kg kh"><strong class="jy hi">class</strong> <strong class="jy hi">SentenceGetter</strong>(object):<br/>    <br/>    <strong class="jy hi">def</strong> __init__(self, data):<br/>        self.n_sent = 1<br/>        self.data = data<br/>        self.empty = False<br/>        agg_func = <strong class="jy hi">lambda</strong> s: [(w, p, t) <strong class="jy hi">for</strong> w, p, t <strong class="jy hi">in</strong> zip(s["Word"].values.tolist(),<br/>                                                           s["POS"].values.tolist(),<br/>                                                           s["Tag"].values.tolist())]<br/>        self.grouped = self.data.groupby("Sentence #").apply(agg_func)<br/>        self.sentences = [s <strong class="jy hi">for</strong> s <strong class="jy hi">in</strong> self.grouped]<br/>    <br/>    <strong class="jy hi">def</strong> get_next(self):<br/>        <strong class="jy hi">try</strong>:<br/>            s = self.grouped["Sentence: {}".format(self.n_sent)]<br/>            self.n_sent += 1<br/>            <strong class="jy hi">return</strong> s<br/>        <strong class="jy hi">except</strong>:<br/>            <strong class="jy hi">return</strong> None</span><span id="7030" class="kc kd hh jy b fi lr kf l kg kh">getter = SentenceGetter(data)<br/>sentences = getter.sentences</span><span id="2009" class="kc kd hh jy b fi lr kf l kg kh">tags = ["[PAD]"]<br/>tags.extend(list(set(data["Tag"].values)))<br/>tag2idx = {t: i <strong class="jy hi">for</strong> i, t <strong class="jy hi">in</strong> enumerate(tags)}<br/><br/>words = ["[PAD]", "[UNK]"]<br/>words.extend(list(set(data["Word"].values)))<br/>word2idx = {t: i <strong class="jy hi">for</strong> i, t <strong class="jy hi">in</strong> enumerate(words)}</span></pre><p id="91b3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们生成一个用于验证目的的训练测试分割。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="6ddf" class="kc kd hh jy b fi ke kf l kg kh">test_sentences, val_sentences, train_sentences = sentences[:15000], sentences[15000:20000], sentences[20000:]</span></pre></div><div class="ab cl ki kj go kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="ha hb hc hd he"><h1 id="d965" class="kp kd hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">使用转换器模型构建数据增强器</h1><p id="59a9" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">在<a class="ae jn" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank"> huggingface transformer库</a>之上，我们构建了一个小的python类来扩充一段文本。注意，这种实现效率很低，因为我们需要保持原始的标记化结构来匹配标签，而<code class="du ls lt lu jy b">fill-mask pipeline</code>一次只允许替换一个屏蔽的标记。有了更复杂的机制来将标签匹配回扩充的文本，这可以变得非常快。为了简单起见，这里省略了这种方法。</p><p id="5460" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们为输入样本创建一个扩充示例，通过用屏蔽令牌<code class="du ls lt lu jy b">&lt;mask&gt;</code>递增地替换令牌，并用预训练模型生成的令牌填充它。我们使用DistilRoBERTa基本模型来生成文本。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="4f61" class="kc kd hh jy b fi ke kf l kg kh"><strong class="jy hi">import</strong> <strong class="jy hi">random</strong><br/><strong class="jy hi">from</strong> <strong class="jy hi">transformers</strong> <strong class="jy hi">import</strong> pipeline</span><span id="dac3" class="kc kd hh jy b fi lr kf l kg kh"><strong class="jy hi">class</strong> <strong class="jy hi">TransformerAugmenter</strong>():<br/>    """<br/>    Use the pretrained masked language model to generate more<br/>    labeled samples from one labeled sentence.<br/>    """<br/>    <br/>    <strong class="jy hi">def</strong> __init__(self):<br/>        self.num_sample_tokens = 5<br/>        self.fill_mask = pipeline(<br/>            "fill-mask",<br/>            topk=self.num_sample_tokens,<br/>            model="distilroberta-base"<br/>        )<br/>    <br/>    <strong class="jy hi">def</strong> generate(self, sentence, num_replace_tokens=3):<br/>        """Return a list of n augmented sentences."""<br/>              <br/>        <em class="lv"># run as often as tokens should be replaced</em><br/>        augmented_sentence = sentence.copy()<br/>        <strong class="jy hi">for</strong> i <strong class="jy hi">in</strong> range(num_replace_tokens):<br/>            <em class="lv"># join the text</em><br/>            text = " ".join([w[0] <strong class="jy hi">for</strong> w <strong class="jy hi">in</strong> augmented_sentence])<br/>            <em class="lv"># pick a token</em><br/>            replace_token = random.choice(augmented_sentence)<br/>            <em class="lv"># mask the picked token</em><br/>            masked_text = text.replace(<br/>                replace_token[0],<br/>                f"{self.fill_mask.tokenizer.mask_token}",<br/>                1            <br/>            )<br/>            <em class="lv"># fill in the masked token with Bert</em><br/>            res = self.fill_mask(masked_text)[random.choice(range(self.num_sample_tokens))]<br/>            <em class="lv"># create output samples list</em><br/>            tmp_sentence, augmented_sentence = augmented_sentence.copy(), []<br/>            <strong class="jy hi">for</strong> w <strong class="jy hi">in</strong> tmp_sentence:<br/>                <strong class="jy hi">if</strong> w[0] == replace_token[0]:<br/>                    augmented_sentence.append((res["token_str"].replace("Ġ", ""), w[1], w[2]))<br/>                <strong class="jy hi">else</strong>:<br/>                    augmented_sentence.append(w)<br/>            text = " ".join([w[0] <strong class="jy hi">for</strong> w <strong class="jy hi">in</strong> augmented_sentence])<br/>        <strong class="jy hi">return</strong> [sentence, augmented_sentence]</span><span id="afc0" class="kc kd hh jy b fi lr kf l kg kh">augmenter = TransformerAugmenter()</span></pre><p id="8b10" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们看看一个增强的句子是什么样子的。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="9b7f" class="kc kd hh jy b fi ke kf l kg kh">augmented_sentences = augmenter.generate(train_sentences[12], num_replace_tokens=7); augmented_sentences</span><span id="ba9b" class="kc kd hh jy b fi lr kf l kg kh">[[('In', 'IN', 'O'),<br/>  ('Washington', 'NNP', 'B-geo'),<br/>  (',', ',', 'O'),<br/>  ('a', 'DT', 'O'),<br/>  ('White', 'NNP', 'B-org'),<br/>  ('House', 'NNP', 'I-org'),<br/>  ('spokesman', 'NN', 'O'),<br/>  (',', ',', 'O'),<br/>  ('Scott', 'NNP', 'B-per'),<br/>  ('McClellan', 'NNP', 'I-per'),<br/>  (',', ',', 'O'),<br/>  ('said', 'VBD', 'O'),<br/>  ('the', 'DT', 'O'),<br/>  ('remarks', 'NNS', 'O'),<br/>  ('underscore', 'VBP', 'O'),<br/>  ('the', 'DT', 'O'),<br/>  ('Bush', 'NNP', 'B-geo'),<br/>  ('administration', 'NN', 'O'),<br/>  ("'s", 'POS', 'O'),<br/>  ('concerns', 'NNS', 'O'),<br/>  ('about', 'IN', 'O'),<br/>  ('Iran', 'NNP', 'B-geo'),<br/>  ("'s", 'POS', 'O'),<br/>  ('nuclear', 'JJ', 'O'),<br/>  ('intentions', 'NNS', 'O'),<br/>  ('.', '.', 'O')],<br/> [('In', 'IN', 'O'),<br/>  ('Washington', 'NNP', 'B-geo'),<br/>  (',', ',', 'O'),<br/>  ('a', 'DT', 'O'),<br/>  ('White', 'NNP', 'B-org'),<br/>  ('administration', 'NNP', 'I-org'),<br/>  ('spokesperson', 'NN', 'O'),<br/>  (',', ',', 'O'),<br/>  ('Scott', 'NNP', 'B-per'),<br/>  ('McClellan', 'NNP', 'I-per'),<br/>  (',', ',', 'O'),<br/>  ('said', 'VBD', 'O'),<br/>  ('his', 'DT', 'O'),<br/>  ('remarks', 'NNS', 'O'),<br/>  ('underscore', 'VBP', 'O'),<br/>  ('his', 'DT', 'O'),<br/>  ('Bush', 'NNP', 'B-geo'),<br/>  ('administration', 'NN', 'O'),<br/>  ("'s", 'POS', 'O'),<br/>  ('concerns', 'NNS', 'O'),<br/>  ('about', 'IN', 'O'),<br/>  ('Iran', 'NNP', 'B-geo'),<br/>  ("'s", 'POS', 'O'),<br/>  ('nefarious', 'JJ', 'O'),<br/>  ('intentions', 'NNS', 'O'),<br/>  (',', '.', 'O')]]</span></pre></div><div class="ab cl ki kj go kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="ha hb hc hd he"><h1 id="d44e" class="kp kd hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">生成扩充数据集</h1><p id="5fd5" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">我们从一个只有1000个标记句子的小数据集开始。从那里我们用我们的增强方法产生更多的数据。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="60a0" class="kc kd hh jy b fi ke kf l kg kh"><em class="lv"># only use a thousand senteces with augmentation</em><br/>n_sentences = 1000<br/><br/>augmented_sentences = []<br/><strong class="jy hi">for</strong> sentence <strong class="jy hi">in</strong> tqdm(train_sentences[:n_sentences]):<br/>    augmented_sentences.extend(augmenter.generate(sentence, num_replace_tokens=7))</span><span id="b8c6" class="kc kd hh jy b fi lr kf l kg kh">len(augmented_sentences)</span></pre><p id="5a7d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以现在我们产生了1000个新样本。</p></div><div class="ab cl ki kj go kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="ha hb hc hd he"><h1 id="d508" class="kp kd hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">建立一个LSTM模型</h1><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="a883" class="kc kd hh jy b fi ke kf l kg kh"><strong class="jy hi">import</strong> <strong class="jy hi">pytorch_lightning</strong> <strong class="jy hi">as</strong> <strong class="jy hi">pl</strong><br/><strong class="jy hi">from</strong> <strong class="jy hi">pytorch_lightning.metrics.functional</strong> <strong class="jy hi">import</strong> accuracy, f1_score</span><span id="2415" class="kc kd hh jy b fi lr kf l kg kh"><strong class="jy hi">from</strong> <strong class="jy hi">keras.preprocessing.sequence</strong> <strong class="jy hi">import</strong> pad_sequences</span><span id="5c4f" class="kc kd hh jy b fi lr kf l kg kh">pl.__version__</span><span id="efc9" class="kc kd hh jy b fi lr kf l kg kh">'0.9.0'</span></pre><p id="fead" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们用<a class="ae jn" href="https://github.com/PyTorchLightning/pytorch-lightning" rel="noopener ugc nofollow" target="_blank"> pytorch-lightning </a>建立了一个相对简单的LSTM模型。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="28ac" class="kc kd hh jy b fi ke kf l kg kh">EMBEDDING_DIM = 128<br/>HIDDEN_DIM = 256<br/>BATCH_SIZE = 64<br/>MAX_LEN = 50</span><span id="f140" class="kc kd hh jy b fi lr kf l kg kh"><strong class="jy hi">class</strong> <strong class="jy hi">LightningLSTMTagger</strong>(pl.LightningModule):</span><span id="1c78" class="kc kd hh jy b fi lr kf l kg kh">    <strong class="jy hi">def</strong> __init__(self, embedding_dim, hidden_dim):<br/>        super(LightningLSTMTagger, self).__init__()<br/>        self.hidden_dim = hidden_dim<br/>        self.word_embeddings = nn.Embedding(len(word2idx), embedding_dim)<br/>        self.lstm = nn.LSTM(embedding_dim, hidden_dim)<br/>        self.fc = nn.Linear(hidden_dim, len(tag2idx))</span><span id="a6a7" class="kc kd hh jy b fi lr kf l kg kh">    <strong class="jy hi">def</strong> forward(self, sentence):<br/>        embeds = self.word_embeddings(sentence)<br/>        lstm_out, _ = self.lstm(embeds)<br/>        lstm_out = lstm_out<br/>        logits = self.fc(lstm_out)<br/>        <strong class="jy hi">return</strong> logits<br/>    <br/>    <strong class="jy hi">def</strong> training_step(self, batch, batch_idx):<br/>        x, y = batch<br/>        y_hat = self(x)<br/>        y_hat = y_hat.permute(0, 2, 1)<br/>        loss = nn.CrossEntropyLoss()(y_hat, y)<br/>        result = pl.TrainResult(minimize=loss)<br/>        result.log('f1', f1_score(torch.argmax(y_hat, dim=1), y), prog_bar=True)<br/>        <strong class="jy hi">return</strong> result<br/>    <br/>    <strong class="jy hi">def</strong> validation_step(self, batch, batch_idx):<br/>        x, y = batch<br/>        y_hat = self(x)<br/>        y_hat = y_hat.permute(0, 2, 1)<br/>        loss = nn.CrossEntropyLoss()(y_hat, y)<br/>        result = pl.EvalResult()<br/>        result.log('val_f1', f1_score(torch.argmax(y_hat, dim=1), y), prog_bar=True)<br/>        <strong class="jy hi">return</strong> result<br/>    <br/>    <strong class="jy hi">def</strong> test_step(self, batch, batch_idx):<br/>        x, y = batch<br/>        y_hat = self(x)<br/>        y_hat = y_hat.permute(0, 2, 1)<br/>        loss = nn.CrossEntropyLoss()(y_hat, y)<br/>        <strong class="jy hi">return</strong> {'test_f1':  f1_score(torch.argmax(y_hat, dim=1), y)}<br/>    <br/>    <strong class="jy hi">def</strong> configure_optimizers(self):<br/>        <strong class="jy hi">return</strong> torch.optim.Adam(self.parameters(), lr=5e-4)</span></pre></div><div class="ab cl ki kj go kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="ha hb hc hd he"><h1 id="7c1e" class="kp kd hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">比较</h1><p id="50d3" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">现在，我们用小的训练数据集和扩充的训练数据集训练出LSTM模型。然后我们在一个大的测试集上比较结果。首先，我们构建数据加载机制并设置数据加载器。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="189b" class="kc kd hh jy b fi ke kf l kg kh"><strong class="jy hi">def</strong> get_dataloader(seqs, max_len, batch_size, shuffle=False):<br/>    input_ids = pad_sequences([[word2idx.get(w[0], word2idx["[UNK]"]) <strong class="jy hi">for</strong> w <strong class="jy hi">in</strong> sent] <strong class="jy hi">for</strong> sent <strong class="jy hi">in</strong> seqs],<br/>                              maxlen=max_len, dtype="long", value=word2idx["[PAD]"],<br/>                              truncating="post", padding="post")</span><span id="d032" class="kc kd hh jy b fi lr kf l kg kh">    tag_ids = pad_sequences([[tag2idx[w[2]] <strong class="jy hi">for</strong> w <strong class="jy hi">in</strong> sent] <strong class="jy hi">for</strong> sent <strong class="jy hi">in</strong> seqs],<br/>                              maxlen=max_len, dtype="long", value=tag2idx["[PAD]"],<br/>                              truncating="post", padding="post")<br/>    <br/>    inputs = torch.tensor(input_ids)<br/>    tags = torch.tensor(tag_ids)<br/>    data = TensorDataset(inputs, tags)<br/>    <strong class="jy hi">return</strong> DataLoader(data, batch_size=batch_size, num_workers=16, shuffle=shuffle)</span><span id="2b2b" class="kc kd hh jy b fi lr kf l kg kh">ner_train_ds = get_dataloader(train_sentences[:2*n_sentences], MAX_LEN, BATCH_SIZE, shuffle=True)<br/>ner_aug_train_ds = get_dataloader(augmented_sentences, MAX_LEN, BATCH_SIZE, shuffle=True)<br/>ner_valid_ds = get_dataloader(val_sentences, MAX_LEN, BATCH_SIZE)<br/>ner_test_ds = get_dataloader(test_sentences, MAX_LEN, BATCH_SIZE)</span></pre></div><div class="ab cl ki kj go kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="ha hb hc hd he"><h1 id="7e07" class="kp kd hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">在小训练数据集上训练LSTM</h1><p id="8297" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">为了比较，我们首先在较小版本的训练数据集上训练LSTM网络。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="9847" class="kc kd hh jy b fi ke kf l kg kh">tagger = LightningLSTMTagger(<br/>    EMBEDDING_DIM,<br/>    HIDDEN_DIM<br/>)</span><span id="5f8b" class="kc kd hh jy b fi lr kf l kg kh">trainer = pl.Trainer(<br/>    max_epochs=30,<br/>    gradient_clip_val=100<br/>)</span><span id="2280" class="kc kd hh jy b fi lr kf l kg kh">GPU available: True, used: False<br/>TPU available: False, using: 0 TPU cores</span><span id="c346" class="kc kd hh jy b fi lr kf l kg kh">trainings_results = trainer.fit(<br/>    model=tagger,<br/>    train_dataloader=ner_train_ds,<br/>    val_dataloaders=ner_valid_ds<br/>)</span><span id="1925" class="kc kd hh jy b fi lr kf l kg kh">| Name            | Type      | Params<br/>----------------------------------------------<br/>0 | word_embeddings | Embedding | 4 M   <br/>1 | lstm            | LSTM      | 395 K <br/>2 | fc              | Linear    | 4 K   </span><span id="7005" class="kc kd hh jy b fi lr kf l kg kh">Saving latest checkpoint..</span><span id="672e" class="kc kd hh jy b fi lr kf l kg kh">test_res = trainer.test(model=tagger, test_dataloaders=ner_test_ds, verbose=0)<br/><strong class="jy hi">print</strong>("Test F1-Score: {:.1%}".format(np.mean([res["test_f1"] <strong class="jy hi">for</strong> res <strong class="jy hi">in</strong> test_res])))</span><span id="2558" class="kc kd hh jy b fi lr kf l kg kh">Test F1-Score: 33.9%</span></pre><p id="7980" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这还不是一个令人信服的表现，但我们也用了很少的数据。</p></div><div class="ab cl ki kj go kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="ha hb hc hd he"><h1 id="5aed" class="kp kd hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">在增强的训练数据上训练LSTM</h1><p id="d888" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">现在，我们在增强的训练数据集上训练LSTM。这使用了先前模型的一半数量的非扩充训练数据。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="4e87" class="kc kd hh jy b fi ke kf l kg kh">tagger = LightningLSTMTagger(<br/>    EMBEDDING_DIM,<br/>    HIDDEN_DIM<br/>)</span><span id="e977" class="kc kd hh jy b fi lr kf l kg kh">trainer = pl.Trainer(<br/>    max_epochs=30,<br/>    gradient_clip_val=100<br/>)</span><span id="13bc" class="kc kd hh jy b fi lr kf l kg kh">GPU available: True, used: False<br/>TPU available: False, using: 0 TPU cores</span><span id="c798" class="kc kd hh jy b fi lr kf l kg kh">trainer.fit(<br/>    model=tagger,<br/>    train_dataloader=ner_aug_train_ds,<br/>    val_dataloaders=ner_valid_ds<br/>)</span><span id="601e" class="kc kd hh jy b fi lr kf l kg kh">| Name            | Type      | Params<br/>----------------------------------------------<br/>0 | word_embeddings | Embedding | 4 M   <br/>1 | lstm            | LSTM      | 395 K <br/>2 | fc              | Linear    | 4 K   </span><span id="2eca" class="kc kd hh jy b fi lr kf l kg kh"> Saving latest checkpoint..</span><span id="23e7" class="kc kd hh jy b fi lr kf l kg kh">test_res = trainer.test(model=tagger, test_dataloaders=ner_test_ds, verbose=0)<br/><strong class="jy hi">print</strong>("Test F1-Score: {:.1%}".format(np.mean([res["test_f1"] <strong class="jy hi">for</strong> res <strong class="jy hi">in</strong> test_res])))</span><span id="c773" class="kc kd hh jy b fi lr kf l kg kh">Test F1-Score: 32.4%</span></pre><p id="1233" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请注意，我们可以仅使用一半的数据获得与上面类似的F1分数。这个挺好看的！</p></div><div class="ab cl ki kj go kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="ha hb hc hd he"><h1 id="fdec" class="kp kd hh bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">总结</h1><p id="794e" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">我们看到了如何使用transformer模型来扩充用于命名实体识别的小型数据集。我们可能可以通过在可用的训练数据或更大的领域特定数据集上微调所使用的语言模型来提高该方法的性能。试试吧，让我知道它对你有什么用。尝试将这种方法应用于其他架构，如<a class="ae jn" href="https://www.depends-on-the-definition.com/lstm-with-char-embeddings-for-ner/" rel="noopener ugc nofollow" target="_blank">角色列表</a>。</p><p id="4483" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这篇文章最初发表在https://www.depends-on-the-definition.com/的<a class="ae jn" href="https://www.depends-on-the-definition.com/" rel="noopener ugc nofollow" target="_blank">上</a>。查看更多关于自然语言处理和机器学习的内容。</p><h1 id="3a06" class="kp kd hh bd kq kr lw kt ku kv lx kx ky kz ly lb lc ld lz lf lg lh ma lj lk ll bi translated">资源</h1><ol class=""><li id="9ae4" class="mb mc hh ir b is lm iw ln ja md je me ji mf jm mg mh mi mj bi translated">莫里斯等人。al:text attack:NLP中对抗性攻击、数据增强和对抗性训练的框架</li><li id="c01c" class="mb mc hh ir b is mk iw ml ja mm je mn ji mo jm mg mh mi mj bi translated"><a class="ae jn" href="https://arxiv.org/abs/2003.02245" rel="noopener ugc nofollow" target="_blank">库马尔等人。a1:使用预先训练的变压器模型进行数据扩充</a></li></ol></div></div>    
</body>
</html>