<html>
<head>
<title>Deep Reinforcement Learning: A Quick Overview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度强化学习:快速概述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-reinforcement-learning-a-quick-overview-6ae43b4cf972?source=collection_archive---------2-----------------------#2021-10-27">https://medium.com/analytics-vidhya/deep-reinforcement-learning-a-quick-overview-6ae43b4cf972?source=collection_archive---------2-----------------------#2021-10-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/877399507a9b924d25e56667b35356a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rTyu9Y8O4p_DszDx"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">帕特里克·托马索在<a class="ae hu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/><p id="a656" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">深度强化学习(RL)可以分解为两个部分。“深度”是指在其目的中使用神经网络，而强化学习(RL)被描述为使用奖励来强化行为。它是人工智能的一个活跃分支，每隔几个月就有新的发展。在本文中，我将只解释RL背后的概念，并且只进入描述RL模型的随机数学和一般函数。深度RL应用在玩视频游戏、运动、驾驶和机器人方面。在这些情况下，有一个目标，机器在采取下一个行动之前从环境中接收连续的反馈，等等。在代理人和环境之间有一个连续的反馈回路。本文中的几乎所有概念都可以参考Laura Graesser和Wah Loon Keng的“Python中深度强化学习理论和实践的基础”。</p><p id="68e7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如前所述，有一个<em class="js">代理</em>或主体，如果你愿意，它作用于<em class="js">环境。</em>环境中的每个<em class="js"> </em>时间戳称为一个<em class="js">状态。</em>这可以想象成一个时间片或者动画序列中的一帧。代理观察状态并选择一个将它带到另一个状态的动作。这种状态转换与奖励有关。之所以称之为RL，是因为奖励是驱动模型进入这个连续循环的因素。该系统有一个目标，奖励强化了有助于系统达到其目标的良好行为。</p><p id="82ba" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个概念的一个简单的例子是一个掷骰子游戏，其中有一辆手推车沿着轨道移动，试图平衡顶部的一根柱子。目标是在200个时间步长内保持极点平衡。</p><figure class="ju jv jw jx fd hj er es paragraph-image"><div class="er es jt"><img src="../Images/70d99d216cbe8c9c6fdadee9ad444334.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*6eS-fDR_sdghtYOfyliE7w.png"/></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">钢管舞游戏(经典深度RL示例)</figcaption></figure><p id="4031" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">关于环境的状态或信息可以是描述[手推车位置、手推车速度、杆角度、杆角速度]的元组。这个动作是一个数字0-1，告诉购物车向左(0)或向右(1)移动。游戏的终止是当杆子下落(&gt;与垂直方向成12度)或当杆子停留≥200个时间步时。</p><p id="281f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有状态空间(游戏的所有可能状态)<em class="js"> s </em>，动作空间(游戏的所有动作)<em class="js"> a </em>，三个参数的奖励函数(t时刻的状态，t时刻的动作，t+1时刻的状态)描述了RL系统的基本信息单元。</p><p id="6ba4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">给定上述信息我们的第一个函数，马尔可夫概率规则是非常有用的。</p><figure class="ju jv jw jx fd hj er es paragraph-image"><div class="er es jy"><img src="../Images/cef3fb1aeb1a0b602714e67b0ab9ba30.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*JLHshV2xnUGdbBwXMvInOQ.png"/></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">马尔可夫概率规则</figcaption></figure><p id="7a67" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">该规则规定，对于给定的状态s，给定状态的概率仅由当前状态和动作决定。前一个时间步长决定了下一个时间步长。翻译成我们的CartPole例子，马尔可夫链给我们一个状态，这个状态是由它以前的状态决定的。因此，输入<em class="js"> s </em>和<em class="js"> a </em>将会给我们一个新的状态来驱动算法前进。一个重要的注意事项是，我们将看到<em class="js"> s </em>和<em class="js"> a </em>如何与我们下面的等式相关联。</p><p id="fd14" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">首先，要认识到的一个重要等式是:</p><figure class="ju jv jw jx fd hj er es paragraph-image"><div class="er es jz"><img src="../Images/8a8745eea9edb30948a9a66f547266a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:232/format:webp/1*p027RcPCbZrowSlP02tjyg.png"/></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">目标方程(J)其中γ是贴现因子(0到1)，t =时间步长，r是需要最大化的报酬。</figcaption></figure><p id="ae43" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上述函数的一个变体是:</p><figure class="ju jv jw jx fd hj er es paragraph-image"><div class="er es ka"><img src="../Images/16eb35b60c7ad663bb9f2ed95d02fc67.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/format:webp/1*KeJ9v-Rw8eoilviqaa9rOA.png"/></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">其中左边是Q pi，表示任何给定状态s的预期收益(报酬),动作a。</figcaption></figure><p id="ed3d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上述三个等式描述了深度RL中的三种主要算法。第一个功能是一个<em class="js"> </em> <strong class="iw hy"> <em class="js">环境模型</em> </strong> <em class="js">。</em>第二个方程是一个<strong class="iw hy"> <em class="js">政策函数</em></strong><em class="js"/>，第三个方程是一个<em class="js"> </em> <strong class="iw hy"> <em class="js">价值函数。</em> </strong></p><p id="e9ab" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">第一个函数马尔可夫模型在环境模型中是有用的。环境模型用于人工智能棋类游戏、围棋、西洋双陆棋等。该模型在做出最佳移动之前，使用马尔可夫链概率规则来绘制或“想象”所有状态和动作。这种想法的一个例子是蒙特卡罗树搜索(MCTS)，其中称为蒙特卡罗展开的样本动作序列被探索并具有估计值。给定这些展开，计算机选择最佳的行动过程。</p><p id="d41d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们解释一下第二个功能。Tau指的是一集的轨迹。一集是从t=0到环境终止的收集时间步长。轨迹(tau)指的是<em class="js"> s，a，r… </em>到时间t的序列。策略函数(pi)的重要性在于它驱动模型前进，同时它的最大化导致RL系统的“学习”。注意:在这种情况下，pi(策略函数)的最大化实际上是无数次运行<em class="js"> s </em>和<em class="js"> a直到我们找到pi的最大值的结果。我们使用</em> <strong class="iw hy"> <em class="js">梯度上升</em> </strong> <em class="js">并将术语</em><strong class="iw hy"><em class="js">θ</em></strong><em class="js">引入方程。</em></p><p id="1ff3" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">与此等式直接相关的是最终函数，它是一个价值函数，通过最大化其回报来帮助RL模型学习。它测量状态动作对的“好”或“坏”程度，并且它的最大化也是RL模型“学习”的原因同样，这里的最大化与上面的一样(使用θ梯度上升)。</p><p id="2c2e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，还有两种或多种上述算法的组合方法。行动者-批评者算法使用策略函数来行动，使用价值函数来批评。属于这一类别的这种算法是A3C、信任区域策略优化(TRPO)、深度确定性策略梯度(DDPG)、近似策略优化(PPO)和软行动者批评(SAC)，其中PPO被最广泛地使用。使用环境模型和/或策略/值函数的方法包括AlphaGo和Dyna-Q。</p><p id="dcd8" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">算法的另一个分组是<em class="js">非策略</em>对<em class="js">策略。基于策略的</em>算法利用仅在同一策略内生成的数据，并在训练后丢弃，而<em class="js">基于策略的</em>算法允许数据重用。非策略算法对内存的要求更高。</p><p id="e967" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">目前我们已经对RL的状态进行了概述。RL的“深度”学习方面要求在我们的三种功能类型中应用神经网络。马尔可夫规则是使这些函数起作用的一个重要组成部分，许多研究正在这些模型上进行。强化学习正被用于视频游戏、机器人和NLP领域。聊天机器人，文本摘要，文本翻译，游戏，机器人，包括自动驾驶汽车都是RL令人兴奋的应用。它通过模仿人类的学习方式，通过反馈试错来做到这一点。虽然这只是对这个人工智能子领域的一个浅显的探索，但我希望它已经启发并让读者意识到RL的可能性和未来发展。</p></div></div>    
</body>
</html>