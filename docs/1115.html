<html>
<head>
<title>Paper Explained- Normalizer Free Nets (NFNETS): High Performance Large Scale Image Recognition Without Normalisation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文解释-规格化自由网(NFNETS):无需规格化的高性能大规模图像识别</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/paper-explained-normalizer-free-nets-nfnets-high-performance-large-scale-image-recognition-75518978b1fe?source=collection_archive---------6-----------------------#2021-02-15">https://medium.com/analytics-vidhya/paper-explained-normalizer-free-nets-nfnets-high-performance-large-scale-image-recognition-75518978b1fe?source=collection_archive---------6-----------------------#2021-02-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/4012a36b731c3d007a7e703d371edc1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CjpipU_oChc899f_Esjpyg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">NFNet-F1模型实现了与EffNet-B7相当的准确性，同时训练速度快8.7倍。图片取自<a class="ae iu" href="https://arxiv.org/pdf/2102.06171.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的第1页。</figcaption></figure><h1 id="4bd7" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">简介和概述</h1><p id="e161" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">因此，本文的重点是在这种情况下构建网络，具体来说，<strong class="jv hj">卷积残差风格网络</strong>，其中没有批量归一化构建。但是，如果没有批量标准化，这些网络通常不会执行得很好，或者无法扩展到更大的批量，然而，本文在这里构建的网络可以<strong class="jv hj">扩展到更大的批量</strong>，并且<strong class="jv hj">比以前最先进的方法</strong>更有效(就像<a class="ae iu" rel="noopener" href="/analytics-vidhya/lambdanetworks-modeling-long-range-interactions-without-attention-337771f42b6f">λnets</a>，我也写了一篇关于它的详细文章，<a class="ae iu" rel="noopener" href="/analytics-vidhya/lambdanetworks-modeling-long-range-interactions-without-attention-337771f42b6f">点击这里</a>查看！！！🤞).训练延迟与准确度图显示，对于在<a class="ae iu" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>上训练的同样的前1准确度分数，<strong class="jv hj">nfnet比EffNet-B7 </strong>快8.7倍。该模型是一种<strong class="jv hj">新的最先进的</strong>模型，没有任何额外的训练数据，也是一种新的最先进的迁移学习模型。<strong class="jv hj">nfnet目前在全球排行榜上排名第二</strong>，落后于使用半监督预训练和额外数据的方法。</p><h1 id="1bab" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">批量归一化有什么问题？</h1><p id="019c" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">如果你有一个数据点通过一个网络，它将经历各种各样的转换，然而，如果你以错误的方式建立网络，其中的一些转换是非常不幸的。在机器学习中，一个好的做法是将数据集中在平均值周围，并将其缩放到单位变量，但是当你在各层中前进时，特别是如果你有ReLU这样的层，它们只会提取信号的积极部分。因此，随着时间的推移，可能会发生更下面的层之间的中间表示非常偏斜和不居中的情况。如果你的数据有一个很好的条件数(即，以平均值为中心，不是非常倾斜，等等)，那么机器学习的当前方法会工作得更好。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/f2a10354bbeadada29e96ff173a9264c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KJCl_9bW2Hx2Igc8h1jtvw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">如何在神经网络中使用批范数转换数据的基本说明。</figcaption></figure><p id="38c0" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated"><strong class="jv hj">批量标准化有3个明显的缺点</strong>。<strong class="jv hj">首先，它是一个非常昂贵的计算原语</strong>，这会导致内存开销。你需要计算均值，标度，你需要把它们存储在内存中，用于反向传播算法。这增加了在一些网络中评估梯度所需的时间。</p><p id="9c24" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated"><strong class="jv hj">其次，它在训练期间</strong> <strong class="jv hj">和推理时</strong>的模型行为之间引入了一个差异，这是真的，因为在推理时你不希望这种批量依赖，你希望能够提供单个数据点，并且结果应该总是相同的。</p><p id="60f9" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated"><strong class="jv hj">第三，批量归一化打破了小批量</strong>中训练样本之间的独立性。这意味着，现在批次中的其他示例很重要。</p><p id="6bf0" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">这有两个主要后果。<strong class="jv hj">首先，批量大小将关系到批量标准化</strong>。如果你有一个小批量，均值将是一个非常嘈杂的近似值，而如果你有一个大批量，均值将是一个很好的近似值。我们知道，对于某些应用，大批量有利于训练，它们可以稳定训练，减少训练时间等等。</p><p id="bd5c" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated"><strong class="jv hj">其次，分布式训练变得非常麻烦</strong>因为，例如，如果你进行数据并行，这意味着，你有一批数据，这批数据被分成3个不同的部分/碎片，这3个碎片被前向传播到一个神经网络中，这对于用于训练的所有3个不同的机器是相同的。现在想象一下，如果在所有3个网络中都有一个批次范数层，从技术上来说，您必须将信号正向传播到批次范数层，然后您必须在批次范数层之间交流批次统计数据，否则您将无法获得输入的整个批次的均值和方差。这使得网络能够<strong class="jv hj">‘欺骗’</strong>某些损失函数。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lb"><img src="../Images/a07dbc2e8e7f9d971a890489229908f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_MhXMUvV5Aokar--pIsC4A.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">概述数据并行性如何使用批处理规范层之间的统计连接来工作。</figcaption></figure><h1 id="a70e" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">论文投稿</h1><ol class=""><li id="737b" class="lc ld hi jv b jw jx ka kb ke le ki lf km lg kq lh li lj lk bi translated">作者提出了<strong class="jv hj">自适应梯度裁剪(AGC) </strong>，它基于梯度范数与参数范数的单位比来裁剪梯度，他们证明了AGC允许我们用更大的批量和更强的数据扩充来训练无规格化器的网络。</li><li id="5fad" class="lc ld hi jv b jw ll ka lm ke ln ki lo km lp kq lh li lj lk bi translated">作者设计了一系列无规格化器的ResNets，称为NFNets，它在ImageNet上为一系列训练延迟设置了新的最先进的验证精度。NFNet-F1模型实现了与EfficientNet-B7类似的准确性，同时训练速度快8.7倍，最大的模型在没有额外数据的情况下创下了86.5%的顶级准确性的新的整体水平。</li><li id="bfac" class="lc ld hi jv b jw ll ka lm ke ln ki lo km lp kq lh li lj lk bi translated">作者表明，在对3亿张标记图像的大型私人数据集进行预训练后，在ImageNet上进行微调时，NFNets实现了比批量标准化网络高得多的验证准确性。最佳模型在微调后达到89.2%的top-1。</li></ol><h1 id="a7d2" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">自适应梯度削波(AGC)</h1><p id="87e5" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">梯度裁剪通常用于语言建模以稳定训练，最近的工作表明，与梯度下降相比，它允许以更大的学习速率进行训练。梯度裁剪通常通过约束梯度的范数来执行。具体而言，对于梯度向量G = ∂L/∂θ，其中l表示损失，θ表示具有所有模型参数的向量，标准裁剪算法在更新θ之前裁剪梯度，如下所示:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/9ab513040971c44cd991228af76d54a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*6XDOENOjjDL_cE0yPlj_hA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">公式片段摘自<a class="ae iu" href="https://arxiv.org/pdf/2102.06171.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的第4页。</figcaption></figure><p id="6264" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">在训练期间，对于优化者来说，采取巨大的跳跃来达到全局最小值并不是很好，所以梯度裁剪只是说，每当任何参数的梯度非常大时，我们就简单地裁剪该梯度。如果梯度是好的，我们肯定会再次看到它，但如果是坏的梯度，我们希望限制其影响。问题是它对削波参数λ非常敏感，原因是它不是自适应的。</p><p id="dc6f" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">AGC所做的是缩放梯度，但它不仅将梯度缩放到自己的范数，而且<strong class="jv hj">将梯度限幅到</strong> <strong class="jv hj">比率(梯度有多大/梯度作用的权重有多大)。</strong>乍一看可能会令人困惑，但我鼓励你看一看这篇文章并通读第4页，以便更清晰地理解AGC。</p><p id="0f95" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">限幅阈值λ是一个必须调整的标量超参数。根据经验，作者发现，虽然这种裁剪算法使他们能够以比以前更高的批量进行训练，但训练稳定性对裁剪阈值的选择极其敏感，需要在改变模型深度、批量或学习速率时进行精细调整。作者通过选择与梯度范数成反比的自适应学习速率来忽略梯度的比例。</p><p id="0095" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">注意，最佳削波参数λ可能取决于优化器、学习速率和批量大小的选择。根据经验，作者发现<strong class="jv hj"> λ对于更大的批量应该更小。</strong></p><h1 id="b2ef" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">用于自适应梯度削波(AGC)的烧蚀</h1><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/af328022b36719ed3b9b851b44df79a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oPLx3USSZEwapwPI8w28nA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图像取自<a class="ae iu" href="https://arxiv.org/pdf/2102.06171.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的第4页。</figcaption></figure><p id="931b" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">例如，如果您比较图1中的批次范数网络，(<strong class="jv hj"> NF-ResNet和NF-ResNet + AGC </strong>)您可以看到，在某个批次大小(2048)之后，非AGC完全崩溃，而AGC占优势。这似乎是提高批量的秘诀。作者抱怨限幅阈值λ非常苛刻。在图2中，您可以看到λ对批量大小有着至关重要的依赖关系，您可以看到，在小批量下，您可以在相当大的阈值下进行削波。在批量较大的情况下，你必须将阈值保持得非常低，因为如果你将它削波得更高，它就会崩溃。</p><p id="096c" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated"><strong class="jv hj">如果你喜欢这篇文章并获得了真知灼见，可以考虑</strong> <a class="ae iu" href="https://www.buymeacoffee.com/nakshatrasinghh" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj">给我买杯咖啡</strong> ☕️ <strong class="jv hj">点击这里</strong> </a> <strong class="jv hj"> :) </strong></p><h1 id="0fd8" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">参考</h1><ol class=""><li id="bbed" class="lc ld hi jv b jw jx ka kb ke le ki lf km lg kq lh li lj lk bi translated"><a class="ae iu" href="https://arxiv.org/pdf/2102.06171.pdf" rel="noopener ugc nofollow" target="_blank">无需归一化的高性能大规模图像识别，2021年2月11日。</a></li></ol><p id="b759" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">如果你喜欢这个帖子，请一定要鼓掌👏。💬连接？让我们来看看社会:<a class="ae iu" href="https://myurls.co/nakshatrasinghh" rel="noopener ugc nofollow" target="_blank"><strong class="jv hj">http://myurls.co/nakshatrasinghh</strong></a><strong class="jv hj">。</strong></p></div></div>    
</body>
</html>