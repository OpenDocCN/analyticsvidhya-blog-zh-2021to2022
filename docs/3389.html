<html>
<head>
<title>Designing a Self-Driving car simulation using python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用python设计自动驾驶汽车模拟</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/designing-a-self-driving-car-simulation-using-python-38dcac8136c6?source=collection_archive---------7-----------------------#2021-06-28">https://medium.com/analytics-vidhya/designing-a-self-driving-car-simulation-using-python-38dcac8136c6?source=collection_archive---------7-----------------------#2021-06-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/82119d4a14d84772fe16c49940466832.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*tz2EbOw8TYsx2lXG49V18A.png"/></div></figure><p id="e634" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi jj translated"><span class="l jk jl jm bm jn jo jp jq jr di"/>自动驾驶汽车，也称为自主车辆、无人驾驶汽车或机器人汽车，能够感知周围环境并在不需要人类干预的情况下工作。<br/>自动驾驶汽车是人工智能最引人注目的应用，也是强化学习的经典例子。<br/>强化学习是机器学习的一个领域。这都是关于采取正确的步骤来从给定的场景中获得最大的收益。各种应用程序和机器人使用它来确定在给定情况下的最佳潜在行动或路径。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es js"><img src="../Images/447e6d683959d023b495cf48077b0cca.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*TYf18ZX9H6qG2MEERToU5A.png"/></div></figure><p id="6aa7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">一种非常简单的可视化自动驾驶汽车如何工作的方法是Q-Learning。Q-learning是一种非策略强化学习算法，它试图在给定的当前情况下确定最佳的行动过程。Q-learning的目标是发现一个使总回报最大化的政策。它被称为脱离策略，因为q-learning函数从当前策略没有覆盖的活动中学习，例如随机行为，因此不需要策略。q-learning中的“q”指的是“质量”，即代理为获得未来回报而采取的行动的质量。</p><p id="ea7a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Q-学习方程:<strong class="in hi">T5】Q(s，a) ← Q(s，a) + α * [r + γ * maxa'Q(s '，a') - Q(s，a)] </strong></p><ul class=""><li id="7238" class="jx jy hh in b io ip is it iw jz ja ka je kb ji kc kd ke kf bi translated">Q(s，a)= Q值的<em class="kg">【状态，动作】</em></li><li id="35b5" class="jx jy hh in b io kh is ki iw kj ja kk je kl ji kc kd ke kf bi translated">Q(s '，a') = Q值的<em class="kg">【新状态，所有可能的动作】</em></li><li id="ea26" class="jx jy hh in b io kh is ki iw kj ja kk je kl ji kc kd ke kf bi translated">r =奖励</li><li id="56bc" class="jx jy hh in b io kh is ki iw kj ja kk je kl ji kc kd ke kf bi translated">α =学习率</li><li id="f5fa" class="jx jy hh in b io kh is ki iw kj ja kk je kl ji kc kd ke kf bi translated">γ =贴现率</li></ul><p id="2caf" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这个问题是情景强化学习的一个很好的例子。情节任务是具有终止状态(end)的任务。在RL中，事件被认为是从初始状态到最终状态的代理-环境相互作用。在我们的问题陈述中，汽车将从初始状态开始，并将探索环境，直到它到达最终状态。这叫插曲。一旦汽车到达最终状态，下一集将开始，汽车将从初始状态开始。</p><p id="6851" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">为了可视化q-learning的工作，我使用了一个环境<a class="ae km" href="https://gym.openai.com/envs/Taxi-v3/" rel="noopener ugc nofollow" target="_blank"> Taxi-v3 </a>。Taxi-v3是OpenAI健身房图书馆的2d环境。Taxi-v3是自动驾驶汽车的一个最佳和简单的例子，我在其中应用了强化学习来训练出租车采取最佳行动并获得未来的奖励。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kn"><img src="../Images/d219eac382af5ae72311c009660edb83.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*9f0kpPPRLvIwIoy7dsXXsA.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">出租车-v3</figcaption></figure><p id="646b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Taxi-v3在网格世界中有四个指定位置，分别用R(红色)、G(绿色)、Y(黄色)和B(蓝色)表示。当这一集开始时，出租车在随机的广场出发，乘客在随机的地点。出租车开到乘客所在地，接乘客，开到乘客的目的地(四个指定地点中的另一个)，然后让乘客下车。一旦乘客下车，这一集就结束了。由于有25个出租车位置、乘客的5个可能位置(包括乘客在出租车中的情况)和4个目的地位置，所以有500个离散状态。<br/>每个动作奖励-1，运送乘客额外奖励+20。非法执行“取货”和“卸货”的奖励为-10。</p><p id="3574" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">从自动驾驶汽车的编码开始，你可以使用<a class="ae km" href="https://colab.research.google.com/notebooks/" rel="noopener ugc nofollow" target="_blank">谷歌合作实验室</a>或者<a class="ae km" href="https://jupyter.org/" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>。创建一个python笔记本，就可以开始编码了。</p><p id="2ef1" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">首先，在你的设备或google colab笔记本上安装健身房环境库。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es ks"><img src="../Images/652574ccf776d9688abb2399bccbd06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*krbGvLh4JwacHj_6b0rn_Q.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">在jupyter笔记本中安装健身房环境</figcaption></figure><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kt"><img src="../Images/9ed5ecf93c847be55cd7bebb50e04165.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*VqRNCzMCo3wPh3OUywTqHQ.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">在google colab笔记本中安装健身房环境</figcaption></figure><p id="0393" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">下一步是导入所有需要的库- <strong class="in hi"> <em class="kg"> numpy，random，gym </em> </strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es ku"><img src="../Images/3565759df0c9000441c8c921b8c04671.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*tOZOheiRlCJekkyQBQX2ig.png"/></div></figure><p id="28a3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">为了创建我们的环境，我们将使用<strong class="in hi"><em class="kg">gym . make(' Taxi-v3)</em></strong>方法。它将返回我们训练自动驾驶汽车所需的环境。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/f13777972519131cb1b671f78222f144.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*ZjNfpRjDSa3DRWxdbEKYDQ.png"/></div></figure><p id="5cae" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这是我们的环境，黄色实体是我们的汽车(出租车)，我们将使用强化学习来训练它。</p><p id="6b09" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">下一步是最重要的一步，<strong class="in hi"> <em class="kg">初始化超参数</em> </strong>。<br/>在这一步中，我们必须初始化训练和测试集的数量、汽车在单集内可以走的最大步数、学习率、折扣率和探索参数。</p><p id="fe99" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">什么是勘探参数？</strong> <br/>这些是控制我们模型的勘探和开采速度的参数。首先，我们的代理(car)将彻底探索环境，它将获得一些积极的奖励以及消极的奖励，并将学习确定最佳路径的环境。探索之后，汽车将开始探索环境，这意味着它将从对环境的探索中测试他的学习。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kw"><img src="../Images/b0a52486807d048f2a350d4839493fe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*Wj7MB3RfD7uies-sVpbg3Q.png"/></div></figure><p id="a624" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我已经进行了50，000次训练，100次测试，car在单次测试中最多可以走99步，0.7是我们的学习率，0.6是我们的折扣率。epsilon是我们的探索率，为1.0，在每一集之后，我们的epsilon值将会减少，因此在某个点之后，我们的模型开始利用环境。</p><p id="e2f2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在开始q学习之前，我们必须创建一个q表。q表是一个遵循<em class="kg">【状态，动作】</em>形状的矩阵，我们将我们的值初始化为零。然后，在一集之后，我们更新并存储我们的<em class="kg"> q值</em>。这个q表成为我们的代理根据q值选择最佳行动的参考表。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kx"><img src="../Images/79f744e950895e55a4a2391b4c90bf72.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*k3BgqBw6WOpvB5EDI0v_xg.png"/></div></figure><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es ky"><img src="../Images/5a209c0b33cc97d7f0b70384456085e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*XXjlL4-5kge51NQ6idC5wQ.png"/></div></figure><p id="97af" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在我们将在我们的环境中实现Q-learning。把它分成几个步骤，我们得到</p><ul class=""><li id="a425" class="jx jy hh in b io ip is it iw jz ja ka je kb ji kc kd ke kf bi translated">开始探索动作:对于每个状态，从当前状态的所有可能动作中选择任意一个。</li><li id="3fb3" class="jx jy hh in b io kh is ki iw kj ja kk je kl ji kc kd ke kf bi translated">作为动作(a)的结果，移动到下一个状态(s’)。</li><li id="2d16" class="jx jy hh in b io kh is ki iw kj ja kk je kl ji kc kd ke kf bi translated">对于状态中所有可能的动作，选择具有最高Q值的一个。</li><li id="ff3c" class="jx jy hh in b io kh is ki iw kj ja kk je kl ji kc kd ke kf bi translated">使用公式更新Q表值。</li><li id="f2cb" class="jx jy hh in b io kh is ki iw kj ja kk je kl ji kc kd ke kf bi translated">将下一个状态设置为当前状态。</li><li id="b99e" class="jx jy hh in b io kh is ki iw kj ja kk je kl ji kc kd ke kf bi translated">如果达到目标状态，则结束并重复该过程。</li></ul><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es kz"><img src="../Images/9d639781e7254b97dd66312381958ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hjJod4VL7l1RhU1uO-sa-w.png"/></div></div></figure><p id="6984" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">五万集之后，我们的车训练有素，已经学会了在那种环境下自动驾驶。在更新q表的帮助下，car可以采取最佳行动以获得最大回报。我们可以想象自动驾驶汽车的工作。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es le"><img src="../Images/da7600d20e24839260ecd4a5b7d2521f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*O-p2d6jW8tdX3F2ZBvIhGw.gif"/></div></figure><p id="4045" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">该可视化的渲染如下所示:</p><ul class=""><li id="b992" class="jx jy hh in b io ip is it iw jz ja ka je kb ji kc kd ke kf bi translated">蓝色:乘客</li><li id="4285" class="jx jy hh in b io kh is ki iw kj ja kk je kl ji kc kd ke kf bi translated">洋红色:目的地</li><li id="364f" class="jx jy hh in b io kh is ki iw kj ja kk je kl ji kc kd ke kf bi translated">黄色:空出租车</li><li id="3ae8" class="jx jy hh in b io kh is ki iw kj ja kk je kl ji kc kd ke kf bi translated">绿色:满载滑行</li><li id="715d" class="jx jy hh in b io kh is ki iw kj ja kk je kl ji kc kd ke kf bi translated">其他字母(R、G、Y和B):乘客的位置和目的地</li></ul><p id="bca4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">正如你所看到的，使用强化学习，我们可以训练汽车在任何环境下自动驾驶。本教程是自动驾驶汽车和强化学习的一个基本和最简单的例子。<br/>本教程只是在确定性的小环境中模拟自动驾驶汽车。在现实世界中，环境是随机的和巨大的。</p><p id="24b5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">你可以从我的github库获得本教程的全部代码。</p><div class="lf lg ez fb lh li"><a href="https://github.com/RushiKanjaria/Design-a-self-driving-car-solution-using-Reinforcement-Learning-using-Gym-Environment/blob/master/Self_Driving_Car.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="lj ab dw"><div class="lk ab ll cl cj lm"><h2 class="bd hi fi z dy ln ea eb lo ed ef hg bi translated">RushiKanjaria/设计-自动驾驶汽车-解决方案-使用强化-学习-使用健身房-环境</h2><div class="lp l"><h3 class="bd b fi z dy ln ea eb lo ed ef dx translated">我们正在使用强化学习和健身房环境创建一个自动驾驶汽车的模拟。我们…</h3></div><div class="lq l"><p class="bd b fp z dy ln ea eb lo ed ef dx translated">github.com</p></div></div><div class="lr l"><div class="ls l lt lu lv lr lw ij li"/></div></div></a></div><p id="08c4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">谢谢你。希望你喜欢读这篇文章，并学到一些新东西。</p></div></div>    
</body>
</html>