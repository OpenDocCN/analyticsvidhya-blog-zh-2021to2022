<html>
<head>
<title>House price prediction:</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">房价预测:</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/house-price-prediction-ea1185212ca7?source=collection_archive---------14-----------------------#2021-10-11">https://medium.com/analytics-vidhya/house-price-prediction-ea1185212ca7?source=collection_archive---------14-----------------------#2021-10-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="8ba4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">执行一些实验，使用基于树的模型来预测房价和贝叶斯优化超参数调整</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/4c37d94a45243b00d874628fe57eeef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AOswV5tmshYohg-S0Tk33w.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">布莱克·惠勒在<a class="ae js" href="https://unsplash.com/s/photos/homes?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="b73e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文的目的是使用Kaggle challenge: <a class="ae js" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation" rel="noopener ugc nofollow" target="_blank">房价预测</a>的数据集比较三个实验的结果。在每个实验中，我们将使用相同的6个基于树的模型，但是使用不同的数据简化方法。进行这些实验的主要原因是为了评估当我们使用不同的特征子集时回归器性能的变化。</p><p id="f442" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于每个实验，我们将使用贝叶斯优化来执行超参数调整，与网格搜索不同，它不会对所有指定的参数值执行穷举搜索，而是使用从指定分布中采样的固定数量的参数设置。这种方法是众所周知的选择下一个超参数组合的基础上的信息，从以前的。</p><p id="4d61" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了简单起见，在继续展示每个实验结果的图表之前，我们将只看到一步一步的代码摘录。不过详细的代码可以在<a class="ae js" href="https://github.com/Michelpayan/House_price_prediction" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> GitHub </strong> </a> <strong class="ig hi">中找到。</strong></p><p id="fda1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 1。第一步:</strong>导入相应的库后，通过替换或删除它们来处理丢失的值。</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="a01b" class="jy jz hh ju b fi ka kb l kc kd">#According to the description of this dataset, replace NAN from these columns with 'None'. <br/>with_NA_type=['Alley','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature']</span><span id="bd89" class="jy jz hh ju b fi ke kb l kc kd">for i in with_NA_type:<br/>    train.loc[:,i]=train.loc[:,i].fillna('None')</span><span id="773c" class="jy jz hh ju b fi ke kb l kc kd">#Replace NAN of this column with median<br/>train["LotFrontage"] = train.groupby("Neighborhood")["LotFrontage"].transform(lambda x: x.fillna(x.median()))</span><span id="5efe" class="jy jz hh ju b fi ke kb l kc kd">#Since there are a few NAN in these columns we can drop them<br/>train=train.dropna(subset=["Electrical", "MasVnrArea", "MasVnrType"])</span><span id="e68f" class="jy jz hh ju b fi ke kb l kc kd">train.loc[:,['GarageArea','GarageYrBlt']]=train[['GarageArea','GarageYrBlt']].fillna(0).values</span></pre><p id="70e7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 2。第二步:</strong>在处理完缺失值后，数据集被分割成训练集和测试集。然后，scikit-learn中的函数<a class="ae js" href="https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html" rel="noopener ugc nofollow" target="_blank"> ColumnTransformer </a>将被用于将一些转换器(取决于实验)应用于具有分类值和数值的列。</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="cdc2" class="jy jz hh ju b fi ka kb l kc kd">train_x=train.iloc[:,:-1]<br/>train_y=pd.DataFrame(train['SalePrice'])</span><span id="3d1b" class="jy jz hh ju b fi ke kb l kc kd">numerical_ix = train_x.select_dtypes(include=['int64', 'float64']).columns<br/>categorical_ix = train_x.select_dtypes(include=['object', 'bool']).columns</span><span id="bcc8" class="jy jz hh ju b fi ke kb l kc kd">X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=0)</span><span id="9f17" class="jy jz hh ju b fi ke kb l kc kd">#Transformers used in the first experiment:<br/>t1 = [('cat', OneHotEncoder(handle_unknown = "ignore"), categorical_ix), <br/>     ('num', StandardScaler(), numerical_ix)]</span><span id="868a" class="jy jz hh ju b fi ke kb l kc kd">col_transform1 = ColumnTransformer(transformers=t1)</span></pre><p id="f66b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 3。第三步:</strong>使用scikit-optimize的<a class="ae js" href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html" rel="noopener ugc nofollow" target="_blank"> BayesSearchCV </a>来调整每个模型的超参数。在此之前，您将看到我如何定义函数“select_model ”,该函数返回将用作BayesSearchCV的估计器对象的管道，如下例所示:</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="92b0" class="jy jz hh ju b fi ka kb l kc kd">#The following function called "select_model" is going to retun the pipeline used in each estimation.</span><span id="f040" class="jy jz hh ju b fi ke kb l kc kd">def select_model(model_name,col_transform,selector=False,dim_red=False):<br/>    '''<br/>    model_name: Call the regressor<br/>    col_transform: Specify the ColumnTransformer method<br/>    selector: If True, introduce SelectFromModel into the pipeline.<br/>    dim_red: If True, introduce TruncatedSVD into the pipeline<br/>    '''<br/>    <br/>    if (selector==True &amp; dim_red==True):<br/>        param_list=[('prep', col_transform),<br/>                       ("select", SelectFromModel(model_name,max_features=1,threshold=-np.inf)),<br/>                    ('reduct',TruncatedSVD()),<br/>                       ('model',model_name)]<br/>     elif selector==True:<br/>        param_list=[('prep', col_transform),<br/>                       ('select', SelectFromModel(model_name,max_features=1,threshold=-np.inf)),<br/>                       ('model',model_name)]<br/>        <br/>    elif dim_red==True:<br/>        param_list=[('prep', col_transform),<br/>                       ('reduct',TruncatedSVD()),<br/>                       ('model',model_name)]<br/>       <br/>    else:<br/>        param_list=[('prep', col_transform),<br/>                       ('model',model_name)]<br/>    <br/>    pipe = Pipeline(steps=param_list)<br/>    return pipe</span><span id="aebd" class="jy jz hh ju b fi ke kb l kc kd">#Hyperparameter tuning with BayesSearchCV:</span><span id="4293" class="jy jz hh ju b fi ke kb l kc kd">params={"model__max_depth":Integer(10,800),<br/>       "model__max_features":Real(0.5,1),<br/>       "reduct__n_components":Integer(2,40)}</span><span id="4391" class="jy jz hh ju b fi ke kb l kc kd">result_dt = BayesSearchCV(estimator=select_model(DecisionTreeRegressor(),col_transform1,dim_red=True),search_spaces=params,cv=3,n_iter=300,<br/>scoring='neg_root_mean_squared_error',iid=False,<br/>return_train_score=True)</span><span id="c56a" class="jy jz hh ju b fi ke kb l kc kd">result_dt.fit(X_train,y_train.values.ravel())</span></pre><p id="41e0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们继续详细解释每个实验及其结果。</p><h1 id="0ad5" class="kf jz hh bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">第一次实验</h1><p id="82ca" class="pw-post-body-paragraph ie if hh ig b ih lc ij ik il ld in io ip le ir is it lf iv iw ix lg iz ja jb ha bi translated">第一个实验包括应用线性降维方法。执行降维的目的是减少输入特征的数量，以提高模型的性能。</p><p id="2695" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于我们的数据集包含许多将被热编码的分类值，我们需要使用适合稀疏数据的降维技术，因此我们将使用截断奇异值分解(SVD)。因此，正如我们对回归变量的参数执行超参数调优一样，我们也将测量组件的最佳数量。</p><p id="3b86" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于该实验和随后的实验，采样的参数设置的数量(迭代次数)将是300。每次迭代的RMSE如下所示。换句话说，您可以看到为验证集和训练集的每次迭代或超参数组合给出的RMSE，红色垂直线表示具有最佳超参数集的迭代。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lh"><img src="../Images/8aad8377527b6eb312f4c9cd9cd2e549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XyCkSkyccwo2mHtrGosaww.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">每次迭代的RMSE(图由作者创建)</figcaption></figure><p id="8ac6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过查看下表，我们可以看到测试集的RMSE最低的模型是EXT Trees Regressor(EXT ),但是，通过比较训练集、验证集和测试集的RMSE，该模型存在较高的方差。我们可以说，自适应增强(ADA)的性能不如EXT，但它是受过拟合影响较小的一种。</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="631c" class="jy jz hh ju b fi ka kb l kc kd">cv_result=[result_dt,result_rf,result_extra,result_gbr,result_ada,result_xgb]<br/>models=["DT","RF","EXT","GBR","ADA","XGB"]</span><span id="5d72" class="jy jz hh ju b fi ke kb l kc kd">results_1 = pd.DataFrame()</span><span id="918f" class="jy jz hh ju b fi ke kb l kc kd">for i,j in zip(cv_result,models):<br/>    results_1.loc[j,'RMSE train']=mean_squared_error(y_train,i.predict(X_train),squared=False)<br/>    results_1.loc[j,'RMSE Val']=(i.best_score_)*(-1)<br/>    results_1.loc[j,'RMSE test']=mean_squared_error(y_test,i.predict(X_test),squared=False)</span><span id="d583" class="jy jz hh ju b fi ke kb l kc kd">#Difference between RMSE of the training set and the test set to see how far they are from one of another<br/>results_1['Dif']=results_1['RMSE train']-results_1['RMSE test']<br/>results_1['Best params']=0<br/>results_1['Best params'] = results_1['Best params'].astype(object)<br/>#Best hyperparameters selected by the Bayesian Search<br/>for i,j in zip(cv_result,models):<br/>    results_1.at[j,'Best params']=list(i.best_params_.items())<br/>results_1</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es li"><img src="../Images/61e6c7c405deaadc1c2ef718654c9b98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R-GDFuXkSP7meMSQ5Trytw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">第一次实验的结果表</figcaption></figure><h1 id="2fbf" class="kf jz hh bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">第二次实验</h1><p id="758b" class="pw-post-body-paragraph ie if hh ig b ih lc ij ik il ld in io ip le ir is it lf iv iw ix lg iz ja jb ha bi translated">对于第二个实验，我们将根据每个模型的属性feature_importances_在整个数据集中选择最重要的特征，而不是使用降维。也就是说，我们将使用scikit learn: Select From Model中提供的方法选择最重要的特性。关于这项技术和其他技术的更多细节，请参考文章<a class="ae js" rel="noopener" href="/@michell.payano.perez/feature-selection-1dde6a486760">特征选择:我们应该如何执行？</a></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lj"><img src="../Images/fcf752a0029214c71855d01094ddc4a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-0BBVd6wLtHEnuVpHQx8Jw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">每次迭代的RMSE(图由作者创建)</figcaption></figure><p id="e31f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种情况下，其他模型中表现最好的是梯度增强(GBR)和极端梯度增强(XGB)。有趣的是，由于训练集的RMSE大于测试集的RMSE，ADA是欠拟合的，但验证集却不是这样。</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="715d" class="jy jz hh ju b fi ka kb l kc kd">cv_result=[result_dt,result_rf,result_extra,result_gbr,result_ada,result_xgb]<br/>models=["DT","RF","EXT","GBR","ADA","XGB"]</span><span id="ab8e" class="jy jz hh ju b fi ke kb l kc kd">results_2 = pd.DataFrame()</span><span id="4b83" class="jy jz hh ju b fi ke kb l kc kd">for i,j in zip(cv_result,models):<br/>    results_2.loc[j,'RMSE train']=mean_squared_error(y_train,i.predict(X_train),squared=False)<br/>    results_2.loc[j,'RMSE Val']=(i.best_score_)*(-1)<br/>    results_2.loc[j,'RMSE test']=mean_squared_error(y_test,i.predict(X_test),squared=False)<br/>#Difference between RMSE of the training set and the test set to see how far they are from one of another<br/>results_2['Dif']=results_2['RMSE train']-results_2['RMSE test']<br/>results_2['Best params']=0<br/>results_2['Best params'] = results_2['Best params'].astype(object)<br/>#Best hyperparameters selected by the Bayesian Search<br/>for i,j in zip(cv_result,models):<br/>    results_2.at[j,'Best params']=list(i.best_params_.items())<br/>results_2</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lk"><img src="../Images/6c4ad4dbb587aeb6fb53417d75aae993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jf4vEtWUuvuBpbVpNuUZYw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">第二次实验的结果表</figcaption></figure><h1 id="e9f5" class="kf jz hh bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">第三个实验</h1><p id="48b9" class="pw-post-body-paragraph ie if hh ig b ih lc ij ik il ld in io ip le ir is it lf iv iw ix lg iz ja jb ha bi translated">最后但并非最不重要的是，第三个实验包括应用特征选择，然后降维。换句话说，我们将分别使用第二个和第一个实验中提到的方法，以便一起评估这些技术的影响。</p><p id="9d81" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">重要的是要注意，因为在这个实验中，特征选择是在降维之前执行的，所以n_features必须大于n_components。然而，每当这不成立时，RMSE将等于100，000。这可以通过给BayesSearchCV的“error_score”参数赋值来实现。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ll"><img src="../Images/69a08d2fb78319fe23b25c787dcd90d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2acXOnD4qlFbfqp2p9PgZQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">每次迭代的RMSE(图由作者创建)</figcaption></figure><p id="999a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作为这个实验的结果，过度拟合较少的模型是ADA和XGB，尽管GBR的性能是所有模型中最好的。然而，这种性能是以较高的方差为代价的。</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="84f4" class="jy jz hh ju b fi ka kb l kc kd">cv_result=[result_dt,result_rf,result_extra,result_gbr,result_ada,result_xgb]<br/>models=["DT","RF","EXT","GBR","ADA","XGB"]</span><span id="ca57" class="jy jz hh ju b fi ke kb l kc kd">results_3 = pd.DataFrame()</span><span id="6646" class="jy jz hh ju b fi ke kb l kc kd">for i,j in zip(cv_result,models):<br/>    results_3.loc[j,'RMSE train']=mean_squared_error(y_train,i.predict(X_train),squared=False)<br/>    results_3.loc[j,'RMSE Val']=(i.best_score_)*(-1)<br/>    results_3.loc[j,'RMSE test']=mean_squared_error(y_test,i.predict(X_test),squared=False)<br/>#Difference between RMSE of the training set and the test set to see how far they are from one of another<br/>results_3['Dif']=results_3['RMSE train']-results_3['RMSE test']<br/>results_3['Best params']=0<br/>results_3['Best params'] = results_3['Best params'].astype(object)<br/>#Best hyperparameters selected by the Bayesian Search<br/>for i,j in zip(cv_result,models):<br/>    results_3.at[j,'Best params']=list(i.best_params_.items())<br/>results_3</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lm"><img src="../Images/337a7aa0f383d51c0d0e3357d6df4603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3s2Y2HhjQUXd33CiJTqgUw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">第三次实验的结果表</figcaption></figure><p id="224c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过比较所有这些模型，我们可以说，来自第三个实验的极端梯度增强在具有最低测试RMSE的那些模型和那些似乎过拟合较少的模型中表现最好。我希望这篇文章能激发您探索项目的更多方法/技术，并记得在这里找到完整的代码<a class="ae js" href="https://github.com/Michelpayan/House_price_prediction" rel="noopener ugc nofollow" target="_blank">以供参考。</a></p></div></div>    
</body>
</html>