<html>
<head>
<title>Multiple Linear Regression from Scratch using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python从头开始多元线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multiple-linear-regression-from-scratch-using-python-db9368859f?source=collection_archive---------2-----------------------#2021-08-26">https://medium.com/analytics-vidhya/multiple-linear-regression-from-scratch-using-python-db9368859f?source=collection_archive---------2-----------------------#2021-08-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/3d407eb28eead4c5d6dceab5098b128c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E7p5qgdsbq9pI3l0a05vKg.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图片来自漫威</figcaption></figure><div class=""/><p id="d8b7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在上一篇文章中，您学习了如何仅使用NumPy从头开始实现一个简单的线性回归。在今天的帖子中，我将展示如何仅使用NumPy从头开始实现多元线性回归。</p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><h1 id="e5df" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">多元线性回归</h1><p id="7969" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr hb bi translated">在简单线性回归中，我们希望仅使用一个解释变量“x”来预测因变量“y ”,如下式所示。</p><p id="aa7c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">y= ax + b</p><p id="5a2c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">“y”是因变量，“x”是解释变量，“a”是直线的斜率，“b”是截距变量，即“x”为零时“y”的值。在线性回归中，我们希望找到使预测误差最小化的“a”和“b”的值。</p><p id="1b0d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">多元线性回归是线性回归的扩展，当您有多个解释变量来预测因变量时使用。</p><figure class="ld le lf lg fd hk er es paragraph-image"><div class="er es lc"><img src="../Images/02a703ff1b3fe2de7ee6d5efb456f2e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*jxT4G7TV9Po1d5Ad-u1jKg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图一。多元线性回归公式</figcaption></figure><p id="1753" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">其中，对于i=n个观察值:</p><ul class=""><li id="4a38" class="lh li hx iw b ix iy jb jc jf lj jj lk jn ll jr lm ln lo lp bi translated">Y =是因变量。</li><li id="07a0" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr lm ln lo lp bi translated">Xs =是解释变量。</li><li id="0397" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr lm ln lo lp bi translated"><em class="lv">图</em> 0 =是y轴截距(常数项)。</li><li id="d5d4" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr lm ln lo lp bi translated">其他<em class="lv">a</em>是每个解释变量的斜率系数。</li><li id="3947" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr lm ln lo lp bi translated"><em class="lv">也称为砝码。</em></li></ul><p id="ab06" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果你注意的话，线性回归是多元回归的简单版本，其中从<em class="lv">2</em>到<em class="lv">p</em>的所有<em class="lv">项都为零。</em></p><p id="a417" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">举个例子，我们假设你想卖你的车，想估计你的车值多少钱。你知道诸如车型年份、马力和里程等因素会影响汽车价格。在这种情况下，您可以创建如下所示的多元线性回归。</p><figure class="ld le lf lg fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lw"><img src="../Images/c1d34521a97e598e29e81928c478120a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z9zP46joqSnn_8GDyDi--w.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图二。一个多元线性回归的例子。</figcaption></figure></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><h2 id="e49c" class="lx ka hx bd kb ly lz ma kf mb mc md kj jf me mf kn jj mg mh kr jn mi mj kv mk bi translated">但是我怎么知道贝塔的最佳值是什么？</h2><p id="a61f" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr hb bi translated">这部分类似于简单的线性回归。我们希望使用梯度下降技术来最小化成本函数。如果你不知道这些术语是什么，你可以在<a class="ae ml" rel="noopener" href="/analytics-vidhya/stop-just-using-machine-learning-and-learn-how-to-build-it-linear-regression-and-gradient-descent-3653de24c6d5">我的媒体帖子</a>中学习。</p><p id="b2c3" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从我之前的帖子，你知道成本函数是下面的函数。</p><figure class="ld le lf lg fd hk er es paragraph-image"><div class="er es mm"><img src="../Images/6102500e1b0616be2d4a478d01994b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*VGVT_2igYyERDagUyFlCPw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图3。简单线性回归误差函数。其中，n是数据中的观察次数，ȳ是预测值，y是实际值</figcaption></figure><p id="b5fd" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们的目标是找到使成本函数值最小的‘a’和‘b’的值。简单线性回归的导数，其中:</p><figure class="ld le lf lg fd hk er es paragraph-image"><div class="er es mn"><img src="../Images/ace3a9ae3e89717bcc3d6c60ff8999a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*iKGKqQ7Zr0W1UHZjH0PIrA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图4。简单线性回归的偏导数</figcaption></figure><p id="f5a8" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于多元线性回归，过程是相同的，但现在我们将一个<em class="lv"> X </em> 0 = 1添加到等式中，这样我们就可以推广成本函数的导数。所以多元线性回归公式变成了:</p><figure class="ld le lf lg fd hk er es paragraph-image"><div class="er es mo"><img src="../Images/ea1928ecc2654f793f6f1a7e037ad461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*ENx9UNeiOn0FmAGClstzjQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图五。完全多元线性回归公式</figcaption></figure><p id="27c3" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这个函数的导数是</p><figure class="ld le lf lg fd hk er es paragraph-image"><div class="er es mp"><img src="../Images/f523681f5141fa88cc5d4b636bb2001f.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*xtS30d80qVvpBMVcWc2_Kw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图六。线性多元回归的偏导数。</figcaption></figure><p id="0ae1" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了更新权重，我们只需要将导数乘以学习率，然后从之前的权重中减去。</p><figure class="ld le lf lg fd hk er es paragraph-image"><div class="er es mq"><img src="../Images/93d87c6329ca9a112c471c3c96f4b815.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*c86toKXYWqxlSH-CnckI6A.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图7。更新<em class="mr">s的公式，其中</em> α是学习率<em class="mr">。</em></figcaption></figure><p id="248d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们同时更新所有的<em class="lv">和</em>是很重要的。</p><p id="4109" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是一个迭代过程，我们能通过使用矩阵使它更有效吗？</p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><h1 id="566d" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">矢量化多元线性回归</h1><p id="1c61" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr hb bi translated">在Python中，我们可以使用矢量化来实现多元线性回归和梯度下降。我们可以将y、s和x转换成矩阵，如下图所示。</p><figure class="ld le lf lg fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ms"><img src="../Images/d33af199e17e41d5fbe811407aed2101.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tZlMxLQmvAhTvI30GN6Geg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图八。矢量化多元线性回归</figcaption></figure><p id="27ac" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">利用该图像，我们可以使用下面的公式得到预测的y:</p><figure class="ld le lf lg fd hk er es paragraph-image"><div class="er es mt"><img src="../Images/fb45e983c6f21690906c5cfcbc2237c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*5XwFA-Zp3y2UTX82BIb7rQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图九。用于获得预测值的矢量化公式</figcaption></figure><p id="5430" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">具有以下公式的导数:</p><figure class="ld le lf lg fd hk er es paragraph-image"><div class="er es mu"><img src="../Images/425a649b08c9af3c37ad182706bdc275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*PfLVgmkGRLNzcRrxITNCTQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图10。矢量化偏导数。</figcaption></figure><p id="a449" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">最后，为了获得更新的权重，我们有下面的等式:</p><figure class="ld le lf lg fd hk er es paragraph-image"><div class="er es mv"><img src="../Images/6749ce2e687e9c3e725e9570890d7e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*5Tzg1bL0E9V1c5c4aX6WLA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图11。更新权重的矢量化公式</figcaption></figure></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><h1 id="af1b" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">让我们写代码吧</h1><figure class="ld le lf lg fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mw"><img src="../Images/a616b9ab53ae629770dcfcfc19d3bcf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bTZYZGosMoE_oziuIKLv9g.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图12。多元线性回归函数声明</figcaption></figure><p id="3e93" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对正在学习数据科学但没有软件工程背景的人的第一条建议是。始终记录您的代码。</p><p id="7a78" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，我们从一个名为fit_linear_regression的函数开始，它将接收Xs，Ys，学习速率和ε。ε作为一个阈值，当误差小于ε时，我们将停止。</p><figure class="ld le lf lg fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mx"><img src="../Images/fca8847d3903953392dcf018db1f209c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vjzkdQcNTTJsOp0iNW9HPA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图13。Python中的多元线性回归</figcaption></figure><ul class=""><li id="562e" class="lh li hx iw b ix iy jb jc jf lj jj lk jn ll jr lm ln lo lp bi translated">在步骤1中，我们将包含1的列作为y轴截距插入到x NumPy数组中。</li><li id="b961" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr lm ln lo lp bi translated">在第二步中，我们初始化，这里我调用权重。权重将是一个NumPy数组，包含x中变量的数量。</li><li id="d268" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr lm ln lo lp bi translated">在步骤3中，我们将更新权重，直到偏导数的范数小于ε。</li><li id="7629" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr lm ln lo lp bi translated">在步骤3.1中，我们得到了如图9所示的预测值和如图10所示的偏导数。</li><li id="ee55" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr lm ln lo lp bi translated">在步骤3.2中，我们得到了norma。</li><li id="ea6d" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr lm ln lo lp bi translated">在步骤3.3中，我们更新了权重，如图11所示。</li><li id="bd09" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr lm ln lo lp bi translated">第41行和第42行中的if是为了在我们输入一个高学习率并且函数发散时警告我们。</li><li id="39a1" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr lm ln lo lp bi translated">函数的返回是调整后的权重。</li></ul><p id="14fa" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在我们有了正确的权重，我们如何预测值呢？</p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><h1 id="1fc1" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">做预测</h1><figure class="ld le lf lg fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es my"><img src="../Images/56db768bfeb912ff7c5fa188c665b619.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LV7aEr5WaaHCR-7-dlOZcQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图14。函数进行预测</figcaption></figure><p id="a7bb" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">要进行预测，我们只需在权重数组(不包括y截距的最后一个值)和之后转置的Xs值之间进行点积，就可以得到这个结果，并将其与y截距相加。</p><p id="f0e6" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在我们从头开始实现了我们的多元线性回归，但是它与sklearn相比如何呢？</p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><h1 id="0c7d" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">与sklearn比较</h1><figure class="ld le lf lg fd hk er es paragraph-image"><div class="er es mz"><img src="../Images/0ad8f95aff20d881f87006a7f2879d6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*ec0th7ZUB3rV2WOYXGiaTQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图15。来自sklearn函数和我们函数的MSE</figcaption></figure><p id="71e0" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">第一个是来自sklearn模型的<a class="ae ml" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方误差</a>，第二个是来自我们函数的MSE。</p><p id="082c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正如我们所看到的，它们是相似的。我们的函数MSE只比sklearn大0.004。</p><h1 id="a207" class="jz ka hx bd kb kc na ke kf kg nb ki kj kk nc km kn ko nd kq kr ks ne ku kv kw bi translated">结论</h1><p id="5c2d" class="pw-post-body-paragraph iu iv hx iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr hb bi translated">在这篇文章中，你学到了</p><ol class=""><li id="0fe2" class="lh li hx iw b ix iy jb jc jf lj jj lk jn ll jr nf ln lo lp bi translated">什么是多元线性回归。</li><li id="ab48" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr nf ln lo lp bi translated">我们如何拟合多元线性回归模型。</li><li id="3017" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr nf ln lo lp bi translated">矢量化多元线性回归公式。</li><li id="f76f" class="lh li hx iw b ix lq jb lr jf ls jj lt jn lu jr nf ln lo lp bi translated">如何只用Python和NumPy实现自己的多元线性回归？</li></ol><p id="c00c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">你可以在这个<a class="ae ml" href="https://colab.research.google.com/drive/1uO-f2GLgBqSXiBmI46MaHHDJccJMdy11?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab </a>笔记本里看到用来写这篇文章的代码。</p><p id="0359" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果你喜欢你所读的，一定要👏下面，分享给你的朋友，关注我，不要错过这一系列的帖子。</p></div></div>    
</body>
</html>