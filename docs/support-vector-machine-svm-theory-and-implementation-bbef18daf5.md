# 支持向量机(SVM)——理论与实现

> 原文：<https://medium.com/analytics-vidhya/support-vector-machine-svm-theory-and-implementation-bbef18daf5?source=collection_archive---------0----------------------->

![](img/7878d0d46bd5319c3d3f2f654495ff64.png)

马克·柯尼希在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

在我使用的案例中，支持向量机(SVM)模型几乎总能产生好的结果。这是一个优秀的分类模型。算法逻辑是合理的，相当容易实现，需要调整的重要参数很少，并且在其线性模型中，具有模型解释能力的系数。在这篇文章中，我将解释 SVM 的机制，并通过它的实现。

## 定义

支持向量机或 SVM 是一种机器学习模型，它基于使用一个超平面，该超平面可以将 n 维空间中的数据点最佳地划分为多个类别。在 Scikit-Learn 库中，这是一个可靠的分类或回归模型。超平面(在 n 维欧几里得空间中)是前述欧几里得空间的平坦的、 **n-1** 维子空间，其将空间分成两个独立的部分。超平面很难想象比我们传统的 3-D 空间更高的任何 n 值(但在数学上是可能的，所以不要担心),只要知道它将我们的数据分成几类。支持向量是靠近超平面的边缘，该超平面将我们的数据分成它的类。如果我们最大限度地提高利润率，我们就能获得数据点的最佳分类。很简单，明白了吗？

## 内核变换

好吧，但是，如果我们的数据点以这样一种方式被分开，以至于不可能拟合一个具有最大限度的边界的线性超平面…例如，如果我们的数据点以这样的方式被排列:

![](img/efb6ba3fd2d9d54bb0c46fad38faee20.png)

图为 Lilly Chen 在[https://towards data science . com/support-vector-machine-simple-explained-fee 28 EBA 5496](https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496)

核变换或“核技巧”是对我们的数据点进行变换，从而使超平面成为可能。这是 SVM 的症结所在，也是 SVM 模式的微妙之处、独特性、独特性和有效性。在 sklearn 中，可能的转换有:`‘***linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed'***` 其中预计算是一个自己创建的转换。在建模过程中，人们会“尝试”不同的内核，并为给定的数据点找到性能最佳的内核。然而，我已经提供了内核选择的指南。解释这些转换需要更高的数学知识(这超出了本文的范围)，但是这里是我自己的建议，我愿意接受关于内核转换的学术建议和讨论。其实我会在这个环节给出转换公式/解释:

[https://data-flair.training/blogs/svm-kernel-functions/](https://data-flair.training/blogs/svm-kernel-functions/)

如果您的数据点具有线性 1:1 关系(我的意思是数据以线性 x:y 模式映射)，我会推荐线性。sklearn 实现中带有“degree”的多项式变换(其中 degree 为变换提供了“bends ”)创建了一个带有曲线的超平面。Rbf 代表径向基函数，它给超平面一个径向(圆形)形状。当数据点具有清晰的决策边界但未显示在数据中时，应使用 sigmoid 变换。其他可以做更多研究的核有:方差分析核、高斯核和贝塞尔函数核。

## 超参数

SVM 类中最重要的参数是 C 和γ。c 指的是超平面在类之间分隔的边缘的距离。默认值为 1，但较高的 C 值意味着减少误分类的较小裕度，而较低的 C 值意味着增加误分类率的较大裕度。c 是正则化参数，因为该值将在模型过拟合中起作用。更高的 C 导致过度拟合。

Gamma 仅用于 rbf 和 sigmoid 核。该参数设置每个点对相邻点的影响程度。低 gamma 允许每个点对其相邻点具有较高的影响，这意味着分类更加自由，相似点之间的距离也更大。高灰度系数意味着每个点的影响较低，这意味着点必须靠得更近才能被分类在一起。较大的灰度系数会导致较低的泛化能力和更多的过拟合，而较低的灰度系数会导致更高的泛化能力。

## 系数

仅适用于线性内核。这些系数赋予模型可解释性。因为这些系数与线性模型中类别的预测相关，所以它解释了模型的特征重要性。SVM 类实例中的`.[coef_](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.coef_)` 属性会列出它的系数，而`.get_feature_names()`会给出特性标签。

## 结论

SVM 在概念上比其他 ML 模型更复杂。SVM 适用于包含许多要素的较小数据集，因为它适用于较高的维度。当我们的数据点没有清晰的划分/分离时，SVM 表现不佳。必须使用定标器对要素进行定标，而 SVM 计算量很大，因此不适合大型数据集。SVM 不容易过拟合，因为它具有良好的正则化参数(C，γ)。它也有一个回归模型。我认为 SVM 是一个多才多艺的、通用的、多用途的模型，它做得很好。