<html>
<head>
<title>NLP Word Prediction by Using Bidirectional LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于双向LSTM的自然语言处理单词预测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nlp-word-prediction-by-using-bidirectional-lstm-9c01c24b2725?source=collection_archive---------2-----------------------#2021-02-01">https://medium.com/analytics-vidhya/nlp-word-prediction-by-using-bidirectional-lstm-9c01c24b2725?source=collection_archive---------2-----------------------#2021-02-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/a1fbabd08849e44767ce75e8245de506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*jqsr1NC61a7zzp_VG2qizA.png"/></div></figure><p id="ddee" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">自然语言处理</strong> ( <strong class="in hi"> NLP </strong>)是<a class="ae jj" href="https://en.wikipedia.org/wiki/Linguistics" rel="noopener ugc nofollow" target="_blank">语言学</a>、<a class="ae jj" href="https://en.wikipedia.org/wiki/Computer_science" rel="noopener ugc nofollow" target="_blank">计算机科学</a>和<a class="ae jj" href="https://en.wikipedia.org/wiki/Artificial_intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a>的一个分支，涉及计算机和人类语言之间的交互，特别是如何给计算机编程，以处理和分析大量的<a class="ae jj" href="https://en.wikipedia.org/wiki/Natural_language" rel="noopener ugc nofollow" target="_blank">自然语言</a>数据。其结果是计算机能够“理解”文档的内容，包括其中语言的上下文细微差别。然后，该技术可以准确地提取文档中包含的信息和见解，并对文档本身进行分类和组织。</p><p id="1c03" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们将讨论如何预测你的诗歌或故事中的下一个单词。我们将用python实现来展示它。</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="c6b5" class="jt ju hh jp b fi jv jw l jx jy"># <strong class="jp hi">first off all we imported libraries which we need</strong></span><span id="60d7" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">import</strong> tensorflow <strong class="jp hi">as</strong> tf</span><span id="780c" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">from</strong> tensorflow.keras.preprocessing.sequence <strong class="jp hi">import</strong> pad_sequences</span><span id="22ab" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">from</strong> tensorflow.keras.layers <strong class="jp hi">import</strong> Embedding, LSTM, Dense, Bidirectional</span><span id="6ed4" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">from </strong>tensorflow.keras.preprocessing.text <strong class="jp hi">import</strong> Tokenizer</span><span id="060f" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">from</strong> tensorflow.keras.models <strong class="jp hi">import</strong> Sequential</span><span id="bd01" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">from</strong> tensorflow.keras.optimizers <strong class="jp hi">import</strong> Adam</span><span id="db32" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">import</strong> numpy <strong class="jp hi">as</strong> np</span></pre><h1 id="251d" class="ka ju hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">预处理</h1><p id="5b55" class="pw-post-body-paragraph il im hh in b io kx iq ir is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji ha bi translated">在开始处理数据之前，我们收集的数据集中有一些问题需要解决。这些都是简单的清理程序，便于在后续步骤中使用数据。但是Tensorflow通常会为我们实现。下面是一些通常要做的预处理步骤。</p><ul class=""><li id="a74b" class="lc ld hh in b io ip is it iw le ja lf je lg ji lh li lj lk bi translated">去除空白</li><li id="edc2" class="lc ld hh in b io ll is lm iw ln ja lo je lp ji lh li lj lk bi translated">小写转换</li><li id="5771" class="lc ld hh in b io ll is lm iw ln ja lo je lp ji lh li lj lk bi translated">移除数字</li><li id="ef6e" class="lc ld hh in b io ll is lm iw ln ja lo je lp ji lh li lj lk bi translated">删除标点符号</li><li id="6cf6" class="lc ld hh in b io ll is lm iw ln ja lo je lp ji lh li lj lk bi translated">删除不需要的单词</li><li id="c868" class="lc ld hh in b io ll is lm iw ln ja lo je lp ji lh li lj lk bi translated">删除非英语单词</li></ul><h1 id="a9d4" class="ka ju hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">标记化</h1><p id="53b2" class="pw-post-body-paragraph il im hh in b io kx iq ir is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji ha bi translated">一种重要的规范化方法叫做标记化。它只是将连续的文本分割成单个的单词片段。一个非常简单的方法是在每个空格上分割输入，并为每个单词分配一个标识符。举个例子，</p><p id="f127" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">tokenizing<em class="lq">“2016里约奥运会不包括乒乓球不酷”</em>会产生；</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/5231736d40043a2250e85a564ddbe2f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*CmgBktUgxzfMSt-p0lstHw.png"/></div></figure><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="6119" class="jt ju hh jp b fi jv jw l jx jy"><strong class="jp hi">#</strong> <strong class="jp hi">We read data and specified Tokenizer</strong></span><span id="a9e9" class="jt ju hh jp b fi jz jw l jx jy">tokenizer = Tokenizer()</span><span id="126c" class="jt ju hh jp b fi jz jw l jx jy">data = open('/tmp/irish-lyrics-eof.txt').read()</span><span id="6049" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># We splited the data by whitespaces and created sentence list </strong></span><span id="2db1" class="jt ju hh jp b fi jz jw l jx jy">corpus = data.lower().split("\n")</span><span id="def7" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># with "fit_on_text" method tokenized each sentence in the corpus</strong></span><span id="db94" class="jt ju hh jp b fi jz jw l jx jy">tokenizer.fit_on_texts(corpus)</span><span id="b4f2" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># Afterwards we created word index specified unique number of words in the corpus by assign index number each of them<br/># like this; </strong>{'car': 1, 'prison': 2, 'him': 3, 'welcome': 4}</span><span id="7e73" class="jt ju hh jp b fi jz jw l jx jy">total_words = len(tokenizer.word_index) + 1 </span><span id="8369" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># we'll add one to this to consider vocabulary words</strong></span><span id="8a62" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># This is a key value pair with the key being the word and the value being the token for that word</strong></span><span id="5a9c" class="jt ju hh jp b fi jz jw l jx jy"><br/>print(tokenizer.word_index)<br/>print(total_words)</span></pre><h1 id="6a01" class="ka ju hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">填充序列</h1><p id="3018" class="pw-post-body-paragraph il im hh in b io kx iq ir is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji ha bi translated">即使在将句子转换成数值之后，仍然存在向我们的神经网络提供相等长度输入的问题——不是每个句子都是相同长度的！有两种主要方法可以处理输入的句子来实现这一点——用零填充较短的句子，以及截断一些较长的序列以使其更短。事实上，您可能会使用这些方法的组合。</p><p id="9646" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">使用TensorFlow，<code class="du ls lt lu jp b">tf.keras.preprocessing.sequence</code>中的<code class="du ls lt lu jp b">pad_sequences</code>函数可用于这两项任务。给定一个序列列表，您可以指定一个<code class="du ls lt lu jp b">maxlen</code>(任何比它长的序列都将被缩短)，以及是否从开头或结尾填充和截断，这取决于<code class="du ls lt lu jp b">padding</code>和<code class="du ls lt lu jp b">truncating</code>参数的<code class="du ls lt lu jp b">pre</code>或<code class="du ls lt lu jp b">post</code>设置。默认情况下，填充和截断将从序列的开始发生，所以如果您希望它发生在序列的结尾，请将它们设置为<code class="du ls lt lu jp b">post</code>。</p><p id="1483" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">因此，让我们来看看将这个语料库转化为训练数据的代码。这是开始，我们将一行一行地解开这个。首先，我们所有的训练轴将被称为输入序列，这将是一个python列表。</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="01c7" class="jt ju hh jp b fi jv jw l jx jy"><strong class="jp hi"># each line of the corpus we'll generate a token list using the tokenizers, text_to_sequences method.</strong></span><span id="820d" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">example:</strong> In the town of Athy one Jeremy Lanigan<br/>                      <br/>                 [4,2,66,67,68,69,70]</span><span id="34ec" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">This will convert a line of text like,sentence above into a list of tokens representing the words.</strong></span><span id="d37d" class="jt ju hh jp b fi jz jw l jx jy">input_sequences = []</span><span id="c662" class="jt ju hh jp b fi jz jw l jx jy">for line in corpus:</span><span id="0f7e" class="jt ju hh jp b fi jz jw l jx jy">    token_list = tokenizer.texts_to_sequences([line])[0]</span><span id="a7a2" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">Then will iterate over this list of tokens and create a number of n-grams sequences, namely the first two words in the sentence or one sequence then the first three are another sequences etc.</strong><br/>    <br/>    for i in range(1, len(token_list)):</span><span id="2f9e" class="jt ju hh jp b fi jz jw l jx jy">        n_gram_sequence = token_list[:i+1]</span><span id="7bb5" class="jt ju hh jp b fi jz jw l jx jy">        input_sequences.append(n_gram_sequence)</span></pre><p id="a3df" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这样做的结果是，对于之前创建的句子，将生成以下输入序列。每一行都会发生同样的过程，但是正如你所看到的，输入序列只是被分解成短语的句子，前两个单词，前三个单词，等等</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lv"><img src="../Images/fa3ddea152e86cfc3172533833aa21f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TveFZCwPjJfDbD3uJmSBvw.png"/></div></div></figure><p id="1c70" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们接下来需要找到语料库中最长句子的长度。为了做到这一点，我们将遍历所有的序列，找到最长的一个，代码如下。</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="3fab" class="jt ju hh jp b fi jv jw l jx jy">max_sequence_len = max([len(x) for x in input_sequences])</span></pre><p id="2b9c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">一旦我们有了最长的序列长度，接下来要做的就是填充所有的序列，使它们长度相同。我们将预先填充零，以便更容易提取标签。</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="0cb9" class="jt ju hh jp b fi jv jw l jx jy">Line:                                 Padded Input Sequences:</span><span id="a260" class="jt ju hh jp b fi jz jw l jx jy">[4 2 66 8 67 68 69 70]               [0 0 0 0 0 0 0 0 0 0 4 2]<br/>                                     [0 0 0 0 0 0 0 0 0 4 2 66]<br/>                                     [0 0 0 0 0 0 0 0 4 2 66 8]<br/>                                     [0 0 0 0 0 0 0 4 2 66 8 67]<br/>                                     [0 0 0 0 0 0 0 4 2 66 8 67 68]<br/>                                     [0 0 0 0 0 4 2 66 8 67 68 69]<br/>                                     [0 0 0 0 4 2 66 8 67 68 69 70]<br/>                                     </span></pre><p id="eeec" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">所以现在，我们的行将由一组填充的输入序列表示，看起来像上面的例子。现在，我们有了我们的序列，接下来我们需要做的是将它们转换成<strong class="in hi"> x </strong>和<strong class="in hi"> y，</strong>我们的输入值和它们的标签。现在你想想，句子是这样表示的，我们要做的就是把除了最后一个字符之外的所有字符都作为<strong class="in hi"> x </strong>，然后把最后一个字符作为标签上的<strong class="in hi"> y </strong>。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es ma"><img src="../Images/d1ce04d9303cc84e63871e465eb54acc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*QF8B4m5lX24nOcz9UbByqA.png"/></div></figure><p id="ccf5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们像上面这样做，其中对于第一个序列，直到<strong class="in hi"> 4 </strong>的所有内容都是我们的输入，而<strong class="in hi"> 2 </strong>是我们的标签。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es mb"><img src="../Images/b7a0a329a95fa72714c4d6fa8c4b432c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*5DL7PnEMK8wclfp1Aln7uQ.png"/></div></figure><p id="7261" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">同样，在第二个序列中，输入是两个字，第三个字，标记为66。</p><p id="aebb" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">至此，我们应该清楚为什么要进行预填充，因为这使得我们通过获取最后一个令牌来获取标签变得更加容易。</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="cd6f" class="jt ju hh jp b fi jv jw l jx jy"><strong class="jp hi">So now, we have to split our sequences into our <em class="lq">x's</em> and our<em class="lq"> y's</em>. To do this, let's grab the first n tokens, and make them our <em class="lq">x's</em>. </strong></span><span id="d9dc" class="jt ju hh jp b fi jz jw l jx jy">X = input_sequences[:,:-1]<br/>labels = input_sequences[:,-1]</span><span id="e50d" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">We'll then get the last token and make it our label. Before the label becomes a y, there' s one more step, and you'll see that shortly. </strong></span><span id="b01b" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># One-hot encode with keras convert list to a categorical. The number of classes which is my number of words.</strong></span><span id="639e" class="jt ju hh jp b fi jz jw l jx jy">Y = tf.keras.utils.to_categorical(labels, num_classes=total_words)</span><span id="3cab" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">Now, I should one-hot encode my labels as this really is a classification problem. We can classify from the corpus, what the next word would likely be.</strong>  </span></pre><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mc"><img src="../Images/6c75152437daa3cf5a6cd53e6f48d330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BPfIr90p_QCuLwsrVA6lOw.png"/></div></div></figure><p id="877e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果我们把这个记号列表看作一个句子，那么x是直到最后一个值的列表，标签是最后一个值，在这个例子中是70。y是一个独热编码数组，不管length是单词集的大小，而设置为1的值是标签索引处的值，在这种情况下是第70个元素。</p><p id="aaa4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">所以现在我们可以建立一个神经网络，给定一个句子，它可以预测下一个单词。</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="4c02" class="jt ju hh jp b fi jv jw l jx jy"><strong class="jp hi"># Now that we have our data as xs and ys, it's relatively simple for us to create a neural network to classify what the next word should be, given a set of words.</strong></span><span id="a7f0" class="jt ju hh jp b fi jz jw l jx jy">model = Sequential()</span><span id="cbd8" class="jt ju hh jp b fi jz jw l jx jy"># <strong class="jp hi">Embedding layer 1st parameter  : we'll want it to handle all of our words, so we set that in the first parameter.</strong></span><span id="2eb4" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># 2nd parameter: number of dimensions to use to plot the vector for a word. </strong></span><span id="2053" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># finally, the size of the input dimensions will be fed in, and this is the length of the longest sequence minus 1. We subtract one because we cropped off the last word of each sequence to get the label.</strong></span><span id="214a" class="jt ju hh jp b fi jz jw l jx jy">model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))</span><span id="f4ff" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># Bidirectional LSTM</strong></span><span id="059f" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">#Unidirectional LSTM only preserves information of the past because the only inputs it has seen are from the past.</strong></span><span id="9bb6" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">#Using bidirectional will run your inputs in two ways, one from past to future and one from future to past and what differs this approach from unidirectional is that in the LSTM that runs backwards you preserve information from the future and using the two hidden states combined you are able in any point in time to preserve information from both past and future.</strong></span><span id="3465" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># We specified 150 units here</strong></span><span id="1dbf" class="jt ju hh jp b fi jz jw l jx jy">model.add(Bidirectional(LSTM(150)))</span><span id="8b31" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># Finally there is a dense layer sized as the total words, because we have as many outputs as the total word count.</strong></span><span id="ac5c" class="jt ju hh jp b fi jz jw l jx jy">model.add(Dense(total_words, activation='softmax'))</span><span id="d900" class="jt ju hh jp b fi jz jw l jx jy">adam = Adam(lr=0.01)</span><span id="a1f5" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">#We're doing a categorical classification, so we'll set the laws to be categorical cross entropy.</strong></span><span id="ee94" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">#And we'll use adam optimizer to minimize loss. Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.</strong></span><span id="3534" class="jt ju hh jp b fi jz jw l jx jy">model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])</span><span id="1d97" class="jt ju hh jp b fi jz jw l jx jy">#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')</span><span id="b986" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">#And we specified epochs number, One Epoch is when an entire dataset is passed forward and backward through the neural network only once.</strong></span><span id="f64f" class="jt ju hh jp b fi jz jw l jx jy">history = model.fit(xs, ys, epochs=500, verbose=1)</span><span id="68a3" class="jt ju hh jp b fi jz jw l jx jy">#print model.summary()</span><span id="103f" class="jt ju hh jp b fi jz jw l jx jy">print(model)</span></pre><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es md"><img src="../Images/e24b833cfb836c419e8dabc340cae764.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_DlDY9uchdUmD_jB-3XIhQ.png"/></div></div></figure><p id="a785" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们可以用历史方法来查看列车损失。正如你所看到的，我们的模型在前10个时期迅速下降到0.1的水平。它继续着，几乎没有起伏。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es me"><img src="../Images/4f0f5c4c731c2ab7a271d2094245c79c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7HywC8Us-8u3rEbC29Fyvg.png"/></div></div></figure><p id="db3a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">要了解更多关于LSTM的信息，您可以点击此<a class="ae jj" href="https://www.youtube.com/watch?v=WCUNPb-5EYI&amp;t=52s" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="b636" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果我们想预测句子中接下来的10个单词。</p><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="cf56" class="jt ju hh jp b fi jv jw l jx jy"><strong class="jp hi"># We need return our text into sequences to do prediction, because our input shape like this below.</strong></span><span id="e700" class="jt ju hh jp b fi jz jw l jx jy">tokent_list = tokenizer.text_to_sequences([text])[0]</span><span id="941f" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">#we need to do pre padding to make each sequences same length by longest sentence in the corpus</strong></span><span id="bbd4" class="jt ju hh jp b fi jz jw l jx jy">token_list = pad_sequences([token_list], maxlen = max_sequence_len-1, padding="pre")</span><span id="dd9f" class="jt ju hh jp b fi jz jw l jx jy">text = Laurence went to dublin</span><span id="7bc9" class="jt ju hh jp b fi jz jw l jx jy">[0,0,0,0,0,0,0,134,13,59]</span><span id="e457" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># and by passing our token list into the prediction function we can do prediction.<br/># This will give us the token of the word most likely to be the next one in the sequence. </strong></span><span id="e1ac" class="jt ju hh jp b fi jz jw l jx jy">predicted = model.predict_classes(token_list)</span></pre><p id="6f01" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果我们运行下面的代码行，我们可以看到预测的下一个单词是<strong class="in hi">【old】</strong>。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mf"><img src="../Images/ad7cfcadb25c11e142f6b7120ee086ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yfigu-P9_zynsxbhOJdtyA.png"/></div></div></figure><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="90cf" class="jt ju hh jp b fi jv jw l jx jy"><strong class="jp hi"># we can do a reverse lookup on the word index items to turn the token back into a word and to add that to our text.</strong></span><span id="9312" class="jt ju hh jp b fi jz jw l jx jy">text = "Laurence went to dublin "</span><span id="69eb" class="jt ju hh jp b fi jz jw l jx jy">next_words = 10</span><span id="f0e7" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">With for loop below, we can predict next word. We can  specify next word number above. Each iterartion add new predicted word our text.</strong></span><span id="48ab" class="jt ju hh jp b fi jz jw l jx jy">for _ in range(next_words):</span><span id="f6e5" class="jt ju hh jp b fi jz jw l jx jy">    token_list = tokenizer.texts_to_sequences([text])[0]</span><span id="f68a" class="jt ju hh jp b fi jz jw l jx jy">    token_list = pad_sequences([token_list],          maxlen=max_sequence_len-1, padding='pre')</span><span id="8162" class="jt ju hh jp b fi jz jw l jx jy">    predicted = model.predict_classes(token_list, verbose=0)</span><span id="5bb7" class="jt ju hh jp b fi jz jw l jx jy">    output_word = ""</span><span id="6adf" class="jt ju hh jp b fi jz jw l jx jy">        for word, index in tokenizer.word_index.items():</span><span id="bf55" class="jt ju hh jp b fi jz jw l jx jy">            if index == predicted:</span><span id="694f" class="jt ju hh jp b fi jz jw l jx jy">            output_word = word</span><span id="37c3" class="jt ju hh jp b fi jz jw l jx jy">            break</span><span id="ada2" class="jt ju hh jp b fi jz jw l jx jy">        seed_text += " " + output_word</span><span id="e65a" class="jt ju hh jp b fi jz jw l jx jy">print(seed_text)</span></pre><p id="b60e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果我们想要预测<strong class="in hi"> seed_text，</strong>之后的下10个单词，我们可以将next_word指定为10，当我们查看输出时，您可以看到添加了10个单词的句子。<strong class="in hi">“劳伦斯去都柏林老欢迎陌生人詹姆斯一些我的她坚持你的立场”</strong>这没有足够的意义，但如果我们用大得多的语料库训练我们的模型，我们可以获得良好的结果。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mg"><img src="../Images/65602a6c1071dea14899a09e503116bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nWRaxqUgQqCY3o40aEx6Ng.png"/></div></div></figure><pre class="jk jl jm jn fd jo jp jq jr aw js bi"><span id="b4ef" class="jt ju hh jp b fi jv jw l jx jy">import tensorflow as tf<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences<br/>from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional<br/>from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.optimizers import Adam<br/>import numpy as np</span><span id="4f81" class="jt ju hh jp b fi jz jw l jx jy">tokenizer = Tokenizer()<br/>data = open('/tmp/irish-lyrics-eof.txt').read()<br/>corpus = data.lower().split("\n")<br/>tokenizer.fit_on_texts(corpus)<br/>total_words = len(tokenizer.word_index) + 1<br/>print(tokenizer.word_index)<br/>print(total_words)</span><span id="6d62" class="jt ju hh jp b fi jz jw l jx jy">input_sequences = []</span><span id="b596" class="jt ju hh jp b fi jz jw l jx jy">for line in corpus:<br/>    token_list = tokenizer.texts_to_sequences([line])[0]<br/>    for i in range(1, len(token_list)):<br/>        n_gram_sequence = token_list[:i+1]<br/>        input_sequences.append(n_gram_sequence)</span><span id="81bf" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># pad sequences</strong></span><span id="2025" class="jt ju hh jp b fi jz jw l jx jy">max_sequence_len = max([len(x) for x in input_sequences])<br/>input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))</span><span id="64e9" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi"># create predictors and label</strong></span><span id="9caf" class="jt ju hh jp b fi jz jw l jx jy">xs, labels = input_sequences[:,:-1],input_sequences[:,-1]<br/>ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)</span><span id="8d76" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">#Model build</strong></span><span id="cf17" class="jt ju hh jp b fi jz jw l jx jy">model = Sequential()<br/>model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))<br/>model.add(Bidirectional(LSTM(150)))<br/>model.add(Dense(total_words, activation='softmax'))<br/>adam = Adam(lr=0.01)<br/>model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])<br/>#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')<br/>history = model.fit(xs, ys, epochs=100, verbose=1)<br/>#print model.summary()<br/>print(model)</span><span id="23df" class="jt ju hh jp b fi jz jw l jx jy">seed_text = "Laurence went to dublin "</span><span id="75e1" class="jt ju hh jp b fi jz jw l jx jy">next_words = 10</span><span id="0f66" class="jt ju hh jp b fi jz jw l jx jy"><strong class="jp hi">#Prediction</strong></span><span id="6374" class="jt ju hh jp b fi jz jw l jx jy">for _ in range(next_words):</span><span id="81a1" class="jt ju hh jp b fi jz jw l jx jy">    token_list = tokenizer.texts_to_sequences([seed_text])[0]</span><span id="fe6c" class="jt ju hh jp b fi jz jw l jx jy">    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')</span><span id="7bde" class="jt ju hh jp b fi jz jw l jx jy">    predicted = model.predict_classes(token_list, verbose=0)</span><span id="9427" class="jt ju hh jp b fi jz jw l jx jy">    output_word = ""</span><span id="3700" class="jt ju hh jp b fi jz jw l jx jy">    for word, index in tokenizer.word_index.items():</span><span id="423c" class="jt ju hh jp b fi jz jw l jx jy">        if index == predicted:<br/>            output_word = word<br/>            break</span><span id="bbf3" class="jt ju hh jp b fi jz jw l jx jy">seed_text += " " + output_word<br/>print(seed_text)</span></pre></div></div>    
</body>
</html>