<html>
<head>
<title>Neural Oblivious Decision Ensembles(NODE) — A State-of-the-Art Deep Learning Algorithm for Tabular Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经不经意决策集成(NODE) —一种针对表格数据的先进深度学习算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-oblivious-decision-ensembles-node-a-state-of-the-art-deep-learning-algorithm-for-tabular-1cb6788c5212?source=collection_archive---------6-----------------------#2021-02-25">https://medium.com/analytics-vidhya/neural-oblivious-decision-ensembles-node-a-state-of-the-art-deep-learning-algorithm-for-tabular-1cb6788c5212?source=collection_archive---------6-----------------------#2021-02-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="69af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">深度学习为计算机视觉、自然语言处理、强化学习等领域的许多机器学习问题带来了革命。但是表格数据仍然牢牢地保留在经典的机器学习算法之下，即梯度推进算法(如果你感兴趣，我有一整个系列关于不同的<a class="ae jc" href="https://deep-and-shallow.com/2020/02/02/the-gradient-boosters-i-the-math-heavy-primer-to-gradient-boosting-algorithm/" rel="noopener ugc nofollow" target="_blank">梯度推进算法</a>)。</p><p id="353f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">直觉上，这很奇怪，不是吗？神经网络是通用逼近器，理想情况下，它们应该能够逼近函数，即使是在表格数据域中。或者他们可以，但是需要海量的数据来正确地学习这个功能？但是梯度增强树是如何做到这一点的呢？决策树的归纳偏向可能很适合表格数据域吗？</p><p id="fdd7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果它们这么好，为什么我们不在神经网络中使用决策树呢？就像我们对图像有卷积运算，对文本有递归网络一样，为什么不能用决策树作为表格数据的基本构建块呢？</p><p id="d757" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">答案很简单——树是不可微的，没有流经网络的梯度，就像反向推进炸弹。但这是研究人员开始绞尽脑汁的地方。我们如何使决策树可区分？</p><h1 id="c9c7" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">可区分决策树</h1><p id="236f" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">2015年，Kontschieder等人提出了深度神经决策森林，它具有类似决策树的结构，但可微分。</p><p id="ff39" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们后退一步，思考一下决策树。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kg"><img src="../Images/55c25be62419efae3fd2ead2d2153dc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/0*CfpMllxWDF6PqLDb"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">常规决策树(来源[1])</figcaption></figure><p id="cee0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">典型的决策树看起来像上面的图片。简单地说，它是一个集合的决策nodes(dᵢ)和叶nodes(πᵢ)，它们一起作为一个函数，y = <strong class="ig hi"> <em class="ks"> F </em> </strong> (θ，x)，其中<strong class="ig hi"> <em class="ks"> F </em> </strong>是决策树，由θ参数化，它将输入x映射到输出y</p><p id="5894" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们先看叶节点，因为比较简单。在传统的决策树中，叶节点通常分布在类别标签上。这正好符合乙状结肠或软最大激活。因此，我们真的可以用SoftMax层替换叶节点，并使该节点可区分。</p><p id="a0b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们更深入地看看一个决策节点。决策节点的核心目的是决定是向左还是向右发送样本。姑且称这些决定为，<em class="ks"> dᵢ和d·ᵢ(读作d·巴因缺乏乳胶支持而责备媒介)</em>。对于这个决定，它使用一个特定的特征(<em class="ks"> f </em>)和一个阈值(<em class="ks">b</em>)——这些是节点的参数。</p><p id="fc8d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在传统的决策树中，这个决策是二元决策；不是右就是左，0就是1。但这是确定性的，不可微的。现在，如果我们放松这一点，使路由随机。它不是一个尖锐的1或0，而是一个介于0和1之间的数字。这感觉像是熟悉的领域，不是吗？乙状结肠函数？</p><p id="991f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这正是Kontschieder等人提出的。如果我们将严格的0–1决策放宽到具有sigmoid函数的随机决策，则节点变得可微。</p><p id="e543" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们知道了单个节点(决策或叶节点)是如何工作的。让我们把它们放在一起。上图中的红色路径是决策树中的一条路径。在确定性版本中，样本要么通过这条路线，要么不通过。如果我们从概率的角度考虑相同的过程，我们知道，对于样本到达路径末端的叶节点的路径中的每个节点，样本进入路径的概率应该是1。</p><p id="aa2b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在概率范例中，我们找到样本向左或向右(<em class="ks"> dᵢ和d·ᵢ</em>)的概率，并将沿着路径的所有概率相乘，以获得样本到达叶节点的概率。</p><p id="2c3d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">样本到达突出显示的叶节点的概率将是(<em class="ks">d₁×₂×₅)</em>。</p><p id="64f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们只需要使用每个决策路径的概率来获取所有叶节点的期望值，以获得样本的预测。</p><p id="2d28" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在你已经对如何在神经网络中使用类似决策树的结构有了直觉，让我们来谈谈节点模型。</p><h1 id="a752" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">神经遗忘树</h1><p id="7e65" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">遗忘树是一种对称生长的决策树。这些是树，相同的特征负责将学习实例分成树的每一层的左和右分区。<a class="ae jc" href="https://deep-and-shallow.com/2020/02/29/the-gradient-boosters-v-catboost/" rel="noopener ugc nofollow" target="_blank"> CatBoost </a>，一个突出的梯度增强实现，使用了不经意树。遗忘树特别有趣，因为它们可以简化为一个具有2ᵈ单元的决策表，其中d是树的深度。这非常简洁地简化了事情。</p><p id="9b2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个不经意决策树(ODT)输出一个2ᵈ响应，其中<em class="ks"> d </em>是树的深度。这是通过使用<em class="ks"> d </em>特征-阈值组合来完成的，这些组合是ODT的参数。</p><p id="e47b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">形式上，ODT可以定义为:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kt"><img src="../Images/46b0fff2119798822e6891b8119151e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/0*H6VspLA7A8jNgHdV"/></div></figure><p id="51f7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">，其中<strong class="ig hi"> I <em class="ks"> </em> </strong>表示亥维赛函数(这是一个阶跃函数，0表示负值，1表示正值)</p><p id="aa8d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，为了使树输出可区分，我们应该替换分裂特征选择(<em class="ks"> f </em>)和使用阈值的比较运算符(<em class="ks"> b </em>)，而是它们的连续对应物。</p><p id="9807" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在传统的树中，选择一个特征来分割节点是一个确定性的决定。但是对于可区分性，我们选择一种更软的方法，即特征的加权和，其中权重被学习。通常，我们会考虑选择Softmax而不是特征，但我们希望选择稀疏特征，即我们希望只对少数(最好是1个)特征做出决定。因此，为此，NODE在可学习的特征选择矩阵上使用α-entmax变换(Peters等人，2019年)</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ku"><img src="../Images/8cd0f5056d3761e4f72d515a1ad79673.png" data-original-src="https://miro.medium.com/v2/resize:fit:156/0*yYgr1Sdf14QPx6Ph"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kv"><img src="../Images/32e5109fa5adbf5dcf56a95e3d6a5ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RrdtNztL0vsM4z5v"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">来源:[2]</figcaption></figure><p id="bcdc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">类似地，我们将Heaviside函数放宽为两类entmax。因为不同的特征可以具有不同的特征尺度，所以我们用参数τ来缩放entmax</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es la"><img src="../Images/e6344ba7c6849b1eb3ecdb9dc0ba11c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/0*jyw0KywkOtEu3afX"/></div></figure><p id="a3f5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中<em class="ks"> bᵢ </em>和<em class="ks"> τᵢ </em>分别是阈值和标度的可学习参数。</p><p id="b6b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们知道一棵树有两面，而到了<em class="ks"> cᵢ </em>，我们只定义了一面。所以为了完成这个树，我们将<em class="ks"> cᵢ </em>和<em class="ks"> (1-cᵢ) </em>一个放在另一个上面。现在我们定义一个“选择”张量<strong class="ig hi"> <em class="ks"> C </em> </strong>作为所有树的外积:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lb"><img src="../Images/9cb01eb5c20eeea59ec5e1230739afd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/0*vSnCCj742YwyGaLU"/></div></figure><p id="1b83" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这给了我们选择权重，或者直观地，每个2ᵈ输出的概率，在响应张量中。所以现在它简化为一个反应张量的加权和，由选择张量加权。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lc"><img src="../Images/b037bc3951a5b046f50fafe2a43d5220.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/0*VstTY7ULGPt9o8Zn"/></div></figure><p id="a7b3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">整个设置如下图所示:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es ld"><img src="../Images/dcee204fab13ba0229ba345321ad05f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*k9VLchaQmX1oYM7N"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">来自报纸[3]</figcaption></figure><h1 id="0420" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">神经遗忘决策集成</h1><p id="dea2" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">从一棵树跳到一片“森林”非常简单。如果集合中有m棵<em class="ks">树，最终的输出是m棵单独的树的连接</em></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es le"><img src="../Images/3eab62769b6d45ec578b3db30ed3713d.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/0*ovRRrDoHW3O_JxCc"/></div></figure><h1 id="5921" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">更深入地了解节点</h1><p id="557d" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">除了开发核心模块(节点层)，他们还提出了一个深度版本，在这个版本中，我们将多个节点层堆叠在彼此之上，但具有剩余连接。所有先前层的输入要素和输出被连接起来，并馈入下一个节点层，依此类推。最后，对所有层的最终输出进行平均(类似于RandomForest)。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lf"><img src="../Images/ef29b99b7920a0b1893961c224dd17a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cb_Fje3zNJUOsTpS"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">来自报纸[3]</figcaption></figure><h1 id="99c4" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">培训和实验</h1><h1 id="883b" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">数据处理</h1><p id="6181" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">在论文的所有实验中，他们使用分位数变换将每个特征变换为遵循正态分布。这一步对于稳定的训练和更快的收敛非常重要。</p><h1 id="63d7" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">初始化</h1><p id="6ee1" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">在训练网络之前，他们建议进行数据感知初始化，以获得良好的初始参数。它们统一初始化特征选择矩阵(<em class="ks"> F </em>)，而阈值(<em class="ks"> b </em>)用随机特征值<em class="ks">fᵢ(x</em>初始化。天平<em class="ks"> τᵢ </em>以这样的方式初始化，使得第一批中的所有样品都落在$latex双侧entmax的线性区域内，因此得到非零梯度。最后，用标准正态分布初始化响应张量。</p><h1 id="9dc9" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">实验和结果</h1><p id="e3d3" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">本文使用Epsilon、YearPrediction、Higgs、Microsoft、Yahoo和Click六个数据集进行了实验。他们将NODE与CatBoost、XGBoost和FCNN进行了比较。</p><p id="48aa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，他们比较了所有算法的默认超参数。节点的默认架构设置如下:深度为6的2048棵树的单层。这些参数继承自CatBoost默认参数。</p><p id="f6ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，他们调整所有的算法，然后进行比较。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lf"><img src="../Images/49bfa91551e2a1127230fa42f371267d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9Nk2GVqzL0vJtN5x"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">对照表(来源:[3])</figcaption></figure><h1 id="6567" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">代码和实现</h1><p id="c838" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">作者在PyTorch <a class="ae jc" href="https://github.com/Qwicen/node" rel="noopener ugc nofollow" target="_blank">这里</a>的一个现成模块中提供了实现。</p><p id="3a9b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它也在我发布的新库PyTorch Tabular中实现，以及其他一些用于表格数据的算法。请点击这里查看:</p><ul class=""><li id="baab" class="lg lh hh ig b ih ii il im ip li it lj ix lk jb ll lm ln lo bi translated"><a class="ae jc" href="https://pypi.org/project/pytorch-tabular/" rel="noopener ugc nofollow" target="_blank"> PyPi </a></li><li id="22c6" class="lg lh hh ig b ih lp il lq ip lr it ls ix lt jb ll lm ln lo bi translated"><a class="ae jc" href="https://github.com/manujosephv/pytorch_tabular" rel="noopener ugc nofollow" target="_blank"> Github </a></li><li id="c595" class="lg lh hh ig b ih lp il lq ip lr it ls ix lt jb ll lm ln lo bi translated"><a class="ae jc" href="https://pytorch-tabular.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">文档</a></li></ul><h1 id="3ec5" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">参考</h1><ol class=""><li id="f050" class="lg lh hh ig b ih kb il kc ip lu it lv ix lw jb lx lm ln lo bi translated">页（page的缩写）Kontschieder，M. Fiterau，A. Criminisi和S. R. Bulò，“深度神经决策森林”，<em class="ks"> 2015 IEEE计算机视觉国际会议(ICCV) </em>，第1467–1475页，doi: 10.1109/ICCV.2015.172</li><li id="65a0" class="lg lh hh ig b ih lp il lq ip lr it ls ix lt jb lx lm ln lo bi translated">Ben Peters，Vlad Niculae，André F. T. Martins，“稀疏序列到序列模型”，<em class="ks"> 2019年计算语言学协会(ACL)第57届年会论文集</em>，第1504–1519页，doi:10.18653/v1/P19–1146。</li><li id="4df9" class="lg lh hh ig b ih lp il lq ip lr it ls ix lt jb lx lm ln lo bi translated">塞尔戈·波波夫，斯坦尼斯拉夫·莫罗佐夫，阿尔特姆·巴本科，“用于表格数据深度学习的<a class="ae jc" href="https://arxiv.org/abs/1909.06312" rel="noopener ugc nofollow" target="_blank">神经不经意决策集成</a>，<a class="ae jc" href="https://deep-and-shallow.com/2021/02/25/neural-oblivious-decision-ensemblesnode-a-state-of-the-art-deep-learning-algorithm-for-tabular-data/1909.06312%20[cs.LG]" rel="noopener ugc nofollow" target="_blank"> <em class="ks"> arXiv:1909.06312 [cs .</em>LG】</a></li></ol></div><div class="ab cl ly lz go ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ha hb hc hd he"><p id="aab6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ks">原载于2021年2月25日http://deep-and-shallow.com</em><em class="ks">T21</em><a class="ae jc" href="https://deep-and-shallow.com/2021/02/25/neural-oblivious-decision-ensemblesnode-a-state-of-the-art-deep-learning-algorithm-for-tabular-data/" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>