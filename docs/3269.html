<html>
<head>
<title>Finding out Optimum Neighbours (n) number in the KNN classification using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python语言在KNN分类中寻找最佳邻域数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/finding-out-optimum-neighbours-n-number-in-the-knn-classification-using-python-9bdcfefff58c?source=collection_archive---------1-----------------------#2021-06-22">https://medium.com/analytics-vidhya/finding-out-optimum-neighbours-n-number-in-the-knn-classification-using-python-9bdcfefff58c?source=collection_archive---------1-----------------------#2021-06-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/8492fcbe916c6248a8774df8104cc676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*SjBEgwWJTXjp4VjhdtQJTw.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">来源:研究之门</figcaption></figure><p id="2334" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">机器学习中的分类算法:</strong>有几种情况我们需要利用机器学习来识别未知的物体或数据属于哪一类。这叫做分类，在这个问题中可能涉及到“m”个类别。Python中有各种类型的分类模型可用，如GuassianNB、DecisionTree、RandomForest、KNN n邻域、LogisticRegression和SVM分类器。现在，我们将讨论KNN是如何工作的，以及如何确定最佳的“n”个邻居以获得最大的精确度。</p><p id="d70b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> KNN分类器:</strong> KNN分类器属于监督分类算法，与其他分类模型相比，监督分类算法产生更好的结果。KNN既可用于分类问题，也可用于回归问题。在训练时，它试图从同一个类的例子中识别模式，并识别每个类与其他类的不同之处。然后，它将这些示例和类映射到一个图中，在类的边缘有边框。当给定一个新的对象或示例来预测它应该属于哪一类时，KNN将找到与给定示例相关联的属性，并将它们与训练中给定的各种类进行比较。如果类的属性与示例的属性匹配，则该示例属于该类。</p><p id="9a83" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">例如，我们把KNN分为会飞和不会飞两类。现在我们需要确定鸡属于哪一类。乍一看，其他分类器会把鸡归类为飞行动物是由于它们的翅膀。但是KNN将鸡肉的所有特性与给定的类别进行比较。在这种情况下，鸡将被放置在飞行和非飞行动物的边缘，因为它有翅膀，但不能用翅膀飞行。现在，计算图中鸡肉和其他相邻样本之间的高斯距离。如果鸡与一个类的更多邻居匹配，意味着鸡属于那个类。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es jn"><img src="../Images/2f619780c163f0d45ed9dadc2e633adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*0Pqqx6wGDfFm_7GLebg2Hw.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">来源:kdnuggets</figcaption></figure><p id="d20e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">如何找出最佳的“n”个邻居:</strong>基于KNN分类器中使用的“n”个邻居，相同的例子可能属于不同的类别。那么，如何计算出最佳的“n”数以获得更高的精度和更低的错误率呢？为了回答这个问题，我们需要画一个错误率和K值的关系图。让我们看看如何使用Python绘制这个图形。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="5cbf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">导入StandardScaler和KNeighborsClassifier，并转换训练和测试数据集。我们还需要Matplotlib来绘制图形。您可以使用<strong class="ir hi"> pip </strong>来安装这些软件包。(别忘了导入matplotlib。我已经在前面的步骤中将其导入为<strong class="ir hi"> plt </strong></p><pre class="jo jp jq jr fd jz ka kb kc aw kd bi"><span id="8be5" class="ke kf hh ka b fi kg kh l ki kj"><em class="kk">#feature Scaling  </em><br/><strong class="ka hi">from</strong> <strong class="ka hi">sklearn.preprocessing</strong> <strong class="ka hi">import</strong> StandardScaler<br/><strong class="ka hi">from</strong> <strong class="ka hi">sklearn.neighbors</strong> <strong class="ka hi">import</strong> KNeighborsClassifier<br/><br/>st_x= StandardScaler()    <br/>x_train= st_x.fit_transform(X_train)    <br/>x_test= st_x.transform(X_test)</span></pre><p id="d3c6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在找出不同“n”个邻居的不同错误率。为此，您可以使用range函数。我已经找到了错误率，并将它们存储在1到40个邻居的数组中。</p><pre class="jo jp jq jr fd jz ka kb kc aw kd bi"><span id="2c7c" class="ke kf hh ka b fi kg kh l ki kj">error_rate = []<br/><strong class="ka hi">for</strong> i <strong class="ka hi">in</strong> range(1,40):<br/> knn = KNeighborsClassifier(n_neighbors=i)<br/> knn.fit(x_train,y_train)<br/> pred_i = knn.predict(x_test)<br/> error_rate.append(np.mean(pred_i != y_test))</span></pre><p id="8381" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们需要将这些错误率与邻居值进行对比，以查看哪“n”个邻居的错误率较低。为此，我们可以使用matplotlib包。为了快速得到结果，我还在图上打印了最低的错误率和相应的“n”邻居值。</p><pre class="jo jp jq jr fd jz ka kb kc aw kd bi"><span id="ebb7" class="ke kf hh ka b fi kg kh l ki kj">plt.figure(figsize=(10,6))<br/>plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed',marker='o',markerfacecolor='red', markersize=10)<br/>plt.title('Error Rate vs. K Value')<br/>plt.xlabel('K')<br/>plt.ylabel('Error Rate')<br/>req_k_value = error_rate.index(min(error_rate))+1<br/>print("Minimum error:-",min(error_rate),"at K =",req_k_value)</span></pre><p id="9a55" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该图类似于下图。因此，为了获得最佳精度，我们需要对KNN分类器使用“n=10”。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es kl"><img src="../Images/bb6449de44a963546e21d5899aa66e83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*4DS32DovsmR2k08tiCerZw.png"/></div></figure><p id="5549" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我将使用n=10个邻居的数据集来训练KNN分类器，看看我获得了多少准确度。我已经将模型保存到<strong class="ir hi"> y_pred </strong>中</p><pre class="jo jp jq jr fd jz ka kb kc aw kd bi"><span id="e68f" class="ke kf hh ka b fi kg kh l ki kj"><em class="kk">#Fitting K-NN classifier to the training set  </em><br/><strong class="ka hi">from</strong> <strong class="ka hi">sklearn.neighbors</strong> <strong class="ka hi">import</strong> KNeighborsClassifier  <br/>classifier= KNeighborsClassifier(n_neighbors=req_k_value, metric='minkowski', p=2 )  <br/>classifier.fit(x_train, y_train) <br/>y_pred= classifier.predict(x_test)</span></pre><p id="72b3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我将预测测试数据集的值，同时我还将计算我们训练的分类器的准确度和混淆矩阵。</p><pre class="jo jp jq jr fd jz ka kb kc aw kd bi"><span id="9ed9" class="ke kf hh ka b fi kg kh l ki kj"><em class="kk">#Creating the Confusion matrix  </em><br/><strong class="ka hi">from</strong> <strong class="ka hi">sklearn.metrics</strong> <strong class="ka hi">import</strong> confusion_matrix  <br/>print("KNN - 10 neighbors Accuracy score and Confusion matrix")<br/>print(accuracy_score(y_pred,y_test))<br/>print(confusion_matrix(y_test, y_pred))<br/>print("-------------------------")</span></pre><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es km"><img src="../Images/070985b6cfff03edd60890d5bc60abfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*RNnJTP1M_iN_3sQJBVyIYw.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">n=10个邻居的精度为88.52</figcaption></figure><p id="d24d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们使用matplotlib来美化混淆矩阵。</p><pre class="jo jp jq jr fd jz ka kb kc aw kd bi"><span id="cd39" class="ke kf hh ka b fi kg kh l ki kj">plot_confusion_matrix(classifier, x_test, y_test)<br/>plt.show()</span></pre><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es kn"><img src="../Images/046d1453a60b5986d2242448fe785775.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*zIkO-LObUUZG1QJO-5VzzA.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">具有n=10(最佳)邻居的KNN的混淆矩阵</figcaption></figure><p id="ab55" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">结论:</strong>通过使用上述步骤，我们将能够使用最优的‘n’个邻居来以低错误率获得训练数据集的最大准确度。然而，在某些情况下，由于训练数据可能非常小，因此由于过拟合和欠拟合，精度可能很高。因此，为了安全起见，请计算召回率、精确度、F值、特异性、灵敏度、ROC分数，并绘制ROC曲线。我将在接下来的文章中讨论如何计算这些。</p><p id="7698" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">敬请关注😉</strong></p><p id="0802" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">注意:</strong>如果这对你学习有所帮助，请点击claps。请在评论中告诉我你对这篇文章的看法。</p></div></div>    
</body>
</html>