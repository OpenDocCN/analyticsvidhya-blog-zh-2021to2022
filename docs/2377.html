<html>
<head>
<title>How to import/scrape Data from CNN’s RSS Feed?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何从CNN的RSS源导入/抓取数据？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-import-data-from-cnns-rss-feed-95c828e446e?source=collection_archive---------18-----------------------#2021-04-20">https://medium.com/analytics-vidhya/how-to-import-data-from-cnns-rss-feed-95c828e446e?source=collection_archive---------18-----------------------#2021-04-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/7de65fda3010cc7f20046af1296e7cc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PDEUCT73EXfAUQSAxFUawg.jpeg"/></div></div></figure><p id="0f20" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">从任何RSS源导入数据非常容易，但是对于这个实现，我们将关注CNN的RSS源数据。在这个示例中，我们使用政治数据的RSS提要。所以让我们开始吧。</p><p id="f9d3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们首先需要导入几个基本库，如<strong class="ir hi">请求</strong>、<strong class="ir hi">美汤、</strong>和<strong class="ir hi">熊猫</strong>。我们将使用pandas将数据保存到数据帧或<strong class="ir hi"> CSV </strong></p><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="64c8" class="jw jx hh js b fi jy jz l ka kb">import requests<br/>from bs4 import BeautifulSoup<br/>import pandas as pd</span></pre><p id="7413" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将在这里定义两个不同的函数。<strong class="ir hi">使用CNN _ news _ scraper</strong>功能是为了用美汤刮数据。我们使用<code class="du kc kd ke js b">features='xml'</code>来解析来自RSS提要的<strong class="ir hi"> XML </strong></p><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="dc26" class="jw jx hh js b fi jy jz l ka kb"><strong class="js hi">#</strong> Function to get the various attributes of the article<br/>def getArticles(articles):<br/>    all_articles = []<br/>    for article in articles:<br/>        article_title = article.find('title').text<br/>        article_link = article.find('link').text<br/>        article_published = article.find('pubDate').text<br/>        all_articles.append({<br/>            'title':article_title,<br/>            'link':article_link,<br/>            'published':article_published<br/>        })<br/>    return all_articles<br/>    <br/># Function to invoke CNN Scrapper<br/>def cnn_news_scrapper(URL):<br/>    try:<br/>        r = requests.get(URL)<br/>        soupContent = BeautifulSoup(r.content,features='xml')<br/>        print('Job Succeeded returning Status Code: ', r.status_code)<br/>        items = soupContent.findAll('item')<br/>        print('Total News Content')<br/>        print(len(items))<br/>        return getArticles(soupContent.findAll('item'))<br/>    except Exception as e:<br/>        print('Scraping failed due to the below exception')<br/>        print(e)</span></pre><h2 id="f0c3" class="jw jx hh bd kf kg kh ki kj kk kl km kn ja ko kp kq je kr ks kt ji ku kv kw kx bi translated">报废流程</h2><p id="598a" class="pw-post-body-paragraph ip iq hh ir b is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji lc jk jl jm ha bi translated">我们将使用上述函数开始刮擦过程。我们将使用下面提到的URL <code class="du kc kd ke js b"><a class="ae ld" href="http://rss.cnn.com/rss/cnn_allpolitics.rss" rel="noopener ugc nofollow" target="_blank">http://rss.cnn.com/rss/cnn_allpolitics.rss</a></code>调用它</p><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="415e" class="jw jx hh js b fi jy jz l ka kb">print('Starting scraping')<br/>data = cnn_news_scrapper('http://rss.cnn.com/rss/cnn_allpolitics.rss')<br/>print('Finished scraping')</span><span id="6be4" class="jw jx hh js b fi le jz l ka kb">Starting scraping<br/>Job Succeeded returning Status Code:  200<br/>Total News Content<br/>30<br/>Finished scraping</span></pre><p id="a87b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">将数据加载到数据帧中很容易。我们只需要获得变量<code class="du kc kd ke js b">data</code>，并将其输入到创建新数据帧的函数中，例如<code class="du kc kd ke js b">pd.DataFrame(data)</code></p><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="9123" class="jw jx hh js b fi jy jz l ka kb">df = pd.DataFrame(data)</span><span id="b747" class="jw jx hh js b fi le jz l ka kb">df.head()</span></pre><figure class="jn jo jp jq fd ii er es paragraph-image"><div class="ab fe cl lf"><img src="../Images/f36121d0d1ca5f9424186ce8a8627962.png" data-original-src="https://miro.medium.com/v2/format:webp/1*maijHtytYso6XKundy3wKA.png"/></div></figure><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="d2f1" class="jw jx hh js b fi jy jz l ka kb"># Persist data as CSV<br/><br/># df.to_csv('cnn_political_news.csv', index=False)</span></pre><h1 id="3484" class="lg jx hh bd kf lh li lj kj lk ll lm kn ln lo lp kq lq lr ls kt lt lu lv kw lw bi translated">结论</h1><p id="92d9" class="pw-post-body-paragraph ip iq hh ir b is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji lc jk jl jm ha bi translated">如您所见，我们使用BeautifulSoup从RSS提要中提取数据的方式与BeautifulSoup的任何其他抓取实现非常相似。它也很容易实现和使用。</p><h1 id="9488" class="lg jx hh bd kf lh li lj kj lk ll lm kn ln lo lp kq lq lr ls kt lt lu lv kw lw bi translated">参考</h1><p id="a6c8" class="pw-post-body-paragraph ip iq hh ir b is ky iu iv iw kz iy iz ja la jc jd je lb jg jh ji lc jk jl jm ha bi translated">[1] <a class="ae ld" href="https://github.com/shivkumarganesh/PythonScrapers/blob/main/CNN%20RSS%20Scraper.ipynb" rel="noopener ugc nofollow" target="_blank">源代码可以在Github </a>中找到</p></div></div>    
</body>
</html>