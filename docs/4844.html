<html>
<head>
<title>Spark Session and the singleton misconception</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">火花会议和单例误解</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/spark-session-and-the-singleton-misconception-1aa0eb06535a?source=collection_archive---------1-----------------------#2022-03-30">https://medium.com/analytics-vidhya/spark-session-and-the-singleton-misconception-1aa0eb06535a?source=collection_archive---------1-----------------------#2022-03-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="fe30" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">关于Spark会话，结构化流揭示了什么</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/87784f6864646b99b0e1d40b856d2830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lp1lHEfJ0_Z8yWXPF3ZZGw.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">照片由pix abay:<a class="ae jm" href="https://www.pexels.com/photo/yellow-and-black-butterflies-cocoon-39862/" rel="noopener ugc nofollow" target="_blank">https://www . pexels . com/photo/黄黑蝴蝶-cocoon-39862/ </a></figcaption></figure><h1 id="6112" class="jn jo hh bd jp jq jr js jt ju jv jw jx in jy io jz iq ka ir kb it kc iu kd ke bi translated">为什么要在乎呢？</h1><p id="6b7c" class="pw-post-body-paragraph kf kg hh kh b ki kj ii kk kl km il kn ko kp kq kr ks kt ku kv kw kx ky kz la ha bi translated">在过去的Spark中，Spark应用程序的入口点是<code class="du lb lc ld le b">SparkContext</code>，根据Spark源代码注释，它“<em class="lf">表示到Spark集群的连接，可用于在该集群上创建rdd、累加器和广播变量</em>。从Spark 2.0开始，引入了一个新的入口点，叫做<code class="du lb lc ld le b">SparkSession</code>。Spark会话是访问大多数spark功能的首选方式，特别是关注SQL API之类的高级API，而不是低级RDD API。</p><p id="f15a" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">乍一看，Spark session听起来像是一个单例，似乎整个应用程序只有一个实例，不管你如何获取指向那个会话的指针。有时你只是使用<code class="du lb lc ld le b">spark</code>变量，有时你从一个数据帧变量中得到它，但在所有情况下都是一样的。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ll"><img src="../Images/6e0563526fd1687b6b6cf345845f475c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O_tIdi20CIjLcFPWJYTHLw.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">作者图片</figcaption></figure><p id="4818" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">从上面的截图可以看出，无论是使用著名的<code class="du lb lc ld le b">spark</code>变量还是从数据帧中获取，变量address和<code class="du lb lc ld le b">sessionUUID</code>属性都是相同的。</p><p id="1f1a" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">你认为这种模式是普遍的吗？或者换句话说，Spark session是单一的吗？</p><p id="d6ba" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">很长一段时间，我认为是的，它是一个单一的，在大多数情况下，你甚至不关心它的性质。但我最近遇到了一个案例，证明我是错的，那就是Spark结构化流。</p></div><div class="ab cl lm ln go lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ha hb hc hd he"><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lt"><img src="../Images/ac268a5aa06b283dd85dcd7fb4505bcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/0*vzUGDS9ZjS9-vv8I.jpg"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated"><a class="ae jm" href="https://imgflip.com/i/6a6mit" rel="noopener ugc nofollow" target="_blank">https://imgflip.com/i/6a6mit</a></figcaption></figure><p id="8dfa" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">让我们试试这个片段，它使用<code class="du lb lc ld le b">foreachBatch</code>在流微批处理上运行批处理操作。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="lu lv l"/></div></figure><p id="75d4" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">这是结果。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lw"><img src="../Images/d6360f9be321fe1752dadc5469877174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q_BYhp7Kh2_VjbxUicrd4A.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">作者图片</figcaption></figure><p id="0f5e" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">全局<code class="du lb lc ld le b">spark</code>变量和连接到微批处理数据帧的Spark会话之间的会话存储器地址和UUID属性不同。但是为什么会发生这种情况，以及它会如何影响您的代码呢？</p><h1 id="947a" class="jn jo hh bd jp jq jr js jt ju jv jw jx in jy io jz iq ka ir kb it kc iu kd ke bi translated">先问为什么！</h1><p id="dda7" class="pw-post-body-paragraph kf kg hh kh b ki kj ii kk kl km il kn ko kp kq kr ks kt ku kv kw kx ky kz la ha bi translated">在对源代码(我使用的Spark版本是3.2.1)进行了一番挖掘之后，我在一个名为<code class="du lb lc ld le b">StreamExecution.scala</code>的文件中找到了<a class="ae jm" href="https://github.com/apache/spark/blob/v3.2.1/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala#L196" rel="noopener ugc nofollow" target="_blank">这一行</a>。</p><pre class="ix iy iz ja fd lx le ly lz aw ma bi"><span id="a56b" class="mb jo hh le b fi mc md l me mf">/** Isolated spark session to run the batches with. */<br/>private val sparkSessionForStream = sparkSession.cloneSession()</span></pre><p id="e44f" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated"><strong class="kh hi">提示:</strong>如果你使用Databricks，那么你会发现它们的Spark实现与GitHub上的开源Spark repo并不完全相同。</p><p id="6336" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">我首先检查了<code class="du lb lc ld le b">SparkSession</code>类，看看它是如何构造的，然后发现了一个<code class="du lb lc ld le b">cloneSession</code>方法，它似乎是从一个位置调用的，并且是流执行文件。这里的克隆意味着运行流代码的Spark SQL API代码将有一个独立的会话。该会话将从根Spark会话继承状态，但它将独立工作，因为它需要实施一些可能不同于根会话配置的行为。</p><p id="6821" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">例如，似乎必须对流式处理禁用自适应查询执行。</p><pre class="ix iy iz ja fd lx le ly lz aw ma bi"><span id="548c" class="mb jo hh le b fi mc md l me mf">// Adaptive execution can change num shuffle partitions, disallow        sparkSessionForStream.conf.set(SQLConf.ADAPTIVE_EXECUTION_ENABLED.key, "false")        </span><span id="b901" class="mb jo hh le b fi mg md l me mf">// Disable cost-based join optimization as we do not want stateful operations        <br/>// to be rearranged        sparkSessionForStream.conf.set(SQLConf.CBO_ENABLED.key, "false")</span></pre><p id="4269" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">很好，现在我们知道每个应用程序可以有多个会话，并且每个会话可以有不同的配置。这用于结构化流API，但也可用于用户应用程序代码，并且有两个公共API(new session和cloneSession)允许您创建新会话。多个会话可以在映射到单个SparkContext的同一个Spark应用程序中愉快地生活。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mh"><img src="../Images/e83b81b6a2992f3a11239cc1de8eb08e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rb0nYbESKC1uNyiT2IaZJg.png"/></div></div></figure><p id="8150" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">上面显示的两个会话有不同的内存地址，但是它们绑定到同一个SparkContext。它们仍可用于创建数据帧，一般来说，其行为类似于根<code class="du lb lc ld le b">spark</code>会话，尽管如果需要，它们可以有不同的配置设置。</p><h1 id="1509" class="jn jo hh bd jp jq jr js jt ju jv jw jx in jy io jz iq ka ir kb it kc iu kd ke bi translated">但是我为什么需要关心呢？</h1><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mi"><img src="../Images/cbde7dde5f43815ab7e50a35acf44f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/0*9lc4NWCmK40_BPe0.jpg"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated"><a class="ae jm" href="https://imgflip.com/i/6a6oc3" rel="noopener ugc nofollow" target="_blank">https://imgflip.com/i/6a6oc3</a></figcaption></figure><p id="fa03" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">嗯，有时候你可能需要关心。首先，知道一些新的东西，在你的工具箱里有一个新的工具，或者知道事情如何在引擎盖下工作是很好的。如果这还不够，让我们看看这个使用PySpark编写的案例，原因我稍后会提到。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="lu lv l"/></div></figure><p id="2abd" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">上述操作将失败，并出现错误<code class="du lb lc ld le b">Table or view not found: microBatch</code>。在上面的代码片段中，假设我们需要使用SQL语句来运行一些逻辑，例如MERGE语句。是的，它可以使用函数式API来完成，但有时编写SQL代码更具可读性。</p><p id="44e2" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">在PySpark中，至少在3.2.1版之前，DataFrame实例上没有<code class="du lb lc ld le b">sparkSession</code>属性(顺便说一下，Scala有)。所以另一个选择是使用<code class="du lb lc ld le b">df.sql_ctx</code>属性，它可以指向一个Spark会话，但不幸的是，它似乎指向了根Spark会话。这就是我们得到该错误消息的原因，因为<code class="du lb lc ld le b">microBatch</code>临时视图是在流微批处理的会话上注册的，而第3行的SQL语句是针对根Spark会话运行的，因此该视图在那里是不可见的。</p><p id="ce87" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">如果你在谷歌上搜索"<strong class="kh hi">spark streaming for each batch createOrReplaceTempView</strong>":你可能会从Databricks网站上得到一个带有<a class="ae jm" href="https://docs.databricks.com/_static/notebooks/merge-in-streaming.html" rel="noopener ugc nofollow" target="_blank">这样代码的笔记本</a>。</p><pre class="ix iy iz ja fd lx le ly lz aw ma bi"><span id="593f" class="mb jo hh le b fi mc md l me mf"># NOTE: You have to use the SparkSession that has been used to define the `updates` dataframe<br/>  <br/>microBatchOutputDF._jdf.sparkSession().sql("""<br/>    MERGE INTO aggregates t<br/>    USING updates s<br/>    ON s.key = t.key<br/>    WHEN MATCHED THEN UPDATE SET *<br/>    WHEN NOT MATCHED THEN INSERT *<br/>  """)</span></pre><p id="88c6" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">嗯，这通过使用<code class="du lb lc ld le b">microBatchOutputDF._jdf.sparkSession()</code>方法解决了问题。评论中提到使用<strong class="kh hi"> SparkSession </strong>来定义<code class="du lb lc ld le b">updates</code> DataFrame，但是它非常模糊。另外，在Python中使用以下划线开头的方法不是很习惯/推荐，因为它们被认为是私有的。</p><p id="faf5" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">实际上，如果你检查<a class="ae jm" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html" rel="noopener ugc nofollow" target="_blank">数据帧</a>的PySpark文档，你不会找到一个叫做<code class="du lb lc ld le b">_jdf</code>的公共属性，也不会找到一个<code class="du lb lc ld le b">sql_ctx</code>。我仍然更喜欢<code class="du lb lc ld le b">sql_ctx</code>方法，因为它以一种火花自然的方式表现。如果您从SQL语句中收集一些记录，您将得到一个由<code class="du lb lc ld le b">Row</code>元素组成的数组。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mj"><img src="../Images/724d2bfd35ee2ce12558f0714afff2ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Rmg1mvDZiR_hm8ROD2k_g.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">作者图片</figcaption></figure><p id="7886" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">使用<code class="du lb lc ld le b">_jdf</code>做同样的事情会产生Java对象，这很奇怪，或者至少不太自然，如果你使用类型注释并使用像VS代码这样的IDE开发的话。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mk"><img src="../Images/c69216a8e78e3880e726112e7e94f227.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F5a_14zAEQNlmioGBLkymw.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">作者图片</figcaption></figure><h1 id="a97d" class="jn jo hh bd jp jq jr js jt ju jv jw jx in jy io jz iq ka ir kb it kc iu kd ke bi translated">短期解决方案</h1><p id="9df8" class="pw-post-body-paragraph kf kg hh kh b ki kj ii kk kl km il kn ko kp kq kr ks kt ku kv kw kx ky kz la ha bi translated">回到我们的流问题，针对正确的Spark会话运行SQL语句的解决方案是什么？</p><p id="40e4" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">最简单的解决方案是使用全局临时视图。只要名称在SQL语句中带有前缀<code class="du lb lc ld le b">global_temp.</code>，就可以从任何Spark会话中访问该视图。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="lu lv l"/></div></figure><p id="db21" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">这里的主要区别在第2行和第3行。该视图在第2行被创建为全局临时视图。在第3行，使用根Spark会话访问视图，但是必须加上前缀<code class="du lb lc ld le b">global_temp.</code>。一旦这样做了，事情就不会有任何错误了。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es et"><img src="../Images/7244e8d25bc35df81728cf576cb9fbfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HD40pv7XjcY9LV5dnqDzuA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">作者图片</figcaption></figure><p id="959e" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">使用<code class="du lb lc ld le b">sql_ctx</code>并不是真正的最佳选择，但它比<code class="du lb lc ld le b">_jdf</code>方法要好得多。</p><h1 id="e5e9" class="jn jo hh bd jp jq jr js jt ju jv jw jx in jy io jz iq ka ir kb it kc iu kd ke bi translated">长期解决方案</h1><p id="684c" class="pw-post-body-paragraph kf kg hh kh b ki kj ii kk kl km il kn ko kp kq kr ks kt ku kv kw kx ky kz la ha bi translated">Spark背后的社区意识到了<code class="du lb lc ld le b">DataFrame</code>的PySpark接口中的这些限制，以及当它没有真正匹配Scala等效API(具有到Spark会话的简单接口)时，它如何使开发人员生活艰难。这就是为什么有一个长期的解决方案，但它可能会伴随着下一个(希望是次要的)Spark版本。</p><p id="71e3" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">有一个名为<a class="ae jm" href="https://issues.apache.org/jira/browse/SPARK-38121" rel="noopener ugc nofollow" target="_blank">的JIRA问题，在PySpark </a>中使用SparkSession而不是SQLContext。本期有一个最近<a class="ae jm" href="https://github.com/apache/spark/pull/35410" rel="noopener ugc nofollow" target="_blank">合并的PR </a>，描述如下:</p><blockquote class="ml mm mn"><p id="edcf" class="kf kg lf kh b ki lg ii kk kl lh il kn mo li kq kr mp lj ku kv mq lk ky kz la ha bi translated">本公关提议<code class="du lb lc ld le b">SparkSession</code>在PySpark内部。这是尊重运行时配置等的基础工作。目前，我们在内部依赖旧的不推荐的<code class="du lb lc ld le b">SQLContext</code>，它不能正确地尊重Spark会话的运行时配置。</p><p id="9aba" class="kf kg lf kh b ki lg ii kk kl lh il kn mo li kq kr mp lj ku kv mq lk ky kz la ha bi translated">该公关还包含相关变更(以及该公关涉及的代码中的一点重构)，如下所示:</p><p id="8220" class="kf kg lf kh b ki lg ii kk kl lh il kn mo li kq kr mp lj ku kv mq lk ky kz la ha bi translated">-像Scala API一样公开<code class="du lb lc ld le b">DataFrame.sparkSession</code>。</p><p id="d854" class="kf kg lf kh b ki lg ii kk kl lh il kn mo li kq kr mp lj ku kv mq lk ky kz la ha bi translated">-移动<code class="du lb lc ld le b">SQLContext._conf</code>-&gt;-<code class="du lb lc ld le b">SparkSession._jconf</code>。</p><p id="e27c" class="kf kg lf kh b ki lg ii kk kl lh il kn mo li kq kr mp lj ku kv mq lk ky kz la ha bi translated">-在<code class="du lb lc ld le b">DataFrame.randomSplit</code>处将<code class="du lb lc ld le b">rdd_array</code>重命名为<code class="du lb lc ld le b">df_array</code>。</p><p id="32b8" class="kf kg lf kh b ki lg ii kk kl lh il kn mo li kq kr mp lj ku kv mq lk ky kz la ha bi translated">-发出警告，劝阻使用<code class="du lb lc ld le b">DataFrame.sql_ctx</code>和<code class="du lb lc ld le b">DataFrame(..., sql_ctx)</code>。</p></blockquote><p id="687d" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">一旦这个PR在你使用的Spark安装上可用，更新代码和移除对<code class="du lb lc ld le b">sql_ctx</code>的依赖就很容易了，而且我们不再需要使用全局临时视图了。下面将顺利工作(手指交叉)。</p><pre class="ix iy iz ja fd lx le ly lz aw ma bi"><span id="9d14" class="mb jo hh le b fi mc md l me mf">df.createOrReplaceTempView("microBatch")  <br/>count = df.sparkSession.sql("select * from microBatch").count()</span></pre><p id="e854" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated"><strong class="kh hi">2022年6月16日更新:</strong></p><p id="8206" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">Spark 3.3已经在Databricks runtime 11.0 beta上发布。因此PySpark数据帧有一个<code class="du lb lc ld le b">SparkSession</code>属性。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mr"><img src="../Images/17971db93ac1aa771aafbdb3a563d813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JUnnE_OVGQY2IuYWjYCl7A.png"/></div></div></figure><h1 id="a019" class="jn jo hh bd jp jq jr js jt ju jv jw jx in jy io jz iq ka ir kb it kc iu kd ke bi translated">这是所有的乡亲</h1><p id="eb02" class="pw-post-body-paragraph kf kg hh kh b ki kj ii kk kl km il kn ko kp kq kr ks kt ku kv kw kx ky kz la ha bi translated">这可能不是一个非常常见的用例，或者对您编写的应用程序产生巨大影响的东西。尽管如此，多了解一些关于Spark session的知识以及它在不同情况下的表现是非常有用的。它可能有助于诊断一个愚蠢的问题，甚至给你一个想法来写一个困难任务的创新解决方案。</p><p id="8635" class="pw-post-body-paragraph kf kg hh kh b ki lg ii kk kl lh il kn ko li kq kr ks lj ku kv kw lk ky kz la ha bi translated">希望有所帮助！🙂</p></div></div>    
</body>
</html>