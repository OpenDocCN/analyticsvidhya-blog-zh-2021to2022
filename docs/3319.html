<html>
<head>
<title>Reinforcement learning use in Healthcare Supply Chains</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习在医疗供应链中的应用</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-use-in-supply-chains-d6d7a337e926?source=collection_archive---------14-----------------------#2021-06-24">https://medium.com/analytics-vidhya/reinforcement-learning-use-in-supply-chains-d6d7a337e926?source=collection_archive---------14-----------------------#2021-06-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="206f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">强化学习(RL)是机器学习的一个领域，它将复杂问题的解决方案建模为环境中发生的代理行为，并试图最大化他们随着时间的推移积累的回报。因此，RL感兴趣的是在潜在的复杂环境中优化这些智能体的顺序决策。本文以医院仓库为例，开发了一个使用RL的模型。</em></p><h1 id="e196" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">什么是强化学习？</h1><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/6db607f41b451b6a39914e800415c7af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLhHRswhjc1zSZRSR-x7zQ.png"/></div></div></figure><p id="95fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi ko translated"><span class="l kp kq kr bm ks kt ku kv kw di"> A </span>长期以来，强化学习(reinforcement learning)伴随着监督学习和非监督学习，是三种基本的机器学习范式之一。我<a class="ae kx" href="https://www.sciencedirect.com/science/article/pii/S0004370221000862" rel="noopener ugc nofollow" target="_blank">在最近的这篇论文</a> (2021年)中，Deepmind的科学家甚至提出<strong class="ih hj">奖励最大化和试错经验足以发展出展现与智力相关的那种能力的行为</strong>。由此，他们得出结论，强化学习(基于回报最大化的人工智能的一个分支)可以导致人工智能的发展。</p><p id="1c50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理人使用试错法来发现一个解决问题的方法，该方法根据它所做的决定来最大化奖励和最小化它所获得的惩罚。</p><p id="57f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">测试环境和代理的设计者定义了奖励政策，这是游戏规则，但是在模型中没有给出关于如何解决问题的提示或暗示。代理人将最大化报酬，首先通过执行随机试验，逐渐发展到使用复杂的策略。</p><h1 id="9a54" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">强化学习模型由哪些部分组成？</h1><p id="5f2d" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">强化学习模型由与环境交互并从环境中学习的代理形成。交互是通过动作进行的，而学习是通过代理从环境中接收的状态和奖励进行的。下图显示了这种关系。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ld"><img src="../Images/e225b7ff55d1ef95350c5761e6cb235c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z4yMUzrqcWvXlKyYrUVbgQ.jpeg"/></div></figure><h1 id="4cd9" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">模型描述</h1><p id="49fb" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">为了探索强化学习在医疗供应链中的应用，我用Python实现了一个模型，并在这里一步一步地开发。</p><h1 id="8afe" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">代理人</h1><p id="c69d" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">该模型由单个医院仓库组成，服从随机客户需求，例如，值a和b，U[a，b]之间的均匀分布。</p><p id="0055" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">仓库是该模型中的决策代理，决定是否订购额外的供应品或是否将供应品交付给客户。</em>T13】</strong></p><h1 id="ce17" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">报酬</h1><p id="7179" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">这个仓库的目标是最大化回报，对于医院来说，这是它从一次分娩中获得的内部收入，也就是收入减去费用。</p><p id="19ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该仓库获得的回报是总收入，包括</p><p id="2137" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a.-所售商品的销售价格</p><p id="5dc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b.-减去存货的持有成本</p><p id="d215" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">c.-减去未满足客户需求的罚金</p><h1 id="3266" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">行动</h1><p id="3cbf" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">仓库持有库存(无最大数量，持有全部数量)，可以采取两种措施:</p><p id="5b7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a.-向客户出售库存，或</p><p id="6847" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b.-从供应商处接收库存</p><p id="944a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">仓库一次只能做其中一个动作。</em></p><h1 id="10ca" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">状态</h1><p id="3c78" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">模型的状态是仓库库存水平</p><h1 id="dd15" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">必需的库</h1><p id="2441" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">我们用python实现了这一点。首先，我们确定需要的库。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="le lf l"/></div></figure><h1 id="4e24" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">模型参数</h1><p id="6217" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">该模型具有成本和价格参数，这些参数可以在模型开始时定义，并且在模型执行时不会发生实质性的变化。</p><p id="1dc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">仓库有多少个状态，就有多少个库存单位</em>。仓库能够容纳的单位数量介于0和max_state(仓库大小)之间。另外，初始状态(初始仓库库存)将是一个介于0和max_state之间的随机数。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="le lf l"/></div></figure><p id="197d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">r矩阵，“奖励结构矩阵”定义为大小为(max_state x max_state)的方阵。它包含奖励，因此也定义了可能采取的行动。首先我们初始化R矩阵，然后用奖励值填充它——矩阵的X和Y坐标对应于决策的初始值和最终值。</p><p id="8d1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，[20，45]处的R矩阵值表示状态从20移动到45，这意味着有25个库存项目被添加到仓库中。因此，报酬是负的，相当于期初购买25种新存货的成本加上维护存货的成本。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="le lf l"/></div></figure><p id="9384" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Q矩阵描述了一个代理人在某一行为后从给定状态得到的总的未来报酬，它与R矩阵具有相同的维数。它在创建时用随机数初始化。此外，学习和折扣过程也需要参数。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="le lf l"/></div></figure><p id="1f46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在开始定义所使用的函数。首先是一个函数，用于确定可以从中选择下一个动作的可用动作。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="le lf l"/></div></figure><p id="5b09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们定义一个函数，它在可用的动作范围内随机选择要执行的动作。该功能定义了未来的操作。有一系列选项，基本上是:</p><ol class=""><li id="60b5" class="lg lh hi ih b ii ij im in iq li iu lj iy lk jc ll lm ln lo bi translated">总是选择具有当前状态(利用)的最大Q值的动作</li><li id="2583" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">始终从当前状态的可用操作中选择一个随机操作(浏览)</li><li id="91d9" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">以一定的概率(可能随时间而变化)在利用和探索之间交替</li></ol><p id="0b65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一般建议是<em class="jd"> </em> <strong class="ih hj"> <em class="jd">大部分时间开始勘探，逐渐减少勘探，以最大化开采</em>。</strong></p><p id="7bb3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为此，我们使用衰减ε贪婪方法，其中ε是选择随机行为的概率。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="le lf l"/></div></figure><p id="e750" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下函数使用需要以下参数的公式更新Q表:</p><ul class=""><li id="bf0e" class="lg lh hi ih b ii ij im in iq li iu lj iy lk jc lu lm ln lo bi translated">q[当前状态，动作] =要更新的值。在这个模型中，动作是代理决定采取的未来状态值。</li><li id="2a04" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc lu lm ln lo bi translated">learning_rate =介于0和1之间的值，表示新信息覆盖旧信息的程度。</li><li id="055f" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc lu lm ln lo bi translated">r[当前状态，动作] =从当前状态转换到未来状态时获得的奖励=动作</li><li id="2946" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc lu lm ln lo bi translated">Q的最大值a[未来状态，a] =未来状态中可能动作a的最大Q值</li></ul><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lv"><img src="../Images/18ca7a218b4f5f51aa80b508494ed069.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o9YKm82oHSBgC4Tsc0mqTg.jpeg"/></div></div></figure><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="le lf l"/></div></figure><h1 id="f8ee" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">模拟训练</h1><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="le lf l"/></div></figure><h1 id="9604" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">标绘结果</h1><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="le lf l"/></div></figure><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lw"><img src="../Images/c02ce551eef1cc27b1a9a9a62d6c1493.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NMjCugx7E23LxF_Z5OO17g.png"/></div></div></figure><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="le lf l"/></div></figure><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lx"><img src="../Images/3fcf135c9923a8a39ac2b3c41eba2363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*jc7i-udrbTUPOI788JRaIQ.png"/></div></figure></div></div>    
</body>
</html>