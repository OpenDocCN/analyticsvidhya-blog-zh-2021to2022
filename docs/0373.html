<html>
<head>
<title>Choosing the right parameters for pre-training BERT using TPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TPU为预训练BERT选择正确的参数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/choosing-the-right-parameters-for-pre-training-bert-using-tpu-4584a598ca50?source=collection_archive---------3-----------------------#2021-01-14">https://medium.com/analytics-vidhya/choosing-the-right-parameters-for-pre-training-bert-using-tpu-4584a598ca50?source=collection_archive---------3-----------------------#2021-01-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/51d772691f194a25e156d5b77364a5ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0HbTDvL0CrOkJRacTu-a0Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:<a class="ae iu" href="https://346034.smushcdn.com/236617/wp-content/uploads/2017/03/Are-You-Giving-Your-Customers-Too-Many-Options-e1490278059717.jpg?lossy=0&amp;strip=1&amp;webp=1" rel="noopener ugc nofollow" target="_blank">谷歌搜索</a></figcaption></figure><p id="bb8a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">预先训练一个BERT模型并不容易，很多文章都给出了关于BERT是什么以及它能做的惊人事情的高层次概述，或者深入讨论了一个非常小的实现细节。这使得像我不久前一样有抱负的数据科学家经常看着笔记本，心想“它看起来很棒，很有效，但为什么作者选择了这个批量大小的数字或这个序列长度，而不是另一个呢？</p><p id="57b8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我想给出一些直觉，告诉你如何做出一些决定，找到正确的配置来预训练BERT模型。</p><p id="96ff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">带有代码和输出的完整文章可以作为笔记本在<a class="ae iu" href="https://github.com/google-research/bert/blob/master/README.md#pre-training-with-bert" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><p id="b79e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们开始查看每个输入参数。</p><h2 id="5e49" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated"><strong class="ak"> 1。做小写</strong></h2><p id="dca1" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">您可以通过将<em class="la"> do_lower_case </em>设置为true或false来预训练“无案例”或“有案例”版本的BERT模型。</p><blockquote class="lb lc ld"><p id="5830" class="iv iw la ix b iy iz ja jb jc jd je jf le jh ji jj lf jl jm jn lg jp jq jr js hb bi translated">不区分大小写意味着文本在标记化之前已经小写，例如john smith变成了John Smith。无外壳模型还去除了任何重音标记。Cased表示保留真实的大小写和重音标记。</p></blockquote><p id="4ca5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您可以根据任务类型和输入数据的语言来决定其值。</p><p id="a83b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="la">任务类型</em> </strong></p><p id="550f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于像这样的任务，</p><ol class=""><li id="5f04" class="lh li hi ix b iy iz jc jd jg lj jk lk jo ll js lm ln lo lp bi translated">命名实体识别</li><li id="7803" class="lh li hi ix b iy lq jc lr jg ls jk lt jo lu js lm ln lo lp bi translated">词性标注</li><li id="fee9" class="lh li hi ix b iy lq jc lr jg ls jk lt jo lu js lm ln lo lp bi translated">情感检测</li></ol><p id="1cc9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">病例信息是一个重要的信号。想象一下“我们”这个词。它可以是在句子中代表“我们”或国家“美国”的代词。</p><p id="e782" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于其他任务，我们应该只使用“无外壳”模型。它没有不必要的重复，例如，dhoni和Dhoni都将出现在“cased”vocab中，但不在其中，因此，能够用相同大小的vocab更好地表示语言。</p><p id="9371" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="la">输入语言</em> </strong></p><p id="7f3d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从梵文和其他非拉丁文字中去除重音符号会改变单词的意思，从而改变句子的意思。</p><p id="ee87" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">例如:कुरान (kuran)变成了करान (kran，ु被删除)</p><p id="6efb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，建议对非拉丁语言使用“装箱”模型。</p><h2 id="f57b" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated"><strong class="ak"> 2。最大序列长度</strong></h2><p id="9bf1" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated"><em class="la"> max_seq_length </em>指定输入的最大令牌数。输入令牌根据其值被截断或填充。它被设置为2的幂，例如64，128，512。</p><p id="086c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您可以根据最终任务来决定它的值。你想预测文章、句子或短语吗？</p><p id="9c35" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">人们应该为文章保留较大的价值，为短语保留较小的价值。</p><h2 id="ef0e" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">3.训练批量</h2><p id="2c6b" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">训练批次大小是模型更新前处理的样本数量。优选较大的批量，以获得对整个数据集的梯度的足够稳定的估计。批量大小始终设置为2的幂，例如512、1024、2048。</p><p id="f182" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最终输入形状看起来像(batch_size，max_seq_length，embedding_size)。对于基于BERT的语言模型，嵌入大小通常为768，并且序列长度基于如上所述的最终任务来决定。</p><p id="ccda" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的目的是充分利用我们的资源。因此，您应该根据可用ram将训练批次大小设置为最大值。</p><h2 id="ed10" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">4.每个序列的最大预测</h2><p id="bdee" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">max_predictions_per_seq是每个序列的屏蔽LM预测的最大数量。这是按如下方式计算的。</p><pre class="lv lw lx ly fd lz ma mb mc aw md bi"><span id="da05" class="ka kb hi ma b fi me mf l mg mh">max_predictions_per_seq<!-- -->= (<!-- -->max_seq_length<!-- -->* <!-- -->masked_lm_prob<!-- -->)</span><span id="1707" class="ka kb hi ma b fi mi mf l mg mh">For instance<br/>max_seq_length<!-- -->= 512 and <!-- -->max_seq_length<!-- --> = 0.15<br/>max_predictions_per_seq <!-- -->= 77</span></pre><p id="11df" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中masked_lm_prob是每个序列中被[MASK]标记替换的单词的百分比。</p><h2 id="a491" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated"><strong class="ak"> 5。训练步骤数</strong></h2><p id="9fcb" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">训练步骤是我们通过该批来训练模型的次数。其计算如下</p><pre class="lv lw lx ly fd lz ma mb mc aw md bi"><span id="735c" class="ka kb hi ma b fi me mf l mg mh">steps = (epoch * examples)/batch size<br/>For instance<br/>epoch = 100, examples = 1000 and batch_size = 1000<br/>steps = 100</span></pre><p id="90ff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中epoch是您希望传递完整数据集的次数。您应该保持它的值大于1。</p><p id="34f4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在现实世界中，我们通常保持一个很高的值，一旦亏损见底就手动止损。</p><h2 id="1f2d" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated"><strong class="ak"> 6。学习率</strong></h2><blockquote class="lb lc ld"><p id="03f8" class="iv iw la ix b iy iz ja jb jc jd je jf le jh ji jj lf jl jm jn lg jp jq jr js hb bi translated"><em class="hi">学习率，一个决定步长大小的正标量。</em></p></blockquote><p id="a388" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们不应该使用太大或太小的学习率。当学习率太大时，梯度下降会无意中增加而不是减少训练误差。当学习率太小时，训练不仅较慢，而且可能永久地陷入高训练误差。</p><h2 id="2305" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">7.预热步骤的数量</h2><p id="f3e2" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">如果您的数据集高度分化，您可能会遭受某种“早期过度拟合”。如果你的混洗数据碰巧包括一组相关的、强特征的观察结果，你的模型的初始训练可能会严重偏向这些特征，或者更糟，偏向与主题完全不相关的附带特征。</p><p id="12bd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">热身是减少早期训练例子首因效应的一种方式。如果没有它，你可能需要运行一些额外的时期来获得期望的收敛，因为模型消除了那些早期的迷信。</p><p id="155c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在预热期间，学习率线性增加。如果目标学习速率为p，预热周期为n，那么第一批迭代使用1*p/n作为其学习速率；第二个使用2*p/n，依此类推:迭代I使用i*p/n，直到我们在迭代n达到名义速率。</p><p id="4140" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">热身时间通常设定为总训练时间的1%。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h2 id="33f5" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">结论</h2><p id="6a3b" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">在本文中，我讨论了用于预训练BERT模型的输入参数，并解释了有助于决定其配置的因素。</p><p id="13f3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="la">干杯！</em></p></div></div>    
</body>
</html>