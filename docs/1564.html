<html>
<head>
<title>The Basics of Hadoop</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Hadoop的基础知识</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-basics-of-hadoop-85bd75a987f6?source=collection_archive---------28-----------------------#2021-03-07">https://medium.com/analytics-vidhya/the-basics-of-hadoop-85bd75a987f6?source=collection_archive---------28-----------------------#2021-03-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="4e56" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">Hadoop的基础知识</strong></p><p id="1f1f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Hadoop是一个用java编写的Apache开源框架，它允许使用简单的编程模型跨计算机集群分布式处理大型数据集。</p><p id="d8aa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，拥有大型数据集意味着什么？它有多大？</p><p id="bd6c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可能知道，像谷歌、脸书、亚马逊等大型科技巨头每天都会产生数十亿字节的数据。</p><p id="6128" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个数据是关于什么的？毫不奇怪，你、我和许多像我们一样的人都是这些公司收集、分析和处理数据的重要贡献者。正是由于这种先进的加工工艺，我们使用他们产品的体验与日俱增。</p><p id="e26e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你有没有想过，Instagram是如何展示你最感兴趣的图片/视频的？或者，亚马逊如何能够推荐你下一次可能需要添加到购物车中的东西？</p><p id="79aa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所有这些都是收集、分析和处理互联网用户产生的大量数据的结果。</p><p id="394f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们给如此大量的数据贴上标签的一个著名的技术术语是“大数据”。</p><p id="89a3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇博客中，我们将探讨如何在存储方面处理“大数据”。</p><p id="ea66" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">为什么大数据是个问题？</strong></p><p id="47bc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">想象一下，你有20个客人来你家。不幸的是，你只能为其中的5个腾出空间。更不用说他们中的每一个对你来说都同样重要。你将如何接纳他们？</p><p id="f48b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢上帝，你想到了一家酒店(虽然不总是五星级的)。</p><p id="24f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将这种类比扩展到数据是理解Hadoop技术的基础。</p><p id="2f90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">简而言之，当数据超出您的容量时，您会从其他计算设备中寻找资源来存储和处理数据(RAM、CPU和硬盘)。</p><p id="bce8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Apache Hadoop是一个用于存储大量数据和执行计算的软件编程框架。</p><p id="e3f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">一群商品硬件</strong></p><p id="d5b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Hadoop集群是一种将大数据分解成较小部分的方式，然后将这些数据存储在Hadoop工作节点网络中。这些工作节点可能是简单的商用硬件，不会给人留下深刻印象。</p><p id="3939" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了管理各种工作节点，有一个主节点保存所有元数据，如数据的哪一部分存储在哪里等等。主节点运行NameNode进程，而从节点或工作节点运行DataNode进程。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/6421d82dadf4248d0617457e79b5eefd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*WdP9MQtzzJEMDvCkAhT9iw.png"/></div></figure><p id="521b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">引擎盖下是什么？</strong></p><p id="e8b2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> HDFS </strong></p><p id="4d5a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当你在本地机器上存储文件/文件夹时，它使用一个文件系统来跟踪你放入的所有数据。例如，维护inode表是文件系统的责任。</p><p id="623a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，我们讨论的是跨多个节点存储文件和文件夹。显然，普通的文件系统是不够的。</p><p id="ea57" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Hadoop使用的分布式文件系统是HDFS——Hadoop分布式文件系统。这个文件系统与传统的文件系统略有不同。它有一个主/从架构。如前所述，主节点将负责所有文件系统命名空间，并将管理客户端对文件的访问，而数据节点将提供用于存储文件的实际存储。</p><p id="bec3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在内部，文件被分割成一个或多个块，这些块存储在一组DataNodes中。NameNode执行文件系统命名空间操作，如打开、关闭和重命名文件和目录。它还决定了块到DataNodes的映射。</p><p id="3313" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">开发HDFS是为了提供高可用性，因此，数据块在整个群集内复制，每个文件的副本数量是可配置的，并上传到HDFS。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/d79894ed3cdca0704599d71631b40583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*tOvSTHRwyXPfOvO46-m61Q.png"/></div></figure><p id="f926" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们已经看到，主节点运行NameNode进程并执行分布式文件系统的名称空间。类似地，从节点运行DataNode进程。从节点是我们存储数据(输入文件的块)的地方。</p><p id="a731" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当客户端有数据要上传到HDFS时，NameNode将分配活动的DataNodes来存储数据块。</p><p id="7ba0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">NameNode如何知道DataNode是活动的？</p><p id="7920" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">DataNode进程每3秒钟向NameNode连续发送一个心跳信号，这样NameNode就知道有一个特定的从机可以容纳数据。</p><p id="314d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">回到前面关于客户端上传文件的讨论——现在NameNode绘制了一个DataNodes列表，输入文件的块将存储在这个列表中。我们已经了解到，为了提供高可用性，每个数据块都被复制(默认为3次)。因此，最初，从客户端机器，一个数据块将被写入其中一个从属机器。该系统依次将同一文件块复制到另一台从机中。这一直持续到创建了数据块的所需副本。</p><p id="ba76" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">主人是如何跟踪元数据的？</strong></p><p id="8ecd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">运行在主节点上的NameNode创建一个本地日志文件来持久地记录文件系统中的变化。例如，每当一个新文件被上传或从文件系统中删除时，在该文件中会创建一个条目来表明这一点。我们称这个日志为<strong class="ig hi">编辑日志</strong>。</p><p id="8cf0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">FsImage是本地文件系统中的文件，它存储了分布式文件系统的整个名称空间。因此，每当NameNode启动时，它都会读取FsImage文件，应用编辑日志中跟踪的所有更改，然后创建fsImage的最终版本并用于会话。</p><p id="bd9f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些信息太多了，一次阅读无法掌握。我们将在下一篇博客中探索更多。</p><p id="3c63" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">只是，万一你想试验这些东西…</p><p id="d160" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所有HDFS通信都通过TCP/IP进行。也就是说，DataNode和NameNode通过TCP/IP协议进行通信。如果您配置了HDFS，这非常简单—安装2–3个CLI模式虚拟机，然后在这些虚拟机上安装java和Hadoop。通过编辑hdfs-site.xml和core-site.xml文件，将一个配置为主服务器，另一个配置为从服务器。然后，尝试使用hdfs命令在HDFS上上传文件。</p><p id="4374" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在所有系统上，启动一个<em class="jk"> tcpdump </em>进程，该进程将捕获网络流量。如果您擅长分析IP包，那么您将会在输入文件被上传的同时探索NameNode和DataNode之间以及DataNode之间正在进行的通信。</p></div></div>    
</body>
</html>