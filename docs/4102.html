<html>
<head>
<title>Image Denoising using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于深度学习的图像去噪</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/image-denoising-using-deep-learning-dc2b19a3fd54?source=collection_archive---------0-----------------------#2021-08-27">https://medium.com/analytics-vidhya/image-denoising-using-deep-learning-dc2b19a3fd54?source=collection_archive---------0-----------------------#2021-08-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/c90677ae4a7033f79f1b2404d0ab7434.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5bsoVIT2La_5-GDK6Vljyg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图像去噪</figcaption></figure><h1 id="2dd3" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">内容</h1><ol class=""><li id="162c" class="jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">商业问题</li><li id="1947" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">为什么要深度学习？</li><li id="c35f" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">业务限制</li><li id="0081" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">绩效指标</li><li id="3699" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">数据集概述</li><li id="c28a" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">探索性数据分析</li><li id="9319" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">现有方法</li><li id="a7ce" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">尝试深度学习模型</li><li id="c9b9" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">结果</li><li id="ec78" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">模型量化</li><li id="d40b" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">使用简化共享进行部署</li><li id="eaa5" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">结论</li><li id="293f" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">潜在的改进</li><li id="c3ee" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">链接到Github和LinkedIn</li><li id="30f2" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">参考</li></ol><h1 id="bc8d" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">商业问题</h1><p id="baed" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">真实世界中捕捉到的图像带有噪声。这些噪声可能由于许多原因而出现，例如电信号不稳定、相机传感器故障、照明条件差、长距离数据传输中的错误等。这可能会降低捕获图像的质量，并可能导致信息丢失，因为原始像素值会因噪声而被随机值替换。因此，当涉及到低级视觉任务和图像处理时，需要从图像中去除这些噪声。从图像中去除这种噪声的过程被称为图像去噪。</p><p id="9f8e" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">因此，当前的任务是开发一种解决方案，从图像中去除这些噪声，从而提高图像质量并保留图像中的相关信息。</p><h1 id="db87" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">为什么要深度学习？</h1><p id="ae39" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">几十年来，图像去噪一直是一个有趣的研究领域。多年来，许多技术和思想已被引入图像去噪。这些技术大多假设图像中的这些噪声是高斯噪声或脉冲噪声。</p><p id="4c79" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi">高斯噪声</strong>-PDF等于正态分布的噪声。即这些噪声可以采用的像素值是高斯分布的。</p><p id="4727" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi">脉冲噪声</strong> -由图像信号中尖锐和突然的干扰引起。它通常作为图像中的黑白像素出现。</p><p id="a282" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">但是这个假设并不完全适用于照片中的真实噪音。真实世界的噪声(也称为<strong class="jt hi">盲噪声</strong>)更加复杂多样。由于这个原因，大多数去噪技术在从图像中去除真正的噪声方面表现不佳。</p><p id="4445" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">因此，为了解决现实世界噪声图像的去噪问题，需要使用更先进的技术。这就是深度学习发挥作用的地方，实验证明，训练卷积盲去噪深度学习网络比其他传统图像去噪技术好得多。这就是我们使用深度学习进行图像去噪任务的原因。</p><h1 id="b666" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">业务限制</h1><p id="ac49" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">解决这个问题时要考虑的两个主要约束是</p><ol class=""><li id="8083" class="jr js hh jt b ju ld jw le jy li ka lj kc lk ke kf kg kh ki bi translated">对真实世界的噪声图像进行去噪，以接近地面真实图像。</li><li id="1380" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">没有延迟限制。当务之急是尽可能接近真实情况地对图像进行去噪，即使这需要合理的时间。</li></ol><p id="8565" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">考虑到这些限制，我将为图像去噪任务建立深度学习模型。</p><h1 id="ff0d" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">绩效指标</h1><p id="bfb1" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">通常使用两个著名的指标来检查图像质量。</p><p id="1b94" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">a)峰值信噪比(PSNR)[<a class="ae ll" href="https://www.ni.com/en-in/innovations/white-papers/11/peak-signal-to-noise-ratio-as-an-image-quality-metric.html" rel="noopener ugc nofollow" target="_blank"><strong class="jt hi">1</strong></a><strong class="jt hi">]:</strong>它是信号的最大可能功率与有害噪声的功率之比，aﬀects其表示的质量。由于信号的取值范围很广，PSNR通常用对数分贝标度来表示。<br/>数学上PSNR可以表示为</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div class="er es lm"><img src="../Images/f0f8bee8f9604d339b3af3ea51c0a2d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*FfLnjnmIuOiIDirTYJY-Mg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">峰值信噪比方程</figcaption></figure><p id="cdcd" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">其中MSE由下式给出</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/dcd6fda6ae7ab14040d2ac74f0a771bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*FvEpRZAouAB5eyKEuXuyAg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">均方误差方程</figcaption></figure><p id="a20b" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi"> B)结构相似性指数(SSIM)【</strong><a class="ae ll" rel="noopener" href="/srm-mic/all-about-structural-similarity-index-ssim-theory-code-in-pytorch-6551b455541e"><strong class="jt hi">2</strong></a><strong class="jt hi">】:</strong>It<strong class="jt hi"/>通过主要关注来自场景的结构信息并识别从参考和样本场景提取的信息之间的差异来测量两个给定图像之间的相似性<em class="ls"> </em>。人们认为，人类的视觉感知系统就是这样工作的。因此，这是一个很好的衡量图像质量的指标。</p><p id="b496" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">该度量提取三个特征，即<strong class="jt hi">亮度、对比度和结构。基于这些特征完成两幅图像之间的比较。</strong></p><p id="0c77" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">可以在上面给出的参考文献中找到对该度量的进一步数学理解。</p><h1 id="1371" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">数据集概述</h1><p id="cff2" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">我采用了两个公开可用的数据集用于图像去噪任务，如下所示:</p><ol class=""><li id="e98f" class="jr js hh jt b ju ld jw le jy li ka lj kc lk ke kf kg kh ki bi translated"><strong class="jt hi">智能手机图像去噪数据集(SIDD)[</strong><a class="ae ll" href="https://www.eecs.yorku.ca/~kamel/sidd/dataset.php" rel="noopener ugc nofollow" target="_blank"><strong class="jt hi">3</strong></a><strong class="jt hi">】</strong>:-它由320个干净-有噪声的图像对组成。</li><li id="8601" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated"><strong class="jt hi">真实微光图像降噪数据集(RENOIR)[</strong><a class="ae ll" href="http://adrianbarburesearch.blogspot.com/p/renoir-dataset.html" rel="noopener ugc nofollow" target="_blank"><strong class="jt hi">4</strong></a><strong class="jt hi">】</strong>:-它由221个干净-有噪声的图像对组成。</li></ol><p id="8a8a" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">我已经合并了这些数据集，并把它们混在一起。因此，对于我们的任务，我们总共得到541个干净的有噪声的图像对。然后，我将数据集按比例(80:20)分成训练和测试图像。因此，我们总共有432对训练图像和109对测试图像。</p><h1 id="8c9b" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">探索性数据分析</h1><p id="03d8" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">让我们通过对图像数据集执行彻底的EDA来更好地理解它们。我们可以查看像像素分布、图像对的PSNR和SSIM值等图，并查看干净图像和有噪声图像之间的差异。</p><p id="f621" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi">可视化少量干净-有噪声的图像对</strong></p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/f0cdbe55cf3c45e2bcbdaa4f5e08022b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F-toE70ZBcRjZ-1G8JHGMg.png"/></div></div></figure><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/ff007e33a4d7bf4612632e0108d22818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GieVhS0OSzHYFyuVXHNjlQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">干净的噪声图像对</figcaption></figure><p id="fa82" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">可以看出，在有噪声的图像中有大量的噪声，地面真实图像显示了相应的没有噪声的干净图像。</p><p id="1f1c" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi">图像的平均像素分布</strong></p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lt"><img src="../Images/a7ab647b96c73c7b99ff6887652292d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ph6qd33LmiUeprgRdSp5KA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图像的平均像素分布</figcaption></figure><p id="38b9" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">对于大多数图像(干净的和有噪声的)，平均像素值在20-75之间。这意味着，大多数图像具有暗到中等的亮度。只有少数图像具有高平均像素值或高亮度。</p><p id="cfe9" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi">通过绘制直方图分析少量干净-噪声图像对的像素分布:</strong></p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lu"><img src="../Images/2ec38ef3136a92c3b45b6327d69cb057.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S5k3Nt65-AUc2CZDEInukA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">干净图像及其相应噪声图像的像素分布</figcaption></figure><p id="c584" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">与干净的图像相比，有噪声的图像似乎具有更平滑的像素强度分布。此外，与干净图像的相应像素值相比，噪声图像中的许多像素取零像素值。这意味着噪波用深色替换了许多实际像素值。</p><p id="247b" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi">分析图像的PSNR和SSIM值</strong></p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lv"><img src="../Images/a459ffd04f29abefcad938fd22f8b086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N08MrUTlDeSeMwk8Gz3DEA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">所有干净-噪声图像对的PSNR值的PDF和直方图</figcaption></figure><p id="f32b" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">大多数干净-噪声图像对的PSNR值在25-30之间。因此，一个好的去噪模型应该给出大于30db的干净-有噪声图像对的平均PSNR值。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lw"><img src="../Images/7156f47db53a14f7e7ec3ce736c925c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2_lHJ56uS81hqTWBd_ssRQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">所有干净-噪声图像对的SSIM值的PDF和直方图</figcaption></figure><p id="d219" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">大多数干净-噪声图像对的SSIM值在0.1-0.7之间。因此，一个好的去噪模型应该为干净-有噪声的图像对给出大于0.7的平均SSIM值。</p><p id="f238" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi">创建补丁</strong></p><p id="b8c7" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">我们将把这些图像分割成小块。实验表明，将图像分割成小块并使用这些小块进行训练可以提高去噪的模型性能。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lx"><img src="../Images/84febcee6dafe718a9cbc84405eb1238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AWHGPoWFn4zA3lacG2E1gQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">将图像分割成小块。</figcaption></figure><p id="f27d" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">这就是修补的作用。它根据给定的补丁大小将图像分割成不同的补丁。我们将绘制一些干净的有噪声的图像块，并将它们可视化。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ly"><img src="../Images/1a514d5a60b80e72a3e1852428a32582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QN9ab3Pciq4m8X-fZVJrEQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">清除有噪声的图像补丁</figcaption></figure><p id="eef4" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">在有噪声的图像块中有大量的噪声，这就是我们试图消除的。由于数据集图像具有不同的大小，为了保持每个图像的固定数量的补丁，我们必须将每个图像的大小调整为固定值。因此，我们将所有图像的大小调整为1024 x 1024的固定大小，并创建大小为256 x 256的面片。这将为每个图像提供4x4=16个补丁。</p><p id="98cf" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">在创建补丁之后，我们为训练图像获得了6912个图像补丁，为测试图像获得了1744个图像补丁。我们将使用这些训练和测试图像块进行建模。</p><h1 id="8cc7" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">现有方法</h1><p id="69f5" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">正如所讨论的，图像去噪的任务几十年来一直是一个有趣的研究领域。多年来，许多技术被用于解决这个问题。其中一个著名的技术是应用过滤器来消除噪音。有许多滤波器可用于图像去噪。这些滤波器中的大多数都是针对图像中存在的噪声类型的。一种著名的滤波器被称为<strong class="jt hi">非局部均值(NLM)算法。</strong></p><p id="d67c" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">NLM滤波器通过图像块中所有像素的平均值来替换图像的每个像素值，并根据这些像素与目标像素的相似程度进行加权。这使得滤波后的清晰度更高，图像中的细节损失更少，并且与许多其他传统滤波器相比，在图像去噪中表现良好。</p><p id="b473" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">查看下面的视频，了解NLM算法的更详细的解释。</p><figure class="ln lo lp lq fd ii"><div class="bz dy l di"><div class="lz ma l"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">基于NLM滤波的图像去噪</figcaption></figure><p id="32da" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">现在，在我们开始使用深度学习模型去噪之前，让我们看看这个简单的滤波器在图像去噪中的表现。我们将从我们的数据集中选取一些图像块，并对它们应用NLM滤波器进行去噪，并将这些去噪后的图像可视化。这将有助于我们理解使用深度学习等更高级技术进行去噪任务的必要性。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mb"><img src="../Images/b090032df4421feb2ae62880810fec6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ycoc-i94VPfUrBbA43PlhA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">基于NLM滤波的图像去噪</figcaption></figure><p id="2ad2" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">可以看出，NLM滤波器可以在一定程度上消除图像的噪声。但是它平滑了地面真相图像中存在的许多细节，导致了本应保留的重要信息的丢失。此外，当噪声太高时，NLM不能提供好的结果。</p><p id="0ee7" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">因此，需要使用更先进的技术，如深度学习来完成图像去噪任务。</p><h1 id="20e8" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">输入数据管道</h1><p id="af03" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">现在我们已经有了从SIDD和雷诺阿数据集的干净-嘈杂图像对中获取的训练和测试图像片，我们已经为建模做好了准备。我们总共有6912和1744个训练和测试图像块，块大小为256×256。</p><p id="178e" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">X _ train _ image _ patches . shape =(6912，256，256，3) —地面实况影像y _ train _ image _ patches . shape =(6912，256，256，3) —噪声影像X _ test _ image _ patches . shape =(1744，256，256，3) —地面实况影像y _ test _ image _ patches . shape =(1744，256，256，3)—噪声影像</p><p id="d8bd" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">我将创建一个输入数据管道，将这些图像补片作为模型训练的输入。我将使用<strong class="jt hi"> Keras定制数据生成器</strong>来构建输入管道。</p><figure class="ln lo lp lq fd ii"><div class="bz dy l di"><div class="mc ma l"/></div></figure><p id="a68c" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">在将它加载到定制生成器之前，<strong class="jt hi">通过将每个像素除以255 </strong>来对训练和测试补丁进行归一化。输入数据管道将以批处理大小= 32的批处理方式将数据加载到模型中。给予模型的输入形状将是(32，256，256，3)。</p><h1 id="6cff" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">尝试深度学习模型</h1><p id="b89d" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">随着深度学习技术的进步，现在可以从图像中去除真实噪声，使得去噪后的图像非常类似于地面真实图像，而细节损失最小。</p><p id="0763" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">近年来，许多深度学习架构已经被开发用于图像去噪任务。其中，我将实施四种最先进的深度学习架构来解决这个问题，如下所示:</p><ol class=""><li id="46e5" class="jr js hh jt b ju ld jw le jy li ka lj kc lk ke kf kg kh ki bi translated">自动编码器(基线模型)</li><li id="a2d5" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">CBDNet</li><li id="05fe" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">PRIDNet</li><li id="0cfa" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">RIDNet</li></ol><h2 id="317c" class="md iu hh bd iv me mf mg iz mh mi mj jd jy mk ml jh ka mm mn jl kc mo mp jp mq bi translated">自动编码器</h2><p id="6cc7" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">这是一个简单的<strong class="jt hi">编码器-解码器网络[</strong><a class="ae ll" href="https://keras.io/examples/vision/autoencoder/" rel="noopener ugc nofollow" target="_blank"><strong class="jt hi"/></a><strong class="jt hi">】</strong>，具有3个卷积层，随后是用于编码器单元的max-pooling和用于解码器单元的3个去卷积层。来自解码器的输出然后被提供给具有3个滤波器的卷积层，以保持相似的输入和输出形状。这是一个简单的架构，将用作基线模型。</p><figure class="ln lo lp lq fd ii"><div class="bz dy l di"><div class="mc ma l"/></div></figure><p id="a56f" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">用于训练模型的<strong class="jt hi">损失函数</strong>是<strong class="jt hi">均方误差(MSE) </strong>。该模型被训练15个时期，并且它给出0.0011的训练和测试损失。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mr"><img src="../Images/c076a74465da85fda842548be2b62e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mn06uTiwiXm0VgPp0xgswQ.png"/></div></div></figure><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ms"><img src="../Images/c2cb410fc4c809990faef79ef3a6634a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e5MStG-vpdOBsR_tW6R0jw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">自动编码器模型结果</figcaption></figure><p id="e5a4" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">尽管这种简单的结构可以减少噪声，但是预测的图像缺乏清晰度。去噪前，测试数据上PSNR和SSIM的平均得分分别为18.74和0.47 。自动编码器模型在相同的测试数据上分别给出了31.19的<strong class="jt hi"> PSNR得分和0.74的</strong>SSIM得分。这意味着，该模型运行良好，我们可以将这些分数作为基准值来比较其他模型的性能。</p><h2 id="ba02" class="md iu hh bd iv me mf mg iz mh mi mj jd jy mk ml jh ka mm mn jl kc mo mp jp mq bi translated">CBDNet —卷积盲去噪网络[ <a class="ae ll" href="https://arxiv.org/pdf/1807.04686v2.pdf" rel="noopener ugc nofollow" target="_blank"> 6 </a></h2><p id="5727" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">CBDNet架构带有2个子网络。首先是噪声估计子网络(CNNe-估计噪声图像中的噪声水平图)，接着是非盲去噪子网络(CNNd-去噪噪声图像)。网络架构如下所示</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mt"><img src="../Images/bad167fd701fe8fdf24bf59c7392a523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TUb9PX_mnA8p3Y0QqANnhA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">CBDNet架构</figcaption></figure><p id="7ae3" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">在最初的研究论文中，他们使用真实的噪声图像和综合添加的噪声图像来训练这个模型。他们使用噪声模型将噪声综合添加到图像中。由于我们正在使用的数据集已经有干净-有噪声的图像对，我没有考虑综合地添加噪声到图像中。所以，我没有采用制造噪音的模型来构建网络。此外，本文中使用的损失函数如下:</p><p id="d324" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi">损失=均方误差(MSE)+(λx全变分正则化)</strong> <br/>其中全变分正则化防止去噪图像的过度平滑，λ是超参数。</p><p id="2fc3" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi">与研究论文相比，CBDNet实施中的修改:</strong></p><ol class=""><li id="870c" class="jr js hh jt b ju ld jw le jy li ka lj kc lk ke kf kg kh ki bi translated">没有添加合成噪声到图像数据集，因为我们有真正的噪声图像对。</li><li id="f0fa" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">将损失函数视为均方误差。</li></ol><figure class="ln lo lp lq fd ii"><div class="bz dy l di"><div class="mc ma l"/></div></figure><p id="99a1" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">该模型被训练30个时期，并且它给出0.00044的训练损失和0.000453的测试损失。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ms"><img src="../Images/959a5a44076b4f0383774ff9f7cc8dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h8GIses-xC4_CkGhxmD-lQ.png"/></div></div></figure><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mu"><img src="../Images/b974666cc17ac347caf54a85c35c997d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9tJFbmOZ2CstzjFdJcb4Jg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">CBDNet模型结果</figcaption></figure><p id="29a4" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">可以看出，与自动编码器模型相比，模型性能有了很大的提高。预测的去噪图像更加清晰，而自动编码器则不是这种情况。该模型在测试数据上给<strong class="jt hi">的平均PSNR分数为35.256，平均SSIM分数为0.848。</strong></p><h2 id="79c0" class="md iu hh bd iv me mf mg iz mh mi mj jd jy mk ml jh ka mm mn jl kc mo mp jp mq bi translated">PRIDNet —金字塔实像去噪网络[ <a class="ae ll" href="https://arxiv.org/pdf/1908.00273v2.pdf" rel="noopener ugc nofollow" target="_blank"> 7 </a></h2><p id="dd46" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">网络架构如下所示:</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mv"><img src="../Images/c28ca32c402a5931b4a678dfb4423d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CvYOeHH50bdFC3bNB5sYuA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">特征图的通道数显示在它们下面，对于“sRGB”模型，它在括号中，而对于“原始”模型，它没有括号。符号||表示连接。</figcaption></figure><p id="8c6f" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">该网络分为三个阶段，解决了许多基于CNN的去噪网络从未真正解决的三个主要问题。</p><p id="9c62" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi"> A)通道注意力模块(CAM): </strong>大多数基于CNN的去噪网络对所有通道特征给予同等的重视。但实际上，有些噪声比其他噪声更重要，应该给予更多的权重。PRIDNet通过在其网络中实施频道注意模块来实现这一点，该模块将根据估计的噪声水平向频道添加diﬀerent权重。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div class="er es mw"><img src="../Images/644c600e55a1422c54db844ee20aa55a.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*HyCazrkKV8ZPi_dip2JawA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">频道关注模块架构</figcaption></figure><p id="2dff" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">CAM使用全局平均池压缩输入信息U，然后进行两次卷积，第一次激活ReLU，第二次激活sigmoid。该计算将为您提供diﬀerent通道的权重𝜇，然后乘以输入信息u，从而重新校准通道重要性。这个阶段被称为<strong class="jt hi">噪声估计阶段。</strong></p><p id="15e1" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi"> B)五层金字塔模块:</strong>传统的基于CNN的去噪网络使用固定的接收场，该接收场捕获图像中噪声的全局信息，但无法捕获不同的信息。PRIDNet通过使用diﬀerent缩放接收场解决了这一问题，该接收场还将捕捉图像中的各种噪声信息。结果表明，这种实现有助于从强噪声中去除suﬀer图像的噪声。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div class="er es mx"><img src="../Images/911c744987bd3a491d0c6167618334ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*mictzRLCs-PUJSBXpMpVjw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">五层金字塔结构</figcaption></figure><p id="199a" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">在这里，输入要素地图被下采样到diﬀerent大小，因此我们可以使用diﬀerent尺度的接收场，从而捕捉全局和多样化的信息。这些下采样的特征被提供给池层，随后是U-Net架构，然后被上采样到原始大小，并且输出被连接在一起。图像中显示了过滤器的数量及其尺寸。这个阶段被称为<strong class="jt hi">多尺度去噪阶段。</strong></p><p id="03ff" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi"> C)核选择模块:</strong>在传统的基于CNN的去噪网络中，多尺度特征通常使用逐元素求和或通过级联它们来组合。这意味着来自diﬀerent尺度的信息被同样对待，这不能自适应地表达多尺度特征。为了防止这种情况，PRIDNet引入了一种核选择模块，该模块对级联多尺度特征的每个通道使用diﬀerent大小的核。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es my"><img src="../Images/d471e8591bcea90d762c98317f38c18c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EN_ZCVsjs8ibOsMnsWkPWQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">内核选择模块架构</figcaption></figure><p id="29a7" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">我们从多尺度去噪阶段获得的多尺度级联输出(U)被给予核大小为3、5、7的三个并行卷积，然后被求和。然后，使用全局平均池对其进行压缩，之后进行2次卷积，以给出应用softmax激活的3个向量𝜶,𝜷,𝛄。然后，将这些向量与最初3个并行卷积的输出相乘，得到V’，V”，V”’，并相加，得到最终的去噪输出图像V = V’+V”+V”’。这个阶段被称为<strong class="jt hi">特征融合阶段。</strong></p><p id="ad95" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">此外，为了避免信息丢失，在整个网络中，每一级的输出都与前一级的输入连接在一起。</p><figure class="ln lo lp lq fd ii"><div class="bz dy l di"><div class="mc ma l"/></div></figure><p id="60c7" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">该模型以MSE作为损失函数被训练30个时期，并且它给出0.000449的训练损失和0.000457的测试损失。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mz"><img src="../Images/7ad1baed75646eb546ed0c18ee87986a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OzBRWR1TAl-Z3zEFQTcV1g.png"/></div></div></figure><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es na"><img src="../Images/7a97ceb44c2cdf6b0c39ba8d7d7d0d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5nr-aO95DeF8Ak8bnP3NXA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">PRIDNet模型结果</figcaption></figure><p id="00a5" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">该模型似乎给出了与CBDNet几乎相同的视觉效果。该模型在测试数据上给出了平均PSNR分数35.126和平均SSIM分数0.848的<strong class="jt hi">。</strong>根据研究论文，PRIDNet模型在训练更多的历元时在性能上超过CBDNet模型。但是PRIDNet模型的缺点是需要训练大量的参数，并且模型规模很大。尽管如此，与CBDNet相比，它在模型性能上并没有显著的改进。</p><h2 id="e06e" class="md iu hh bd iv me mf mg iz mh mi mj jd jy mk ml jh ka mm mn jl kc mo mp jp mq bi translated">RIDNet —残差图像去噪网络[ <a class="ae ll" href="https://arxiv.org/pdf/1904.07396v2.pdf" rel="noopener ugc nofollow" target="_blank"> 8 </a></h2><p id="bc41" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">网络架构如下所示:</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nb"><img src="../Images/b2644be06be691ea4ba8d28e4a5daa5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H_3bWoukDY4e0rdPzDLH9Q.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">卷积层的不同绿色表示不同的膨胀，而卷积层的较小尺寸意味着核是1 × 1。第二行显示了每个EAM的架构。</figcaption></figure><p id="2271" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">该网络由以下三个主要模块组成:</p><p id="dab8" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi"> A)特征提取模块:</strong>它只由一个卷积层组成，从有噪输入中提取初始特征。我在卷积层使用了64个内核大小为3的过滤器。</p><p id="6b9a" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi"> B)残差模块上的特征学习残差:</strong>它由一个称为增强注意模块(EAM)的网络组成，该网络使用残差结构上的残差，具有局部跳过和短跳过连接。EAM的初始部分通过核膨胀和分支卷积使用宽感受野，从而从输入图像中捕获全局和多样的信息。使用两个卷积的残差块，后面跟着三个卷积的增强残差块(ERB ),学习附加特征。最后，它被给予特征关注块，该特征关注块给予重要特征更多的权重。</p><p id="b585" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">我们可以通过增加EAM区块的数量来增加RIDNet网络的深度。然而，在研究论文中，他们将网络限制在EAM的四个街区。</p><p id="c204" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi"> C)重构模块:</strong>最终EAM块的输出被提供给重构模块，该重构模块再次仅由一个卷积层组成，该卷积层给出去噪图像作为输出。</p><p id="75a1" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">研究论文中使用的损失函数是平均绝对误差(L1损失)，但我将使用均方误差(L2损失)。</p><figure class="ln lo lp lq fd ii"><div class="bz dy l di"><div class="mc ma l"/></div></figure><p id="0d3b" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">该模型以MSE作为损失函数被训练25个时期，并且它给出0.000321的训练损失和0.000334的测试损失。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mz"><img src="../Images/5b46f7493db4943efc83ec9dc4576956.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HQwhPjyFUp8e84A69BD61Q.png"/></div></div></figure><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mu"><img src="../Images/7b73caa559a7e395f96ec48d47d1ecd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hI735uhtKQ-9Fk2gaU1RTw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">RIDNet模型结果</figcaption></figure><p id="3714" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">从视觉上看，该模型似乎也给出了与PRIDNet和CBDNet相似的性能。该模型在测试数据上给出的<strong class="jt hi">PSNR平均得分为36.595，SSIM平均得分为0.881。</strong></p><h1 id="663f" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">结果</h1><p id="6238" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">我们将根据PSNR和SSIM评分比较所有模型的性能，并将模型大小纳入考虑范围，以决定最佳模型。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nc"><img src="../Images/12e96317efd85cf130c6fd5eb2bc03d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ek7YIa_csE8cDI9S69lBCg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">结果</figcaption></figure><p id="7dac" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">与CBDNet和PRIDNet模型相比，RIDNet模型具有更好的PSNR和SSIM评分以及更小的模型规模。因此，我们将最终确定RIDNet模型作为图像去噪任务的最佳模型。</p><p id="6da0" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><strong class="jt hi"> RIDNet模型在少量噪声图像上的性能:</strong></p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nd"><img src="../Images/e8474c1d1e90173bd18b5bc01e94cdd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*57AGk49jmjT0Kb7LcHKcLw.png"/></div></div></figure><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ne"><img src="../Images/664b008b40107544ca0907500b38fc55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZMMsmvxm7LZRvXdrPQrs9g.png"/></div></div></figure><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nf"><img src="../Images/b2e292239f51a8fbb6131d9089d359be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lYhAPnTEvIfOaVokAEzfRg.png"/></div></div></figure><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ng"><img src="../Images/75937f8eb148347d3034b4f67df9ab63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cqWe4P5uroMsNqus-mz2Lw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">RIDNet模型去噪少量噪声图像</figcaption></figure><h1 id="97d1" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">模型量化</h1><p id="2429" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">模型量化是一种转换技术，它可以减小模型大小，同时还可以改善CPU和硬件加速器的延迟，而模型精度几乎不会下降。它通过降低用于表示模型参数的数字的精度来工作，默认情况下，这些数字是32位浮点数。这导致更小的模型尺寸和更快的计算。</p><p id="889f" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">当您使用<strong class="jt hi"> TensorFlow Lite转换器[</strong><a class="ae ll" href="https://www.tensorflow.org/lite/convert/" rel="noopener ugc nofollow" target="_blank"><strong class="jt hi">9</strong></a><strong class="jt hi">】</strong>将已经训练好的浮点TensorFlow模型转换为TensorFlow Lite格式时，可以对其进行量化。</p><figure class="ln lo lp lq fd ii"><div class="bz dy l di"><div class="mc ma l"/></div></figure><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nh"><img src="../Images/ac0471b542fbe4e333f9f9084f2f2a01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*__Izb09HR076Vaomvmt9Pw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">量化前后RIDNet模型性能和大小。</figcaption></figure><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ni"><img src="../Images/b0a7c0608c84b51a8ccc2bb96a4ce890.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V1kTvA4jESUcAcrlDLeTww.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">使用原始RIDNet和量化RIDNet模型预测噪声图像。</figcaption></figure><p id="c6de" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">在模型量化之后，模型的大小从20.956 MB减小到6.877 MB，而模型性能没有任何显著下降。但不幸的是，量化模型的预测时间急剧增加。这是因为预测是在图像补片上完成的，然后这些补片被合并以给出最终的去噪输出。因此，对于每个补丁，TensorFlow lite版本需要调用模型，从而花费更多时间。</p><h1 id="b63e" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">使用简化共享进行部署</h1><p id="2f86" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">为了给读者提供更好的用户体验，我使用Streamlit部署了该模型，这是一个用于机器学习和数据科学项目的开源应用框架。</p><p id="9fde" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">可以在此处访问部署的模型:</p><p id="94a3" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated"><a class="ae ll" href="https://share.streamlit.io/sharathsolomon/imagedenoising/main/model.py" rel="noopener ugc nofollow" target="_blank">https://share . streamlit . io/sharath Solomon/image降噪/main/model.py </a></p><p id="166a" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">创建的web应用程序有两个选项。</p><ol class=""><li id="6f4c" class="jr js hh jt b ju ld jw le jy li ka lj kc lk ke kf kg kh ki bi translated">预测样本图像:有一些样本图像已经上传到应用程序，你可以选择看看模型的表现如何。选择列出的任何样本图像，并获取其去噪输出。</li><li id="e9fd" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">上传一个嘈杂的图像:用户也可以上传一个嘈杂的图像，并获得其降噪输出。</li></ol><p id="aaf2" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">这里有一个视频展示了部署的模型是如何进行预测的。</p><figure class="ln lo lp lq fd ii"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="a6d3" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">web应用程序在CPU上运行，因此预测时间大约需要10秒。使用GPU可以将预测时间减少到毫秒级。</p><h1 id="d16b" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">结论</h1><ol class=""><li id="e427" class="jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">CBDNet和PRIDNet模型给出了可比较的性能。根据研究论文，PRIDNet模型在经过大量时期的训练后，其性能优于CBDNet模型。但是PRIDNet模型的缺点是需要训练大量的参数，并且模型规模很大。尽管如此，与CBDNet相比，它并没有给模型性能带来显著的改进。</li><li id="0a17" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">与CBDNet和PRIDNet相比，RIDNet模型是较新的技术。它还在PSNR和SSIM值方面显著提高了模型性能。另一个优点是，与其他模型相比，参数数量和模型大小更少。</li><li id="85cb" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">与像PRIDNet这样需要训练大量参数的复杂网络相比，像RIDNet这样的简单网络证明可以显著提高图像去噪性能。这意味着，与简单的网络相比，复杂的模型不一定能更好地解决问题。</li></ol><h1 id="22c7" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">潜在的改进</h1><p id="ec29" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">图像去噪是一个活跃的研究领域，并且正在开发许多令人惊奇的架构来对图像去噪。最近，研究人员正在使用<strong class="jt hi"> GANs </strong>对图像进行去噪处理，事实证明这种方法能产生惊人的效果。</p><p id="f6a2" class="pw-post-body-paragraph ko kp hh jt b ju ld kq kr jw le ks kt jy lf kv kw ka lg ky kz kc lh lb lc ke ha bi translated">此外，<strong class="jt hi">图像恢复</strong>是另一个活跃的研究领域，试图恢复受损图像，如去模糊图像、图像去模糊等。多年来，已经开发了许多高级深度学习架构来解决这个问题，这些网络在图像去噪任务中也工作得很好。根据www.paperswithcode.com<strong class="jt hi">[</strong><a class="ae ll" href="https://paperswithcode.com/sota/image-denoising-on-sidd" rel="noopener ugc nofollow" target="_blank"><strong class="jt hi">10</strong></a><strong class="jt hi">]的说法，与仅为图像去噪目的而设计的模型相比，像HINet、Uformer32、MIRNet </strong>这样的图像恢复模型为图像去噪任务提供了更好的性能。</p><h1 id="a10a" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">链接到GitHub和LinkedIn</h1><p id="ff9c" class="pw-post-body-paragraph ko kp hh jt b ju jv kq kr jw jx ks kt jy ku kv kw ka kx ky kz kc la lb lc ke ha bi translated">您可以在我的<a class="ae ll" href="https://github.com/sharathsolomon/ImageDenoising" rel="noopener ugc nofollow" target="_blank"> <strong class="jt hi"> Github资源库</strong> </a> <strong class="jt hi">中找到这个案例研究的完整代码。</strong>请随时通过<a class="ae ll" href="https://www.linkedin.com/in/sharath-solomon-0a5436112/" rel="noopener ugc nofollow" target="_blank"> <strong class="jt hi"> LinkedIn </strong> </a>或通过<strong class="jt hi">sharath.solomon@outlook.com</strong>的电子邮件与我联系</p><h1 id="da6d" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">参考</h1><ol class=""><li id="7925" class="jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><strong class="jt hi">峰值信噪比(PSNR):</strong><a class="ae ll" href="https://www.ni.com/en-in/innovations/white-papers/11/peak-signal-to-noise-ratio-as-an-image-quality-metric.html" rel="noopener ugc nofollow" target="_blank">https://www . ni . com/en-in/innovations/white-papers/11/Peak-Signal-to-Noise-Ratio-as-an-image-quality-metric . html</a></li><li id="baff" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated"><strong class="jt hi">结构相似指数(SSIM):</strong><a class="ae ll" rel="noopener" href="/srm-mic/all-about-structural-similarity-index-ssim-theory-code-in-pytorch-6551b455541e">https://medium . com/SRM-mic/all-about-Structural-Similarity-Index-ssim-theory-code-in-py torch-6551 b 455541 e</a></li><li id="27a5" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated"><strong class="jt hi">智能手机图像去噪数据集(https://www.eecs.yorku.ca/~kamel/sidd/dataset.php)</strong><a class="ae ll" href="https://www.eecs.yorku.ca/~kamel/sidd/dataset.php" rel="noopener ugc nofollow" target="_blank"/></li><li id="34fb" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated"><strong class="jt hi">真实微光图像降噪数据集(RENOIR):</strong><a class="ae ll" href="http://adrianbarburesearch.blogspot.com/p/renoir-dataset.html" rel="noopener ugc nofollow" target="_blank">http://adrianbarburesearch . blogspot . com/p/RENOIR-Dataset . html</a></li><li id="4b17" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated"><strong class="jt hi">自动编码器:</strong><a class="ae ll" href="https://keras.io/examples/vision/autoencoder/" rel="noopener ugc nofollow" target="_blank">https://keras.io/examples/vision/autoencoder/</a></li><li id="df98" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated"><strong class="jt hi"> CBDNet研究论文:</strong><a class="ae ll" href="https://arxiv.org/pdf/1807.04686v2.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1807.04686v2.pdf</a></li><li id="4745" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated"><strong class="jt hi"> PRIDNet研究论文:</strong>【https://arxiv.org/pdf/1908.00273v2.pdf T2】</li><li id="3d22" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated">【RIDNet研究论文:<a class="ae ll" href="https://arxiv.org/pdf/1904.07396v2.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1904.07396v2.pdf</a></li><li id="8867" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated"><strong class="jt hi"> Tensorflow Lite转换器:</strong><a class="ae ll" href="https://www.tensorflow.org/lite/convert/" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/lite/convert/</a></li><li id="c69f" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated"><strong class="jt hi">https://paperswithcode.com/sota/image-denoising-on-sidd</strong>SIDD<a class="ae ll" href="https://paperswithcode.com/sota/image-denoising-on-sidd" rel="noopener ugc nofollow" target="_blank">图像去噪</a></li><li id="de68" class="jr js hh jt b ju kj jw kk jy kl ka km kc kn ke kf kg kh ki bi translated"><strong class="jt hi">应用人工智能课程:</strong><a class="ae ll" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></li></ol></div></div>    
</body>
</html>