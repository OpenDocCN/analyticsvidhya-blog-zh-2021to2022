<html>
<head>
<title>Enabling Testing and Local Development for Spark and AWS S3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持Spark和AWS S3的测试和本地开发</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/enabling-testing-and-local-development-with-spark-and-aws-cacbe4ed391f?source=collection_archive---------0-----------------------#2021-10-09">https://medium.com/analytics-vidhya/enabling-testing-and-local-development-with-spark-and-aws-cacbe4ed391f?source=collection_archive---------0-----------------------#2021-10-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><h2 id="b879" class="hf hg hh bd b fp hi hj hk hl hm hn dx ho translated" aria-label="kicker paragraph">开发工作流程</h2><div class=""/><div class=""><h2 id="c580" class="pw-subtitle-paragraph in hq hh bd b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je dx translated">将Spark与LocalStack集成</h2></div><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jf"><img src="../Images/d260dce154554a81fd14987fe5167a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-887JBh_Zh4x5nMY"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated"><a class="ae jv" href="https://unsplash.com/@eklektikum?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">伊瓦·拉乔维奇</a>在<a class="ae jv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="51e1" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">测试Spark是一项具有挑战性的任务。隔离测试环境是工程师很少或没有访问AWS服务(如S3)的场景的需求。LocalStack提供带有AWS兼容API的模拟服务。</p><p id="52e6" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">这篇文章着眼于集成Spark和Localstack所需的最少工作。假设你了解Docker、Docker Compose、Spark (pyspark)、LocalStack、AWS S3和AWS CLI的基础知识。</p><h1 id="07a2" class="ks kt hh bd ku kv kw kx ky kz la lb lc iw ld ix le iz lf ja lg jc lh jd li lj bi translated">问题陈述</h1><p id="21b6" class="pw-post-body-paragraph jw jx hh jy b jz lk ir kb kc ll iu ke kf lm kh ki kj ln kl km kn lo kp kq kr ha bi translated">作为一名工程师，我想在不连接AWS的情况下开发和测试我的Spark应用程序。</p><h1 id="712e" class="ks kt hh bd ku kv kw kx ky kz la lb lc iw ld ix le iz lf ja lg jc lh jd li lj bi translated">解决办法</h1><p id="84de" class="pw-post-body-paragraph jw jx hh jy b jz lk ir kb kc ll iu ke kf lm kh ki kj ln kl km kn lo kp kq kr ha bi translated">起点是使用Localstack模拟AWS S3。使用下面的Docker组合定义，我们可以很容易地让S3兼容的服务在本地运行。</p><pre class="jg jh ji jj fd lp lq lr ls aw lt bi"><span id="dd4c" class="lu kt hh lq b fi lv lw l lx ly">version: '3.7'<br/>services:<br/>  localstack:<br/>    image: localstack/localstack<br/>    environment:<br/>      - SERVICES=s3<br/>      - DEFAULT_REGION=eu-west-1<br/>      - AWS_ACCESS_KEY_ID=foo<br/>      - AWS_SECRET_ACCESS_KEY=foo<br/>    ports:<br/>      - 4566:4566</span></pre><h1 id="be62" class="ks kt hh bd ku kv kw kx ky kz la lb lc iw ld ix le iz lf ja lg jc lh jd li lj bi translated">火花和S3配置</h1><p id="0ed8" class="pw-post-body-paragraph jw jx hh jy b jz lk ir kb kc ll iu ke kf lm kh ki kj ln kl km kn lo kp kq kr ha bi translated">下一步是查看Spark以及如何配置到Localstack的连接。很快，我们知道我们需要在LocalStack中更改模拟S3的URL。如果您正在使用AWS CLI，您知道<code class="du lz ma mb lq b">--endpoint-url</code>选项可用于此。对于spark，这是类似的，但是需要更多的配置更新。</p><p id="4917" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">默认情况下，Spark不提供S3数据源，所以我们需要添加AWS SDK和Hadoop SDK。这就像添加以下包一样简单</p><pre class="jg jh ji jj fd lp lq lr ls aw lt bi"><span id="29c8" class="lu kt hh lq b fi lv lw l lx ly">--packages software.amazon.awssdk:s3:2.17.52,org.apache.hadoop:hadoop-aws:3.1.2</span></pre><p id="2de8" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">接下来，我们需要设置文件系统配置。</p><pre class="jg jh ji jj fd lp lq lr ls aw lt bi"><span id="cc26" class="lu kt hh lq b fi lv lw l lx ly">--conf spark.hadoop.fs.s3a.endpoint=http://localhost:4566 \<br/>--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \<br/>--conf spark.hadoop.fs.s3a.access.key=foo \<br/>--conf spark.hadoop.fs.s3a.secret.key=foo \<br/>--conf spark.hadoop.fs.s3a.path.style.access=true</span></pre><ul class=""><li id="507a" class="mc md hh jy b jz ka kc kd kf me kj mf kn mg kr mh mi mj mk bi translated">端点URL:这是S3文件系统的URL</li><li id="8d11" class="mc md hh jy b jz ml kc mm kf mn kj mo kn mp kr mh mi mj mk bi translated">访问密钥和秘密密钥:AWS身份验证详细信息。</li><li id="8762" class="mc md hh jy b jz ml kc mm kf mn kj mo kn mp kr mh mi mj mk bi translated">实现:用<code class="du lz ma mb lq b">s3a</code>协议指定了S3文件系统的类</li><li id="5191" class="mc md hh jy b jz ml kc mm kf mn kj mo kn mp kr mh mi mj mk bi translated">路径样式:这将强制使用Localstack支持的HTTP URL路径样式的访问。</li></ul><p id="3d25" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">既然我们已经知道了如何配置Spark来为任何S3 URL连接到Localstack，那么让我们看看如何访问数据。我将使用Python来演示，但语言并不重要。</p><pre class="jg jh ji jj fd lp lq lr ls aw lt bi"><span id="b4eb" class="lu kt hh lq b fi lv lw l lx ly">spark.read.csv('s3a://my-bucket/requests.csv').show()</span></pre><h2 id="25d8" class="lu kt hh bd ku mq mr ms ky mt mu mv lc kf mw mx le kj my mz lg kn na nb li hn bi translated">引导本地堆栈S3</h2><p id="e5d1" class="pw-post-body-paragraph jw jx hh jy b jz lk ir kb kc ll iu ke kf lm kh ki kj ln kl km kn lo kp kq kr ha bi translated">在运行Spark应用程序之前，我们需要向Localstack的S3实例添加一些测试数据。为此，有两种方法，使用AWS CLI或使用带有Localstack的引导脚本。</p><p id="6091" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">首先使用AWS CLI，我们需要像以前一样覆盖端点URL。</p><pre class="jg jh ji jj fd lp lq lr ls aw lt bi"><span id="72d2" class="lu kt hh lq b fi lv lw l lx ly">export AWS_ACCESS_KEY_ID=foo <br/>export AWS_SECRET_ACCESS_KEY=foo  </span><span id="e377" class="lu kt hh lq b fi nc lw l lx ly"># add data to Localstack S3<br/>cat &gt; requests.csv &lt;&lt;EOF<br/>"email1"<br/>"email2"<br/>"email3"<br/>EOF</span><span id="f7b5" class="lu kt hh lq b fi nc lw l lx ly">aws s3 --endpoint-url http://localhost:4566 --region eu-west-1 mb s3://my-bucket <br/>aws s3 --endpoint-url http://localhost:4566 --region eu-west-1 cp requests.csv s3://my-bucket/</span></pre><p id="f3dc" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">这个代码片段创建了一个包含3行数据的<code class="du lz ma mb lq b">requests.csv</code>文件，创建了一个新的bucket并将CSV复制到bucket中。</p><p id="88c9" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">第二种方法是编写启动Localstack容器时执行的引导脚本。为此，您需要将脚本目录挂载到<code class="du lz ma mb lq b">/docker-entrypoint-initaws.d/</code>。然后，Localstack将执行该repo中的任何脚本。例如，您可以使用<code class="du lz ma mb lq b">awslocal</code> CLI将上述内容作为脚本。</p><pre class="jg jh ji jj fd lp lq lr ls aw lt bi"><span id="66d7" class="lu kt hh lq b fi lv lw l lx ly">#!/usr/bin/env bash</span><span id="7acd" class="lu kt hh lq b fi nc lw l lx ly">awslocal s3 mb s3://my-bucket </span><span id="2746" class="lu kt hh lq b fi nc lw l lx ly">cat &gt; requests.csv &lt;&lt;EOF<br/>"email1"<br/>"email2"<br/>"email3"<br/>EOF</span><span id="c208" class="lu kt hh lq b fi nc lw l lx ly">awslocal s3 cp requests.csv s3://my-bucket/</span></pre><p id="28e6" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">现在这可以作为样本数据在localstack中获得，您可以运行您的Spark应用程序。</p><p id="8794" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">记住，如果你也在Docker中运行Spark，使用相同的网络和Localstack contain的主机名，而不是<code class="du lz ma mb lq b">localhost</code>。可以在这里找到完整代码:<a class="ae jv" href="https://gist.github.com/atharvai/a0bed7d989b3b316fe6f056d674b99b0" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/athar vai/a0bed 7d 989 B3 b 316 Fe 6 f 056d 674 b 99 b 0</a></p></div></div>    
</body>
</html>