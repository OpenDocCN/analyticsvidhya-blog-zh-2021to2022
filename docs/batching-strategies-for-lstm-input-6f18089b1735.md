# LSTM 输入的批处理策略

> 原文：<https://medium.com/analytics-vidhya/batching-strategies-for-lstm-input-6f18089b1735?source=collection_archive---------10----------------------->

如何在 Pytorch 中正确完成

![](img/441fbf9e4110a98cef927b0b7b92c1c3.png)

# 介绍

使用 RNN 图层时，尤其是在处理不同大小的数据时，一个常见的问题就是批处理。众所周知，批量处理我们的数据有很多好处，但是仅仅考虑如何去做就会让人精神崩溃。

一想到我们将不得不编写或从其他地方获取只有代码作者才理解的复杂而低效的代码，就感到恐怖！在其他一些时候，这是不可能的。嗯，我说不一定要这样。

在这篇文章中，我将分享我在批处理数据时使用的一些策略。虽然我将在这里使用 Pytorch，但同样的想法也适用于其他库。本文中使用的完整代码位于这个 [Github 库](https://github.com/judahsemi/Dino-Name-Generator)中。

# 概观

我将介绍我们深度学习过程中不同阶段的策略。所以我们可以专注于题目；我将使用我写的[上一篇文章](/analytics-vidhya/generating-dinosaur-names-with-pytorch-ee839e97bb76)中的任务。但是，阅读它并不是必需的！

我们将训练我们的模型来生成恐龙的名字；这是我们将使用的[数据](https://gist.github.com/judahsemi/210d42e0410c122aa0e437861c46cf94)。这是一个文本文件，在一个新的行中包含每个恐龙的名字。如果你需要我在这个任务中所做的事情，我可能已经在另一篇文章中谈到了。

就这样，让我们开始吧。

# 1.词汇

我们的策略列表中的第一个与我们如何定义我们的词汇有关。但是在我们开始之前，我们需要预处理我们的数据。目标是将每个恐龙的名字转换成它的字符列表，并将 EOS(序列结束)标记附加到列表中。例如，亚琛龙的名字是:

```
["a", "a", "c", "h", "e", "n", "o", "s", "a", "u", "r", "u", "s", "<EOS>"]
```

之后，我们创建我们的词汇表，它将包含英语字母表的 26 个字母、EOS 标记和 PAD(填充)标记，使我们的词汇表大小为 28。然后我们创建两个字典；第一种方法将词汇表中的每个条目映射到一个惟一的整数，第二种方法正好相反。

现在，在定义我们的词汇表时，我们**最后添加 PAD 标记是非常重要的。这样做将使填充索引(它是填充令牌的索引)为 27。稍后当我们定义我们的模型时，我会解释为什么我们要这样做。**

# 2.数据加载

批量处理数据的下一个(也是最明显的)阶段是获取几个长度相等的样本，然后将它们堆叠在一起。现在，有很多方法可以做到这一点，但我们想保持事情简单明了。我们要做的是**用填充索引填充每个样本，直到它的长度等于我们的数据**中最长的训练样本(X)的长度。

我们首先将已经处理过的数据中的每个字符转换为整数，同时，我们跟踪最长 X 的长度。我们很快就会看到，X 比实际样本少 1，这就是为什么我在更新 max_seqlen 时减 1。当我们不想批处理数据时，batched 参数允许我们跳过整个填充过程。

接下来是 __getitem__ 方法，我们从索引 ix 处的样本中截取 X 和 Y。然后我们追加尽可能多的填充索引，使它们的长度等于 max_seqlen，将它们转换成张量，最后返回它们。我们还**在填充 X 之前存储它的大小**。将来某个特定的 Pytorch 方法会需要它。

最后，我们创建一个 batch_size 为 16 的 DataLoader 对象，并将 shuffle 设置为 True。如果上面的 for 循环产生一个错误，那么您可能正在做一些不同的事情。

# 3.模型定义

在我们模型的 __init__ 方法中，您会注意到我们将嵌入层的 padding_idx 参数设置为 PAD 索引。这将**使填充索引处的嵌入成为零向量，并且还确保其梯度总是零**。因此，与其他嵌入不同，当我们执行反向支持时，它不会改变。

还要注意的是，我们**将线性层的输出特征数量设置为 1——比词汇表大小**小。现在，如果你还记得的话，我们故意让 PAD token 成为我们词汇中的最后一个。这样做的效果是，我们的模型将永远不会预测 PAD 指数作为答案。

如果我们将 PAD index 设置为其他值，如零(常见情况)，也可以实现这一点，但这样做会使结果很难解释。

# 4.前进传球

通过我们的模型的正向传递(由于我们添加的填充)，我们必须**屏蔽输入，这样 LSTM 层忽略每个样本的填充**。这里，我们的正向传递完全是在我们模型的正向方法中定义的。它看起来和我们不进行批处理时的情况很相似，除了我们现在有两个奇怪命名的方法，让我们的生活轻松了 100 倍。

这些方法中的第一个，pack_padded_sequence，为我们做了屏蔽。它将我们的嵌入和批量中每个样本的真实长度作为输入；这是我们使用数据集类中的 x_len 的地方。然后，它执行所需的转换，并返回 LSTM 层理解的 PackedSequence 对象。

请务必注意，LSTM 图层的输出也是一个 PackedSequence 对象。现在，线性层不接受这个对象，所以我们需要先将其转换为张量。我们用第二种方法 pad_packed_sequence 来实现，它与第一种方法相反。

# 5.损失函数

我们策略的最后一点是，**在计算我们的损失时，我们忽略目标(Y)值与填充指数**相同的指数。这是为了使它们不会影响我们的梯度，也不会影响我们模型的训练。当我们评估我们的模型时也是如此。如果我们不忽略它们，填充会增加我们不想要的噪声。

幸运的是，Pytorch 的交叉熵损失函数可以为我们做到这一点。我们只需要将 ignore_index 参数设置为 PAD index，就可以了。我还提供了一个例子，说明在没有 ignore_index 功能的情况下使用损失函数是如何做到这一点的。

# 结论

如果你遵循这里列出的简单策略，我敢肯定你现在会尖叫。我还应该提到的是，我测试了非批处理 VS 批处理的单遍，它们都给出了相同的输出和损失。就好像我们从来没有对我们的数据进行批处理，却获得了批处理的所有好处。

我们讨论的主要观点用粗体字表示；您可以浏览它们以获得摘要。当处理其他库或不同的序列数据(如音频、视频、信号等)时，这些相同的想法也适用。

您可以在我在本文开头发布的 Github 资源库中找到完整的代码，包括测试代码。

如果你喜欢这篇文章，请记得鼓掌，分享和关注我。感谢阅读！

[](https://unsplash.com/photos/-2vD8lIhdnw) [## JESHOOTS.COM 在 Unsplash 上拍摄的照片

### 下载 JESHOOTS.COM 的免费高清照片(@jeshoots)

unsplash.com](https://unsplash.com/photos/-2vD8lIhdnw)