<html>
<head>
<title>Linear Regression from Scratch using Python and its Time Complexity (Part I)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python从头开始线性回归及其时间复杂性(第一部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/day-1-3-linear-regression-from-scratch-using-python-and-its-time-complexity-part-i-372026eb58b6?source=collection_archive---------7-----------------------#2021-02-28">https://medium.com/analytics-vidhya/day-1-3-linear-regression-from-scratch-using-python-and-its-time-complexity-part-i-372026eb58b6?source=collection_archive---------7-----------------------#2021-02-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/78bb21c8163a24fd56f2d7b7a45d0f64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*c8DBeyc007wcBmdg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">照片由<a class="ae it" href="https://unsplash.com/@isaacmsmith?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">艾萨克·史密斯</a>在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><h1 id="024c" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">介绍</h1><p id="6ce3" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">我喜欢写和记录我学到的东西，最近决定在一个更大的平台上写作！<strong class="ju hi">这是使用Python和利用面向对象编程来保持代码整洁和可重用的第一系列线性回归。</strong></p><p id="e377" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">为了编写代码，我们需要了解一些简单的线性代数。在写之前，从数学上对机器学习算法有一个清晰的轮廓总是<em class="kv">超级必要</em>。</p><p id="0585" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">在更高的层次上，线性回归算法采用二维矩阵或二维<code class="du kw kx ky kz b">numpy array</code> X和一维向量或一维数组y，并使用正规方程求解最佳β系数β:</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es la"><img src="../Images/0275d4e6b17c59334cca9f658a2541b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*l5bSfpfCkI09GDAlTMdkJw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">正规方程</figcaption></figure></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><h1 id="60af" class="iu iv hh bd iw ix lm iz ja jb ln jd je jf lo jh ji jj lp jl jm jn lq jp jq jr bi translated">数学</h1><p id="49fa" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">我不会太深入线性回归的数学细微差别和细节，但是对于感兴趣的人，你可以关注我的<a class="ae it" href="https://www.kaggle.com/reighns" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>简介，以获得对其背后的数学的更严格的解释。为了有更具体的理解，附件是一个房价数据集，其中前三列是预测变量x₁、x₂和x₃，最后一列价格是响应变量y</p><figure class="lb lc ld le fd ii"><div class="bz dy l di"><div class="lr ls l"/></div></figure><p id="5411" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">想法很简单，给定m个样本(这里只有5行)和n个预测变量(这里只有3个预测变量)，我们的目标是找到最适合数据的系数，这样我们的模型可以表示如下:</p><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es lt"><img src="../Images/3e0a706025eb3b3fe3092d93672c89e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*TPeAGf_mQlyiDJFCLhXDSA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">线性回归矩阵形式</figcaption></figure><h2 id="471c" class="lu iv hh bd iw lv lw lx ja ly lz ma je kd mb mc ji kh md me jm kl mf mg jq mh bi translated">注释</h2><p id="e585" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">符号很重要，许多作者没有给出每个符号代表什么的明确定义，这对初学者来说可能很难读懂。所以现在开始:</p><ul class=""><li id="82bf" class="mi mj hh ju b jv kq jz kr kd mk kh ml kl mm kp mn mo mp mq bi translated">X为设计矩阵:设X为尺寸为<em class="kv"> m </em> × ( <em class="kv"> n </em> + 1)的设计矩阵，其中<em class="kv"> m </em>为观察值(训练样本)的数量，<em class="kv"> n </em>为独立特征/输入变量。请注意，在这个矩阵的第一列中有一列1。这就是截距列，如果我们想用截距项β₀.来拟合我们的线性模型，我们通常会包括它</li></ul><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mr"><img src="../Images/1dc0f8e54b59bf47dc3f559bded7d81b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e4L8o1Lwfo_v_fD58L_nfA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">设计矩阵X</figcaption></figure><ul class=""><li id="4a44" class="mi mj hh ju b jv kq jz kr kd mk kh ml kl mm kp mn mo mp mq bi translated">X的第<em class="kv">I</em>列定义为X的I次幂，也就是第<em class="kv"> i </em>个训练样本，表示为一个<em class="kv"> n </em> × 1个向量。</li></ul><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es ms"><img src="../Images/7bd03baac7cdce4a0e60895e60b218b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*d4LN0ZzowwSA4lKV0vLGGg.png"/></div></figure><ul class=""><li id="07c5" class="mi mj hh ju b jv kq jz kr kd mk kh ml kl mm kp mn mo mp mq bi translated">y输出向量:列向量y包含第<em class="kv"> m </em>次观察的输出。</li></ul><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/0b8d3461aa9d2940535525408de57ceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*4Ko4jutykWENaKPP0quNfA.png"/></div></figure><ul class=""><li id="9a6c" class="mi mj hh ju b jv kq jz kr kd mk kh ml kl mm kp mn mo mp mq bi translated">β系数/参数的向量:列向量β包含线性模型的所有系数。</li></ul><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es mu"><img src="../Images/b78872263b27f6f40c4bd5d419af8199.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*BW461YD_6xeSuocdc3y-Dg.png"/></div></figure><ul class=""><li id="5154" class="mi mj hh ju b jv kq jz kr kd mk kh ml kl mm kp mn mo mp mq bi translated">ε误差项的随机向量:列向量ε包含对应于<em class="kv"> m </em>个观测值的<em class="kv"> m个</em>误差项。</li></ul><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es mv"><img src="../Images/20809b9398113dbdd98f8c79dbf385b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*QtpBDzlN0KCNTbRzLyUWWg.png"/></div></figure></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><h1 id="f184" class="iu iv hh bd iw ix lm iz ja jb ln jd je jf lo jh ji jj lp jl jm jn lq jp jq jr bi translated">正态方程</h1><p id="50a4" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">利用上面的等式，我们的目标是找到真实总体参数β的最佳估计。我们称最佳估计值<strong class="ju hi">ḃ</strong>为贝塔帽。我们通过使用普通最小二乘法(OLS)最小化成本函数来找到这个最优估计。简而言之，我们将数据集中观察到的y或<code class="du kw kx ky kz b">y_true</code>与我们所拥有的线性模型预测的y或<code class="du kw kx ky kz b">y_hat</code>之间的差的平方和最小化。注意，成本函数通常表示为:</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mw"><img src="../Images/6b1c7552135b34642e97f9a0c26c8b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UHeMw_W--0yh1Yz1VVIz6Q.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">线性回归的成本函数。</figcaption></figure><p id="164a" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">在通常的机器学习过程中，我们可能会使用梯度下降算法来优化代价函数。但是在线性回归中，已经存在一个闭合形式的解，可以让我们找到上述函数的最小值，对于一个不关心其他东西的人来说，拥有这个方便的闭合形式的解是天赐的。我们发现这是:</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es la"><img src="../Images/0275d4e6b17c59334cca9f658a2541b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*l5bSfpfCkI09GDAlTMdkJw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">正规方程</figcaption></figure><h1 id="eb30" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">编码来自Scatch的线性回归</h1><p id="5a59" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">我知道，我们厌倦了数学方程式，这里是承诺的代码:</p><figure class="lb lc ld le fd ii"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">线性回归代码</figcaption></figure><p id="2e4a" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">在本文中，我不会深入讨论代码的细节，但是我想把注意力放在这个算法的时间复杂度上。</p></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><h1 id="48d8" class="iu iv hh bd iw ix lm iz ja jb ln jd je jf lo jh ji jj lp jl jm jn lq jp jq jr bi translated">时间复杂度</h1><p id="a7d2" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">时间复杂性是一个重要的话题，你不希望你的代码运行10亿年，因此，一个高效的代码对企业来说是很重要的。这也是为什么时间复杂性问题在机器学习和数据科学面试中越来越受欢迎！</p><p id="7f10" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">我们在这里使用的线性算法仅仅使用矩阵乘法。我们也将忽略恒定时间为O(1)的码。例如,<code class="du kw kx ky kz b">self.coef_=None</code>在构造函数中是O(1 ),我们真的不希望在<em class="kv">中考虑这一点。</em></p><p id="7618" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">真正重要的在代码行37-40。假设X是一个m乘n的矩阵/阵列，其中m是样本数，n是特征数。另外，y是一个m乘1的向量。参考这个<a class="ae it" href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations" rel="noopener ugc nofollow" target="_blank">维基百科页面</a>获得关于数学运算的各种时间复杂度的便利帮助表。</p></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><p id="541c" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">第37行:<code class="du kw kx ky kz b">np.dot(X.T,X)</code>在点积中，我们将m × n矩阵转置成n × m，这个操作需要O(m × n)时间，因为我们实际上是在执行两个for循环。接下来是执行矩阵乘法，仔细注意两个二维数组之间的<code class="du kw kx ky kz b">np.dot</code>不是指<a class="ae it" href="https://stackoverflow.com/questions/3744094/time-and-space-complexity-of-vector-dot-product-computation#:~:text=Assuming%20that%20multiplication%20and%20addition,computed%2C%20i.e.%20ai%20*%20bi%20." rel="noopener ugc nofollow" target="_blank">点积</a>，而是矩阵乘法，需要O(m × n)时间。这一步的输出矩阵是n× n。</p><p id="a4cc" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">第38行:对一个n × n的矩阵求逆需要n次。输出矩阵为n × n。</p><p id="5fda" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">第39行:现在我们进行n × n和n × m的矩阵相乘，得到O(m × n)，输出矩阵是n × m。</p><p id="fd56" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">第40行:最后，时间复杂度为O(m × n)。</p><p id="76ba" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">将它们相加得到O(2mn+2mn +n)，由此得到mn的简单三角形不等式<mn implies="" we="" can="" remove="" the="" less="" dominant="" term.="" in="" end="" run="" time="" complexity="" of="" this="" linear="" regression="" algorithm="" using="" normal="" equation="" is="" o="" however="" you="" noticed="" that="" there="" are="" two="" variables="" bigo="" notation="" and="" wonder="" if="" further="" reduce="" to="" a="" single="" variable="" well="" number="" small="" which="" means="" n="" kept="" constant="" your="" increasing="" then="" will="" explode=""/></p><p id="8f7a" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">This ends the first series, and also the first article published by me. Stay tuned for updates and see me code various Machine Learning Algorithms from scratch.</p></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><h1 id="b3b4" class="iu iv hh bd iw ix lm iz ja jb ln jd je jf lo jh ji jj lp jl jm jn lq jp jq jr bi translated">Preamble for the next series on Linear Regression</h1><p id="0be0" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">Just a heads up, I may not be doing part II of the series for Linear Regression just yet, as I want to cover a wide variety of algorithms on a surface level, just enough for beginners (or intermediate) learners. However, as a preamble, I will definitely include more and touch on the following topics that are not covered in today’s session.</p><h2 id="f93d" class="lu iv hh bd iw lv lw lx ja ly lz ma je kd mb mc ji kh md me jm kl mf mg jq mh bi translated">Orthogonalization</h2><p id="d38d" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">We can speed up the Normal Equation’s time complexity by using a technique called Orthogonalization, whereby we make use of <a class="ae it" href="http://mlwiki.org/index.php/QR_Factorization" rel="noopener ugc nofollow" target="_blank"> QR因式分解</a>，因此我们不需要对耗时n次的恼人的XᵀX求逆！</p><h2 id="f279" class="lu iv hh bd iw lv lw lx ja ly lz ma je kd mb mc ji kh md me jm kl mf mg jq mh bi translated">正规化</h2><p id="dc2f" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">你基本上不能离开线性模型而不知道L1-2正则化！山脊，套索，还有橡皮筋！请注意，正则化是一个广泛的术语，贯穿所有机器学习模型。请继续关注如何理解正则化可以减少过度拟合。另外，我没有提到的一个警告是，如果XᵀX在我们的正规方程中是不可逆的呢？如果X的一些列是线性相关的(我们的特征变量中的冗余)，或者有太多的特征，从而以某种方式…训练样本的数量m小于特征的数量n，就会发生这种情况。如果您使用例如岭回归，那么修改后的正态方程保证有解。我们将在线性回归的第二部分中讨论它。</p><h2 id="8692" class="lu iv hh bd iw lv lw lx ja ly lz ma je kd mb mc ji kh md me jm kl mf mg jq mh bi translated">线性回归的统计和解释</h2><p id="41fd" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">关于如何解读线性回归，我没有多提。这一点很重要，即使你知道如何从头开始编写一个线性回归算法，如果你不知道如何以一种统计上严格的方式解释结果，那么这是没有意义的！了解有关假设检验、标准误差和置信度的更多信息。我也可能会深入研究一下最大似然估计量！</p><p id="254a" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">学分和参考:</p><p id="334d" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">吉姆统计。(2020年7月06日)。2021年3月1日从<a class="ae it" href="https://statisticsbyjim.com/" rel="noopener ugc nofollow" target="_blank">https://statisticsbyjim.com/</a>检索</p><p id="ef2b" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">数学运算的计算复杂性。(2021年2月04日)。2021年3月1日检索，来自<a class="ae it" href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Computational _ complexity _ of _ mathematic _ operations # Matrix _ algebra</a></p><div class="mx my ez fb mz na"><a href="https://stackoverflow.com/questions/1955088/what-is-the-bigo-of-linear-regression" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab dw"><div class="nc ab nd cl cj ne"><h2 class="bd hi fi z dy nf ea eb ng ed ef hg bi translated">线性回归的BigO是什么？</h2><div class="nh l"><h3 class="bd b fi z dy nf ea eb ng ed ef dx translated">感谢贡献一个堆栈溢出的答案！请务必回答问题。提供详细信息并分享…</h3></div><div class="ni l"><p class="bd b fp z dy nf ea eb ng ed ef dx translated">stackoverflow.com</p></div></div><div class="nj l"><div class="nk l nl nm nn nj no in na"/></div></div></a></div></div></div>    
</body>
</html>