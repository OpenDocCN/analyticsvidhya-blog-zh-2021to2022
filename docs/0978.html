<html>
<head>
<title>Basics of Reinforcement Learning (with example)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习基础(举例)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/basics-of-reinforcement-learning-with-example-fe3c0fb0fd60?source=collection_archive---------22-----------------------#2021-02-08">https://medium.com/analytics-vidhya/basics-of-reinforcement-learning-with-example-fe3c0fb0fd60?source=collection_archive---------22-----------------------#2021-02-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/39414a781c7cf07ad480fe4f9d72ddc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZBfa09rBOc1dDZSWK-PlJQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">DeepMind的AlphaGo利用强化学习赢了围棋</figcaption></figure><p id="5976" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">机器学习提供了各种公式来解决问题。强化学习是继监督学习和非监督学习之后的第三种机器学习范式。这里的目标是从错误中发展和学习，与其他两个范例不同，这里的数据主要是在遇到错误时开发的。引用Kaelbling，L.P .在他1996年的评论论文(强化学习:调查)中的话:</p><blockquote class="jr js jt"><p id="a4ec" class="it iu ju iv b iw ix iy iz ja jb jc jd jv jf jg jh jw jj jk jl jx jn jo jp jq ha bi translated">强化学习是通过与环境互动来学习</p></blockquote><p id="9a80" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">它的应用主要在于机器人甚至游戏。利用RL，2015年，谷歌的<a class="ae jy" href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" rel="noopener ugc nofollow" target="_blank"> AlphaGo </a>击败了拥有无限可能性的围棋世界冠军。这篇文章用一个如何训练山地车到达山顶的例子来描述RL的基础知识(我在最后添加了GitHub代码链接)。</p><h1 id="d374" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">背景</h1><p id="ee56" class="pw-post-body-paragraph it iu hh iv b iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm lb jo jp jq ha bi translated">强化学习的基本组成部分是为给定的状态创建一个奖励系统，并采取相应的行动。以下是主要组件:</p><ul class=""><li id="45ab" class="lc ld hh iv b iw ix ja jb je le ji lf jm lg jq lh li lj lk bi translated">状态空间</li><li id="d437" class="lc ld hh iv b iw ll ja lm je ln ji lo jm lp jq lh li lj lk bi translated">答:一组可能的行动</li><li id="1ee6" class="lc ld hh iv b iw ll ja lm je ln ji lo jm lp jq lh li lj lk bi translated">R(s，a):基于状态和行为的奖励</li><li id="1b10" class="lc ld hh iv b iw ll ja lm je ln ji lo jm lp jq lh li lj lk bi translated">p(s'|s，a):从状态s到s '的转移概率</li></ul><p id="5b49" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在，有两个范例来解决这些问题:</p><ol class=""><li id="e832" class="lc ld hh iv b iw ix ja jb je le ji lf jm lg jq lq li lj lk bi translated"><strong class="iv hi">值迭代:</strong>我们创建一个策略P(s)，基于它我们返回下一个值。该递归函数可用于确定起始点(s0)处的最佳值V(s0)。</li><li id="c7d4" class="lc ld hh iv b iw ll ja lm je ln ji lo jm lp jq lq li lj lk bi translated"><strong class="iv hi"> Q-learning: </strong>这种方法用在我们对环境一无所知的时候。基本上，理想的Q*(s，a)取决于下一步Q的贴现未来值。</li></ol><p id="9b10" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">当前的场景示例是一辆山地汽车，目标是将汽车开到山顶(参见下面的图1)。因此，这类似于后者，其中不存在关于状态空间的任何信息。</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lr"><img src="../Images/74ef42129e26d18f3a3dde38aed75c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*gTO5_w2frrA0Y3_gmPUdYw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><strong class="bd kb">图一。</strong>山地车问题演示</figcaption></figure><p id="1055" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">从数学上来说，我们的目标是找到最优Q*</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es lw"><img src="../Images/e9d6bd864d0aabe6b5695b8bb452b08b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/0*_MCvdKjUZl9NEZyZ"/></div></figure><p id="bf68" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">其中a '是未来行动，γ是未来的贴现因子</p><h1 id="8711" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">q学习</h1><p id="4508" class="pw-post-body-paragraph it iu hh iv b iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm lb jo jp jq ha bi translated">如前所述，我们可以有探索性和开发性的移动(例如，对下一步移动应用贪婪的方法，或者尝试通过做出新的移动来学习)。因此，更新后的等式为:</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es lw"><img src="../Images/6d310c6fa2bee48b664e60d2e55bf984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/0*2679fceaiYeb7nPl"/></div></figure><p id="a97c" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">其中α是学习率。第一部分是开发，第二部分是模式探索。更新后的权重计算如下:</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es lw"><img src="../Images/16437c537ceeecdf6fde96be90ea1f61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/0*2_9QGUa_k23DU22t"/></div></figure><p id="21a5" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在这个场景中，我们有3组动作:停留、向左移动或向右移动。为了通过Q-learning进行学习，我们将状态更新应用为:</p><figure class="ls lt lu lv fd ii"><div class="bz dy l di"><div class="lx ly l"/></div></figure><p id="0417" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我们行动的选择可以是随机的，也可以是根据情况计算出来的。对于渲染，代码使用了<strong class="iv hi"> MountainCar </strong>类。通过等式(2)对模型的训练在我们将探索的剧集数中进行。在每集中，我们基于状态空间中的运动迭代地更新权重。</p><figure class="ls lt lu lv fd ii"><div class="bz dy l di"><div class="lx ly l"/></div></figure><p id="97da" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">下面可以看到上面代码的运行演示。</p><figure class="ls lt lu lv fd ii"><div class="bz dy l di"><div class="lz ly l"/></div></figure><p id="631a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">一个显而易见的疑问出现了，探索与开发如何影响结果。如果汽车不动，我们可以提高学习率，而如果汽车的行为非常随机，我们可能需要降低学习率。</p><h1 id="5a12" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">结论</h1><p id="6e78" class="pw-post-body-paragraph it iu hh iv b iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm lb jo jp jq ha bi translated">强化学习领域是一个在没有任何数据的情况下寻找最优解的迭代过程。除了Deep Mind在Atari游戏中的成功之外，RL的进步，包括深度学习的使用，无疑有助于机器人技术、自动驾驶汽车和无人机的发展。</p><p id="0445" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">你可以在谷歌上找到许多关于这方面的高级主题的链接，但是主要的基本原理是一样的。</p><p id="16f2" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">你可以在这里找到GitHub代码<a class="ae jy" href="https://github.com/kmair/mountain-car" rel="noopener ugc nofollow" target="_blank"/>！</p><p id="fc86" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">注意:</strong>交叉张贴在我的<a class="ae jy" href="https://kanishkmair.com/ml/basics-of-q-learning/" rel="noopener ugc nofollow" target="_blank">博客</a>中，在那里你可以关注我关于人工智能和人工智能的文章</p></div></div>    
</body>
</html>