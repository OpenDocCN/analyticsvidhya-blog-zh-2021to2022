<html>
<head>
<title>Neural networks cost and gradient calculation deep dive 104</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络成本和梯度计算深潜104</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/cost-and-gradient-calculation-in-neural-networks-deep-dive-104-2e16f26ce3f3?source=collection_archive---------9-----------------------#2021-07-09">https://medium.com/analytics-vidhya/cost-and-gradient-calculation-in-neural-networks-deep-dive-104-2e16f26ce3f3?source=collection_archive---------9-----------------------#2021-07-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/8f5aadfd095287cc57a68ea4769a1d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7XryJtBBZPn25PkKNJWGWQ.jpeg"/></div></div></figure><p id="c2ed" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在理解了向前和向后传播之后，让我们进入<strong class="ir hi">计算成本和梯度</strong>。这是神经网络的重要组成部分。</p><p id="5c8b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是我的神经网络系列的第2部分。如果你只是想要代码，欢迎从<a class="ae jn" href="https://shaun-enslin.medium.com/explaining-neural-networks-101-a36356113cbd" rel="noopener">第一部分</a>开始，或者跳到<a class="ae jn" href="https://shaun-enslin.medium.com/implementing-neural-networks-in-matlab-105-6b71c5872b3c" rel="noopener">第五部分</a>。</p><p id="2a65" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，为了执行梯度下降或成本优化，我们需要编写一个成本函数，它执行:</p><ol class=""><li id="f639" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><a class="ae jn" href="https://shaun-enslin.medium.com/forward-propagation-deep-dive-102-bbeabe4d2fb2" rel="noopener">正向传播</a></li><li id="8d65" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><a class="ae jn" href="https://shaun-enslin.medium.com/backward-propagation-deep-dive-103-60390714d2b0" rel="noopener">反向传播</a></li><li id="f0b1" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><a class="ae jn" href="https://shaun-enslin.medium.com/cost-and-gradient-calculation-in-neural-networks-deep-dive-104-2e16f26ce3f3" rel="noopener">计算成本&amp;坡度</a></li></ol><p id="26e2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这篇文章中，我们将处理(3)和(4)。你可以点击上面的链接深入了解前进/后退道具。</p><p id="c855" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">提醒一下，下面是我们的神经网络，我们使用了前向和后向传播，计算了Z，A和s。</p><figure class="kd ke kf kg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kc"><img src="../Images/af21fb622be50d5eb366a69587902e2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_hGDZ3aZAVJEHPv6npui5w.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图1</figcaption></figure><h1 id="2db1" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">成本计算</h1><p id="8e77" class="pw-post-body-paragraph ip iq hh ir b is lj iu iv iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm ha bi translated">向前传播后，我们计算了A3(如图1所示)。我们可以认为A3是我们的特征(x)和这组权重的一个假设。因此，让我们继续<strong class="ir hi">计算它的成本，看看这些权重的表现如何</strong>。</p><p id="0dff" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们的第一步是计算一个可以用来调整我们的成本的惩罚。如果你想要一个关于正规化的解释，那么看看这篇<a class="ae jn" rel="noopener" href="/geekculture/logistics-regression-regularisation-2-3-4a0d8b85564c">文章</a>。</p><pre class="kd ke kf kg fd lo lp lq lr aw ls bi"><span id="cc59" class="lt km hh lp b fi lu lv l lw lx">% calculate penalty without theta0,<br/>p = sum(sum(Theta1(:, 2:end).², 2)) + sum(sum(Theta2(:, 2:end).², 2));</span></pre><p id="7601" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们有了惩罚，我们可以计算成本并应用惩罚。稍后，<strong class="ir hi">成本优化</strong>函数将使用该值来得出我们可以用于预测的最佳权重。</p><pre class="kd ke kf kg fd lo lp lq lr aw ls bi"><span id="e49f" class="lt km hh lp b fi lu lv l lw lx">% Calculate the cost of our forward prop<br/>J = sum(sum(-yv .* log(a3) — (1 — yv) .* log(1-a3), 2))/m + lambda*p/(2*m);</span></pre><h1 id="9071" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">梯度</h1><p id="3ae6" class="pw-post-body-paragraph ip iq hh ir b is lj iu iv iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm ha bi translated">对于成本优化，我们还需要反馈这组特定权重的梯度。图2显示了梯度一旦被绘制出来。对于输入到我们的成本函数的权重集，这将是绘制线的梯度。</p><figure class="kd ke kf kg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ly"><img src="../Images/a53d55205c603242bb3614278c9378ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KXhoClXogcckzwxfvIFReQ.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图2</figcaption></figure><p id="c5fc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们有了这样的理解，让我们涵盖计算。使用矩阵乘法，我们首先使用S2和A1计算δ。图3显示了每个θ的这个增量。</p><pre class="kd ke kf kg fd lo lp lq lr aw ls bi"><span id="2289" class="lt km hh lp b fi lu lv l lw lx">% Calculate DELTA’s (accumulated deltas)<br/>delta_1 = (s2'*a1);<br/>delta_2 = (s3'*a2);</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lz"><img src="../Images/4159420a3542c98fdd81390c417d6abc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BMHW0WNlUIs_ILaO-cayKg.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图3</figcaption></figure><p id="afea" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">同样，我们想要正则化我们的梯度，因此需要计算一个惩罚。</p><pre class="kd ke kf kg fd lo lp lq lr aw ls bi"><span id="3645" class="lt km hh lp b fi lu lv l lw lx">% calculate regularized gradient, replace 1st column with zeros<br/>p1 = (lambda/m)*[zeros(size(Theta1, 1), 1) Theta1(:, 2:end)];<br/>p2 = (lambda/m)*[zeros(size(Theta2, 1), 1) Theta2(:, 2:end)];</span></pre><p id="3b02" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后，我们计算每个θ的梯度并应用权重。图4显示了梯度</p><pre class="kd ke kf kg fd lo lp lq lr aw ls bi"><span id="1fec" class="lt km hh lp b fi lu lv l lw lx">% gradients / partial derivitives<br/>Theta1_grad = delta_1./m + p1;<br/>Theta2_grad = delta_2./m + p2;</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ma"><img src="../Images/af2f99ff81ada62ba011fa9a55db5a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uwHKT8iofA4YbvN81A6ntw.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图4</figcaption></figure><p id="fe40" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然而，成本优化函数不知道如何处理2θ，所以让我们把它们展开成一个向量，结果如图5所示。</p><pre class="kd ke kf kg fd lo lp lq lr aw ls bi"><span id="23af" class="lt km hh lp b fi lu lv l lw lx">% Unroll gradients<br/>grad = [Theta1_grad(:) ; Theta2_grad(:)];</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es mb"><img src="../Images/c9f77b6ac5436b7609608290133186d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*bMl7d4VAQ7SX4g-Q3wejsg.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图5</figcaption></figure><h1 id="a3c3" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">结论</h1><p id="3c70" class="pw-post-body-paragraph ip iq hh ir b is lj iu iv iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm ha bi translated">学习了本系列的4个部分之后，您现在可以在第5部分将它们放在一起了。</p></div></div>    
</body>
</html>