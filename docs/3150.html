<html>
<head>
<title>Eigenvalue and Eigenvector — Computation hidden beneath algorithm but never shows up</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征值和特征向量——计算隐藏在算法之下，但永远不会显示出来</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/eigenvalue-and-eigenvector-computation-hidden-beneath-algorithm-but-never-shows-up-10278975169b?source=collection_archive---------2-----------------------#2021-06-12">https://medium.com/analytics-vidhya/eigenvalue-and-eigenvector-computation-hidden-beneath-algorithm-but-never-shows-up-10278975169b?source=collection_archive---------2-----------------------#2021-06-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/48134be9ea7eb218b01ee759b6dd2451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*ycM1QkfiiiMhi_XRT1T2Gg.png"/></div></figure><p id="5e6a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">大多数有数据科学和机器学习背景的人都会熟悉特征值和特征向量，以及它们在不同算法中的大量使用。</p><p id="94af" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">主题地图-</p><p id="d3b2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">1.特征值和特征向量的应用范围</p><p id="c6fe" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">2.特征值和特征向量的使用</p><p id="87fb" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">3.特征值和特征向量的计算</p><p id="b35a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">4.展示RStudio上的示例。</p><p id="c7af" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">因此，我不会在这里触及特征向量和特征值的定义，但肯定会尝试向你展示它们是如何以最简单的方式计算的。</p><p id="91f1" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">但在此之前，让我告诉你机器学习的不同领域，这些可以主要用于。</p><p id="1aa6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">特征值和特征向量是许多众所周知的机器学习算法的构建块。</p><p id="2afa" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">1.它们被广泛用于自然语言处理中的潜在语义分析(LSA)，</p><p id="a0a7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">2.用于SVD(奇异值分解)的推荐引擎中，</p><p id="2110" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">3.用于文本摘要，</p><p id="dba7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">4.用于特征选择和降维，包括PCA(主成分分析)</p><p id="17ef" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">5.用于计算机视觉，包括图像分割、人脸识别、光谱聚类、三维重建等。</p><p id="8cab" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">所有上面列出的领域使用特征值和特征向量作为他们的核心逻辑。</p><p id="5939" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">因此，让我们通过一个示例来深入了解这两者是如何手动计算的。</p><p id="f97e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">让我们从一个基于用户评级的产品推荐系统的例子开始。该表描述了用户对产品的评分，评分范围为-3到3。其中3表示优秀，-3表示较差</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es jj"><img src="../Images/d511461b8eeabc9c658eb68383632f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KVTMHzXOxfRbPymMObNzPA.png"/></div></div></figure><p id="a43a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">凡用户未对产品进行评级的地方，该列用0填充。</p><p id="8ce2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这是一个小维度的矩阵，但是想象一下，如果我们有一个非常大维度的矩阵，比如100，000行和100列，那么肯定会带来许多挑战。如此大的矩阵最终会占用磁盘上相当大的空间。此外，模型在数据上运行和自我训练变得非常耗时。让我们面临更多挑战的是，理解和可视化如此多维度的数据可能会变得更加困难。因此，理想的情况将是应用一种机制来将该矩阵转换/缩减为具有较小维度的较小空间。</p><p id="b61f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="js">特征值和特征向量帮助我们减少维度空间，同时确保</em>大部分关键信息被保留<em class="js">。</em></p><p id="757a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这个3×3矩阵是基于上表描述的:</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es jt"><img src="../Images/00e754bb1d4d00cc3b104a2f0dbaa712.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*ge_hoYwCkXYVO4NGGYM0xA.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">作者图片</figcaption></figure><p id="49ea" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">该矩阵是3×3矩阵，即n×m矩阵，其中n是行，m是列。</p><p id="4bea" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="js">为了进行这样的变换，原始矩阵A乘以一个向量v。矩阵A乘以一个向量v计算出一个新的向量，该向量称为变换后的向量。</em></p><p id="ed27" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="js">变换后的向量只是向量v的缩放形式，即标量(</em> λ)乘以<em class="js">向量v。因此，向量v被称为原始矩阵的特征向量，而标量</em> λ被称为特征值。</p><p id="99b0" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">因此，利用这些特征向量，可以在没有太多信息损失的情况下表示大维矩阵。</p><p id="9039" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="js">我们不必每次都手动计算特征值和特征向量，有算法可以做到这一点，但了解内部工作原理会让我们更有信心使用算法并更好地理解它。</em></p><p id="1eeb" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="js">因此它的数学部分:</em></p><p id="eebe" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> Av = λv </strong> <strong class="in hi">其中A =矩阵，v =特征向量，λ =特征值</strong></p><p id="095a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="js">这意味着矩阵A和向量v可以用标量</em> <strong class="in hi"> λ </strong> <em class="js">和向量v</em>代替</p><p id="4285" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">为了进一步计算，我们需要熟悉两个概念。</p><ol class=""><li id="7892" class="jy jz hh in b io ip is it iw ka ja kb je kc ji kd ke kf kg bi translated"><em class="js">一个矩阵的行列式=对角元素相乘，然后再相减得到。一旦我们做了计算，下面就清楚了。例如——</em></li></ol><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es kh"><img src="../Images/a8758b7135f361283e8c7bcc436073c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NvobnuSfWJph28N8_g2DKg.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">作者图片</figcaption></figure><p id="8e2a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"><em class="js">a(e * I—f * h)—b(d * I—f * g)+c(d * h—e * g)= 0</em></strong></p><p id="b874" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="js"> 2。单位矩阵——它是一个对角线为1且所有其他元素都为0的矩阵。例如——</em></p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es ki"><img src="../Images/47960a2843e9f237d0a868166c5e12ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/1*kba2m3aqOqXv10SaEyzb3w.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">作者图片</figcaption></figure><p id="7347" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">此外，<strong class="in hi"> Av = λv </strong>可以表示为:</p><p id="8001" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> Av — λv </strong> <strong class="in hi"> = 0，即| A — λ * I | = 0 </strong></p><p id="72c9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">随后，<strong class="in hi">行列式</strong> ( <strong class="in hi"> A — λ * I ) = 0 </strong></p><p id="7765" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="js">上式，</em><strong class="in hi">Av—λv</strong><strong class="in hi">= 0</strong><em class="js">也可以写成</em></p><p id="1fd7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> A *特征向量—特征值*特征向量= 0 </strong></p><p id="3483" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="js">更进一步，</em></p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es kj"><img src="../Images/1967c709a4c3b26af98ab16714360152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*HSMJ2jI__T9DdIH0UA8x1Q.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">作者图片</figcaption></figure><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es kk"><img src="../Images/33201b32d6b37caa1999be5ac3dfaf05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JMAesawkhBg_OwFspkl9yg.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">作者图片</figcaption></figure><p id="5b32" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在可以计算相关特征值的特征向量。</p><p id="54ba" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">对于我们的计算，考虑λ = 2。</p><p id="d142" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> | A — λ * I | =行列式</strong> ( <strong class="in hi"> A — λ * I ) = 0 </strong></p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es kl"><img src="../Images/2bb0dc555c953684a4af337d136862f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GYlA4C5kN8HnIH5aNFwaDw.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">作者图片</figcaption></figure><p id="b115" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="js">现在，</em>这可以通过高斯消去法找到特征向量来简化。</p><p id="ac45" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">行缩减的第一步是从行2中减去行1。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es km"><img src="../Images/2a6c5ff07eb80e0e9579784520f6859c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dNBfeemCYrVb0b1r6r09aA.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">作者图片</figcaption></figure><p id="e4ce" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">最终的简化矩阵如下所示。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es kn"><img src="../Images/d6bcc8ac1eaf91668ce3c5da0173e558.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*JUICn9DtWsKLIH4qNhS2Gw.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">作者图片</figcaption></figure><p id="0ed8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">随着矩阵的简化，我们可以通过求解下面的方程找到λ = 2的相关特征向量，该方程本身是从上面表示的简化矩阵导出的。</p><p id="e8fc" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">注意:—第三行的所有元素都为0。因此，只有行1和行2被考虑用于导出λ1、λ2和λ3值。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es ko"><img src="../Images/57a10c6f366a12c82514436e8850d409.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*oW-9zWTuRXLqxcw_Vw8tgA.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">作者图片</figcaption></figure><p id="a681" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">因此，与λ = 2相关的特征向量为</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es kp"><img src="../Images/6054c0a19312bbbe601a3bdb98ca480d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*umsQogPMEXeTxoSO9RyvYw.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">作者图片</figcaption></figure><figure class="jk jl jm jn fd ii er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es kq"><img src="../Images/3435d294589289cd642a695ebcc43877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i8MROS0Fbg0MiZKAjgxWWw.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">作者图片</figcaption></figure><p id="3f07" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">类似地，你可以计算另外两个特征值的特征向量。</p><p id="a57c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在，我将使用相同的矩阵a，在R中得到相同的结果。</p><p id="e7cc" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">下面是给出相同结果的R代码。</p><p id="0830" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">#读取矩阵，A<br/>A&lt;-as . Matrix(data . frame(c(1，-1，0)，c(1，2，1)，c(-2，1，-1))</p><p id="c3dc" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">#显示矩阵<br/> A</p><p id="0059" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">#计算特征值和特征向量<br/> e &lt; -特征值(A)</p><p id="72db" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">#显示特征值<br/>e $值</p><p id="48d1" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">#显示特征向量<br/> e$vectors</p><p id="c0c9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">因此，我总结了特征向量和特征值的计算。</p></div></div>    
</body>
</html>