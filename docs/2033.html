<html>
<head>
<title>How to Improve Naive Bayes?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何改进朴素贝叶斯？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-improve-naive-bayes-9fa698e14cba?source=collection_archive---------0-----------------------#2021-04-02">https://medium.com/analytics-vidhya/how-to-improve-naive-bayes-9fa698e14cba?source=collection_archive---------0-----------------------#2021-04-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="5c58" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">朴素贝叶斯指南</h2><div class=""/><div class=""><h2 id="1d72" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">第3节:用Python调优模型</h2></div><p id="60d1" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated"><em class="kc">参考</em><a class="ae kd" href="https://kopaljain95.medium.com/how-to-implement-naive-bayes-24e92f2b49f3" rel="noopener"><strong class="ji hs">T12】如何实现朴素贝叶斯？第2节:在继续… 之前，用Python  </strong> </a> <em class="kc">构建模型</em></p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ke"><img src="../Images/b72d93681e5262a9801b40bd4b2874e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Di4e9lH_xnbkyOdVZv4TeA.png"/></div></div></figure><blockquote class="kq kr ks"><p id="d339" class="jg jh kc ji b jj jk is jl jm jn iv jo kt jq jr js ku ju jv jw kv jy jz ka kb hb bi translated"><strong class="ji hs"><em class="hi">【10】定义网格搜索参数</em> </strong></p></blockquote><pre class="kf kg kh ki fd kw kx ky kz aw la bi"><span id="b448" class="lb lc hi kx b fi ld le l lf lg">param_grid_nb = {<br/>    'var_smoothing': np.logspace(0,-9, num=100)<br/>}</span></pre><ul class=""><li id="96a1" class="lh li hi ji b jj jk jm jn jp lj jt lk jx ll kb lm ln lo lp bi translated"><code class="du lq lr ls kx b">var_smoothing</code>是一种稳定性计算，用于加宽(或平滑)曲线，因此考虑更多远离分布均值的样本。在这种情况下，np.logspace返回在对数标度上均匀分布的数字，从0开始，到-9结束，并生成100个样本。</li></ul><p id="9c25" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di"> W </span> hy这一步:设置所选择的参数用来寻找最佳组合。通过引用<a class="ae kd" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB" rel="noopener ugc nofollow" target="_blank"> sklearn.naive_bayes。GaussianNB </a>文档，您可以找到一个完整的参数列表，其中包含可用于网格搜索功能的描述。</p><blockquote class="kq kr ks"><p id="6de3" class="jg jh kc ji b jj jk is jl jm jn iv jo kt jq jr js ku ju jv jw kv jy jz ka kb hb bi translated"><strong class="ji hs"><em class="hi">【11】超参数调整使用训练数据</em> </strong></p></blockquote><pre class="kf kg kh ki fd kw kx ky kz aw la bi"><span id="00c1" class="lb lc hi kx b fi ld le l lf lg">from sklearn.naive_bayes import GaussianNB<br/>from sklearn.model_selection import GridSearchCV</span><span id="820b" class="lb lc hi kx b fi mc le l lf lg">nbModel_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10, n_jobs=-1)</span><span id="fc0a" class="lb lc hi kx b fi mc le l lf lg">nbModel_grid.fit(X_train, y_train)</span><span id="621f" class="lb lc hi kx b fi mc le l lf lg">print(nbModel_grid.best_estimator_)</span><span id="86ea" class="lb lc hi kx b fi mc le l lf lg">...</span><span id="bd06" class="lb lc hi kx b fi mc le l lf lg"><em class="kc">Fitting 10 folds for each of 100 candidates, totalling 1000 fits</em></span><span id="7e9f" class="lb lc hi kx b fi mc le l lf lg"><em class="kc">GaussianNB(priors=None, var_smoothing=1.0)</em></span></pre><p id="adeb" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">注意:由于<code class="du lq lr ls kx b">cv</code>被定义为10，有100个候选值(<code class="du lq lr ls kx b">var_smoothing</code>有100个定义的参数)，所以总拟合数为1000。因此，总配合数的计算→ 10 x [100] = 1000。</p><ul class=""><li id="7d5f" class="lh li hi ji b jj jk jm jn jp lj jt lk jx ll kb lm ln lo lp bi translated"><code class="du lq lr ls kx b">estimator</code>是感兴趣的机器学习模型，假设该模型具有评分功能；在这种情况下，分配的模型是GaussianNB()。</li><li id="3347" class="lh li hi ji b jj md jm me jp mf jt mg jx mh kb lm ln lo lp bi translated"><code class="du lq lr ls kx b">param_grid</code>是一个字典，以参数名(字符串)作为键，以参数设置列表作为值进行尝试；这使得能够搜索任何参数设置序列。</li><li id="9d47" class="lh li hi ji b jj md jm me jp mf jt mg jx mh kb lm ln lo lp bi translated"><code class="du lq lr ls kx b">verbose</code>是详细度:越高，消息越多；在这种情况下，它被设置为1。</li><li id="8329" class="lh li hi ji b jj md jm me jp mf jt mg jx mh kb lm ln lo lp bi translated"><code class="du lq lr ls kx b">cv</code>是交叉验证生成器还是iterable，在本例中，有10重交叉验证。</li><li id="a767" class="lh li hi ji b jj md jm me jp mf jt mg jx mh kb lm ln lo lp bi translated"><code class="du lq lr ls kx b">n_jobs</code>是并发运行的工作线程的最大数量；在这种情况下，它被设置为-1，这意味着使用了所有的CPU。</li></ul><p id="6fc5" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di"> W </span>通过这一步:找到<strong class="ji hs">超参数</strong>的最佳组合，使预定义的损失函数最小化，以给出更好的结果。</p><blockquote class="kq kr ks"><p id="9fca" class="jg jh kc ji b jj jk is jl jm jn iv jo kt jq jr js ku ju jv jw kv jy jz ka kb hb bi translated"><strong class="ji hs"><em class="hi">【12】对测试数据进行预测</em> </strong></p></blockquote><pre class="kf kg kh ki fd kw kx ky kz aw la bi"><span id="94d0" class="lb lc hi kx b fi ld le l lf lg">y_pred = nbModel_grid.predict(X_test)</span><span id="5a49" class="lb lc hi kx b fi mc le l lf lg">print(y_pred)</span><span id="da21" class="lb lc hi kx b fi mc le l lf lg">...</span><span id="6fe7" class="lb lc hi kx b fi mc le l lf lg"><em class="kc">[0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0]</em></span></pre><p id="0d1d" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di"> W </span>通过这一步:获得对测试数据的模型预测，以评估模型的准确性和效率。</p><blockquote class="kq kr ks"><p id="f935" class="jg jh kc ji b jj jk is jl jm jn iv jo kt jq jr js ku ju jv jw kv jy jz ka kb hb bi translated"><strong class="ji hs"><em class="hi">【13】数值分析</em> </strong></p></blockquote><pre class="kf kg kh ki fd kw kx ky kz aw la bi"><span id="5092" class="lb lc hi kx b fi ld le l lf lg">from sklearn.metrics import confusion_matrix<br/>print(confusion_matrix(y_test, y_pred), ": is the confusion matrix")</span><span id="7439" class="lb lc hi kx b fi mc le l lf lg">from sklearn.metrics import accuracy_score<br/>print(accuracy_score(y_test, y_pred), ": is the accuracy score")</span><span id="5d52" class="lb lc hi kx b fi mc le l lf lg">from sklearn.metrics import precision_score<br/>print(precision_score(y_test, y_pred), ": is the precision score")</span><span id="81c8" class="lb lc hi kx b fi mc le l lf lg">from sklearn.metrics import recall_score<br/>print(recall_score(y_test, y_pred), ": is the recall score")</span><span id="c39d" class="lb lc hi kx b fi mc le l lf lg">from sklearn.metrics import f1_score<br/>print(f1_score(y_test, y_pred), ": is the f1 score")</span><span id="a757" class="lb lc hi kx b fi mc le l lf lg">...</span><span id="c99f" class="lb lc hi kx b fi mc le l lf lg"><em class="kc">[[81 27]<br/> [19 81]] : is the confusion matrix <br/><br/>0.7788461538461539 : is the accuracy score<br/>0.75 : is the precision score<br/>0.81 : is the recall score<br/>0.7788461538461539 : is the f1 score</em></span></pre><p id="11f2" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">注意:使用混淆矩阵，可以提取真阳性、假阳性、假阴性和真阴性值，这将有助于计算准确度分数、精确度分数、回忆分数和f1分数:</p><ul class=""><li id="e2e9" class="lh li hi ji b jj jk jm jn jp lj jt lk jx ll kb lm ln lo lp bi translated"><strong class="ji hs">真正</strong> = 81</li><li id="3402" class="lh li hi ji b jj md jm me jp mf jt mg jx mh kb lm ln lo lp bi translated"><strong class="ji hs">假阳性</strong> = 27</li><li id="44da" class="lh li hi ji b jj md jm me jp mf jt mg jx mh kb lm ln lo lp bi translated"><strong class="ji hs">假阴性</strong> = 19</li><li id="38b8" class="lh li hi ji b jj md jm me jp mf jt mg jx mh kb lm ln lo lp bi translated"><strong class="ji hs">真负值</strong> = 81</li></ul><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es mi"><img src="../Images/26e4f1261eb17819c55ee2fbef5092cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1rB5CecD6UPllcQhtdIx6Q.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">准确度、精确度、召回率和F1的方程式。</figcaption></figure><p id="411e" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di"> W </span>通过这一步:评估调优分类模型的性能。如您所见，通过调整在第2节中创建的基本高斯朴素贝叶斯模型，准确度、精确度、召回率和F1分数都得到了提高。</p></div><div class="ab cl mn mo gp mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="hb hc hd he hf"><div class="kf kg kh ki fd mu"><a href="https://github.com/kopaljain95/import-data.science-classification/blob/main/NaiveBayes%5B4%5D/NaiveBayes.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab dw"><div class="mw ab mx cl cj my"><h2 class="bd hs fi z dy mz ea eb na ed ef hr bi translated">kopaljain 95/import-data . science-分类</h2><div class="nb l"><h3 class="bd b fi z dy mz ea eb na ed ef dx translated">在GitHub上创建一个帐户，为kopaljain 95/import-data . science-classification的开发做出贡献。</h3></div><div class="nc l"><p class="bd b fp z dy mz ea eb na ed ef dx translated">github.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni ko mu"/></div></div></a></div><p id="7fcf" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated"><em class="kc">接下来— </em> <a class="ae kd" href="https://kopaljain95.medium.com/why-use-naive-bayes-a56cbae55181" rel="noopener"> <em class="kc">为什么要用朴素贝叶斯？第4部分:评估模型权衡</em> </a> <em class="kc"> … </em></p></div></div>    
</body>
</html>