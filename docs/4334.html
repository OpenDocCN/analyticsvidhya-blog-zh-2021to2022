<html>
<head>
<title>KL Divergence to Find the Best</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">KL发散寻找最佳</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/kl-divergence-to-find-the-best-5c2d38560b13?source=collection_archive---------6-----------------------#2021-09-23">https://medium.com/analytics-vidhya/kl-divergence-to-find-the-best-5c2d38560b13?source=collection_archive---------6-----------------------#2021-09-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/3402e8e2858f5c78cdda2fbc8188fe98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GC5qFGpCOWotA5pzA6EepA.png"/></div></div></figure><p id="8fb8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好吧，让我告诉你，我不知道KL散度，直到我参加了一个课程。因为对我来说这是一个相当复杂的概念，所以我想写一下！</p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="114c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我看了一些关于KL发散的文章。根据我的经验，用语言表达的最好方式就是举例。我将继续引用<a class="ae ju" href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" rel="noopener ugc nofollow" target="_blank">这篇令人惊叹的文章</a>中的一段话(我鼓励你去读它)。有时，我们可能希望将我们的数据分布呈现为更简单的分布。</p><p id="d26a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">想象一下，美国宇航局有一个新的任务，派遣太空科学家到一个新的星球观察咬人的蠕虫。科学家们发现蠕虫有同样数量的牙齿，但随着时间的推移，它们会因为<em class="jv">咀嚼</em>而失去牙齿。不管怎样？所以，他们把他们的数据放在一起，这就是他们的任务结束:</p><figure class="jx jy jz ka fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es jw"><img src="../Images/d54b5ff4468ce51ce3b13f6806714c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PyNmQ1npYIk4oi7g"/></div></div><figcaption class="kb kc et er es kd ke bd b be z dx translated">摘自<a class="ae ju" href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" rel="noopener ugc nofollow" target="_blank">此处</a>。</figcaption></figure><p id="04fe" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一个问题。他们必须将这些数据发送到地球进行更多的检查。但是它们离地球很远。发送这些数据会很昂贵。他们能做什么？他们可以减少参数的数量！但是，要做到这一点，他们必须找到最佳分布来拟合这些数据，以减少信息损失。</p><p id="9988" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">他们可以尝试均匀分布。这个分布需要唯一参数是可能的值。数据有11个可能的值。</p><p id="408b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> <em class="jv"> P_uniform = 1 /可能值= 1 / 11 = 0.0909 </em> </strong></p><p id="2365" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">每个蠕虫都有相同的统一可能性。</p><figure class="jx jy jz ka fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es jw"><img src="../Images/cdc6cf9e1aef3a714101038934c62e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1PjNo3SsaR6Y4uCM"/></div></div><figcaption class="kb kc et er es kd ke bd b be z dx translated">取自<a class="ae ju" href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" rel="noopener ugc nofollow" target="_blank">此处</a>。</figcaption></figure><p id="a52d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们没有多样性。显然，均匀分布不是他们数据的最佳选择。</p><p id="74e3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">他们可以试试二项分布。我们可以定义二项分布的均值和方差。</p><p id="633a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> <em class="jv">均值= np <br/>方差= np(1-p) </em> </strong></p><p id="bc47" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">他们可以使用二项分布的平均值。<em class="jv"> n </em>是从蠕虫种群中观察到的最大齿数，为10。<em class="jv">表示</em>是蜗杆的预期齿数。我们可以这样计算:</p><figure class="jx jy jz ka fd ii er es paragraph-image"><div class="er es kf"><img src="../Images/de9a8d5ea810cf174535a63d9a1be411.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*LTVWdixuLSjTzyoKu0ceGQ.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx translated">摘自<a class="ae ju" href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8" rel="noopener" target="_blank">此处</a>。</figcaption></figure><p id="5d10" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"><em class="jv">5.44 = 10×p</em></strong></p><p id="3cc6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> <em class="jv"> p = 0.544 </em> </strong></p><figure class="jx jy jz ka fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es jw"><img src="../Images/dc30edae4522f648c3aedb45dc82fa32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*s9E96XKbnjCWbCQJ"/></div></div><figcaption class="kb kc et er es kd ke bd b be z dx translated">摘自<a class="ae ju" href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure><p id="a08a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好像也不符合我们原来的分布。那他们该怎么办？他们如何找到最适合他们原始分布的分布呢？</p><p id="3df1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们稍微分解一下。</p><p id="bdda" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">他们想要的是用较少的参数发送数据以降低成本，并找到最佳分布以减少信息的损失。</p><p id="3dca" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我更清楚了。为了理解我们设法减少信息，我们需要知道数据最初有多少信息。</p><h2 id="989c" class="kg kh hh bd ki kj kk kl km kn ko kp kq ja kr ks kt je ku kv kw ji kx ky kz la bi translated">熵</h2><p id="274f" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">熵是<em class="jv">信息论</em>中的一个重要度量。信息论的主要目标是量化数据中的信息量。</p><p id="4589" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我想引用一个我在网上找到的关于熵的很好的定义:</p><blockquote class="lg"><p id="756d" class="lh li hh bd lj lk ll lm ln lo lp jm dx translated">熵允许我们对生活中最紧迫的问题之一做出精确的陈述和进行计算:不知道事情会如何发展。换句话说，熵是不确定性的一种T2度量。</p></blockquote><p id="b371" class="pw-post-body-paragraph ip iq hh ir b is lq iu iv iw lr iy iz ja ls jc jd je lt jg jh ji lu jk jl jm ha bi translated">当然还有一个也是唯一的等式:</p><figure class="jx jy jz ka fd ii er es paragraph-image"><div class="er es lv"><img src="../Images/e693dc0923cb45cbcb15d0fdbcad0b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YF9I0f-DAOaSMVYF.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx translated"><a class="ae ju" href="https://towardsdatascience.com/entropy-is-a-measure-of-uncertainty-e2c000301c2c" rel="noopener" target="_blank">你可以说香农熵</a></figcaption></figure><p id="0986" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们使用<em class="jv"> log </em> 2进行计算，我们可以将熵解释为“对信息进行编码所需的最小位数”。在这种情况下，信息将是给定我们的经验分布的牙齿计数的每个观察值。给定我们观察到的数据，我们的概率分布具有3.12比特的熵。位数告诉我们，平均来说，我们需要多少位来编码我们在单一情况下观察到的齿数的下限。[1]</p><p id="d5b5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们的目标是最小化不确定性(熵),我们应该远离均匀分布。因为对于<strong class="ir hi">均匀</strong>分布，所有结果具有相同的概率(你可以从上面看到)。在我们的例子中，对于蠕虫的牙齿计数概率，它不能正确地代表我们的数据。</p><p id="4442" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">哪个更好的最好测试是询问哪个分布保存了来自我们原始数据源的最多信息。这就是Kullback-Leibler散度 的用武之地。</p><h1 id="21b1" class="lw kh hh bd ki lx ly lz km ma mb mc kq md me mf kt mg mh mi kw mj mk ml kz mm bi translated">KL-散度</h1><blockquote class="mn mo mp"><p id="622e" class="ip iq jv ir b is it iu iv iw ix iy iz mq jb jc jd mr jf jg jh ms jj jk jl jm ha bi translated">Kullback-Leibler散度只是对我们的熵公式的一个小小的修改。除了概率分布p，我们还加入了近似分布q。</p></blockquote><p id="2bdc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">设p(x)和q(x)是一个离散随机变量x的两个概率分布，即p(x)和q(x)之和均为1，且对于x中的任意一个x，p(x) &gt; 0，q(x)&gt; 0 . D _ KL(p(x)，q(x))定义为:[4]</p><figure class="jx jy jz ka fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/1795735ea350f060d6ad381ea1a68273.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*9nIXCopBa8luuqh_UbwBbg.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx translated">摘自<a class="ae ju" href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure><p id="cf18" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">KL散度(T1)是一个T2概率分布(T3)与另一个参考概率分布(T3)不同的度量。</p><blockquote class="lg"><p id="56ec" class="lh li hh bd lj lk ll lm ln lo lp jm dx translated"><em class="mu">【p | | g】</em>中的两条垂直线是指q分布不同于p分布。</p></blockquote><p id="1912" class="pw-post-body-paragraph ip iq hh ir b is lq iu iv iw lr iy iz ja ls jc jd je lt jg jh ji lu jk jl jm ha bi translated">你可能会得出这样的结论，因为<em class="jv"> KL散度度量两个分布之间的距离</em>，所以它是一个<strong class="ir hi"> <em class="jv">距离</em> </strong>度量。但这并不是因为它的<strong class="ir hi"> <em class="jv">不是一个</em> </strong> <strong class="ir hi"> <em class="jv">公制度量</em> </strong>。</p><p id="f8a3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">KL散度不<strong class="ir hi">对称</strong>:从p(x)到q(x)的KL一般和从q(x)到p(x)的KL不一样。</p><p id="a7d7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">DKL( <em class="jv"> p </em> || <em class="jv"> q </em>)是一个非负测度。DKL(<em class="jv">p</em>|<em class="jv">q</em>)≥0且DKL(<em class="jv">p</em>|<em class="jv">q</em>)= 0当且仅当<em class="jv"> p </em> = <em class="jv"> q </em>。</p><h1 id="2bf1" class="lw kh hh bd ki lx ly lz km ma mb mc kq md me mf kt mg mh mi kw mj mk ml kz mm bi translated"><strong class="ak">实施</strong></h1><p id="042e" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">下面的代码是实现两个高斯分布之间的KL散度。对于这种实现，我们需要两个高斯分布样本。</p><figure class="jx jy jz ka fd ii"><div class="bz dy l di"><div class="mv mw l"/></div></figure></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="2b21" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好吧，过了一段时间就这样了。我将写更多关于高级统计概念的内容。等着吧！</p><figure class="jx jy jz ka fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/3402e8e2858f5c78cdda2fbc8188fe98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GC5qFGpCOWotA5pzA6EepA.png"/></div></div></figure><h1 id="5a31" class="lw kh hh bd ki lx ly lz km ma mb mc kq md me mf kt mg mh mi kw mj mk ml kz mm bi translated">参考</h1><p id="cb5c" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated"><a class="ae ju" href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" rel="noopener ugc nofollow" target="_blank">【1】</a>库尔贝克-莱布勒散度解释</p><p id="1884" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae ju" href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8" rel="noopener" target="_blank">【2】</a>光论数学机器学习直观指南理解KL发散</p><p id="5aab" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">熵是对不确定性的一种度量</p><p id="64dd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae ju" href="http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf" rel="noopener ugc nofollow" target="_blank">【4】</a>KL-散度</p><p id="5ec8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae ju" href="https://github.com/inzva/AI-Labs-Joint-Program/blob/main/inzva%20x%20METU%20ImageLab%20Joint%20Program/Week%202%20-%20Autoregressive%20Models%2C%20Maximum%20Likelihood%20Estimation/inzva_x_METU_ImageLab_Joint_Program_Week_2_Solutions_Notebook.ipynb" rel="noopener ugc nofollow" target="_blank">【5】</a>AI实验室联合计划</p></div></div>    
</body>
</html>