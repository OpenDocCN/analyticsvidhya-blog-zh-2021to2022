<html>
<head>
<title>Random Forest Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林分类器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/random-forest-classifier-d8666aa34a55?source=collection_archive---------21-----------------------#2021-02-08">https://medium.com/analytics-vidhya/random-forest-classifier-d8666aa34a55?source=collection_archive---------21-----------------------#2021-02-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/12bf8b56a000c96a23da4877dc9566dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*R5zylsU8gVg2wAh5"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">森林|照片由<a class="ae it" href="https://unsplash.com/@jaymantri?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">杰伊·曼特里</a>在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="b0ae" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">本课使用的笔记本:<a class="ae it" href="https://www.kaggle.com/tanishsawant2002/random-forest-classifier" rel="noopener ugc nofollow" target="_blank">点击此处</a>(如果你喜欢，请投赞成票😉)</p><blockquote class="js jt ju"><p id="58a1" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">随机森林是一种元估计器，它在数据集的各个子样本上拟合多个决策树分类器，并使用平均来提高预测精度和控制过拟合。如果<code class="du jz ka kb kc b">bootstrap=True</code>(默认)，子样本大小由<code class="du jz ka kb kc b">max_samples</code>参数控制，否则使用整个数据集构建每棵树。~ <a class="ae it" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank"> sklearn文档</a></p></blockquote><p id="a9c3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上面的定义很费解，我们来分解一下。</p><p id="2ee2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在机器学习中，有一个概念叫做决策树。通过基于属性值测试将源集合分割成子集，可以<em class="jv">“学习”</em>一个树。这个过程以一种称为<em class="jv">递归分割</em>的递归方式在每个派生子集上重复。当拆分没有给预测增加任何值时，递归停止。</p><p id="134c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">还有…..森林是树木的集合体。所以随机森林算法包含了大量的决策树。它既可以用于分类，也可以用于回归。这也是最灵活和最容易使用的算法。</p><p id="e6c7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">随机森林有多种应用，例如推荐引擎、图像分类和特征选择。它可以用来对忠诚的贷款申请人进行分类，识别欺诈活动和预测疾病。</p><blockquote class="js jt ju"><p id="8409" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">在这节课中，我们将使用澳大利亚数据集中<a class="ae it" href="https://www.kaggle.com/#" rel="noopener ugc nofollow" target="_blank"> Kaggle的</a> <a class="ae it" href="https://www.kaggle.com/jsphyg/weather-dataset-rattle-package" rel="noopener ugc nofollow" target="_blank"> Rain。</a></p></blockquote><p id="a9e7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">首先是分类问题。因此我们必须使用<a class="ae it" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank"> RandomForestClassifier </a>来达到这个目的。</p><p id="3896" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在此之前，让我们对数据进行一些清理和预处理。</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="a335" class="kl km hh kc b fi kn ko l kp kq">import numpy as np <em class="jv"># linear algebra</em><br/>import pandas as pd <em class="jv"># data processing, CSV file I/O (e.g. pd.read_csv)</em><br/><br/><em class="jv"># Input data files are available in the read-only "../input/" directory</em><br/><em class="jv"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</em><br/><br/>import os<br/>for dirname, _, filenames <strong class="kc hi">in</strong> os.walk('/kaggle/input'):<br/>    for filename <strong class="kc hi">in</strong> filenames:<br/>        print(os.path.join(dirname, filename))<br/><br/><em class="jv"># You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save &amp; Run All" </em><br/><em class="jv"># You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session<br/></em></span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es kr"><img src="../Images/128575d2343dd8f7fc78a11761820ad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*SVko32ngi0NSMN7h-WY4Gw.png"/></div></figure><p id="4d2a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这将输出数据的目录。</p><p id="6f91" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们把这些数据转换成熊猫的数据框架。</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="6f96" class="kl km hh kc b fi kn ko l kp kq">df = pd.read_csv("/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv")</span><span id="a854" class="kl km hh kc b fi ks ko l kp kq">df.head()</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es kr"><img src="../Images/dcb79398b33d8cbc47c6c0b9b37a3c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*O0GGJyEFzlB62SHTLRKGQg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">输出</figcaption></figure><p id="8fba" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">别忘了，我总是把数据框的所有栏目列成一个清单，以后，它肯定会派上用场。</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="7507" class="kl km hh kc b fi kn ko l kp kq">cols = list(df.columns)<br/>cols</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es kt"><img src="../Images/e0b38fe60bc80dcf3d203ed5e5d9b0fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*PM292e96-cWqZK5pnDYB2g.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">列列表</figcaption></figure><p id="e5b0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">描述表格:</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="cc2f" class="kl km hh kc b fi kn ko l kp kq">df.describe().T<br/></span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es ku"><img src="../Images/162a5f19bd4f55f1f63751cd7ad444a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*uPSNj_ikEVJOrE1BDgVV4A.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">肯定没必要拿转置，但是好看。</figcaption></figure><p id="556f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个describe()方法确实有助于解析数据库的数字数据。</p><p id="9e0c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们导入一些库来可视化数据。</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="093e" class="kl km hh kc b fi kn ko l kp kq">import matplotlib.pyplot as plt<br/>import seaborn as sns</span></pre><p id="211a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在到了一个重要的部分，我们必须检查特征之间的相关性，去掉那些高度相关的特征。</p><p id="5adb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为此，我们需要首先聚集特征，我们可以通过分离目标列(RainTomorrow)来做到这一点。)</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="959c" class="kl km hh kc b fi kn ko l kp kq">X = df.drop('RainTomorrow', axis=1)<br/>X.head()</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/a311979ad2e8a63b0af93c7dcc6c510a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*f7YYZYUIpdJDkQhOcO5-8w.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">X</figcaption></figure><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="5620" class="kl km hh kc b fi kn ko l kp kq">y = df[["RainTomorrow"]]<br/>y.head()<br/></span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es kw"><img src="../Images/37a7d8d59b2ac1354895cb34e915fb18.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*1bVopGl5IwM7IJse4YO0yA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Y</figcaption></figure><p id="85c6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们准备好检查相关性了。将相关性可视化的最佳工具是Seaborn的热图。</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="c595" class="kl km hh kc b fi kn ko l kp kq">plt.figure(figsize=(20,20))<br/>sns.heatmap(X.corr())<br/>plt.show()<br/></span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es kx"><img src="../Images/43acb8bed6aeb1f187c89d10fe7296b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*heDqNPLug9pRwl_aPUwPIQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">你可以看到这样的热图。</figcaption></figure><p id="44a1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对角线显然是要关联的。除此之外，深色方块代表我们想要的较小相关性。</p><p id="e084" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">到目前为止，很明显有些特征是绝对相关的。让我们放弃他们。</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="346b" class="kl km hh kc b fi kn ko l kp kq">cor_matrix = df.corr().abs()<br/>print(cor_matrix)</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/d602ca9d409ffbf09b6397cdf66e6a8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*G3urxnb-jugv3kIRTPNZaQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">相关矩阵</figcaption></figure><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="c6f5" class="kl km hh kc b fi kn ko l kp kq">upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))<br/>print(upper_tri)</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es ky"><img src="../Images/b7c98c0bcb2e2da4b31f98e27a7f7fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*sh2o1K5pql-FX2cUen1rDA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">上三角矩阵。</figcaption></figure><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="a661" class="kl km hh kc b fi kn ko l kp kq">to_drop = [column for column <strong class="kc hi">in</strong> upper_tri.columns if any(upper_tri[column] &gt; 0.95)]<br/>print(); print(to_drop)<br/></span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es kz"><img src="../Images/2d516a5fda3a7445a27d41ff584db676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*l29cHHXaKjczW18prYTOUw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">喔喔喔！！我们得到了多余的功能！</figcaption></figure><p id="2503" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">好吧…..代码太多了。我们所做的是非常有说服力的，首先，相关矩阵被创建，然后它被转换成上三角矩阵。从上面训练矩阵中，我们选择指数大于0.95的特征。</p><h1 id="a0cf" class="la km hh bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">有一件事没有被注意到！</h1><p id="6426" class="pw-post-body-paragraph iu iv hh iw b ix lx iz ja jb ly jd je jf lz jh ji jj ma jl jm jn mb jp jq jr ha bi translated">日期和位置栏对于确定明天是否会下雨也没有多大帮助。(把它们也扔掉)。</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="c133" class="kl km hh kc b fi kn ko l kp kq">df = df.drop(['Pressure3pm', 'Temp3pm', 'Date', 'Location'], axis=1)<br/>print(df.head())</span></pre><p id="e303" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们就要做出我们的模型了！</p><p id="026b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在解决分类问题时要记住的一点是，</p><p id="34fb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们需要预处理我们的数据，以便算法可以有效地预测值。分类变量必须转换成指示变量。为此，我们可以使用pandas的get_dummies() 方法。</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="607a" class="kl km hh kc b fi kn ko l kp kq">ohe = pd.get_dummies(data=df, columns=['WindGustDir','WindDir9am','WindDir3pm'])<br/>ohe.info()</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es mc"><img src="../Images/793719b3ae96770f817567001a67630d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*cNAB8TNl2DZxVNiQ0j-VeQ.png"/></div></figure><p id="4ccd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们缺少更多的预处理…..</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="68cc" class="kl km hh kc b fi kn ko l kp kq">from sklearn import preprocessing<br/>from numpy import array<br/><br/>ohe['RainToday'] = df['RainToday'].astype(str)<br/>ohe['RainTomorrow'] = df['RainTomorrow'].astype(str)<br/><br/>lb = preprocessing.LabelBinarizer()<br/><br/>ohe['RainToday'] = lb.fit_transform(ohe['RainToday'])<br/>ohe['RainTomorrow'] = lb.fit_transform(ohe['RainTomorrow'])</span><span id="d6b4" class="kl km hh kc b fi ks ko l kp kq">y = ohe['RainTomorrow']<br/>X = ohe.drop(['RainTomorrow'], axis=1)</span></pre><p id="e705" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">好……振作起来！！我们已经走了一半了！🎉🎉</p><p id="8549" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们接下来要做的是分割数据并定义我们的模型。</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="f872" class="kl km hh kc b fi kn ko l kp kq">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)</span><span id="2582" class="kl km hh kc b fi ks ko l kp kq">from sklearn.impute import SimpleImputer<br/>imp = SimpleImputer(missing_values=np.nan, strategy='mean')<br/>imp.fit_transform(X_train)</span></pre><blockquote class="js jt ju"><p id="e4af" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">简单的估算器充当空值填充器。它用数据的平均值替换空值。</p></blockquote><p id="2f3b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">创建一个管道，并将SimpleImputer和RandomForestClassifier添加到管道中。</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="3025" class="kl km hh kc b fi kn ko l kp kq">from sklearn.ensemble import RandomForestClassifier<br/><br/>model = make_pipeline(<br/>    SimpleImputer(),<br/>    RandomForestClassifier(n_jobs=4, verbose=True, n_estimators=200, max_depth=10, criterion='gini', )<br/>)<br/><br/>model.fit(X_train, y_train)</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es md"><img src="../Images/031ac844c9ab96135d9ae36784de6236.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*dIBsa5EDzKNAy5N1wBWIbQ.png"/></div></figure><p id="9650" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">n_jobs是用于确定有助于拟合数据的内核数量的参数。</p><p id="a056" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，评估模型。</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="44b3" class="kl km hh kc b fi kn ko l kp kq">model.score(X_test, y_test)</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/7d80614893b28a5aee7db8fdf24ab62a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*YKWseaAOMHDol6KNIThnsQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">分数接近83%</figcaption></figure><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="68d9" class="kl km hh kc b fi kn ko l kp kq">model.score(X_train, y_train)</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es me"><img src="../Images/fb21fab702f550105482df9c4df064f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*DP_YM3McScG1LTvOf9NQUQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">不出所料，训练分数更多。</figcaption></figure><p id="e8e5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">计算F1分数</p><pre class="kd ke kf kg fd kh kc ki kj aw kk bi"><span id="ee34" class="kl km hh kc b fi kn ko l kp kq">from sklearn.metrics import f1_score<br/><br/>pred = model.predict(X_test)<br/>print(f1_score(y_test, pred))</span></pre><figure class="kd ke kf kg fd ii er es paragraph-image"><div class="er es md"><img src="../Images/21665affec0f47f20224b6e8cdba6b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*n3aCGxgTHTwl1mITosUPHQ.png"/></div></figure><p id="17fd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">就是这样！！！</p><p id="5172" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们完了。</p><p id="36b1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">恭喜你！！🎉你设计了一个预测澳大利亚降雨量的模型。</p><p id="8ef0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这显然不是你能做的全部。您甚至可以使用称为参数调整的概念来争取更好的模型。</p><p id="1eee" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你有任何问题，请留下评论…</p><p id="7ee6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">谢谢大家！！</p></div></div>    
</body>
</html>