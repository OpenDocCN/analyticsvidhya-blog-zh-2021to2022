<html>
<head>
<title>Squeeze and Excitation Networks — Idiot Developer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">挤压和激励网络——白痴开发者</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/squeeze-and-excitation-networks-idiot-developer-17de2fd02596?source=collection_archive---------1-----------------------#2021-10-05">https://medium.com/analytics-vidhya/squeeze-and-excitation-networks-idiot-developer-17de2fd02596?source=collection_archive---------1-----------------------#2021-10-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="4ecd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">卷积神经网络(CNN)在计算机视觉和视觉感知领域得到了最广泛的应用，以解决多项任务，如<a class="ae jc" href="https://idiotdeveloper.com/dog-breed-classification-using-transfer-learning-in-tensorflow/" rel="noopener ugc nofollow" target="_blank">图像分类</a>、<a class="ae jc" href="https://idiotdeveloper.com/polyp-segmentation-using-unet-in-tensorflow-2/" rel="noopener ugc nofollow" target="_blank">语义分割</a>等等。然而，需要能够进一步提高其性能的方法。一种这样的方法是给已经存在的CNN体系结构添加一些注意机制，以便进一步改进。挤压和激励网络(SENet)就是这样一种注意力机制，它被最广泛地用于性能改进。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/5c775fb60794773edd08c622c52b7ba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jc7LnmSNiuhjmvrU.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:原始论文。挤压和激励网络的框图。</figcaption></figure><p id="e4e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们将了解更多关于挤压和激励网络的知识，它们是如何工作的，以及它们如何帮助提高性能。</p><p id="852f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">目录:</strong></p><ol class=""><li id="7f05" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">什么是挤压和激励网络？</li><li id="c8c9" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">压缩-激发网络背后的直觉。</li><li id="139a" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">压缩和激励网络的结构。</li><li id="adc8" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">挤压和激励网络如何帮助？</li><li id="3c55" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">摘要</li><li id="b4c0" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">阅读更多</li></ol><h1 id="c431" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">什么是挤压和激励网络？</h1><p id="4bf5" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">2018年，胡等人在2018年的论文“<a class="ae jc" href="https://arxiv.org/abs/1709.01507" rel="noopener ugc nofollow" target="_blank">挤压和激励网络</a>”中介绍了挤压和激励注意机制，并在TPAMI中发表了期刊版本。这是注意力机制领域最具优势的论文之一，被引用超过8000次。</p><h1 id="ec50" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">queeze和Excitation Network是一种基于通道的注意力机制，它可以通过增强重要特征来相应地重新校准每个通道，从而创建更强大的表示。</h1><p id="4d7c" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">挤压和激励网络基本上为CNN(卷积神经网络)引入了一种新颖的通道式注意机制，以改善它们的通道相关性。网络添加一个参数，该参数相应地重新加权每个通道，使得它对重要特征变得更加敏感，同时忽略不相关的特征。</p><p id="1638" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在ILSVRC 2017分类提交中使用了挤压和激励网络，并获得了第一名，并将前5名误差降至2.251%，超过了2016年的获奖条目，相对提高了25%。</p><h1 id="9281" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">挤压和激发网络背后的直觉</h1><p id="a725" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">卷积神经网络(CNN)使用卷积算子从图像中提取层次信息。下层检测线条、边缘等。，而上层检测完整的对象，如人脸、猫或狗。所有这些都是通过融合每一层的空间和信道信息来实现的。</p><p id="5407" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">卷积运算符使用不同数量的通道生成一个特征图，其中它平等地对待所有通道。这意味着每一个渠道都同样重要，这可能不是最好的方式。挤压和激发注意机制为每个通道添加了一个参数，可以独立地重新调整它们的大小。</p><h1 id="9c3e" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">挤压和激励基本上充当内容感知机制，自适应地对每个通道重新加权。</h1><h1 id="a05b" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">压缩和激励网络的结构</h1><p id="1729" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">作者提出了一种简单易用的模块，称为挤压和激励模块，也称为se模块。SE块由三个操作组成:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ls"><img src="../Images/27c933d899e325bd48ee6ab5512cb0d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3CVen7hMoTGGvm-K.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">具有适当尺寸和不同操作的挤压和激励网络的详细图。</figcaption></figure><h1 id="8012" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">挤压</h1><p id="7b58" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">挤压操作主要用于从特征图的每个通道中提取全局信息。特征图基本上是卷积层的输出，卷积层是大小为B x H x W x C的4D张量。这里:</p><ul class=""><li id="8e65" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb lt jz ka kb bi translated"><strong class="ig hi"> B: </strong>指批量。</li><li id="bba3" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb lt jz ka kb bi translated"><strong class="ig hi"> H: </strong>是指每个特征图的高度。</li><li id="80c4" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb lt jz ka kb bi translated"><strong class="ig hi"> W: </strong>指每张特征图的宽度。</li><li id="bda4" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb lt jz ka kb bi translated"><strong class="ig hi"> C: </strong>指特征图中的通道数。</li></ul><p id="2451" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们知道，卷积是一种局部运算，因为它只能看到完整输入图像的一小部分。因此，有必要对特征地图有一个全面的了解。</p><p id="0156" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为我们处理的是4D张量，它包含很多参数。这实质上意味着，随着现代卷积神经网络中通道数量的急剧增加，我们需要处理大量的参数。因此，我们需要一种方法来将每个特征通道分解成单个数值。这种分解将减少参数的数量，从而降低计算复杂度。</p><p id="83ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在现代卷积神经网络中，池操作用于降低特征图的空间维度。两种广泛使用的池操作是:</p><ol class=""><li id="468a" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated"><strong class="ig hi"> Max Pooling: </strong>操作用于从定义的窗口中获取最大像素值。</li><li id="6830" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated"><strong class="ig hi">平均池:</strong>操作用于计算定义窗口的平均像素值。</li></ol><p id="472f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者进行了一组实验来研究每个池操作的性能，它们是全局最大池(GMP)和全局平均池(GAP)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lu"><img src="../Images/bead4f56053d834e3b98a7359afa7666.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8tFy7PinG1dCGOBgASCwIg.png"/></div></div></figure><p id="db53" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上表可以清楚地看出，全局平均汇集(GAP)比全局最大汇集(GMP)表现得更好。因此，在<strong class="ig hi">挤压</strong>操作中，全局平均池(GAP)用于将B x H x W x C特征图减少到B x 1 x 1 x C。</p><h1 id="5fe7" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">激发，兴奋</h1><p id="d30d" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">特征图现在被减少到一个更小的维度(B×1×1×C)，基本上对于大小为H×W的每个通道被减少到一个奇异向量。对于激励操作，使用具有瓶颈结构的全连接多层感知器(MLP)。MLP用于生成权重，以自适应地缩放特征图的每个通道。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/99ccde46b0569478f13cc6bb7861c477.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/0*Sl5KZZkaWLTWLCC8.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">具有瓶颈结构的多层感知器(MLP)。</figcaption></figure><p id="1e04" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">MLP由三层组成，其中隐藏层用于通过缩减因子<strong class="ig hi"> r </strong>来减少要素的数量。图层中要素地图的尺寸为:</p><ol class=""><li id="811c" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">输入的形状是(B×1×1×C)，它被简化为B×C。因此输入层有C个神经元。</li><li id="08e8" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">隐藏层以<strong class="ig hi"> r </strong>的因子减少神经元的数量。因此，隐藏层有一个<strong class="ig hi"> C/r </strong>数量的神经元。</li><li id="fcf3" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">最后，在输出层，神经元数量增加回<strong class="ig hi"> C </strong>。</li></ol><p id="e862" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总的来说，MLP以B x 1 x 1 x C作为输入，以同样的维度作为输出。</p><h1 id="ca06" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">缩放比例</h1><p id="79d7" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">激发操作传递形状为B x 1 x 1 x C的“激发”张量，然后该张量通过一个sigmoid激活函数。sigmoid激活函数转换0和1范围内的张量值。然后，我们在sigmoid激活函数的输出和输入特征图之间执行逐元素乘法。</p><p id="6472" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果该值接近0，则意味着该频道不太重要，因此，该特征频道的值将会减小，如果该值接近1，则意味着该频道是重要的。</p><p id="eef7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了进一步研究缩放操作，作者通过用其他非线性激活函数代替sigmoid进行了消融研究。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/838691ff7bd2fc42cae96f21bc135ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rYmyIdgdSXqXBUKRQBeDvg.png"/></div></div></figure><p id="1ee8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上表中，我们可以说sigmoid是最好的非线性激活函数。</p><h1 id="a2a2" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">挤压和激励网络如何帮助？</h1><p id="9b7b" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">至此，你已经了解了压缩和激励网络的结构。在缩放操作期间，我们在初始特征图和sigmoid激活函数的输出之间执行逐元素乘法。</p><p id="f122" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">sigmoid激活函数输出一个介于0和1之间的值，每个通道乘以该值。</p><p id="2252" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在想象一下，一个通道乘以一个接近0的值。这将减少该特征图的像素值，因为根据se块，这些像素值不太相关。</p><p id="a038" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果与上面的情况相比，当通道乘以接近1的值时，它不会减少那么多像素值。</p><p id="77df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们可以看到，压缩和激励网络基本上缩放了每个通道的信息。它减少了非相关信道信息，相关信道不受太大影响。所以，在整个操作之后，特征图只包含相关信息，这增加了整个网络的表示能力。</p><h1 id="7c19" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">摘要</h1><p id="ac00" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">在本文中，你已经了解了一种最广泛使用的通道式注意力机制，称为“挤压和激发网络”。</p><p id="e3f7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你已经了解了以下关于它的事情。</p><ol class=""><li id="0c0a" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">什么是挤压和激励网络？</li><li id="efd4" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">SENet背后的直觉是什么？</li><li id="4695" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">SENet的建筑结构。</li><li id="d41e" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">有什么帮助？</li></ol><p id="0896" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">仍然，有一些问题或疑问？就在下面评论吧。更多更新。跟我来。</p><h1 id="a80d" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">阅读更多</h1><ol class=""><li id="6964" class="jt ju hh ig b ih lf il lg ip lx it ly ix lz jb jy jz ka kb bi translated"><a class="ae jc" href="https://amaarora.github.io/2020/07/24/SeNet.html" rel="noopener ugc nofollow" target="_blank">用PyTorch实现解释挤压和激励网络</a></li><li id="f40f" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated"><a class="ae jc" href="https://blog.paperspace.com/channel-attention-squeeze-and-excitation-networks/" rel="noopener ugc nofollow" target="_blank">通道注意力和挤压-激发网络(SENet) </a></li><li id="34bb" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated"><a class="ae jc" href="https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7" rel="noopener" target="_blank">压缩和激励网络</a></li></ol></div><div class="ab cl ma mb go mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ha hb hc hd he"><p id="59f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="mh">原载于2021年10月5日https://idiotdeveloper.com</em><em class="mh"/><a class="ae jc" href="http://www.idiotdeveloper.com/squeeze-and-excitation-networks/" rel="noopener ugc nofollow" target="_blank"><em class="mh">。</em></a></p></div></div>    
</body>
</html>