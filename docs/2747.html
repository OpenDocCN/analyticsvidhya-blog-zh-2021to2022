<html>
<head>
<title>Brain Tumor Classification Using Different Machine Learning Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于不同机器学习算法的脑肿瘤分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/brain-tumor-classification-using-different-machine-learning-algorithms-423421175028?source=collection_archive---------13-----------------------#2021-05-15">https://medium.com/analytics-vidhya/brain-tumor-classification-using-different-machine-learning-algorithms-423421175028?source=collection_archive---------13-----------------------#2021-05-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="2762" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在当今世界，脑瘤和其他神经系统癌症是男女死亡的第十大原因。在早期诊断肿瘤可以挽救一个人的生命。发现脑瘤实际上是一个巨大的挑战。脑瘤的检测可以在机器学习的帮助下完成。在该领域，机器学习在正确预测大脑内部肿瘤的存在方面发挥了至关重要的作用。</p><h1 id="767a" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated"><strong class="ak">数据集:</strong></h1><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es ka"><img src="../Images/3c1e4c254da7023b7734de34d91b643c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*QtXfAmSnW3BrZ83F3ktS2A.png"/></div></figure><p id="6c19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">来源:https://www.kaggle.com/jakeshbohaju/brain-tumor<a class="ae ki" href="https://www.kaggle.com/jakeshbohaju/brain-tumor" rel="noopener ugc nofollow" target="_blank"/></p><p id="1004" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据包括五个一阶特征和八个目标级别的纹理特征(在列类中)。在我们引用的研究论文中，他们首先使用深度学习技术从给定的图像中提取与脑瘤相关的特征，但由于我们的限制，我们直接采用提取的特征，并应用机器学习算法来根据特征值预测此人是否患有肿瘤。</p><p id="2702" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的目标变量有两个值:“0”和“1”，因此这是一个二元分类问题。</p><h1 id="e488" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated"><strong class="ak">探索性数据分析和数据可视化:</strong></h1><ol class=""><li id="627f" class="kj kk hh ig b ih kl il km ip kn it ko ix kp jb kq kr ks kt bi translated">数据集的形状:(3762，15)</li><li id="b2f6" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated">数据类型:除了图像属性，所有其他属性都有int/float数据类型。因此，我们删除了图像属性，因为它在确定类别(即目标属性)时没有任何重要作用。</li><li id="5d25" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated">目标属性的分布:类</li></ol><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es kz"><img src="../Images/719735c5c709365d8b77723747e413af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JbEjvH5ueCiOCDMnwaxjXg.png"/></div></div></figure><p id="30f5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以说，我们有一个高度平衡的数据集，因为我们有55.26% (2079)的样本被分类为0类，即非肿瘤，44.74% (1683)的样本被分类为1类，即肿瘤。</p><p id="a00c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.相关矩阵</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es le"><img src="../Images/1411077f47e753a5ec027db65ca34f61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y6-wIqHqydYQzsqMgP8E-w.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">Corr矩阵显示不同的功能如何相互依赖。</figcaption></figure><p id="eb56" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">5.检测缺失值</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lj"><img src="../Images/7b0ed3e779c672197e3d6093a10097ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*wUidD9nv-FnWdWXTd_i-zg.png"/></div></figure><p id="ee4b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，我们在任何属性中都没有任何空值/缺失值。</p><p id="4084" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">6.异常值检测和去除:</p><p id="6da8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用箱线图，我们检测到在许多特征的数据中存在异常值，为了消除这些异常值，我们使用了一种称为异常值消除的四分位间距方法的技术。它说，计算数据的<strong class="ig hi">四分位间距</strong>。将<strong class="ig hi">四分位距</strong> ( <strong class="ig hi"> IQR </strong>)乘以1.5(用于辨别<strong class="ig hi">异常值</strong>的常数)。将1.5 x ( <strong class="ig hi"> IQR </strong>)加到第三个四分位数。任何大于此值的数字都是可疑的异常值<strong class="ig hi">。</strong></p><p id="786f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">7.特征选择和重要性</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lk"><img src="../Images/cb0444823988aa33b047dc173147cc62.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*D2csCdUIjzZ2vbFW0Eealg.png"/></div></figure><p id="b167" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">(a)单变量特征选择通过基于单变量统计测试选择最佳特征来工作。它可以被看作是估计器的预处理步骤。scikit-learn库提供了SelectKBest类，它可以与一组不同的统计测试一起使用，以选择特定数量的特性。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es ll"><img src="../Images/603841b9327feebff2d9619b8a7a87d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*4dWZ82ihAFypOLrH9XKHhQ.png"/></div></figure><p id="29f2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">(b)为了使用上述森林结构执行特征选择，在森林的构建期间，对于每个特征，计算在分割特征(基尼指数)的决定中使用的数学标准的归一化总缩减。该值称为要素的基尼系数。为了执行特征选择，每个特征根据每个特征的基尼重要性以降序排序，并且用户根据他/她的选择选择前k个特征。</p><p id="7960" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">8.特征标准化:</p><p id="ef6d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">标准标量:通过移除平均值并缩放至单位方差来标准化要素。通过计算训练集中样本的相关统计数据，对每个特征独立进行居中和缩放。然后，存储平均值和标准偏差，以便使用变换在以后的数据中使用。</p><h1 id="39d6" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated"><strong class="ak">使用的机器学习算法:</strong></h1><ol class=""><li id="b2b1" class="kj kk hh ig b ih kl il km ip kn it ko ix kp jb kq kr ks kt bi translated"><a class="ae ki" href="https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c" rel="noopener" target="_blank">朴素贝叶斯分类器</a></li><li id="d2cd" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated"><a class="ae ki" href="https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761" rel="noopener" target="_blank"> K个最近邻居</a></li><li id="b9c2" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated"><a class="ae ki" href="https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc" rel="noopener" target="_blank">逻辑回归</a></li><li id="f6c3" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated"><a class="ae ki" href="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47" rel="noopener" target="_blank">支持向量分类器/机器</a></li><li id="91b7" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated"><a class="ae ki" href="https://www.javatpoint.com/artificial-neural-network" rel="noopener ugc nofollow" target="_blank">人工神经网络</a></li><li id="e167" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated"><a class="ae ki" href="https://towardsdatascience.com/understanding-decision-tree-classifier-7366224e033b" rel="noopener" target="_blank">决策树</a></li><li id="7f19" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated"><a class="ae ki" href="https://towardsdatascience.com/decision-tree-ensembles-bagging-and-boosting-266a8ba60fd9" rel="noopener" target="_blank">树木集合:装袋、随机造林和助推</a></li></ol><h1 id="21cf" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">考虑的性能指标:</h1><ol class=""><li id="6776" class="kj kk hh ig b ih kl il km ip kn it ko ix kp jb kq kr ks kt bi translated">混淆矩阵/特异性/敏感性:</li></ol><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lm"><img src="../Images/34294538b79e468c8226542702529291.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*CEDAX5yGHJgcPTXIsiMQVA.png"/></div></figure><p id="1cd5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">混淆矩阵是一种总结分类算法性能的技术。通过计算混淆矩阵，可以更好地了解分类模型的正确程度和错误类型。</p><p id="06d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">灵敏度/TP率/精度:分类器发现阳性结果的性能与灵敏度有关。特异性计算如下:TP/(TP+FN)。</p><p id="af5f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">特异性/TN比率:分类器识别阴性结果的性能与特异性相关。它由:TN/(TN+FP)给出。</p><p id="8de7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.ROC曲线:</p><p id="adde" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">ROC代表接收机工作特性。ROC图在y轴上绘制TP比率，在x轴上绘制FP比率。ROC分析提供了选择可能的最优模型并抛弃次优模型的工具，独立于(并在指定之前)成本背景或类别分布。ROC图左上角的分类器是优选的，因为它们的灵敏度和特异性都很高，具有最高灵敏度和特异性的分类器是该数据集的最佳模型。曲线下的面积越接近1，绘制ROC曲线的算法越好。</p><p id="a9d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.OOB分数和对数损失:</p><p id="a1fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">出袋(OOB)误差，对于数据集D的平均值，如果总共有N个样本，则2N/3个样本将是独立的，并包括在集成技术中。因此，N/3个样本保持不变，这些样本可用于测试，并有助于避免过度拟合。对这些样本的测试准确度被称为OOB分数。</p><p id="5671" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对数损失表示预测概率与相应的实际/真实值(在二元分类的情况下为0或1)有多接近。预测概率偏离实际值越多，对数损失值就越高。我们已经使用对数损失/二进制交叉熵来最小化我们的神经网络中的损失。</p><h1 id="66a1" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">结果分析:</h1><p id="2678" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip ln ir is it lo iv iw ix lp iz ja jb ha bi translated">模特表演:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lq"><img src="../Images/5c67d2aa34c96b4b5aecd78bb730a103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*tSY8hhJsOgNcH5pvauj_Uw.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">显示不同算法的准确性/敏感性/特异性。</figcaption></figure><p id="11eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了避免在应用的各种算法中过度拟合，我们选择了以下技术:</p><p id="c1b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1)使用GridSearchCV进行超参数调整，以给出最佳参数。2) K倍交叉验证，以测试模型预测未用于估算的新数据的能力。</p><p id="478f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3)正则化显著降低了模型的方差，而没有显著增加其偏差。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lr"><img src="../Images/c32329ba10ebf23f2aef0d774c0a4620.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*znWG6PjREuhK0aPTElbY4A.png"/></div></div></figure><h1 id="7bbe" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">结论:</h1><p id="7ad1" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip ln ir is it lo iv iw ix lp iz ja jb ha bi translated">所有模型都达到了非常好的测试精度。然而，敏感性、特异性、FP率和FN率是决定最佳模型的最重要因素。如果我们考虑特异性和敏感性的平均值，几乎所有算法的平均值都超过99%。如果我们考虑FP率和FN率，随机森林分类器给出了最好的结果，显著的FP率为0，FN率为0.003。此外，对于随机森林分类器，特异性和敏感性的平均值为0.9985，是所有分类器中最高的。</p><p id="a818" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，可以得出结论，对于我们的数据集，随机森林分类器是最佳和有效的模型，因为与所有其他模型相比，它获得了最佳结果。它还确保了在模型中不会发生过度拟合，因为所有的超参数都被适当地调整。</p><p id="70e0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">项目完整代码及报告链接:<a class="ae ki" href="https://github.com/ishdeep-10/Brain-Tumor-Classification" rel="noopener ugc nofollow" target="_blank">https://github.com/ishdeep-10/Brain-Tumor-Classification</a>。</p><p id="15ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">谢谢大家！</p></div></div>    
</body>
</html>