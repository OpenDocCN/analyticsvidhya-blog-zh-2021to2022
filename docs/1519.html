<html>
<head>
<title>Gradient Descent vs Stochastic GD vs Mini-Batch SGD</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降vs随机GD vs小批量SGD</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-vs-stochastic-gd-vs-mini-batch-sgd-fbd3a2cb4ba4?source=collection_archive---------2-----------------------#2021-03-06">https://medium.com/analytics-vidhya/gradient-descent-vs-stochastic-gd-vs-mini-batch-sgd-fbd3a2cb4ba4?source=collection_archive---------2-----------------------#2021-03-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="7057" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">警告:以防术语“偏导数”或“梯度”听起来不熟悉，我建议查看这些资源！</p><p id="4888" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://www.mathsisfun.com/calculus/derivatives-partial.html" rel="noopener ugc nofollow" target="_blank">https://www . mathsisfun . com/calculus/derivatives-partial . html</a></p><p id="ffeb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient" rel="noopener ugc nofollow" target="_blank">https://www . khanacademy . org/math/multivariable-calculus/multivariable-derivatives/gradient-and-direction-derivatives/v/gradient</a></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/c8aa92a7c36d4429891fb7be26f3e91e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FXHp55rpDM0tkaD5oz3Dvg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">PS，批量梯度下降=梯度下降</figcaption></figure><p id="249b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">梯度下降是一种迭代算法，其目的是对一组参数(即数据集的特征)进行改变，希望达到一组最佳参数，从而尽可能降低损失函数值。</p><p id="5a36" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">损失或成本或目标函数(这些命名惯例中的任何一个在实践中都有效)是我们寻求最小化其值的函数。损失函数的形式如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jt"><img src="../Images/3d96a55195eb3ee288df9f097778c323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ene0W5beS_1CkaSCtK0RkQ.png"/></div></div></figure><p id="e425" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当执行梯度下降时，每次我们更新参数，我们期望观察到min f(w)的变化。也就是说，在每次迭代中，取包含w中的参数的函数的梯度，使得函数相对于参数的变化使我们更接近于达到最终将导致最低可能损失函数值的最佳参数集的目标。</p><p id="0cc8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用的模型(例如最小二乘法、逻辑回归等。)需要学习这个最佳参数集，以便模型的预测非常接近我们想要获得的目标。</p><p id="dca3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以我们取损失函数关于参数向量w的梯度。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ju"><img src="../Images/dd73d27d6fea0e105442739cd30fed72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*BtTmKPC1G4ARQC8EYlH56w.png"/></div></figure><p id="3e5c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">w可以是d维向量或矩阵，其在梯度下降中的每次迭代之后通过采用朝向全局最小值移动的相反梯度方向来更新。所谓全局最小值，我指的是我们希望优化的函数值比所有其他可行点给出的值小的点。下面是如何更新参数w的向量的一般形式。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jv"><img src="../Images/a19b9e07ee16870c44a07fd03f27ce86.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*hjmVMX8a3TZxJltYycQx9Q.png"/></div></figure><p id="b10f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">学习率α决定了我们达到这个全局最小值所采取的步骤的大小。</p><p id="a576" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据学习率的设置，向全局最优的收敛会有所不同。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jw"><img src="../Images/9392767037acb3ff413a075f2cb9a31e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ln65Mu3CYzYBYwPI7cxp3g.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图片来自:<a class="ae jc" href="https://www.math.purdue.edu/~nwinovic/deep_learning_optimization.html" rel="noopener ugc nofollow" target="_blank">https://www . math . purdue . edu/~ nwinovic/deep _ learning _ optimization . html</a></figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jx"><img src="../Images/4789c4c835cb539b1007bad925ceb0b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*cyrgBEkH-iK4ZWZODLHkmQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">我想我的职业是艺术。</figcaption></figure><p id="f05b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">左图代表执行梯度下降的主要目标。从一个初始向量w开始，最小化w和w*之间的距离。</p><p id="3ef8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，有一些重要的性质(如凸性、严格凸性等。)某些功能可能会使下降方法的生活更容易。如果你有兴趣阅读更多关于这方面的内容，请查看下面的链接，链接到我觉得信息量很大的课堂讲稿。</p><p id="75d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://sboyles.github.io/teaching/ce377k/convexity.pdf" rel="noopener ugc nofollow" target="_blank">https://sboyles.github.io/teaching/ce377k/convexity.pdf</a></p><p id="9cc6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在继续之前，先回顾一下梯度下降的步骤:</p><ol class=""><li id="37f7" class="jy jz hh ig b ih ii il im ip ka it kb ix kc jb kd ke kf kg bi translated">最初从一组随机的参数值开始，这可以是一个权重向量:w</li><li id="2fbd" class="jy jz hh ig b ih kh il ki ip kj it kk ix kl jb kd ke kf kg bi translated">使用整个数据集计算w中每个参数的偏导数</li><li id="0657" class="jy jz hh ig b ih kh il ki ip kj it kk ix kl jb kd ke kf kg bi translated">更新w中的参数集</li><li id="08a5" class="jy jz hh ig b ih kh il ki ip kj it kk ix kl jb kd ke kf kg bi translated">返回更新的w和损失函数值</li></ol><p id="0a75" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">重要的是要注意，当权重向量中的参数改变时，损失函数中发生的改变不应以大得多的因子改变。如果出现这种情况，模型或系统被认为是病态的。梯度下降将花费比保证收敛到最佳权重向量更多的迭代。编程梯度下降时，跟踪w和损失函数值的变化很重要。指定w更新的次数或迭代次数。</p><p id="22a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看一个例子。假设我们有40，000个训练样本和20个特征。这意味着我们使用40，000个样本来更新我们的20个功能中的每一个。我们正在计算40，000项，对于向量w的每个偏导数！这意味着计算800，000个偏导数以达到最低损失函数值。</p><p id="9836" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，梯度下降要求使用整个数据集计算梯度，以更新模型的参数。实际上，梯度下降的计算成本可能非常高，并且达到最佳权重向量w所花费的时间可能非常长。</p><p id="c139" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是，如果我们不想使用可能包含数百万训练样本的整个数据集来更新我们的参数向量/矩阵w，该怎么办呢？</p><p id="bfbc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机梯度下降(SGD)是梯度下降的一种变体，它从数据集中随机抽取<strong class="ig hi">一个</strong>训练样本，用于计算每次迭代的梯度。随机梯度下降工作良好，因为我们仅使用一个数据点来计算梯度，更新权重向量w，并计算损失函数值。更新w的等式稍微改变为。上标I表示用于更新下面θ的单个数据点。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es km"><img src="../Images/b1b6f9fdc440c5a670c624d4e31f15ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PyutA6u7wo8EmLO8c1efPw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图片来自:<a class="ae jc" href="https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/ml-随机-梯度-下降-sgd/ </a></figcaption></figure><p id="b405" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在算法的每次迭代中，随机梯度下降法随机抽取一个样本，并计算所有的偏导数。回到我们之前的例子，从我们的n = 40，000样本量中一次随机抽取一个样本，计算20个偏导数，因为我们有20个参数，并且这个过程持续指定数量的时期。</p><p id="923c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于SGD一次从一个数据集中随机抽取一个样本(从头开始编码时，请确保在每次迭代中打乱数据集)，最终它必须遍历整个数据集。一次通过整个数据集称为一个时期。历元可用于梯度下降和SGD之间的比较，以比较w中的起始参数接近其最佳值的速率。</p><p id="ff78" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机梯度下降算法；</p><ol class=""><li id="ef7e" class="jy jz hh ig b ih ii il im ip ka it kb ix kc jb kd ke kf kg bi translated">初始化权重向量</li><li id="248a" class="jy jz hh ig b ih kh il ki ip kj it kk ix kl jb kd ke kf kg bi translated">对于t = 0，1，…，T，其中T表示一个历元，请执行以下操作:</li></ol><p id="ba98" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">a.从数据集中随机抽取一个数据点</p><p id="09a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">b.使用训练样本和权重来计算梯度</p><p id="cefb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">c.更新权重向量</p><p id="9d69" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">d.降低学习率/步长(实践中很好)</p><p id="8afa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3)返回(接近)最佳权重向量</p><p id="ea92" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在SGD中处理单个样本需要执行许多单个矩阵-向量(或向量-向量)乘法，这在计算上变得昂贵。</p><p id="dff6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，如果我们不仅想在每次迭代中使用1个样本，而是想通过使用1个数据点和整个数据集之间的一批样本来推动事情的发展，会怎么样呢？对多个样本进行采样以计算SGD的梯度，这样1 &lt; <strong class="ig hi"> b </strong> &lt; n被称为<strong class="ig hi">小批量</strong>随机梯度下降(MBSGD)。有些人可能称他们的算法实现为SGD，同时使用<strong class="ig hi">小批量</strong>来解决他们的优化问题，所以请留意这一点！</p><p id="9ab9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在MBSGD中，梯度是在小批量的训练样本上计算的，以便更新参数，并且损失函数值(希望)在每次迭代中继续降低。小批量随机梯度下降的优势包括:</p><ul class=""><li id="ac17" class="jy jz hh ig b ih ii il im ip ka it kb ix kc jb kn ke kf kg bi translated">模型参数被更频繁地更新，这允许更强地向最佳参数值收敛</li><li id="93c5" class="jy jz hh ig b ih kh il ki ip kj it kk ix kl jb kn ke kf kg bi translated">计算效率更高，因为MBSGD不使用完整的数据集。因为使用数百万训练样本来更新潜在的数千个参数是非常低效的</li></ul><p id="d43e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">MBSGD在算法实现上非常接近。使用小批量方法时，从数据集中随机抽取b个数据点来计算梯度。MBSGD中的每个历元具有n/b次迭代(n是数据集的大小),以使用b个样本来计算梯度和损失函数。</p><p id="7995" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于深度学习任务，为了适应CPU和GPU硬件的内存要求，请在对数据训练模型之前指定最小批量。在实现MBSGD时，8、32、64、128等小批量是合适的批量。始终跟踪损失函数是如何变化的。</p><p id="9f2a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">记住，在每一种梯度下降法中，我们都非常想最小化损失函数。因此，在MBSGD的情况下，如果函数值在一定数量的历元后碰壁，这可能意味着有必要微调批量大小和/或微调其他几个值(即学习率、正则化/权重衰减、动量等)。).</p><p id="9824" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我希望你喜欢读这篇文章。快乐学习！</p></div></div>    
</body>
</html>