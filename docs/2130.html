<html>
<head>
<title>Advice for Picking Model for Simple Classification Task</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为简单分类任务选择模型的建议</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/advice-for-picking-model-for-simple-classification-task-fadce6a504ee?source=collection_archive---------5-----------------------#2021-04-06">https://medium.com/analytics-vidhya/advice-for-picking-model-for-simple-classification-task-fadce6a504ee?source=collection_archive---------5-----------------------#2021-04-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="2feb" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">比较和对比逻辑回归、随机森林和神经网络以及如何在R</h2></div><p id="2ff5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">分类是统计分析中最古老、最经典的问题之一。有许多现有的机器学习模型可以利用。对于这个博客，我们将提供一些关于在一个简单的多类分类任务中选择三种类型的机器学习模型的见解:线性分类、随机森林和神经网络。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es js"><img src="../Images/fadded19c3f0dff7f3e1a5246b1cbd17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-MIW2X-nYUHRbEPk"/></div></div><figcaption class="ke kf et er es kg kh bd b be z dx translated">凯文·Ku在<a class="ae ki" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure></div><div class="ab cl kj kk go kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="ha hb hc hd he"><h1 id="3bf0" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">资料组</h1><p id="599a" class="pw-post-body-paragraph iw ix hh iy b iz li ii jb jc lj il je jf lk jh ji jj ll jl jm jn lm jp jq jr ha bi translated">例如，我们将使用来自Kaggle的<a class="ae ki" href="https://www.kaggle.com/abisheksudarshan/customer-segmentation" rel="noopener ugc nofollow" target="_blank">客户细分</a>数据集。以下是从Kaggle网页上重试的“客户细分”的描述:</p><blockquote class="ln lo lp"><p id="5e70" class="iw ix lq iy b iz ja ii jb jc jd il je lr jg jh ji ls jk jl jm lt jo jp jq jr ha bi translated">客户细分是将客户群划分为在与营销相关的特定方面(如年龄、性别、兴趣和消费习惯)相似的个人群体的做法。</p><p id="9d94" class="iw ix lq iy b iz ja ii jb jc jd il je lr jg jh ji ls jk jl jm lt jo jp jq jr ha bi translated">采用客户细分的公司在这样一个事实下运作，即每个客户都是不同的，如果他们针对特定的较小群体，向这些消费者提供相关的信息，并引导他们购买某些东西，他们的营销工作将会得到更好的服务。公司还希望更深入地了解客户的偏好和需求，以发现每个细分市场最有价值的东西，从而更准确地为该细分市场定制营销材料。</p></blockquote><p id="f1ec" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">可以使用公共Kaggle API下载数据集，命令行代码如下:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="3a42" class="lz kr hh lv b fi ma mb l mc md">pip install kaggle<br/>kaggle datasets download -d abisheksudarshan/customer-segmentation<br/>unzip customer-segmentation.zip</span></pre><p id="3cab" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="lq">注:“~/。kaggle/kaggle.json”是该下载操作所必需的认证令牌。可以按照</em> <a class="ae ki" href="https://www.kaggle.com/docs/api" rel="noopener ugc nofollow" target="_blank"> <em class="lq">这条指令</em> </a> <em class="lq">生成。</em></p><p id="3f58" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然后，我们可以使用以下代码读取数据，删除包含缺失数据的行，并将数据类型更改为因子:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="9dd6" class="lz kr hh lv b fi ma mb l mc md">data &lt;- read_csv("train.csv")<br/>data &lt;- data[complete.cases(data),]<br/>data[sapply(data, is.character)] &lt;- lapply(data[sapply(data, is.character)], as.factor)</span></pre><p id="f07f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">删除包含“NA”的行后，数据集还剩5，534个实例。共有9个解释变量:<em class="lq">、曾经结过婚</em>(客户是否曾经结过婚)<em class="lq">、年龄、毕业</em>(客户是否毕业)<em class="lq">、职业、工作经历</em>(年)<em class="lq">、花费分数</em>(低、中或高)<em class="lq">、家庭规模</em>(家庭人数)<em class="lq"> var_1 </em>(客户的匿名类别)。我们试图根据9个变量预测他们的<em class="lq">细分</em> (A、B、C或D)。以下是<em class="lq"> glimpse(data) </em>的输出:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="8ea7" class="lz kr hh lv b fi ma mb l mc md">Rows: 5,534<br/>Columns: 11<br/>$ ID              <em class="lq">&lt;dbl&gt;</em> 462809, 466315, 461735, 461319, 460156, 464347, 465015, 465176, 464041, 464942, 460…<br/>$ Gender          <em class="lq">&lt;fct&gt;</em> Male, Female, Male, Male, Male, Female, Female, Female, Female, Male, Female, Femal…<br/>$ Ever_Married    <em class="lq">&lt;fct&gt;</em> No, Yes, Yes, Yes, No, No, Yes, Yes, No, No, Yes, No, No, No, Yes, Yes, Yes, No, Ye…<br/>$ Age             <em class="lq">&lt;dbl&gt;</em> 22, 67, 67, 56, 32, 33, 61, 55, 26, 19, 58, 41, 32, 31, 58, 79, 49, 18, 33, 36, 56,…<br/>$ Graduated       <em class="lq">&lt;fct&gt;</em> No, Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, No, No, No, No, No, Yes, Yes, Yes, No, Y…<br/>$ Profession      <em class="lq">&lt;fct&gt;</em> Healthcare, Engineer, Lawyer, Artist, Healthcare, Healthcare, Engineer, Artist, Eng…<br/>$ Work_Experience <em class="lq">&lt;dbl&gt;</em> 1, 1, 0, 0, 1, 1, 0, 1, 1, 4, 0, 1, 9, 1, 1, 0, 12, 3, 13, 5, 1, 9, 1, 0, 5, 0, 1, …<br/>$ Spending_Score  <em class="lq">&lt;fct&gt;</em> Low, Low, High, Average, Low, Low, Low, Average, Low, Low, Low, Low, Low, Low, Aver…<br/>$ Family_Size     <em class="lq">&lt;dbl&gt;</em> 4, 1, 2, 2, 3, 3, 3, 4, 3, 4, 1, 2, 5, 6, 4, 1, 1, 4, 2, 2, 3, 8, 3, 1, 4, 3, 2, 3,…<br/>$ Var_1           <em class="lq">&lt;fct&gt;</em> Cat_4, Cat_6, Cat_6, Cat_6, Cat_6, Cat_6, Cat_7, Cat_6, Cat_6, Cat_4, Cat_3, Cat_1,…<br/>$ Segmentation    <em class="lq">&lt;fct&gt;</em> D, B, B, C, C, D, D, C, A, D, B, C, D, B, B, C, A, D, A, B, C, A, C, B, C, C, A, D,…</span></pre><p id="b356" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在拟合任何模型之前，我们将首先将数据分成分数为0.9和0.1的训练和测试子集:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="102b" class="lz kr hh lv b fi ma mb l mc md">set.seed(303)<br/>train_ind &lt;- sample(seq_len(nrow(data)), size = nrow(data)*0.9)<br/>train &lt;- data[train_ind, ]<br/>test &lt;- data[-train_ind, ]</span></pre><h1 id="a6d4" class="kq kr hh bd ks kt me kv kw kx mf kz la in mg io lc iq mh ir le it mi iu lg lh bi translated">逻辑回归</h1><p id="aea9" class="pw-post-body-paragraph iw ix hh iy b iz li ii jb jc lj il je jf lk jh ji jj ll jl jm jn lm jp jq jr ha bi translated">由于我们在这个问题中的响应变量是分类的，我们将在这里使用多项逻辑回归。多项式逻辑回归根据解释变量的值来估计给定实例属于不同类别的概率。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es mj"><img src="../Images/1dbe19f3a0dcfbbe448939bed775ce54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CCNqKM8wCil_dpAHibAUFg.png"/></div></div><figcaption class="ke kf et er es kg kh bd b be z dx translated">来源:<a class="ae ki" rel="noopener" href="/ds3ucsd/multinomial-logistic-regression-in-a-nutshell-53c94b30448f">https://medium . com/ds 3 ucsd/multinomial-logistic-regression-in-shell-53c 94 b 30448 f</a></figcaption></figure><p id="5e93" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们首先加载必要的库，并用<em class="lq"> nnet </em>库中的<em class="lq"> multinom </em>拟合模型:<em class="lq">(</em><a class="ae ki" href="https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/" rel="noopener ugc nofollow" target="_blank"><em class="lq">source</em></a><em class="lq">)</em></p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="83bc" class="lz kr hh lv b fi ma mb l mc md"># install.packages(c("foreign", "nnet", "reshape2"))<br/>library(foreign)<br/>library(nnet)<br/>library(reshape2)<br/>logistic &lt;- multinom(Segmentation ~ ., data = train)</span></pre><p id="991e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">以下是<em class="lq">汇总(逻辑)</em>的输出:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="f3f7" class="lz kr hh lv b fi ma mb l mc md">Call:<br/>multinom(formula = Segmentation ~ Gender + Ever_Married + Age + <br/>    Graduated + Profession + Work_Experience + Spending_Score + <br/>    Family_Size + Var_1, data = train)<br/><br/>Coefficients:<br/>  (Intercept) GenderMale Ever_MarriedYes         Age GraduatedYes ProfessionDoctor ProfessionEngineer<br/>B  -0.4221270 -0.1988533       0.1391316  0.01708554    0.2583479       -0.4604366         -0.6672424<br/>C  -0.6969034 -0.3095092      -0.1037293  0.02631203    0.7297757       -0.6300941         -1.8221096<br/>D  -0.5720053  0.3096303      -0.3161027 -0.02382555   -0.5087015        1.1676784          0.8334373<br/>  ProfessionEntertainment ProfessionExecutive ProfessionHealthcare ProfessionHomemaker ProfessionLawyer<br/>B              -0.8121714          -0.2638353           0.08730237          -0.1527637        -1.139400<br/>C              -1.5926832          -0.5908095           0.39070485          -1.2175544        -1.599622<br/>D               0.6859054           1.5881880           2.89578113           1.6162463         1.835006<br/>  ProfessionMarketing Work_Experience Spending_ScoreHigh Spending_ScoreLow Family_Size Var_1Cat_2 Var_1Cat_3<br/>B          -0.7095880     -0.03532825         -0.4111455        -0.9156952  0.14580953  0.4088346  0.1768805<br/>C          -0.6461845     -0.04013034         -0.7505069        -1.8174721  0.33535530  0.1425072 -0.1727507<br/>D           2.1397329      0.03880093         -0.1673184         0.4548072  0.08658522 -0.4029584 -0.4651911<br/>  Var_1Cat_4  Var_1Cat_5 Var_1Cat_6 Var_1Cat_7<br/>B -0.1247568  0.08728294  0.2141020 -0.1617883<br/>C -1.0174653 -0.26241151  0.2840689 -0.4145876<br/>D -0.2923172 -0.92159680 -0.2840632 -0.4794811<br/><br/>Std. Errors:<br/>  (Intercept) GenderMale Ever_MarriedYes         Age GraduatedYes ProfessionDoctor ProfessionEngineer<br/>B   0.4513680 0.09339209       0.1246480 0.003990277   0.09878927        0.1606340          0.1536374<br/>C   0.4775832 0.09820846       0.1439393 0.004257025   0.11203885        0.1675221          0.1997269<br/>D   0.4782653 0.10212671       0.1351695 0.004884103   0.10094036        0.1850112          0.1948838<br/>  ProfessionEntertainment ProfessionExecutive ProfessionHealthcare ProfessionHomemaker ProfessionLawyer<br/>B               0.1333589           0.2003719            0.2004370           0.2649502         0.198043<br/>C               0.1545122           0.2057309            0.1966512           0.3439174         0.209678<br/>D               0.1731890           0.2473922            0.1938104           0.2701236         0.257483<br/>  ProfessionMarketing Work_Experience Spending_ScoreHigh Spending_ScoreLow Family_Size Var_1Cat_2 Var_1Cat_3<br/>B           0.2997303      0.01307695          0.1632608         0.1292463  0.03423526  0.4175040  0.3948757<br/>C           0.3105096      0.01398284          0.1659144         0.1408120  0.03597520  0.4374611  0.4177020<br/>D           0.2453768      0.01315404          0.2283467         0.1802314  0.03420730  0.4068696  0.3837485<br/>  Var_1Cat_4 Var_1Cat_5 Var_1Cat_6 Var_1Cat_7<br/>B  0.3899082  0.5444062  0.3749715  0.4591273<br/>C  0.4189915  0.5908614  0.3907861  0.4850656<br/>D  0.3736922  0.5654122  0.3597524  0.4494961<br/><br/>Residual Deviance: 10842.03 <br/>AIC: 10980.03</span></pre><p id="1acc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这些系数也可以被可视化:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="519c" class="lz kr hh lv b fi ma mb l mc md"># install.packages(c("broom", "dplyr", "ggstance"))<br/>logisticCoef &lt;- broom::tidy(logistic,conf.int=TRUE)<br/>logisticCoef &lt;- dplyr::filter(logisticCoef, term!="(Intercept)")</span><span id="a873" class="lz kr hh lv b fi mk mb l mc md">library(ggplot2)<br/>library(ggstance)<br/>ggplot(logisticCoef, aes(x=estimate,y=term,colour=y.level))+<br/>  geom_pointrangeh(aes(xmin=conf.low,<br/>                     xmax=conf.high),<br/>    position=position_dodgev(height=0.75))</span></pre><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es ml"><img src="../Images/de0bdb1216c80fac7b9948ee15c5a997.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SZ8JqYobrv-POYO1hqpLHg.png"/></div></div></figure><p id="8ece" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这些系数可以解释为对数优势的增长率，即相应概率与基线概率(在这种情况下为A)的对数比。为了更清楚，我们可以通过执行<em class="lq"> exp(coef(summary(logistic)))对系数取幂，得到</em>:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="9049" class="lz kr hh lv b fi ma mb l mc md">(Intercept) GenderMale Ever_MarriedYes      Age GraduatedYes ProfessionDoctor ProfessionEngineer<br/>B   0.6556507  0.8196701       1.1492753 1.017232    1.2947892        0.6310081          0.5131216<br/>C   0.4981254  0.7338070       0.9014693 1.026661    2.0746153        0.5325417          0.1616843<br/>D   0.5643926  1.3629211       0.7289846 0.976456    0.6012758        3.2145211          2.3012150<br/>  ProfessionEntertainment ProfessionExecutive ProfessionHealthcare ProfessionHomemaker ProfessionLawyer<br/>B               0.4438931           0.7681000             1.091227           0.8583325        0.3200111<br/>C               0.2033792           0.5538787             1.478022           0.2959531        0.2019728<br/>D               1.9855688           4.8948716            18.097633           5.0341582        6.2651691<br/>  ProfessionMarketing Work_Experience Spending_ScoreHigh Spending_ScoreLow Family_Size Var_1Cat_2 Var_1Cat_3<br/>B           0.4918468       0.9652885          0.6628904         0.4002383    1.156976  1.5050627  1.1934885<br/>C           0.5240414       0.9606642          0.4721271         0.1624359    1.398437  1.1531614  0.8413473<br/>D           8.4971679       1.0395635          0.8459302         1.5758695    1.090444  0.6683399  0.6280151<br/>  Var_1Cat_4 Var_1Cat_5 Var_1Cat_6 Var_1Cat_7<br/>B  0.8827116  1.0912054  1.2387490  0.8506213<br/>C  0.3615101  0.7691944  1.3285245  0.6606126<br/>D  0.7465317  0.3978832  0.7527191  0.6191046</span></pre><p id="cb6c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如，0.8197位于第一行的第二列。这意味着，在所有其他变量的条件下，如果该个体是男性，女性个体被分类为B段对A段的概率odd将改变0.8197倍。因此，我们可以说，由于0.8197小于1，与分段A相比，男性而不是女性使得个体更不可能被分类为分段B。</p><p id="e7e8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然后，我们可以对测试数据集运行预测:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="2dd9" class="lz kr hh lv b fi ma mb l mc md">logisticPred &lt;- predict(logistic, type="probs", newdata=test)</span></pre><p id="3823" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">并且<em class="lq"> logisticPred </em>的值将如下所示:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="26c9" class="lz kr hh lv b fi ma mb l mc md">A          B           C           D<br/>1   0.13271824 0.35385636 0.495882878 0.017542517<br/>2   0.11437084 0.13800631 0.236004931 0.511617926<br/>3   0.05315289 0.02370280 0.009055350 0.914088962<br/>4   0.04043156 0.03323670 0.029556623 0.896775118<br/>5   0.22012487 0.35575141 0.395457037 0.028666685<br/>6   0.06785814 0.04958093 0.029743505 0.852817429<br/>7   0.07138501 0.22619329 0.427858435 0.274563265<br/>...</span></pre><p id="e6b2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在预测中，第一个实例有0.1327的概率是A，0.3539的概率是B，0.4959的概率是C，0.0175的概率是d。</p><p id="81f7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，让我们来检查混淆矩阵和预测的准确性:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="c484" class="lz kr hh lv b fi ma mb l mc md">segmentationsLabel &lt;- c("A", "B", "C", "D")<br/>logisticPred &lt;- as.factor(segmentationsLabel[max.col(logisticPred)])<br/># install.packages(c("caret"))<br/>library(caret)<br/>confusionMatrix(logisticPred, test$Segmentation)</span></pre><p id="1304" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将得到输出:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="3e48" class="lz kr hh lv b fi ma mb l mc md">Confusion Matrix and Statistics<br/><br/>          Reference<br/>Prediction  A  B  C  D<br/>         A 63 44 23 45<br/>         B 17 25 15 16<br/>         C 18 48 79  5<br/>         D 23 17 21 95<br/><br/>Overall Statistics<br/>                                          <br/>               Accuracy : 0.4729          <br/>                 95% CI : (0.4307, 0.5155)<br/>    No Information Rate : 0.2906          <br/>    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       <br/>                                          <br/>                  Kappa : 0.2971          <br/>                                          <br/> Mcnemar's Test P-Value : 2.013e-08       <br/><br/>Statistics by Class:<br/><br/>                     Class: A Class: B Class: C Class: D<br/>Sensitivity            0.5207  0.18657   0.5725   0.5901<br/>Specificity            0.7413  0.88571   0.8293   0.8448<br/>Pos Pred Value         0.3600  0.34247   0.5267   0.6090<br/>Neg Pred Value         0.8470  0.77339   0.8540   0.8342<br/>Prevalence             0.2184  0.24188   0.2491   0.2906<br/>Detection Rate         0.1137  0.04513   0.1426   0.1715<br/>Detection Prevalence   0.3159  0.13177   0.2708   0.2816<br/>Balanced Accuracy      0.6310  0.53614   0.7009   0.7174</span></pre><p id="9652" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">多项式逻辑回归的最终检验精度为0.4729。</p><h1 id="060a" class="kq kr hh bd ks kt me kv kw kx mf kz la in mg io lc iq mh ir le it mi iu lg lh bi translated">随机森林</h1><p id="c380" class="pw-post-body-paragraph iw ix hh iy b iz li ii jb jc lj il je jf lk jh ji jj ll jl jm jn lm jp jq jr ha bi translated">随机森林是一种由决策树组成的分类模型。简单决策树的一个例子是，给定一些猫或人的图片，我们试图通过一个策略来决定给定图片中的内容:<em class="lq">所有包含尾巴的图片都是猫的图片，否则都是人的图片。</em>这可能不是一个完美的模型(因为不是所有的猫图片都有尾巴)，所以我们可能想要添加更多的策略:是否有尖耳朵，是否有皮毛……这些策略中的每一个都是决策树，我们可以通过将它们放在一起得到一个随机森林:<em class="lq">所有包含尾巴、尖耳朵或皮毛的图片都是猫的图片，否则都是人的图片。最终的决定是由多数票做出的。</em></p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es mm"><img src="../Images/e62cedd9197ea0de942c4ac566c4770d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hmtbIgxoflflJqMJ_UHwXw.jpeg"/></div></div><figcaption class="ke kf et er es kg kh bd b be z dx translated">来源:https://www.youtube.com/watch?v=goPiwckWE9M<a class="ae ki" href="https://www.youtube.com/watch?v=goPiwckWE9M" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="86f5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了适应一个随机森林，我们将使用包<em class="lq"> party </em>:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="eeac" class="lz kr hh lv b fi ma mb l mc md"># install.packages(c("party", "partykit"))<br/>library(party)<br/>library(partykit) <br/>rf &lt;- ctree(Segmentation ~. , data=train)</span></pre><p id="20a8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过运行下面的代码，我们将得到下面的图:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="9d14" class="lz kr hh lv b fi ma mb l mc md">plot(rf, gp = gpar(fontsize = 3))</span></pre><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es ml"><img src="../Images/b05d7ca0b8f6166720d077019a78c7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Los1NzaVuveMOdrG3LFevQ.png"/></div></div></figure><p id="4628" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在图中，如果我们放大，我们可以清楚地看到随机森林的结构——每个决策是如何做出的，决策树是如何组成的。在这种情况下，决策树太多，图形变得太密集，所以我们也可以运行<em class="lq"> print(rf) </em>来获得文本结构:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="1030" class="lz kr hh lv b fi ma mb l mc md">Model formula:<br/>Segmentation ~ ID + Gender + Ever_Married + Age + Graduated + <br/>    Profession + Work_Experience + Spending_Score + Family_Size + <br/>    Var_1<br/><br/>Fitted party:<br/>[1] root<br/>|   [2] Profession in Artist, Doctor, Engineer, Entertainment, Executive, Homemaker, Lawyer<br/>|   |   [3] Spending_Score in Average, High<br/>|   |   |   [4] Graduated in No<br/>|   |   |   |   [5] Var_1 in Cat_1, Cat_5, Cat_6, Cat_7<br/>|   |   |   |   |   [6] Profession in Artist, Entertainment<br/>|   |   |   |   |   |   [7] Age &lt;= 32: A (n = 9, err = 55.6%)<br/>|   |   |   |   |   |   [8] Age &gt; 32: C (n = 91, err = 56.0%)<br/>|   |   |   |   |   [9] Profession in Doctor, Engineer, Executive, Homemaker, Lawyer: B (n = 223, err = 61.9%)<br/>|   |   |   |   [10] Var_1 in Cat_2, Cat_3, Cat_4: A (n = 171, err = 63.7%)</span><span id="01c7" class="lz kr hh lv b fi mk mb l mc md">...</span><span id="c6e6" class="lz kr hh lv b fi mk mb l mc md">Number of inner nodes:    34<br/>Number of terminal nodes: 35</span></pre><p id="edd5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然后，让我们用这个模型在测试集上运行预测:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="5dfc" class="lz kr hh lv b fi ma mb l mc md">predictRf &lt;- predict(rf, test)</span></pre><p id="aff5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">并计算混淆矩阵和测试准确度:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="cb49" class="lz kr hh lv b fi ma mb l mc md">confusionMatrix(predictRf, test$Segmentation)</span></pre><p id="4676" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">它会产生以下输出:</p><pre class="jt ju jv jw fd lu lv lw lx aw ly bi"><span id="e0f2" class="lz kr hh lv b fi ma mb l mc md">Confusion Matrix and Statistics<br/><br/>          Reference<br/>Prediction   A   B   C   D<br/>         A  64  39  18  43<br/>         B  16  20  12  11<br/>         C  17  54  86   5<br/>         D  24  21  22 102<br/><br/>Overall Statistics<br/>                                          <br/>               Accuracy : 0.491           <br/>                 95% CI : (0.4486, 0.5335)<br/>    No Information Rate : 0.2906          <br/>    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       <br/>                                          <br/>                  Kappa : 0.3196          <br/>                                          <br/> Mcnemar's Test P-Value : 3.522e-10       <br/><br/>Statistics by Class:<br/><br/>                     Class: A Class: B Class: C Class: D<br/>Sensitivity            0.5289   0.1493   0.6232   0.6335<br/>Specificity            0.7691   0.9071   0.8173   0.8295<br/>Pos Pred Value         0.3902   0.3390   0.5309   0.6036<br/>Neg Pred Value         0.8538   0.7697   0.8673   0.8468<br/>Prevalence             0.2184   0.2419   0.2491   0.2906<br/>Detection Rate         0.1155   0.0361   0.1552   0.1841<br/>Detection Prevalence   0.2960   0.1065   0.2924   0.3051<br/>Balanced Accuracy      0.6490   0.5282   0.7202   0.7315</span></pre><p id="a702" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">随机森林的最终测试精度为0.4910。</p><h1 id="4e30" class="kq kr hh bd ks kt me kv kw kx mf kz la in mg io lc iq mh ir le it mi iu lg lh bi translated">神经网络</h1><p id="fe9e" class="pw-post-body-paragraph iw ix hh iy b iz li ii jb jc lj il je jf lk jh ji jj ll jl jm jn lm jp jq jr ha bi translated">多层前馈神经网络是通用逼近器，这意味着它们可以很好地逼近任何函数。它们由基于输入和权重计算的互连节点层组成。权重在训练期间被调整，因此网络可以从输入数据中学习重要的模式。在本演示中，我们将使用一个两层前馈神经网络。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es mn"><img src="../Images/781ad6142f364184230ab90acac80d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BJSeR-TsSzO_yMEUwPx-3A.png"/></div></div><figcaption class="ke kf et er es kg kh bd b be z dx translated">来源:<a class="ae ki" href="https://commons.wikimedia.org/wiki/File:Neural_network.svg" rel="noopener ugc nofollow" target="_blank"><em class="mo">https://commons.wikimedia.org/wiki/File:Neural_network.svg</em></a></figcaption></figure><p id="c598" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将使用PyTorch来适应网络。您可能希望确保您的计算机上已经安装了Python3。</p><p id="0c23" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由于已经有很多关于如何编写神经网络的文章，我就不详细介绍这些步骤了。以下是用于产生结果的脚本:</p><div class="mp mq ez fb mr ms"><a href="https://github.com/jinyu-hou/medium-blog-scripts/blob/main/2021-Apr-advice/neuralnet.py" rel="noopener  ugc nofollow" target="_blank"><div class="mt ab dw"><div class="mu ab mv cl cj mw"><h2 class="bd hi fi z dy mx ea eb my ed ef hg bi translated">金玉-侯/中-博客-剧本</h2><div class="mz l"><h3 class="bd b fi z dy mx ea eb my ed ef dx translated">在GitHub上创建一个帐户，为jinyu-hou/medium-blog-scripts的开发做出贡献。</h3></div><div class="na l"><p class="bd b fp z dy mx ea eb my ed ef dx translated">github.com</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng kc ms"/></div></div></a></div><p id="e86d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过使用上面的脚本来训练200个epoch，我们得到了下面的训练曲线，该曲线是训练/验证准确度与epoch数量的关系图。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es nh"><img src="../Images/1ca62eef4c9d94c6ee45a8d9bcaaab9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*QQ9vFfvtTOhptBdCv9qqSQ.png"/></div></figure><p id="bd4a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最终测试精度为0.5045。</p><p id="467c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你可能已经知道，神经网络的参数是不可解释的。这意味着我们正在训练一个黑盒，没有人能说出模型是如何产生输出的。</p></div><div class="ab cl kj kk go kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="ha hb hc hd he"><h1 id="46b3" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">讨论</h1><p id="5c46" class="pw-post-body-paragraph iw ix hh iy b iz li ii jb jc lj il je jf lk jh ji jj ll jl jm jn lm jp jq jr ha bi translated">关于模型的选择，我想从最终的<strong class="iy hi">检验精度</strong>、<strong class="iy hi">拟合容易度</strong>和<strong class="iy hi">可解释性</strong>三个方面对三个模型——逻辑回归、随机森林和前馈神经网络进行比较和对比。</p><p id="d4b5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">三种模型产生的最终检验精度相似，其中神经网络产生的精度最高，逻辑回归产生的精度最低。然而，关于测试准确性的模型性能可能高度依赖于它被训练的数据。神经网络的结果还取决于参数初始化和超参数选择的随机性(学习速率、神经网络结构、缓存数、正则化等)。最好的办法是尝试拟合和调整所有候选模型，并选择一个产生最高测试精度的模型。这可能不是在所有情况下都可行。因此，我们也可以根据数据集的属性选择模型:它是多项式的吗？能否根据一组决策树策略轻松地对数据进行分类？模型有固定的形状吗？等等。</p><p id="772a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当您训练计算能力有限的模型时，您可能希望在选择模型时考虑拟合的容易程度。当数据集很大时，这一点尤其重要。如果您无法访问GPU资源，拟合的难易程度将是逻辑回归&gt;随机森林&gt;神经网络。如果没有GPU资源，使用神经网络进行深度学习可能会非常昂贵。</p><p id="411a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，可解释性也是您可能要考虑的一个重要因素，尤其是当您想要使用模型从数据集学习人类可读的模式时(例如，响应和解释变量之间的相关性)。正如我们上面提到的，神经网络很难解释。逻辑回归系数和随机森林政策都有不同的解释方式。您可能希望根据它们可能被解释的方式来选择模型。</p></div><div class="ab cl kj kk go kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="ha hb hc hd he"><p id="24e3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">您可以在下面的资源库中找到所有演示脚本:</p><div class="mp mq ez fb mr ms"><a href="https://github.com/jinyu-hou/medium-blog-scripts/tree/main/2021-Apr-advice" rel="noopener  ugc nofollow" target="_blank"><div class="mt ab dw"><div class="mu ab mv cl cj mw"><h2 class="bd hi fi z dy mx ea eb my ed ef hg bi translated">金玉-侯/中-博客-剧本</h2><div class="mz l"><h3 class="bd b fi z dy mx ea eb my ed ef dx translated">在GitHub上创建一个帐户，为jinyu-hou/medium-blog-scripts的开发做出贡献。</h3></div><div class="na l"><p class="bd b fp z dy mx ea eb my ed ef dx translated">github.com</p></div></div></div></a></div></div></div>    
</body>
</html>