# ML18: NLP —姓氏分类

> 原文：<https://medium.com/analytics-vidhya/ml18-6e9b1b66c30e?source=collection_archive---------19----------------------->

## 一个迷你项目实现 MLP，有线电视新闻网，RNN 与 PyTorch

**关键词**:自然语言处理(NLP)、文本挖掘、分类、多层感知器(MLP)、卷积神经网络(CNN)、递归神经网络(RNN)

> ***完整 PPT&WORD on GitHub****:*[https://bit.ly/3VLcxYp](https://bit.ly/3VLcxYp)***完整 Python code on Colab****:
> 1 .预处理:*[https://bit.ly/3hGWYhs](https://bit.ly/3hGWYhs) *2。*[https://bit.ly/3nTLxpf](https://bit.ly/3nTLxpf) *MLP 3。CNN:*[https://bit.ly/3rwRCKr](https://bit.ly/3rwRCKr) *4。*https://bit.ly/3ptfcG8[RNN](https://bit.ly/3ptfcG8)

代码主要来源于本书 ***Rao，D. & McMahan，B. (2019)。用 PyTroch 进行自然语言处理。加利福尼亚州:奥莱利媒体*** 。在此基础上，我调整了模型并可视化了结果。

这篇文章实际上是我在 *NCCU* 的 *PyTorch 和机器学习*课程的最终项目。

![](img/24d67889a85c8c45a225e333e1aa2217.png)

图 1: Rao，d .和 McMahan，B. (2019 年)。用 PyTroch 进行自然语言处理。加利福尼亚州:奥赖利媒体。

> ***大纲*** *(1)*[*数据介绍&数据预处理*](#aefc) *(2)*[*神经网络调优技巧*](#b4a9)**[*模型介绍&基线模型*](#a76f)**

# **(1)* 数据介绍&数据预处理*

## *1.数据介绍*

*姓氏数据集，是作者从互联网上不同的姓名来源收集到的 18 个不同国籍的 10000 个姓氏的集合。我们将数据分成 70%的训练数据、15%的验证数据和 15%的测试数据。*

## *2.数据预处理*

*首先，我们将姓氏数据集分成 70%的训练数据、15%的验证数据和 15%的测试数据。*

*接下来，我们使用*一键编码*对姓氏进行矢量化。我们利用两种变体——“折叠的独热向量”和“独热矩阵”我们在 MLP 使用“折叠的热点向量”来节省运行时间，在 RNN 使用 CNN &的“热点矩阵”。简而言之，“折叠的独热向量”不保留顺序信息，只给出字符出现或不出现的布尔值，而“独热矩阵”记录顺序信息。*

# *(2)神经网络调谐提示*

*检查 ML17 的神经网络调谐提示。*

*[](/analytics-vidhya/ml17-a2f9315e5f1a) [## ML17:调整深层网络

### 活化剂、优化剂、时期、小批量、BN、漏失和重量衰减

medium.com](/analytics-vidhya/ml17-a2f9315e5f1a)* 

# *(3)模型介绍&基线模型*

## *1.模型介绍*

***神经元**:神经网络的最小单元。
**感知器**:单层神经网络。
**FNN(前馈神经网络)** : MLP(多层感知器，也叫“全连接”网络)，CNN(卷积神经网络)。
**RNN(递归神经网络)** : RNN，LSTM(长短期记忆)，GRU(门控递归单元)。*

## *2.基线模型*

*我们使用从书中检索的 MLP 和 CNN 的完全相同的模型分别作为 MLP 和 CNN 的基线模型。至于 RNN，我们做了同样的事情，但略微调整了*下降概率*从 50%到 0%,因为 0%产生更好的性能。*

# **MLP**

## *1.基线 MLP 模型*

*![](img/10cfda801698e587fed1e4621030e96f.png)**![](img/4aa49319059db2e36561f2a512403beb.png)**![](img/16b660f362a5f3ca5f9d1de8b53c5f0c.png)*

*图 2、3 和 4:基线 MLP 模型的细节。*

## *2.最佳 MLP 模特*

*![](img/f16886da2fb3e9c00519135ba722de99.png)**![](img/e2fce791cc0407743e25702ba56448eb.png)*

*图 5 和图 6:最佳 MLP 模型的细节。*

## *3.MLP 摘要*

*![](img/b68128e6f273746c185c9ab1e63bf74c.png)*

*图 7:MLP 模型总结。*

# *(5)美国有线电视新闻网*

## *1.基线 CNN 模型*

*![](img/a53c780f7024e68b0e7fa8b10a6bbc33.png)**![](img/cad945d6b9ae2812971c50ce5fec5e86.png)**![](img/2220298e1aeb92680b7e88f2d56f5fdf.png)*

*图 8、9 和 10:基线 CNN 模型的细节。*

## *2.最佳 CNN 模式*

*![](img/bcf10a3ae5ec7575782ae3c2e01c7613.png)**![](img/83dc5609714c819974e1dc04285fe3af.png)**![](img/0233b696d3f6af468ac50689d7bda0b6.png)*

*图 11 & 12 & 13:最佳 CNN 模型的细节。*

## *3.CNN 摘要*

*![](img/2263a2fbad19b7b6c80190ff9c69d6ab.png)*

*图 14:CNN 模型总结。*

# *(6) RNN*

## *1.基线 RNN 模型*

*![](img/755909e9e0a1899aba51201046f50004.png)**![](img/a4a65aa780e7afa3fdf8568f180a7c66.png)**![](img/39e907f378be6b120528b38b7fa3844d.png)*

*图 15 & 16 & 17:基线 RNN 模型的细节。*

## *2.最佳 RNN 模特*

*![](img/3c5c0b9085edf9684d641f886e7e8fdb.png)**![](img/a6f532b543eeb16a3e4206ea59231b36.png)**![](img/87a8a8872d04c75af6d6cc1f889c609a.png)*

*图 18 & 19 & 20:最佳 RNN 模型的细节。*

## *3.RNN 摘要*

*![](img/9e86bf049d7a30277edef7542e88d6ec.png)*

*图 21:RNN 模型的总结。*

# *(7)结论*

*![](img/cf97f3d69afea922877f6d5dfafb3905.png)**![](img/a727556e7882b817d17438c5d20b81c1.png)*

*图 22 和 23:“MLP 对 CNN”和“CNN 对 RNN”*

*![](img/b2f32f7e5561d6f86146db68f86bbefa.png)**![](img/54017748f3dcc70b808e45cd414a53df.png)*

*图 24 和 25:“集成学习”和“前景”*

# **(8)参考文献**

*1.Géron，A. (2019)。使用 Scikit-Learn、Keras 和 TensorFlow 进行机器学习(第二版。).加利福尼亚州:奥赖利媒体。*

*2.Rao d .和 McMahan b .(2019 年)。用 PyTroch 进行自然语言处理。加利福尼亚州:奥赖利媒体。*

*3.萨卡尔博士(2019)。用 Python 进行文本分析(第二版。).印度卡纳塔克邦:新闻。*

*4.t . ganegedara(2018 年)。用张量流进行自然语言处理。英国伯明翰:Packt 出版公司。*

*5.Subramanian，V. (2018)。用 PyTorch 进行深度学习。英国伯明翰:Packt 出版公司。*

*6.j .帕特森和 a .吉布森(2017)。一个实践者的方法。加利福尼亚州:奥赖利媒体。*

*7\. 斎藤康毅 (2016). ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実. Japan, JP: O’Reilly Japan.*

*8.郭，男(2021)。ML16:动手文本预处理。检索自*

*[](/analytics-vidhya/ml16-ef6105b5bb34) [## ML16:动手文本预处理

### 自然语言处理的第一步&文本挖掘

medium.com](/analytics-vidhya/ml16-ef6105b5bb34)*