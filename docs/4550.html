<html>
<head>
<title>Analyzing Namo’s West-Bengal speech with Text Analytics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用文本分析法分析纳莫的西孟加拉邦演讲</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/analyzing-namos-west-bengal-speech-with-text-analytics-7a17f0c95917?source=collection_archive---------16-----------------------#2021-11-10">https://medium.com/analytics-vidhya/analyzing-namos-west-bengal-speech-with-text-analytics-7a17f0c95917?source=collection_archive---------16-----------------------#2021-11-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="1801" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="6a6a" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们知道日常生活中产生的大部分数据是非结构化数据。我们使用WhatsApp、脸书、Instagram等。我们发送消息，张贴图片，视频等。从这个角度来看，非结构化数据的分析是至关重要的，因为它具有价值&amp;我们可以从中提取可操作的见解。在本文中，我们将以印度总理纳伦德拉·莫迪的西孟加拉邦选举集会演讲的文本数据为案例，分析其演讲的显著性度量。</p></div><div class="ab cl ka kb go kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="ha hb hc hd he"><blockquote class="kh ki kj"><p id="a55d" class="jc jd kk je b jf kl jh ji jj km jl jm kn ko jp jq kp kq jt ju kr ks jx jy jz ha bi translated"><strong class="je hi">言语是心灵的镜子；一个人说什么，他就是什么。</strong></p><p id="5beb" class="jc jd kk je b jf kl jh ji jj km jl jm kn ko jp jq kp kq jt ju kr ks jx jy jz ha bi translated">-普布利柳斯·西鲁斯</p></blockquote><h1 id="b1c8" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">让我们开始工作吧</h1><h2 id="7acc" class="kt if hh bd ig ku kv kw ik kx ky kz io jn la lb is jr lc ld iw jv le lf ja lg bi translated"><strong class="ak">正在初始化</strong></h2><p id="4f19" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们导入用于分析的基本库。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="53e0" class="kt if hh lm b fi lq lr l ls lt"><em class="kk">#Import the requisite packages for analysis</em></span><span id="c87b" class="kt if hh lm b fi lu lr l ls lt">import pandas as pd     <br/>from urllib.request import Request,urlopen   <br/>from bs4 import BeautifulSoup as soup<br/>import nltk   #required library for text analytics<br/>import operator  <br/>from nltk.corpus import stopwords<br/>from wordcloud import WordCloud<br/>import matplotlib<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span></pre><p id="bc5b" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated">这里，主要的库是<em class="kk">URL lib</em>&amp;<em class="kk">beautiful soup</em>以可读格式访问数据。我们有<em class="kk"> nltk </em>库用于一些基本的文本预处理&amp; <em class="kk"> matplotlib，WordCloud </em>用于可视化。</p><h1 id="6abd" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">访问数据</strong></h1><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="2aa8" class="kt if hh lm b fi lq lr l ls lt"><em class="kk">#store the web url in variable url</em><br/>url = '<a class="ae lv" href="https://www.bjp.org/en/speechdetail/4542801/Salient-points-of-speech-of-Hon-ble-Prime-Minister-Shri-Narendra-Modi-addressing-public-meetings-in-West-Bengal-'" rel="noopener ugc nofollow" target="_blank">https://www.bjp.org/en/speechdetail/4542801/Salient-points-of-speech-of-Hon-ble-Prime-Minister-Shri-Narendra-Modi-addressing-public-meetings-in-West-Bengal-'</a></span><span id="05d5" class="kt if hh lm b fi lu lr l ls lt"><em class="kk"># Launch a formal request to the website server</em><br/>req = Request(url,headers={'User-Agent':'Mozilla/5.0'})</span><span id="dc09" class="kt if hh lm b fi lu lr l ls lt"><em class="kk">#Read the webpage</em><br/>webpage = urlopen(req).read()</span><span id="eb89" class="kt if hh lm b fi lu lr l ls lt"><em class="kk">#Use Beautiful Soup to parse the html webpage</em><br/>Soup = soup(webpage,"html.parser")</span><span id="6691" class="kt if hh lm b fi lu lr l ls lt"># Get the text from the soup &amp; store in data<br/>data = Soup.getText()</span></pre><p id="e0e0" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated">2021年4月10日，印度总理纳伦德拉·莫迪在西里古里的一次选举集会上发表了演讲。你可以访问网址阅读整个演讲。请注意-演讲是在印度。</p><h1 id="3588" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">文本预处理</strong></h1><p id="933c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">现在我们有了数据。让我们看看变量“数据”包含什么。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="21c2" class="kt if hh lm b fi lq lr l ls lt">print(data)</span></pre><figure class="lh li lj lk fd lx er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es lw"><img src="../Images/782c96492d362cc66d186ae14839fd04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pluuRYzg1cEL2kJpYRLSog.png"/></div></div></figure><p id="72fb" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated">我们看到美汤方法提取了网站的所有文字。我们必须清理提取的文本，以便只留下我们需要的单词。</p><p id="0980" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated">在这里，我们进行如下操作:</p><ol class=""><li id="9264" class="me mf hh je b jf kl jj km jn mg jr mh jv mi jz mj mk ml mm bi translated"><strong class="je hi">符号化&amp;规范化:</strong>我们将从这个文本中删除所有标点符号&amp;不必要的字符。仔细观察，我们使用<em class="kk">‘replace’</em>方法替换文本中的下列字符&amp;使用<em class="kk">‘split’</em>将句子转换为单词列表</li></ol><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="c280" class="kt if hh lm b fi lq lr l ls lt">#Remove garbage charachters, then, Tokenize &amp; Normalize.</span><span id="731e" class="kt if hh lm b fi lu lr l ls lt">tokens = data.replace("\n"," ").replace('*'," ").replace(',',' ').replace('।'," ").lower().split()</span><span id="0f2e" class="kt if hh lm b fi lu lr l ls lt">print(tokens)</span></pre><figure class="lh li lj lk fd lx er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mn"><img src="../Images/cc31625bed1b5f509414307ca97c6c09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Qq0ERbFfzWlVsmHU1TYjA.png"/></div></div></figure><p id="d970" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated"><strong class="je hi"> <em class="kk">标记化:</em> </strong> <em class="kk">将一个句子或段落拆分成更小的单元。用我们的话说。</em></p><p id="41f8" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated"><strong class="je hi"> <em class="kk">规范化:</em> </strong> <em class="kk">将文本数据转换为标准形式。在我们的例子中，我们将所有的英文单词转换成小写。对于印地语文本，我们在这个例子中不做任何标准化。</em></p><p id="9cb3" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated">2.<strong class="je hi">停用词移除:</strong>停用词是一种语言中可能存在于文本数据中但可能不会给句子本身增加太多价值的任何词。由于本网页的主要语言是印地语，我们将需要一个印地语停用词语料库。我们在这种情况下使用一个<a class="ae lv" href="#source -&gt; https://www.kaggle.com/ruchi798/hindi-stopwords" rel="noopener ugc nofollow"> <strong class="je hi"> <em class="kk">库</em> </strong> </a>。我们有3组停用词。一个是包含印地语单词的普通停用词文件。一个文件有积极的话&amp;另一个表示消极情绪的话。我们将它们导入&amp;并存储在一个列表中。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="376a" class="kt if hh lm b fi lq lr l ls lt"><em class="kk">#Defining the stopwords for Hindi.</em><br/>sw_hi = pd.read_csv("sw.txt", sep="\t",names=['stopword'])</span><span id="c89d" class="kt if hh lm b fi lu lr l ls lt"><em class="kk">#dataframe of stop words indicating positive &amp; negative sentiments.</em> <br/>pos_hi = pd.read_csv("pos.txt", sep="\t", names=['positive'])<br/>neg_hi = pd.read_csv("neg.txt", sep="\t", names=['negative'])</span><span id="6f17" class="kt if hh lm b fi lu lr l ls lt"><strong class="lm hi"><em class="kk">#source -&gt; </em></strong><a class="ae lv" href="https://www.kaggle.com/ruchi798/hindi-stopwords" rel="noopener ugc nofollow" target="_blank"><strong class="lm hi"><em class="kk">https://www.kaggle.com/ruchi798/hindi-stopwords</em></strong></a></span><span id="5aa9" class="kt if hh lm b fi lu lr l ls lt"><em class="kk">#Defining the stopwords for English using NLTK.</em><br/>sw_en = set(stopwords.words('english'))</span></pre><h1 id="a62e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">词频绘图&amp;绘图</strong></h1><p id="29d4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">现在，我们将检查单词列表中的每个单词是否出现在停用词文件中&amp;将它们分离为阳性、阴性和过滤的单词(不出现在任何列表中)</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="4e24" class="kt if hh lm b fi lq lr l ls lt"># Stopwords filtering</span><span id="2ee8" class="kt if hh lm b fi lu lr l ls lt">filtered_words =[] <em class="kk">#stores words which are not in any stopword list</em><br/>en_sw = []   <em class="kk">#stores english stopwords</em><br/>hi_sw = []   <em class="kk">#stores hindi stopwords</em><br/>hi_possw=[]  <em class="kk">#stores hindi postive stopwords</em><br/>hi_negsw=[]  <em class="kk">#stores negative positive stopwords</em><br/>    </span><span id="58e5" class="kt if hh lm b fi lu lr l ls lt">    for w in tokens:<br/>        if w in neg_hi:<br/>            hi_negsw.append(w)<br/>        elif w in sw_en:<br/>            en_sw.append(w)<br/>        elif w in sw_hi:<br/>            hi_sw.append(w)<br/>        elif w in pos_hi:<br/>            hi_possw.append(w)<br/>        else:<br/>            filtered_words.append(w)</span></pre><p id="3fe8" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated">接下来，我们将创建一个频率分布&amp;按降序对其进行排序，以获得使用频率最高的单词的数据。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="2f32" class="kt if hh lm b fi lq lr l ls lt">Freq_dist_fil =nltk.FreqDist(filtered_words)<br/>Freq_dist_pos =nltk.FreqDist(hi_possw)<br/>Freq_dist_neg =nltk.FreqDist(hi_negsw)</span><span id="0842" class="kt if hh lm b fi lu lr l ls lt">sorted_fil = sorted(Freq_dist_fil.items(),key=operator.itemgetter(1),reverse=True)</span><span id="49ca" class="kt if hh lm b fi lu lr l ls lt">sorted_pos = sorted(Freq_dist_pos.items(),key=operator.itemgetter(1),reverse=True)</span><span id="5412" class="kt if hh lm b fi lu lr l ls lt">sorted_neg = sorted(Freq_dist_neg.items(),key=operator.itemgetter(1),reverse=True)</span></pre><p id="f59e" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated">接下来，我们将绘制每个词分布的频率线图&amp;词云。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="e08e" class="kt if hh lm b fi lq lr l ls lt"><em class="kk"># filtered words</em><br/>print(sorted_fil[0:20])<br/>fil= pd.DataFrame(filtered_words,columns=['fil_words'])        <br/>filter_words = fil.groupby('fil_words').agg({'fil_words':'count'}).rename(columns={'fil_words':'Frequency'}).sort_values(by='Frequency',ascending=False)</span><span id="0504" class="kt if hh lm b fi lu lr l ls lt"><em class="kk"># Generate a word cloud image</em><br/>wordcloud = WordCloud(font_path='NirmalaB.ttf',stopwords = sw_hi,background_color='white')<br/>data = dict(sorted_fil)<br/>word = wordcloud.generate_from_frequencies(data)<br/>    <br/>    <br/>plt.figure(figsize=(15,5))<br/>plt.subplot(121)<br/>plt.plot(filter_words[:20])<br/>plt.xticks(fontname='Nirmala UI',rotation='vertical')<br/>plt.title('Frequency of Words used in website')<br/>plt.subplot(122)<br/>plt.imshow(word.to_image())<br/>plt.title('Wordcloud Frequency')<br/>plt.show();</span></pre><figure class="lh li lj lk fd lx er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mo"><img src="../Images/a184156528702ebf6b27b071fbbe9aa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gFQE-zK_JtktR-nIekW0HA.png"/></div></div></figure><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="fb74" class="kt if hh lm b fi lq lr l ls lt"><em class="kk">#positive words</em><br/>print(sorted_pos[0:20])<br/>pos= pd.DataFrame(hi_possw,columns=['pos_words'])        <br/>pos_words = pos.groupby('pos_words').agg({'pos_words':'count'}).rename(columns={'pos_words':'Frequency'}).sort_values(by='Frequency',ascending=False)</span><span id="f965" class="kt if hh lm b fi lu lr l ls lt">data_pos = dict(sorted_pos)<br/>word_pos = wordcloud.generate_from_frequencies(data_pos)</span><span id="55b8" class="kt if hh lm b fi lu lr l ls lt">plt.figure(figsize=(15,5))<br/>plt.subplot(121)<br/>plt.plot(pos_words[:20])<br/>plt.xticks(fontname='Nirmala UI',rotation='vertical')<br/>plt.title('Frequency of postive words')<br/>plt.subplot(122)<br/>plt.imshow(word_pos.to_image())<br/>plt.title('Postive words wordcloud')<br/>plt.show();</span></pre><figure class="lh li lj lk fd lx er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mp"><img src="../Images/6b11a9ca8995e00649e67d3d54c89c03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3uBoCe-sVTm-1i6xOQzaSw.png"/></div></div></figure><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="2aab" class="kt if hh lm b fi lq lr l ls lt"><em class="kk">#negative words</em><br/>print(sorted_neg[0:20])<br/>neg= pd.DataFrame(hi_negsw,columns=['neg_words'])        <br/>neg_words = neg.groupby('neg_words').agg({'neg_words':'count'}).rename(columns={'neg_words':'Frequency'}).sort_values(by='Frequency',ascending=False)</span><span id="aa36" class="kt if hh lm b fi lu lr l ls lt">data_neg = dict(sorted_neg)<br/>word_neg = wordcloud.generate_from_frequencies(data_neg)</span><span id="5f7a" class="kt if hh lm b fi lu lr l ls lt">plt.figure(figsize=(15,5))<br/>plt.subplot(121)<br/>plt.plot(neg_words[:20])<br/>plt.xticks(fontname='Nirmala UI',rotation='vertical')<br/>plt.title('Frequency of negative words')<br/>plt.subplot(122)<br/>plt.imshow(word_neg.to_image())<br/>plt.title('Negative words wordcloud')<br/>plt.show();</span></pre><figure class="lh li lj lk fd lx er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mq"><img src="../Images/63391da0a959da82643d70a8c5ac2281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1np_gEBw77mVsqtE7tWLHA.png"/></div></div></figure><h1 id="2af3" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">洞察力</h1><p id="4aa7" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">过滤后的单词:</strong></p><p id="4d71" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated">('दीदी'，76岁)，('टीएमसी'，15岁):他的讲话集中在执政党&amp;被广泛称为“迪迪”的世行首席部长身上。</p><p id="b8df" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated">('बंगाल'，59岁) :这是一个显而易见的词，因为集会是在世界银行。</p><p id="b208" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated">(“印度人民党”，30),('भाजपा'，20):这些话可能与印度人民党作为执政党能拿出什么有关，他经常提到他的政党。</p><p id="f9a9" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">正面词:</strong></p><p id="1684" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated">('सुरक्षा', 6),('सम्मान', 6) ,('मुक्त', 6),('संरक्षण', 4), ('लाभ', 4) ('परिवर्तन', 3), ('नया', 3).这些词在演讲中被认为是积极的词。这些词可能会影响公众的心态，预示着骄傲、安全、变革的承诺&amp;一个新时代的到来。</p><p id="7c2d" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated"><strong class="je hi">否定词:</strong></p><p id="e512" class="pw-post-body-paragraph jc jd hh je b jf kl jh ji jj km jl jm jn ko jp jq jr kq jt ju jv ks jx jy jz ha bi translated">('समस्या', 8), ('सख्त', 5), ('भय', 5), ('गलत', 5), ('गुस्सा', 5), ('हार', 4), ('अत्याचार', 3), ('अपमान', 3), ('बुराई', 2),('हिंसक', 2), ('खिलवाड़', 2), ('डराने', 2), ('डर', 2), ('अन्याय', 2), ('बोझ', 2), ('हत्या', 2).演讲中的这些话可能表明孟加拉人民遭受的暴行或不公正。</p><h1 id="658a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论:</h1><p id="73ba" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">词频分析可以帮助我们在一张快照中总结出给定文本中使用了什么情感或主要词汇。将非结构化数据结构化将带来洞见，有助于改变模式或理解现象为何如此。</p></div></div>    
</body>
</html>