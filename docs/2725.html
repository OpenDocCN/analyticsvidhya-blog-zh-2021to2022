<html>
<head>
<title>Linear Regression (Mathematics and Intuition behind Linear Regression)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归(线性回归背后的数学和直觉)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-38f0cd6856f5?source=collection_archive---------22-----------------------#2021-05-14">https://medium.com/analytics-vidhya/linear-regression-38f0cd6856f5?source=collection_archive---------22-----------------------#2021-05-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e6d0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">了解关于<strong class="ig hi">线性回归的一切。</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/ef29c2d2a9d6722dfe9f83fe759e7e3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sL_dtOLs8TAqICsU"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">由<a class="ae js" href="https://unsplash.com/@lukechesser?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">卢克·切瑟</a>在<a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="8e41" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">目录:</strong></p><p id="2424" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> ∘ </strong> <a class="ae js" href="#8247" rel="noopener ugc nofollow"> <strong class="ig hi">简介:</strong></a><strong class="ig hi"><br/>∘</strong><a class="ae js" href="#019b" rel="noopener ugc nofollow"><strong class="ig hi">代价函数:</strong></a><strong class="ig hi"><br/>∘</strong><a class="ae js" href="#8d01" rel="noopener ugc nofollow"><strong class="ig hi">梯度下降算法:</strong></a><strong class="ig hi"><br/>∘</strong><a class="ae js" href="#063d" rel="noopener ugc nofollow"><strong class="ig hi">实现:</strong></a><strong class="ig hi"><br/>∘</strong><a class="ae js" href="#51a6" rel="noopener ugc nofollow"><strong class="ig hi">总结:</strong></a></p><p id="961c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">线性回归是监督学习算法。<br/>用于预测连续值。<br/>它使用直线的方程来预测输出值。</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="5921" class="jy jz hh ju b fi ka kb l kc kd">y = mx+c </span></pre><p id="28d9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">考虑一下，我们有以下x和y的值，并且想要预测一些x值的值(y_pred)。那么我们如何做到这一点呢？？<br/>让我们看看...</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="dbd9" class="jy jz hh ju b fi ka kb l kc kd">x = np.round(np.random.uniform(5, 25, 10))<br/>y = x*2<br/>print(y, x)</span><span id="c067" class="jy jz hh ju b fi ke kb l kc kd">output - <br/>[34. 14. 28. 24. 30. 30. 44. 30. 18. 14.] [17.  7. 14. 12. 15. 15. 22. 15.  9.  7.]</span></pre><p id="4441" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们为x和y生成了一些值，我们将预测x值的值(y_pred ),然后我们将这些预测值(y_pred)与y的实际值进行比较，我们还将找到实际值和预测值之间的误差。</p><p id="1a82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们有一条直线的方程式。<strong class="ig hi"> y = mx+c. </strong> <br/>所以首先我们会预测当<strong class="ig hi"> m=1 </strong>和<strong class="ig hi"> c =0 </strong>时的值。</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="38af" class="jy jz hh ju b fi ka kb l kc kd">def predict_y(x, m=1, c=0):<br/>    return m*x+c<br/>y_pred = predict_y(x)<br/>y_pred</span><span id="c123" class="jy jz hh ju b fi ke kb l kc kd">output:<br/>array([17.,  7., 14., 12., 15., 15., 22., 15.,  9.,  7.])</span></pre><p id="0efc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们已经找到了<strong class="ig hi"> y_pred </strong> <br/>现在是时候比较<strong class="ig hi">预测的</strong>和<strong class="ig hi">y</strong>的实际值了。</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="ecd1" class="jy jz hh ju b fi ka kb l kc kd">plt.figure(figsize=(10, 5), dpi=200)<br/>plt.scatter(x, y, label="actual")<br/>plt.plot(x, y_pred, label="predicted")<br/>plt.xlabel('X values')<br/>plt.ylabel('Y values')<br/>plt.legend()<br/>plt.show()<br/>plt.savefig('lr.jpg')</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kf"><img src="../Images/13f2fa925d088e3d2b10484d49324d75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UZFpdAjWzMb2QG9lL-ypTg.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图1.1实际与预测</figcaption></figure><p id="e75e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里我们可以看到，我们预测的线不是最佳拟合线。<br/>我们使用了<strong class="ig hi"> m=1 </strong>和<strong class="ig hi"> c=0 </strong>的值。我们可以使用任何值的组合和无限数量的值的组合，所以我们如何获得那些最优值来找到我们的最佳拟合线。</p><p id="ff54" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">梯度下降算法来了。</strong></p><p id="0adf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先我们来了解一下<strong class="ig hi">成本函数</strong>。</p><h2 id="019b" class="jy jz hh bd kg kh ki kj kk kl km kn ko ip kp kq kr it ks kt ku ix kv kw kx ky bi translated"><strong class="ak">成本函数:</strong></h2><p id="6d58" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated"><strong class="ig hi">成本函数</strong>用于找出实际值和预测值之间的误差。下面是成本函数的公式。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es le"><img src="../Images/896dec59b9f20d8f953554fef84da706.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*dZe_QuY68HkYynQxN-uH0A.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图2.1成本函数的公式</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lf"><img src="../Images/96d754b11729ecbd4ed65a13af360e4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*XrXv6O9sWqbjyXcrb7A-cQ.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图2.2</figcaption></figure><p id="2d06" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是成本函数的python代码:</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="5475" class="jy jz hh ju b fi ka kb l kc kd">def cost_function(y_pred, y):<br/>    return (np.sum(np.square(y_pred-y)))/(2*len(y))</span><span id="e725" class="jy jz hh ju b fi ke kb l kc kd">cost_function(y_pred, y)</span><span id="38ba" class="jy jz hh ju b fi ke kb l kc kd">output:<br/>98.35</span></pre><p id="e7ff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们发现，当m=1和c=0时，我们的误差是<strong class="ig hi"> 98.35 </strong>，这是非常大的<strong class="ig hi">。</strong></p><p id="ace1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，为了简单起见，我们将看到'<strong class="ig hi"> m </strong>'与<strong class="ig hi"> c </strong> = <strong class="ig hi"> 0 </strong>的不同值的<strong class="ig hi">成本值</strong>。我们也可以使用'<strong class="ig hi"> c </strong>的任何值，但为此，我们需要绘制一个<strong class="ig hi"> 3D图</strong>，这将很难理解概念。</p><p id="7f06" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面的python代码是为了找出不同m值的误差值，我们还绘制了不同m值的图形。</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="ba27" class="jy jz hh ju b fi ka kb l kc kd">plt.figure(figsize=(10,5))</span><span id="330e" class="jy jz hh ju b fi ke kb l kc kd">M = [0, 0.5, 1, 1.5, 2, 2.5, 3]<br/>for m in M:<br/>    error = cost_function(predict_y(x, m), y)<br/>    <br/>    print(f'for m = {m} error is {error}')<br/>    plt.scatter(m, error)<br/>    <br/>plt.ylabel("cost function")<br/>plt.xlabel("m")<br/>plt.title("Error vs m")<br/>plt.savefig('costfunc.jpg')<br/>plt.show()</span><span id="417f" class="jy jz hh ju b fi ke kb l kc kd">output:</span><span id="ba4d" class="jy jz hh ju b fi ke kb l kc kd">for m = 0 error is 393.4<br/>for m = 0.5 error is 221.2875<br/>for m = 1 error is 98.35<br/>for m = 1.5 error is 24.5875<br/>for m = 2 error is 0.0<br/>for m = 2.5 error is 24.5875<br/>for m = 3 error is 98.35</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lg"><img src="../Images/d8530a634f2343d6a53fdd6b26836182.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4U1yIB_iWh7uFtNo08PYiw.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图2.3误差与m值的关系</figcaption></figure><p id="e020" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里我们可以看到对于<strong class="ig hi">m = 2</strong>T18】误差为0，其中是m的最优值。</p><p id="d039" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图中，我们为不同的<strong class="ig hi"> m </strong>值绘制了不同的预测线</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="bdc6" class="jy jz hh ju b fi ka kb l kc kd">plt.figure(figsize=(10,5))</span><span id="d523" class="jy jz hh ju b fi ke kb l kc kd">def plot_m(m, x):<br/>    plt.plot(x, predict_y(x, m), label=f'm={m}')</span><span id="ebcb" class="jy jz hh ju b fi ke kb l kc kd">for m in M:<br/>    plot_m(m, x)<br/>    <br/>plt.title('Diffrent values of M')<br/>plt.legend()<br/>plt.savefig('diff_M.jpg')<br/>plt.show()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lg"><img src="../Images/d26cbda20d9cc30fe6f32066dc1c5156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L9QqDNy9nlGQFQD19M8Vog.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图2.4不同的m值</figcaption></figure><p id="47ca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在该看<strong class="ig hi">梯度下降算法</strong>了。</p><h2 id="8d01" class="jy jz hh bd kg kh ki kj kk kl km kn ko ip kp kq kr it ks kt ku ix kv kw kx ky bi translated"><strong class="ak">梯度下降算法:</strong></h2><p id="5285" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">它是机器学习中非常重要的算法。<br/>也用于深度学习。<br/>此算法用于寻找最优属性值。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lh"><img src="../Images/daa6de7cb3be6bffb9c3c2ba85e0cb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*G3evFxIAlDchOx5Wl7bV5g.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图3.1梯度下降算法使用的公式</figcaption></figure><p id="9725" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是梯度下降算法使用的公式。<br/>第一项是<strong class="ig hi"> θ </strong>的旧值，第二项是<strong class="ig hi">成本函数</strong>的导数。<br/><br/>如果学习率太小，则需要太多的步骤才能找到值，因此需要太多的时间才能找到值。<br/>如果它太大，那么它将超过这些值，永远不会达到最佳值。<br/>该算法一直运行，直到没有找到<strong class="ig hi"> θ </strong>的最佳值。</p><p id="8dc4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果学习率很小，我们将观察到，在图形中会有大量彼此非常接近的点，而学习率很大，则会有非常少量的彼此远离的点。<br/>仅供参考，请看图2.3<strong class="ig hi">。</strong></p><p id="1db4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">到目前为止，我们已经讨论了<strong class="ig hi">线性回归、成本函数、</strong>和<strong class="ig hi">梯度下降算法</strong>。</p></div><div class="ab cl li lj go lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ha hb hc hd he"><p id="e06f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">现在是实施线性回归的时候了。</strong></p><h2 id="063d" class="jy jz hh bd kg kh ki kj kk kl km kn ko ip kp kq kr it ks kt ku ix kv kw kx ky bi translated">实施:</h2><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="f5a8" class="jy jz hh ju b fi ka kb l kc kd">from sklearn.linear_model import LinearRegression</span><span id="9e5e" class="jy jz hh ju b fi ke kb l kc kd">lr = LinearRegression()<br/>lr.fit(x.reshape(-1,1), y)<br/>pred = lr.predict(x.reshape(-1,1))</span><span id="efc4" class="jy jz hh ju b fi ke kb l kc kd">print(y, pred, sep='\n')</span><span id="b482" class="jy jz hh ju b fi ke kb l kc kd">output:</span><span id="1108" class="jy jz hh ju b fi ke kb l kc kd">[34. 14. 28. 24. 30. 30. 44. 30. 18. 14.]<br/>[34. 14. 28. 24. 30. 30. 44. 30. 18. 14.]</span></pre><p id="2d03" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> m </strong>的值:</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="7e85" class="jy jz hh ju b fi ka kb l kc kd">lr.coef_<br/>output:<br/>array([2.])</span></pre><p id="78a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> c </strong>的值:</p><pre class="jd je jf jg fd jt ju jv jw aw jx bi"><span id="777f" class="jy jz hh ju b fi ka kb l kc kd">lr.intercept_<br/>output:<br/>-7.105427357601002e-15</span></pre><p id="3e24" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">c值很小，几乎等于0。</p><p id="8701" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae js" href="https://github.com/sbswapnil/Data-Science/blob/main/Linear%20Regression.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">点击这里</strong> </a> <strong class="ig hi">获取我关于线性回归的完整Jupyter笔记本。</strong></p><h2 id="51a6" class="jy jz hh bd kg kh ki kj kk kl km kn ko ip kp kq kr it ks kt ku ix kv kw kx ky bi translated"><strong class="ak">概要:</strong></h2><ul class=""><li id="a54d" class="lp lq hh ig b ih kz il la ip lr it ls ix lt jb lu lv lw lx bi translated">我们讨论<strong class="ig hi">线性回归、</strong>成本函数和<strong class="ig hi">梯度下降算法。</strong></li></ul></div></div>    
</body>
</html>