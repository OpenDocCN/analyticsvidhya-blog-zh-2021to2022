<html>
<head>
<title>Solving Open AI’s CartPole Using Reinforcement Learning Part-2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用强化学习解决开放人工智能的横向问题(下)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/solving-open-ais-cartpole-using-reinforcement-learning-part-2-73848cbda4f1?source=collection_archive---------6-----------------------#2021-04-20">https://medium.com/analytics-vidhya/solving-open-ais-cartpole-using-reinforcement-learning-part-2-73848cbda4f1?source=collection_archive---------6-----------------------#2021-04-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="5f05" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在第一篇<a class="ae jc" rel="noopener" href="/analytics-vidhya/q-learning-is-the-most-basic-form-of-reinforcement-learning-which-doesnt-take-advantage-of-any-8944e02570c5"> <strong class="ig hi">教程</strong>，</a>中，我介绍了最基础的强化学习方法，叫做<strong class="ig hi"> Q-learning </strong>来解决侧翻问题。由于它的计算限制，它工作在简单的环境中，其中状态的数量相对较少。</p><p id="ea19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在更复杂的环境中，为每个动作计算、存储和更新<strong class="ig hi"> Q值</strong>要么是不可能的，要么是非常低效的<strong class="ig hi">。这就是<strong class="ig hi">深度Q网</strong>发挥作用的地方。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/9c77971481dd80881e451bb51ea61a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*C_Aht8nRx_GxE4JJ"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">亨特·哈里特在<a class="ae jc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><h1 id="116b" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">背景信息</strong></h1><p id="c242" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">深度Q学习是DeepMind团队在2013年的<strong class="ig hi"> </strong> <a class="ae jc" href="https://arxiv.org/pdf/1312.5602.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">用深度强化学习玩雅达利</strong> </a>论文中引入的。第一个类似的方法是在1992年使用TD-gammon实现的。算法达到了一个<strong class="ig hi">超人</strong>级别的玩双陆棋。不过，这种方法并不适用于国际象棋、围棋或跳棋等游戏。</p><p id="bac3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">DeepMind在7款雅达利游戏中有3款超过了人类的表现，使用了<strong class="ig hi">原始</strong> <strong class="ig hi">图像</strong>和所有游戏的相同超参数。这是在更普通的学习领域的一个突破。</p><p id="e700" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">DQN的基本思想是它结合了Q学习和深度学习。我们去掉了Q表，而使用神经网络来代替<strong class="ig hi">逼近</strong>动作值函数(Q(s，a))。状态被传递到网络，作为输出，我们接收每个动作的估计的<strong class="ig hi"> Q值</strong>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kw"><img src="../Images/93ad2d69af5a4f7a6c77739ea55e2781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*SOJDSYH_txhuEV4AFSAbHg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">DQN建筑</figcaption></figure><p id="b8dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了训练网络，我们需要一个目标值，也称为基本事实。问题是，我们如何在实际上没有标记数据集的情况下评估<strong class="ig hi">损失函数</strong>？</p><p id="4094" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">嗯，我们<strong class="ig hi">使用贝尔曼方程在运行中创建</strong>目标值。</p><div class="je jf jg jh fd ab cb"><figure class="kx ji ky kz la lb lc paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/81ffde076c25f8bcd438769f8304c1fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*x4ZCq4Bs7ti-GrPjdxSNjA.png"/></div></figure><figure class="kx ji ld kz la lb lc paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/a1957e6a8f5d6b8e4eefdce15f9804a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*iMFr6Z1PELFPjXLohJW5ng.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx le di lf lg translated">贝尔曼方程和损失函数L</figcaption></figure></div><p id="ccc2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个方法叫做<strong class="ig hi">自举</strong>。我们正试图<strong class="ig hi">根据另一个估计来估计</strong>某事。本质上，我们通过估计<strong class="ig hi">未来</strong>Q(s’，a)来估计<strong class="ig hi">当前</strong>动作值Q(s，a)。</p><p id="fd82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当使用一个网络来预测两个值时，问题就出现了。类似于狗抓自己的尾巴。权重被更新以使<strong class="ig hi">预测</strong>更接近目标Q值。尽管如此，目标值也将<strong class="ig hi">向前移动</strong>，因为我们使用相同的网络。</p><p id="e345" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">解决方案已经在DeepMind论文<a class="ae jc" href="https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">中提出，通过深度强化学习</strong> </a> <strong class="ig hi">进行人级控制。</strong>我们的想法是使用一个独立的网络来预测目标值。每C个时间步，来自策略网络的权重被复制到<strong class="ig hi">目标网络</strong>。它为算法提供了更大的稳定性，因为我们的网络不是试图追逐一个不稳定的目标。</p><p id="00e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了使神经网络工作，我们需要四个值状态，行动(A)，回报(R)，未来状态(S’)。这些值存储在重放记忆向量中，然后随机采样进行训练。这个过程叫做<strong class="ig hi">体验回放</strong>，也被<a class="ae jc" href="https://arxiv.org/pdf/1312.5602.pdf" rel="noopener ugc nofollow" target="_blank"> DeepMind </a>引入。</p><p id="4477" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，我们使用了一种受<strong class="ig hi">生物学</strong>启发的机制，称为经验重放，它对数据进行随机化处理，从而消除观察序列中的相关性，并平滑数据分布的变化。为了执行体验重放，我们存储代理的体验et=(st，at，rt，st+1)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lh"><img src="../Images/9211d1d3561d7b5951c048f586579b78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dUmk8U6S54o9cRXL0hZi_g.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">使用经验重放和目标网络的结果[3]</figcaption></figure><blockquote class="li lj lk"><p id="1225" class="ie if ll ig b ih ii ij ik il im in io lm iq ir is ln iu iv iw lo iy iz ja jb ha bi translated">具有经验回放和目标网络的深度Q学习培训流程:</p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lp"><img src="../Images/ad51b97c6472e39a3bfa3f6f0d07c481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CRaNtPXkw51PRs9QbvEM5w.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">[3]</figcaption></figure><h1 id="8c9a" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">实施细节</strong></h1><ol class=""><li id="ec5f" class="lq lr hh ig b ih kr il ks ip ls it lt ix lu jb lv lw lx ly bi translated"><strong class="ig hi">环境</strong></li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/b8b5dcb5c90291e80b3d81ae8c47f879.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*C6T-7ytta7KRbejnCfOtXQ.gif"/></div></figure><blockquote class="li lj lk"><p id="f47d" class="ie if ll ig b ih ii ij ik il im in io lm iq ir is ln iu iv iw lo iy iz ja jb ha bi translated">一根杆子通过一个非驱动关节连接到一辆小车上，小车沿着一条无摩擦的轨道移动。通过对推车施加+1或-1的力来控制该系统。钟摆开始直立，目标是防止它翻倒。杆保持直立的每个时间步长提供+1的奖励。当柱子偏离垂直方向超过15度，或者手推车偏离中心超过2.4个单位时，该集结束。[4]</p></blockquote><p id="1ff6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.<strong class="ig hi">网络</strong></p><p id="188d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该架构基于具有<strong class="ig hi"> Relu </strong>激活功能的<strong class="ig hi">全连接层</strong>。输出层是一个全连接层，每个<strong class="ig hi">动作</strong>有<strong class="ig hi">两个</strong>输出。<br/>和强化学习中的很多论文一样，我使用了<strong class="ig hi"> RMSProp </strong>优化器。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ma"><img src="../Images/4f72faef779150448e9b55cada17087e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ui5OfhB9aR1bIySNHr94pw.png"/></div></div></figure><p id="38fb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.<strong class="ig hi">超参数</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mb"><img src="../Images/b50cb859f3109fca6e3a1444f969727b.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*OhUCV2i9yxBz8zZMoKYQSQ.png"/></div></figure><p id="b939" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.<strong class="ig hi">代码</strong></p><p id="6cba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在<a class="ae jc" href="https://github.com/maciejbalawejder/ReinforcementLearning-collection/tree/main/DQN" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> Github </strong> </a> <strong class="ig hi">上有剧情的版本。</strong></p><ul class=""><li id="a7af" class="lq lr hh ig b ih ii il im ip mc it md ix me jb mf lw lx ly bi translated"><strong class="ig hi">定义车型</strong></li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mg mh l"/></div></figure><ul class=""><li id="53d3" class="lq lr hh ig b ih ii il im ip mc it md ix me jb mf lw lx ly bi translated"><strong class="ig hi">经验回放</strong></li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mg mh l"/></div></figure><ul class=""><li id="17ec" class="lq lr hh ig b ih ii il im ip mc it md ix me jb mf lw lx ly bi translated"><strong class="ig hi">ε用系数和用a，b，c参数来控制函数的形状</strong></li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mg mh l"/></div></figure><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mg mh l"/></div></figure><ul class=""><li id="f56f" class="lq lr hh ig b ih ii il im ip mc it md ix me jb mf lw lx ly bi translated"><strong class="ig hi">选择一个动作</strong></li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mg mh l"/></div></figure><ul class=""><li id="402b" class="lq lr hh ig b ih ii il im ip mc it md ix me jb mf lw lx ly bi translated"><strong class="ig hi">训练功能</strong></li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mg mh l"/></div></figure><ul class=""><li id="9eeb" class="lq lr hh ig b ih ii il im ip mc it md ix me jb mf lw lx ly bi translated"><strong class="ig hi">训练循环</strong></li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mg mh l"/></div></figure><ul class=""><li id="6e32" class="lq lr hh ig b ih ii il im ip mc it md ix me jb mf lw lx ly bi translated"><strong class="ig hi">测试回路</strong></li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mg mh l"/></div></figure><h2 id="01d2" class="mi ju hh bd jv mj mk ml jz mm mn mo kd ip mp mq kh it mr ms kl ix mt mu kp mv bi translated">结果</h2><p id="df00" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">左边的第一个图显示了在发作期间每次迭代中<strong class="ig hi">ε</strong>值的衰减。右图显示了由三个参数定义的ε函数，以实现阶跃函数形状。</p><p id="f543" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在剧集中获得最高分与epsilon值密切相关。当动作的随机性降低时，神经网络开始训练。保持最小值是为了防止随机状态转换记忆，也称为过拟合。</p><div class="je jf jg jh fd ab cb"><figure class="kx ji mw kz la lb lc paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/9ddfdab99151192230a12e78c23da9a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*7FlVC4DvpB-b4o6745IaYQ.png"/></div></figure><figure class="kx ji mw kz la lb lc paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/f6f23e71c2abbd7e14978437717c24a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*0yO86aKOc5eGwbV-H1wFIw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx mx di my lg translated">训练图</figcaption></figure></div><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mz"><img src="../Images/8d374a77ade4161184ea16c71c5bb2ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*tQxS1HC9WwUobbSnCAHt3g.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">使用上图右侧的模型对100集进行测试</figcaption></figure><p id="fcc5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用英特尔酷睿i5–10210 u CPU的培训过程大约需要4个小时，该模型似乎可以解决环境问题。</p><h1 id="7910" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">丰富</h1><p id="9ebb" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">与大多数突破模型不同，这里描述的问题是使用<strong class="ig hi">低维输入</strong>，它使用原始图像作为输入，然后提取所有特征。然而，这是一个很好的平台来理解深度Q学习的想法是多么美好和强大。</p><p id="072e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了减少训练时间，我会尝试不同形状的<strong class="ig hi"> epsilon的</strong>函数。另一个重要的超参数是目标模型更新频率。可以用软更新<em class="ll">、</em>来代替，其中<em class="ll"> </em>我们不立刻更新目标网络，而是频繁且非常少的更新【5】。</p><p id="cfdf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，对重放记忆中的经验进行优先排序可以提高训练过程的有效性[6]。</p><p id="7321" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你想看我的其他项目检查我的<a class="ae jc" href="https://maciejbalawejder.medium.com/" rel="noopener"> <strong class="ig hi">中</strong> </a>和<a class="ae jc" href="https://github.com/maciejbalawejder" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">Github</strong></a><strong class="ig hi"/>简介。</p><h1 id="bd0f" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">参考文献</strong></h1><p id="b731" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">[1] <a class="ae jc" href="https://arxiv.org/pdf/1312.5602.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1312.5602.pdf </a></p><p id="56fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2] <a class="ae jc" href="https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/</a></p><p id="7417" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3] <a class="ae jc" href="https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf </a></p><p id="bdb3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4] <a class="ae jc" href="https://gym.openai.com/envs/CartPole-v0/" rel="noopener ugc nofollow" target="_blank">https://gym.openai.com/envs/CartPole-v0/</a></p><p id="2020" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[5] <a class="ae jc" href="https://arxiv.org/abs/2008.10861" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2008.10861 </a></p><p id="935a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[6] <a class="ae jc" href="https://arxiv.org/abs/1511.05952" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1511.05952 </a></p></div></div>    
</body>
</html>