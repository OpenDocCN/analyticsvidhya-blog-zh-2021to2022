<html>
<head>
<title>Multi-label Text Classification using Transformers(BERT)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用变压器的多标签文本分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multi-label-text-classification-using-transformers-bert-93460838e62b?source=collection_archive---------2-----------------------#2021-03-12">https://medium.com/analytics-vidhya/multi-label-text-classification-using-transformers-bert-93460838e62b?source=collection_archive---------2-----------------------#2021-03-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="44e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">使用来自拥抱脸和PyTorch Lightning的预训练BERT模型预测Stack Exchange上发布的问题的标签</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/bf96b3036f1cc954a0aec64327df04ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WadwGDTk8_y9cFqhikp2QA.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><a class="ae ju" href="https://stackexchange.com/" rel="noopener ugc nofollow" target="_blank">栈交换</a></figcaption></figure><p id="2ef8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Stack Exchange </strong>是一个由176个社区组成的网络，由对某个特定主题充满热情的专家和爱好者创建和运营。该网站主要为用户提供一个提问和回答问题的平台。</p><p id="28d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有问题都用主题领域<strong class="ih hj">标记</strong>。每个标签最多可以有5个，因为一个问题可能与几个主题相关。标签使你很容易找到你感兴趣的领域的问题，并使你能够从给出的答案中学习，或者能够回答属于你专业领域的问题。标签可以由用户在发布问题时输入，也可以由StackExchange根据提出的问题进行预测。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jv"><img src="../Images/c78b4523c2a51f75230af1dfc1870359.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e1V_yV_aeKCYOjZ96DNTuQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">与问题相关联的标签示例</figcaption></figure></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><p id="3dfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">"预测&amp;将正确的标签与问题相关联是很重要的，这样可以确保问题引起所有人的注意，这些人可以根据标记的主题领域回答问题。这将增加更快响应的机会，从而推动更多参与"</em></p><h1 id="bede" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">目标:</strong></h1><p id="fcea" class="pw-post-body-paragraph if ig hi ih b ii lb ik il im lc io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">开发一个机器学习模型，该模型将准确预测可能与某个问题相关联的所有标签(一个或多个)。</p><p id="96ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">假设读者对自然语言处理(NLP)有一定的了解，并且对PyTorch &amp;变压器有一定的了解，特别是对BERT。这篇文章是我用变形金刚解决多标签文本分类问题的成果，希望它能帮助一些读者！</em></p><h1 id="12e9" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">方法:</strong></h1><p id="7f7d" class="pw-post-body-paragraph if ig hi ih b ii lb ik il im lc io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">预测“标签”的任务基本上是一个多标签文本分类问题。虽然可能有多种方法来解决这个问题，但我们的解决方案将基于利用预训练变压器(BERT)模型和<em class="jd"> PyTorch Lightning </em>框架的能力。</p><p id="e47a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">高级步骤:</strong></p><ol class=""><li id="82e0" class="lg lh hi ih b ii ij im in iq li iu lj iy lk jc ll lm ln lo bi translated">安装和导入库</li><li id="c087" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">加载和预处理数据</li><li id="c634" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">准备PyTorch数据集和Lightning数据模块</li><li id="8e85" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">定义模型(基于BERT的分类器)</li><li id="f3ee" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">使用闪电训练器训练模型</li><li id="5d5c" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">评估模型的性能</li><li id="2f61" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">模型推理</li></ol></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><h1 id="731a" class="kd ke hi bd kf kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la bi translated"><strong class="ak"> 1。安装&amp;导入库</strong></h1><p id="8cbf" class="pw-post-body-paragraph if ig hi ih b ii lb ik il im lc io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">我们需要的主要库是a) <em class="jd">拥抱人脸变形器</em>(用于BERT模型和记号化器)，b) <em class="jd"> PyTorch </em> (DL框架&amp;数据集准备)，c) <em class="jd"> PyTorch Lightning </em>(模型定义和训练)，d) <em class="jd"> Sklearn </em>(用于拆分数据集&amp;度量)和e)<em class="jd">beautiful ulsoup</em>(用于从给定数据的原始文本中移除HTML标签)。</p></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><h1 id="a462" class="kd ke hi bd kf kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la bi translated"><strong class="ak"> 2。加载&amp;预处理数据</strong></h1><p id="13bc" class="pw-post-body-paragraph if ig hi ih b ii lb ik il im lc io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">所需的数据集可在<a class="ae ju" href="https://www.kaggle.com/stackoverflow/statsquestions" rel="noopener ugc nofollow" target="_blank"> Kaggle StatsQuestion </a>的两个文件Questions.csv和Tags.csv中获得。将它们装入单独的<em class="jd">熊猫</em>数据帧。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lz"><img src="../Images/da9084be508ddb4477cd28767b8940f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KE7v0X_U2AjgXmwhgjyu_A.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">问题数据</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ma"><img src="../Images/10b04fb5e3881e282df9e92c249eedb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/format:webp/1*eTqcrzvYtfoXqfcho_JACw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">标签数据</figcaption></figure><p id="ba80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">questions data frame:“Body”列包含HTML格式的文本，这也是数据集中除Id列之外唯一对我们的任务有用的列。</p><p id="35ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Tags dataframe:包含与问题相关的标签。一个问题有一个独特的ID。请注意，Id=1有3个标签。</p><p id="669c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了进一步处理，我们需要在连接它们的Id列上连接这两个数据帧。在此之前，我们清理Body列中的文本——首先使用<em class="jd"> Beautiful soup </em>删除HTML标签，然后使用<em class="jd"> Regex </em>删除除字母之外的所有字符，最后将所有文本转换为小写。</p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="d3f2" class="mg ke hi mc b fi mh mi l mj mk"><strong class="mc hj">def</strong> pre_process(text):<br/>  text = BeautifulSoup(text).get_text()<br/>  <em class="jd"># fetch alphabetic characters</em><br/>  text = re.sub("[^a-zA-Z]", " ", text)<br/>  <em class="jd"># convert text to lower case</em><br/>  text = text.lower()<br/>  <em class="jd"># split text into tokens to remove whitespaces</em><br/>  tokens = text.split()<br/>  <strong class="mc hj">return</strong> " ".join(tokens)</span></pre><p id="c65a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大约有85 k行问题和1315个唯一标签。许多标签数量很少，并不重要。就这个问题的范围而言，我们将只限于前10个标签。这给了我们总共大约11 k行的问题——考虑到我们使用的是预先训练好的BERT模型，这已经足够了。最后，我们需要合并两个数据帧来生成一个只包含3列的数据帧——Id、Body和Tags。</p><p id="27a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是10大标签:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ml"><img src="../Images/66b50b5ffa9e6e51486281c4dca369c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*ry-fWxMu5gC5dQ_xrH8RJw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">十大标签</figcaption></figure><p id="f990" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是将用于训练和测试模型的数据结构:“Clean_Body”(问题)列包含训练的输入，“tags”列包含标签或目标。标签的多标签结构在下面非常明显:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mm"><img src="../Images/4b76f3fdcf9047d394da0ebf7636c349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*EC9yfTu6rwWLPk39uj9LJw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">最终数据帧</figcaption></figure><p id="c618" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">检查文字长度(一句话的字数):</strong></p><p id="b5c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们继续之前，我们需要将文本数据转换成数字表示(模型只理解数字)。Transformer型号不能一次处理超过512个单词。快速直方图显示，大多数问题都有字数统计&lt; 300. Also in general, that much length is reasonable for the model develops sufficient context to be able to perform classification for a narrow problem. We will restrict ourselves to the first 300 words</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mn"><img src="../Images/f000c1025a8af84c7ce317e00f94f3af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*-Z15H3_loffIRcZWvj5Bbw.png"/></div></figure><p id="365b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">将数据分为训练、验证和测试数据集:</strong></p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="23ef" class="mg ke hi mc b fi mh mi l mj mk"><strong class="mc hj">from</strong> <strong class="mc hj">sklearn.model_selection</strong> <strong class="mc hj">import</strong> train_test_split<br/><em class="jd"># First Split for Train and Test</em><br/>x_train,x_test,y_train,y_test = train_test_split(x, yt, test_size=0.1, random_state=RANDOM_SEED,shuffle=<strong class="mc hj">True</strong>)</span><span id="7052" class="mg ke hi mc b fi mo mi l mj mk"><em class="jd"># Next split Train in to training and validation</em><br/>x_tr,x_val,y_tr,y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=RANDOM_SEED,shuffle=<strong class="mc hj">True</strong>)</span></pre></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><h1 id="06f5" class="kd ke hi bd kf kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la bi translated">3.准备数据集和数据模块</h1><p id="1762" class="pw-post-body-paragraph if ig hi ih b ii lb ik il im lc io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">由于机器学习模型只能处理数字数据，因此我们需要将标签和正文编码成数字格式。</p><p id="d24c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">编码标签</strong>:我们使用来自<em class="jd"> sklearn </em>的<em class="jd">multilabel binary izer()</em>类。这用于将标签转换成二进制格式——每个唯一的标签都有一个位置——在对应于标签的位置上的1表示标签的存在，0表示标签的不存在。我们只有10个标签，所以我们将有一个长度为10的标签向量。</p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="d77f" class="mg ke hi mc b fi mh mi l mj mk"><strong class="mc hj">from</strong> <strong class="mc hj">sklearn.preprocessing</strong> <strong class="mc hj">import</strong> MultiLabelBinarizer<br/>mlb = MultiLabelBinarizer()<br/>yt = mlb.fit_transform(y)</span><span id="1ca9" class="mg ke hi mc b fi mo mi l mj mk"><em class="jd"># Getting a sense of how the tags data looks like</em><br/>print(yt[0])<br/>print(mlb.inverse_transform(yt[0].reshape(1,-1)))<br/>print(mlb.classes_)<br/>------------------------------------------<br/><strong class="mc hj">Output</strong>:</span><span id="73a1" class="mg ke hi mc b fi mo mi l mj mk">[0 0 0 0 0 0 1 0 0 1]<br/>[('r', 'time series')]<br/>['classification' 'distributions' 'hypothesis testing' 'logistic'<br/> 'machine learning' 'probability' 'r' 'regression' 'self study'<br/> 'time series']</span></pre><p id="f21f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">编码输入(问题):</strong>我们需要以BERT所需的结构化格式对文本数据进行符号化和数字编码，拥抱脸(transformers)库中的<em class="jd">bertokenizer</em>类使这成为一件简单的事情。<em class="jd"> encode_plus() </em>为我们提供了一行代码。</p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="4ab0" class="mg ke hi mc b fi mh mi l mj mk">inputs = self.tokenizer.encode_plus(<br/>            text,<br/>            <strong class="mc hj">None</strong>,<br/>            add_special_tokens=<strong class="mc hj">True</strong>,#Add [CLS] [SEP] tokens<br/>            max_length= self.max_len,<br/>            padding = 'max_length',<br/>            return_token_type_ids= <strong class="mc hj">False</strong>, <br/>            return_attention_mask= <strong class="mc hj">True</strong>,#diff normal/pad tokens<br/>            truncation= <strong class="mc hj">True</strong>,# Truncate data beyond max length<br/>            return_tensors = 'pt' # PyTorch Tensor format<br/>          )</span></pre><p id="436c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先基于<em class="jd"> Dataset </em>类创建QTagDataset类，它以BERT模型所需的格式准备文本。</p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="1831" class="mg ke hi mc b fi mh mi l mj mk"><strong class="mc hj">class</strong> <strong class="mc hj">QTagDataset</strong> (Dataset):<br/>    <strong class="mc hj">def</strong> __init__(self,quest,tags, tokenizer, max_len):<br/>        self.tokenizer = tokenizer<br/>        self.text = quest<br/>        self.labels = tags<br/>        self.max_len = max_len<br/>        <br/>    <strong class="mc hj">def</strong> __len__(self):<br/>        <strong class="mc hj">return</strong> len(self.text)<br/>    <br/>    <strong class="mc hj">def</strong> __getitem__(self, item_idx):<br/>        text = self.text[item_idx]<br/>        inputs = self.tokenizer.encode_plus(<br/>            text,<br/>            <strong class="mc hj">None</strong>,<br/>            add_special_tokens=<strong class="mc hj">True</strong>,<br/>            max_length= self.max_len,<br/>            padding = 'max_length',<br/>            return_token_type_ids= <strong class="mc hj">False</strong>,<br/>            return_attention_mask= <strong class="mc hj">True</strong>,<br/>            truncation=<strong class="mc hj">True</strong>,<br/>            return_tensors = 'pt'<br/>          )<br/>        <br/>        input_ids = inputs['input_ids'].flatten()<br/>        attn_mask = inputs['attention_mask'].flatten()<br/>               <br/>    <strong class="mc hj">return</strong> {<br/>      'input_ids': input_ids ,<br/>      'attention_mask': attn_mask,<br/>      'label':torch.tensor(self.labels[item_idx],dtype= torch.float)<br/>        <br/>    }</span></pre><p id="615b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于我们使用<em class="jd"> Pytorch Lightning </em>进行模型训练，我们将设置从<em class="jd"> LightningDataModule </em>派生的QTagDataModule类。</p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="c2a2" class="mg ke hi mc b fi mh mi l mj mk"><strong class="mc hj">class</strong> <strong class="mc hj">QTagDataModule</strong> (pl.LightningDataModule):<br/>    <br/>  <strong class="mc hj">def</strong> _init__(self,x_tr,y_tr,x_val,y_val,x_test,y_test,tokenizer,<br/>batch_size=16,max_token_len=200):<br/>        super().__init__()<br/>        self.tr_text = x_tr<br/>        self.tr_label = y_tr<br/>        self.val_text = x_val<br/>        self.val_label = y_val<br/>        self.test_text = x_test<br/>        self.test_label = y_test<br/>        self.tokenizer = tokenizer<br/>        self.batch_size = batch_size<br/>        self.max_token_len = max_token_len<br/><br/>  <strong class="mc hj">def</strong> setup(self):<br/>     self.train_dataset = QTagDataset(quest=self.tr_text,  tags=self.tr_label,tokenizer=self.tokenizer,max_len= self.max_token_len)</span><span id="4b79" class="mg ke hi mc b fi mo mi l mj mk">     self.val_dataset= QTagDataset(quest=self.val_text, tags=self.val_label,tokenizer=self.tokenizer,max_len = self.max_token_len)</span><span id="08c1" class="mg ke hi mc b fi mo mi l mj mk">      self.test_dataset =QTagDataset(quest=self.test_text, tags=self.test_label,tokenizer=self.tokenizer,max_len = self.max_token_len)<br/>        <br/>        <br/>  <strong class="mc hj">def</strong> train_dataloader(self):<br/>     <strong class="mc hj">return</strong> DataLoader(self.train_dataset,batch_size= self.batch_size, shuffle = <strong class="mc hj">True</strong> , num_workers=4)</span><span id="df6d" class="mg ke hi mc b fi mo mi l mj mk"><br/>  <strong class="mc hj">def</strong> val_dataloader(self):<br/>     <strong class="mc hj">return</strong> DataLoader (self.val_dataset,batch_size= 16)</span><span id="f728" class="mg ke hi mc b fi mo mi l mj mk"><br/>  <strong class="mc hj">def</strong> test_dataloader(self):<br/>     <strong class="mc hj">return</strong> DataLoader (self.test_dataset,batch_size= 16)</span></pre><p id="4ed4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">设置数据模块:</strong></p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="e58c" class="mg ke hi mc b fi mh mi l mj mk"><em class="jd"># Instantiate and set up the data_module</em><br/>QTdata_module = QTagDataModule(x_tr,y_tr,x_val,y_val,x_test,y_test,<br/>Bert_tokenizer,BATCH_SIZE,MAX_LEN)<br/>QTdata_module.setup()</span></pre></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><h1 id="1fb7" class="kd ke hi bd kf kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la bi translated"><strong class="ak"> 4。定义模型(基于BERT的分类器)</strong></h1><p id="6124" class="pw-post-body-paragraph if ig hi ih b ii lb ik il im lc io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">现成的BERT模型已经在维基百科和书籍语料库上进行了预训练，因此对通用英语文本有很好的理解。然而，来自<em class="jd"> StackExchange </em>的特定数据集包含许多与技术相关的单词，这些单词可能是BERT模型在预训练阶段没有看到的。</p><p id="d6fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们需要<em class="jd">微调</em>我们数据集上的模型，以便它可以建立对我们数据集的理解，并在文本分类任务中变得更好。方法是在核心BERT模型上添加一个分类头，然后在我们的数据集上训练整个模型。这篇文章讨论了使用BERT进行多标签分类，但是，BERT也可以用于执行其他任务，如问题回答、命名实体识别或关键字提取。NLP术语中类似上述的任务也被称为<em class="jd">下游</em>任务。</p><p id="fa48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个文本分类任务中，我们利用了BERT基本模型，该模型为每个单词(令牌)输出长度为768的向量，也为<em class="jd">汇集输出</em> (CLS)输出长度为768的向量。模型训练周期结束时的<em class="jd">汇总输出</em>收集了足够的任务背景，能够帮助进行预测。因为我们的预测任务基本上只需要10个标签的概率，所以我们在来自BERT的768个输出之上添加了10个输出的线性层。</p><p id="9e18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于输出是多标签的(多个标签与一个问题相关联)，我们可能倾向于为最终输出使用Sigmoid激活函数和二进制交叉熵损失函数。然而，Pytorch文档推荐使用<em class="jd"> BCEWithLogitsLoss () </em>函数，该函数将Sigmoid层和<em class="jd"> BCELoss </em>组合在一个类中，而不是在普通的Sigmoid之后再加上一个<em class="jd"> BCELoss </em>。</p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="f8b4" class="mg ke hi mc b fi mh mi l mj mk"><em class="jd"># we will use the BERT base model(the smaller one)</em><br/>BERT_MODEL_NAME = "bert-base-cased"</span><span id="99b1" class="mg ke hi mc b fi mo mi l mj mk"><strong class="mc hj">class</strong> <strong class="mc hj">QTagClassifier</strong>(pl.LightningModule):<br/>    <em class="jd"># Set up the classifier</em><br/>  <strong class="mc hj">def</strong> __init__(self,n_classes=10,steps_per_epoch=<strong class="mc hj">None</strong>,n_epochs=3, lr=2e-5):<br/>    super().__init__()<br/><br/>    self.bert=BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=<strong class="mc hj">True</strong>)<br/>    self.classifier=nn.Linear(self.bert.config.hidden_size,<br/>n_classes) <br/>    self.steps_per_epoch = steps_per_epoch<br/>    self.n_epochs = n_epochs<br/>    self.lr = lr<br/>    self.criterion = nn.BCEWithLogitsLoss()</span><span id="3146" class="mg ke hi mc b fi mo mi l mj mk"><strong class="mc hj">def</strong> forward(self,input_ids, attn_mask):<br/>    output = self.bert(input_ids=input_ids,attention_mask=attn_mask)<br/>    output = self.classifier(output.pooler_output)<br/>          <br/>    <strong class="mc hj">return</strong> output</span></pre></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><h1 id="6dce" class="kd ke hi bd kf kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la bi translated"><strong class="ak"> 5。训练模型(使用Pytorch闪电训练器)</strong></h1><p id="402b" class="pw-post-body-paragraph if ig hi ih b ii lb ik il im lc io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">对于典型的Pytorch训练周期，我们需要实现历元的循环，迭代通过小批量，对每个小批量执行前馈传递，计算损失，对每个批量执行反向传播，然后最终更新梯度。</p><p id="72df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PyTorch Lightning 对其进行了重组和抽象，我们基本上提供了可配置的细节，如优化器、学习率、时期数，Lightning负责剩下的工作。</p><p id="d2ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"> PyTorch Lightning </em>是构建在PyTorch之上的高级框架。它为使用PyTorch代码进行深度学习的传统方式提供了结构化和抽象。基本上，它减少了我们需要编写的代码，并允许我们专注于实验问题，如超参数调整、找到最佳模型和可视化结果。此外，Lightning将处理如何在多个GPU上运行您的模型或加速代码。</p><p id="9e9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"> pl。照明模块</em>类似于<em class="jd"> nn。PyTorch的模块</em>,但是增加了功能——我们的分类器模型就是从它派生出来的。<em class="jd"> LightningModule </em>定义了模型结构，因此具有实现训练、验证和配置优化器的方法。它将我们的PyTorch代码分为5个部分:</p><ul class=""><li id="8ce3" class="lg lh hi ih b ii ij im in iq li iu lj iy lk jc mp lm ln lo bi translated">初始化(__init__)</li><li id="f208" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc mp lm ln lo bi translated">推理(向前)</li><li id="0aad" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc mp lm ln lo bi translated">训练循环(训练_步骤)</li><li id="b081" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc mp lm ln lo bi translated">验证循环(验证_步骤)</li><li id="974a" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc mp lm ln lo bi translated">测试循环(test_step)</li><li id="f02f" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc mp lm ln lo bi translated">优化器(configure _ optimizers)</li></ul><p id="3821" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Lightning还使我们能够为<em class="jd"> Modelcheckpoint() </em>定义一个回调，该回调与训练循环一起自动运行。一个<em class="jd">检查点</em>是一个模型的整个内部状态(架构、权重、优化器的状态、纪元、超参数等)的中间转储。)到机器上的一个文件。因此，对训练过程设置检查点允许我们在训练过程被中断的情况下恢复训练过程，微调模型，或者使用预训练的模型进行推理，而不必重新训练模型。检查点还允许我们定义“损失”或“准确性”标准来保存性能最佳的模型。</p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="99dc" class="mg ke hi mc b fi mh mi l mj mk"><em class="jd"># saves a file like: input/QTag-epoch=02-val_loss=0.32.ckpt</em><br/>checkpoint_callback = ModelCheckpoint(<br/>    monitor='val_loss',<em class="jd"># monitored quantity</em><br/>    filename='QTag-<strong class="mc hj">{epoch:02d}</strong>-<strong class="mc hj">{val_loss:.2f}</strong>',<br/>    save_top_k=3, <em class="jd">#  save the top 3 models</em><br/>    mode='min', <em class="jd"># mode of the monitored quantity  for optimization</em><br/>)</span></pre><p id="9b0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">初始化超参数</strong></p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="4dab" class="mg ke hi mc b fi mh mi l mj mk"><em class="jd"># Initialize the parameters that will be use for training</em><br/>N_EPOCHS = 12<br/>BATCH_SIZE = 32<br/>MAX_LEN = 300<br/>LR = 2e-05</span></pre><p id="dbd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们将代码组织成一个<em class="jd"> LightningModule </em>之后，<em class="jd"> Trainer() </em>会自动完成其他所有工作。下面是我们通常如何使用<em class="jd">训练器()</em></p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="d567" class="mg ke hi mc b fi mh mi l mj mk">trainer <strong class="mc hj">=</strong> Trainer()<br/>trainer<strong class="mc hj">.</strong>fit(model, datamodule)</span></pre><p id="f273" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在引擎盖下，闪电<em class="jd">训练器</em>为我们处理训练循环细节，下面是它做的一些事情:</p><ul class=""><li id="f896" class="lg lh hi ih b ii ij im in iq li iu lj iy lk jc mp lm ln lo bi translated">自动启用/禁用梯度</li><li id="2d84" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc mp lm ln lo bi translated">运行培训、验证和测试数据加载器</li><li id="4ab7" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc mp lm ln lo bi translated">在适当的时候调用回调</li><li id="9ba8" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc mp lm ln lo bi translated">将批处理和计算放在正确的设备上(GPU/CPU)</li><li id="ccea" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc mp lm ln lo bi translated">进度指示器</li></ul><p id="0ea3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是我们的实现:</p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="e928" class="mg ke hi mc b fi mh mi l mj mk"><em class="jd"># Instantiate the Model Trainer</em><br/>trainer = pl.Trainer(max_epochs = N_EPOCHS , gpus = 1, callbacks=[checkpoint_callback],progress_bar_refresh_rate = 30)</span><span id="c60c" class="mg ke hi mc b fi mo mi l mj mk"><em class="jd"># Train the Classifier Model</em><br/>trainer.fit(model, QTdata_module)</span></pre></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><h1 id="1d04" class="kd ke hi bd kf kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la bi translated"><strong class="ak"> 6。评估测试数据集的性能</strong></h1><p id="81d1" class="pw-post-body-paragraph if ig hi ih b ii lb ik il im lc io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">Lightning支持与流行框架(<em class="jd"> TensorBoard，Comet，Weights&amp;bias，Neptune)的集成..</em>)帮助我们记录、跟踪和可视化机器学习实验的性能/结果。</p><p id="9161" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这基本上需要一个<em class="jd">记录器</em>来记录配置信息(参数/超参数)、结果和度量。我们只需简单地将<em class="jd">记录器</em>传递给<em class="jd">训练器</em>()。由于Lightning默认使用<em class="jd">tensor board</em>——在我们的代码<em class="jd">中，你不会发现它被明确传递给<em class="jd">训练师</em>。</em></p><p id="7c2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这通常是如何完成的:</p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="7a8a" class="mg ke hi mc b fi mh mi l mj mk"># Example of using logger from wandb.ai (Weights &amp; Biases Inc.)</span><span id="1ec9" class="mg ke hi mc b fi mo mi l mj mk">from pytorch_lightning.loggers import WandbLogger <br/>wandb_logger = WandbLogger()<br/>trainer <strong class="mc hj">=</strong> Trainer(logger<strong class="mc hj">=</strong>wandb_logger)</span></pre><p id="5e6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">使用Tensorboard可视化性能:</strong></p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="5516" class="mg ke hi mc b fi mh mi l mj mk"><em class="jd"># Visualize the logs using tensorboard.</em></span><span id="d518" class="mg ke hi mc b fi mo mi l mj mk">%load_ext tensorboard<br/>%tensorboard --logdir lightning_logs/</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mq"><img src="../Images/0a74336f7b765b2dec80b70cc1e09bdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*Ak0OVQ64fdoS8vI2nNwi2Q.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">训练损失与训练步骤</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mr"><img src="../Images/0f3159d032a36b5fda6900ad7f16f168.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*VSMa2ySScNKv7doMNIcXag.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">验证损失与培训步骤</figcaption></figure><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="6f52" class="mg ke hi mc b fi mh mi l mj mk"><em class="jd"># Evaluate the model performance on the test dataset</em><br/>trainer.test(model,datamodule=QTdata_module)</span><span id="efe4" class="mg ke hi mc b fi mo mi l mj mk">Out[ ]:</span><span id="1072" class="mg ke hi mc b fi mo mi l mj mk">[{'test_loss': 0.2652013897895813}]</span></pre><p id="0faa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分类器输出一个向量，该向量具有每个标签的概率——然而，我们需要根据它是高于还是低于阈值来将其滚动到1或0。虽然使用阈值= 0.5是可能的，但是我们可以在0.3和0.51之间尝试不同的阈值值，以查看哪个最大化测试集的预测性能。对于此问题，阈值0.4给出了最佳结果。详细内容请看一下代码。</p><p id="2b5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是性能指标:</p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="e075" class="mg ke hi mc b fi mh mi l mj mk">              precision    recall  f1-score   support<br/><br/>           0       0.94      0.93      0.94      8748<br/>           1       0.76      0.77      0.77      2362<br/><br/>    accuracy                           0.90     11110<br/>   macro avg       0.85      0.85      0.85     11110<br/>weighted avg       0.90      0.90      0.90     11110</span></pre><p id="052a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是随机抽样测试数据的预测和实际标签表:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ms"><img src="../Images/6be6a03857b995e50b9e467ff96bf95a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fAoPdnuCJdbP1AxNgVQp-w.png"/></div></div></figure><h1 id="3e49" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">7。模型推断</h1><p id="0631" class="pw-post-body-paragraph if ig hi ih b ii lb ik il im lc io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">使用表现最好的训练模型，我们可以开始预测与我们的任何相关问题相关联的标签。</p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="6208" class="mg ke hi mc b fi mh mi l mj mk">question = "based on the following relationship between matthew s correlation coefficient mcc and chi square mcc is the pearson product moment correlation coefficient is it possible to conclude that by having imbalanced binary classification problem n and p df following mcc is significant mcc sqrt which is mcc when comparing two algorithms a b with trials of times if mean mcc a mcc a mean mcc b mcc b then a significantly outperforms b thanks in advance edit roc curves provide an overly optimistic view of the performance for imbalanced binary classification regarding threshold i m not a big fan of not using it as finally one have to decide for a threshold and quite frankly that person has no more information than me to decide upon hence providing pr or roc curves are just for the sake of circumventing the problem for publishing"<br/><br/>tags = predict(question)<br/><strong class="mc hj">if</strong> <strong class="mc hj">not</strong> tags[0]:<br/>    print('No Known Tags')<br/><strong class="mc hj">else</strong>:<br/>    print(f'Following are the Tags associated : <strong class="mc hj">{</strong>tags<strong class="mc hj">}</strong>')</span></pre><p id="d93f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是输出:</p><pre class="jf jg jh ji fd mb mc md me aw mf bi"><span id="0ab4" class="mg ke hi mc b fi mh mi l mj mk">Following Tags are associated : <br/>[('classification', 'machine learning')]</span></pre></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><h1 id="c206" class="kd ke hi bd kf kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la bi translated"><strong class="ak">结论</strong></h1><p id="3714" class="pw-post-body-paragraph if ig hi ih b ii lb ik il im lc io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">在本文中，我们使用预训练的BERT建立了一个多标签文本分类模型。我们还想了解一下<em class="jd"> PyTorch Lightning </em>如何帮助模型的训练。我已经试着解释了文章中的重要方面和<em class="jd">这里是</em> <a class="ae ju" href="https://github.com/pnageshkar/NLP/blob/master/Medium/Multi_label_Classification_BERT_Lightning.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="jd">链接</em></strong></a><strong class="ih hj"><em class="jd"/></strong><em class="jd">到我的代码</em>。</p><p id="605c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于我们的想法主要是进行POC，所以并没有认真尝试过提高性能，您可能希望利用超参数来提高性能。以前我曾经用CNN (Keras/Tensorflow)为这个问题建立过模型。与该模型相比，BERT模型的训练时间要长得多，而性能提升却非常小。</p><p id="0e9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可能希望对代码进行一点重组，并将推理代码封装到一个API中(如<em class="jd"> FastAPI </em>)，并使用<em class="jd"> Streamlit </em>为用户提供一个web界面，以使用该模型来预测标签。</p><p id="7b23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在<em class="jd">谷歌实验室运行笔记本。</em>使用来自<em class="jd"> Kaggle </em>的数据集(笔记本中提供了链接)。不要忘记修改代码，以便为包含数据的文件设置正确的路径。</p></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><h1 id="9efd" class="kd ke hi bd kf kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la bi translated">感谢您的阅读！</h1><p id="500f" class="pw-post-body-paragraph if ig hi ih b ii lb ik il im lc io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">如果你在这篇文章中学到了新的东西，请鼓掌！这将激励我写更多的东西来帮助更多的人！</p><h1 id="b37d" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">参考</h1><p id="cc29" class="pw-post-body-paragraph if ig hi ih b ii lb ik il im lc io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated"><a class="ae ju" href="https://youtu.be/UJGxCsZgalA" rel="noopener ugc nofollow" target="_blank">用拥抱脸和PyTorch闪电微调BERT】</a></p><p id="7612" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ju" href="https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09" rel="noopener" target="_blank">py torch闪电简介</a></p><p id="e83d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ju" href="https://pytorch-lightning.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Pytorch闪电文件</a></p><p id="bc9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ju" href="https://www.kaggle.com/stackoverflow/statsquestions" rel="noopener ugc nofollow" target="_blank">ka ggle上的StackExchange数据集</a></p></div></div>    
</body>
</html>