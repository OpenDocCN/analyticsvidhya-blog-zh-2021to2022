<html>
<head>
<title>An Overview of the Various BERT Pre-Training Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">各种BERT预训练方法概述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-overview-of-the-various-bert-pre-training-methods-c365512342d8?source=collection_archive---------6-----------------------#2021-05-14">https://medium.com/analytics-vidhya/an-overview-of-the-various-bert-pre-training-methods-c365512342d8?source=collection_archive---------6-----------------------#2021-05-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/3e164b2b3ac3b18b3a5004af86ab8f66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUuxeQKHA5RHaaZ8-oVQ_A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">训练前可视化。图来自<a class="ae iu" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">伯特</a>论文。</figcaption></figure><p id="8995" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你对机器学习感兴趣，那么在过去的几年里，你可能听说过变革了自然语言处理的Transformer模型。</p><p id="7fec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Transformer的一个非常流行的变体叫做BERT，它使用Transformer编码器从无标签的语料库中学习文本表示。你会问他们如何从未标记的数据中学习？嗯，他们定义了一组预训练任务供模型学习。即掩蔽语言建模(MLM)和下一句预测(NSP)。</p><p id="cd8c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意:在本文中，我将假设您具有关于BERT的背景知识，并且您正在寻找关于培训前目标的更多信息。</p><h1 id="427b" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">伯特</h1><h2 id="38a1" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jg ky kz kh jk la lb kl jo lc ld kp le bi translated"><strong class="ak">蒙面语言造型</strong></h2><p id="59c7" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">这是预测句子中缺失单词的任务。正如你所想象的，你不需要这个学习目标的标签，因为你可以从输入的句子中屏蔽掉任何单词。因此，如果我们的数据集中有一个像这样的样本</p><blockquote class="lk ll lm"><p id="01c5" class="iv iw ln ix b iy iz ja jb jc jd je jf lo jh ji jj lp jl jm jn lq jp jq jr js hb bi translated">机器学习超级酷</p></blockquote><p id="e8c4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在训练过程中，伯特实际上可能会输入类似</p><blockquote class="lk ll lm"><p id="0863" class="iv iw ln ix b iy iz ja jb jc jd je jf lo jh ji jj lp jl jm jn lq jp jq jr js hb bi translated">机器[面膜]超级酷</p></blockquote><p id="53a1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">模型的目标是预测丢失的单词。具体来说，BERT将随机屏蔽输入句子的某个百分比(通常为15%)，然后要求模型填充空白。</p><h2 id="1d45" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jg ky kz kh jk la lb kl jo lc ld kp le bi translated"><strong class="ak">下一句预测</strong></h2><p id="e6f0" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">由于许多重要的下游任务涉及两个句子之间的关系，BERT还预先训练了一种叫做下一句预测的东西。</p><p id="e03a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">简而言之，这里的目标是，给定两个句子<em class="ln"> A </em>和<em class="ln"> B </em>，使用【CLS】标记来预测B实际上是否是A后面的句子</p><blockquote class="lk ll lm"><p id="efb3" class="iv iw ln ix b iy iz ja jb jc jd je jf lo jh ji jj lp jl jm jn lq jp jq jr js hb bi translated">[CLS]第一句第二句</p></blockquote><p id="4821" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后我们把这个输入到BERT中，目标是让[CLS]令牌学习一个表示，允许我们识别句子B是否应该在句子A之后！</p><p id="d3cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BERT的作者声称，这个非常简单的想法在BERT执行诸如问题回答和自然语言推理等需要比较两个句子的任务的能力中发挥了巨大的作用。</p><h1 id="c57f" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">其他预培训目标</strong></h1><p id="00ed" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">上面我们已经建立了所谓的“基线”训练前目标。近年来，研究人员已经找到了更好的方法来预先训练BERT体系结构，以鼓励学习更好的文本表示。本文的剩余部分将重点介绍其他流行的培训前方法。</p><h1 id="ddaa" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak"> RoBERTa:一种稳健优化的BERT预训练方法</strong></h1><p id="40a0" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">在RoBERTa的论文中，作者强调了基线BERT预训练目标的缺陷。</p><ol class=""><li id="9265" class="lr ls hi ix b iy iz jc jd jg lt jk lu jo lv js lw lx ly lz bi translated">罗伯塔不同意伯特使用静态蒙版。在最初的BERT实现中，作者在其预处理阶段执行一次屏蔽。具体来说，每个样本被复制10次，每次都以独特的方式被掩盖。因此，在40个时期的训练中，模型将看到相同的样本以10种不同的方式被掩盖。<br/> <br/>在RoBERTa中，作者提出使用动态掩码，即每次将样本输入模型时随机生成掩码。结果显示与静态掩码非常相似的性能，但是，这种方法更有效，因为不需要将数据复制10倍。</li></ol><p id="33c6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.RoBERTa表明BERT中使用的NSP任务实际上是不必要的，删除它可以提高性能。具体来说，这指的是询问“句子B是句子A后面的句子”的二元分类任务。BERT使用该分类任务的损失来指导训练。</p><p id="fb24" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">相反，RoBERTa只需输入两个连续的完整句子</p><blockquote class="lk ll lm"><p id="01ac" class="iv iw ln ix b iy iz ja jb jc jd je jf lo jh ji jj lp jl jm jn lq jp jq jr js hb bi translated">[CLS]第一句第二句</p></blockquote><p id="b396" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">而不要求模型预测句子是否是连续的。</p><h1 id="0126" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak"> BART:用于自然语言生成、翻译和理解的去噪序列间预训练</strong></h1><figure class="mb mc md me fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/c8373ffa3cce4f36a86dbb3094614046.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*r1KC2-ROxbX5s0H-mHTrTw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">巴特模型。图来自原<a class="ae iu" href="https://arxiv.org/pdf/1910.13461.pdf" rel="noopener ugc nofollow" target="_blank">纸</a></figcaption></figure><p id="b41d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我不确定我是否应该在这篇文章中包括BART，因为它与BERT有一点不同，因为它包含一个解码器。冒着过于简化的风险，BART就是BERT +一个自回归解码器(比如GPT-2)。给定解码器的存在，该模型在如何制定预训练目标方面具有更大的灵活性。</p><p id="7d06" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BART培训方式的高级概述如下。1)破坏输入的句子。2)用BERT编码。3)解码BERT输出4)将解码与基本事实语句进行比较。如你所见，这是一个奇特的去噪自动编码器。</p><h2 id="202b" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jg ky kz kh jk la lb kl jo lc ld kp le bi translated">文本腐败策略(培训前目标)</h2><h2 id="3f5d" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jg ky kz kh jk la lb kl jo lc ld kp le bi translated"><strong class="ak">令牌屏蔽</strong></h2><p id="01f1" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">这和伯特的蒙版是一样的。BART将简单地屏蔽输入中的一些标记，并尝试让解码器预测丢失了什么。</p><h2 id="b207" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jg ky kz kh jk la lb kl jo lc ld kp le bi translated">令牌删除</h2><p id="a292" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">随机删除输入句子中的标记。这迫使模型不仅要预测丢失的令牌，还要预测它们在哪里。</p><h2 id="e44c" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jg ky kz kh jk la lb kl jo lc ld kp le bi translated">文本填充</h2><p id="744b" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">删除输入句子的某些部分，并用单个掩码标记替换它们。这迫使模型学习需要解码成多少个单词(掩码)。</p><h2 id="529e" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jg ky kz kh jk la lb kl jo lc ld kp le bi translated">文档旋转</h2><p id="089b" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">从输入中随机选择一个令牌，然后旋转/包装输入，使得输入现在从随机选择的令牌开始。</p><p id="06a3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在BART中，通过将一个句子输入到BERT编码器中，并使用解码器的[CLS]等价物进行预测，对分类任务进行微调。</p><h1 id="6254" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">斯潘伯特:通过描述和预测跨度来改善预训练</h1><p id="0270" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">斯潘伯特提出了一个非常简单的想法，在基线预训练目标的基础上提供了一个巨大的性能提升。考虑原始<a class="ae iu" href="https://arxiv.org/pdf/1907.10529.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中的下图:</p><figure class="mb mc md me fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/3693e1ebf693bff8eeb253248566a1d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOQr0zDEkeRhJVW5s4wOJg.png"/></div></div></figure><p id="f4cb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">斯潘伯特在训练前做了两件新奇的事情</p><ol class=""><li id="42de" class="lr ls hi ix b iy iz jc jd jg lt jk lu jo lv js lw lx ly lz bi translated">它们掩盖了原句中文本的连续范围。在上图中，您可以看到一组4个连续的标记被替换为[MASK]。该模型的任务是预测这些缺失的信息。</li><li id="c677" class="lr ls hi ix b iy mg jc mh jg mi jk mj jo mk js lw lx ly lz bi translated">他们引入了一个跨度边界目标(SBO ),该目标迫使模型使用沿掩蔽区域边界(上图中的x4和x9)的记号的表示来预测缺失的记号。</li></ol><p id="fa16" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，SpanBERT从BERT中移除了NSP目标，并且仅在单句输入上进行训练。</p><h1 id="ac5a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">即时记笔记有助于语言前期训练(TNF)</h1><p id="94d8" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">这篇<a class="ae iu" href="https://arxiv.org/abs/2008.01466#:~:text=Specifically%2C%20TNF%20maintains%20a%20note,semantics%20of%20the%20current%20sentence." rel="noopener ugc nofollow" target="_blank">论文</a>指出，BERT训练在某种程度上如此昂贵，因为它必须学习极其罕见的单词的良好表示。如果你考虑句子“新冠肺炎已经花费了成千上万的生命”，你可以想象新冠肺炎在给定的训练语料库中不是一个常见的术语。此外，如果我们需要预测“新冠肺炎已经花费了数千[面具]”，那么这种情况下，一个罕见的词是唯一有用的信息片段，我们必须预测丢失的令牌。</p><p id="a06b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">TNF通过保存一个字典来解决这个问题，字典中的关键字是这些罕见的单词，值是单词历史上下文的向量表示。换句话说，每个罕见的单词都有一个表示，根据它在其他句子中的上下文进行更新。通过这样做，当一个生僻字再次出现时，我们可以使用这种历史上下文嵌入来辅助生僻字存在时的学习任务。</p><p id="d554" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">实际上，如果检测到一个稀有单词，只需在稀有单词查找表中找到历史上下文向量，并将其添加到最终的输入嵌入中。例如(word _ embedding+position _ embedding+TNF _ embedding)。</p><h1 id="5c79" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">ELECTRA:预训练文本编码器作为鉴别器而不是生成器</h1><figure class="mb mc md me fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/2634a7511e39a86df6709efa53e9749b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*H84Oz__S1AjO65bRwrnbUQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">训练前可视化。图来源于<a class="ae iu" href="https://arxiv.org/pdf/2003.10555.pdf" rel="noopener ugc nofollow" target="_blank">原创论文</a></figcaption></figure><p id="58e9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这种预训练方法也旨在使训练BERT更有效。这里的关键思想是使用替换令牌检测进行训练。</p><h2 id="4367" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jg ky kz kh jk la lb kl jo lc ld kp le bi translated">替换令牌检测</h2><p id="438c" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">为了执行这一步，我们需要两个变压器模型1)生成器和2)鉴别器。</p><p id="bbbc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们首先如上所示屏蔽一些输入标记，生成器输出预测标记。接下来，鉴别器(这是伊莱克特的关键创新)<em class="ln">必须决定哪些输入是真实的，哪些是合成的。</em></p><p id="ead8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">效率如此之高的原因是，现在我们需要计算所有输入令牌的损耗，而不是只计算屏蔽令牌的损耗(如BERT中所做的)。这提供了对资源的更好利用，并允许人们更快地训练BERT模型。</p><h1 id="8364" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">问答的跨度选择预训练</h1><figure class="mb mc md me fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/9b4504d753c87156aee17331061b0b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*UC8ekUGqX4qc9GoVHH45tg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图来自原<a class="ae iu" href="https://arxiv.org/pdf/1909.04120.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></figcaption></figure><p id="daff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我不会详细讨论这一点，但最重要的想法是使用外部资源来预测屏蔽语言建模过程中丢失的信息。在上面的图像中，我们看到[空白]标记不是由模型本身填充的(如BERT中所做的)，而是实际上从其他相关段落中预测的。该模型在机器阅读理解任务上比BERT高出3 F1分。<a class="ae iu" href="https://arxiv.org/abs/1909.04120" rel="noopener ugc nofollow" target="_blank">来源</a></p><h1 id="4d4a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">SenseBERT:让BERT明白一些道理</h1><p id="491a" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">在这篇<a class="ae iu" href="https://arxiv.org/abs/1908.05646" rel="noopener ugc nofollow" target="_blank">论文</a>中提出的预训练思想迫使模型不仅执行屏蔽语言建模，而且使用WordNet预测每个屏蔽令牌的超级含义。</p><h1 id="0dcc" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">CAPT:学习去噪序列表征的对比预训练</h1><p id="6c79" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">如果你熟悉ML中的对比学习，那么你可能会对<a class="ae iu" href="https://arxiv.org/abs/2010.06351#:~:text=CAPT%3A%20Contrastive%20Pre%2DTraining%20for%20Learning%20Denoised%20Sequence%20Representations,-Fuli%20Luo%2C%20Pengcheng&amp;text=The%20proposed%20CAPT%20encourages%20the,unsupervised%20instance%2Dwise%20training%20signals." rel="noopener ugc nofollow" target="_blank"> CAPT </a>感兴趣。这项工作的目标是鼓励输入句子的表示尽可能接近被破坏的输入句子的表示。这有助于模型变得对噪声更加不变，以及更好地捕捉给定句子的全局语义。</p><h1 id="0b81" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">极限测试:语言学指导的多任务测试</h1><p id="dd0d" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated"><a class="ae iu" href="https://arxiv.org/pdf/1910.14296.pdf" rel="noopener ugc nofollow" target="_blank">极限伯特</a>做了两件与传统伯特不同的事情。1)它们不是随机屏蔽记号，而是屏蔽从预先训练的语言学模型中确定的语义上有意义的记号。2)他们改变了BERT训练目标以具有多任务损失，这要求LIMIT-BERT不仅执行MLM，而且能够学习关于输入数据的语言属性(识别词性、识别语义角色等)。).</p></div></div>    
</body>
</html>