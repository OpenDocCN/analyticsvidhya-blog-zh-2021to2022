<html>
<head>
<title>Scraping Reddit using Python Reddit API Wrapper (PRAW)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python Reddit API包装器抓取Reddit(PRAW)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/scraping-reddit-using-python-reddit-api-wrapper-praw-5c275e34a8f4?source=collection_archive---------4-----------------------#2021-02-19">https://medium.com/analytics-vidhya/scraping-reddit-using-python-reddit-api-wrapper-praw-5c275e34a8f4?source=collection_archive---------4-----------------------#2021-02-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/409c21c26ecef6d70eff47438850ba7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*_EoACDpuunpuqScB.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">用Python刮Reddit</figcaption></figure><p id="fcb0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">内容:<br/> </strong> 1。简介<br/> 2。创建Reddit API账户<br/> 3。刮Reddit帖子<br/> 4。抓取Reddit子编辑<br/> 5。清理数据</p><p id="59e5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">简介:<br/> </strong>随着数据的急剧增长，从脸书、Twitter和Reddit等多个来源搜集、收集、积累和许多其他同样有意义的各种信息只会变得更好。考虑到这一点，Reddit很久以前就有了一个API，叫做Python Reddit API Wrapper，是PRAW的缩写，使用Python(我知道这个名字里已经有了！)来抓取数据。在本文中，我将使用代码片段展示如何抓取帖子和整个子编辑。</p><p id="dd56" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">创建Reddit API帐户:<br/> </strong>首先需要一个Reddit帐户，所以如果你还没有，你需要在继续之前创建它。为了使用PRAW，你必须首先注册Reddit API，这可以通过下面的<a class="ae jn" href="https://www.reddit.com/wiki/api" rel="noopener ugc nofollow" target="_blank">链接</a>来完成。这非常简单，与注册Twitter API没有太大区别。完成这个过程后，调用API需要三个东西，它们是客户端ID、客户端密码和用户代理。稍后将解释如何使用它们。</p><p id="11e1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">抓取Reddit帖子:<br/> </strong>既然所有先决条件都已满足，抓取Reddit应该很容易。有两个图书馆将用于这个练习:PRAW和熊猫。PRAW用来访问Reddit，熊猫用来将数据制成表格并在以后进行清理。所以现在我们首先导入库。</p><figure class="jo jp jq jr fd ii"><div class="bz dy l di"><div class="js jt l"/></div></figure><p id="c28a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来，我们需要创建Reddit对象，我们将在其中插入注册Reddit API时创建的凭证。</p><figure class="jo jp jq jr fd ii"><div class="bz dy l di"><div class="js jt l"/></div></figure><p id="df68" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们有了一个可以调用API的对象。接下来，我们需要选择一个职位刮。为了这篇文章的目的，我已经选择了一个合适的职位。在这种情况下，要做的是获取所述帖子的完整URL。然后我们创建一个submission对象，URL作为参数传入其中。</p><figure class="jo jp jq jr fd ii"><div class="bz dy l di"><div class="js jt l"/></div></figure><p id="44f2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来是好玩的部分！下面的部分是我们使用PRAW对象检索数据的地方。首先，我们创建一个名为posts的列表来存储所有检索到的文章。接下来，我们创建一个for循环来检查注释。在这个片段中，如下所示，我们在提交的评论中检索顶级评论。我没有检索第一条评论(也恰好是最上面的评论)的原因是，在这个特定的例子中，因为这个subreddit有一个bot，它通常会占用那个位置，而不是我们应该抓取的东西(然后这又取决于您)。内部是一个嵌套的if循环，用于检查锚定的注释是顶级注释还是MoreComments对象，这意味着它是否有更多的注释。如果是这样，那么将评论的正文附加到帖子中。接下来，在for循环完成后，文章列表被更改为pandas数据框。其中有一列名为“正文”的所有帖子。</p><figure class="jo jp jq jr fd ii"><div class="bz dy l di"><div class="js jt l"/></div></figure><p id="050f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">运行前面的代码后，应该会显示如下内容。如图所示的body列有从文章中提取的评论，左边基本上是索引。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="ab fe cl ju"><img src="../Images/931b8855190fae5c8055f08bf4ef322d.png" data-original-src="https://miro.medium.com/v2/0*Iq59LlVVOVx_toKD"/></div><figcaption class="il im et er es in io bd b be z dx translated">显示检索到的注释的表格</figcaption></figure><p id="f44b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们有了这篇文章中所有的顶级评论。</p><p id="7003" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是完整的脚本。关于最后几行，我稍后会解释如何做一些简单的清理。</p><figure class="jo jp jq jr fd ii"><div class="bz dy l di"><div class="js jt l"/></div></figure><p id="152c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">抓取Reddit子编辑:<br/> </strong>之前我们已经收集了单个帖子中的评论，接下来我们将处理如何抓取子编辑中的所有顶级评论。如果你不考虑时间因素的话，这应该和刮一个单独的帖子没什么不同。我们将遵循之前完成的大部分步骤，但稍作调整。</p><p id="496e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们仍将重新创建前两个步骤，即导入库和创建subreddit对象。</p><figure class="jo jp jq jr fd ii"><div class="bz dy l di"><div class="js jt l"/></div></figure><figure class="jo jp jq jr fd ii"><div class="bz dy l di"><div class="js jt l"/></div></figure><p id="d58f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来，我们将尝试访问一个子编辑中的所有提交，并获得所有顶级评论。首先，还有一个名为posts的列表。选择subreddit时，我们将它作为“Reddit . subreddit(" INSERTSUBREDDIT ")”中传递的值插入第二行。在这种情况下，我们直接遍历通过访问subreddit检索到的所有提交。为了删除尽可能多的注释，我们添加了“.”。top("all ")"到子编辑以获得最多投票的帖子。接下来，是一个遍历锚定提交中所有评论的for循环。如果这个评论有更多的评论，那么就把它添加到帖子列表中，然后在它结束的时候把帖子变成熊猫数据框。</p><figure class="jo jp jq jr fd ii"><div class="bz dy l di"><div class="js jt l"/></div></figure><p id="2253" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来，让我们看看我们得到了什么。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="er es jv"><img src="../Images/55ecdfa537608812a2571eef17aea18d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BH0oHoUr5osAwzQ_"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">从子编辑内的所有提交中检索的注释</figcaption></figure><figure class="jo jp jq jr fd ii"><div class="bz dy l di"><div class="js jt l"/></div></figure><p id="4342" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">清理数据:</strong> <br/>最后但并非最不重要的是数据清理。与真实数据一样，它经常会陷入不完整或错误数据的泥潭。特别是，我们应该对Reddit数据做的一件事是替换移除和删除的评论以及它们的索引。</p><figure class="jo jp jq jr fd ii"><div class="bz dy l di"><div class="js jt l"/></div></figure><p id="c88a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来，我们删除选定的索引并替换它们。</p><figure class="jo jp jq jr fd ii"><div class="bz dy l di"><div class="js jt l"/></div></figure></div></div>    
</body>
</html>