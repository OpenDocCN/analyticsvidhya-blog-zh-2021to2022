<html>
<head>
<title>How to create a Chabot in Python II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在Python II中创建Chabot</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-create-a-chabot-in-python-5080f0d0029?source=collection_archive---------5-----------------------#2021-04-29">https://medium.com/analytics-vidhya/how-to-create-a-chabot-in-python-5080f0d0029?source=collection_archive---------5-----------------------#2021-04-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="0e2f" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用高效转换器开发基于自然语言处理的聊天机器人第二部分</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/ba236dbfe6da4cc9daec90b3e8f6cd55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yOp25w3ukA6LMKbZbm4CiA.jpeg"/></div></div></figure><p id="4c8b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们首先回忆一下我们在第一部分讨论过的事情</p><blockquote class="kf kg kh"><p id="20c4" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><a class="ae km" href="https://shsarv.medium.com/how-to-create-a-chatbot-in-python-7ab924f10125" rel="noopener">如何用Python创建聊天机器人</a>。</p></blockquote><p id="4212" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">数据集和源代码在-<a class="ae km" href="https://github.com/shsarv/ChatBot" rel="noopener ugc nofollow" target="_blank">https://github.com/shsarv/ChatBot</a></p><p id="3cc8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi kn translated">我们知道聊天机器人是一个人造的个体或人类，它与人类或其他机器人进行交互。聊天机器人使用自然语言处理(NLP)和高级机器学习(ML)算法从数据洞察中学习&amp; NLP是计算机理解和处理人类语音的能力，并以人类可以理解的语言做出回应。这样，它使交互看起来像是两个人之间的交流。我们也开始了解chabots的优势，如24*7支持、即时回答和无需人工帮助的订单。最后，我们开始使用<strong class="jl hj"> Reformer </strong>或efficient Transformer创建我们自己的聊天机器人，在这里我们探索了MultiWoz数据集，并看到了MultiWoz数据集的不同方面。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kw"><img src="../Images/aa789e5c840275711bcf2786a1b2555a.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*O6CXmokrl3pibs4DhFbp_w.jpeg"/></div></figure><p id="ee30" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，我们将开始聊天机器人创建的第二阶段，我们将处理数据，将其输入模型，训练我们的模型&amp;通过向模型输入问题来生成对话。这些是我们为完成这些任务将要执行的步骤。</p><ul class=""><li id="d2de" class="kx ky hi jl b jm jn jp jq js kz jw la ka lb ke lc ld le lf bi translated"><strong class="jl hj">处理转化炉输入数据— </strong>标记化，用分桶法分批</li><li id="92c5" class="kx ky hi jl b jm lg jp lh js li jw lj ka lk ke lc ld le lf bi translated"><strong class="jl hj">可逆层</strong></li><li id="9131" class="kx ky hi jl b jm lg jp lh js li jw lj ka lk ke lc ld le lf bi translated"><strong class="jl hj">可逆层和随机性</strong></li><li id="a050" class="kx ky hi jl b jm lg jp lh js li jw lj ka lk ke lc ld le lf bi translated"><strong class="jl hj">改革者培训</strong></li><li id="63d3" class="kx ky hi jl b jm lg jp lh js li jw lj ka lk ke lc ld le lf bi translated"><strong class="jl hj">从预训练的模型解码。</strong></li></ul><h1 id="6004" class="ll lm hi bd ln lo lp lq lr ls lt lu lv io lw ip lx ir ly is lz iu ma iv mb mc bi translated">2.<strong class="ak">处理转化炉输入数据。</strong></h1><p id="e58f" class="pw-post-body-paragraph jj jk hi jl b jm md ij jo jp me im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">我们现在将使用<code class="du mi mj mk ml b">get_conversation()</code>函数来处理数据。重整者期望这种形式的输入:</p><p id="26e9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">人1:为什么我这么开心？因为你正在学习如何创建聊天机器人。人员1: …人员2: …* </strong></p><p id="f5e2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">对话继续进行，并有一些文本。如您所见,“人物1”和“人物2”充当分隔符，因此模型会自动识别人物以及正在说话的人。然后，它可以为每个人提供相应的文本响应。让我们以这种方式为改革者处理文本。首先，让我们从所有对话文件中获取所有对话字符串，并将它们放在一个列表中。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="e57f" class="mq lm hi ml b fi mr ms l mt mu"># the keys are the file names<br/>all_files = DIALOGUE_DB.keys()<br/><br/># initialize empty list<br/>untokenized_data = []<br/><br/># loop over all files<br/>for file in all_files:<br/>    # this is the graded function you coded<br/>    # returns a string delimited by Person 1 and Person 2<br/>    result = get_conversation(file, DIALOGUE_DB)<br/>    <br/>    # append to the list<br/>    untokenized_data.append(result)<br/><br/># print the first element to check if it's the same as the one we got before<br/>print(untokenized_data[0])</span></pre><p id="f561" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出-</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="196a" class="mq lm hi ml b fi mr ms l mt mu">Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it's cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.<br/>Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</span></pre><p id="0e6a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，让我们将列表拆分为一个训练和评估数据集。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="33eb" class="mq lm hi ml b fi mr ms l mt mu"># shuffle the list we generated above<br/>random.shuffle(untokenized_data)<br/><br/>cut_off = int(len(untokenized_data) * .05)<br/><br/># slice the list. the last elements after the cut_off value will be the eval set. the rest is for training. <br/>train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]<br/><br/>print(f'number of conversations in the data set: {len(untokenized_data)}')<br/>print(f'number of conversations in train set: {len(train_data)}')<br/>print(f'number of conversations in eval set: {len(eval_data)}')</span></pre><p id="5ff4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出-</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="65db" class="mq lm hi ml b fi mr ms l mt mu">number of conversations in the data set: 10438<br/>number of conversations in train set: 9917<br/>number of conversations in eval set: 521</span></pre><p id="8f52" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">因此，我们的训练集的会话数等于9917，而测试集中的会话数等于521。</p><h2 id="6fe7" class="mq lm hi bd ln mv mw mx lr my mz na lv js nb nc lx jw nd ne lz ka nf ng mb nh bi translated">2.1令牌化，使用分桶进行批处理</h2><p id="8601" class="pw-post-body-paragraph jj jk hi jl b jm md ij jo jp me im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">我们现在可以继续生成标记化的数据批次。让我们首先定义一个效用生成器函数，从我们的数据集中产生元素:</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="c0e7" class="mq lm hi ml b fi mr ms l mt mu">def stream(data):<br/>    # loop over the entire data<br/>    while True:<br/>        # get a random element<br/>        d = random.choice(data)<br/>        <br/>        # yield a tuple pair of identical values <br/>        # (i.e. our inputs to the model will also be our targets during training)<br/>        yield (d, d)</span></pre><p id="da3b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在让我们定义数据管道，对数据进行标记和批处理。我们将按长度存储桶，并且对令牌长度也有一个上限。我们将使用trax，它允许我们使用组合子来生成数据管道。然后，我们将对数据进行标记，并过滤掉长序列。最后，我们将把数据管道应用于我们的训练和评估集。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="1bc8" class="mq lm hi ml b fi mr ms l mt mu">data_pipeline = trax.data.Serial(<br/>    # randomize the stream<br/>    trax.data.Shuffle(),<br/>    <br/>    # tokenize the data<br/>    trax.data.Tokenize(vocab_dir=VOCAB_DIR,<br/>                       vocab_file=VOCAB_FILE),<br/>    <br/>    # filter too long sequences<br/>    trax.data.FilterByLength(2048),<br/>    <br/>    # bucket by length<br/>    trax.data.BucketByLength(boundaries=[128, 256,  512, 1024],<br/>                             batch_sizes=[16,    8,    4,   2, 1]),<br/>    <br/>    # add loss weights but do not add it to the padding tokens (i.e. 0)<br/>    trax.data.AddLossWeights(id_to_mask=0)<br/>)<br/><br/>train_stream = data_pipeline(stream(train_data))<br/>eval_stream = data_pipeline(stream(eval_data))</span></pre><p id="208b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们来看看训练流。流生成器将产生(输入、目标、权重)。让我们只获取输入进行检查</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="b317" class="mq lm hi ml b fi mr ms l mt mu">inp, _, _ = next(train_stream)<br/><br/># print the shape. format is (batch size, token length)<br/>print("input shape: ", inp.shape)<br/><br/># detokenize the first element<br/>print(trax.data.detokenize(inp[0], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))</span></pre><p id="c1bb" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="c3c3" class="mq lm hi ml b fi mr ms l mt mu">input shape:  (2, 1024)<br/> Person 1: Well, I am planning a trip and need some help with a train. Person 2: Of course, do you know your departure location and time? Person 1: I weill departing on thursday from cambridge and need to arrive by 10:30 in stevenage.  Person 2: I have three, leaving between 5:21 and 9:21. Do you have a preference? Person 1: Not really. I need to know how much a ticket costs and how long it travels. Person 2: Train TR0552 arrives by 10:10 the ticket price is 12.80 pounds and the travel time is 49 minutes.  Person 1: Perfect. I am also looking for a place to stay with free parking Person 2: No problem, how many nights will you be staying? Person 1: I'm not sure of that yet. It does need to be in the north.  Person 2: Would you prefer a guest house or hotel? Person 1: I would like a hotel in the north, the star of the hotel and free internet. Person 2: how about acorn guest house? it's 4 stars. Person 1: Thank you, I'll take it. Can you book me for that hotel? Person 2: I would be happy to- I just need to know for which nights and for how many people. Person 1: Just for myself. And say, 2 nights ought to do it.  Person 2: 2 nights starting on Thursday? Person 1: Actually I am just calling for information not a booking. I need a hotel, not guesthouse, in the north with free parking. Can you recommend a hotel? Person 2: I don't have any hotels in the north that meet your criteria. Would you like me to look in a different area? Person 1: I'm really needing something in the north.  Please try again. Person 2: I just double checked. Still no hotels in the north that meet your criteria. What about a guesthouse? Person 1: No, I need a hotel in the north with free parking, no other criteria. I don't need free internet. Person 2: I'm sorry but I don't have anything meeting that criteria.  Person 1: Well okay then let's just go with whatever's available in the north. Person 2: Does the number of stars matter? Person 1: Not really, can you give me the number of stars and whether or not they have internet? Person 2: Ashley hotel, it has two stars and yes they have internet  Person 1: Sweet. That's all I needed then. Person 2: Thank you for calling today. Please call again if you have anything else that you need. Goodbye.</span></pre><h1 id="9315" class="ll lm hi bd ln lo lp lq lr ls lt lu lv io lw ip lx ir ly is lz iu ma iv mb mc bi translated">第3部分:可逆层</h1><p id="48a0" class="pw-post-body-paragraph jj jk hi jl b jm md ij jo jp me im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">当运行大型深度模型时，我们经常会耗尽内存，因为每一层都分配内存来存储用于反向传播的激活。为了节省这个资源，我们需要能够在向后传递期间重新计算这些激活，而不需要在向前传递期间存储它们。首先看看下面最左边的图表。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ni"><img src="../Images/c0a0eab248d3e0ee5e441cfd5f68fc5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NfNXGzX5a9e3LZqr.PNG"/></div></div></figure><p id="a2e5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这就是剩余网络在标准变压器中的实现方式。由此可见，给定<code class="du mi mj mk ml b">F()</code>是注意力，<code class="du mi mj mk ml b">G()</code>是前馈(FF)。：</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nj"><img src="../Images/d257677b88b38d59fd77221509edef22.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*HTwC1D-1TVqOqrkMGiNwOA.png"/></div></figure><p id="dc6c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如您所见，它要求保存x和y(a ),以便在反向传播过程中使用。我们希望避免这种情况，以节省内存，这就是可逆剩余连接出现的原因。它们显示在上面中间和最右边的图表中。关键的想法是，我们将从模型输入的两个副本开始，在每一层，我们将只更新其中的一个。我们<em class="ki">没有</em>更新的激活将用于计算残差。现在，在这个可逆设置中，您将得到以下内容:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nk"><img src="../Images/d60a1f9d920a112a0c11143a67e044c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*ccHrhqxiN_Rc6N-Nter8Xg.png"/></div></figure><p id="4af1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">从(y1，y2)恢复(x1，x2)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nl"><img src="../Images/79f6c52951a6b635e8f95e3d08f59420.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*xr_BazZJfrQH9AdhkclS_A.png"/></div></figure><p id="c488" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">有了这种配置，我们现在能够完全反向运行网络。您会注意到，在向后传递的过程中，x1和x2可以只根据y1和y2的值重新计算。向前传球时不需要扑球。</p><p id="3fe0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，我们将使用上面的等式实现<code class="du mi mj mk ml b">reversible_layer_forward</code>函数。这个函数接受输入向量<code class="du mi mj mk ml b">x</code>和函数<code class="du mi mj mk ml b">f</code>和<code class="du mi mj mk ml b">g</code>，并返回y1和y2的连接。为此，我们将在进行可逆剩余步骤之前拆分<code class="du mi mj mk ml b">x</code>。然后我们可以将这两个向量用于<code class="du mi mj mk ml b">reversible_layer_reverse</code>函数。我们将利用<code class="du mi mj mk ml b">np.concatenate()</code>形成输出，小心匹配<code class="du mi mj mk ml b">np.split()</code>的轴。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="0c2f" class="mq lm hi ml b fi mr ms l mt mu">def reversible_layer_forward(x, f, g):<br/>    """<br/>    Args: <br/>        x (np.array): an input vector or matrix<br/>        f (function): a function which operates on a vector/matrix<br/>        g (function): a function which operates on a vector/matrix<br/>    Returns: <br/>        y (np.array): an output vector or matrix whose form is determined by 'x', f and g<br/>    """<br/>    # split the input vector into two (* along the last axis because it is the depth dimension)<br/>    x1, x2 = np.split(x, 2, axis=-1) <br/>    y1 = x1 + f(x2)<br/>    y2 = x2 + g(y1)<br/>    <br/>    # concatenate y1 and y2 along the depth dimension. be sure output is of type np.ndarray<br/>    y = np.concatenate([y1, y2], axis=-1)<br/>    return y</span></pre><p id="cbdf" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们现在将实现<code class="du mi mj mk ml b">reversible_layer_reverse</code>函数，这是可能的，因为在每个时间步，你有x1和x2，y1和y2，以及函数<code class="du mi mj mk ml b">f</code>和<code class="du mi mj mk ml b">g</code>。其中<code class="du mi mj mk ml b">f</code>是注意力，<code class="du mi mj mk ml b">g</code>是前馈。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="599a" class="mq lm hi ml b fi mr ms l mt mu">def reversible_layer_reverse(y, f, g):<br/>    """<br/>    Args: <br/>        y (np.array): an input vector or matrix<br/>        f (function): a function which operates on a vector/matrix of the form of 'y'<br/>        g (function): a function which operates on a vector/matrix of the form of 'y'<br/>    Returns: <br/>        y (np.array): an output vector or matrix whose form is determined by 'y', f and g<br/>    """<br/>    <br/>    # split the input vector into two (* along the last axis because it is the depth dimension)<br/>    y1, y2 = np.split(y, 2, axis=-1)<br/>    x2 = y2 - g(y1)<br/>    x1 = y1 - f(x2)<br/>    # concatenate x1 and x2 along the depth dimension<br/>    x = np.concatenate([x1, x2], axis=-1) <br/>    return x</span></pre><h2 id="a402" class="mq lm hi bd ln mv mw mx lr my mz na lv js nb nc lx jw nd ne lz ka nf ng mb nh bi translated">3.1可逆层和随机性</h2><p id="598f" class="pw-post-body-paragraph jj jk hi jl b jm md ij jo jp me im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">我们将使用fastmath的随机函数和键&amp;利用同一个键，<code class="du mi mj mk ml b">trax.fastmath.random.uniform()</code>将返回相同的值。当层中引入随机噪声时，这是反向传递返回正确的层输入所必需的。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="b088" class="mq lm hi ml b fi mr ms l mt mu"># Layers like dropout have noise, so let's simulate it here:<br/>f = lambda x: x + np.random.uniform(size=x.shape)<br/><br/># See that the above doesn't work any more:<br/>output_vector = reversible_layer_forward(input_vector, f, g)<br/>reversed_vector = reversible_layer_reverse(output_vector, f, g)<br/><br/>assert not np.allclose(reversed_vector, input_vector)  # Fails!!<br/><br/># It failed because the noise when reversing used a different random seed.<br/><br/>random_seed = 27686<br/>rng = trax.fastmath.random.get_prng(random_seed)<br/>f = lambda x: x + trax.fastmath.random.uniform(key=rng, shape=x.shape)<br/><br/># See that it works now as the same rng is used on forward and reverse.<br/>output_vector = reversible_layer_forward(input_vector, f, g)<br/>reversed_vector = reversible_layer_reverse(output_vector, f, g)<br/><br/>assert np.allclose(reversed_vector, input_vector,  atol=1e-07)</span></pre><h1 id="946e" class="ll lm hi bd ln lo lp lq lr ls lt lu lv io lw ip lx ir ly is lz iu ma iv mb mc bi translated">第4部分:改革者培训</h1><p id="4eb9" class="pw-post-body-paragraph jj jk hi jl b jm md ij jo jp me im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">我们现在开始训练你的模型。既然你已经知道了区别于标准变形金刚的两个主要组件，LSH和上面的可逆层，我们可以使用Trax中已经实现的预建模型。它将具有这样的架构:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nm"><img src="../Images/6e80e155368f01e6d5e4173d06a7a546.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/0*7X_HWUUvWp3Hj-Hs.jpg"/></div></figure><p id="12e6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">与Transformer类似，我们希望将注意力和前馈层应用到我们的输入中。对于重整器，我们通过使用<strong class="jl hj">可逆解码器模块</strong>来提高内存效率，您可以在Trax中描绘它的实现，如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nn"><img src="../Images/999f3027ef7a19ffb39fee1c5486f011.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/0*3X0oLt-_8iiDMk1J.png"/></div></figure><p id="d56c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">你可以看到，它采用初始输入<code class="du mi mj mk ml b">x1</code>和<code class="du mi mj mk ml b">x2</code>，并计算第3部分中得到的可逆网络的第一个方程。如你所知，可逆残差有两个正向传递方程，因此只需其中一个方程就能构成可逆解码器模块的一半。在进行第二个等式(即可逆残差的后半部分)之前，首先需要交换元素，以考虑Trax中的堆栈语义。它只是把<code class="du mi mj mk ml b">x2</code>放在栈顶，这样它就可以被送到半剩余层的add块。然后，它再次交换两个输出，以便可以将其馈送到网络的下一层。所有这些都得到了第3部分中的两个方程，并且它可以用于在反向传递期间重新计算激活。</p><p id="91e1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在我们将实现一个包装器函数，它返回一个重整器语言模型。我们可以使用Trax的<a class="ae km" href="https://trax-ml.readthedocs.io/en/latest/trax.models.html#trax.models.reformer.reformer.ReformerLM" rel="noopener ugc nofollow" target="_blank"> ReformerLM </a>来快速完成这项工作。它将具有如上所示的相同架构。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="e014" class="mq lm hi ml b fi mr ms l mt mu">def ReformerLM(vocab_size=33000, n_layers=2, mode='train', attention_type=tl.SelfAttention):<br/>    """<br/>    Args: <br/>        vocab_size (int): size of the vocabulary<br/>        n_layers (int): number of decoder layers<br/>        mode (string): setting of the model which can be 'train', 'eval', or 'predict' <br/>        attention_type(class): attention class to use <br/>    Returns: <br/>        model (ReformerLM): a reformer language model implemented in Trax<br/>    """    <br/>    # initialize an instance of Trax's ReformerLM class<br/>    model = trax.models.reformer.ReformerLM( <br/>        # set vocab size<br/>        vocab_size=vocab_size,<br/>        # set number of layers<br/>        n_layers=n_layers,<br/>        # set mode<br/>        mode=mode,<br/>        # set attention type<br/>        attention_type=attention_type<br/>    )<br/>    return model</span><span id="8ff1" class="mq lm hi ml b fi no ms l mt mu"># display the model<br/>temp_model = ReformerLM('train')<br/>print(str(temp_model))<br/><br/># free memory<br/>del temp_model</span></pre><p id="350e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，您将编写一个函数来接收我们的模型并对其进行训练。我们将实现<code class="du mi mj mk ml b">training_loop</code>来训练上面的神经网络。这是我们应该做的事情的清单:</p><ul class=""><li id="9311" class="kx ky hi jl b jm jn jp jq js kz jw la ka lb ke lc ld le lf bi translated">创建<code class="du mi mj mk ml b">TrainTask</code>和<code class="du mi mj mk ml b">EvalTask</code></li><li id="c273" class="kx ky hi jl b jm lg jp lh js li jw lj ka lk ke lc ld le lf bi translated">创建训练循环<code class="du mi mj mk ml b">trax.supervised.training.Loop</code></li></ul><p id="5478" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">根据train_task传入以下内容:</p><ul class=""><li id="261a" class="kx ky hi jl b jm jn jp jq js kz jw la ka lb ke lc ld le lf bi translated"><code class="du mi mj mk ml b">labeled_data=train_gen</code></li><li id="26e0" class="kx ky hi jl b jm lg jp lh js li jw lj ka lk ke lc ld le lf bi translated"><code class="du mi mj mk ml b">loss_layer=tl.CrossEntropyLoss()</code></li><li id="96a3" class="kx ky hi jl b jm lg jp lh js li jw lj ka lk ke lc ld le lf bi translated"><code class="du mi mj mk ml b">optimizer=trax.optimizers.Adam(0.01)</code></li><li id="c2ff" class="kx ky hi jl b jm lg jp lh js li jw lj ka lk ke lc ld le lf bi translated"><code class="du mi mj mk ml b">lr_schedule=lr_schedule</code></li><li id="024c" class="kx ky hi jl b jm lg jp lh js li jw lj ka lk ke lc ld le lf bi translated"><code class="du mi mj mk ml b">n_steps_per_checkpoint=10</code></li></ul><p id="1013" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们将使用带有Adam优化器的CrossEntropyLoss损失函数。请阅读<a class="ae km" href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=adam#trax.optimizers.adam.Adam" rel="noopener ugc nofollow" target="_blank"> trax </a>文档以获得全面理解。</p><p id="ff56" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">将以下内容传递给eval_task:</p><ul class=""><li id="4161" class="kx ky hi jl b jm jn jp jq js kz jw la ka lb ke lc ld le lf bi translated"><code class="du mi mj mk ml b">labeled_data=eval_gen</code></li><li id="9add" class="kx ky hi jl b jm lg jp lh js li jw lj ka lk ke lc ld le lf bi translated"><code class="du mi mj mk ml b">metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]</code></li></ul><p id="31b5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这个函数应该返回一个<code class="du mi mj mk ml b">training.Loop</code>对象。要了解更多信息，请查看<a class="ae km" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html?highlight=loop#trax.supervised.training.Loop" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="b6a8" class="mq lm hi ml b fi mr ms l mt mu">def training_loop(ReformerLM, train_gen, eval_gen, output_dir = "./model/"):<br/>    """<br/>    Args:<br/>        ReformerLM:  the Reformer language model you are building<br/>        train_gen (generator): train data generator.<br/>        eval_gen (generator): Validation generator. <br/>        output_dir (string): Path to save the model output. Defaults to './model/'.<br/><br/>    Returns:<br/>        trax.supervised.training.Loop: Training loop for the model.<br/>    """<br/><br/>    # use the warmup_and_rsqrt_decay learning rate schedule<br/>    lr_schedule = trax.lr.warmup_and_rsqrt_decay(<br/>        n_warmup_steps=1000, max_value=0.01)<br/>    <br/>    # define the train task<br/>    train_task = training.TrainTask(            <br/>        # labeled data<br/>        labeled_data=train_gen,<br/>        # loss layer<br/>        loss_layer=tl.CrossEntropyLoss(),<br/>        # optimizer<br/>        optimizer=trax.optimizers.Adam(0.01),<br/>        # lr_schedule<br/>        lr_schedule=lr_schedule,<br/>        # n_steps<br/>        n_steps_per_checkpoint=10<br/>    )<br/><br/>    # define the eval task<br/>    eval_task = training.EvalTask(                      <br/>        # labeled data<br/>        labeled_data=eval_gen,<br/>        # metrics<br/>        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]<br/>    )<br/><br/>    loop = training.Loop(ReformerLM(mode='train'),<br/>                         train_task,<br/>                         eval_tasks=[eval_task],<br/>                         output_dir=output_dir)<br/>    return loop</span></pre><h1 id="3e49" class="ll lm hi bd ln lo lp lq lr ls lt lu lv io lw ip lx ir ly is lz iu ma iv mb mc bi translated">第5部分:从预训练模型解码</h1><p id="bbb8" class="pw-post-body-paragraph jj jk hi jl b jm md ij jo jp me im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">我们现在将继续使用您刚刚实现的模型架构进行解码。我们将使用Trax的<a class="ae km" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.decoding.autoregressive_sample_stream" rel="noopener ugc nofollow" target="_blank">auto regressive _ sample _ stream()</a>解码方法进行快速推断。让我们定义几个参数来初始化我们的模型。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="c0b8" class="mq lm hi ml b fi mr ms l mt mu"># define the `predict_mem_len` and `predict_drop_len` of tl.SelfAttention<br/>def attention(*args, **kwargs):<br/>    # number of input positions to remember in a cache when doing fast inference. <br/>    kwargs['predict_mem_len'] = 120<br/>    # number of input elements to drop once the fast inference input cache fills up.<br/>    kwargs['predict_drop_len'] = 120<br/>    # return the attention layer with the parameters defined above<br/>    return tl.SelfAttention(*args, **kwargs)<br/><br/># define the model using the ReformerLM function we implemented earlier.<br/>model = ReformerLM(<br/>    vocab_size=33000,<br/>    n_layers=6,<br/>    mode='predict',<br/>    attention_type=attention,<br/>)<br/><br/># define an input signature so we can initialize our model. shape will be (1, 1) and the data type is int32.<br/>shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)</span></pre><p id="2c80" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们现在可以从包含预训练权重的文件中初始化我们的模型。我们将保存这个起始状态，以便在生成新对话时可以重置模型状态。这将在后面的<code class="du mi mj mk ml b">generate_dialogue()</code>功能中变得更加清晰。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="b99a" class="mq lm hi ml b fi mr ms l mt mu"># initialize from file<br/>model.init_from_file('chatbot_model1.pkl.gz',<br/>                     weights_only=True, input_signature=shape11)<br/><br/># save the starting state<br/>STARTING_STATE = model.state</span></pre><p id="57a6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们定义几个效用函数来帮助我们令牌化和去令牌化。我们可以使用<code class="du mi mj mk ml b">trax.data.tf_inputs</code>中的<a class="ae km" href="https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.tokenize" rel="noopener ugc nofollow" target="_blank"> tokenize() </a>和<a class="ae km" href="https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.detokenize" rel="noopener ugc nofollow" target="_blank"> detokenize() </a>来实现这一点。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="c1ea" class="mq lm hi ml b fi mr ms l mt mu">def tokenize(sentence, vocab_file, vocab_dir):<br/>    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]<br/><br/>def detokenize(tokens, vocab_file, vocab_dir):<br/>    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)</span></pre><p id="8a21" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们现在准备定义我们的解码函数。这将返回一个生成器，该生成器生成模型输出下一个符号。只要给它输入一个起始句，它就能预测接下来的单词。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="0af4" class="mq lm hi ml b fi mr ms l mt mu">def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):<br/>    """<br/>    Args:<br/>        ReformerLM:  the Reformer language model you just trained<br/>        start_sentence (string): starting sentence of the conversation<br/>        vocab_file (string): vocabulary filename<br/>        vocab_dir (string): directory of the vocabulary file<br/>        temperature (float): parameter for sampling ranging from 0.0 to 1.0.<br/>            0.0: same as argmax, always pick the most probable token<br/>            1.0: sampling from the distribution (can sometimes say random things)<br/><br/>    Returns:<br/>        generator: yields the next symbol generated by the model<br/>    """<br/>    <br/>    # Create input tokens using the the tokenize function<br/>    input_tokens = tokenize(start_sentence, vocab_file=vocab_file, vocab_dir=vocab_dir)<br/>    <br/>    # Add batch dimension to array. Convert from (n,) to (x, n) where <br/>    # x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)<br/>    input_tokens_with_batch = np.array(input_tokens)[None, :]<br/>    <br/>    # call the autoregressive_sample_stream function from trax<br/>    output_gen = trax.supervised.decoding.autoregressive_sample_stream( <br/>        # model<br/>        ReformerLM,<br/>        # inputs will be the tokens with batch dimension<br/>        inputs=input_tokens_with_batch,<br/>        # temperature<br/>        temperature=temperature<br/>    )<br/>    <br/>    return output_gen</span></pre><p id="7e9d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">单元测试-</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="30b0" class="mq lm hi ml b fi mr ms l mt mu">import pickle<br/><br/>WEIGHTS_FROM_FILE = ()<br/><br/>with open('weights', 'rb') as file:<br/>    WEIGHTS_FROM_FILE = pickle.load(file)<br/><br/>shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)<br/><br/>def attention(*args, **kwargs):<br/>    kwargs['predict_mem_len'] = 120<br/>    kwargs['predict_drop_len'] = 120<br/>    return tl.SelfAttention(*args, **kwargs)<br/><br/>test_model = ReformerLM(vocab_size=5, n_layers=1, mode='predict', attention_type=attention)<br/><br/>test_output_gen = ReformerLM_output_gen(test_model, "test", vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=0)<br/><br/>test_model.init_weights_and_state(shape11)<br/><br/>test_model.weights = WEIGHTS_FROM_FILE<br/><br/>output = []<br/><br/>for i in range(6):<br/>    output.append(next(test_output_gen)[0])<br/><br/>print(output)<br/><br/># free memory<br/>del test_model <br/>del WEIGHTS_FROM_FILE<br/>del test_output_gen</span></pre><p id="c282" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">太好了！现在，您将能够看到运行中的模型。下面的实用函数将调用您刚刚实现的生成器，并将格式化输出以便于阅读。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="c1df" class="mq lm hi ml b fi mr ms l mt mu">shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)<br/><br/>def attention(*args, **kwargs):<br/>    kwargs['predict_mem_len'] = 120  # max length for predictions<br/>    kwargs['predict_drop_len'] = 120  # never drop old stuff<br/>    return tl.SelfAttention(*args, **kwargs)<br/><br/>model = ReformerLM(<br/>    vocab_size=33000,<br/>    n_layers=6,<br/>    mode='predict',<br/>    attention_type=attention,<br/>)</span></pre><p id="0187" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">和</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="cab5" class="mq lm hi ml b fi mr ms l mt mu">model.init_from_file('chatbot_model1.pkl.gz',<br/>                     weights_only=True, input_signature=shape11)<br/><br/>STARTING_STATE = model.state</span></pre><p id="da22" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">还有，</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="ad17" class="mq lm hi ml b fi mr ms l mt mu">def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):<br/>    """<br/>    Args:<br/>        ReformerLM:  the Reformer language model you just trained<br/>        model_state (np.array): initial state of the model before decoding<br/>        start_sentence (string): starting sentence of the conversation<br/>        vocab_file (string): vocabulary filename<br/>        vocab_dir (string): directory of the vocabulary file<br/>        max_len (int): maximum number of tokens to generate <br/>        temperature (float): parameter for sampling ranging from 0.0 to 1.0.<br/>            0.0: same as argmax, always pick the most probable token<br/>            1.0: sampling from the distribution (can sometimes say random things)<br/><br/>    Returns:<br/>        generator: yields the next symbol generated by the model<br/>    """  <br/>    <br/>    # define the delimiters we used during training<br/>    delimiter_1 = 'Person 1: ' <br/>    delimiter_2 = 'Person 2: '<br/>    <br/>    # initialize detokenized output<br/>    sentence = ''<br/>    <br/>    # token counter<br/>    counter = 0<br/>    <br/>    # output tokens. we insert a ': ' for formatting<br/>    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]<br/>    <br/>    # reset the model state when starting a new dialogue<br/>    ReformerLM.state = model_state<br/>    <br/>    # calls the output generator implemented earlier<br/>    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)<br/>    <br/>    # print the starting sentence<br/>    print(start_sentence.split(delimiter_2)[0].strip())<br/>    <br/>    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.<br/>    for o in output:<br/>        <br/>        result.append(o)<br/>        <br/>        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)<br/>        <br/>        if sentence.endswith(delimiter_1):<br/>            sentence = sentence.split(delimiter_1)[0]<br/>            print(f'{delimiter_2}{sentence}')<br/>            sentence = ''<br/>            result.clear()<br/>        <br/>        elif sentence.endswith(delimiter_2):<br/>            sentence = sentence.split(delimiter_2)[0]<br/>            print(f'{delimiter_1}{sentence}')<br/>            sentence = ''<br/>            result.clear()<br/><br/>        counter += 1<br/>        <br/>        if counter &gt; max_len:<br/>            break</span></pre><p id="a607" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们现在可以输入不同的起始句，看看这个模型是如何生成对话的。你甚至可以输入自己的起始句。请记住提出一个涵盖Multiwoz数据集中的主题的问题，这样您就可以展开有意义的对话。</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="0401" class="mq lm hi ml b fi mr ms l mt mu">sample_sentence = ' Person 1: Are there theatres in town? Person 2: '<br/>generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)</span></pre><p id="05dc" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出-</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="b71e" class="mq lm hi ml b fi mr ms l mt mu">Person 1: Are there theatres in town?<br/>Person 2: : There are 4 theatres in town. Do you have a preference? <br/>Person 1: Not really, can you recommend one and give me the address and postcode? <br/>Person 2: How about the ADC Theatre located at Park Street? <br/>Person 1: That sounds great. Can I get the postcode and phone number? <br/>Person 2: The phone number is 01223300085. The postcode is cb58as. <br/>Person 1: I also need a train to Cambridge on Thursday the week I will be traveling alone.</span></pre><p id="a9ba" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">接下来，</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="24d0" class="mq lm hi ml b fi mr ms l mt mu">sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '<br/>generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)</span></pre><p id="cf1d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出-</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="442c" class="mq lm hi ml b fi mr ms l mt mu">Person 1: Is there a hospital nearby?<br/>Person 2: : Addensbrookes Hospital is located at Hills Rd, Cambridge, postcode CB20QQ. Do you need the phone number? <br/>Person 1: No, but I do need the main phone number, please. <br/>Person 2: The phone number is 01223245151. <br/>Person 1: Thank you for your help. <br/>Person 2: You're welcome. Have a nice day.<br/>Person 1: Thank you for your help. <br/>Person 1: You're welcome 43, Fensounds good!</span></pre><p id="864c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">最后，</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="0cc3" class="mq lm hi ml b fi mr ms l mt mu">sample_sentence = ' Person 1: Can you book a taxi? Person 2: '<br/>generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)</span></pre><p id="0297" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出-</p><pre class="iy iz ja jb fd mm ml mn mo aw mp bi"><span id="3c3d" class="mq lm hi ml b fi mr ms l mt mu">Person 1: Can you book a taxi?<br/>Person 2: : I sure can. Where would you like to be picked up? <br/>Person 1: I'm going to be picked up from aylesbray lodge guest house. <br/>Person 2: I'd be happy to help you. What time would you like to arrive? <br/>Person 1: I need to leave after 11:00. <br/>Person 2: Booking completed! Booked car type	:	grey ford<br/>Contact number	:	07262372<br/> <br/>Person 1: I'm looking for a train to Cambridge on Saturday.</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es np"><img src="../Images/c1406ed394315c1a5b3e50b2795cb01d.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*g7Yx6lpBYp0uuHp40cLFYw.jpeg"/></div></figure><p id="ba9e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">恭喜你！，您刚刚创建了一个自动化的夏波特。希望你旅途愉快。</strong></p><p id="6f1e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这个故事变得很长，但进一步分割可能不会有影响，因为所有部分都是相互关联的。</p></div></div>    
</body>
</html>