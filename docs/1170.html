<html>
<head>
<title>Cyclical Learning Rates for Training Neural Networks- Paper Summarization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练神经网络的循环学习率-论文摘要</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/cyclical-learning-rates-for-training-neural-networks-paper-summarization-744661cf6dcf?source=collection_archive---------12-----------------------#2021-02-17">https://medium.com/analytics-vidhya/cyclical-learning-rates-for-training-neural-networks-paper-summarization-744661cf6dcf?source=collection_archive---------12-----------------------#2021-02-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/127db227bc625fb74624e4622b771bbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kFZxqr1MkpJdklXF.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图片来自纸质快照集(https://arxiv.org/abs/1704.00109)</figcaption></figure><p id="6036" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">听从知识随着我们的分享而增加的建议，我开始了一个新的系列，在那里我将努力抓住容易实现的论文的要点。</p><p id="fadd" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">第一行是用于训练神经网络的<a class="ae jr" href="https://arxiv.org/pdf/1506.01186.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="iv hi"/></a><strong class="iv hi">——les lie n . Smith的循环学习率。</strong></p><p id="b9f7" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">本文讨论了一种调整学习速率的方法，这种方法消除了寻找最佳值和全局学习时间表的使用。这种方法不是单调地降低学习率，而是让学习率在合理的边界值之间循环变化。用循环学习率而不是固定值进行训练可以提高分类精度，而不需要调整，并且通常迭代次数更少。</p><p id="1da5" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">本文在CIFAR-10和CIFAR-100数据集上验证了循环学习率的性能。在CIFAR-10和CIFAR-100数据集上，以及在具有两个众所周知的架构的ImageNet上，使用ResNets、随机深度网络和DenseNets演示了循环学习率。结果请参考<a class="ae jr" href="https://arxiv.org/pdf/1506.01186.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。我想用这个博客来解释周期性学习率背后的直觉。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es js"><img src="../Images/9c4a9a30d7031d46f5fdc8719563c4b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*R588LmaCsFoi0ZsvomBhFg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图2显示了CLR在分类准确性方面的潜在优势</figcaption></figure><ol class=""><li id="b7fa" class="jx jy hh iv b iw ix ja jb je jz ji ka jm kb jq kc kd ke kf bi translated">循环学习率(CLR)方法实际上消除了调整学习率的需要，但仍达到接近最佳的分类精度。此外，与自适应学习率不同，CLR方法基本上不需要额外的计算。</li></ol><p id="6bc6" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi">2.</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kg"><img src="../Images/96814bde9966f403abfcbf4fde44413a.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*odrWOHdGHrYYb1zThLOkig.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">三角学习率政策。蓝线表示学习率值在界限之间变化。输入参数stepsize是半个周期内的迭代次数。</figcaption></figure><p id="da66" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">学习率在一个值的范围内变化，而不是采用逐步固定的或指数递减的值。也就是说，设置最小和最大边界，学习率在这些边界之间循环变化。使用多种函数形式的实验，如<strong class="iv hi">三角形窗口(线性)、韦尔奇窗口(抛物线)和汉恩窗口(正弦曲线)</strong>都产生了相同的结果。这导致采用三角形窗口(线性增加然后线性减少),因为它是包含这种思想的最简单的函数。</p><p id="fa82" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">3.对CLR方法工作原理的直观理解来自于对损失函数拓扑的考虑。<a class="ae jr" href="https://arxiv.org/abs/1502.04390" rel="noopener ugc nofollow" target="_blank"> <em class="kh"> Dauphin等人</em> </a>认为最小化损失的困难源于鞍点而不是糟糕的局部极小值。鞍点具有小的梯度，这减缓了学习过程。然而，增加学习速率允许更快地穿越鞍点平台。</p><blockquote class="ki kj kk"><p id="3e07" class="it iu kh iv b iw ix iy iz ja jb jc jd kl jf jg jh km jj jk jl kn jn jo jp jq ha bi translated">因此，与固定LR方法相比，CLR方法中LR的循环增加有助于快速克服鞍点</p></blockquote><p id="f105" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">4.关于CLR为什么工作的一个更实际的原因是，最佳学习速率可能在界限之间，并且在整个训练中将使用接近最佳的学习速率。<em class="kh">(当我们查看第6部分中选择的CLR范围时，这一点得到了证明)</em></p><p id="eb47" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">5.<strong class="iv hi">如何估计周期长度的合适值？</strong></p><p id="6a48" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">通过将训练图像的数量除以所使用的批次大小来计算时期。例如，CIFAR-10有50，000个训练图像，批次大小为100，因此一个时期= 50，000/100 = 500次迭代。最终精度结果实际上对周期长度相当稳健，但实验表明，通常最好将步长设置为一个历元中迭代次数的2-10倍。</p><p id="bbf0" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">6.如何估计合理的最小和最大边界值？</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es ko"><img src="../Images/806d0a68a06ff451c97bc840d1ab4836.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*PktZIiaVDNuLSDyFve43dg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">分类准确度作为8个时期学习率增加的函数(LR范围测试)</figcaption></figure><p id="b4b7" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">无论何时开始一个新的架构或数据集，单个<strong class="iv hi"> LR范围测试</strong>都会提供良好的LR值和范围。</p><p id="0ef5" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在让学习率在低LR值和高LR值之间线性增加的同时，运行您的模型几个时期。在这种情况下，学习率将在短时间内从最小值线性增加到最大值。接下来，绘制准确度与学习率的关系图(如上图所示)。当精度开始增加时，以及当精度降低、变得粗糙或开始下降时，记录学习率值。这两个学习率是边界的好选择；也就是说，将base lr设置为第一个值，将max lr设置为后一个值。</p><p id="3672" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">或者，可以使用经验法则，即最佳学习速率通常在最大学习速率的两倍之内，该最大学习速率收敛并将基lr设置为最大lr的1/3或1/4。</p><p id="626b" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">7.可以推测，三角策略的好处来自于降低学习率，因为这是准确性攀升最多的时候。作为测试，实施了衰减策略，其中学习速率从最大lr值开始，然后线性降低到基本lr值，以步长迭代次数。之后，学习率固定为基数lr。但是这种方法仅实现了78.5%的最终准确度，这提供了证据，即增加和降低学习速率对于CLR方法的益处是必不可少的。<em class="kh">(详见4.1.1节论文)</em></p><blockquote class="ki kj kk"><p id="a801" class="it iu kh iv b iw ix iy iz ja jb jc jd kl jf jg jh km jj jk jl kn jn jo jp jq ha bi translated">“提高和降低学习率对于CLR方法的优势至关重要”</p></blockquote><p id="b6fd" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">8.<strong class="iv hi">部分结果快照</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kp"><img src="../Images/e11989c459c528723eca5864947eb39f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*NjxRdbv17QZ5NlQmy8NqPA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">CLR(三角形)有助于在更少的迭代中实现与固定LR相同的精度</figcaption></figure><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kq"><img src="../Images/35b19d141423cd1381c41a9c2d60ff11.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*V7YysTsS8ikBQ3BqXgyOew.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">批量标准化CIFAR-10示例</figcaption></figure><h1 id="ca26" class="kr ks hh bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated"><strong class="ak">遗言</strong></h1><p id="8ca4" class="pw-post-body-paragraph it iu hh iv b iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm lt jo jp jq ha bi translated">我希望这个博客能简单介绍一下循环学习及其优势。CLR背后的主要动机是它能够轻松克服鞍点，并在明显更少的时期内实现更好的准确性。该方法非常容易在任何框架中实现。我在我的一个专业项目中尝试了这种方法，用于在生物医学图像分类中对预训练的GoogleNet进行分层微调。这帮助我用非常少的计算能力实现了更好的拟合模型。</p><h1 id="0c2d" class="kr ks hh bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated"><strong class="ak">参考文献</strong></h1><p id="d864" class="pw-post-body-paragraph it iu hh iv b iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm lt jo jp jq ha bi translated">训练神经网络的循环学习率— <a class="ae jr" href="https://arxiv.org/pdf/1506.01186.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="f266" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">非凸优化的均衡自适应学习率— <a class="ae jr" href="https://arxiv.org/abs/1502.04390" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="81d8" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">快照集— <a class="ae jr" href="https://arxiv.org/abs/1704.00109" rel="noopener ugc nofollow" target="_blank">链接</a></p></div></div>    
</body>
</html>