<html>
<head>
<title>Reinforcement Learning in Continuous Action Spaces: DDPG</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">连续动作空间中的强化学习:DDPG</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-in-continuous-action-spaces-ddpg-bbd64aa5434?source=collection_archive---------16-----------------------#2021-03-09">https://medium.com/analytics-vidhya/reinforcement-learning-in-continuous-action-spaces-ddpg-bbd64aa5434?source=collection_archive---------16-----------------------#2021-03-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="37ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">深入强化学习</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/bc760856eb0c70f6940b37d4708f700c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/0*CovY52wifDrKoArS"/></div></figure><p id="ed10" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">作者安东尼奥·李斯</em></p><h1 id="88a0" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">介绍</h1><p id="3778" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">大家好，这是关于强化学习的第三篇文章，也是关注持续行动环境的新系列的开始。</p><p id="c566" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本帖中，我们将从零开始实现DDPG，我们将尝试解决<a class="ae kp" href="https://gym.openai.com/envs/Pendulum-v0/" rel="noopener ugc nofollow" target="_blank">摆</a>和<a class="ae kp" href="https://gym.openai.com/envs/LunarLanderContinuous-v2/" rel="noopener ugc nofollow" target="_blank">月球着陆器</a>。</p><h1 id="c173" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">为什么是连续的行动空间？</h1><p id="2412" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">在继续下一步之前，我们需要激励我们为什么要解决连续的空间环境。到目前为止，我们总是解决pong，但你可以使用算法、方法，甚至所有代码来解决任何其他Atari游戏。</p><p id="f1dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是强化学习的许多现实应用需要代理从连续空间中选择动作。例如，自主机器人需要一个代理在连续的空间中采取行动。自动驾驶也是如此，顺便说一下，这是汽车行业最热门的话题之一。</p><h1 id="3750" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">要解决的环境</h1><p id="1c36" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">如介绍部分所述，我们将解决OpeanAI体育馆库中的两个环境。</p><h1 id="e024" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">钟摆</h1><p id="a9aa" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">我们从钟摆开始，这是一个经典的环境。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kq"><img src="../Images/c549621426a3a94fb96b464c899a3b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/0*ejDByj6ud2eatzmE"/></div></figure><p id="c42f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目标是试图保持一个无摩擦的钟摆直立。我们可以通过下面的代码看到输入和动作特征:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/3e4e5b2cf216932554f5f5870ffb5713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/0*D1CIfn-y5FLyvb7y"/></div></figure><p id="791e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入由三个观察值组成。看<a class="ae kp" href="https://github.com/openai/gym/wiki/Pendulum-v0" rel="noopener ugc nofollow" target="_blank">文档</a>，可以看到它们分别代表了摆的角度(θ的余弦和正弦)和它的角速度(θ点)。另一方面，我们知道作用量是-2.0到2.0之间的单值，它代表了摆所受的向左或向右的力的大小。奖励的精确公式是:</p><p id="4ae4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-(θ2+0.1 *θ2 dt+0.001 * action 2)</p><h1 id="6e40" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">月球着陆器</h1><p id="55b2" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">另一个要解决的经典环境是月球着陆器(在其连续版本中)。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kr"><img src="../Images/568488c7e666fd7e1c63930780294e0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zERVQi5VbPjX8Cmz"/></div></div></figure><p id="1079" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个游戏的主要目标是尽可能轻柔和省油地引导代理到达着陆平台。从<a class="ae kp" href="https://github.com/openai/gym/wiki/Leaderboard#lunarlander-v2" rel="noopener ugc nofollow" target="_blank">文档</a>中，我们知道着陆垫总是在坐标(0，0)处。和前面一样，我们可以看到使用相同代码的输入和动作特征:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/d45e66966494032ef3b6978ca91beec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/0*KCtHcq3RSfwzBQUA"/></div></figure><p id="d22a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入由8个值组成，它们是:</p><ol class=""><li id="2ff5" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated">着陆器的x坐标</li><li id="029b" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">着陆器的y坐标</li><li id="f1d5" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">vx，水平速度</li><li id="b0ee" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">vy，垂直速度</li><li id="1926" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">θ，空间中的方向</li><li id="c759" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">vθ，角速度</li><li id="0cff" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">左腿着地(布尔型)</li><li id="7e10" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">右腿着地(布尔型)</li></ol><p id="6f63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于两个维度，该操作是一个从-1到+1的双值数组。第一个控制主发动机，-1.0关，从0到1.0，发动机的功率从50%到100%功率。发动机不能在低于50%的功率下工作。第二个值控制左右发动机。从-1.0到-0.5，它点燃左引擎；从0.5到1.0，它点燃右边的引擎；从-0.5到0.5，发动机关闭。</p><p id="4893" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从屏幕顶部移动到着陆垫和零速度的奖励是从100到140分。如果着陆器离开着陆垫，它就失去奖励。如果着陆器崩溃或停止，这一集结束，获得额外的-100或+100点。每个支腿接地触点为+10。点燃主引擎每帧-0.3分。</p><h1 id="735a" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">深度确定性政策梯度</h1><p id="8a12" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">DDPG算法(深度确定性策略梯度)是由Timothy P. Lillicrap等人于2015年在名为<a class="ae kp" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">具有深度强化学习的连续控制</a>的论文中引入的。它属于演员-评论家家族，但同时，政策是确定性的(相同的输入，相同的输出/要采取的行动)。DDPG也和DQN有一些相同的想法。具体而言，使用来自重放缓冲器的样本(以避免相关性)和目标Q网络来偏离策略地训练网络。</p><p id="a5b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总之，DDPG与DQN有共同之处，即确定性政策，这是政策外的训练，但同时也有行动者-批评家的方法。所有这些现在看起来可能有点复杂，但是当我们看到代码时，在下一节中会变得更容易。</p><h1 id="7514" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">重放缓冲器</h1><p id="246b" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">让我们从最简单的部分开始，重放缓冲区:</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="07bb" class="lp jn hi ll b fi lq lr l ls lt"><em class="jd">class</em> ReplayBuffer():<br/>    <em class="jd">def</em> __init__(self, env, buffer_capacity=BUFFER_CAPACITY, batch_size=BATCH_SIZE, min_size_buffer=MIN_SIZE_BUFFER):<br/>        self.buffer_capacity = buffer_capacity<br/>        self.batch_size = batch_size<br/>        self.min_size_buffer = min_size_buffer<br/>        self.buffer_counter = 0<br/>        self.n_games = 0<br/>        <br/>        self.states = np.zeros((self.buffer_capacity, env.observation_space.shape[0]))<br/>        self.actions = np.zeros((self.buffer_capacity, env.action_space.shape[0]))<br/>        self.rewards = np.zeros((self.buffer_capacity))<br/>        self.next_states = np.zeros((self.buffer_capacity, env.observation_space.shape[0]))<br/>        self.dones = np.zeros((self.buffer_capacity), dtype=<em class="jd">bool</em>)<br/>        </span><span id="daed" class="lp jn hi ll b fi lu lr l ls lt">        <br/>    <em class="jd">def</em> __len__(self):<br/>        return self.buffer_counter</span><span id="dc6d" class="lp jn hi ll b fi lu lr l ls lt">    <em class="jd">def</em> add_record(self, state, action, reward, next_state, done):<br/>        # Set index to zero if counter = buffer_capacity and start again (1 % 100 = 1 and 101 % 100 = 1) so we substitute the older entries<br/>        index = self.buffer_counter % self.buffer_capacity</span><span id="71e2" class="lp jn hi ll b fi lu lr l ls lt">        self.states[index] = state<br/>        self.actions[index] = action<br/>        self.rewards[index] = reward<br/>        self.next_states[index] = next_state<br/>        self.dones[index] = done<br/>        <br/>        # Update the counter when record something<br/>        self.buffer_counter += 1<br/>    <br/>    <em class="jd">def</em> check_buffer_size(self):<br/>        return self.buffer_counter &gt;= self.batch_size and self.buffer_counter &gt;= self.min_size_buffer<br/>    <br/>    <em class="jd">def</em> update_n_games(self):<br/>        self.n_games += 1<br/>    <br/>    <em class="jd">def</em> get_minibatch(self):<br/>        # If the counter is less than the capacity we don't want to take zeros records, <br/>        # if the cunter is higher we don't access the record using the counter <br/>        # because older records are deleted to make space for new one<br/>        buffer_range = <em class="jd">min</em>(self.buffer_counter, self.buffer_capacity)<br/>        <br/>        batch_index = np.random.choice(buffer_range, self.batch_size, replace=False)</span><span id="bf53" class="lp jn hi ll b fi lu lr l ls lt">        # Take indices<br/>        state = self.states[batch_index]<br/>        action = self.actions[batch_index]<br/>        reward = self.rewards[batch_index]<br/>        next_state = self.next_states[batch_index]<br/>        done = self.dones[batch_index]<br/>        <br/>        return state, action, reward, next_state, done<br/>    <br/>    <em class="jd">def</em> save(self, folder_name):<br/>        """<br/>        Save the replay buffer<br/>        """<br/>        if not os.path.isdir(folder_name):<br/>            os.mkdir(folder_name)</span><span id="e44e" class="lp jn hi ll b fi lu lr l ls lt">        np.save(folder_name + '/states.npy', self.states)<br/>        np.save(folder_name + '/actions.npy', self.actions)<br/>        np.save(folder_name + '/rewards.npy', self.rewards)<br/>        np.save(folder_name + '/next_states.npy', self.next_states)<br/>        np.save(folder_name + '/dones.npy', self.dones)<br/>        <br/>        dict_info = {"buffer_counter": self.buffer_counter, "n_games": self.n_games}<br/>        <br/>        with <em class="jd">open</em>(folder_name + '/dict_info.json', 'w') as f:<br/>            json.dump(dict_info, f)</span><span id="e7aa" class="lp jn hi ll b fi lu lr l ls lt">    <em class="jd">def</em> load(self, folder_name):<br/>        """<br/>        Load the replay buffer<br/>        """<br/>        self.states = np.load(folder_name + '/states.npy')<br/>        self.actions = np.load(folder_name + '/actions.npy')<br/>        self.rewards = np.load(folder_name + '/rewards.npy')<br/>        self.next_states = np.load(folder_name + '/next_states.npy')<br/>        self.dones = np.load(folder_name + '/dones.npy')<br/>        <br/>        with <em class="jd">open</em>(folder_name + '/dict_info.json', 'r') as f:<br/>            dict_info = json.load(f)<br/>        self.buffer_counter = dict_info["buffer_counter"]<br/>        self.n_games = dict_info["n_games"]</span></pre><p id="149f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个概念与<a class="ae kp" href="https://antonai.blog/?p=137#more-137" rel="noopener ugc nofollow" target="_blank"> DDDQN文章</a>中使用的相同。我们正在存储所有的状态、动作、奖励、next_states以及从调用函数add_record的环境的交互中得到的终端标志。然后我们有一个get_minibatch方法，它返回这些观察值的随机子集。</p><p id="38e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，我们有一个check_buffer_size方法，它确保缓冲区大小大于批处理大小和我们在配置文件中定义的最小值。</p><p id="1fca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们有保存和加载缓冲区的保存和加载方法。这在你需要停止和重新开始训练的时候特别有用(如果你用的是Google Colab，这会救你一命)。</p><h1 id="ac13" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">网络</h1><p id="e973" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">正如通过介绍DDPG算法所说的，我们有标准的演员和评论家的训练和目标版本。所以我们实际上有四个神经网络，两个演员和两个评论家。</p><p id="97b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们从定义参与者的代码开始。</p><h1 id="a4f5" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">行动者</h1><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="40fc" class="lp jn hi ll b fi lq lr l ls lt"><em class="jd">class</em> Actor(tf.keras.Model):<br/>    <em class="jd">def</em> __init__(self, name, actions_dim, upper_bound, hidden_0=CRITIC_HIDDEN_0, hidden_1=CRITIC_HIDDEN_1, init_minval=INIT_MINVAL, init_maxval=INIT_MAXVAL):<br/>        <em class="jd">super</em>(Actor, self).__init__()<br/>        self.hidden_0 = hidden_0<br/>        self.hidden_1 = hidden_1<br/>        self.actions_dim = actions_dim<br/>        self.init_minval = init_minval<br/>        self.init_maxval = init_maxval<br/>        self.upper_bound = upper_bound<br/>        <br/>        self.net_name = name</span><span id="d9b9" class="lp jn hi ll b fi lu lr l ls lt">        self.dense_0 = Dense(self.hidden_0, activation='relu')<br/>        self.dense_1 = Dense(self.hidden_1, activation='relu')<br/>        self.policy = Dense(self.actions_dim, kernel_initializer=random_uniform(minval=self.init_minval, maxval=self.init_maxval), activation='tanh')</span><span id="1d36" class="lp jn hi ll b fi lu lr l ls lt">    <em class="jd">def</em> call(self, state):<br/>        x = self.dense_0(state)<br/>        policy = self.dense_1(x)<br/>        policy = self.policy(policy)</span><span id="90db" class="lp jn hi ll b fi lu lr l ls lt">        return policy * self.upper_bound</span></pre><p id="bbea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你所看到的，这是一个非常简单的网络，有两个隐藏层和最后一个激活层。提醒一下，双曲正切函数定义为:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lv"><img src="../Images/210cce45316f0755cf0ba6f09636e6b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/0*uFxFoK8MnZhjp0sn"/></div></figure><p id="cdfc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在这里找到所有激活功能<a class="ae kp" href="https://www.wikiwand.com/en/Activation_function" rel="noopener ugc nofollow" target="_blank">的定义。</a></p><p id="6152" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它的输出范围从-1到1，所以我们需要乘以动作的上限。例如，对于钟摆环境，我们需要将tanh函数的输出乘以2，以根据环境的要求获得-2和2之间的动作。对于登月，我们实际上并不需要它，因为在两个维度上的行动空间都被限制在-1到1之间。</p><p id="906a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如您所看到的，Actor输出是确定的。给定相同的输入(状态)，将会有相同的输出向量(动作)。</p><p id="c465" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到最后一层的权重初始化有所不同。我们用最小值和最大值(在我们的例子中是+/-0.005)从均匀分布中初始化它们。这对于防止在训练的初始阶段出现零梯度是必要的，因为我们使用了双曲正切激活函数。</p><h1 id="0bc7" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">批评家</h1><p id="74d3" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">我们现在可以看到批评家网络的代码:</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="ad6e" class="lp jn hi ll b fi lq lr l ls lt"><em class="jd">class</em> Critic(tf.keras.Model):<br/>    <em class="jd">def</em> __init__(self, name, hidden_0=CRITIC_HIDDEN_0, hidden_1=CRITIC_HIDDEN_1):<br/>        <em class="jd">super</em>(Critic, self).__init__()<br/>        <br/>        self.hidden_0 = hidden_0<br/>        self.hidden_1 = hidden_1</span><span id="690c" class="lp jn hi ll b fi lu lr l ls lt">        self.net_name = name</span><span id="950a" class="lp jn hi ll b fi lu lr l ls lt">        self.dense_0 = Dense(self.hidden_0, activation='relu')<br/>        self.dense_1 = Dense(self.hidden_1, activation='relu')<br/>        self.q_value = Dense(1, activation=None)</span><span id="d193" class="lp jn hi ll b fi lu lr l ls lt">    <em class="jd">def</em> call(self, state, action):<br/>        state_action_value = self.dense_0(tf.concat([state, action], axis=1))<br/>        state_action_value = self.dense_1(state_action_value)</span><span id="8e78" class="lp jn hi ll b fi lu lr l ls lt">        q_value = self.q_value(state_action_value)</span><span id="f1de" class="lp jn hi ll b fi lu lr l ls lt">        return q_value</span></pre><p id="b441" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像往常一样，批评家估计Q值，即在某个州采取行动的贴现回报。但是我们的动作是一个数组，所以我们需要接受状态和动作作为输入。如您所见，在call方法中，使用TensorFlow中的concat函数将状态和动作堆叠在一起。像往常一样，输出将是一个单一的数字。</p><p id="5d40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">行动是由行动者给出的，因此评论家将把行动者网络的输出作为输入。</p><h1 id="5e71" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">代理人</h1><p id="02fd" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">我们可以看到代理逻辑的实现。</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="7e71" class="lp jn hi ll b fi lq lr l ls lt"><em class="jd">class</em> Agent:<br/>    <em class="jd">def</em> __init__(self, env, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR, gamma=GAMMA, max_size=BUFFER_CAPACITY, tau=TAU, path_save=PATH_SAVE, path_load=PATH_LOAD):<br/>        <br/>        self.gamma = gamma<br/>        self.tau = tau<br/>        self.replay_buffer = ReplayBuffer(env, max_size)<br/>        self.actions_dim = env.action_space.shape[0]<br/>        self.upper_bound = env.action_space.high[0]<br/>        self.lower_bound = env.action_space.low[0]<br/>        self.actor_lr = actor_lr<br/>        self.critic_lr = critic_lr<br/>        self.path_save = path_save<br/>        self.path_load = path_load<br/>        <br/>        self.actor = Actor(name='actor', actions_dim=self.actions_dim, upper_bound=self.upper_bound)<br/>        self.critic = Critic(name='critic')<br/>        self.target_actor = Actor(name='target_actor', actions_dim=self.actions_dim, upper_bound=self.upper_bound)<br/>        self.target_critic = Critic(name='target_critic')</span><span id="e9f3" class="lp jn hi ll b fi lu lr l ls lt">        self.actor.<em class="jd">compile</em>(optimizer=opt.Adam(learning_rate=actor_lr))<br/>        self.critic.<em class="jd">compile</em>(optimizer=opt.Adam(learning_rate=critic_lr))<br/>        self.target_actor.<em class="jd">compile</em>(optimizer=opt.Adam(learning_rate=actor_lr))<br/>        self.target_critic.<em class="jd">compile</em>(optimizer=opt.Adam(learning_rate=critic_lr))</span><span id="7f38" class="lp jn hi ll b fi lu lr l ls lt">        actor_weights = self.actor.get_weights()<br/>        critic_weights = self.critic.get_weights()<br/>        <br/>        self.target_actor.set_weights(actor_weights)<br/>        self.target_critic.set_weights(critic_weights)<br/>        <br/>        self.noise = np.zeros(self.actions_dim)</span></pre><p id="b1a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看类输入，您可以看到它是如何利用已定义的环境来定义动作的维度、上限和下限，以及定义重放缓冲区的。</p><p id="3d17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们使用这些维度来定义四个网络:行动者、目标行动者、评论家和目标评论家。一开始，我们只是将演员和评论家的权重复制到目标网络中。</p><h1 id="f593" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">目标网络</h1><p id="5594" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">目标网络是其原始网络的延时副本，缓慢更新其权重。至于DDDQN，我们正在使用这些目标网络来提高学习稳定性。我们会看到它们是如何在损失函数中使用的。现在，让我们看看如何更新这些网络:</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="00c6" class="lp jn hi ll b fi lq lr l ls lt"><em class="jd">def</em> update_target_networks(self, tau):<br/>        actor_weights = self.actor.weights<br/>        target_actor_weights = self.target_actor.weights<br/>        for index in <em class="jd">range</em>(<em class="jd">len</em>(actor_weights)):<br/>            target_actor_weights[index] = tau * actor_weights[index] + (1 - tau) * target_actor_weights[index]</span><span id="fb4b" class="lp jn hi ll b fi lu lr l ls lt">        self.target_actor.set_weights(target_actor_weights)<br/>        <br/>        critic_weights = self.critic.weights<br/>        target_critic_weights = self.target_critic.weights<br/>    <br/>        for index in <em class="jd">range</em>(<em class="jd">len</em>(critic_weights)):<br/>            target_critic_weights[index] = tau * critic_weights[index] + (1 - tau) * target_critic_weights[index]</span><span id="ae60" class="lp jn hi ll b fi lu lr l ls lt">        self.target_critic.set_weights(target_critic_weights)</span></pre><p id="eaee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如您所见，我们将网络的权重更新为目标权重和训练好的网络权重之间的加权平均值。在我们的例子中，Tau等于配置文件中定义的0.05。我们用τ作为训练好的网络的权重。鉴于tau值较低，我们正在进行“软”更新，但我们每次更新训练好的网络的权重时都会更新它们，正如我们将在训练循环中看到的那样。</p><h1 id="946d" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">保存并加载</h1><p id="d551" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">像往常一样，我们希望有一些方法来保存和加载网络的权重和重放缓冲区:</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="cc06" class="lp jn hi ll b fi lq lr l ls lt"><em class="jd">def</em> save(self):<br/>        date_now = time.strftime("%Y%m%d%H%M")<br/>        if not os.path.isdir(f"{self.path_save}/save_agent_{date_now}"):<br/>            os.makedirs(f"{self.path_save}/save_agent_{date_now}")<br/>        self.actor.save_weights(f"{self.path_save}/save_agent_{date_now}/{self.actor.net_name}.h5")<br/>        self.target_actor.save_weights(f"{self.path_save}/save_agent_{date_now}/{self.target_actor.net_name}.h5")<br/>        self.critic.save_weights(f"{self.path_save}/save_agent_{date_now}/{self.critic.net_name}.h5")<br/>        self.target_critic.save_weights(f"{self.path_save}/save_agent_{date_now}/{self.target_critic.net_name}.h5")<br/>        <br/>        np.save(f"{self.path_save}/save_agent_{date_now}/noise.npy", self.noise)<br/>        <br/>        self.replay_buffer.save(f"{self.path_save}/save_agent_{date_now}")</span><span id="d168" class="lp jn hi ll b fi lu lr l ls lt">    <em class="jd">def</em> load(self):<br/>        self.actor.load_weights(f"{self.path_load}/{self.actor.net_name}.h5")<br/>        self.target_actor.load_weights(f"{self.path_load}/{self.target_actor.net_name}.h5")<br/>        self.critic.load_weights(f"{self.path_load}/{self.critic.net_name}.h5")<br/>        self.target_critic.load_weights(f"{self.path_load}/{self.target_critic.net_name}.h5")<br/>        <br/>        self.noise = np.load(f"{self.path_load}/noise.npy")<br/>        <br/>        self.replay_buffer.load(f"{self.path_load}")</span></pre><p id="85c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里没什么特别的，但是你可以注意到，我们也保存了我们现在要讨论的噪声变量。</p><h1 id="1126" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">探索与开发</h1><p id="c4f7" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">我们需要为确定性政策付出的代价是，我们需要一些探索环境的策略。最简单的方法是给演员返回的动作添加一些噪声。例如，我们可以从正态分布中随机抽取噪声。但在最初的论文中，他们使用了奥恩斯坦-乌伦贝克过程，所以我们将以同样的方式实现它:</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="39a1" class="lp jn hi ll b fi lq lr l ls lt"><em class="jd">def</em> _ornstein_uhlenbeck_process(self, x, theta=THETA, mu=0, dt=DT, std=0.2):<br/>        """<br/>        Ornstein–Uhlenbeck process<br/>        """<br/>        return x + theta * (mu-x) * dt + std * np.sqrt(dt) * np.random.normal(size=self.actions_dim)</span><span id="d47e" class="lp jn hi ll b fi lu lr l ls lt">    <em class="jd">def</em> get_action(self, observation, noise, evaluation=False):<br/>        state = tf.convert_to_tensor([observation], dtype=tf.float32)<br/>        actions = self.actor(state)<br/>        if not evaluation:<br/>            self.noise = self._ornstein_uhlenbeck_process(noise)<br/>            actions += self.noise</span><span id="87e3" class="lp jn hi ll b fi lu lr l ls lt">        actions = tf.clip_by_value(actions, self.lower_bound, self.upper_bound)</span><span id="f044" class="lp jn hi ll b fi lu lr l ls lt">        return actions[0]</span></pre><p id="e702" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于奥恩斯坦-乌伦贝克过程，我只是用了一个公式，你可以很容易地通过谷歌搜索找到它。接下来，在get_action方法中，我们使用噪声在动作值中创建随机性。最后，我们使用下限和上限来裁剪动作的值。</p><h1 id="47d0" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">损失函数和梯度</h1><p id="c3aa" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">最后，我们只需要使用从损失函数计算的梯度来更新演员和评论家的权重。</p><p id="008b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">批评家损失是目标网络看到的预期回报和批评家网络预测的Q值之间的均方误差。</p><p id="3cb2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定演员网络所采取的行动，演员损失是评论家网络价值的平均值。我们想要最大化这个值(我们想要更高的Q值)，所以我们使用负号将其转换为损失函数。</p><p id="c9e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看代码:</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="dad1" class="lp jn hi ll b fi lq lr l ls lt"><em class="jd">def</em> learn(self):<br/>        if self.replay_buffer.check_buffer_size() == False:<br/>            return</span><span id="86c2" class="lp jn hi ll b fi lu lr l ls lt">        state, action, reward, new_state, done = self.replay_buffer.get_minibatch()</span><span id="9512" class="lp jn hi ll b fi lu lr l ls lt">        states = tf.convert_to_tensor(state, dtype=tf.float32)<br/>        new_states = tf.convert_to_tensor(new_state, dtype=tf.float32)<br/>        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)<br/>        actions = tf.convert_to_tensor(action, dtype=tf.float32)</span><span id="a0cc" class="lp jn hi ll b fi lu lr l ls lt">        with tf.GradientTape() as tape:<br/>            target_actions = self.target_actor(new_states)<br/>            target_critic_values = tf.squeeze(self.target_critic(<br/>                                new_states, target_actions), 1)<br/>            critic_value = tf.squeeze(self.critic(states, actions), 1)<br/>            target = reward + self.gamma * target_critic_values * (1-done)<br/>            critic_loss = tf.keras.losses.MSE(target, critic_value)</span><span id="2179" class="lp jn hi ll b fi lu lr l ls lt">        critic_gradient = tape.gradient(critic_loss,<br/>                                            self.critic.trainable_variables)<br/>        self.critic.optimizer.apply_gradients(<em class="jd">zip</em>(<br/>            critic_gradient, self.critic.trainable_variables))</span><span id="a753" class="lp jn hi ll b fi lu lr l ls lt">        with tf.GradientTape() as tape:<br/>            policy_actions = self.actor(states)<br/>            actor_loss = -self.critic(states, policy_actions)<br/>            actor_loss = tf.math.reduce_mean(actor_loss)</span><span id="497f" class="lp jn hi ll b fi lu lr l ls lt">        actor_gradient = tape.gradient(actor_loss, <br/>                                    self.actor.trainable_variables)<br/>        self.actor.optimizer.apply_gradients(<em class="jd">zip</em>(<br/>            actor_gradient, self.actor.trainable_variables))</span><span id="280f" class="lp jn hi ll b fi lu lr l ls lt">        self.update_target_networks(self.tau)</span></pre><p id="a9dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们使用ReplayBuffer类的check_buffer_size()方法检查缓冲区大小是否大于批处理大小和配置脚本中定义的最小大小。然后，我们从重播缓冲器中取出一个小批次，并开始如上所述计算两个损失，即我们应用于演员和评论家网络以更新权重的梯度。最后，我们使用方法update_target_networks“软”更新目标网络的权重。</p><h1 id="8b66" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">培训和结果</h1><p id="f4b9" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">我们现在可以转到训练循环和两种环境的结果。</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="ae4b" class="lp jn hi ll b fi lq lr l ls lt">config = <em class="jd">dict</em>(<br/>  learning_rate_actor = ACTOR_LR,<br/>  learning_rate_critic = ACTOR_LR,<br/>  batch_size = BATCH_SIZE,<br/>  architecture = "DDPG",<br/>  infra = "Ubuntu",<br/>  env = ENV_NAME<br/>)</span><span id="b3dc" class="lp jn hi ll b fi lu lr l ls lt">wandb.init(<br/>  project=f"tensorflow2_ddpg_{ENV_NAME.lower()}",<br/>  tags=["DDPG", "FCL", "RL"],<br/>  config=config,<br/>)</span><span id="35de" class="lp jn hi ll b fi lu lr l ls lt">env = gym.make(ENV_NAME)<br/>agent = Agent(env)</span><span id="9bb4" class="lp jn hi ll b fi lu lr l ls lt">scores = []<br/>evaluation = True</span><span id="f5b8" class="lp jn hi ll b fi lu lr l ls lt">if PATH_LOAD is not None:<br/>    print("loading weights")<br/>    observation = env.reset()<br/>    action = agent.actor(observation[None, :])<br/>    agent.target_actor(observation[None, :])<br/>    agent.critic(observation[None, :], action)<br/>    agent.target_critic(observation[None, :], action)<br/>    agent.load()<br/>    print(agent.replay_buffer.buffer_counter)<br/>    print(agent.replay_buffer.n_games)<br/>    print(agent.noise)</span></pre><p id="99e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像往常一样，我们开始为日志配置wandb，如果我们正在加载一个以前训练过的代理，我们将加载代理和重放缓冲区。在加载所有内容之前，我们需要调用四个网络来构建它们，因此我们将使用一个随机的初始观察来初始化它们。</p><p id="c5ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我们有了训练循环:</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="22ab" class="lp jn hi ll b fi lq lr l ls lt">for _ in tqdm(<em class="jd">range</em>(MAX_GAMES)):<br/>    start_time = time.time()<br/>    states = env.reset()<br/>    done = False<br/>    score = 0<br/>    while not done:<br/>        action = agent.get_action(states, evaluation)<br/>        new_states, reward, done, info = env.step(action)<br/>        score += reward<br/>        agent.add_to_replay_buffer(states, action, reward, new_states, done)<br/>        agent.learn()<br/>        states = new_states<br/>        <br/>    agent.replay_buffer.update_n_games()<br/>    <br/>    scores.append(score)<br/>    </span><span id="05cb" class="lp jn hi ll b fi lu lr l ls lt">    wandb.log({'Game number': agent.replay_buffer.n_games, '# Episodes': agent.replay_buffer.buffer_counter, <br/>               "Average reward": <em class="jd">round</em>(np.mean(scores[-10:]), 2), \<br/>                      "Time taken": <em class="jd">round</em>(time.time() - start_time, 2)})</span><span id="cc70" class="lp jn hi ll b fi lu lr l ls lt">    if (_ + 1) % EVALUATION_FREQUENCY == 0:<br/>        evaluation = True<br/>        states = env.reset()<br/>        done = False<br/>        score = 0<br/>        while not done:<br/>            action = agent.get_action(states, evaluation)<br/>            new_states, reward, done, info = env.step(action)<br/>            score += reward<br/>            states = new_states<br/>        wandb.log({'Game number': agent.replay_buffer.n_games, <br/>                   '# Episodes': agent.replay_buffer.buffer_counter, <br/>                   'Evaluation score': score})<br/>        evaluation = False<br/>     <br/>    if (_ + 1) % SAVE_FREQUENCY == 0:<br/>        print("saving...")<br/>        agent.save()<br/>        print("saved")</span></pre><p id="7ec2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在训练循环中，我们与环境进行交互，我们将状态、动作、奖励和终端标志保存在重放缓冲区中，并使用之前看到的learn()方法训练代理。当这一集结束时，我们评估结果，记录最后十场比赛的平均值。我们还记录每100场游戏的评估分数，其中我们简单地不将噪声添加到动作中，并且每200场游戏，我们保存网络的权重、噪声和重放缓冲。</p><p id="4740" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以开始看钟摆结果，记住<a class="ae kp" href="https://gym.openai.com/evaluations/eval_GH4gO123SvukqlMmqhIyeQ/" rel="noopener ugc nofollow" target="_blank"> OpenAI文档</a>说“最好的100集平均奖励是-138.98±9.06。(钟摆-v0没有一个特定的奖励阈值，在这个阈值上它被认为是解决了。)".</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lw"><img src="../Images/b973e86c7bce85f6197a5e16d942742e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/0*KRIK9xNDLuJTDVVl"/></div></figure><p id="1f54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如你所见，算法收敛非常快。在仅仅300场比赛之后，我们有相似的表现，如果不比文档中的那些更好的话。</p><p id="b76d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们看完了月球着陆器的结果；在这种情况下，<a class="ae kp" href="https://github.com/openai/gym/wiki/Leaderboard#lunarlander-v2" rel="noopener ugc nofollow" target="_blank">文档</a>说“LunarLander-v2将“解决”定义为在100次连续试验中获得平均200英镑的奖励。”。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lx"><img src="../Images/e8cdb0289fe3ddb51233c3c672557ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*CT784b-ZzYTGIEEA"/></div></figure><p id="0034" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们花了更多的时间来获得好的结果。在将近150，000集之后，我们得到了平均200分。但是这集真的很短，所以训练时间不多，大约一个小时我们就解决了环境问题。</p><p id="8b32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在我的<a class="ae kp" href="https://antonai.blog/reinforcement-learning-in-continuous-action-spaces-part-1-ddpg/" rel="noopener ugc nofollow" target="_blank">博客</a>上找到原文，在我的<a class="ae kp" href="https://github.com/antonai91/reinforcement_learning/tree/master/ddpg" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到所有代码。有任何问题，可以通过<a class="ae kp" href="https://www.linkedin.com/in/lisiantonio/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我。</p><p id="96c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你喜欢这篇文章，分享给你的朋友和同事吧！我会在下一篇文章中看到你。与此同时，要小心，保持安全，记住<em class="jd">不要成为另一块墙砖</em>。</p><p id="a203" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Anton.ai</p></div></div>    
</body>
</html>