<html>
<head>
<title>RNN with Attention Unit Testing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">注意单元测试的RNN</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/rnn-with-attention-unit-testing-825b2963d31e?source=collection_archive---------23-----------------------#2021-01-02">https://medium.com/analytics-vidhya/rnn-with-attention-unit-testing-825b2963d31e?source=collection_archive---------23-----------------------#2021-01-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="56a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我写过一篇<strong class="ig hi"> Transformer(自我关注)</strong>机制的教程。(<a class="ae jc" href="https://celikkam.medium.com/nlp-transformer-unit-test-95459fefbea9" rel="noopener">链接</a>)那个教程从注意力机制开始似乎有点难，所以我决定做一个更简单版本的<strong class="ig hi">注意力</strong>对<strong class="ig hi"> RNN </strong>。标题有点误导:)我只是想写一个非常小的，可测试的和类似单元测试的东西，所以你可以测试网络的小部分。虽然标题有点误导，但我认为我创建了一个可测试的网络，你可以玩网络的不同部分，看看它对整体架构的影响。<br/>代码在github( <a class="ae jc" href="https://github.com/mcelikkaya/medium_articles/blob/main/rnnattention.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a> ) <br/>中，用于使用nbviewer ( <a class="ae jc" href="https://nbviewer.jupyter.org/github/mcelikkaya/medium_articles/blob/main/rnnattention.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>)查看彩色视图</p><p id="bb04" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我假设你已经知道<strong class="ig hi">编码器-解码器</strong>的架构，并对注意力机制有所了解。为了更好地理解，我将只尝试解释、想象一些注意力方面。我将尝试用一个非常简单的数据集来形象化注意力的好处。</p><p id="23ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该数据集包含英语到德语的句子。这是一个平衡的集合，每个单词在它的组(主语、宾语、动词)中出现的频率相同。有一种“想”的模式，导致翻译长短不一。此外,“能”和“想要”模式在翻译中导致不同的顺序，因此翻译也必须了解位置。还有简单的句型“我吃苹果”和情态动词，生成不同的(动词、宾语)顺序。即使你不懂德语，单词也很简单。你可以查下表，看看哪个单词对应哪个英语单词。此外，句子非常简单，不完全正确(丢失的文章…)事实上，你可以改变第二栏，把你的语言和尝试。(还必须将笔记本中的德语单词换成您的语言中的单词。)</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="ji jj l"/></div></figure><p id="1aac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在RNN编码器-解码器中(无需关注)，我们创建编码器状态的摘要，并将此作为初始状态提供给解码器。解码器使用这个初始状态，在第一步产生输出字。然后在下一步，使用解码器最后状态，并丢弃编码器状态。如果我们不仅仅总结和创建1个向量(在编码过程结束时)，而是在解码过程的所有步骤中使用所有编码器状态(编码过程中的所有步骤)，会发生什么？这会提高我们网络的建模能力吗？</p><p id="c1d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">编码过程从源句子的第一个单词开始，并为以“eos”结尾的输入句子创建向量。【sos，我，能，吃，苹果，eos】。这个过程有5个步骤。这里我们给编码器这5个向量，创建5个输出向量。在<strong class="ig hi">第8行</strong>你可以看到循环，它在每一步创建编码器输出向量。(查看tensorFromSentence方法以了解输入准备)</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="ji jj l"/></div></figure><p id="5b54" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们完成编码过程，我们有5个矢量的5个编码步骤。现在我们想学习这些在目标语言(德语)中的表示。在上面的代码行18，我们开始一次创建一个输出向量。正如你在第20行看到的，"<strong class="ig hi">编码器_输出</strong>"向量在这个过程中总是一个参数。<br/>为什么我们在所有步骤中都使用这个输入，而不是在初始步骤。语言有不同的顺序。因此，我们可以假设源语言的5个步骤在解码所有状态时并不同等重要。所以最好把这些都交给解码器，让它学习这种潜在的关系。</p><p id="aa88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我想我给出了100个句子，形式为"<strong class="ig hi"> I can动词宾语</strong>"英语句子，其中将有一个翻译为"<strong class="ig hi"> Ich konnen宾语动词</strong>"。正如你在德语中看到的，动词和宾语的顺序是变化的。当我们通过训练重复这个编码-解码过程时，我们的网络就学会了这种交换。怎么会？因为在每个时期，我们都会计算损耗，并更新我们的权重，因此我们的网络层权重会进行这种交换计算。</p><p id="be43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你检查解码器代码，我们正在学习一种带有torch.nn.Embedding的编码，并注意创建一个。并将这2 ^ 2个向量与“attention_combine”，线性层(第11行)组合。所以解码器使用两个数据源并一起学习它们。所以注意力向量并不是网络唯一使用的东西，对此要小心。有时你可以看到网络把很多注意力放在错误的单词上，但仍然做出正确的猜测。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="ji jj l"/></div></figure><p id="466a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">检查第9行，有一个创建矢量的torch . nn . embedding层。NLP问题中的向量是输入的多维表示。如果你想写出一个单词的所有语言属性，你需要几千列。苹果-&gt;“红色、可食用、水果、颜色(红、黄、绿)、形状、质地、神话、科学……”。因此，实际上，NLP问题是对这种有限资源的无限表示的近似。所以当你为一个<strong class="ig hi"> torch.nn.Embedding </strong>层定义维度(256)时，实际上就是用一个256的维度来表达这个无穷无尽的矩阵。我们对网络所做的是，用我们的数据来近似这个维度上的真实值。</p></div><div class="ab cl jk jl go jm" role="separator"><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp"/></div><div class="ha hb hc hd he"><h1 id="8d93" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">注意与否</h1><p id="4228" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">正如我上面展示的，解码器有复杂的(没有那么多)架构来计算输出。而注意力只是其中的一部分。我可以写一个测试代码来展示注意力的重要性吗？假设我们有两个句子。<br/><strong class="ig hi">-我们吃苹果<br/>-我们吃面包</strong> <br/>我们会为【我们，吃，苹果，面包】生成向量，为句子生成向量。我们在RNN注意到的是，利用编码过程的步骤，改变解码步骤的重要性。所以我会消除这种注意力变化的重要性，把所有的步骤都视为同等重要。如果你认为，这是我能做的测试效果的最小改变。为了展示这一点的重要性，我将把这个过程创建的矢量转储到2d上。</p><p id="9c6c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我有一个方法叫做translate_info，它展示了翻译的逻辑性和自我关注。<strong class="ig hi"> Logits </strong>是Softmax之前分类模型的原始(非标准化)分数。在这个笔记本中，方法是LogSoftmax，所以要小心，值都是负数。越接近0，越好。(思考对数图)</p><p id="fcbd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">翻译是从<strong class="ig hi"> </strong>输入<strong class="ig hi">【SOS】</strong>生成下一个单词，直到生成输出<strong class="ig hi">【EOS】</strong>的过程(或者到我们尝试的最大长度)。所以当你看到下图时，它显示了每个翻译步骤的逻辑和注意事项。所以如果输入句子是<strong class="ig hi">“我能吃苹果”</strong>，翻译过程有5个步骤，<strong class="ig hi">“ich konnen apfel essen&lt;EOS&gt;”</strong>。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ku"><img src="../Images/55d90ec97b8246481e1eaa6e6be90bad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DHljP2-UoHSYn568La_Acg.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx translated"><strong class="bd jt">逻辑和注意事项</strong></figcaption></figure><p id="19fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">查看表格第一行。</p><p id="9d39" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> "1)sos- &gt; wir" </strong>显示当前预测步骤，“sos”是当前单词“ich”将在该步骤结束时生成的单词，颜色显示最可能的列。在其他教程中他们写“<strong class="ig hi"> sos </strong>”，但我认为这种方式更好。</p><p id="2574" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果您检查<strong class="ig hi">,</strong>的得分为0.0，概率最高。检查其他步骤以更好地理解。你仍然可以认为，即使所有的值都是负的，最接近0的值是最好的。上面的热图显示了对所有步骤的关注。在翻译步骤1，注意力在<strong class="ig hi">“我们”</strong>，在步骤2，注意力在<strong class="ig hi">“能”</strong>。检查每一步，并理解网络如何在每一步运用注意力。由于我们在这一步生成了<strong class="ig hi"> "we" </strong>，在第二行u见<strong class="ig hi"> "2)wir- &gt; konnen" </strong>，这意味着第二步解码器已经生成了【sos，ich】并将生成“konnen”。(如果运行notebook，可能会有不同的结果，因为数据集很小，有时网络可能会过度适应，或者因为数据非常小，它可以在不注意的情况下学习生成正确的输出。)</p></div><div class="ab cl jk jl go jm" role="separator"><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp"/></div><div class="ha hb hc hd he"><h1 id="9c02" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">可视化解码器输出</h1><p id="b286" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">AttnDecoderRNN处有一个名为<strong class="ig hi">“output”</strong>的向量。我在翻译过程的每一步都保存这个向量。我将用两种方式生成它。第一，我将应用RNN编解码器注意(默认方式)。第二种方法，我会告诉网络不要使用学习过的向量，只是对所有的编码句子使用相同的注意力。我将努力展示这一变化所带来的不同。字面意思是，认为在所有的输出步骤中，所有的输入状态都是同等重要的。</p><p id="1613" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">还有我一定要小心，不要<strong class="ig hi">去“拿橘子和苹果比”</strong>。例如，如果我在第一步检查输出向量，网络必须正常生成<strong class="ig hi">【ich】</strong>或<strong class="ig hi">【wir】</strong>。我的第二个网络可以生成不同的东西，如果翻译如此糟糕，但我只会选择那些创建了<strong class="ig hi">【ich】</strong>或<strong class="ig hi">【wir】</strong>的网络。我的意思是，即使没有适当的注意，对于一些句子，网络仍然会在第一步产生<strong class="ig hi">【ich】</strong>。所以我要调查一下这个<strong class="ig hi">【ich】</strong>和另一个<strong class="ig hi">【ich】</strong>是否不同。所以我比较苹果和苹果，橘子和橘子。</p><p id="74d0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我给你看一个逻辑和注意力的例子，用“平均分配注意力”来说明我的意思。在下图中，您可以看到注意力热图完全是黑色，因为注意力在任何地方都是一样的。由于网络逐步学习注意力，当注意力相等时，网络不知道根据当前状态它必须产生什么。“essen -&gt; apfel -&gt; essen”序列实际上是我们训练集中多个模式的组合。网络不知道翻译步骤的确切位置。当它生成足够多的“<strong class="ig hi"> apfel </strong>”、“<strong class="ig hi">埃森</strong>”时，它就能明白它必须生成“<strong class="ig hi"> eos </strong>”。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lf"><img src="../Images/e1f8347059c4dca56bedd0e46c0d284a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SOKY5LYgIV51rS_FUIpCqg.png"/></div></div></figure><p id="edc7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是收集解码过程输出的方法。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="ji jj l"/></div></figure><p id="e716" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上面，我得到所有的句子(第2行)，我用<strong class="ig hi"> attention_type </strong>输入(适当地或同等地注意)翻译它们(第7行)，然后我收集“AtnnDecoderRNN”层的“<strong class="ig hi">输出</strong>向量。我检查输出长度(第9行)和filter_words(第10行)，如果它们符合我的期望，就保留结果。</p><p id="764b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下面你可以看到我是如何调用这个方法的。我给“word_index_target”取0。因此，收集在翻译过程的第一步创造的单词。(第一步数组索引0)。可以看到“filter_words”是[ich，wir]。意思是如果这个步骤产生了"<strong class="ig hi"> ich </strong>或"<strong class="ig hi"> wir </strong>"保留它们。<br/>我把同样的方法调用了两次，第一次用“ATTENTION_DEFAULT”，正常的方式，第二次用“ATTENTION_EQUAL_DISTRIBUTED”。在下图中，通过一个学习注意力的RNN，你可以看到生成的向量，如左边的<strong class="ig hi"> "ich" </strong>，右边的<strong class="ig hi"> "wir" </strong>。(我使用英语源句子作为标签，这给出了更多的想法)<br/>你也可以看到这些是根据句子的动词分组的。现在你可以看到我们的网络是如何成功区分<strong class="ig hi">【ich】</strong>向量和<strong class="ig hi">【wir】</strong>向量的。也看到吃(蓝)、读(红)、喝(绿)是分开的。我们的网络也知道如何通过动词区分“ich”向量。在一个好的NLP模型中，<strong class="ig hi">“我”</strong>，<strong class="ig hi">我读</strong>，<strong class="ig hi">我吃</strong>一定是不一样的。这就是人类和机器捕捉上下文的方式。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lg"><img src="../Images/b8cf946d9f2194132b62baef13179034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xTz6LPlNizSmfSqJedQN3g.png"/></div></div></figure><p id="e617" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们看看，如果我们给予同等的关注，一代人会发生怎样的变化。在下图中，我将正确的项目(上图)和不正确的项目(同等关注)放在同一张图中。不合适的用粉色表示。如你所见，粉色物品散落在周围。</p><p id="c314" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们口头陈述下图:</p><blockquote class="lh li lj"><p id="832a" class="ie if lk ig b ih ii ij ik il im in io ll iq ir is lm iu iv iw ln iy iz ja jb ha bi translated">没有适当的注意(运用同等的注意)<br/>即使我们创造了<br/>相同的向量【ich，wir】<br/>在同一个步骤(翻译的第一步)<br/>那些向量【ich，wir】不同于用注意创造的相同向量【ich，wir】<br/>。</p></blockquote><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lo"><img src="../Images/8865df8071b5606a58b5ce2ea4b2e69f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oe6lnZgrc-urW_XsEZhoow.png"/></div></div></figure><p id="decc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在翻译步骤2，网络生成[konnen，mochten](对于<strong class="ig hi"> can </strong>和<strong class="ig hi"> want </strong>句子，我过滤掉类似“我吃苹果”的模式)。在下图中你可以看到，左边是“<strong class="ig hi"> mochten </strong>”，右边是“<strong class="ig hi"> konnen </strong>”。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lp"><img src="../Images/df6bd30ca209824e49ac8c1b4c090981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*crjR4N5479KMAGYg7MZHng.png"/></div></div></figure><p id="3653" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，当我们添加注意平均分布的句子时，(带有粉红色的项目)你可以看到它们是分散的。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lq"><img src="../Images/78c4b1920a9094627b2097c94cd2aca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZYWJNAv9xpJJtuJPfQlCcA.png"/></div></div></figure><p id="4c7b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在翻译步骤3，网络正在生成对象。在下面，你可以看到一簇簇的物体。子集群向我们展示了我们的模型也捕捉到了意义。由于能(能力)、要(意志)和裸动作不同，它们为同一物体占据2d空间的不同部分。这个小样本展示了机器是如何理解语言的。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lr"><img src="../Images/8411e8a3a23a79296d91a9d276a8a4dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cSqp931UH2BSYpw-OW9oyw.png"/></div></div></figure><p id="a5dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们以同样的注意力添加句子时，我们可以看到这些句子是分散的。(下面的粉色项目)</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ls"><img src="../Images/38f11a0c3a318c66d19f3dabf7a3ba91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tYDYoeH65EuL1Lj43mOSFw.png"/></div></div></figure><p id="cc7c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这里，我试图通过一些形象化的方法来展示注意力机制的好处。<br/> <strong class="ig hi"> <br/> </strong>在一个NLP问题中，我们的意图是捕捉<strong class="ig hi">上下文</strong>。因此，当我们通过解码器步骤传播编码器状态时，我们在每一步都有源的摘要。这实现了一个学习阶段，以便在每一步进行最佳编码。</p></div></div>    
</body>
</html>