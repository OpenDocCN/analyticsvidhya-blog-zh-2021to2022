<html>
<head>
<title>Optimizers for machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习优化器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/optimizers-for-machine-learning-302323e4e896?source=collection_archive---------16-----------------------#2021-06-30">https://medium.com/analytics-vidhya/optimizers-for-machine-learning-302323e4e896?source=collection_archive---------16-----------------------#2021-06-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="b3f7" class="if ig hh bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="d3b9" class="pw-post-body-paragraph jd je hh jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated">在这篇文章中，我们将学习优化器，这是机器学习中最重要的部分，在这篇博客中，我试图用简单的术语和可视化的方式解释优化器的每一个概念，以便初学者也能理解。</p><p id="086c" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">这个博客仅限于机器学习，看完博客，你就会知道优化器是如何工作的。</p><h2 id="0659" class="kg ig hh bd ih kh ki kj il kk kl km ip jo kn ko it js kp kq ix jw kr ks jb kt bi translated">我们将要学习的内容</h2><ol class=""><li id="3728" class="ku kv hh jf b jg jh jk jl jo kw js kx jw ky ka kz la lb lc bi translated">什么是优化</li><li id="57b2" class="ku kv hh jf b jg ld jk le jo lf js lg jw lh ka kz la lb lc bi translated">凸函数和非凸函数</li><li id="d2b0" class="ku kv hh jf b jg ld jk le jo lf js lg jw lh ka kz la lb lc bi translated">优化者</li><li id="d41f" class="ku kv hh jf b jg ld jk le jo lf js lg jw lh ka kz la lb lc bi translated">基础微积分</li><li id="a7af" class="ku kv hh jf b jg ld jk le jo lf js lg jw lh ka kz la lb lc bi translated">梯度下降(批量梯度下降)</li><li id="c00a" class="ku kv hh jf b jg ld jk le jo lf js lg jw lh ka kz la lb lc bi translated">梯度下降的缺点</li><li id="3004" class="ku kv hh jf b jg ld jk le jo lf js lg jw lh ka kz la lb lc bi translated">随机梯度下降</li><li id="8ad6" class="ku kv hh jf b jg ld jk le jo lf js lg jw lh ka kz la lb lc bi translated">小批量梯度下降</li><li id="6106" class="ku kv hh jf b jg ld jk le jo lf js lg jw lh ka kz la lb lc bi translated">执行优化时的挑战</li></ol><h2 id="b51e" class="kg ig hh bd ih kh ki kj il kk kl km ip jo kn ko it js kp kq ix jw kr ks jb kt bi translated">最佳化</h2><p id="023e" class="pw-post-body-paragraph jd je hh jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated">首先，我们必须知道什么是最优化，用数学术语来说，寻找任何非线性函数的全局最小值或最大值被称为最优化。这里寻找最小值和最大值是一个特定的问题</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/6033996680f16bcd1f2e9c9e0a2a797c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*V3mmIIZQ_j7sOY6F5Nvapw.gif"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">你可以看到上面的函数在-1时最大，在1时最小。最优化就是找到x的值，其中函数f(x)将是最小值或最大值</figcaption></figure><h1 id="dcd3" class="if ig hh bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">凸函数和非凸函数</strong></h1><p id="d22c" class="pw-post-body-paragraph jd je hh jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated">在机器学习和深度学习中，我们经常会遇到这种类型的函数。如果我们在一个函数的图上取任意两个点，并在这两个点之间画一条线段(最短路径)，如果这条线段在一个函数的图上，这类函数我们称之为凸函数，对于凸函数，任意两点之间的最短路径应该在图的上面，否则它是非凸函数。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es lu"><img src="../Images/949d0d7b1c282b0764551504a31e2a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*h--8ZKSA-PqmhXud.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">左边的函数f(x)你可以看到两点之间的线段在图的上方，右边的函数f(x)两点之间的线段不完全在图的上方，因此左边的函数f(x)是凸函数，右边的函数f(x)是非凸函数</figcaption></figure><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es lz"><img src="../Images/61f2187030a34057f9a01838426ae849.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*_us5NXZAC8_VU46qaQbixA.png"/></div></figure><p id="a2ed" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">一个函数可以有多个最小值和最大值。包括端点的函数的绝对最高点和最低点称为全局最大值和全局最小值，全局最大值和全局最小值的邻域称为局部最大值和局部最小值。</p><p id="ce17" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">对于凸函数，我们只有一个全局最小值或最大值，没有局部最小值或最大值，但在非凸函数中，我们可以看到局部和全局最小值和最大值。在深度学习中，我们遇到了更复杂的非凸函数</p><h1 id="714f" class="if ig hh bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">优化师</strong></h1><p id="cf9e" class="pw-post-body-paragraph jd je hh jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated">有一些算法可以帮助找到一个函数的全局最小值。</p><p id="2bc6" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">在这里，为什么我们应该找到全局最小值，而不是全局最大值？</p><p id="ce54" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">在机器学习和深度学习中，我们的大多数成本函数都是以这样一种方式定义的，即为了找到最小值，这只是为了我们的方便，这使得数学变得非常容易，例如以线性回归为例，我们必须最小化(最小值)成本函数∑ (yᵢ- h(w*xᵢ+ b))，如果你愿意，你可以以这样的方式改变成本函数，以便找到全局最大值，然后必须修改下面的算法来解决你的问题。</p><p id="7ac8" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">然而，让我们看看有哪些算法有助于任何函数的优化(全局最小值)。</p><ul class=""><li id="4cb9" class="ku kv hh jf b jg kb jk kc jo ma js mb jw mc ka md la lb lc bi translated">梯度下降(批量梯度下降)</li><li id="9088" class="ku kv hh jf b jg ld jk le jo lf js lg jw lh ka md la lb lc bi translated">随机梯度下降</li><li id="8347" class="ku kv hh jf b jg ld jk le jo lf js lg jw lh ka md la lb lc bi translated">小批量梯度下降</li></ul><h1 id="4db6" class="if ig hh bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">基础微积分</strong></h1><p id="abd0" class="pw-post-body-paragraph jd je hh jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated">在进入优化之前，我们必须知道一些微积分的基础知识。</p><p id="7db7" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">在数学中，每一条线和每一个函数都有一个斜率，但是什么是斜率呢</p><ul class=""><li id="51a1" class="ku kv hh jf b jg kb jk kc jo ma js mb jw mc ka md la lb lc bi translated">y相对于x的变化率称为斜率</li><li id="96e6" class="ku kv hh jf b jg ld jk le jo lf js lg jw lh ka md la lb lc bi translated">假设x1变为x 1→x2，y1变为y1 →y2</li><li id="d9d4" class="ku kv hh jf b jg ld jk le jo lf js lg jw lh ka md la lb lc bi translated">那么斜率(m)=(y2-y1)/(x2-x1)</li><li id="1f3a" class="ku kv hh jf b jg ld jk le jo lf js lg jw lh ka md la lb lc bi translated">斜率通过x的变化告诉你y的变化</li></ul><h2 id="7d4a" class="kg ig hh bd ih kh ki kj il kk kl km ip jo kn ko it js kp kq ix jw kr ks jb kt bi translated">让我们回忆一下在学校里学过的斜坡基础知识</h2><p id="6916" class="pw-post-body-paragraph jd je hh jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated">→我们如何求直线的斜率？</p><ul class=""><li id="18cc" class="ku kv hh jf b jg kb jk kc jo ma js mb jw mc ka md la lb lc bi translated">这条线与x轴所成的角度称为斜率(m)</li></ul><blockquote class="me mf mg"><p id="a5db" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">其中m = tanθθ=角度</p></blockquote><p id="fe53" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">→如果已知直线上的两点(x1，y1)，(x2，y2)，另一种求直线斜率的方法</p><blockquote class="me mf mg"><p id="3c81" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">那么斜率(m)=(y2-y1)/(x2–x1)</p></blockquote><p id="6b5c" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">但是当涉及到连续函数时，斜率在点与点之间变化，我们不能使用上面的公式，我们必须找到另一种方法来寻找连续函数的斜率。如果你不知道什么是连续函数，我建议你去维基百科或者可汗学院。</p><p id="8075" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">让我们来讨论如何在任意特定点找到连续函数的斜率，看这里，对于每个连续函数，我们在特定点画一条切线，在这里我们要找到斜率或导数，不要担心导数是微积分中用来代替斜率的术语，切线与x轴所成的角度称为特定点的斜率或导数，我们也可以用极限来定义导数， 假设我们有函数f(c)当c变为c + h时，函数f(c)的值也变为f(c + h)</p><blockquote class="me mf mg"><p id="8d2c" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">c →c + h，f(c) →f(c + h)</p><p id="adce" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">那么斜率=f(c + h)-f(c)/(c + h-c)(我们的基本斜率公式(x2–x1)/(y2–y1))</p><p id="6aa0" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">其中斜率=(f(c + h)-f(c))/h</p></blockquote><p id="8ed9" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">如果h的极限接近于零而不是零，意味着h变得很小，我们可以假设为0.0001或任何点的值</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es ml"><img src="../Images/55b8162c581e5d3eddd57acf4824598e.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/0*fJ66UVKA5ifSE8m6.gif"/></div></figure><blockquote class="me mf mg"><p id="a95e" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">为了更清楚地理解，如果我们使用我们原来的斜率公式</p><p id="0941" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">(y2-y1)/(x2-x1) — -(1)</p><p id="e55a" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">假设h=0.0001，斜率=f( c + h )-f(c)/c + h-c</p><p id="1eab" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">斜率= f(c+h)-f(c)/h = f(c+0.0001)-f(c)/0.0001—--(2)</p><p id="7925" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">比较(1)和(2)</p><p id="690f" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">x2-x1 = 0.0001，y2-y1=f(c + 0.0001)-f(c)，</p><p id="c9c4" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">在x轴上有点变化，所以在y轴上也会有点变化</p></blockquote><p id="ebe2" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">(<strong class="jf hi"><em class="mh">c-&gt;c+0.01中的小变化(点变化)函数变化很小f(c) - &gt; f(c+0.0001)，所以c+0.0001处的斜率几乎等于c </em> </strong>处的斜率)</p><p id="64ad" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">在上图中，你可以看到当h很小时，它几乎等于c点</p><blockquote class="me mf mg"><p id="5d22" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">因此c点的斜率(m)= f(c+h)-f(c)/h；其中限制h — -&gt; 0.0001或者可以取任意点的值</p><p id="e0d2" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">在微积分中，我们称斜率为导数(dy/dx)或梯度</p></blockquote><p id="2ab1" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">每次我们不用极限来计算函数的导数。为了快速计算函数的导数或斜率，我们必须知道一些基本的微分公式。我强烈建议，如果你不知道微分，请回忆一下我们在学校学过的微分基本公式，因为通过使用这些公式，我们可以直接计算函数在任意点的导数。</p><p id="4995" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated"><strong class="jf hi">梯度下降</strong></p><p id="19f5" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">这是所有优化器之父，大多数优化器在梯度下降中使用相同的逻辑，首先什么是梯度，梯度只不过是斜率，下降意味着向下移动，梯度下降背后的思想是</p><p id="bbcb" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated"><em class="mh">在该值处使用梯度向下移动该值，直到梯度变为零</em></p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mm"><img src="../Images/55c591505da369e689a3b1ba0bb00b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*msTEfG7IxwIJC1qo4NQP-A.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">图(1)</figcaption></figure><p id="940a" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">让我用图中的一个例子来解释，我们可以看到图(1)中有一个凸函数，现在我们的座右铭是找到损失函数最小的“w ”,从图(1)中我们知道损失函数在w₁₀最小，但在实时情况下，我们很难想象在“w”的哪个值下函数会最小，因为每个函数的行为都不同。通过使用梯度下降，我们可以很容易地找到函数的最小值，而不需要任何可视化，一旦你理解它背后的数学，你就会知道它为什么工作，看看它是如何工作的</p><p id="1c98" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">让我们初始化任意随机值“w ”,假设随机值是上图中的w₁，在w₁，函数损失(l)没有最小化，我们必须将w₁向下移动到函数“l”最小的地方，但这里的问题是如何移动</p><blockquote class="mn"><p id="a9a7" class="mo mp hh bd mq mr ms mt mu mv mw ka dx translated">步骤:w_new=w_old - α * (dL/dw)</p></blockquote><p id="2689" class="pw-post-body-paragraph jd je hh jf b jg mx ji jj jk my jm jn jo mz jq jr js na ju jv jw nb jy jz ka ha bi translated">上面的公式用于移动权重‘w’，在上面的公式中‘w _ new’是我们必须移动的新步骤，其中w_old是当前的权重，这意味着w₁，关于权重w_old的函数的dL/dw导数，其中α是学习率</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es nc"><img src="../Images/7d5faae5bdf2621478a5754e8505f544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zcY9Y0aY3_eoJELJXzO1qQ.png"/></div></div></figure><p id="bb2d" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">从上图中，你可以看到导数或斜率的表现，当切线或直线平行于x轴时，其斜率为零，当直线向上和向下移动时，其斜率大于零且小于零，当计算函数在特定点的导数时，其遵循相同的表现。</p><p id="c862" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">我们正在计算基于切线的函数的导数，我们已经在上面的微积分基本概念中看到了，</p><p id="b506" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">在图(1)中，我们在图的左侧初始化权重w₁，因此在该点的切线将具有小于零的负斜率，因此函数w.r.t .对w₁的导数或斜率总是小于零，如果我们在函数的右侧初始化权重，则它的导数将大于零</p><blockquote class="mn"><p id="0443" class="mo mp hh bd mq mr ms mt mu mv mw ka dx translated">步骤:w_new = w_old - α * (dL/dw)</p></blockquote><p id="3d31" class="pw-post-body-paragraph jd je hh jf b jg mx ji jj jk my jm jn jo mz jq jr js na ju jv jw nb jy jz ka ha bi translated">仔细观察这里的游戏规则改变者是导数，让我们看看当我们在图(1)的左侧初始化时，导数将是负值。如果我们将负值放在上面的公式中，导数将通过乘以它前面的负变成正，然后它将变成</p><blockquote class="mn"><p id="820f" class="mo mp hh bd mq mr ms mt mu mv mw ka dx translated">w _ new = w _ old–α*(某个值)</p><p id="e3e4" class="mo mp hh bd mq mr ms mt mu mv mw ka dx translated">w_new = w_old + α *(某个值)</p></blockquote><p id="1172" class="pw-post-body-paragraph jd je hh jf b jg mx ji jj jk my jm jn jo mz jq jr js na ju jv jw nb jy jz ka ha bi translated">这里发生的事情你可以看到“w_old”正在增加，意味着向前移动。在图(1)中，你可以看到w_new =w3，w₁移动到w₃</p><p id="150a" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">如果我们再重复一次这一步，那么我们就移动到另一个值“w ”,随着向前移动，导数慢慢趋向于零</p><p id="e35a" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">当我们多次重复这个步骤时，w会更新(移动)，随着向前移动，导数慢慢趋向于零，在这里，每一步导数都会使w的步长变小(长步长到小步长)，因为随着向前移动，导数越来越接近零(减小)，在重复多个步骤后，导数变为零，如果导数变为零，这意味着w_old处的导数为零， 我们知道何时导数为零(切线平行于x轴)，这意味着权重w_old处切线的斜率为零，因此w_old是函数的最小值，当你再次重复一步时，它会停止下一步，然后w_new变成w_old，w_new= w_old</p><blockquote class="me mf mg"><p id="3a2a" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">w _ new = w _ old+α*(0)；dL/dw=0 → w_old斜率变为' 0 '</p><p id="7a65" class="jd je mh jf b jg kb ji jj jk kc jm jn mi kd jq jr mj ke ju jv mk kf jy jz ka ha bi translated">w _ new = w _ oldw_old变成w_new，它不能进一步更新(我们达到最小值)不能进一步更新，因为在达到最小值后,“w_old”处的导数将总是零</p></blockquote><p id="d914" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">我建议你通过初始化图表右侧的权重来尝试，这样你就可以知道步长方程是如何变化的，以及它将如何收敛</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es nd"><img src="../Images/11d1d3025f6802a777fd3997e0091d8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/0*Y0WNDfOY_Upf5NH2.jpeg"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">j(w)被称为我们的损失函数，正如我们在上面的图(1)中看到的</figcaption></figure><p id="8248" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">在这里，我们必须知道什么是α，它被称为学习率，在梯度下降步骤中，我们将导数或梯度乘以某个值，称为α，以某种速度移动“w ”,让我们看看当α很大时会发生什么</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es ne"><img src="../Images/014ef4678f31ce94882ff758bfc01981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LiQ5nR5u6h26XqQ1"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">这里可以看到，当α较大时，它有可能远离最小值，由于导数或梯度，它将开始在最小值附近振荡，如果α较小，它将需要更多时间才能达到最小值</figcaption></figure><p id="26bf" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">“α”是一个超参数，我们必须通过对不同的值进行实验来选择“α”的最佳值，通常大多数人在使用梯度下降时选择“α”为1，以最短的时间达到全局最小值，使α在每一步的上升中减少。首先，随着向前移动，步长将慢慢减小并达到最小值。这被称为调度</p><h2 id="8d4a" class="kg ig hh bd ih kh ki kj il kk kl km ip jo kn ko it js kp kq ix jw kr ks jb kt bi translated">梯度下降的缺点</h2><p id="bd4b" class="pw-post-body-paragraph jd je hh jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated">假设我们的数据集中有100万个数据点，我们必须使用所有的数据点来为“w”的每次更新寻找梯度(dL/dw ),为每一步计算100万个数据点在计算上非常昂贵，这是梯度下降的主要缺点。实际上，我们获得的数据将会非常庞大，以数十亿或数万亿计，这就是所谓的大数据。我们不能在大数据上使用梯度下降，如果我们的数据集很小，梯度下降可以很好地工作，但对于大数据，它的计算成本非常高</p><h2 id="425e" class="kg ig hh bd ih kh ki kj il kk kl km ip jo kn ko it js kp kq ix jw kr ks jb kt bi translated">随机梯度下降</h2><p id="52a5" class="pw-post-body-paragraph jd je hh jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated">我们知道，如果我们的数据集很大，使用梯度下降将导致计算复杂和昂贵的收敛到一个函数的最小值，这就是随机梯度下降的情况。SGD是梯度下降的近似，除了计算梯度之外，它使用与梯度下降相同的思想。在梯度下降法中，当更新权重“w”时，我们通过使用所有数据点计算每一步的梯度(导数),但是在随机梯度下降法中，不是使用所有数据点，而是取任意一个随机点，计算导数并更新“w”。</p><blockquote class="mn"><p id="8aeb" class="mo mp hh bd mq mr ms mt mu mv mw ka dx translated">w _ new = w _ old-α*(dL/dw)；dL/dw→基于任意一个随机数据点计算</p></blockquote><p id="13db" class="pw-post-body-paragraph jd je hh jf b jg mx ji jj jk my jm jn jo mz jq jr js na ju jv jw nb jy jz ka ha bi translated">dL/dw在每一步更新时，将从数据集中选取一个随机数据点，因为这个“w”更新将采取之字形运动</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es nf"><img src="../Images/933a8dc6a5bf00bbc5d5e45828bba9c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jlNIhhfa93bmc7qh.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">图(二)</figcaption></figure><p id="cb9c" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">当我们更新或移动“w”时，我们采用不同的数据点来计算导数或梯度，因此“w”每次更新时的导数将是不同的，这将使我们的“w”过度移动或采取小步骤，就像之字形步骤一样，因此我们可能会达到全局最小值或接近全局最小值，但与梯度下降相比，它需要更多的步骤(时期)</p><div class="lj lk ll lm fd ab cb"><figure class="ng ln nh ni nj nk nl paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/59017b6a11afc8218f901e5a9c71b706.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/0*TyrI0dF8HtmxGIAs.png"/></div></figure><figure class="ng ln nm ni nj nk nl paragraph-image"><img src="../Images/b7d55727d7365302f99bfe084d6ee6c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/0*--wjO5AUoOncK-sV.png"/><figcaption class="lq lr et er es ls lt bd b be z dx nn di no np translated">左边的是一个凸函数的三维图，如果我们从顶角看它，它看起来像圆，在这里你可以看到梯度下降和随机梯度下降是如何收敛到最小值的</figcaption></figure></div><p id="18fa" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">你可以怀疑，在每次权重更新时取一个随机点，它将如何收敛到最小值，仔细观察我们的目标是走向全局最小值，这里导数的符号将告诉哪一侧移动，导数或梯度将告诉步长， 在梯度下降的情况下，我们取所有数据点并计算梯度，因此它将采取一些长步骤并在较少的时期内达到最小值，在随机梯度下降的情况下，我们随机取一个数据点，因此在上升(步骤)中，导数将很小，并且之字形运动是因为数据点的随机性。</p><h2 id="0c21" class="kg ig hh bd ih kh ki kj il kk kl km ip jo kn ko it js kp kq ix jw kr ks jb kt bi translated">小批量梯度下降</h2><p id="1d92" class="pw-post-body-paragraph jd je hh jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated">小批量梯度下降与随机梯度下降相同，它不是在数据集中随机选取一个数据点来计算导数或梯度，而是在数据集中每次更新权重“w”时选取数据点子集，因此与SGD相比，它将具有较少的曲折运动</p><blockquote class="mn"><p id="2f0a" class="mo mp hh bd mq mr ms mt mu mv mw ka dx translated">w _ new = w _ old—α*(dL/dw)；dL/dw→计算数据点子集</p></blockquote><figure class="nr ns nt nu nv ln er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es nq"><img src="../Images/f42de7ea8f40a4338f3e0ad53398c7dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ejAuFKZebcw3jmTN.png"/></div></div></figure><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es nw"><img src="../Images/f6a475f6e765d5bcc9dc10708e861a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xLNgwi4r8m7CGwbk.png"/></div></div></figure><p id="5841" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">在上图中，您可以看到小批量梯度下降如何收敛到最小值，达到全局最小值需要更少的步骤，这是因为导数(dL/dw)，与随机梯度下降相比，导数将会很大，并且由于数据点的不同子集，它需要很大的曲折步骤，或者您可以像计算梯度一样考虑子集的随机性。而在SGD中，我们采用单个随机点，因此它将采取小的锯齿形步骤</p><h2 id="5634" class="kg ig hh bd ih kh ki kj il kk kl km ip jo kn ko it js kp kq ix jw kr ks jb kt bi translated">我们在执行优化时面临哪些挑战</h2><p id="e46e" class="pw-post-body-paragraph jd je hh jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated">当执行梯度下降或SGD或小批量SGD时，我们的重量有可能会卡在鞍点</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es nx"><img src="../Images/6278da3ba11c1416971c8ec0e984b663.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pbTLD4d1IFxd4BRx.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">在二维空间中</figcaption></figure><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es ny"><img src="../Images/65a738d9d8d7a79025cd78016776aad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nfH5wt0XvZKi6Bed.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">在三维空间中，它看起来像这样</figcaption></figure><p id="d082" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">鞍点只不过是一个点，在这个点上我们的函数既不是最大值也不是最小值，但梯度将为零，你可以在非凸函数中看到这个鞍点，如果我们对非凸函数执行梯度下降或sgd或最小批量sgd，我们的权重“w”将收敛，但很难说我们是在全局最小值或局部最小值还是在鞍点，这完全取决于权重的初始化， 如果我们在全局最小值附近初始化我们的权重，我们到达全局最小值，否则我们到达局部最小值或鞍点</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es nf"><img src="../Images/0c9d203250c44e57fe6db541b7a54ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oeHoZl22tMXaaohC.png"/></div></div></figure><p id="02b1" class="pw-post-body-paragraph jd je hh jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka ha bi translated">有一些技术可以帮助克服这个鞍点，但是如果我们停留在局部极小值，对我们来说就复杂了。研究学者对如何在高维空间中克服局部极小值有争议，但他们认为在高维空间中出现局部极小值的概率很小，所以我们遇到的大多是鞍点。在机器学习中，我们尝试优化凸函数，因此在这里我们不需要担心鞍点或局部最小值，但在深度学习中，我们主要遇到非凸函数，在那里我们看到更多的优化器，它们可以帮助克服鞍点，并使速度收敛到最小值</p></div></div>    
</body>
</html>