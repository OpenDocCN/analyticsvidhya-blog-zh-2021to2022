<html>
<head>
<title>Deep Residual Learning for Image Recognition (ResNet paper explained)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于图像识别的深度残差学习(ResNet论文解释)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-residual-learning-for-image-recognition-resnet-paper-explained-26b29e0fa73e?source=collection_archive---------7-----------------------#2021-01-01">https://medium.com/analytics-vidhya/deep-residual-learning-for-image-recognition-resnet-paper-explained-26b29e0fa73e?source=collection_archive---------7-----------------------#2021-01-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8af5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着层数的增加，深度神经网络趋向于提供更高的精度。但是，随着我们深入网络，网络的准确性不增反减。随着越来越多的层被堆叠，出现了渐变消失的问题。该论文提到已经通过归一化初始化和中间归一化层解决了消失梯度。随着深度的增加，精度达到饱和，然后迅速下降。</p><p id="64b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">*消失梯度:消失梯度是指深层多层前馈网络或RNN无法将有用的梯度信息从模型的输出端传播到模型输入端附近的层。在这种情况下，梯度变得非常小，并防止权重改变它们的值。这导致网络难以训练。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/6aef7f9f98bfafe107c4b9ee652a27a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/0*dC41ghFcKNP1j7H2.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><strong class="bd jp">训练错误</strong> <em class="jq">(左)</em> <strong class="bd jp">和测试错误</strong> <em class="jq">(右)</em> <strong class="bd jp">在CIFAR-10 </strong> <strong class="bd jp">上用20层和56层的“普通”网络</strong> <br/> <a class="ae jr" href="https://arxiv.org/pdf/1512.03385" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="a9b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图显示，随着网络深度的增加，训练误差增加，从而增加了测试误差。这里，20层网络的训练误差小于56层网络的误差。因此，网络不能很好地概括新数据，成为一个低效的模型。这种退化表明增加模型层无助于模型的性能，并且不是所有的系统都容易优化。</p><p id="0286" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该论文通过引入深度剩余学习框架来解决退化问题。ResNet的主要创新是剩余模块。残差模块具体是恒等式残差模块，它是两个卷积层的块，具有相同的滤波器数目和较小的滤波器大小。第二层的输出与第一卷积层的输入相加。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es js"><img src="../Images/f7e25dedeb22e881e81e36c459a61adb.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/0*CkQYq5XSoeT4J8U3.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">剩余学习:一个积木。<br/> <a class="ae jr" href="https://arxiv.org/pdf/1512.03385" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="0090" class="jt ju hi bd jp jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">网络体系结构</h1><p id="da74" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">本文将VGGNet的基线模型视为主要具有3×3滤波器的平面网络，具有两个设计规则:a)对于相同的输出特征图大小，层具有相同数量的滤波器；b)如果特征图大小减半，则滤波器的数量加倍，以保持每层的时间复杂度。网络以一个全局平均池层和一个1000路全连接层结束。</p><p id="2ffd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在普通网络的基础上，添加快捷连接，将普通版本转换为剩余版本。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kv"><img src="../Images/6057f35d2f2db35abc92ca4ba9c7e357.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/0*bkHv3eeeWAz-IiWz.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><strong class="bd jp">左</strong> : VGG-19型<strong class="bd jp">中</strong>:34参数层平原网<strong class="bd jp">右</strong>:34参数层残差网<br/> <a class="ae jr" href="https://arxiv.org/pdf/1512.03385" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="3c95" class="jt ju hi bd jp jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">履行</h1><ul class=""><li id="d2ee" class="kw kx hi ih b ii kq im kr iq ky iu kz iy la jc lb lc ld le bi translated">首先调整图像大小，将其短边采样为256 x 480</li><li id="82f3" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">采用了数据扩充技术</li><li id="836e" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">在每次卷积后和激活前进行批量标准化</li><li id="ea37" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">随机梯度下降被用于训练具有256个小批量的网络。</li><li id="8ddf" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">使用0.0001的重量衰减和0.9的动量。</li></ul><h1 id="c1a9" class="jt ju hi bd jp jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">实验</h1><p id="643f" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">Resnet架构在由1000个类组成的ImageNet 2012分类数据集上进行评估。该模型在128万幅训练图像上进行训练，并在5万幅验证图像上进行评估。此外，100k图像用于测试模型的准确性。</p><p id="c86e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当在平面网络上执行实验时，34层平面网络显示出比18层平面网络更高的验证误差。发现34层平面网络的训练误差高于18层平面网络。这里，当我们深入网络时，出现了一个降级问题。深度平面网络可能具有低收敛率，这会影响模型的准确性(影响训练误差的减少)。<br/>与普通网络不同，每对3×3滤波器增加了一个快捷连接。在层数与普通网络相同的情况下，Resnet 34的性能优于Resnet 18网络。Resnet-34显示出较少的误差，并且在概括验证数据方面表现良好。这解决了普通深层网络中出现的性能下降问题。普通网络和剩余网络的比较如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lk"><img src="../Images/357dcc4a04b387fe381b9dd70070439b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*d3zt1VsPaoSB4a5w.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">ImageNet培训。细曲线表示训练误差，粗曲线表示中心作物的验证误差。左图:18层和34层的平面网络。右图:18层和34层的结果。在该图中，残差网络与它们的普通对应物相比没有额外的参数。<br/> <a class="ae jr" href="https://arxiv.org/pdf/1512.03385" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="1760" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考</p><ul class=""><li id="1ba6" class="kw kx hi ih b ii ij im in iq lp iu lq iy lr jc lb lc ld le bi translated"><a class="ae jr" href="https://arxiv.org/pdf/1512.03385" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习</a></li></ul></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><p id="b400" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lz">原载于2021年1月1日</em><a class="ae jr" href="https://prabinnepal.com/deep-residual-learning-for-image-recognition-resnet-paper-explained/" rel="noopener ugc nofollow" target="_blank"><em class="lz">https://prabinnepal.com</em></a><em class="lz">。</em></p></div></div>    
</body>
</html>