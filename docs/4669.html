<html>
<head>
<title>Principal Component Analysis Code Walkthrough(PCA)from scratch in python.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">python中从头开始的主成分分析代码演练(PCA)。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/principal-component-analysis-from-first-principles-code-walkthrough-pca-b7cc471d7c63?source=collection_archive---------2-----------------------#2021-12-22">https://medium.com/analytics-vidhya/principal-component-analysis-from-first-principles-code-walkthrough-pca-b7cc471d7c63?source=collection_archive---------2-----------------------#2021-12-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/cf835af2193a372b75b981fed81b49e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mQeROU_hw38oCDgFGZnzLw.jpeg"/></div></div></figure><h1 id="c293" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">在我们深入PCA之前，让我们先来理解降维。我们为什么要降维？</h1><p id="bd41" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">如果你想要数学概念和证明，你可以访问:</p><p id="3c07" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><a class="ae kq" href="https://bit.ly/3ehFg3q" rel="noopener ugc nofollow" target="_blank">https://bit.ly/3ehFg3q</a></p><p id="a10a" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><em class="kr">《作为纳什均衡的主成分分析》。这也是可以实现的。看看我的另一个博客来了解更多关于这个</em></p><p id="1d5d" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><a class="ae kq" rel="noopener" href="/analytics-vidhya/what-is-eigen-game-d6dda6c980b1">https://medium . com/analytics-vid hya/what-is-eigen-game-d 6 DD a6 c 980 b 1</a></p><p id="1a60" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">当我们试图可视化数据以获得洞察力或更好的理解时，我们使用散点图、配对图等图。但是，如果我们有n维数据，其中n远远大于3维数据，那该怎么办呢？没有办法在我们的屏幕上显示3维以上的数据。所以像PCA和T-SNE这样的技术出现了，帮助我们摆脱这些问题。</p><p id="08d3" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">所以我们可以说主成分分析是一种用于降维的数学技术。它的目标是在保留大部分原始信息的同时减少特征的数量。</p><p id="c67b" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">为什么PCA对任何机器学习任务都有用。</p><ol class=""><li id="a402" class="ks kt hh jp b jq kl ju km jy ku kc kv kg kw kk kx ky kz la bi translated"><strong class="jp hi">使可视化成为可能</strong></li><li id="6015" class="ks kt hh jp b jq lb ju lc jy ld kc le kg lf kk kx ky kz la bi translated"><strong class="jp hi">对于基于距离的算法，如果维数非常大，则通过克服维数灾难来减少特征。</strong></li></ol><p id="a9fa" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我们将看到如何使用MNIST数据集使可视化成为可能。</p><h1 id="54f8" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">数据集和导入</h1><p id="3ec6" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated"><strong class="jp hi">关于数据集:</strong></p><p id="6d1b" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">数据文件train.csv和test.csv包含手绘数字的灰度图像，从0到9。</p><p id="d860" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">每幅图像高28像素，宽28像素，总共784像素。每个像素都有一个相关联的像素值，表示该像素的亮度或暗度，数字越大表示越暗。该像素值是0到255之间的整数，包括0和255。</p><p id="b456" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">训练数据集(train.csv)有785列。第一列称为“标签”，是用户绘制的数字。其余的列包含相关图像的像素值。</p><p id="b457" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">定型集中的每个像素列都有一个类似像素的名称，其中x是0到783之间的整数，包括0和783。为了在图像上定位这个像素，假设我们将x分解为x = i * 28 + j，其中I和j是0到27之间的整数，包括0和27。那么像素位于28×28矩阵的第I行和第j列(由零索引)。</p><p id="9454" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">例如，pixel31表示位于左起第四列和上数第二行的像素，如下面的ASCII图所示。</p><p id="e07c" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">从视觉上看，如果我们省略“像素”前缀，像素组成的图像如下:</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="f0a9" class="lp iq hh ll b fi lq lr l ls lt">000 001 002 003 ... 026 027<br/>028 029 030 031 ... 054 055<br/>056 057 058 059 ... 082 083<br/> |   |   |   |  ...  |   |<br/>728 729 730 731 ... 754 755<br/>756 757 758 759 ... 782 783</span></pre><p id="052f" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">您可以从这里下载数据集:</p><div class="lu lv ez fb lw lx"><a href="https://www.kaggle.com/c/digit-recognizer/data" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">数字识别器</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">用著名的MNIST数据学习计算机视觉基础</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">www.kaggle.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml in lx"/></div></div></a></div><p id="71d2" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">让我们加载数据集并导入所需的库</strong></p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="addc" class="lp iq hh ll b fi lq lr l ls lt"># MNIST dataset downloaded from Kaggle :<br/># <a class="ae kq" href="https://www.kaggle.com/c/digit-recognizer/data" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/digit-recognizer/data</a><br/># Functions to read and show images.</span><span id="f9a2" class="lp iq hh ll b fi mm lr l ls lt">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span><span id="6fbc" class="lp iq hh ll b fi mm lr l ls lt">d0 = pd.read_csv('mnist_train.csv')<br/>print(d0.head(5)) # print first five rows of d0.</span><span id="8d43" class="lp iq hh ll b fi mm lr l ls lt"># save the labels into a variable l.<br/>l = d0['label']</span><span id="cdee" class="lp iq hh ll b fi mm lr l ls lt"># Drop the label feature and store the pixel data in d.<br/>d = d0.drop("label",axis=1)</span></pre><p id="04b5" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">执行上面的代码应该会产生以下数据帧:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mn"><img src="../Images/28379c16be23f42cb94e5d5ac80436e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dVOA3TTPbXiwWg8MBrGfNQ.png"/></div></div></figure><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="cc8f" class="lp iq hh ll b fi lq lr l ls lt"># display or plot a number.<br/>plt.figure(figsize=(7,7))<br/>idx = 1<br/>grid_data = d.iloc[idx].to_numpy().reshape(28,28)  # reshape from 1d to 2d pixel array<br/>plt.imshow(grid_data, interpolation = "none", cmap = "gray")<br/>plt.show()<br/>print(l[idx])</span></pre><p id="6270" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">执行上面的代码应该会产生以下结果:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mo"><img src="../Images/5a67aa05de51b28170ed621d507bbf95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*3ydJMxHCk5i4WKVY6VhBFA.png"/></div></figure><h1 id="aac5" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">PCA的步骤</h1><ol class=""><li id="5536" class="ks kt hh jp b jq jr ju jv jy mp kc mq kg mr kk kx ky kz la bi translated"><strong class="jp hi">缩放数据</strong> —我们不希望由于缩放差异，一些功能被投票认为“更重要”。10m = 10000mm，但是该算法不知道不同的比例。我们应该使数据标准化。如果你想了解扩展数据背后的影响，请看看我的另一个博客:</li><li id="a2ef" class="ks kt hh jp b jq lb ju lc jy ld kc le kg lf kk kx ky kz la bi translated"><strong class="jp hi">计算协方差矩阵</strong> —给出随机向量的每对元素之间的协方差的平方对称矩阵。</li><li id="c2ae" class="ks kt hh jp b jq lb ju lc jy ld kc le kg lf kk kx ky kz la bi translated"><strong class="jp hi">特征分解</strong>——我们将使用这些来找到最大方差的方向。</li><li id="8ea2" class="ks kt hh jp b jq lb ju lc jy ld kc le kg lf kk kx ky kz la bi translated">关于优化问题和证明，请查看我的另一个博客。本博客仅用于代码演练:<a class="ae kq" href="https://bit.ly/3ehFg3q" rel="noopener ugc nofollow" target="_blank">https://bit.ly/3ehFg3q</a></li></ol><p id="4732" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">让我们从提到的步骤开始:</strong></p><h1 id="e91d" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">数据缩放</h1><p id="49aa" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">为了进行缩放，我们将使用来自<em class="kr"> Scikit-Learn </em>的<em class="kr">标准缩放器</em>:</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="203b" class="lp iq hh ll b fi lq lr l ls lt"># Data-preprocessing: Standardizing the data</span><span id="9bb0" class="lp iq hh ll b fi mm lr l ls lt">from sklearn.preprocessing import StandardScaler<br/>standardized_data = StandardScaler().fit_transform(data)<br/>print(standardized_data.shape)</span></pre><h1 id="79ef" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">协方差矩阵</h1><p id="1d08" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">方差报告单个随机变量的变化，比如说一个人的体重。重量偏离其平均值的程度代表了变化。协方差报告两个随机变量的变化程度——比如一个人的体重和身高。在协方差矩阵的对角线上，我们有方差，其他元素是协方差。如果数据矩阵X的大小为n*d，那么我们找到协方差矩阵，假设我们已经将该数据标准化为S=X.T*X。因此矩阵S的维数为d*d。这将导致平方对称矩阵。让我们看看代码。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="d1d7" class="lp iq hh ll b fi lq lr l ls lt">#find the co-variance matrix which is : (A^T * A)/n</span><span id="643f" class="lp iq hh ll b fi mm lr l ls lt">sample_data = standardized_data</span><span id="1088" class="lp iq hh ll b fi mm lr l ls lt"># matrix multiplication using numpy</span><span id="e35c" class="lp iq hh ll b fi mm lr l ls lt">covar_matrix = np.matmul(sample_data.T , sample_data)/42000</span><span id="cbcf" class="lp iq hh ll b fi mm lr l ls lt">print ( "The shape of variance matrix = ", covar_matrix.shape)</span></pre><h1 id="f81f" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">特征分解</h1><p id="c22a" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated"><strong class="jp hi">特征分解</strong>是将<a class="ae kq" href="https://en.wikipedia.org/wiki/Matrix_(math)" rel="noopener ugc nofollow" target="_blank">矩阵</a>分解为<a class="ae kq" href="https://en.wikipedia.org/wiki/Canonical_form" rel="noopener ugc nofollow" target="_blank">标准形式</a>的<a class="ae kq" href="https://en.wikipedia.org/wiki/Matrix_factorization" rel="noopener ugc nofollow" target="_blank">分解</a>，由此矩阵被表示为其<a class="ae kq" href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors" rel="noopener ugc nofollow" target="_blank">特征值和特征向量</a>。只有<a class="ae kq" href="https://en.wikipedia.org/wiki/Diagonalizable_matrix" rel="noopener ugc nofollow" target="_blank">可对角化的矩阵</a>可以用这种方式分解。当被分解的矩阵是一个<a class="ae kq" href="https://en.wikipedia.org/wiki/Normal_matrix" rel="noopener ugc nofollow" target="_blank">正规</a>或实<a class="ae kq" href="https://en.wikipedia.org/wiki/Symmetric_matrix" rel="noopener ugc nofollow" target="_blank">对称矩阵</a>时，这种分解称为“谱分解”，来源于<a class="ae kq" href="https://en.wikipedia.org/wiki/Spectral_theorem" rel="noopener ugc nofollow" target="_blank">谱定理</a>。特征向量是简单的单位向量，特征值是给特征向量大小的系数。到目前为止，我们知道我们的协方差矩阵是对称的。事实证明，<a class="ae kq" href="https://math.stackexchange.com/questions/82467/eigenvectors-of-real-symmetric-matrices-are-orthogonal" rel="noopener ugc nofollow" target="_blank">对称矩阵的特征向量是正交的</a>。对于主成分分析，这意味着我们有第一个主成分，它解释了大部分的差异。与之正交的是第二个主成分，它解释了大部分剩余的方差。对N个主成分重复这一过程，其中N等于原始特征的数量。</p><p id="1662" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">主成分按解释的方差百分比排序，因为我们可以决定我们应该保留多少。例如，如果我们最初有100个特征，但是前3个主成分解释了95%的方差，那么只保留这3个用于可视化和模型训练是有意义的。</p><p id="9024" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">探讨深入理论部分※访问:【https://bit.ly/3ehFg3q】<a class="ae kq" href="https://bit.ly/3ehFg3q" rel="noopener ugc nofollow" target="_blank"/></strong></p><p id="752d" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我们可以通过scipy执行特征分解</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="9145" class="lp iq hh ll b fi lq lr l ls lt"># finding the top two eigen-values and corresponding eigen-vectors<br/># for projecting onto a 2-Dim space.</span><span id="c3f0" class="lp iq hh ll b fi mm lr l ls lt">from scipy.linalg import eigh</span><span id="5bef" class="lp iq hh ll b fi mm lr l ls lt"># the parameter 'eigvals' is defined (low value to heigh value)<br/># eigh function will return the eigen values in asending order<br/># this code generates only the top 2 (782 and 783) eigenvalues.</span><span id="5f89" class="lp iq hh ll b fi mm lr l ls lt">values, vectors = eigh(covar_matrix, eigvals=(782,783))<br/>print("Shape of eigen vectors = ",vectors.shape)</span><span id="1acd" class="lp iq hh ll b fi mm lr l ls lt"># converting the eigen vectors into (2,d) shape for easyness of further computations</span><span id="bca7" class="lp iq hh ll b fi mm lr l ls lt">vectors = vectors.T<br/>print("Updated shape of eigen vectors = ",vectors.shape)</span><span id="52d5" class="lp iq hh ll b fi mm lr l ls lt"># here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector</span><span id="6c3b" class="lp iq hh ll b fi mm lr l ls lt"># here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector</span></pre><h1 id="16d0" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">现在我们做了特征分解，是时候将原始数据样本投影到平面上了。</h1><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="a4b8" class="lp iq hh ll b fi lq lr l ls lt"># projecting the original data sample on the plane</span><span id="be4f" class="lp iq hh ll b fi mm lr l ls lt">#formed by two principal eigen vectors by vector-vector multiplication.</span><span id="89d6" class="lp iq hh ll b fi mm lr l ls lt"><br/>new_coordinates = np.matmul(vectors, sample_data.T)<br/>print (" resultanat new data points' shape ", vectors.shape, "X", sample_data.T.shape," = ", new_coordinates.shape)</span><span id="19dc" class="lp iq hh ll b fi mm lr l ls lt"># appending label to the 2d projected data<br/>new_coordinates = np.vstack((new_coordinates, labels)).T</span><span id="4c5a" class="lp iq hh ll b fi mm lr l ls lt"># creating a new data frame for ploting the labeled points.</span><span id="5b4e" class="lp iq hh ll b fi mm lr l ls lt">dataframe = pd.DataFrame(data=new_coordinates, columns=("1st_principal", "2nd_principal", "label"))<br/>print(dataframe.head())</span></pre><h1 id="144d" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">让我们绘制数据框。</h1><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="8ade" class="lp iq hh ll b fi lq lr l ls lt">sn.FacetGrid(dataframe, hue="label", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()</span><span id="129d" class="lp iq hh ll b fi mm lr l ls lt">plt.show()</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ms"><img src="../Images/3b33a2090c506fd53dd3a0654c665650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GW5xoj4YwCbQdUb2o5Ij0A.png"/></div></div></figure><h1 id="4a83" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">让我们通过Scikit-learn来验证我们的代码</h1><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="a4b1" class="lp iq hh ll b fi lq lr l ls lt"># initializing the pca<br/>from sklearn import decomposition<br/>pca = decomposition.PCA()</span><span id="598a" class="lp iq hh ll b fi mm lr l ls lt"># configuring the parameteres<br/># the number of components = 2<br/>pca.n_components = 2<br/>pca_data = pca.fit_transform(sample_data)</span><span id="c185" class="lp iq hh ll b fi mm lr l ls lt"># pca_reduced will contain the 2-d projects of simple data<br/>print("shape of pca_reduced.shape = ", pca_data.shape)</span><span id="5f04" class="lp iq hh ll b fi mm lr l ls lt"># attaching the label for each 2-d data point</span><span id="b5ad" class="lp iq hh ll b fi mm lr l ls lt">pca_data = np.vstack((pca_data.T, labels)).T</span><span id="a33a" class="lp iq hh ll b fi mm lr l ls lt"># creating a new data fram which help us in ploting the result data<br/>pca_df = pd.DataFrame(data=pca_data, columns=("1st_principal", "2nd_principal", "label"))</span><span id="9936" class="lp iq hh ll b fi mm lr l ls lt">sn.FacetGrid(pca_df, hue="label", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()</span><span id="79fb" class="lp iq hh ll b fi mm lr l ls lt">plt.show()</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/364858c81423cbad6106d3cba0c99bfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*wnmYVj_hJBH4ZkFTP2DLMg.png"/></div></figure><p id="8188" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我们可以观察到两个代码生成相同的结果，但在scikit learn visualization中，它被旋转了90度。除了所有的事情都很相似。</p><p id="7732" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">你可以在<a class="ae kq" href="https://github.com/Khanamin-XOR/PCA-From-Stratch" rel="noopener ugc nofollow" target="_blank">https://github.com/Khanamin-XOR/PCA-From-Stratch</a>找到完整的代码</p><p id="8783" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">希望你喜欢代码演练。喜欢这个帖子并分享。这激励我写更多的博客。我将为包括ML在内的各种算法发布更多的代码演练。请随时指出代码中的错误，并使用不同的数据集来玩代码。关于PCA的理论概念和优化问题可以参考这个博客:</p><p id="78dc" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">感谢阅读。</p><p id="2243" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">如果你发现错误或漏洞，请随意评论。请原谅我所犯的任何错误。</strong></p><p id="cd72" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在LinkedIn上关注我:</p><p id="baf0" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">【https://www.linkedin.com/in/mdaminkhan/ T4】</p></div></div>    
</body>
</html>