# 使用随机森林的智能数据插补

> 原文：<https://medium.com/analytics-vidhya/smart-data-imputation-using-random-forest-58a9aedd557a?source=collection_archive---------7----------------------->

![](img/d285a4ede710fcc1d65b1a458123df80.png)

您为机器学习算法提供的功能对预测模型的影响比在两个相似的算法之间进行选择更大(假设它们都适合您的数据)。事实上，有了智能特征工程，甚至更简单的算法也能打败复杂的算法。数据插补是特征工程中的一个关键要素。如果您的数据中有很多缺失值，那么插补的质量会决定您预测的成败。

一个不好的诋毁会如何伤害你？任何机器学习算法都试图理解会导致目标变量值不同的输入特征组合。使用所提供的数据学习特征之间的这种相互关系以及它们与目标变量的组合关系。对缺失值使用不良插补会给模型增加噪声，从而阻碍其探索数据关系的能力。因此，如果您有足够的数据，并且缺失值是随机分布的，您应该删除包含缺失值的观测值，而不是输入这些值。

**随机森林插补案例:**虽然均值、中值和众数都是有效的插补，但它们只比随机猜测缺失条目的值好。一个更好的方法是使用，比如说线性回归进行插补。

但是，线性回归是一个参数模型，有自己的一套假设。如果特征向量不满足线性回归的假设，你会得到一个很差的缺失值插补。此外，如果您需要估算 10 个不同要素的缺失值，您需要运行此模型 10 次，每次都将缺失值要素作为目标变量。

其他流行的技术，如 MICE(通过链式方程的多变量插补)，通过建立几个线性和逻辑回归模型并取预测的平均值来插补缺失值，从而改进了回归插补。但是，如果特征值不是线性分布的，它们的性能就很差，并且存在回归插补的一般缺点。

那么，你需要的是一个非参数模型，它不对数据的分布做任何假设，可以学习数据中的复杂结构，并且可以对连续变量和分类变量都有效。进入随机森林！！RF 的另一个优势是，它容易出现异常值，并且可以处理输入要素之间的相关性。

**随机森林插补的要点:**随机森林不是对可用特征值进行简单平均，而是进行加权平均，其中权重与两个观察值之间的相似性成比例。因此，与正在插补的当前数据点相似的数据点得到更多的权重-年龄，反之亦然。随机森林插补在直觉上也更有意义。在为某个人的物理测试分数输入缺失值时，您是倾向于将所有个人的物理测试分数进行平均，还是倾向于给与该特定学生相似的学生更高的权重？但是，如何衡量两个观察值之间的相似性呢？

**这些相似性是如何计算出来的？**这些是随机森林中最有用但被低估的工具之一。每棵树长大后，random forest 会将所有数据(包括训练数据和 OOB 数据)放入树中。如果两个观察结果出现在同一个终端节点(具有相同的预测)，则它们的相似度增加 1。该相似性分数是针对 RF 中生长的每棵树上的所有数据点计算的。最后，将每次观察的单个分数相加，并除以树的数量，以使其标准化。对于具有 N 个观察值的数据集，计算每对观察值之间的相似性得分，并以 NxN 矩阵形式保存。

**填充:** RF 从传统方式开始。在每个类中，连续变量的缺失值由该类的中值替换。类似地，在每个类中，分类变量的缺失值由类的模式替换。这些替换值称为填充。

**训练数据的缺失值替换(存在标签的观察值):** RF 首先对缺失值进行粗略填充。然后，它进行一次森林运行，并计算观察结果之间的相似性。然后，在每个类别中，对于连续特征的缺失值，插补计算为非缺失值的加权平均值。权重只不过是在森林运行期间计算的相似性。对于分类特征，使用模式。这构成了一次迭代。使用先前估算的值再次重复该迭代，直到估算的值和相似性分数不稳定。通常，4-6 次迭代足以稳定估算值。

**测试数据的缺失值替换(标签不存在的观察值):**如果标签不存在，作为分类算法的 RF 不能创建树，因此不能计算邻近度。为了克服这个问题，RF 使用了一个聪明的技巧:没有标签的每一行数据被复制 m 次，m 是类的数量。例如，如果 100 个观察值没有标签，而其余的数据有 10 个不同的标签，那么它将每个未标记的观察值重复 10 次，每次分配一个不同的标签。这样，对于 100 个未标记的行和 10 个不同的标签，它创建了一个 1000 个观察值的数据集。

假设行的第一个副本是类 1，类 1 填充用于替换丢失的值。第二次复制被假定为类别 2，并且对其使用类别 2 填充，以此类推，直到最后一次复制被假定为类别 m。然后，这个新的数据集沿着树向下运行。在每组观察值(重复 m 次)中，获得多数票的类别被假定为该观察值的标签。因此，在第一次运行期间，除了计算相似性，RF 还为测试数据中的每个观察值分配了标签。对于测试数据的 100 个观察值和训练数据中的 10 个类别，它将每个观察值复制 10 次，得到 1000 个观察值，并对这 1000 个观察值进行森林运行，以计算每个观察值的类别。一旦分配了类别，您又会剩下最初的 100 个观察值，您需要在这些观察值中进行缺失值插补，但是现在这些观察值有了标签，并且可以在这些观察值上应用用于训练数据的缺失值插补技术。

然后，使用 RF 完成的缺失值插补可用于任何其他机器学习算法。

*最初发表于*[T5【https://www.linkedin.com】](https://www.linkedin.com/pulse/do-smart-data-imputation-using-random-forest-arvind-shukla/)*。*