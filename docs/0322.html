<html>
<head>
<title>Predicting Vehicle Price With Random Forest Regressor</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用随机森林回归器预测汽车价格</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/predicting-vehicle-price-with-random-forest-regressor-d1c272668be5?source=collection_archive---------9-----------------------#2021-01-12">https://medium.com/analytics-vidhya/predicting-vehicle-price-with-random-forest-regressor-d1c272668be5?source=collection_archive---------9-----------------------#2021-01-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="c92a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">预测销售价格|回归模型|交叉验证</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/be2a6160a4f165634e7d3f6b8455342f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GDjVt1eUGYVOxn1d04g7uw.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">来源:<a class="ae js" href="https://www.vwstaug.com/blogs/1984/best-practices-on-buying-a-car/how-to-choose-the-right-car-dealership/attachment/15075074577_bda5ec36c4_b/" rel="noopener ugc nofollow" target="_blank">https://www . vwstaug . com/blogs/1984/best-practices-on-buying-a-car/how-to-choose-the-right-car-deadlines/attachment/15075074577 _ BDA 5 EC 36 c 4 _ b/</a></figcaption></figure><p id="77b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在机器学习中，有分类和回归模型。两者的不同之处在于分类将输出(或y)预测为是或否、向上或向下或其他类别。例如，在情感分析中，我们想知道评论属于好的还是坏的情感。然而，在回归中，我们想要的输出是价值，比如房子的价格。</p><p id="b630" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这次汽车价格预测中，回归模型是一个很好的实践。它包含了几个我们需要准备的特征，我们也面临着非正态分布。</p><p id="3564" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文的目的是介绍进行回归分析的步骤，其中我们将比较几个模型，并通过交叉验证来运行它们。所以，敬请关注。以下是我们将涉及的内容列表。</p><h1 id="c596" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">这篇文章的大纲</h1><ol class=""><li id="ba89" class="kr ks hh ig b ih kt il ku ip kv it kw ix kx jb ky kz la lb bi translated">数据概述</li><li id="65b8" class="kr ks hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">数据可视化</li><li id="e320" class="kr ks hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">数据准备(缺少值和类别数据)</li><li id="3280" class="kr ks hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">模型比较和选择(线性回归、随机森林回归、梯度推进回归、极端梯度推进)</li><li id="b953" class="kr ks hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">预言；预测；预告</li></ol><h1 id="4b61" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">1.数据概述</h1><p id="8416" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated"><a class="ae js" href="https://www.kaggle.com/austinreese/craigslist-carstrucks-data" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>提供了一组二手车价格的数据。我们将在我们的项目中使用这些数据。完整的数据集包含具有25个属性的423，857行数据。让我们来看看。</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="c345" class="lp ju hh ll b fi lq lr l ls lt">import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>import numpy as np</span><span id="9ec7" class="lp ju hh ll b fi lu lr l ls lt">data = pd.read_csv('.../vehicles.csv')<br/>pd.set_option('display.max_columns', 25)<br/>data.info()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lv"><img src="../Images/e7ae6e0c133f921f3b4d3da6ddbbd10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*8FErNyjQIp-Pq7jQ2YcQsw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">数据信息</figcaption></figure><p id="f3b3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以看到，我们的数据包含许多缺失值，并且大多数是类别数据的对象类型。我们将有大量的数据准备工作要做。但在此之前，我们应该将数据可视化。</p><h1 id="cfaf" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">2.数据可视化</h1><p id="9bbe" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">因为我们有经度和纬度属性，我们可以从中创建地理视图。</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="e7b5" class="lp ju hh ll b fi lq lr l ls lt">data.plot(kind=’scatter’, x=’long’, y=’lat’, <br/>alpha=0.4, figsize=(10,7))</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lw"><img src="../Images/1efae7a93552c691181207b7edd64f7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5QJZ3JDAXG0ZfWhW5s-OUg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">用散点图可视化数据</figcaption></figure><p id="6bf8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的数据主要是在美国境内收集的。我们可以从地理可视化中看到这一点，因为大多数数据位于美国地区，只有少数数据分散在主群之外。</p><p id="82ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">属性之间有多少关联呢？</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="ea52" class="lp ju hh ll b fi lq lr l ls lt">matrix = np.triu(data.corr())<br/>fig, ax = plt.subplots(figsize=(15,10))<br/>sns.heatmap(X_train.corr(), mask=matrix, ax=ax, cbar=True, annot=True, square=True)<br/>plt.savefig('heatmap.png')</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lx"><img src="../Images/a1c40619038af62dd735208f1abe029a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0-2YFFfYxQTxAPZqWiwMbA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">显示属性相关性的热图</figcaption></figure><h1 id="53be" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">3.数据准备</h1><p id="bc61" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">首先，我们将看看销售价格与车辆年份的关系。</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="863c" class="lp ju hh ll b fi lq lr l ls lt">from scipy import stats<br/>from scipy.stats import norm, skew</span><span id="1733" class="lp ju hh ll b fi lu lr l ls lt">fig, ax = plt.subplots()<br/>ax.scatter(x = data['year'], y = data['price'])<br/>plt.ylabel('SalePrice', fontsize=13)<br/>plt.xlabel('Year', fontsize=13)<br/>plt.show()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ly"><img src="../Images/226084739450b5bbf68b134b6307a049.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*tiop5GeW-tLalO0NnDQhtA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">每年的汽车销售价格</figcaption></figure><p id="d862" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">似乎我们有离群值。为此，我们选择删除低于第一个分位数但高于第三个分位数的数据。删除后，我们还有324，723行数据。</p><p id="78d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们想看看价格的分布。我们可以用seaborn来绘制这个图。</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="84d7" class="lp ju hh ll b fi lq lr l ls lt">sns.distplot(data['price'], fit=norm)<br/>fig = plt.figure()<br/>res = stats.probplot(data['price'], plot=plt)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lz"><img src="../Images/af961cb3ca2b58fc8e380ad1de6fd2e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*4EUNVVqI88WpUUsoL-T-VQ.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ma"><img src="../Images/859b6693fb6ac99510c1f3d070c5a338.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*Eqq_K89LNawUfU1Z16KjDg.png"/></div></figure><p id="10c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由此我们可以看出，数据不具有正态分布，它包含峰度(有一个峰值)，具有正偏度(右边的尾部较长)，不在对角线上。</p><p id="24fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了解决这个问题，我们可以将对数函数应用到价格中。</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="3d4c" class="lp ju hh ll b fi lq lr l ls lt">data['price'] = np.log(data['price'])</span><span id="7982" class="lp ju hh ll b fi lu lr l ls lt">sns.distplot(data['price'], fit=norm)<br/>fig = plt.figure()<br/>res = stats.probplot(data['price'], plot=plt)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mb"><img src="../Images/32b517eb35e379eda307d146b388506b.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*hq5ARh3DyMuVqw2-tCTbnA.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mc"><img src="../Images/32191154369a3fd49327d0a50428b60d.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*PVZO-9mdG04YkM-i-tBK_g.png"/></div></figure><p id="459f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，是处理其他属性的时候了。但是在我们开始之前，我们将再添加一列并将其命名为“年龄”,并删除“年份”列。有了这个，我们就能知道车的年龄了。我们这样做是因为我们想知道这些二手车有多旧。</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="f5de" class="lp ju hh ll b fi lq lr l ls lt">data['age'] = 2021 - data['year']</span></pre><h2 id="58c5" class="lp ju hh bd jv md me mf jz mg mh mi kd ip mj mk kh it ml mm kl ix mn mo kp mp bi translated">数据清理步骤1</h2><p id="8e91" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">然后，利用这些数据，我们需要创建训练集和测试集。我们通过sklearn.model_selection中的train_test_split来实现这一点</p><p id="b293" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">拆分数据后，我们将要使用的填充数据缺失值的技术是<br/> 1。用“无”填充缺少的值，<br/> 2。用0.0填充缺少的值，<br/> 3。用mode()，<br/> 4填充缺失值。用最常出现的类别填充缺失值，并<br/> 5。将有值的行替换为“是”,将无值的行替换为“否”。</p><p id="81ca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了用“无”填充缺少的值，我们将它们应用于“制造商”、“型号”、“条件”、“标题_状态”、“驱动”、“尺寸”、“类型”和“油漆_颜色”。</p><p id="6d58" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了用0.0填充缺失值，我们将它们应用于“里程表”、“纬度”、“经度”和“年龄”。</p><p id="ff30" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了用mode()填充缺失值，我们将它们应用于“气缸”、“标题_状态”、“变速器”和“燃油”。</p><p id="300e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于“圆柱体”属性，我们用“其他”填充none值。</p><p id="cfc1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于“燃料”属性，我们用“气体”填充非值。</p><p id="caf5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于“传输”，我们用“自动”填充值。</p><p id="eecb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于“vin ”,我们使用re查找有值的行，其中我们用“yes”替换值，用“no”填充非值行。</p><p id="914d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些清洗需要应用于X_train和X_test设备。</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="ceb8" class="lp ju hh ll b fi lq lr l ls lt">import re<br/>from sklearn.model_selection import train_test_split<br/>train, test = train_test_split(data)</span><span id="45ec" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Split the data set</strong><br/>X_train = train.drop(['price'], axis='columns')<br/>y_train = train['price']<br/>X_test = test.drop(['price'], axis='columns')<br/>y_test = test['price']</span><span id="5a14" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Fill the missing value with “None”</strong><br/>cols_for_none = ('manufacturer','model','condition','title_status','drive','size',<br/>                'type','paint_color')</span><span id="2cad" class="lp ju hh ll b fi lu lr l ls lt">for c in cols_for_none:<br/>    X_train[c] = X_train[c].fillna("None") <br/>    X_test[c] = X_test[c].fillna("None") </span><span id="f137" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Fill the missing value with 0.0</strong><br/>cols_for_zero = ('odometer','lat','long','age')</span><span id="18c8" class="lp ju hh ll b fi lu lr l ls lt">for c in cols_for_zero:<br/>    X_train[c] = X_train[c].fillna(0.0)<br/>    X_test[c] = X_test[c].fillna(0.0)   </span><span id="c4f7" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Fill the missing value with mode()</strong><br/>cols_for_mode = ('cylinders','title_status','transmission','fuel')</span><span id="6152" class="lp ju hh ll b fi lu lr l ls lt">for c in cols_for_mode:<br/>    X_train[c] = X_train[c].fillna(X_train[c].mode())<br/>    X_test[c] = X_test[c].fillna(X_test[c].mode())</span><span id="b9ce" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Fill the missing value with most occurred categories</strong><br/>X_train['cylinders'] = X_train['cylinders'].fillna("other")<br/>X_train['fuel'] = X_train['fuel'].fillna('gas')<br/>X_train['transmission'] = X_train['transmission'].fillna('automatic')</span><span id="ebed" class="lp ju hh ll b fi lu lr l ls lt">X_test['cylinders'] = X_test['cylinders'].fillna("other")<br/>X_test['fuel'] = X_test['fuel'].fillna('gas')<br/>X_test['transmission'] = X_test['transmission'].fillna('automatic')</span><span id="581e" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Replace ‘yes’ for rows with values and ‘no’ for rows without values<br/></strong>X_train = X_train.replace({'vin':r'(\w*\S)'}, {'vin':"Yes"}, regex=True)<br/>X_train['vin'] = X_train['vin'].fillna("No")</span><span id="8421" class="lp ju hh ll b fi lu lr l ls lt">X_test = X_test.replace({'vin':r'(\w*\S)'}, {'vin':"Yes"}, regex=True)<br/>X_test['vin'] = X_test['vin'].fillna("No")</span></pre><h2 id="6c52" class="lp ju hh bd jv md me mf jz mg mh mi kd ip mj mk kh it ml mm kl ix mn mo kp mp bi translated">数据清理步骤2</h2><p id="d46e" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">现在，我们填充了所有缺失的数据，但在我们训练模型之前，我们需要将所有类别数据转换为数值。我们可以通过使用sklearn.preprocessing中的LabelEncoder来实现这一点。我们需要管理的类别包括“区域”、“制造商”、“型号”、“条件”、“燃料”、“标题状态”、“标题状态”、“传输”、“车辆识别号”、“驱动”、“尺寸”、“类型”、“油漆颜色”、“状态”和“气缸”。</p><p id="ba49" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于“里程表”，它也没有正态分布，因此我们需要把它们变成正态分布。</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="a556" class="lp ju hh ll b fi lq lr l ls lt">from sklearn.preprocessing import LabelEncoder<br/>le = LabelEncoder()</span><span id="f234" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Listing all the categorial attributes</strong><br/>cols = ['region','manufacturer','model','condition','fuel','title_status','title_status','transmission','vin',<br/>       'drive','size','type','paint_color','state','cylinders']</span><span id="dd62" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Apply to both X_train and X_test data</strong><br/>for c in cols:<br/>    le.fit(list(X_train[c].values))<br/>    X_train[c] = le.transform(list(X_train[c].values))<br/>    <br/>for c in cols:<br/>    le.fit(list(X_test[c].values))<br/>    X_test[c] = le.transform(list(X_test[c].values))</span><span id="d784" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Manage the distribution type of odometer data</strong><br/>log_value = ('odometer')</span><span id="cf5a" class="lp ju hh ll b fi lu lr l ls lt">for c in log_value:<br/>    X_train[c] = np.log1p(X_train[c])<br/>    X_test[c] = np.log1p(X_test[c])</span></pre><p id="3e25" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，由于我们所有的属性都用数字表示，我们可以看看每个属性有多重要。</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="ed63" class="lp ju hh ll b fi lq lr l ls lt">from sklearn.feature_selection import SelectKBest<br/>from sklearn.feature_selection import f_regression</span><span id="2358" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Show the important attributes in descending order</strong><br/>best_features = SelectKBest(score_func=f_regression, k=18)<br/>top_features = best_features.fit(X_train,y_train)<br/>scores = pd.DataFrame(top_features.scores_)<br/>columns = pd.DataFrame(X_train.columns)</span><span id="ccce" class="lp ju hh ll b fi lu lr l ls lt">featureScores = pd.concat([columns, scores], axis=1)<br/>featureScores.columns = ['Features','Scores']<br/>print(featureScores.nlargest(18, 'Scores'))</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mq"><img src="../Images/27eec79c520deff46bc3ac3f6d2a2ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*DE9ghAB98F7_2zZzEIP_yQ.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">重要功能</figcaption></figure><h1 id="d8b0" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">4.模型比较和评估</h1><p id="b880" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">大部分工作已经完成了。现在，我们可以运行几个回归模型来比较每个模型的准确性。对于每个模型，我们应用5重交叉验证。</p><p id="d117" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过交叉验证，我们的5 K折叠，它将数据随机分成5个折叠，并训练模型5次。每一次折叠，将在不同的时间进行评估，而其他4个将被保留用于培训。</p><p id="b315" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们了解一下我们将要选择的模型。</p><h2 id="35cb" class="lp ju hh bd jv md me mf jz mg mh mi kd ip mj mk kh it ml mm kl ix mn mo kp mp bi translated">随机森林回归量</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mr"><img src="../Images/a379c20bce495faae759e7cc60289161.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8O6vMnj_LV8SSc4Edu62Ew.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">来源:<a class="ae js" href="https://www.analyticsvidhya.com/blog/2020/05/decision-tree-vs-random-forest-algorithm/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2020/05/decision-tree-vs-random-forest-algorithm/</a></figcaption></figure><p id="4379" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该模型训练多个决策树，最终结果是投票多数(用于分类)或平均投票(用于回归)。每棵树将抽取随机样本，因此这防止了过度拟合，且对于大数据集，随机森林似乎表现良好。</p><div class="ms mt ez fb mu mv"><a rel="noopener follow" target="_blank" href="/swlh/random-forest-and-its-implementation-71824ced454f"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hi fi z dy na ea eb nb ed ef hg bi translated">随机森林及其实现</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">在这篇博客中，我们将试图理解机器学习中最重要的算法之一，即随机森林…</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">medium.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj jm mv"/></div></div></a></div><h2 id="f500" class="lp ju hh bd jv md me mf jz mg mh mi kd ip mj mk kh it ml mm kl ix mn mo kp mp bi translated">梯度推进回归器</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nk"><img src="../Images/f075619c7bb6b0443cc54987f73122f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*0KqYN--E0iWg50EAoQdc1A.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">来源:https://www.geeksforgeeks.org/ml-gradient-boosting/<a class="ae js" href="https://www.geeksforgeeks.org/ml-gradient-boosting/" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="d4a0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">模型预测来自对先前模型的弱预测的学习。第一棵树预测误差<em class="nl"> r1 </em>将在第二棵树中训练，在那里它将得到误差<em class="nl"> r2 </em>。然后用<em class="nl"> r2 </em>训练第三棵树，依此类推，直到到达<em class="nl"> N </em>棵树。</p><div class="ms mt ez fb mu mv"><a href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hi fi z dy na ea eb nb ed ef hg bi translated">机器学习的梯度推进算法简介-机器学习…</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">梯度推进是构建预测模型的最强大的技术之一。在这篇文章中，你会发现…</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">machinelearningmastery.com</p></div></div><div class="ne l"><div class="nm l ng nh ni ne nj jm mv"/></div></div></a></div><h2 id="7ad0" class="lp ju hh bd jv md me mf jz mg mh mi kd ip mj mk kh it ml mm kl ix mn mo kp mp bi translated">线性回归</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nn"><img src="../Images/885c4d74ce637318f0c6c2da2ccfcd95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tcOibV7rjD-WXkOaFtvS6g.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">来源:<a class="ae js" href="https://www.researchgate.net/figure/Linear-Regression-model-sample-illustration_fig3_333457161" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/figure/Linear-Regression-model-sample-illustration _ fig 3 _ 333457161</a></figcaption></figure><p id="d478" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最常见的回归模型。预测(<em class="nl"> y </em>)是从独立变量(<em class="nl"> X1，X2，X3，…，Xn </em>)中估计出来的。误差是<em class="nl"> y </em>距离线性回归线的距离。</p><div class="ms mt ez fb mu mv"><a href="https://machinelearningmastery.com/linear-regression-for-machine-learning/#:~:text=Linear%20regression%20is%20a%20linear,the%20input%20variables%20%28x%29." rel="noopener  ugc nofollow" target="_blank"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hi fi z dy na ea eb nb ed ef hg bi translated">机器学习的线性回归-机器学习掌握</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">线性回归也许是统计学和机器中最著名和最容易理解的算法之一</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">machinelearningmastery.com</p></div></div><div class="ne l"><div class="no l ng nh ni ne nj jm mv"/></div></div></a></div><h2 id="684b" class="lp ju hh bd jv md me mf jz mg mh mi kd ip mj mk kh it ml mm kl ix mn mo kp mp bi translated">极端梯度推进</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es np"><img src="../Images/417b070859b16c86f4fb3eda27a46c6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9BowVV2E_LctnBrU6ZJw4A.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">来源:<a class="ae js" href="https://www.abiqos.com/2019/08/expand-your-predictive-palette-xgboost-in-alteryx/" rel="noopener ugc nofollow" target="_blank">https://www . ABI QoS . com/2019/08/expand-your-predictive-palette-xgboost-in-alter yx/</a></figcaption></figure><p id="1b80" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">极端梯度推进(XGB)使用梯度推进决策树算法，其主要目标是执行速度和模型性能。近年来，许多kaggle比赛使用XGB作为他们的模型。</p><div class="ms mt ez fb mu mv"><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hi fi z dy na ea eb nb ed ef hg bi translated">提升树简介-xgboost 1 . 4 . 0-快照文档</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">XGBoost代表“极端梯度增强”，其中术语“梯度增强”源于论文Greedy…</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">xgboost.readthedocs.io</p></div></div><div class="ne l"><div class="nq l ng nh ni ne nj jm mv"/></div></div></a></div><div class="ms mt ez fb mu mv"><a href="https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d" rel="noopener follow" target="_blank"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hi fi z dy na ea eb nb ed ef hg bi translated">XGBoost算法:愿她统治长久！</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">接管世界的机器学习算法的新女王…</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="nr l ng nh ni ne nj jm mv"/></div></div></a></div><p id="2ce4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们来看看每个模型的实现和评估。</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="8461" class="lp ju hh ll b fi lq lr l ls lt">from sklearn.linear_model import LinearRegression<br/>from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor<br/>import xgboost as xgb<br/>from sklearn.model_selection import cross_val_score<br/></span><span id="ab64" class="lp ju hh ll b fi lu lr l ls lt">lr = LinearRegression()<br/>rr = RandomForestRegressor()<br/>gbr = GradientBoostingRegressor()<br/>xgb = xgb.XGBRegressor()</span><span id="d451" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Create function to displaying scores</strong><br/>def display_scores(scores):<br/>    print("Scores: ", scores)<br/>    print("Mean: ", scores.mean())<br/>    print("Standard Deviation: ", scores.std())</span><span id="de38" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Training the Random Forest Regressor</strong><br/>print("Random Forest Regressor Scores")<br/>scores = cross_val_score(rr, X_train, y_train, scoring='neg_mean_squared_error', cv=5)<br/>random_forest_scores = np.sqrt(-scores)<br/>display_scores(random_forest_scores)<br/>print("\n")</span><span id="bf1e" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Training the Gradient Boosting Regressor</strong><br/>print('Gradient Boosting Regressor Scores')<br/>scores = cross_val_score(gbr, X_train, y_train, scoring='neg_mean_squared_error', cv=5)<br/>gradient_boosting_regressor = np.sqrt(-scores)<br/>display_scores(gradient_boosting_regressor)<br/>print("\n")</span><span id="69b1" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Training the Linear Regression</strong><br/>print('Linear Regression Scores')<br/>scores = cross_val_score(lr, X_train, y_train, scoring='neg_mean_squared_error', cv=5)<br/>linear_regression = np.sqrt(-scores)<br/>display_scores(linear_regression)<br/>print("\n")</span><span id="1842" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Training the Extreme Gradient Boosting</strong><br/>print("xGB Scores")<br/>scores = cross_val_score(xgb, X_train, y_train, scoring='neg_mean_squared_error', cv=5)<br/>xgb_regressor = np.sqrt(-scores)<br/>display_scores(xgb_regressor)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ns"><img src="../Images/7619f1bd5b1604e398cee39dc50dc59b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E6ipfbGIEmH1HKEe6UAdKA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">不同回归模型的分数。</figcaption></figure><p id="324a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由此可以看出，我们的随机森林回归方程得到了最好的分数，平均值为0.3822，标准偏差为0.0027。下一个表现最好的是梯度推进回归器，其平均值为0.4471，标准偏差为0.0028。第三名是XGB，平均值为0.4473，标准偏差为0.0023。</p><h1 id="03aa" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">5.预言；预测；预告</h1><p id="4ab9" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">从我们的训练模型来看，随机森林回归器具有最好的性能和较低的平均误差。有了这个，我们将使用它们进行预测。</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="5cbe" class="lp ju hh ll b fi lq lr l ls lt"><strong class="ll hi">#Do the prediction</strong><br/>rr.fit(X_train, y_train)<br/>pred = rr.predict(X_test)</span><span id="0e86" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Convert the log value back to the original value</strong><br/>y_test = np.exp(y_test)<br/>pred = np.exp(pred)<br/></span><span id="c494" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Calculate the error and accuracy</strong><br/>errors = abs(pred - y_test)<br/>print('Average absolute error:', round(np.mean(errors), 2), 'degrees.')</span><span id="792b" class="lp ju hh ll b fi lu lr l ls lt">mape = 100*(errors / y_test)<br/>accuracy = 100 - np.mean(mape)<br/>print('Accuracy:', round(accuracy, 2), '%.')</span><span id="1064" class="lp ju hh ll b fi lu lr l ls lt"><strong class="ll hi">#Put the y_test and predict value into DataFrame to the ease of comparing the values</strong><br/>compare = pd.DataFrame()<br/>compare['y_true'] = y_test<br/>compare['y_predict'] = pred</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nt"><img src="../Images/49249286373c79038719d201d66355e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*Mz3Nm3pdPH-HxnQHOI9xug.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">模型精度</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nu"><img src="../Images/928f2c360035e603ce77f9e8da993ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*INIQ6jzJuPF8dafm_LpURA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">实际值和预测值的比较</figcaption></figure><p id="30ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从预测可以看出，其准确率为68.4%，平均误差为5105.99度。该表比较了<em class="nl">实际y </em>和<em class="nl">预测y </em>的结果</p><h1 id="5290" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="6cb5" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">有了这个，我们涵盖了很多东西，从可视化数据，清理数据，评估模型，并做预测。模型的精确度可能不是很高，但是我们可以看到实现回归预测的过程。为了改进预测或模型训练，我们可以在模型超参数中应用调整，或者只为我们的模型选择最相关的属性。</p></div></div>    
</body>
</html>