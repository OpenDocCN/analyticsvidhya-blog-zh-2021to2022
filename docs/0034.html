<html>
<head>
<title>Apache Spark Discretized Streams (DStreams) with Pyspark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Pyspark的Apache Spark离散化流(数据流)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/apache-spark-discretized-streams-dstreams-with-pyspark-4882026b4fa4?source=collection_archive---------4-----------------------#2021-01-02">https://medium.com/analytics-vidhya/apache-spark-discretized-streams-dstreams-with-pyspark-4882026b4fa4?source=collection_archive---------4-----------------------#2021-01-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex ie if ig ih er es paragraph-image"><div class="ab fe cl ii"><img src="../Images/836aeab067c852987d1e550391c15dab.png" data-original-src="https://miro.medium.com/v2/format:webp/1*fwrEofd4EWfcBPqUApT7SQ.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">火花流</figcaption></figure><h1 id="e995" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">什么是流媒体？</h1><p id="457f" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">试着想象一下。根据互联网实时统计数据，每秒钟有近9000条推文被发送，1000张照片被上传到instagram，超过200万封电子邮件被发送，还有近8万次搜索。</p><p id="d36c" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">如此多的数据从许多来源不间断地生成，并以小数据包的形式同时发送到另一个来源。</p><p id="16c8" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">许多应用还会生成持续更新的数据，如机器人、车辆和许多其他工业和电子设备中使用的传感器，用于监控进度和性能的数据流。</p><p id="8007" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">这就是为什么每秒产生的大量数据必须实时快速处理和分析，这意味着“<strong class="jp hi">流</strong>”。</p><h1 id="1f1d" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">数据流</h1><p id="afc7" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">火花数据流(离散流)是火花流的基本概念。数据流是连续的数据流。数据流从Kafka、Kinesis、Flume、TCP sockets等不同类型的源接收输入，或者在对原始数据进行一些处理后接收输入。DStream也是RDD(弹性分布式数据集)的连续流。数据流中的每个RDD都包含特定时间间隔的数据。Spark流还具有数据流的容错功能，就像rdd一样。</p><p id="16b6" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">让我们看看我们的DStream示例</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="89a7" class="kz iq hh kv b fi la lb l lc ld">from pyspark.context import SparkContext<br/>from pyspark.streaming import StreamingContext<br/>from time import sleep</span><span id="a764" class="kz iq hh kv b fi le lb l lc ld">sc = SparkContext(appName="DStream_QueueStream")<br/>ssc = StreamingContext(sc, 2)<br/>    <br/>rddQueue = []<br/>for i in range(3):<br/>    rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 21)],10)]</span><span id="ae9c" class="kz iq hh kv b fi le lb l lc ld">inputStream = ssc.queueStream(rddQueue)<br/>mappedStream = inputStream.map(lambda x: (x % 10, 1))<br/>reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)<br/>reducedStream.pprint()</span><span id="1533" class="kz iq hh kv b fi le lb l lc ld">ssc.start()<br/>#sleep(6)<br/>ssc.stop(stopSparkContext=True, stopGraceFully=True)</span></pre><p id="c759" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">如您所见，首先，我们创建了一个Spark上下文，然后创建了一个streaming上下文，其中包含“2 ”,这意味着我们希望每2秒钟读取一次流数据。</p><p id="89ca" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我们已经创建了一个输入数据，它是一个从1到20的整数列表。借助Spark上下文并行化，我们将数据分成10个分区，并使用for循环重复3次。最后，我们有一个输入流，每2秒钟读取一次。让我们看看我们的输入数据的结构，我们在Spark流上下文中将其包装为<code class="du lf lg lh kv b">queueStream</code>。</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="7d86" class="kz iq hh kv b fi la lb l lc ld">rddQueue</span><span id="06d9" class="kz iq hh kv b fi le lb l lc ld">[ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262,<br/> ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:262,<br/> ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:262]</span><span id="5dc7" class="kz iq hh kv b fi le lb l lc ld">rddQueue[0].glom().collect()</span><span id="4e70" class="kz iq hh kv b fi le lb l lc ld">[[1, 2],<br/> [3, 4],<br/> [5, 6],<br/> [7, 8],<br/> [9, 10],<br/> [11, 12],<br/> [13, 14],<br/> [15, 16],<br/> [17, 18],<br/> [19, 20]]</span></pre><p id="f31d" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在一些像map和reduce函数这样的转换之后，我们有一个动作<code class="du lf lg lh kv b">pprint</code>，它将开始计算。最后，流从ssc.start()开始，经过6秒钟的睡眠时间后，我们用ssc.stop()命令终止了数据流。</p><p id="385e" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">以下是我们的DStream示例的输出数据:</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="1045" class="kz iq hh kv b fi la lb l lc ld">-------------------------------------------<br/>Time: 2021-01-01 22:57:16<br/>-------------------------------------------<br/>(8, 2)<br/>(0, 2)<br/>(1, 2)<br/>(9, 2)<br/>(2, 2)<br/>(3, 2)<br/>(4, 2)<br/>(5, 2)<br/>(6, 2)<br/>(7, 2)<br/><br/>-------------------------------------------<br/>Time: 2021-01-01 22:57:18<br/>-------------------------------------------<br/>(8, 2)<br/>(0, 2)<br/>(1, 2)<br/>(9, 2)<br/>(2, 2)<br/>(3, 2)<br/>(4, 2)<br/>(5, 2)<br/>(6, 2)<br/>(7, 2)<br/><br/>-------------------------------------------<br/>Time: 2021-01-01 22:57:20<br/>-------------------------------------------<br/>(8, 2)<br/>(0, 2)<br/>(1, 2)<br/>(9, 2)<br/>(2, 2)<br/>(3, 2)<br/>(4, 2)<br/>(5, 2)<br/>(6, 2)<br/>(7, 2)<br/><br/></span></pre><p id="4cba" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">现在让我们试着用另一个例子来证明它</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="63d2" class="kz iq hh kv b fi la lb l lc ld">from pyspark.context import SparkContext<br/>from pyspark.streaming import StreamingContext<br/>from time import sleep</span><span id="5720" class="kz iq hh kv b fi le lb l lc ld">sc=SparkContext("local[*]","StreamingExample")<br/>ssc=StreamingContext(sc,5)<br/>lines=ssc.textFileStream(r'home/data')</span><span id="fcb0" class="kz iq hh kv b fi le lb l lc ld">words=lines.flatMap(lambda x:x.split(" "))<br/>mapped_words=words.map(lambda x:(x,1))<br/>reduced_words=mapped_words.reduceByKey(lambda x,y:x+y)<br/>sorted_words=reduced_words.map(lambda x:(x[1],x[0])).transform(lambda x:x.sortByKey(False))<br/>sorted_words.pprint()</span><span id="e0c3" class="kz iq hh kv b fi le lb l lc ld">ssc.start()<br/>sleep(20)<br/>ssc.stop(stopSparkContext=True, stopGraceFully=True)</span><span id="5a71" class="kz iq hh kv b fi le lb l lc ld">-------------------------------------------<br/>Time: 2021-01-02 00:46:20<br/>-------------------------------------------<br/>(4, 'and')<br/>(4, 'can')<br/>(3, 'data')<br/>(3, 'be')<br/>(2, 'of')<br/>(2, 'like')<br/>(2, 'algorithms')<br/>(2, 'processing')<br/>(2, 'Spark')<br/>(2, 'live')<br/>(2, 'processed')<br/>(1, 'an')<br/>(1, 'high-throughput,')<br/>(1, 'using')<br/> ...<br/><br/>-------------------------------------------<br/>Time: 2021-01-02 00:46:25<br/>-------------------------------------------</span></pre><p id="f5f1" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在第二个例子中，我们有一个文本文件作为输入数据，其中有一小段是关于Spark流的。在对数据(一个著名的单词count☺的例子)进行一些转换后，我们开始流式传输，每5秒读取一次。最重要的部分是，输入数据应该在我们用ssc.start()命令开始流式传输后立即发送。文本文件的最后修改时间必须在流的开始时间之后。</p><h1 id="c057" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">窗口操作</h1><figure class="kq kr ks kt fd ih er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es li"><img src="../Images/04e3086f28ec8fd158f4cb4c9a5cb481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pCgtSmgRVao9qOtSDLt46w.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">基于窗口的操作</figcaption></figure><p id="bbf4" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">最后，我想提一下数据流的窗口操作。它提供在数据的滑动窗口上应用变换。本质上，任何火花窗操作都需要两个主要参数；</p><ul class=""><li id="170a" class="ln lo hh jp b jq kl ju km jy lp kc lq kg lr kk ls lt lu lv bi translated">窗口持续时间，定义窗口的持续时间</li><li id="7262" class="ln lo hh jp b jq lw ju lx jy ly kc lz kg ma kk ls lt lu lv bi translated">滑动持续时间，定义执行窗口操作的持续时间</li></ul><p id="9768" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">关于这两个参数的另一件重要事情是，它们必须是源数据流的批处理间隔的倍数。</p><p id="c3c7" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">让我们试着用下面的例子来说明窗口操作</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="3d33" class="kz iq hh kv b fi la lb l lc ld">from pyspark.context import SparkContext<br/>from pyspark.streaming import StreamingContext</span><span id="cb51" class="kz iq hh kv b fi le lb l lc ld">sc = SparkContext(master=”local[*]”, appName=”WindowWordcount”)<br/>ssc = StreamingContext(sc, 1)<br/>ssc.checkpoint(r”C:\Users\SERCAN\Desktop\databases\checkpoint”)</span><span id="13e4" class="kz iq hh kv b fi le lb l lc ld">lines = ssc.socketTextStream(“localhost”, 9999)<br/>words = lines.flatMap(lambda line: line.split(“ “))<br/>pairs = words.map(lambda word: (word, 1))<br/>pairs.window(10, 5).pprint()</span><span id="6cb1" class="kz iq hh kv b fi le lb l lc ld">ssc.start()<br/><br/>-------------------------------------------<br/>Time: 2021-01-02 01:53:28<br/>-------------------------------------------<br/><br/>-------------------------------------------<br/>Time: 2021-01-02 01:53:33<br/>-------------------------------------------<br/>('foo', 1)<br/><br/>-------------------------------------------<br/>Time: 2021-01-02 01:53:38<br/>-------------------------------------------<br/>('foo', 1)<br/>('bar', 1)<br/><br/>-------------------------------------------<br/>Time: 2021-01-02 01:53:43<br/>-------------------------------------------<br/>('bar', 1)<br/>('baz', 1)<br/><br/>-------------------------------------------<br/>Time: 2021-01-02 01:53:48<br/>-------------------------------------------<br/>('baz', 1)<br/><br/>-------------------------------------------<br/>Time: 2021-01-02 01:53:53<br/>-------------------------------------------</span></pre><p id="f945" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在第三个例子中，我使用socket从命令终端提供输入数据。输入时间为:</p><p id="1f55" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">01:53:30 —福</p><p id="d4cb" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">01时53分35秒—小节</p><p id="4dd3" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">01:53:40 —巴兹</p><p id="30ab" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">正如您在第一个10秒窗口(:28-:38)的示例中看到的，我们只能看到“foo，1 ”,但在第二个窗口中，我们仍然可以看到“foo ”,而且我们现在还可以看到“bar，1 ”,也可以看到:33-:43窗口。在第三个窗口中，这一次“foo”消失了，因为它的窗口关闭了，但是现在我们有了来自新窗口(:38–48)的“baz，1 ”,并且还有用于:33–43窗口的“bar”。最后，对于第四个窗口，现在我们只有“baz ”,因为它的窗口时间结束于:48。</p><h1 id="4869" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">结论</h1><p id="931d" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">在本文中，我试图介绍DStreams，它是Spark Streaming的一个非结构化部分。数据流代表连续的数据流。就像《星火》中的RDD一样，数据流也变得过时了，但了解基础知识或从基础开始总是好的。</p><p id="af14" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我希望这篇文章对你有所帮助。在下一篇文章中，我将讨论Spark Streaming的结构化部分，这是一个构建在Spark SQL引擎上的可伸缩且容错的流处理引擎。</p><p id="fd20" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我将很高兴听到您的任何意见或问题。愿数据伴随你！</p></div></div>    
</body>
</html>