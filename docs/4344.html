<html>
<head>
<title>Computer Vision and Deep Learning -Part 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算机视觉和深度学习——第四部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/computer-vision-and-deep-learning-part-4-dbaf250b8c54?source=collection_archive---------2-----------------------#2021-09-25">https://medium.com/analytics-vidhya/computer-vision-and-deep-learning-part-4-dbaf250b8c54?source=collection_archive---------2-----------------------#2021-09-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="7e6b" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">简介:</h1><p id="6ff0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">欢迎回到计算机视觉和深度学习系列。在过去的几篇文章中，你已经了解了什么是图像，如何执行一些基本的操作，如调整图像大小，改变它的颜色空间和渐变知识，以确定图像的边界。也就是说，到目前为止，我们执行的所有操作都需要机器对主题的零知识来玩图像。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es ka"><img src="../Images/e9b9429c07bda82c1c8b5a7f349df247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NbmhK2MD7CB14Nyrkp_lRQ.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">图4.1美丽孔雀图片来源:<a class="ae kq" href="http://santa-banta-wallpapers.blogspot.com/2014/03/peacock-bird-hd-wallpapers.html" rel="noopener ugc nofollow" target="_blank">访问</a></figcaption></figure><p id="e055" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">我所说的主题是指(参见图4.1)一只<strong class="je hi">孔雀</strong>站在<strong class="je hi">树下的<strong class="je hi">岩石</strong>上的知识。</strong>当人们看到图像时，这些是人们会在图像中观察到的最初几个元素。人类识别图像中内容的能力与人类的智力有关。为了给机器提供这种识别能力，我们需要人工智能。但在我将这些帖子完全引向人工智能之前，我想分享一些与2D功能框架相关的精彩算法，这些算法是由极客们超时理解图像而引入的。同样的问题可以用简单的方法解决，为什么要用复杂的方法呢？人工智能应用程序计算量很大，因此尽可能避免使用它，你的生活会更轻松。</p><h1 id="c5a7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2D特色框架:</h1><p id="92cc" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">如题所示，我们将研究2D图像的特征。这种方法在处理对象识别和对象跟踪时非常方便。回想一下，我们在<a class="ae kq" rel="noopener" href="/analytics-vidhya/computer-vision-and-deep-learning-part-3-862173b3e249">之前的文章</a>中学到了渐变。当你在任何一个方向导航时，像素强度的急剧变化告诉我们边缘的存在。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es kw"><img src="../Images/5d80525265439492cae7a942b1f89de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*GytILsl_Bcp6y_2_4omK-g.gif"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">图4.2矢量表示。图片来源:<a class="ae kq" href="https://www.ducksters.com/science/physics/vector_math.php" rel="noopener ugc nofollow" target="_blank">访问</a></figcaption></figure><p id="0ba0" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">为了存储和进一步操作，该数据(梯度细节)以矢量的形式存储。向量的方向告诉我们图像中像素强度变化的方向，向量的长度告诉我们强度变化的幅度。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es et"><img src="../Images/5c0e1d9bde74ccb19f0fbc61927beacd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u46U3IMo5qucDYGvn8AWwA.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">图4.3关键区域和特征说明。图片来源:<a class="ae kq" href="https://www.wallpaperflare.com/vector-illustration-of-blue-and-white-bird-birds-artwork-simple-background-wallpaper-hxeua" rel="noopener ugc nofollow" target="_blank">访问</a></figcaption></figure><p id="e0ab" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">考虑图4.3，正如人们所观察到的，像素强度的主要变化在图像的左侧。图像的中心和右侧具有恒定的像素强度。因此，我们可以得出结论，图像的关键区域是左侧。将被进一步使用的图像的组成部分被称为特征。在显示的图像中，鸟的翅膀、尾巴、头、喙或腿是我们可能需要在后面的部分中提取或匹配的特征。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es kx"><img src="../Images/524b6ce126256b8a075e304490f82040.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5RFn8oHfVVqJvymGIros-g.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">图4.4考虑了图像的8×8像素部分，并且这个8×8的方框被进一步分成4个块，每个块的尺寸为4×4。在每个4x4块内，图像梯度以向量的形式表示。通过搜索最独特或明显的特征，在图像中找到关键点。这里，通过组合4个相邻向量来形成关键点描述符。关键点描述符显示图像该部分梯度变化的方向和幅度。关键点周围的区域被归一化，并且为关键点区域计算局部描述符。局部描述符是描述关键点视觉外观的数字向量。图片来源:<a class="ae kq" href="https://ai.stanford.edu/~syyeung/cvweb/tutorial2.html" rel="noopener ugc nofollow" target="_blank">访问</a></figcaption></figure><p id="007a" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated"><strong class="je hi">2D特征框架的主要子部分是</strong>:</p><ol class=""><li id="fcbd" class="ky kz hh je b jf kr jj ks jn la jr lb jv lc jz ld le lf lg bi translated">特征检测</li><li id="002c" class="ky kz hh je b jf lh jj li jn lj jr lk jv ll jz ld le lf lg bi translated">特征匹配</li></ol><p id="de27" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">如果你在较低的水平上看图像，即没有聚焦在图像的主体上，图像的突出特征将是角落、边缘和具有最小像素强度变化的一致部分。其中，最重要的是一个角落。猜猜为什么？如果你猜对了，因为它是两条边的交点。矩形(图像是矩形)中的每个角都有独特的强度变化模式(您可以将左下角的形式想象为大写字母l(L，即图像中从右到左和从上到下导航的剧烈强度变化)，右下角为大写字母L的镜像，上面两个以类似方式)。角点是指由于相邻的边而在两个方向上具有严重梯度变化的点。因此，Opencv提供了一些非常酷的方法来检测图像中的角点。让我们把它们列出来</p><h1 id="7bab" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">特征检测方法:</strong></h1><p id="c476" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">a)哈里斯角点检测</p><p id="8073" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">b) Shi-Tomasi角点检测</p><p id="713f" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">c) ORB(定向快速旋转简报)</p><p id="bba9" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">d)角点检测的快速算法</p><p id="8f99" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">e) SIFT(尺度不变特征变换)——专利(付费使用)</p><p id="2c40" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">f) SURF(加速的强大功能)—专利(付费使用)</p><p id="2c47" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">(SIFT和SURF已获得专利，不能用于免费商业用途，因此我们将跳过这两种方法的演示)</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es lm"><img src="../Images/3f245ff757ea8e3fa357dee00de4d600.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKrabmve5c21r5Gl2pkQxQ.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">图4.5用于角点检测算法实现的图像。图片来源:<a class="ae kq" href="https://www.gocollette.com/en-ca/traveling-well/2019/1/the-beautiful-birds-of-costa-rica" rel="noopener ugc nofollow" target="_blank">访问</a></figcaption></figure><h1 id="b863" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">哈里斯角点检测:</h1><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es ln"><img src="../Images/aa5ebf5f7896c3c3e88182aa92351764.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*zxeJ9ZkHRtH0MByOnv0t3g.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">哈里斯角点检测背后的主要等式</figcaption></figure><p id="67c4" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">让我们理解这个等式是什么意思。E(u，v)是一个需要被最大化以检测拐角的函数。该等式基本上找到了(<em class="lo"> u </em>，<em class="lo"> v </em>)在所有方向上的位移的强度差异。例如，如果我们取u为2，v为3，我们将不断改变(x，y)值，并计算像素强度的平方差。强度差相乘的窗口函数是内核(如果你不理解内核，请参考前面的)，这里是矩形或高斯窗口，它为下面的像素赋予权重。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lp"><img src="../Images/709351e6662028e4116f8c2c92166ad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*y4HbrRdUepEFWVh-kUFNUw.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">方程的泰勒展开式</figcaption></figure><p id="2193" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">使用泰勒展开求解E(u，v)方程，以获得上面的方程，其中M代表</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lq"><img src="../Images/63560ac5474298891736d409064c66d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*w64YxYLNi2nvyr9uDT7AcA.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">这里Ix和Iy代表x和y方向上的图像导数</figcaption></figure><p id="5276" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">等式的主要部分是R</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lr"><img src="../Images/13775a857886a1c67338c0b74935fc9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*RbVffCsrvSSgNo159EdPbA.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">r决定下面的像素是包含角点、边缘还是普通图像。</figcaption></figure><p id="12f0" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">如果我们把L1和L2看作矩阵M的特征值，det(M )=特征值的乘积(L1×L2)和trace(M)=特征值的和(L1+L2)。根据该方法，R的值决定了待测区域的内容。如果</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es kx"><img src="../Images/43e8070a74976ef7b1e0a939d6521dd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wfuZSUFcSZBjUM-0I6KYog.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">图4.6哈里斯角点检测的图示</figcaption></figure><ol class=""><li id="39b7" class="ky kz hh je b jf kr jj ks jn la jr lb jv lc jz ld le lf lg bi translated">R值很小，所考虑的地区是平坦的(L1和L2很小)</li><li id="a393" class="ky kz hh je b jf lh jj li jn lj jr lk jv ll jz ld le lf lg bi translated">r为负，所考虑的区域包含边缘(当L1与L2相比太大时，反之亦然)</li><li id="2c6e" class="ky kz hh je b jf lh jj li jn lj jr lk jv ll jz ld le lf lg bi translated">r较大，区域包含拐角(L1和L2较大且可比较)</li></ol><h2 id="4f42" class="ls if hh bd ig lt lu lv ik lw lx ly io jn lz ma is jr mb mc iw jv md me ja mf bi translated">哈里斯角检测代码:</h2><pre class="kb kc kd ke fd mg mh mi mj aw mk bi"><span id="d9f9" class="ls if hh mh b fi ml mm l mn mo">import cv2<br/>import numpy as np</span><span id="3ddb" class="ls if hh mh b fi mp mm l mn mo">#read the original image<br/>cv_image = cv2.imread("/home/rupali/tutorials/blue_flower.jpg")</span><span id="614c" class="ls if hh mh b fi mp mm l mn mo">#convert into gray scale<br/>gray_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)</span><span id="9e5b" class="ls if hh mh b fi mp mm l mn mo">#convert image pixel data into float32, to avoid further size not compatible clashes<br/>gray_image = np.float32(gray_image)</span><span id="1a4a" class="ls if hh mh b fi mp mm l mn mo">#syntax cv2.corenrHarris(input_image, block size for neighborhood pixels to be considered, sobel operator size, border type)<br/>result_image = cv2.cornerHarris(gray_image, blockSize=2, ksize= 3, k =0.04)</span><span id="bb1f" class="ls if hh mh b fi mp mm l mn mo">#dilate to highlight corners<br/>result_image = cv2.dilate(result_image, None)</span><span id="3d35" class="ls if hh mh b fi mp mm l mn mo">#reverting back to original image using optimal threshold <br/>cv_image[result_image &gt; 0.01* result_image.max()]=[0,0, 255]</span><span id="ef2a" class="ls if hh mh b fi mp mm l mn mo">cv2.imshow("haris", cv_image)<br/>cv2.waitKey()</span></pre><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es lm"><img src="../Images/6dca0f876636500eec4030425f88ae78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*72U-573X27UkqR3EGftrTQ.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">哈里斯角点检测结果</figcaption></figure><h1 id="50c5" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">Shi-Tomasi角点检测方法:</h1><p id="4536" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这种角点检测方法类似于Harris。Shi-Tomasi在他的研究论文<strong class="je hi"> Good Features to Track </strong>中提出寻找图像中N个最强的角点。得分函数如下所示</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es mq"><img src="../Images/2e0b19ce9f6a021f34eae5b3dd561b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*t3LBngNg1GfWpcPhjNMUqg.png"/></div></figure><p id="80b0" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">实际上，施托马西比哈里斯更有效率。我们可以形象地把这种方法想象成</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es kx"><img src="../Images/f5c509298e4542bf2f1f0f47eb1c4de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BxqZVUG80vYnm3msZSnvbQ.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">寻找最强N角点的Shi-Tomasi方法</figcaption></figure><pre class="kb kc kd ke fd mg mh mi mj aw mk bi"><span id="7264" class="ls if hh mh b fi ml mm l mn mo">import numpy as np<br/>import cv2</span><span id="17ca" class="ls if hh mh b fi mp mm l mn mo">#input image</span><span id="4881" class="ls if hh mh b fi mp mm l mn mo">cv_img = cv2.imread('/home/rupali/tutorials/bird.jpg')<br/>cv_gray = cv2.cvtColor(cv_img,cv2.COLOR_BGR2GRAY)</span><span id="f359" class="ls if hh mh b fi mp mm l mn mo">#syntax cv2.goodFeaturesToTrack(input_image, max_corner_to_detect, qualityLevel, minDistance)</span><span id="51ca" class="ls if hh mh b fi mp mm l mn mo">corners = cv2.goodFeaturesToTrack(cv_gray,maxCorners=25, qualityLevel=0.01,minDistance=10)<br/>corners = np.float32(corners)</span><span id="4801" class="ls if hh mh b fi mp mm l mn mo">for item in corners:<br/>    x,y = item[0]<br/>    cv2.circle(cv_img,(x,y),3,(0,0,255),-1)</span><span id="c248" class="ls if hh mh b fi mp mm l mn mo">cv2.imshow("image", cv_img)<br/>cv2.imwrite("shi_result.jpg", cv_img)<br/>cv2.waitKey()</span></pre><h2 id="64b2" class="ls if hh bd ig lt lu lv ik lw lx ly io jn lz ma is jr mb mc iw jv md me ja mf bi translated">结果:</h2><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es lm"><img src="../Images/e51447e7368e24983b2b07f7d31bbff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ut9GGqeRaraqGrwQml19GA.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">N=25时的Shi-Tomasi结果</figcaption></figure><h1 id="a3ed" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">ORB(定向快速旋转简报):</h1><p id="fc68" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">该方法是SIFT和SURF的有效替代方法。ORB的出现主要是因为SIFT和SURF获得了专利，因此需要一种有效的开源方法来开发ORB。ORB使用FAST检测关键点，使用BRIEF计算图像描述符。</p><pre class="kb kc kd ke fd mg mh mi mj aw mk bi"><span id="d7e8" class="ls if hh mh b fi ml mm l mn mo">import numpy as np<br/>import cv2<br/>#input image<br/>cv_img = cv2.imread('/home/rupali/tutorials/bird.jpg')<br/>cv_gray = cv2.cvtColor(cv_img,cv2.COLOR_BGR2GRAY)</span><span id="082c" class="ls if hh mh b fi mp mm l mn mo">orb = cv2.ORB_create(nfeatures=200)<br/>key_point, descriptors = orb.detectAndCompute(cv_gray, None)</span><span id="0ad2" class="ls if hh mh b fi mp mm l mn mo">keypoint_image = cv2.drawKeypoints(cv_img, key_point, None,color=(0,0,255), flags=0)</span><span id="fee6" class="ls if hh mh b fi mp mm l mn mo">cv2.imshow("ORB", keypoint_image)<br/>cv2.imwrite("orb.jpg", keypoint_image)<br/>cv2.waitKey()</span></pre><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es lm"><img src="../Images/0d6a31a33c615fc0f26118143d702bfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_29vNii0fR9sfkJkFDaDyg.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">特征数量= 200的ORB结果</figcaption></figure><h2 id="f39d" class="ls if hh bd ig lt lu lv ik lw lx ly io jn lz ma is jr mb mc iw jv md me ja mf bi translated">角点检测的快速算法；</h2><p id="6cd4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">上面讨论的角点检测技术很好，但是对于实时应用来说不够快。作为对此的解决方案，FAST(Features from Accelerated Segment Test)算法是由Edward Rosten和Tom Drummond于2006年在他们的论文《用于高速角点检测的机器学习》中提出的(后来在2010年对其进行了修订)。</p><p id="6cbe" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">加速分段测试的特征总结如下:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es kx"><img src="../Images/cac00bda99530c656bc18c24afb48aae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FwyB2EeO9uzbpG9PSAQYyw.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">快速算法解释</figcaption></figure><ol class=""><li id="93f2" class="ky kz hh je b jf kr jj ks jn la jr lb jv lc jz ld le lf lg bi translated">在图像中选择一个像素p，该像素是否被识别为兴趣点。让它的强度是Ip。</li><li id="ddfc" class="ky kz hh je b jf lh jj li jn lj jr lk jv ll jz ld le lf lg bi translated">选择合适的阈值t</li><li id="a62f" class="ky kz hh je b jf lh jj li jn lj jr lk jv ll jz ld le lf lg bi translated">考虑在测试像素周围有一个16像素的圆圈(见下图)。</li><li id="d5c7" class="ky kz hh je b jf lh jj li jn lj jr lk jv ll jz ld le lf lg bi translated">现在，如果在圆(16个像素)中存在一组n个相邻像素，它们都比Ip+t亮，或者都比Ip t暗，则像素p就是一个角点(在上图中显示为白色虚线)。n被选为12。</li><li id="f660" class="ky kz hh je b jf lh jj li jn lj jr lk jv ll jz ld le lf lg bi translated">提出了一个<strong class="je hi">高速测试</strong>排除大量非弯道。该测试仅检查1、9、5和13处的四个像素(首先测试1和9是否太亮或太暗。如果是，则检查5和13)。如果p是一个角点，则其中至少有三个必须比Ip+t亮或比Ip t暗，如果两者都不是，则p不可能是一个角点。然后，通过检查圆圈中的所有像素，可以将完整片段测试标准应用于通过的候选。</li></ol><p id="0124" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">在必须在图像的相同区域中执行多个特征的检测的情况下，FAST不会执行得很好。为此，使用非最大抑制。</p><p id="05d3" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">在非最大抑制中，为所有检测到的特征点计算得分函数V。</p><ol class=""><li id="69d6" class="ky kz hh je b jf kr jj ks jn la jr lb jv lc jz ld le lf lg bi translated">v是p(考虑中的主像素)和16个周围像素值之间的绝对差值的和。</li><li id="7a4d" class="ky kz hh je b jf lh jj li jn lj jr lk jv ll jz ld le lf lg bi translated">考虑两个相邻的关键点，并计算它们的V值。</li><li id="508a" class="ky kz hh je b jf lh jj li jn lj jr lk jv ll jz ld le lf lg bi translated">丢弃V值较低的那个。</li></ol><p id="b545" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">简而言之，FAST比许多现有的特征检测器更快，但是在存在高水平噪声的情况下表现不佳。主要是因为像素值会因为高水平的噪声而改变。</p><pre class="kb kc kd ke fd mg mh mi mj aw mk bi"><span id="5bd3" class="ls if hh mh b fi ml mm l mn mo">import numpy as np<br/>import cv2<br/>#input image<br/>cv_img = cv2.imread('/home/rupali/tutorials/bird.jpg')<br/>cv_gray = cv2.cvtColor(cv_img,cv2.COLOR_BGR2GRAY)</span><span id="aff7" class="ls if hh mh b fi mp mm l mn mo">fast = cv2.FastFeatureDetector_create()<br/>fast.setNonmaxSuppression(False)</span><span id="1b1e" class="ls if hh mh b fi mp mm l mn mo">keypoint = fast.detect(cv_gray, None)<br/>keypoint_image = cv2.drawKeypoints(cv_img, keypoint, None, color=(0,0,255))</span><span id="9383" class="ls if hh mh b fi mp mm l mn mo">cv2.imshow("FAST", keypoint_image)<br/>cv2.imwrite("fast.jpg", keypoint_image)<br/>cv2.waitKey()</span></pre><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es lm"><img src="../Images/9a33e7eb264c2b6fd172b1585914793f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zsqkCDTQrizwU_AvmzpVCQ.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">快速角点检测器结果</figcaption></figure><h1 id="341f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">特征匹配</h1><p id="6c88" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Opencv文档提到了两种特征匹配方法。即强力匹配器和FLANN匹配器。</p><h2 id="8363" class="ls if hh bd ig lt lu lv ik lw lx ly io jn lz ma is jr mb mc iw jv md me ja mf bi translated">强力匹配器:</h2><p id="be4b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">蛮力匹配器很简单。它采用第一组中一个特征的描述符，并使用某种距离计算(多种距离计算方法)与第二组中的所有其他特征进行匹配。并且返回最接近的一个。</p><pre class="kb kc kd ke fd mg mh mi mj aw mk bi"><span id="66a0" class="ls if hh mh b fi ml mm l mn mo">import cv2</span><span id="7b31" class="ls if hh mh b fi mp mm l mn mo">cv_img1 = cv2.imread('/home/rupali/tutorials/bird.jpg', 0)<br/>cv_img2 = cv2.imread('/home/rupali/tutorials/bird_rotated.jpg', 0)</span><span id="3171" class="ls if hh mh b fi mp mm l mn mo">orb = cv2.ORB_create(nfeatures=500)<br/>kp1, des1 = orb.detectAndCompute(cv_img1, None)<br/>kp2, des2 = orb.detectAndCompute(cv_img2, None)</span><span id="f2be" class="ls if hh mh b fi mp mm l mn mo"># matcher takes normType, which is set to cv2.NORM_L2 for SIFT and SURF, cv2.NORM_HAMMING for ORB, FAST and BRIEF<br/>bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)<br/>matches = bf.match(des1, des2)<br/>matches = sorted(matches, key=lambda x: x.distance)# draw first 50 matches<br/>match_img = cv2.drawMatches(cv_img1, kp1, cv_img2, kp2, matches[:50], None)<br/>cv2.imshow('Matches', match_img)<br/>cv2.imwrite("Match.jpg", match_img)<br/>cv2.waitKey()</span></pre><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es mr"><img src="../Images/f70309a531ac7fe4c09affeeba9e1fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oq1lwk8CoxUFMIFE57Ep8g.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">原始图像和300度旋转图像之间的暴力图像特征匹配器</figcaption></figure><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es mr"><img src="../Images/cbc47a1fd73912aedaba09e673ee5600.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6y-s8Bhs8P7CZzMpy5sskA.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">即使在彩色空间也有明显的良好性能</figcaption></figure><h2 id="5027" class="ls if hh bd ig lt lu lv ik lw lx ly io jn lz ma is jr mb mc iw jv md me ja mf bi translated">弗兰匹配器:</h2><p id="9645" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">FLANN使用SIFT检测器来获取关键点。为了避免SIFT、SURF的使用，我们将通过使用ORB检测器和通过校正扭曲的图像来实现匹配操作。</p><pre class="kb kc kd ke fd mg mh mi mj aw mk bi"><span id="f7d6" class="ls if hh mh b fi ml mm l mn mo">import cv2<br/>import numpy as np</span><span id="214b" class="ls if hh mh b fi mp mm l mn mo">def get_corrected_img(cv_img1, cv_img2):<br/>    MIN_MATCHES = 50</span><span id="2ba7" class="ls if hh mh b fi mp mm l mn mo">orb = cv2.ORB_create(nfeatures=500)<br/>    kp1, des1 = orb.detectAndCompute(cv_img1, None)<br/>    kp2, des2 = orb.detectAndCompute(cv_img2, None)</span><span id="d119" class="ls if hh mh b fi mp mm l mn mo">index_params = dict(algorithm=6,<br/>                        table_number=6,<br/>                        key_size=12,<br/>                        multi_probe_level=2)<br/>    search_params = {}<br/>    flann = cv2.FlannBasedMatcher(index_params, search_params)<br/>    matches = flann.knnMatch(des1, des2, k=2)</span><span id="17fd" class="ls if hh mh b fi mp mm l mn mo"># As per Lowe's ratio test to filter good matches<br/>    good_matches = []<br/>    for m, n in matches:<br/>        if m.distance &lt; 0.75 * n.distance:<br/>            good_matches.append(m)</span><span id="d0b8" class="ls if hh mh b fi mp mm l mn mo">if len(good_matches) &gt; MIN_MATCHES:<br/>        src_points = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)<br/>        dst_points = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)<br/>        m, mask = cv2.findHomography(src_points, dst_points, cv2.RANSAC, 5.0)<br/>        corrected_img = cv2.warpPerspective(cv_img1, m, (cv_img2.shape[1], cv_img2.shape[0]))</span><span id="0d6b" class="ls if hh mh b fi mp mm l mn mo">return corrected_img<br/>    return img2</span><span id="e462" class="ls if hh mh b fi mp mm l mn mo">if __name__ == "__main__":</span><span id="f19a" class="ls if hh mh b fi mp mm l mn mo">cv_im1 = cv2.imread('/home/rupali/tutorials/bird.jpg')<br/>    cv_im2 = cv2.imread('/home/rupali/tutorials/bird_rotated.jpg')</span><span id="1867" class="ls if hh mh b fi mp mm l mn mo">img = get_corrected_img(cv_im2, cv_im1)<br/>    cv2.imshow('Corrected image', img)<br/>    cv2.imwrite("corrected_image.jpg", img)<br/>    <br/>    cv2.waitKey()</span></pre><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es lm"><img src="../Images/90878a1d5ecea91b5070c01dfc1be6ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jAK7xHdebWsXnuesWPDTUQ.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">根据基础图像的方向呈现300度旋转图像。</figcaption></figure><h1 id="8a51" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论:</h1><p id="5400" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这就是核心opencv应用程序！提醒一下，讨论仅限于opencv最有用的特性。在接下来的文章中，我将从机器学习开始，解释为什么我决定继续进行深度学习。会很有趣的！在那之前，敬请关注，继续学习！</p></div></div>    
</body>
</html>