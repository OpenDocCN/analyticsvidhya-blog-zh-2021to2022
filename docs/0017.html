<html>
<head>
<title>Neural Style Transfer Part 2 : Fast Style Transfer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经风格转移第2部分:快速风格转移</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-style-transfer-part-2-fast-style-transfer-c2654f854f4?source=collection_archive---------16-----------------------#2021-01-01">https://medium.com/analytics-vidhya/neural-style-transfer-part-2-fast-style-transfer-c2654f854f4?source=collection_archive---------16-----------------------#2021-01-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/20eb45346ab02d2f8a5d3eb29258473d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4XwQ0vrR01KW7WnT.jpg"/></div></div></figure><p id="18b3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是风格转换系列的第2部分，在这里我们用TensorFlow的理论和实现来介绍使用自动编码器的快速风格转换。</p><p id="d42f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是神经类型转换的第二部分，在这一部分中，我们将讨论另一种类型转换的技术，我们称之为快速类型转换。这是对<a class="ae jn" href="https://tarunbisht11.medium.com/neural-style-transfer-part-1-introduction-dc17a3eb86d2" rel="noopener">上一篇文章</a>的跟进，如果你正在直接阅读它的第二部分，那么我建议你先阅读<a class="ae jn" href="https://tarunbisht11.medium.com/neural-style-transfer-part-1-introduction-dc17a3eb86d2" rel="noopener">上一部分</a>，因为许多话题都是从那篇文章跟进的。</p><p id="012e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在gatys风格转移中，我们没有训练任何网络，我们只是根据损失函数(style_loss + content_loss)优化输出图像，并且优化需要一些轮次，因此生成风格化图像是一个非常缓慢的过程。将这种技术用于实时视频😭别提了。</p><p id="08e5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这似乎是一个迭代过程，如果我们想要生成多个相同样式的图像，因为我们每次都在为相同样式的图像优化输出图像。如果有一种方法可以学习样式图像的输入-输出映射，那么我们可以一次性生成该样式的图像。🤔是的，我们可以使用自动编码器来学习输入图像和样式化输出图像之间的映射，方法是使用之前定义的损失函数来训练它。</p><p id="66ea" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">快速风格转移让我们训练一次，并生成无限的图像，是的，我们可以用它来设计视频，甚至是实时网络视频。</p><h1 id="590c" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">要点</h1><p id="74c6" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">快速的风格转换让我们一次训练就能生成无限的图像。我们讨论的关于损失函数理论的大部分要点是相同的，这里的主要区别是我们将更多地关注使用损失函数的训练模型和学习映射。</p><p id="0e5a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在阅读这篇文章之前，先复习一下关于自动编码器的知识，特别是卷积自动编码器和深度学习中的残差层(跳过连接),因为我不会解释它们，但我们将在这里实现它们，所以先复习一些关于卷积自动编码器和残差层的基本知识，这将有助于轻松理解实现</p><p id="1686" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">1.我们训练一个前馈网络，使用<a class="ae jn" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank"> Gatys等人</a>论文中定义的损失函数将艺术风格应用于图像，更多解释参见<a class="ae jn" href="https://tarunbisht11.medium.com/neural-style-transfer-part-1-introduction-dc17a3eb86d2" rel="noopener">之前的文章</a>。</p><p id="584e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">2.我们将使用的前馈网络是一个残差自动编码器网络，它将内容图像作为输入，并输出一个风格化的图像。这与在<a class="ae jn" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">原始实现</a>中使用的网络相同</p><p id="10d9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">3.Model还使用实例规范化，而不是基于纸张实例规范化的批处理规范化:快速样式化缺少的要素，因为这提供了更好的结果。</p><p id="469f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">4.我们将使用vgg19来计算感知损失更多的工作描述在纸上。</p><p id="47e1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果现在有人想尝试视频和图像的风格转换，我已经为同样的目的创建了一个<a class="ae jn" href="https://github.com/tarun-bisht/fast-style-transfer" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>，并附有说明。</p><h1 id="dac4" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">导入必要的模块</h1><p id="e7bb" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">让我们从导入所有必需的模块开始:</p><ul class=""><li id="ff87" class="kr ks hh ir b is it iw ix ja kt je ku ji kv jm kw kx ky kz bi translated">numpy:用于数组操作</li><li id="947e" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">张量流:用于张量运算</li><li id="32b0" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">tensor flow . keras:tensor flow的高级神经网络库，用于创建神经网络</li><li id="4745" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">pillow:用于将图像转换为numpy数组，将numpy数组转换为图像，保存输出图像。</li><li id="023a" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">时间:用于计算每次迭代的时间</li><li id="d3e4" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">matplotlib:用于显示笔记本中的图像和图形</li><li id="93f9" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">请求，base64，io:用于从URL下载和加载图像</li><li id="3ab4" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">os:操作系统级命令</li></ul><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="8945" class="lo jp hh lk b fi lp lq l lr ls"><strong class="lk hi">import</strong> numpy <strong class="lk hi">as</strong> np<br/><strong class="lk hi">import</strong> tensorflow <strong class="lk hi">as</strong> tf<br/><strong class="lk hi">from</strong> tensorflow.keras.applications <strong class="lk hi">import</strong> vgg19<br/><strong class="lk hi">from</strong> tensorflow.keras.models <strong class="lk hi">import</strong> load_model,Model<br/><strong class="lk hi">from</strong> PIL <strong class="lk hi">import</strong> Image<br/><strong class="lk hi">import</strong> time<br/><strong class="lk hi">import</strong> matplotlib.pyplot <strong class="lk hi">as</strong> plt<br/><strong class="lk hi">import</strong> matplotlib<br/><strong class="lk hi">import</strong> requests<br/><strong class="lk hi">import</strong> base64<br/><strong class="lk hi">import</strong> os<br/><strong class="lk hi">from</strong> pathlib <strong class="lk hi">import</strong> Path<br/><strong class="lk hi">from</strong> io <strong class="lk hi">import</strong> BytesIO<br/>matplotlib.rcParams['figure.figsize'] = (12,12)<br/>matplotlib.rcParams['axes.grid'] = <strong class="lk hi">False</strong></span></pre><h1 id="af63" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">定义效用函数</h1><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="7e3e" class="lo jp hh lk b fi lp lq l lr ls"><strong class="lk hi">def</strong> <strong class="lk hi">load_image</strong>(image_path, dim=None, resize=False):<br/>    img= Image.open(image_path)<br/>    <strong class="lk hi">if</strong> dim:<br/>        <strong class="lk hi">if</strong> resize:<br/>            img=img.resize(dim)<br/>        <strong class="lk hi">else</strong>:<br/>            img.thumbnail(dim)<br/>    img= img.convert("RGB")<br/>    <strong class="lk hi">return</strong> np.array(img)</span></pre><p id="c8e8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上面的函数用于从指定的路径加载图像并将其转换成numpy数组</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="b9d4" class="lo jp hh lk b fi lp lq l lr ls"><strong class="lk hi">def</strong> <strong class="lk hi">load_url_image</strong>(url,dim=None,resize=False):<br/>    img_request=requests.get(url)<br/>    img= Image.open(BytesIO(img_request.content))<br/>    <strong class="lk hi">if</strong> dim:<br/>        <strong class="lk hi">if</strong> resize:<br/>            img=img.resize(dim)<br/>        <strong class="lk hi">else</strong>:<br/>            img.thumbnail(dim)<br/>    img= img.convert("RGB")<br/>    <strong class="lk hi">return</strong> np.array(img)</span></pre><p id="6591" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这个函数从URL加载图像，并将其转换成numpy数组</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="b35e" class="lo jp hh lk b fi lp lq l lr ls"><strong class="lk hi">def</strong> <strong class="lk hi">array_to_img</strong>(array):<br/>    array=np.array(array,dtype=np.uint8)<br/>    <strong class="lk hi">if</strong> np.ndim(array)&amp;gt;3:<br/>        <strong class="lk hi">assert</strong> array.shape[0]==1<br/>        array=array[0]<br/>    <strong class="lk hi">return</strong> Image.fromarray(array)</span><span id="02cc" class="lo jp hh lk b fi lt lq l lr ls"><strong class="lk hi">def</strong> <strong class="lk hi">show_image</strong>(image,title=None):<br/>    <strong class="lk hi">if</strong> len(image.shape)&amp;gt;3:<br/>        image=tf.squeeze(image,axis=0)<br/>    plt.imshow(image)<br/>    <strong class="lk hi">if</strong> title:<br/>        plt.title=title</span><span id="d013" class="lo jp hh lk b fi lt lq l lr ls"><strong class="lk hi">def</strong> <strong class="lk hi">plot_images_grid</strong>(images,num_rows=1):<br/>    n=len(images)<br/>    <strong class="lk hi">if</strong> n &amp;gt; 1:<br/>        num_cols=np.ceil(n/num_rows)<br/>        fig,axes=plt.subplots(ncols=int(num_cols),nrows=int(num_rows))<br/>        axes=axes.flatten()<br/>        fig.set_size_inches((15,15))<br/>        <strong class="lk hi">for</strong> i,image <strong class="lk hi">in</strong> enumerate(images):<br/>            axes[i].imshow(image)<br/>    <strong class="lk hi">else</strong>:<br/>        plt.figure(figsize=(10,10))<br/>        plt.imshow(images[0])</span></pre><p id="4007" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">以上三个函数用于转换和绘制图像:</p><ul class=""><li id="4ca4" class="kr ks hh ir b is it iw ix ja kt je ku ji kv jm kw kx ky kz bi translated">将一个数组转换成图像</li><li id="a2f1" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">show_image:绘制单个图像</li><li id="b58f" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">plot_images_grid:在网格中绘制批量图像</li></ul><h1 id="9b73" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">快速风格转换的步骤</h1><p id="4a26" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">训练模型是具有残差层的编码器-解码器架构。输入图像被传递到编码器部分，并传播到解码器部分。输出与输入大小相同，并显示生成的图像。</p><p id="7472" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这个模型是根据一种称为感知损失的损失训练的，这种损失的计算方法与我们在gatys风格转移中计算的方法相同。使用预训练模型从定义的样式和内容层提取特征图，并使用它们来计算样式损失和内容损失。(更多细节请阅读<a class="ae jn" href="https://tarunbisht11.medium.com/neural-style-transfer-part-1-introduction-dc17a3eb86d2" rel="noopener">之前的帖子</a>，那里有解释)</p><p id="2a69" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">作为训练模型的一部分，我们需要训练数据，对于训练模型，我们需要不同图像的数据集(可以是任何像人、狗、汽车等)..)散装。在这篇文章中，我们使用的是<a class="ae jn" href="http://images.cocodataset.org/zips/train2014.zip" rel="noopener ugc nofollow" target="_blank">可可数据集</a>，它有很多图片。我也使用过<a class="ae jn" href="https://www.kaggle.com/c/gan-getting-started" rel="noopener ugc nofollow" target="_blank"> Kaggle挑战数据集</a>，它有不同风景的图像，你可以在这里检查代码内核<a class="ae jn" href="https://www.kaggle.com/tarunbisht11/generate-art-using-fast-style-transfer-in-a-second" rel="noopener ugc nofollow" target="_blank">。我们还需要一个样式图像，我们希望使用autoencoder学习它的样式。我们可以使用任何绘画或素描(从互联网上选择一个)</a></p><p id="09da" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于训练，该模型我们将一批各种内容的输入训练图像发送到autoencoder，auto encoder为我们提供输出。该输出必须是我们的风格图像，而训练时，我们将这些输出图像分批传递到我们的损失模型(vgg19 ),并提取不同层(内容层和风格层)的特征。这些特征然后用于计算风格损失和内容损失，其加权和产生训练网络的感知损失。下面这张来自论文的图片很好地描述了它。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lu"><img src="../Images/8d597d6d43c6576d28c7327ead5f3c5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UlLnY4Vb4jbBF7Gd"/></div></div></figure><p id="a15f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">经过训练后，我们可以使用该网络一次性设计任何图像，而无需优化</p><p id="d834" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该网络的主要亮点:</p><ul class=""><li id="a587" class="kr ks hh ir b is it iw ix ja kt je ku ji kv jm kw kx ky kz bi translated">残留层</li><li id="b895" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">编码器-解码器模型</li><li id="b0d0" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">解码器的输出被传递到损耗模型(VGG)以计算损耗</li><li id="a554" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">训练需要计算，因为我们每一步都要将这些图像传递给两个网络</li></ul><h1 id="b223" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">定义损失</h1><p id="f752" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">为了计算风格损失和内容损失，我们需要一个预训练的模型，我们使用vgg19，原始实现使用vgg16。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="052a" class="lo jp hh lk b fi lp lq l lr ls">vgg=vgg19.VGG19(weights='imagenet',include_top=<strong class="lk hi">False</strong>)<br/>vgg.summary()</span><span id="a08c" class="lo jp hh lk b fi lt lq l lr ls">Model: "vgg19"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_1 (InputLayer)         [(<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 3)]   0         <br/>_________________________________________________________________<br/>block1_conv1 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 64)    1792      <br/>_________________________________________________________________<br/>block1_conv2 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 64)    36928     <br/>_________________________________________________________________<br/>block1_pool (MaxPooling2D)   (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 64)    0         <br/>_________________________________________________________________<br/>block2_conv1 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 128)   73856     <br/>_________________________________________________________________<br/>block2_conv2 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 128)   147584    <br/>_________________________________________________________________<br/>block2_pool (MaxPooling2D)   (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 128)   0         <br/>_________________________________________________________________<br/>block3_conv1 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 256)   295168    <br/>_________________________________________________________________<br/>block3_conv2 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 256)   590080    <br/>_________________________________________________________________<br/>block3_conv3 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 256)   590080    <br/>_________________________________________________________________<br/>block3_conv4 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 256)   590080    <br/>_________________________________________________________________<br/>block3_pool (MaxPooling2D)   (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 256)   0         <br/>_________________________________________________________________<br/>block4_conv1 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 512)   1180160   <br/>_________________________________________________________________<br/>block4_conv2 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 512)   2359808   <br/>_________________________________________________________________<br/>block4_conv3 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 512)   2359808   <br/>_________________________________________________________________<br/>block4_conv4 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 512)   2359808   <br/>_________________________________________________________________<br/>block4_pool (MaxPooling2D)   (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 512)   0         <br/>_________________________________________________________________<br/>block5_conv1 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 512)   2359808   <br/>_________________________________________________________________<br/>block5_conv2 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 512)   2359808   <br/>_________________________________________________________________<br/>block5_conv3 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 512)   2359808   <br/>_________________________________________________________________<br/>block5_conv4 (Conv2D)        (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 512)   2359808   <br/>_________________________________________________________________<br/>block5_pool (MaxPooling2D)   (<strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, <strong class="lk hi">None</strong>, 512)   0         <br/>=================================================================<br/>Total params: 20,024,384<br/>Trainable params: 20,024,384<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="df4b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这里，我们定义将用于计算损失的层。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="13f5" class="lo jp hh lk b fi lp lq l lr ls">content_layers=['block4_conv2']</span><span id="e4c2" class="lo jp hh lk b fi lt lq l lr ls">style_layers=['block1_conv1',<br/>            'block2_conv1',<br/>            'block3_conv1',<br/>            'block4_conv1',<br/>            'block5_conv1']</span></pre><p id="8a34" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们定义一个类，该类使用一些额外的方法来创建损失模型，以便从网络访问要素地图。我们在<a class="ae jn" href="https://tarunbisht11.medium.com/neural-style-transfer-part-1-introduction-dc17a3eb86d2" rel="noopener">之前的文章</a>中也使用过这些函数，这里我们只是将它们封装在一个类中。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="0aa2" class="lo jp hh lk b fi lp lq l lr ls"><strong class="lk hi">class</strong> <strong class="lk hi">LossModel</strong>:<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">__init__</strong>(self,pretrained_model,content_layers,style_layers):<br/>        self.model=pretrained_model<br/>        self.content_layers=content_layers<br/>        self.style_layers=style_layers<br/>        self.loss_model=self.get_model()</span><span id="a0cf" class="lo jp hh lk b fi lt lq l lr ls">    <strong class="lk hi">def</strong> <strong class="lk hi">get_model</strong>(self):<br/>        self.model.trainable=<strong class="lk hi">False</strong><br/>        layer_names=self.style_layers + self.content_layers<br/>        outputs=[self.model.get_layer(name).output <strong class="lk hi">for</strong> name <strong class="lk hi">in</strong> layer_names]<br/>        new_model=Model(inputs=self.model.input,outputs=outputs)<br/>        <strong class="lk hi">return</strong> new_model<br/>    <br/>    <strong class="lk hi">def</strong> <strong class="lk hi">get_activations</strong>(self,inputs):<br/>        inputs=inputs*255.0<br/>        style_length=len(self.style_layers)<br/>        outputs=self.loss_model(vgg19.preprocess_input(inputs))<br/>        style_output,content_output=outputs[:style_length],outputs[style_length:]<br/>        content_dict={name:value <strong class="lk hi">for</strong> name,value <strong class="lk hi">in</strong> zip(self.content_layers,content_output)}<br/>        style_dict={name:value <strong class="lk hi">for</strong> name,value <strong class="lk hi">in</strong> zip(self.style_layers,style_output)}<br/>        <strong class="lk hi">return</strong> {'content':content_dict,'style':style_dict}</span></pre><p id="7b78" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们使用上面的类创建我们的损失模型</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="493d" class="lo jp hh lk b fi lp lq l lr ls">loss_model = LossModel(vgg, content_layers, style_layers)</span></pre><p id="f9ee" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们定义损失函数来计算内容和风格损失，下面方法<code class="du lv lw lx lk b">content_loss</code>和<code class="du lv lw lx lk b">style _loss</code>分别计算内容和风格损失。通过对这些损失进行加权平均，我们得到了定义在<code class="du lv lw lx lk b">preceptual_loss</code>函数中的感知损失。这些损失函数的细节包含在<a class="ae jn" href="https://tarunbisht11.medium.com/neural-style-transfer-part-1-introduction-dc17a3eb86d2" rel="noopener">前一篇文章</a>中。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="c2c6" class="lo jp hh lk b fi lp lq l lr ls"><strong class="lk hi">def</strong> <strong class="lk hi">content_loss</strong>(placeholder,content,weight):<br/>    <strong class="lk hi">assert</strong> placeholder.shape == content.shape<br/>    <strong class="lk hi">return</strong> weight*tf.reduce_mean(tf.square(placeholder-content))</span><span id="58d1" class="lo jp hh lk b fi lt lq l lr ls"><strong class="lk hi">def</strong> <strong class="lk hi">gram_matrix</strong>(x):<br/>    gram=tf.linalg.einsum('bijc,bijd-&amp;gt;bcd', x, x)<br/>    <strong class="lk hi">return</strong> gram/tf.cast(x.shape[1]*x.shape[2]*x.shape[3],tf.float32)</span><span id="88bc" class="lo jp hh lk b fi lt lq l lr ls"><strong class="lk hi">def</strong> <strong class="lk hi">style_loss</strong>(placeholder,style, weight):<br/>    <strong class="lk hi">assert</strong> placeholder.shape == style.shape<br/>    s=gram_matrix(style)<br/>    p=gram_matrix(placeholder)<br/>    <strong class="lk hi">return</strong> weight*tf.reduce_mean(tf.square(s-p))</span><span id="d3c5" class="lo jp hh lk b fi lt lq l lr ls"><strong class="lk hi">def</strong> <strong class="lk hi">preceptual_loss</strong>(predicted_activations,content_activations,<br/>                    style_activations,content_weight,style_weight,<br/>                    content_layers_weights,style_layer_weights):<br/>    pred_content = predicted_activations["content"]<br/>    pred_style = predicted_activations["style"]<br/>    c_loss = tf.add_n([content_loss(pred_content[name],content_activations[name],<br/>                                  content_layers_weights[i]) <strong class="lk hi">for</strong> i,name <strong class="lk hi">in</strong> enumerate(pred_content.keys())])<br/>    c_loss = c_loss*content_weight<br/>    s_loss = tf.add_n([style_loss(pred_style[name],style_activations[name],<br/>                                style_layer_weights[i]) <strong class="lk hi">for</strong> i,name <strong class="lk hi">in</strong> enumerate(pred_style.keys())])<br/>    s_loss = s_loss*style_weight<br/>    <strong class="lk hi">return</strong> c_loss+s_loss</span></pre><h1 id="5846" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">创建自动编码器</h1><p id="2ad9" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">在这里，我们首先为我们的网络定义了所有必要的层:</p><ul class=""><li id="6729" class="kr ks hh ir b is it iw ix ja kt je ku ji kv jm kw kx ky kz bi translated">ReflectionPadding2D:用于将反射填充应用到conv网中的图像</li><li id="c645" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">实例规范化:我们使用实例规范化而不是批处理规范化，因为它能给出更好的结果。它将通道上的输入标准化。</li><li id="be19" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">conv layer:conv图层的块，具有填充-&gt; conv _图层-&gt;实例_规范化组合</li><li id="1ef9" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">剩余层:具有两个转化层块的剩余层</li><li id="035b" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">UpsampleLayer:对autoencoder中的瓶颈表示进行上采样(如果你读过关于auto encoder的文章，你就会明白我的意思)。它可以被认为是反卷积层。</li></ul><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="2b33" class="lo jp hh lk b fi lp lq l lr ls"><strong class="lk hi">class</strong> <strong class="lk hi">ReflectionPadding2D</strong>(tf.keras.layers.Layer):<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">__init__</strong>(self, padding=(1, 1), **kwargs):<br/>        super(ReflectionPadding2D, self).__init__(**kwargs)<br/>        self.padding = tuple(padding)<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">call</strong>(self, input_tensor):<br/>        padding_width, padding_height = self.padding<br/>        <strong class="lk hi">return</strong> tf.pad(input_tensor, [[0,0], [padding_height, padding_height], <br/>                                     [padding_width, padding_width], [0,0] ], 'REFLECT')</span><span id="19e2" class="lo jp hh lk b fi lt lq l lr ls"><strong class="lk hi">class</strong> <strong class="lk hi">InstanceNormalization</strong>(tf.keras.layers.Layer):<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">__init__</strong>(self,**kwargs):<br/>        super(InstanceNormalization, self).__init__(**kwargs)<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">call</strong>(self,inputs):<br/>        batch, rows, cols, channels = [i <strong class="lk hi">for</strong> i <strong class="lk hi">in</strong> inputs.get_shape()]<br/>        mu, var = tf.nn.moments(inputs, [1,2], keepdims=<strong class="lk hi">True</strong>)<br/>        shift = tf.Variable(tf.zeros([channels]))<br/>        scale = tf.Variable(tf.ones([channels]))<br/>        epsilon = 1e-3<br/>        normalized = (inputs-mu)/tf.sqrt(var + epsilon)<br/>        <strong class="lk hi">return</strong> scale * normalized + shift</span><span id="2595" class="lo jp hh lk b fi lt lq l lr ls"><strong class="lk hi">class</strong> <strong class="lk hi">ConvLayer</strong>(tf.keras.layers.Layer):<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">__init__</strong>(self,filters,kernel_size,strides=1,**kwargs):<br/>        super(ConvLayer,self).__init__(**kwargs)<br/>        self.padding=ReflectionPadding2D([k//2 <strong class="lk hi">for</strong> k <strong class="lk hi">in</strong> kernel_size])<br/>        self.conv2d=tf.keras.layers.Conv2D(filters,kernel_size,strides)<br/>        self.bn=InstanceNormalization()<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">call</strong>(self,inputs):<br/>        x=self.padding(inputs)<br/>        x=self.conv2d(x)<br/>        x=self.bn(x)<br/>        <strong class="lk hi">return</strong> x</span><span id="5e3d" class="lo jp hh lk b fi lt lq l lr ls"><strong class="lk hi">class</strong> <strong class="lk hi">ResidualLayer</strong>(tf.keras.layers.Layer):<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">__init__</strong>(self,filters,kernel_size,**kwargs):<br/>        super(ResidualLayer,self).__init__(**kwargs)<br/>        self.conv2d_1=ConvLayer(filters,kernel_size)<br/>        self.conv2d_2=ConvLayer(filters,kernel_size)<br/>        self.relu=tf.keras.layers.ReLU()<br/>        self.add=tf.keras.layers.Add()<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">call</strong>(self,inputs):<br/>        residual=inputs<br/>        x=self.conv2d_1(inputs)<br/>        x=self.relu(x)<br/>        x=self.conv2d_2(x)<br/>        x=self.add([x,residual])<br/>        <strong class="lk hi">return</strong> x</span><span id="4c46" class="lo jp hh lk b fi lt lq l lr ls"><strong class="lk hi">class</strong> <strong class="lk hi">UpsampleLayer</strong>(tf.keras.layers.Layer):<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">__init__</strong>(self,filters,kernel_size,strides=1,upsample=2,**kwargs):<br/>        super(UpsampleLayer,self).__init__(**kwargs)<br/>        self.upsample=tf.keras.layers.UpSampling2D(size=upsample)<br/>        self.padding=ReflectionPadding2D([k//2 <strong class="lk hi">for</strong> k <strong class="lk hi">in</strong> kernel_size])<br/>        self.conv2d=tf.keras.layers.Conv2D(filters,kernel_size,strides)<br/>        self.bn=InstanceNormalization()<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">call</strong>(self,inputs):<br/>        x=self.upsample(inputs)<br/>        x=self.padding(x)<br/>        x=self.conv2d(x)<br/>        <strong class="lk hi">return</strong> self.bn(x)</span></pre><p id="e094" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用上面定义的这些层，让我们创建一个卷积自动编码器。</p><p id="bdc3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">建筑:</p><ul class=""><li id="1150" class="kr ks hh ir b is it iw ix ja kt je ku ji kv jm kw kx ky kz bi translated">3个ConvLayer</li><li id="83ea" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">5个残渣层</li><li id="cecd" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">3个上采样层</li></ul><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="10db" class="lo jp hh lk b fi lp lq l lr ls"><strong class="lk hi">class</strong> <strong class="lk hi">StyleTransferModel</strong>(tf.keras.Model):<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">__init__</strong>(self,**kwargs):<br/>        super(StyleTransferModel, self).__init__(name='StyleTransferModel',**kwargs)<br/>        self.conv2d_1= ConvLayer(filters=32,kernel_size=(9,9),strides=1,name="conv2d_1_32")<br/>        self.conv2d_2= ConvLayer(filters=64,kernel_size=(3,3),strides=2,name="conv2d_2_64")<br/>        self.conv2d_3= ConvLayer(filters=128,kernel_size=(3,3),strides=2,name="conv2d_3_128")<br/>        self.res_1=ResidualLayer(filters=128,kernel_size=(3,3),name="res_1_128")<br/>        self.res_2=ResidualLayer(filters=128,kernel_size=(3,3),name="res_2_128")<br/>        self.res_3=ResidualLayer(filters=128,kernel_size=(3,3),name="res_3_128")<br/>        self.res_4=ResidualLayer(filters=128,kernel_size=(3,3),name="res_4_128")<br/>        self.res_5=ResidualLayer(filters=128,kernel_size=(3,3),name="res_5_128")<br/>        self.deconv2d_1= UpsampleLayer(filters=64,kernel_size=(3,3),name="deconv2d_1_64")<br/>        self.deconv2d_2= UpsampleLayer(filters=32,kernel_size=(3,3),name="deconv2d_2_32")<br/>        self.deconv2d_3= ConvLayer(filters=3,kernel_size=(9,9),strides=1,name="deconv2d_3_3")<br/>        self.relu=tf.keras.layers.ReLU()<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">call</strong>(self, inputs):<br/>        x=self.conv2d_1(inputs)<br/>        x=self.relu(x)<br/>        x=self.conv2d_2(x)<br/>        x=self.relu(x)<br/>        x=self.conv2d_3(x)<br/>        x=self.relu(x)<br/>        x=self.res_1(x)<br/>        x=self.res_2(x)<br/>        x=self.res_3(x)<br/>        x=self.res_4(x)<br/>        x=self.res_5(x)<br/>        x=self.deconv2d_1(x)<br/>        x=self.relu(x)<br/>        x=self.deconv2d_2(x)<br/>        x=self.relu(x)<br/>        x=self.deconv2d_3(x)<br/>        x = (tf.nn.tanh(x) + 1) * (255.0 / 2)<br/>        <strong class="lk hi">return</strong> x<br/>    <br/>    ## used to print shapes of each layer to check if input shape == output shape<br/>    ## I don't know any better solution to this right now<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">print_shape</strong>(self,inputs):<br/>        print(inputs.shape)<br/>        x=self.conv2d_1(inputs)<br/>        print(x.shape)<br/>        x=self.relu(x)<br/>        x=self.conv2d_2(x)<br/>        print(x.shape)<br/>        x=self.relu(x)<br/>        x=self.conv2d_3(x)<br/>        print(x.shape)<br/>        x=self.relu(x)<br/>        x=self.res_1(x)<br/>        print(x.shape)<br/>        x=self.res_2(x)<br/>        print(x.shape)<br/>        x=self.res_3(x)<br/>        print(x.shape)<br/>        x=self.res_4(x)<br/>        print(x.shape)<br/>        x=self.res_5(x)<br/>        print(x.shape)<br/>        x=self.deconv2d_1(x)<br/>        print(x.shape)<br/>        x=self.relu(x)<br/>        x=self.deconv2d_2(x)<br/>        print(x.shape)<br/>        x=self.relu(x)<br/>        x=self.deconv2d_3(x)<br/>        print(x.shape)</span></pre><p id="8a83" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在此定义输入形状和批量大小</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="d8b7" class="lo jp hh lk b fi lp lq l lr ls">input_shape=(256,256,3)<br/>batch_size=4</span></pre><p id="eb0f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用<code class="du lv lw lx lk b">StyleTransferModel</code>类创建样式模型</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="db59" class="lo jp hh lk b fi lp lq l lr ls">style_model = StyleTransferModel()</span></pre><p id="5bfc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这里，我们检查所有层的形状，并验证输入形状和输出形状</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="538a" class="lo jp hh lk b fi lp lq l lr ls">style_model.print_shape(tf.zeros(shape=(1,*input_shape)))</span><span id="ebf0" class="lo jp hh lk b fi lt lq l lr ls">(1, 256, 256, 3)<br/>(1, 256, 256, 32)<br/>(1, 128, 128, 64)<br/>(1, 64, 64, 128)<br/>(1, 64, 64, 128)<br/>(1, 64, 64, 128)<br/>(1, 64, 64, 128)<br/>(1, 64, 64, 128)<br/>(1, 64, 64, 128)<br/>(1, 128, 128, 64)<br/>(1, 256, 256, 32)<br/>(1, 256, 256, 3)</span></pre><h1 id="94d5" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">培训模式</h1><p id="7338" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">这里我们定义了一个用于训练的优化器，我们使用学习率为1e-3的Adam优化器</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="806a" class="lo jp hh lk b fi lp lq l lr ls">optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)</span><span id="15c0" class="lo jp hh lk b fi lt lq l lr ls"><strong class="lk hi">def</strong> <strong class="lk hi">train_step</strong>(dataset,style_activations,steps_per_epoch,style_model,loss_model,optimizer,<br/>               checkpoint_path="./",content_weight=1e4, style_weight=1e-2,<br/>               total_variation_weight=0.004):<br/>    batch_losses=[]<br/>    steps=1<br/>    save_path=os.path.join(checkpoint_path,f"model_checkpoint.ckpt")<br/>    print("Model Checkpoint Path: ",save_path)<br/>    <strong class="lk hi">for</strong> input_image_batch <strong class="lk hi">in</strong> dataset:<br/>        <strong class="lk hi">if</strong> steps-1 &amp;gt;= steps_per_epoch:<br/>            <strong class="lk hi">break</strong><br/>        <strong class="lk hi">with</strong> tf.GradientTape() <strong class="lk hi">as</strong> tape:<br/>            outputs=style_model(input_image_batch)<br/>            outputs=tf.clip_by_value(outputs, 0, 255)<br/>            pred_activations=loss_model.get_activations(outputs/255.0)<br/>            content_activations=loss_model.get_activations(input_image_batch)["content"] <br/>            curr_loss=preceptual_loss(pred_activations,content_activations,style_activations,content_weight,<br/>                                      style_weight,content_layers_weights,style_layers_weights)<br/>            curr_loss += total_variation_weight*tf.image.total_variation(outputs)<br/>        batch_losses.append(curr_loss)<br/>        grad = tape.gradient(curr_loss,style_model.trainable_variables)<br/>        optimizer.apply_gradients(zip(grad,style_model.trainable_variables))<br/>        <strong class="lk hi">if</strong> steps % 1000==0:<br/>            print("checkpoint saved ",end=" ")<br/>            style_model.save_weights(save_path)<br/>            print(f"Loss: {tf.reduce_mean(batch_losses).numpy()}")<br/>        steps+=1<br/>    <strong class="lk hi">return</strong> tf.reduce_mean(batch_losses)</span></pre><p id="ffa2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上面的函数中，我们定义了一个单独的训练步骤。在函数内部:</p><ul class=""><li id="3fd7" class="kr ks hh ir b is it iw ix ja kt je ku ji kv jm kw kx ky kz bi translated">首先，我们为模型检查点定义了save_path</li><li id="3905" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">对于每个时期的步数，我们运行一个训练循环</li><li id="0195" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">对于每一步，我们向前传递一批图像将其传递给我们的损失模型</li><li id="b04d" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">获取批量图像的内容层激活</li><li id="a405" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">连同来自样式图像的样式激活和内容激活，我们计算感知损失</li><li id="66de" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">为了平滑，我们给图像增加了一些总变差损失</li><li id="5d16" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">计算损失函数相对于模型可训练参数的梯度</li><li id="2b74" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">最后反向传播优化</li><li id="7247" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated">每1000步保存一个检查点</li></ul><h1 id="aa25" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">为训练配置数据集</h1><p id="291a" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">下载coco数据集用于训练，我们可以使用任何其他批量图像的图像数据集。使用wget以zip格式下载coco数据集。此外，我们创建一个目录，将下载zip文件解压缩。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="4bcf" class="lo jp hh lk b fi lp lq l lr ls">wget <a class="ae jn" href="http://images.cocodataset.org/zips/train2014.zip" rel="noopener ugc nofollow" target="_blank">http://images.cocodataset.org/zips/train2014.zip</a></span><span id="e499" class="lo jp hh lk b fi lt lq l lr ls">--2020-07-12 08:14:59--  http://images.cocodataset.org/zips/train2014.zip<br/>Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.224.88<br/>Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.224.88|:80... connected.<br/>HTTP request sent, awaiting response... 200 OK<br/>Length: 13510573713 (13G) [application/zip]<br/>Saving to: ‘train2014.zip’</span><span id="eccd" class="lo jp hh lk b fi lt lq l lr ls">train2014.zip       100%[===================&amp;gt;]  12.58G  25.0MB/s    <strong class="lk hi">in</strong> 6m 56s  </span><span id="6b18" class="lo jp hh lk b fi lt lq l lr ls">2020-07-12 08:21:55 (31.0 MB/s) - ‘train2014.zip’ saved [13510573713/13510573713]</span><span id="73b5" class="lo jp hh lk b fi lt lq l lr ls">mkdir coco<br/>unzip -qq train2014.zip -d coco</span></pre><p id="57b0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于训练，该模型允许创建tensorflow数据集，该数据集从指定的路径加载所有图像，调整它们的大小使其具有相同的大小，以便进行有效的批量训练，并实现批处理和预取。下面的类为训练创建tfdataset。</p><p id="8a66" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请注意，我们使用固定大小的图像训练模型，但我们可以生成任何大小的图像，因为模型中的所有层都是卷积层。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="b578" class="lo jp hh lk b fi lp lq l lr ls"><strong class="lk hi">class</strong> <strong class="lk hi">TensorflowDatasetLoader</strong>:<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">__init__</strong>(self,dataset_path,batch_size=4, image_size=(256, 256),num_images=None):<br/>        images_paths = [str(path) <strong class="lk hi">for</strong> path <strong class="lk hi">in</strong> Path(dataset_path).glob("*.jpg")]<br/>        self.length=len(images_paths)<br/>        <strong class="lk hi">if</strong> num_images <strong class="lk hi">is</strong> <strong class="lk hi">not</strong> <strong class="lk hi">None</strong>:<br/>            images_paths = images_paths[0:num_images]<br/>        dataset = tf.data.Dataset.from_tensor_slices(images_paths).map(<br/>            <strong class="lk hi">lambda</strong> path: self.load_tf_image(path, dim=image_size),<br/>            num_parallel_calls=tf.data.experimental.AUTOTUNE,<br/>        )<br/>        dataset = dataset.batch(batch_size,drop_remainder=<strong class="lk hi">True</strong>)<br/>        dataset = dataset.repeat()<br/>        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)<br/>        self.dataset=dataset<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">__len__</strong>(self):<br/>        <strong class="lk hi">return</strong> self.length<br/>    <strong class="lk hi">def</strong> <strong class="lk hi">load_tf_image</strong>(self,image_path,dim):<br/>        image = tf.io.read_file(image_path)<br/>        image = tf.image.decode_jpeg(image, channels=3)<br/>        image= tf.image.resize(image,dim)<br/>        image= image/255.0<br/>        image = tf.image.convert_image_dtype(image, tf.float32)<br/>        <strong class="lk hi">return</strong> image</span></pre><p id="000b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用上面的类，让我们从coco数据集图像创建tfdataset。我们指定图像文件夹的路径(所有图像都在这里)和批量大小</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="e72f" class="lo jp hh lk b fi lp lq l lr ls">loader=TensorflowDatasetLoader("coco/train2014/",batch_size=4)</span><span id="bf44" class="lo jp hh lk b fi lt lq l lr ls">loader.dataset.element_spec</span><span id="e07c" class="lo jp hh lk b fi lt lq l lr ls">TensorSpec(shape=(4, 256, 256, 3), dtype=tf.float32, name=<strong class="lk hi">None</strong>)</span></pre><p id="76df" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">绘制一些图像以查看数据集中的图像</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="c4d5" class="lo jp hh lk b fi lp lq l lr ls">plot_images_grid(next(iter(loader.dataset.take(1))))</span></pre><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ly"><img src="../Images/d68216635928afc810ea1ead0fd04f53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*h7ML704t6GyiHdGV"/></div></div></figure><p id="1018" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在让我们使用<code class="du lv lw lx lk b">load_url_image</code>从URL加载样式图像并绘制它。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="8f4e" class="lo jp hh lk b fi lp lq l lr ls"># setting up style image<br/>url="https://www.edvardmunch.org/images/paintings/the-scream.jpg"<br/>style_image=load_url_image(url,dim=(input_shape[0],input_shape[1]),resize=<strong class="lk hi">True</strong>)<br/>style_image=style_image/255.0</span><span id="6204" class="lo jp hh lk b fi lt lq l lr ls">show_image(style_image)</span></pre><p id="6c61" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来，利用损失模型提取风格图像的风格层特征图</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="2518" class="lo jp hh lk b fi lp lq l lr ls">style_image=style_image.astype(np.float32)<br/>style_image_batch=np.repeat([style_image],batch_size,axis=0)<br/>style_activations=loss_model.get_activations(style_image_batch)["style"]</span></pre><h1 id="287f" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">培训模式</h1><p id="4729" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">定义内容权重、样式权重和总变化权重这些是我们可以调整以改变输出图像中样式和内容的强度的超参数</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="59b3" class="lo jp hh lk b fi lp lq l lr ls">content_weight=1e1<br/>style_weight=1e2<br/>total_variation_weight=0.004</span></pre><p id="6232" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在定义要训练的时期数、每个时期的步数和模型检查点路径</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="6003" class="lo jp hh lk b fi lp lq l lr ls">epochs=2</span><span id="7992" class="lo jp hh lk b fi lt lq l lr ls">num_images=len(loader)<br/>steps_per_epochs=num_images//batch_size<br/>print(steps_per_epochs)</span><span id="6c14" class="lo jp hh lk b fi lt lq l lr ls">20695</span><span id="5ab5" class="lo jp hh lk b fi lt lq l lr ls">save_path = "./scream"</span><span id="dead" class="lo jp hh lk b fi lt lq l lr ls">os.makedirs(save_path, exist_ok=<strong class="lk hi">True</strong>)</span></pre><p id="c825" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">启用混合精度训练通过以半精度格式执行运算，它提供了显著的计算加速。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="6dfe" class="lo jp hh lk b fi lp lq l lr ls"><strong class="lk hi">try</strong>:<br/>    policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')<br/>    tf.keras.mixed_precision.experimental.set_policy(policy) <br/><strong class="lk hi">except</strong>:<br/>    <strong class="lk hi">pass</strong></span></pre><p id="e163" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果先前的检查点存在于该路径，则加载该检查点并继续进一步训练，否则我们从头开始训练</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="d35a" class="lo jp hh lk b fi lp lq l lr ls"><strong class="lk hi">if</strong> os.path.isfile(os.path.join(save_path,"model_checkpoint.ckpt.index")):<br/>    style_model.load_weights(os.path.join(save_path,"model_checkpoint.ckpt"))<br/>    print("resuming training ...")<br/><strong class="lk hi">else</strong>:<br/>    print("training scratch ...")</span><span id="d3bd" class="lo jp hh lk b fi lt lq l lr ls">training scratch ...</span></pre><p id="041b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后，我们开始训练模型。在每个时期，我们调用<code class="du lv lw lx lk b">train_step</code>函数，该函数运行到每个时期定义的步骤数，并且在每个时期之后保存模型检查点，用于进一步的推理和训练。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="146e" class="lo jp hh lk b fi lp lq l lr ls">epoch_losses=[]<br/><strong class="lk hi">for</strong> epoch <strong class="lk hi">in</strong> range(1,epochs+1):<br/>    print(f"epoch: {epoch}")<br/>    batch_loss=train_step(loader.dataset,style_activations,steps_per_epochs,style_model,loss_model,optimizer,<br/>                          save_path,<br/>                          content_weight,style_weight,total_variation_weight,<br/>                          content_layers_weights,style_layers_weights)<br/>    style_model.save_weights(os.path.join(save_path,"model_checkpoint.ckpt"))<br/>    print("Model Checkpointed at: ",os.path.join(save_path,"model_checkpoint.ckpt"))<br/>    print(f"loss: {batch_loss.numpy()}")<br/>    epoch_losses.append(batch_loss)</span><span id="ce75" class="lo jp hh lk b fi lt lq l lr ls">epoch: 1<br/>Model Checkpoint Path:  ./scream/model_checkpoint.ckpt<br/>checkpoint saved  Loss: 6567731.5<br/>checkpoint saved  Loss: 6464426.5<br/>checkpoint saved  Loss: 6402768.0<br/>checkpoint saved  Loss: 6336974.5<br/>checkpoint saved  Loss: 6281922.5<br/>checkpoint saved  Loss: 6232056.0<br/>checkpoint saved  Loss: 6191586.5<br/>checkpoint saved  Loss: 6155332.0<br/>checkpoint saved  Loss: 6119712.5<br/>checkpoint saved  Loss: 6085571.5<br/>checkpoint saved  Loss: 6062698.0<br/>checkpoint saved  Loss: 6036787.0<br/>checkpoint saved  Loss: 6011265.5<br/>checkpoint saved  Loss: 5988809.5<br/>checkpoint saved  Loss: 5969908.0<br/>checkpoint saved  Loss: 5950925.0<br/>checkpoint saved  Loss: 5931179.5<br/>checkpoint saved  Loss: 5912791.5<br/>checkpoint saved  Loss: 5894602.0<br/>checkpoint saved  Loss: 5880713.0<br/>Model Checkpointed at:  ./scream<br/>loss: 5869695.5<br/>epoch: 2<br/>Model Checkpoint Path:  ./scream/model_checkpoint.ckpt<br/>checkpoint saved  Loss: 5520494.5<br/>checkpoint saved  Loss: 5532450.5<br/>checkpoint saved  Loss: 5529669.0<br/>checkpoint saved  Loss: 5524684.0<br/>checkpoint saved  Loss: 5518524.5<br/>checkpoint saved  Loss: 5508913.5<br/>checkpoint saved  Loss: 5503493.5<br/>checkpoint saved  Loss: 5501864.0<br/>checkpoint saved  Loss: 5497016.0<br/>checkpoint saved  Loss: 5491713.0<br/>checkpoint saved  Loss: 5491244.5<br/>checkpoint saved  Loss: 5484620.0<br/>checkpoint saved  Loss: 5482881.0<br/>checkpoint saved  Loss: 5476766.5<br/>checkpoint saved  Loss: 5472491.0<br/>checkpoint saved  Loss: 5466294.5<br/>checkpoint saved  Loss: 5459984.0<br/>checkpoint saved  Loss: 5454912.5<br/>checkpoint saved  Loss: 5449535.5<br/>checkpoint saved  Loss: 5446370.0<br/>Model Checkpointed at:  ./scream<br/>loss: 5442546.5</span></pre><p id="5137" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在训练模型之后，让绘制与时期相关的损失，并检查损失摘要</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="cd95" class="lo jp hh lk b fi lp lq l lr ls">plt.plot(epoch_losses)<br/>plt.xlabel("Epochs")<br/>plt.ylabel("Loss")<br/>plt.title("Training Process")<br/>plt.show()</span></pre><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lz"><img src="../Images/a29608337530d1fd411dc7bc3e02c982.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*LbhIqYwtgiiKtF-s"/></div></figure><p id="2447" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在是时候生成一些样式图像了。我们首先将保存的模型检查点加载到autoencoder中。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="dbe9" class="lo jp hh lk b fi lp lq l lr ls"><strong class="lk hi">if</strong> os.path.isfile(os.path.join(save_path,"model_checkpoint.ckpt.index")):<br/>    style_model.load_weights(os.path.join(save_path,"model_checkpoint.ckpt"))<br/>    print("loading weights ...")<br/><strong class="lk hi">else</strong>:<br/>    print("no weights found ...")</span><span id="04a1" class="lo jp hh lk b fi lt lq l lr ls">loading weights ...</span></pre><p id="7d89" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">加载图像进行样式化，并将其转换为浮点型。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="8cf6" class="lo jp hh lk b fi lp lq l lr ls">test_image_url="https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/chicago-skyline-on-a-clear-day-royalty-free-image-115891582-1557159569.jpg"</span><span id="e8bf" class="lo jp hh lk b fi lt lq l lr ls">test_image=load_url_image(test_image_url,dim=(640,480))<br/>test_image=np.expand_dims(test_image,axis=0)</span><span id="00a5" class="lo jp hh lk b fi lt lq l lr ls">test_image=test_image.astype(np.float32)</span></pre><p id="6de8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在模型的一次向前传递中，我们得到生成的样式图像</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="4826" class="lo jp hh lk b fi lp lq l lr ls">predicted_image=style_model(test_image)</span></pre><p id="0eec" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">将生成的图像像素箝位在0到255之间，并将其转换为uint8。我们得到了我们生成的风格图像，绘制它并检查它的外观，还保存它并与朋友分享</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="6814" class="lo jp hh lk b fi lp lq l lr ls">predicted_image=np.clip(predicted_image,0,255)<br/>predicted_image=predicted_image.astype(np.uint8)</span><span id="c26d" class="lo jp hh lk b fi lt lq l lr ls">test_output=test_image.astype(np.uint8)<br/>test_output=tf.squeeze(test_output).numpy()<br/>predicted_output=tf.squeeze(predicted_image).numpy()</span><span id="d8e2" class="lo jp hh lk b fi lt lq l lr ls">plot_images_grid([test_output,predicted_output])</span></pre><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ma"><img src="../Images/7fae5e2b8c87ece1f8ec80fa4fb523ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nTfFZhxBbczxzBRq"/></div></div></figure><p id="c3ef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你没有足够的计算能力，使用colab或kaggle内核，他们提供免费的GPU甚至TPU来训练这些模型，一旦训练完毕，我们可以使用训练好的检查点在任何有GPU或CPU的系统中进行风格转换。</p><p id="6e66" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用<code class="du lv lw lx lk b">opencv</code>,我们也可以轻松创建风格视频。</p><h1 id="0069" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结果</h1><p id="3602" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">一些图像结果</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mb"><img src="../Images/9d984165b4867ca837d678d1a2db0bed.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/0*yqH_9IGEYwHkIt4L"/></div></figure><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mc"><img src="../Images/32196514533bbba2ad1699a058dbd11c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*b8Eupi3yXSWN8T6P"/></div></figure><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es md"><img src="../Images/ef22dea7bce9e50dad92a2e787c1a46e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fbhZ8MWmJPdeAU5h"/></div></div></figure><p id="dece" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这里，我们有实时视频风格化的行动</p><figure class="lf lg lh li fd ii"><div class="bz dy l di"><div class="me mf l"/></div></figure><p id="b99a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下面是视频风格化的行动</p><figure class="lf lg lh li fd ii"><div class="bz dy l di"><div class="mg mf l"/></div></figure><figure class="lf lg lh li fd ii"><div class="bz dy l di"><div class="mg mf l"/></div></figure><p id="3df2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在生成不同的图像和视频，玩它，分享令人兴奋的结果。</p><p id="f206" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果现在有人想尝试视频和图像的风格转换，我已经为同样的目的创建了一个<a class="ae jn" href="https://github.com/tarun-bisht/fast-style-transfer" rel="noopener ugc nofollow" target="_blank"> github库</a>，并附有说明。</p><p id="3753" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢阅读。✌✌✌</p><h1 id="c48b" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">参考</h1><p id="88be" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated"><a class="ae jn" href="https://arxiv.org/abs/1607.08022" rel="noopener ugc nofollow" target="_blank">实例规范化:快速风格化缺少的要素</a></p><p id="17b8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae jn" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">实时风格转换和超分辨率的感知损失</a></p><p id="25ce" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae jn" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">艺术风格的神经算法</a></p><h1 id="627f" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">重要链接</h1><p id="c5ca" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated"><a class="ae jn" href="https://github.com/tarun-bisht/fast-style-transfer" rel="noopener ugc nofollow" target="_blank"> Github库</a></p><p id="9952" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae jn" href="https://github.com/tarun-bisht/blogs-notebooks/blob/master/style-transfer/Neural%20Style%20Transfer%20Part%202.ipynb" rel="noopener ugc nofollow" target="_blank">谷歌Colab笔记本</a></p><p id="2d05" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae jn" href="https://www.youtube.com/watch?v=GrS4rWifdko" rel="noopener ugc nofollow" target="_blank"> Youtube视频</a></p></div></div>    
</body>
</html>