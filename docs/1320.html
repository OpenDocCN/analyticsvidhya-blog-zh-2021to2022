<html>
<head>
<title>A Deep Dive into Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入了解变形金刚</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-deep-dive-into-transformers-cdad59bf4260?source=collection_archive---------23-----------------------#2021-02-23">https://medium.com/analytics-vidhya/a-deep-dive-into-transformers-cdad59bf4260?source=collection_archive---------23-----------------------#2021-02-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/36837fe982b2aecb524ce1e6359e846c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WA8mYKRt3ISHlWEWb9nt9A.jpeg"/></div></div></figure><p id="28a7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你最近没有听说过NLP(自然语言处理)或人工智能领域的变形金刚，那么你很可能生活在一块岩石下。在过去的几年里，有一系列基于Transformer的架构频繁发布，如BERT、SpanBERT、Transformer-XL、XLNet、GPT-2等。OpenAI的GPT-3因其在问答、理解甚至编程等任务上表现出色的能力而风靡互联网(最棒的部分是它添加了评论)。点击这里查看所有GPT-3能做什么。</p><p id="95cb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但这一切都始于早在2017年发布的一篇研究论文“<a class="ae jn" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <em class="jo">注意力是你所需要的全部</em> </a>”。本文提出了一种新的深度学习架构来处理序列数据，而不使用RNNs和CNN。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es jp"><img src="../Images/874a6567afb678a7c492eb4cda4cb0ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*RgvVre89TCPEAJenldxNLw.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">变压器架构</figcaption></figure><p id="fe92" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该架构看起来相当简单，并声称与基于CNN / RNN的模型相比，可以用更少的训练时间在语言任务中实现更高的准确性。该架构解释了注意力机制，这是该架构的基石。我们将试着理解注意力。</p><p id="d2bd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们考虑下面的句子。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es jy"><img src="../Images/4c9c793a87db5349da0b117448474105.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*qbRZtvCAbGM_TYaiiMM_wg.png"/></div></figure><p id="3403" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一般来说，我们会将这个句子中的单词转换成令牌嵌入，并将这些嵌入发送到网络中。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es jz"><img src="../Images/12ce0816e69a375c3984780101f652b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*h_zknVVfuyqMJNUGuTsN6Q.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">令牌嵌入</figcaption></figure><p id="e7d3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">与基于RNN的模型不同，变压器需要同时看到所有的令牌，因为它没有递归。注意机制根据位置(顺序)和上下文告诉模型关注或注意哪个标记。嵌入之间的相关性是通过在它们内部做内积来计算的。如果我们在n维空间中投影这些嵌入，如果向量之间的角度较小，点积将导致高标量值。例如</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es ka"><img src="../Images/d0c357953f9f72a51b163456cbcc734d.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*-sKcF3di1CDrA0FlV8Dxww.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">点积和上下文</figcaption></figure><p id="9e53" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果<strong class="ir hi"> θ </strong>更接近0，则点积产生更高的值(cos 0 = 1)。您可以将嵌入向量中的每个维度视为一个上下文。如果任何两个单词在上下文空间中更接近，那么它们的相关性将更高。现在，我们取所有的嵌入，做点积。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es kb"><img src="../Images/4864b86b0d2653e58eca1294a55c9263.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*OANXrJJe-Nsf7wgLv9Srmw.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">计算注意力</figcaption></figure><p id="d1fe" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，我们复制了两个令牌嵌入，分别称为Key和Query。我们在键和查询之间做点积，得到标量值作为结果。方块越黑，产品的价值越高。由于这些值在数量上可能很大，我们将这个标量输出除以矢量大小的平方根，这将给出很好的缩放效果。现在我们沿着y轴应用softmax来计算注意力。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es kc"><img src="../Images/7b5d415b56f71f9d3ca86934b55af1a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*Y13qJ4vH_Wtxsigb0ReRbw.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">语境化向量</figcaption></figure><p id="3f35" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们在注意力和原始令牌嵌入(值)之间进行标量乘法，以获得<strong class="ir hi">上下文化嵌入</strong>。这种嵌入中的每个标记都将具有从其他标记扩散的一定量的上下文。量级由注意力图给出。花一点时间想象这是如何发生的。</p><p id="3d11" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在让我们看看编码器部分，即变压器架构的左侧。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es kd"><img src="../Images/288354242bb8b89e90d15cc03a6dca60.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*u9EyNU4wZC6KGm88kW7yPw.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">编码器</figcaption></figure><p id="2c26" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，一旦我们理解了注意力机制，架构中的其余组件就相当简单了；等一下！你注意到编码器中有一种叫做“多头注意力”的东西吗？它仅仅是沿着嵌入维度分割注意力过程，以增加网络的多功能性。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ke"><img src="../Images/0de2722a17e7dfc3012258c91580b9a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CY9GEUMlV2QJz9tlXa8haA.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">多头注意力</figcaption></figure><p id="edac" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">注意，在嵌入头之前存在前馈网络，这些网络学习根据上下文将嵌入混合和匹配到多头中。最后，计算出的上下文化嵌入头被连接回去。</p><p id="e408" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好了，现在我们已经准备好编码编码器部分了！</p><figure class="jq jr js jt fd ii"><div class="bz dy l di"><div class="kf kg l"/></div></figure><p id="efc5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在多头注意了。</p><figure class="jq jr js jt fd ii"><div class="bz dy l di"><div class="kf kg l"/></div></figure><p id="b27a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">编码器部分的输出(潜在向量)进入解码器部分。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es kh"><img src="../Images/53fc0bfee3fb3926eaf82d9f0bfd8877.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/0*m2IZfJ8Fps1-J1kP.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">解码器</figcaption></figure><p id="747b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">解码器部分非常简单，因为它获取目标序列并完成所有位置、标记嵌入和多头关注。但是多头注意力层利用编码器输出和目标嵌入来计算注意力。对目标序列的自我关注由“屏蔽多头关注”层完成，该层简单地屏蔽了应该由网络预测的单词。</p><p id="56d2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请注意，编码器和解码器部分都有高速/跳跃连接。这负责位置信息在整个网络中的流动(记住这里没有递归的概念)。</p><p id="da04" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们编码解码器部分！</p><figure class="jq jr js jt fd ii"><div class="bz dy l di"><div class="kf kg l"/></div></figure><p id="7256" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我使用上述编码器和解码器设计，用Seq2Seq变压器网络训练了一个德语-英语翻译。结果如下所示</p><figure class="jq jr js jt fd ii"><div class="bz dy l di"><div class="kf kg l"/></div></figure><p id="54a9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下面是一些翻译和它们各自的注意力地图</p><figure class="jq jr js jt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ki"><img src="../Images/7e97752804af93b77a25bfea32f41594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*13zEgXa7VnBcY_dDFf36iA.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">德-英翻译的注意力地图</figcaption></figure><p id="6ae6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有关完整的代码，请参考本colab笔记本。<a class="ae jn" href="https://colab.research.google.com/drive/1pA-mafHWx6Jh4xzEAKqk1Oi2RsX1vO7l?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/drive/1pA-MAF hwx 6 jh4xzeakqk 1 oi 2 rsx 1 VO 7 l？usp =共享</a></p><p id="1ee1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你可以为你自己的目的训练转换器，比如问答、聊天机器人、理解、机器翻译等等，通过相应地设计输入和输出序列。</p><p id="470f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">干杯！</p></div></div>    
</body>
</html>