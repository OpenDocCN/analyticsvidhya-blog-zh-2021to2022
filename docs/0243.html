<html>
<head>
<title>A Beginner’s Guide to KNN and MNIST Handwritten Digits Recognition using KNN from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始使用KNN识别KNN和MNIST手写数字的初学者指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-beginners-guide-to-knn-and-mnist-handwritten-digits-recognition-using-knn-from-scratch-df6fb982748a?source=collection_archive---------0-----------------------#2021-01-10">https://medium.com/analytics-vidhya/a-beginners-guide-to-knn-and-mnist-handwritten-digits-recognition-using-knn-from-scratch-df6fb982748a?source=collection_archive---------0-----------------------#2021-01-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="52b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="jd"/></strong><em class="jd"/><strong class="ih hj"><em class="jd">【改良型国家标准技术研究院】</em> </strong> <em class="jd">是计算机视觉事实上的“hello world”数据集。自1999年发布以来，这个经典的手写图像数据集一直是基准分类算法的基础。随着新的机器学习技术的出现，MNIST仍然是研究人员和学习者的可靠资源。</em></p><blockquote class="je jf jg"><p id="2abf" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated"><strong class="ih hj">最终目标是从数万张手写图像数据集中正确识别数字。</strong></p></blockquote><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es jk"><img src="../Images/7ecd4accc307fea873f3614f3e73ea2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/0*CCO8qrBn-J14H62o.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图片来源:维基百科</figcaption></figure><p id="6309" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在将尝试从<strong class="ih hj"> <em class="jd">开始使用<strong class="ih hj"> <em class="jd"> KNN (K近邻)算法</em> </strong>对数字进行分类。</em> </strong></p><blockquote class="je jf jg"><p id="b459" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">在此之前，我们应该了解KNN到底是什么！</p></blockquote><h1 id="709f" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">什么是KNN？</h1><p id="8cef" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq kw is it iu kx iw ix iy ky ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd"> K近邻</em> </strong>既可以用于分类，也可以用于回归。k-最近邻是一种简单的算法，它存储所有可用的案例，并根据相似性度量对新案例进行分类。</p><p id="d027" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd"> KNN </em> </strong>是一种<em class="jd">基于实例的学习</em>，或<em class="jd">懒惰学习</em>，其中函数只是局部近似，所有计算都推迟到分类。KNN算法是所有机器学习算法中最简单的。</p><p id="4aa3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它是一种<strong class="ih hj"> <em class="jd">非参数算法</em> </strong>，其中它不需要训练数据来进行推理，因此，由于所有显而易见的原因，与<strong class="ih hj"> <em class="jd">参数学习算法</em> </strong>相比，训练快得多，而推理慢得多。</p><h1 id="5ad5" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">KNN到底是如何运作的？</h1><blockquote class="je jf jg"><p id="bcbc" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">我们举一个简单的案例来理解这个算法。</p></blockquote><p id="ed8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">以下是</em> <strong class="ih hj"> <em class="jd">红色圆圈【RC】</em></strong><em class="jd">和</em> <strong class="ih hj"> <em class="jd">绿色方块(GS): </em> </strong></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es kz"><img src="../Images/ea9d85407ec18263923ab9f3ebc789fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*KZfNdxJ4iKp8555X"/></div></figure><p id="1d10" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你打算找出蓝星的等级(BS)。BS可以是RC，也可以是GS，不能是别的。KNN算法中的“K”是我们希望进行投票的最近邻居。假设K = 3。因此，我们现在将画一个以BS为中心的圆，其大小正好能在平面上仅包含三个数据点。<em class="jd">详见下图:</em></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es kz"><img src="../Images/b2a8b21d894d5a082d6486609d4ec640.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*xGWGbiNqzzZBvMni"/></div></figure><p id="9f33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">离某人最近的3个点都是RC。因此，根据我们的smart置信度，我们可以说bs应该属于RC类别。这里，选择变得非常明显，因为来自最近邻居的所有3张选票都投给了RC。在该算法中，参数K的选择极其重要。接下来，我们将了解得出最有效的k值需要考虑哪些因素。</p><blockquote class="je jf jg"><p id="6151" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated"><strong class="ih hj">注:</strong>KNN的一些假设——</p></blockquote><ul class=""><li id="a55c" class="la lb hi ih b ii ij im in iq lc iu ld iy le jc lf lg lh li bi translated"><em class="jd">当你有两个职业时，选择一个奇数的K值来避免平局。即，如果新的数据点正好在两个类之间，它不能决定使用哪一个。</em></li><li id="1b8a" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><em class="jd"> K不能是班级人数的倍数</em></li><li id="c943" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><em class="jd">如果K很小(过度拟合)，如果有很多数据点(n) </em>将会不准确</li><li id="1aa0" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><em class="jd">如果K非常大(欠拟合)，K一定不等于数据点数n </em></li></ul><h1 id="79db" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">我们如何选择因子K？</h1><blockquote class="je jf jg"><p id="aa97" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">首先，让我们试着理解K在算法中到底影响了什么。如果我们看到最后一个例子，假设所有6个训练观察值保持不变，利用给定的K值，我们可以制作每个类别的边界。</p></blockquote><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lo"><img src="../Images/d02d7547330fcd0b84a62413901b62b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nx6YP6AFVUQmGiSX.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">K值的训练误差</figcaption></figure><p id="7a78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如您所见，对于训练样本，K=1时的错误率始终为零。这是因为最接近任何训练数据点的点是它本身。因此，当K=1时，预测总是准确的。如果验证误差曲线是相似的，我们选择的K应该是1。</p><p id="25e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">以下是K值变化时的验证误差曲线:</em></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lt"><img src="../Images/542a34159f6403fa0da23ceff42c6698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qf1n9i7bf1CWD1b3.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">K值测试/验证错误</figcaption></figure><p id="5406" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这让故事更加清晰。在K=1时，我们过度拟合了边界。因此，错误率最初降低并达到最小值。在最小值点之后，它随K的增加而增加。要获得K的最佳值，可以将训练和验证与初始数据集分开。现在绘制验证误差曲线，以获得K的最佳值。K的这个值应该用于所有预测，这类似于<a class="ae lu" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)#:~:text=In%20cluster%20analysis%2C%20the%20elbow,number%20of%20clusters%20to%20use." rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="jd">肘方法</em> </strong> </a> <strong class="ih hj"> <em class="jd">。</em> </strong></p><h1 id="3355" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">KNN的伪代码</h1><blockquote class="je jf jg"><p id="029c" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">任何人都可以按照下面给出的伪代码步骤实现KNN模型。</p></blockquote><ol class=""><li id="781c" class="la lb hi ih b ii ij im in iq lc iu ld iy le jc lv lg lh li bi translated"><em class="jd">加载数据</em></li><li id="e7be" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lv lg lh li bi translated"><em class="jd">初始化k的值</em></li><li id="0fe1" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lv lg lh li bi translated"><em class="jd">为了得到预测的类别，从1迭代到训练数据点的总数</em></li><li id="5571" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lv lg lh li bi translated"><em class="jd">计算测试数据与每一行训练数据之间的距离。这里我们将使用欧几里德距离作为我们的距离度量，因为它是最流行的方法。可以使用的其他度量是切比雪夫、余弦等。</em></li><li id="501d" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lv lg lh li bi translated"><em class="jd">根据距离值将计算出的距离按升序排序</em></li><li id="a94e" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lv lg lh li bi translated"><em class="jd">从排序后的数组中获取前k行</em></li><li id="9c6f" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lv lg lh li bi translated"><em class="jd">得到这些行中最频繁的类</em></li><li id="7ac7" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lv lg lh li bi translated"><em class="jd">返回预测类</em></li></ol><h1 id="2638" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">KNN变奏曲</h1><p id="a27b" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq kw is it iu kx iw ix iy ky ja jb jc hb bi translated">正如我们所见，在传统的KNN中，我们对所有的等级和距离都给予同等的权重，这里有一个你应该知道的KNN的变体！</p><h2 id="5311" class="lw jx hi bd jy lx ly lz kc ma mb mc kg iq md me kk iu mf mg ko iy mh mi ks mj bi translated">距离加权KNN</h2><p id="b898" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq kw is it iu kx iw ix iy ky ja jb jc hb bi translated">在距离加权KNN中，您基本上更强调接近测试值的值，而不是远离测试值的值，并同样为每个值分配权重。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es mk"><img src="../Images/411009cafdfa807aa78e346df18b2d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*ViY9zEl0-NHHqWcYrs5BhA.png"/></div></figure><p id="8bca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">其中wk为— </em></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ml"><img src="../Images/acab45580eec66284bccbb123f3b848c.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*eZpUbfo6aSNRtDDjiTZbKA.png"/></div></figure><h2 id="3804" class="lw jx hi bd jy lx ly lz kc ma mb mc kg iq md me kk iu mf mg ko iy mh mi ks mj bi translated">加权距离函数</h2><p id="7989" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq kw is it iu kx iw ix iy ky ja jb jc hb bi translated">既然我们已经在传统的KNN中给了我们所有的特征相同的权重，那么让我们在这个变体中给每个特征分配不同的权重。重要的特征将具有较大的权重，而不太重要的特征将具有较低的权重，最不重要的特征将具有0或接近0的权重。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es mm"><img src="../Images/3c0b18d15d5b0fb54c66dcdcee4bc4d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*mDMSZ-JrCmw0JIb8d2GGhA.png"/></div></figure><h1 id="373d" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">测量距离的方法</h1><ul class=""><li id="83f0" class="la lb hi ih b ii ku im kv iq mn iu mo iy mp jc lf lg lh li bi translated"><strong class="ih hj"> <em class="jd">闵可夫斯基距离</em> </strong></li><li id="026e" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><strong class="ih hj"> <em class="jd">曼哈顿距离</em> </strong></li><li id="d77a" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><strong class="ih hj"> <em class="jd">欧几里得距离</em> </strong></li><li id="ed6e" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><strong class="ih hj"> <em class="jd">海明距离</em> </strong></li><li id="f88c" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><strong class="ih hj"> <em class="jd">余弦距离</em> </strong></li></ul><p id="fdc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">曼哈顿距离</em> </strong>在数据存在高维度时，通常优先于更常见的<strong class="ih hj"> <em class="jd">欧氏距离</em> </strong>。<strong class="ih hj"> <em class="jd">海明距离</em> </strong>用于度量分类变量之间的距离，而<strong class="ih hj"> <em class="jd">余弦距离</em> </strong>度量主要用于发现两个数据点之间的相似程度，而<strong class="ih hj"> <em class="jd">闵可夫斯基</em> </strong>是欧几里德距离和曼哈顿距离在更低层次上的推广。</p><blockquote class="je jf jg"><p id="e497" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">关于这方面的更多信息，请查看机器学习中使用的不同类型的距离度量<a class="ae lu" rel="noopener" href="/@kunal_gohrani/different-types-of-distance-metrics-used-in-machine-learning-e9928c5e26c7#:~:text=Manhattan%20distance%20is%20usually%20preferred,similarity%20between%20two%20data%20points."><strong class="ih hj"/></a><strong class="ih hj">。</strong></p></blockquote><h1 id="5b64" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">从头开始实施</h1><p id="72d6" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq kw is it iu kx iw ix iy ky ja jb jc hb bi translated"><em class="jd">您最初需要导入的库:</em></p><pre class="jl jm jn jo fd mq mr ms mt aw mu bi"><span id="a298" class="lw jx hi mr b fi mv mw l mx my">import numpy as np<br/>import operator <br/>from operator import itemgetter</span></pre><p id="31ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">让我们先定义一个函数，返回两点之间的</em> <a class="ae lu" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="jd">欧氏距离</em></strong></a><em class="jd">:</em></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es mz"><img src="../Images/c984451891ebd6634e5549f787750e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dNvV40WTaVdOQSPkwfhIug.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图片来源:Science Direct——欧几里德距离公式</figcaption></figure><pre class="jl jm jn jo fd mq mr ms mt aw mu bi"><span id="a809" class="lw jx hi mr b fi mv mw l mx my">def euc_dist(x1, x2):<br/>    return np.sqrt(np.sum((x1-x2)**2))</span></pre><p id="afb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">现在，让我们写一个类“KNN”，并为</em><strong class="ih hj"><em class="jd">【K】值:</em> </strong>初始化一个实例</p><pre class="jl jm jn jo fd mq mr ms mt aw mu bi"><span id="a9d5" class="lw jx hi mr b fi mv mw l mx my">class KNN:<br/>    def __init__(self, K=3):<br/>        self.K = K</span></pre><p id="e67e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">让我们添加另一个函数到我们的类中，它初始化实例来适应我们的训练集——X-train和y-train: </em></p><pre class="jl jm jn jo fd mq mr ms mt aw mu bi"><span id="0ef4" class="lw jx hi mr b fi mv mw l mx my">class KNN:<br/>    def __init__(self, K=3):<br/>        self.K = K<br/>    def fit(self, x_train, y_train):<br/>        self.X_train = x_train<br/>        self.Y_train = y_train</span></pre><p id="e246" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">现在让我们将预测函数添加到这个类中:</em></p><pre class="jl jm jn jo fd mq mr ms mt aw mu bi"><span id="e04c" class="lw jx hi mr b fi mv mw l mx my">def predict(self, X_test):<br/>    predictions = [] <em class="jd"><br/>    </em>for i in range(len(X_test)):<br/>        dist = np.array([euc_dist(X_test[i], x_t) for x_t in   <br/>        self.X_train])<br/>        dist_sorted = dist.argsort()[:self.K]<br/>        neigh_count = {}<br/>        for idx in dist_sorted:<br/>            if self.Y_train[idx] in neigh_count:<br/>                neigh_count[self.Y_train[idx]] += 1<br/>            else:<br/>                neigh_count[self.Y_train[idx]] = 1<br/>        sorted_neigh_count = sorted(neigh_count.items(),    <br/>        key=operator.itemgetter(1), reverse=True)<br/>        predictions.append(sorted_neigh_count[0][0]) <br/>    return predictions</span></pre><blockquote class="je jf jg"><p id="7bb5" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">哇哦。代码太多了！让我们一行一行地理解这个—</p></blockquote><p id="d0d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经初始化了一个列表来存储我们的预测，然后运行一个循环来计算每个测试示例到每个对应的训练示例的欧几里德距离，并将所有这些距离存储在一个NumPy数组中，之后我们返回了距离的前K个排序值的索引，然后我们创建了一个字典，将我们的类标签作为键，将它们的出现次数作为每个键的值。</p><p id="231f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们将每个计数附加到每个键-值对的neigh_count字典中，之后，我们从最常出现的值到最少出现的值对我们的键-值对进行排序，其中，我们最常出现的值将是我们对每个训练示例的预测。然后我们返回预测。</p><blockquote class="je jf jg"><p id="e141" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">这就是KNN的全部实现，现在让我们在MNIST数据集上测试我们的模型！</p></blockquote><pre class="jl jm jn jo fd mq mr ms mt aw mu bi"><span id="75c0" class="lw jx hi mr b fi mv mw l mx my"><strong class="mr hj">from</strong> <strong class="mr hj">sklearn.datasets</strong> <strong class="mr hj">import</strong> load_digits</span><span id="6805" class="lw jx hi mr b fi na mw l mx my">mnist = load_digits()<br/>print(mnist.data.shape)</span><span id="33ab" class="lw jx hi mr b fi na mw l mx my"><strong class="mr hj">Out:<br/></strong>(1797, 64)</span><span id="f418" class="lw jx hi mr b fi na mw l mx my">X = mnist.data <br/>y = mnist.target</span></pre><p id="a058" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">将我们的数据分为训练和测试:</em></p><pre class="jl jm jn jo fd mq mr ms mt aw mu bi"><span id="b3a3" class="lw jx hi mr b fi mv mw l mx my">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)</span><span id="fe43" class="lw jx hi mr b fi na mw l mx my">print(X_train.shape, y_train.shape)<br/>print(X_test.shape, y_test.shape)</span><span id="1e75" class="lw jx hi mr b fi na mw l mx my"><strong class="mr hj">Out:<br/></strong>(1347, 64) (1347,) <br/>(450, 64) (450,)</span><span id="713f" class="lw jx hi mr b fi na mw l mx my">print(np.unique(y_train,return_counts=<strong class="mr hj">True</strong>))<br/>print(np.unique(y_test,return_counts=<strong class="mr hj">True</strong>))</span><span id="1d33" class="lw jx hi mr b fi na mw l mx my"><strong class="mr hj">Out:<br/></strong>(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([127, 140, 136, 143, 129, 134, 133, 138, 129, 138])) (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([51, 42, 41, 40, 52, 48, 48, 41, 45, 42]))</span></pre><blockquote class="je jf jg"><p id="55e1" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">把数据拆分成测试和训练就够了吗？真的有帮助吗？</p></blockquote><p id="41fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常建议使用<a class="ae lu" href="https://machinelearningmastery.com/k-fold-cross-validation/#:~:text=Cross%2Dvalidation%20is%20a%20resampling,k%2Dfold%20cross%2Dvalidation." rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="jd">交叉验证</em></strong></a><strong class="ih hj"><em class="jd">—</em></strong>来拆分我们的数据</p><p id="382a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<em class="jd">交叉验证</em>中，我们没有将数据拆分为<em class="jd">两部分</em>，而是拆分为3部分<em class="jd">(或者</em> <strong class="ih hj"> <em class="jd"> K </em> </strong> <em class="jd">取决于</em> <strong class="ih hj"> <em class="jd"> K重交叉验证</em> </strong> <em class="jd">中的K值)。</em>训练数据、交叉验证数据和测试数据。在这里，我们使用训练数据来寻找最近的邻居，我们使用交叉验证数据来寻找“K”<em class="jd">(K个邻居在这里)</em>的最佳值，最后我们在完全看不见的测试数据上测试我们的模型。这个测试数据相当于未来看不见的数据点。</p><p id="ba9d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">让我们从Sklearn导入更多的辅助函数来评估我们的模型:</em></p><pre class="jl jm jn jo fd mq mr ms mt aw mu bi"><span id="bee0" class="lw jx hi mr b fi mv mw l mx my"><strong class="mr hj">from</strong> <strong class="mr hj">sklearn.metrics</strong> <strong class="mr hj">import</strong> precision_recall_fscore_support<br/><strong class="mr hj">from</strong> <strong class="mr hj">sklearn.metrics</strong> <strong class="mr hj">import</strong> accuracy_score</span></pre><p id="4467" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">在从3到100的所有可能的K值(奇数)上训练我们的模型:</em></p><pre class="jl jm jn jo fd mq mr ms mt aw mu bi"><span id="9c4d" class="lw jx hi mr b fi mv mw l mx my">kVals = np.arange(3,100,2)<br/>accuracies = []<br/><strong class="mr hj">for</strong> k <strong class="mr hj">in</strong> kVals:<br/>  model = KNN(K = k)<br/>  model.fit(X_train, y_train)<br/>  pred = model.predict(X_test)<br/>  acc = accuracy_score(y_test, pred)<br/>  accuracies.append(acc)<br/>  print("K = "+str(k)+"; Accuracy: "+str(acc))</span><span id="eab0" class="lw jx hi mr b fi na mw l mx my"><strong class="mr hj">Out:</strong><br/>K = 3; Accuracy: 0.9755555555555555<br/>K = 5; Accuracy: 0.9755555555555555<br/>K = 7; Accuracy: 0.9755555555555555<br/>K = 9; Accuracy: 0.9755555555555555<br/>K = 11; Accuracy: 0.9733333333333334<br/>K = 13; Accuracy: 0.9711111111111111<br/>K = 15; Accuracy: 0.9688888888888889<br/>K = 17; Accuracy: 0.9666666666666667<br/>K = 19; Accuracy: 0.9666666666666667<br/>K = 21; Accuracy: 0.9666666666666667<br/>K = 23; Accuracy: 0.9644444444444444<br/>K = 25; Accuracy: 0.9644444444444444<br/>K = 27; Accuracy: 0.9666666666666667<br/>K = 29; Accuracy: 0.9622222222222222<br/>K = 31; Accuracy: 0.96<br/>K = 33; Accuracy: 0.96<br/>K = 35; Accuracy: 0.9577777777777777<br/>K = 37; Accuracy: 0.9577777777777777<br/>K = 39; Accuracy: 0.9577777777777777<br/>K = 41; Accuracy: 0.9555555555555556<br/>K = 43; Accuracy: 0.9511111111111111<br/>K = 45; Accuracy: 0.9488888888888889<br/>K = 47; Accuracy: 0.9444444444444444<br/>K = 49; Accuracy: 0.9444444444444444<br/>K = 51; Accuracy: 0.9377777777777778<br/>K = 53; Accuracy: 0.9355555555555556<br/>K = 55; Accuracy: 0.9333333333333333<br/>K = 57; Accuracy: 0.9333333333333333<br/>K = 59; Accuracy: 0.9311111111111111<br/>K = 61; Accuracy: 0.9333333333333333<br/>K = 63; Accuracy: 0.9333333333333333<br/>K = 65; Accuracy: 0.9311111111111111<br/>K = 67; Accuracy: 0.9288888888888889<br/>K = 69; Accuracy: 0.9266666666666666<br/>K = 71; Accuracy: 0.9288888888888889<br/>K = 73; Accuracy: 0.9311111111111111<br/>K = 75; Accuracy: 0.9288888888888889<br/>K = 77; Accuracy: 0.9266666666666666<br/>K = 79; Accuracy: 0.92<br/>K = 81; Accuracy: 0.9222222222222223<br/>K = 83; Accuracy: 0.9222222222222223<br/>K = 85; Accuracy: 0.92<br/>K = 87; Accuracy: 0.9177777777777778<br/>K = 89; Accuracy: 0.9177777777777778<br/>K = 91; Accuracy: 0.9111111111111111<br/>K = 93; Accuracy: 0.9111111111111111<br/>K = 95; Accuracy: 0.9088888888888889<br/>K = 97; Accuracy: 0.9088888888888889<br/>K = 99; Accuracy: 0.9066666666666666</span></pre><p id="0ad3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">模型在K=3时最准确:</em></p><pre class="jl jm jn jo fd mq mr ms mt aw mu bi"><span id="25b2" class="lw jx hi mr b fi mv mw l mx my">max_index = accuracies.index(max(accuracies))<br/>print(max_index)</span><span id="1de7" class="lw jx hi mr b fi na mw l mx my"><strong class="mr hj">Out:<br/></strong>0</span></pre><p id="dc2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">绘制我们的准确度:</em></p><pre class="jl jm jn jo fd mq mr ms mt aw mu bi"><span id="bc49" class="lw jx hi mr b fi mv mw l mx my"><strong class="mr hj">from</strong> <strong class="mr hj">matplotlib</strong> <strong class="mr hj">import</strong> pyplot <strong class="mr hj">as</strong> plt <br/>plt.plot(kVals, accuracies) <br/>plt.xlabel("K Value") <br/>plt.ylabel("Accuracy")</span><span id="2f34" class="lw jx hi mr b fi na mw l mx my"><strong class="mr hj">Out:<br/></strong>Text(0, 0.5, 'Accuracy')</span></pre><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es nb"><img src="../Images/1fda694b1c90b103ce05c4fb7a56728d.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*DrUNoxTRQN0hPOCerlPFmg.png"/></div></figure><p id="d8b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">检查精确度、召回率和F值(我们最精确的K值):</em></p><pre class="jl jm jn jo fd mq mr ms mt aw mu bi"><span id="4dd6" class="lw jx hi mr b fi mv mw l mx my">model = KNN(K = 3) <br/>model.fit(X_train, y_train) <br/>pred = model.predict(X_train)</span><span id="baa3" class="lw jx hi mr b fi na mw l mx my">precision, recall, fscore, _ = precision_recall_fscore_support(y_train, pred)<br/>print("Precision <strong class="mr hj">\n</strong>", precision)<br/>print("<strong class="mr hj">\n</strong>Recall <strong class="mr hj">\n</strong>", recall)<br/>print("<strong class="mr hj">\n</strong>F-score <strong class="mr hj">\n</strong>", fscore)</span><span id="fb9e" class="lw jx hi mr b fi na mw l mx my"><strong class="mr hj">Out:</strong></span><span id="b5b3" class="lw jx hi mr b fi na mw l mx my">Precision <br/> [1.         0.9929078  1.         1.         1.         1.<br/> 0.98518519 1.         0.9921875  0.99275362]<br/><br/>Recall <br/> [1.         1.         1.         1.         1.         0.99253731<br/> 1.         0.99275362 0.98449612 0.99275362]<br/><br/>F-score <br/> [1.         0.99644128 1.         1.         1.         0.99625468<br/> 0.99253731 0.99636364 0.98832685 0.99275362]</span></pre><p id="4df6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">在我们的测试集上执行我们的训练模型的推断:</em></p><pre class="jl jm jn jo fd mq mr ms mt aw mu bi"><span id="d483" class="lw jx hi mr b fi mv mw l mx my">model = KNN(K = 3)<br/>model.fit(X_train, y_train)<br/>pred = model.predict(X_test)<br/>acc = accuracy_score(y_test, pred)</span><span id="ce34" class="lw jx hi mr b fi na mw l mx my">precision, recall, fscore, _ = precision_recall_fscore_support(y_test, pred)<br/>print("Precision <strong class="mr hj">\n</strong>", precision)<br/>print("<strong class="mr hj">\n</strong>Recall <strong class="mr hj">\n</strong>", recall)<br/>print("<strong class="mr hj">\n</strong>F-score <strong class="mr hj">\n</strong>", fscore)</span><span id="1c0f" class="lw jx hi mr b fi na mw l mx my"><strong class="mr hj">Out:<br/></strong>Precision <br/> [1.         0.89361702 1.         0.93023256 0.98113208 1.<br/> 1.         1.         1.         0.95      ]<br/><br/>Recall <br/> [1.         1.         0.97560976 1.         1.         0.95833333<br/> 1.         1.         0.91111111 0.9047619 ]<br/><br/>F-score <br/> [1.         0.94382022 0.98765432 0.96385542 0.99047619 0.9787234<br/> 1.         1.         0.95348837 0.92682927]</span><span id="6e21" class="lw jx hi mr b fi na mw l mx my">print(acc) <em class="jd">#testing accuracy</em></span><span id="dc96" class="lw jx hi mr b fi na mw l mx my"><strong class="mr hj">Out:<br/></strong>0.9755555555555555 </span></pre><blockquote class="je jf jg"><p id="6d81" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">我知道一下子很难接受。但是你坚持到了最后！恭喜你。不要忘记看看我即将发表的文章！</p></blockquote><h1 id="247d" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">其他资源和参考</h1><div class="nc nd ez fb ne nf"><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab dw"><div class="nh ab ni cl cj nj"><h2 class="bd hj fi z dy nk ea eb nl ed ef hh bi translated">k-最近邻算法</h2><div class="nm l"><h3 class="bd b fi z dy nk ea eb nl ed ef dx translated">在统计学中，k-最近邻算法(k-NN)是一种非参数的机器学习方法，首先由…</h3></div><div class="nn l"><p class="bd b fp z dy nk ea eb nl ed ef dx translated">en.wikipedia.org</p></div></div><div class="no l"><div class="np l nq nr ns no nt jq nf"/></div></div></a></div><div class="nc nd ez fb ne nf"><a rel="noopener follow" target="_blank" href="/@kunal_gohrani/different-types-of-distance-metrics-used-in-machine-learning-e9928c5e26c7"><div class="ng ab dw"><div class="nh ab ni cl cj nj"><h2 class="bd hj fi z dy nk ea eb nl ed ef hh bi translated">机器学习中使用的不同类型的距离度量</h2><div class="nm l"><h3 class="bd b fi z dy nk ea eb nl ed ef dx translated">在这篇博文中，我们将了解机器学习模型中使用的一些距离度量。</h3></div><div class="nn l"><p class="bd b fp z dy nk ea eb nl ed ef dx translated">medium.com</p></div></div><div class="no l"><div class="nu l nq nr ns no nt jq nf"/></div></div></a></div><div class="nc nd ez fb ne nf"><a href="https://machinelearningmastery.com/k-fold-cross-validation/#:~:text=Cross%2Dvalidation%20is%20a%20resampling,k%2Dfold%20cross%2Dvalidation" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab dw"><div class="nh ab ni cl cj nj"><h2 class="bd hj fi z dy nk ea eb nl ed ef hh bi translated">k-fold交叉验证的温和介绍-机器学习掌握</h2><div class="nm l"><h3 class="bd b fi z dy nk ea eb nl ed ef dx translated">交叉验证是一种统计方法，用于评估机器学习模型的技能。它通常用于…</h3></div><div class="nn l"><p class="bd b fp z dy nk ea eb nl ed ef dx translated">machinelearningmastery.com</p></div></div><div class="no l"><div class="nv l nq nr ns no nt jq nf"/></div></div></a></div><div class="nc nd ez fb ne nf"><a href="https://towardsdatascience.com/cross-validation-using-knn-6babb6e619c8" rel="noopener follow" target="_blank"><div class="ng ab dw"><div class="nh ab ni cl cj nj"><h2 class="bd hj fi z dy nk ea eb nl ed ef hh bi translated">使用KNN进行交叉验证</h2><div class="nm l"><h3 class="bd b fi z dy nk ea eb nl ed ef dx translated">了解交叉验证，它的需要和k-fold交叉验证</h3></div><div class="nn l"><p class="bd b fp z dy nk ea eb nl ed ef dx translated">towardsdatascience.com</p></div></div><div class="no l"><div class="nw l nq nr ns no nt jq nf"/></div></div></a></div><p id="b265" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">完整的代码，请查看我的GitHub库— </em> </strong></p><div class="nc nd ez fb ne nf"><a href="https://github.com/tanvipenumudy/Winter-Internship-Internity/blob/main/Day%2008/Day-8%20Notebook-1%20%28MNIST%20Digit%20Recognition%29.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab dw"><div class="nh ab ni cl cj nj"><h2 class="bd hj fi z dy nk ea eb nl ed ef hh bi translated">tanvipenumudy/Winter-实习-实习</h2><div class="nm l"><h3 class="bd b fi z dy nk ea eb nl ed ef dx translated">存储库跟踪每天分配的工作-tanvipenumudy/Winter-实习-实习</h3></div><div class="nn l"><p class="bd b fp z dy nk ea eb nl ed ef dx translated">github.com</p></div></div><div class="no l"><div class="nx l nq nr ns no nt jq nf"/></div></div></a></div><p id="72ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">名人人脸识别从零开始使用KNN—</em></strong></p><div class="nc nd ez fb ne nf"><a href="https://tp6145.medium.com/celebrity-face-recognition-using-knn-from-scratch-76287bdab088" rel="noopener follow" target="_blank"><div class="ng ab dw"><div class="nh ab ni cl cj nj"><h2 class="bd hj fi z dy nk ea eb nl ed ef hh bi translated">从零开始使用KNN的名人人脸识别</h2><div class="nm l"><h3 class="bd b fi z dy nk ea eb nl ed ef dx translated">你能从下面的图片中找出你最喜欢的名人吗？当然可以！计算机是如何完成同样的任务的…</h3></div><div class="nn l"><p class="bd b fp z dy nk ea eb nl ed ef dx translated">tp6145.medium.com</p></div></div><div class="no l"><div class="ny l nq nr ns no nt jq nf"/></div></div></a></div><h1 id="9e3e" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">承认</h1><p id="593b" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq kw is it iu kx iw ix iy ky ja jb jc hb bi translated">有关数据集的更多详细信息，包括已在数据集上尝试的算法及其成功程度，请访问—<a class="ae lu" href="http://yann.lecun.com/exdb/mnist/index.html" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"><em class="jd">http://yann.lecun.com/exdb/mnist/index.html</em></strong></a></p></div></div>    
</body>
</html>