<html>
<head>
<title>Opening the black box of Machine learning — TensorFlow Dense layers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">打开机器学习的黑匣子— TensorFlow密集层</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/opening-the-black-box-of-machine-learning-tensorflow-dense-layers-8164441ed38f?source=collection_archive---------1-----------------------#2022-01-03">https://medium.com/analytics-vidhya/opening-the-black-box-of-machine-learning-tensorflow-dense-layers-8164441ed38f?source=collection_archive---------1-----------------------#2022-01-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="71c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我一直对机器学习有点困惑…它看起来像一个黑箱，我天生就对黑箱持怀疑态度。</p><p id="1af0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我想把引擎盖掀开一点，弄清楚ML是如何工作的，为此我想专注于一些简单的事情…</p><p id="71a6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">密集层如何确定一个<em class="jc"> y = x </em>关系？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/8007e3453ba7eb974688128e27faa465.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*Bf3UADA6LFP90SPXiRcfLg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">y = x</figcaption></figure><p id="8031" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我选择这个，只是因为我熟悉回归，我知道回归会如何处理这个问题，我并不真正理解ML会如何处理这个问题。</p><h1 id="a61f" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">稠密的一层</h1><p id="7f6e" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">密集层本质上是一组应用于输入的<em class="jc">权重</em>和<em class="jc">偏差</em>，包裹在<em class="jc">激活函数</em>中</p><p id="8a64" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在<em class="jc"> y = x </em>的例子中，我们有一个单一的输入<em class="jc"> x </em>，我们将应用一个权重<em class="jc"> w </em>和偏差<em class="jc"> b </em>，然后将它包装在一个激活函数<em class="jc"> a </em>中，给我们..</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="880b" class="kx jq hh kt b fi ky kz l la lb"><em class="jc">a(wx + b)</em></span></pre><p id="3edb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据我们希望我们的密集层有多少个单元/节点，我们可以用不同的权重和偏差重复这个过程</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="ff9b" class="kx jq hh kt b fi ky kz l la lb">[a(w₀x + b₀),<br/> a(w₁x + b₁), <br/> ... <br/> a(wₙx + bₙ)]</span></pre><p id="e695" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">激活功能:有几个不同的，但是我们将集中在<em class="jc"> relu </em>上，它非常简单。</p><p id="eac1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">relu(x)= x if x&gt;0 else 0</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lc"><img src="../Images/45117c49b0289252933e5214593cad54.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*hbl3fZ4Fp-YSuWqTKMM7zg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">relu激活功能</figcaption></figure><h1 id="3cb9" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">学习y = x</h1><p id="3bfb" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">你可能想知道……如果x看起来是线性运算，那么这个密集层将如何计算出像x这样的非线性关系。</p><p id="6574" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此..假设您有一个密集(2)层(具有两个节点的密集层)，为了简单起见，将权重保持为1，将偏差保持为0</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="6520" class="kx jq hh kt b fi ky kz l la lb"><strong class="kt hi">Dense(2) layer</strong></span><span id="e4b6" class="kx jq hh kt b fi ld kz l la lb">node 0 = a(w₀x + b₀) = relu(x)</span><span id="b374" class="kx jq hh kt b fi ld kz l la lb">node 1 = a(w₁x + b₁) = relu(-x)</span></pre><p id="ee99" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们将这些输出输入到具有线性激活函数(a(x) = x)的密集(1)层中，我们将得到(再次保持权重和偏差为1和0)</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="7e23" class="kx jq hh kt b fi ky kz l la lb"><strong class="kt hi">Dense(1) layer</strong> (linear activation function a(x) = x)</span><span id="3e33" class="kx jq hh kt b fi ld kz l la lb">node 0 = a(w₀relu(x) + b₀) + a(w₁relu(-x) + b₁) <br/>node 0 = relu(x) + relu(-x)</span></pre><p id="456f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">策划这个开始看起来有希望了..</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lc"><img src="../Images/3ec0dcfa69de5b8e70c137102b7ac485.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*yPP3y5icRR31HA8akcGCMQ.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">relu(x) + relu(-x)</figcaption></figure><p id="c1c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，我们的机器学习拓扑将是具有relu激活功能的密集(2)层，连接到具有线性激活功能的密集(1)层。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es le"><img src="../Images/50f8d924379d3739b2aa8b8f92f25030.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*e5kkVHAho33dh7COx2Ufzg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">我们的ML拓扑</figcaption></figure><p id="354a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是你如何在张量流中定义它</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="2d3f" class="kx jq hh kt b fi ky kz l la lb">model = keras.Sequential([<br/>  layers.Dense(units = 2, activation = 'relu'),<br/>  layers.Dense(1, activation = None)<br/>])</span></pre><p id="0929" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">希望你能开始明白这整个ML的工作原理…你把复杂的关系分解成一系列线性函数，用relu这样的非线性激活函数相互叠加。</p><h1 id="7162" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">增加节点</h1><p id="b59f" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">你可能已经猜到了，通过增加第一个密集层中的节点/单元数量，我们可以得到一个更好的模型</p><p id="644a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是Tensorflow中的一个示例，使用均方误差损失进行训练</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="9f70" class="kx jq hh kt b fi ky kz l la lb">import tensorflow as tf<br/>from tensorflow import keras<br/>from tensorflow.keras import layers</span><span id="95db" class="kx jq hh kt b fi ld kz l la lb">import numpy as np <br/>import matplotlib<br/>from matplotlib import pyplot as plt</span><span id="cc83" class="kx jq hh kt b fi ld kz l la lb">m1 = keras.Sequential([<br/>  # need this to flatten the input [1,2,3] =&gt; [[1],[2],[3]]<br/>  layers.Flatten(), <br/>  layers.Dense(units = 4, <br/>               activation = 'relu'), <br/>  layers.Dense(1)<br/>])</span><span id="9845" class="kx jq hh kt b fi ld kz l la lb">m1.compile(<br/>    loss=tf.keras.losses.MSE,<br/>    optimizer=tf.optimizers.Adam(learning_rate=0.01))</span><span id="5db5" class="kx jq hh kt b fi ld kz l la lb">m1.fit(x, y, epochs=1000, batch_size=10, verbose=0)</span><span id="d2f8" class="kx jq hh kt b fi ld kz l la lb"># plot the results <br/>plt.plot(x , y)<br/>plt.plot(x, m1(x))</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/7c279601bff48f5c4c33c88bed119342.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*PYYujJt0yEcCSoIKbXLRhg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">橙色训练后的模型输出</figcaption></figure><p id="802b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们在引擎盖下看看第一个密集层生成的4条线</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="76b3" class="kx jq hh kt b fi ky kz l la lb"># Get our 1st dense layer <br/>l = m1.layers[1] <br/># get x in the shape expected by the layer <br/>x1 = x.reshape((-1, 1)) <br/># apply the layer weights and bias <br/>y1 = (np.dot(x1, l.kernel.numpy()) + l.bias.numpy())<br/># apply relu <br/>y1[y1 &lt; 0] = 0 <br/># plot lines <br/>plt.plot(x, y1)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lc"><img src="../Images/d9ba41f31c5b137b74668d716f75151c.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*D8o32IuqrlbgJBbbI0r6WA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">由我们的模型训练的4条线</figcaption></figure><p id="f43b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你真的可以看到，把这些加在一起，可以开始逼近y = x</p><h1 id="de83" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">机器学习</h1><p id="9239" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">现在你明白了如何堆叠密集的层来生成非线性图案，我将谈谈我们如何计算这些线的权重和偏差。</p><p id="38bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基本思想与回归非常相似，事实上我们在这个模型中使用了相同的损失函数。均方差。</p><p id="76ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里的损失函数定义为</p><pre class="je jf jg jh fd ks kt ku kv aw kw bi"><span id="2e9a" class="kx jq hh kt b fi ky kz l la lb">loss = (y₀ - m(x₀))² + (y₁ - m(x₁))² + ... + (yₙ - m(xₙ))²</span><span id="2aa2" class="kx jq hh kt b fi ld kz l la lb">where m is our model</span><span id="5eac" class="kx jq hh kt b fi ld kz l la lb">m(x) = w₀m₀(x) + b₀ + .. + w₀m₃(x) + b₃</span><span id="c47f" class="kx jq hh kt b fi ld kz l la lb">and mᵢ is our 1st dense layer </span><span id="a982" class="kx jq hh kt b fi ld kz l la lb">mᵢ(x) = wᵢx + bᵢ</span></pre><p id="2cda" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这基本上给了我们一些多项式方程，依赖于第一密集层的4个权重和偏差，以及第二层的4个权重和偏差</p><p id="c24a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不需要太多的细节，我们可以很容易地估计出8维空间中特定点的梯度，并开始向极小点的方向移动。</p><p id="ce6f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">显然，不能保证我们的8维空间只有一个极小点，可能会陷入局部极小。</p><p id="6186" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，机器学习模型要小心行事，不要过快地移动到确定的最小点(由learning_rate、批量大小和时期的选择来指定)</p><h1 id="be06" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">概括起来</h1><p id="462b" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">机器学习中的密集层只是一堆线的花哨名称，这些线与激活函数一起可以堆叠在一起，以模拟复杂的数据关系。</p><p id="42d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">这下有好戏看了！</em> </strong>既然是一堆台词，那就要在<em class="jc">新</em>数据上表现的真的很差..我说的新数据是指x的值，它与我们已经看到的值不接近…我们在-10，10之间的x上训练我们的模型…如果我们将x = 100输入到模型中，这将是可怕的..因此，关键的见解是…规范化您的数据！</p><p id="a0c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">机器学习与回归没有什么不同，在这个意义上，你试图找到损失函数的导数等于0。</p><p id="0874" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我希望这能让人们对机器学习工具包有更深的理解，因为大多数其他层都是基于类似的原理构建的。</p><p id="397f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">祝您好运</p></div></div>    
</body>
</html>