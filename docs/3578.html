<html>
<head>
<title>Feature selection for K-means</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-均值的特征选择</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-means-algorithm-in-4-parts-4-4-42bc6c781e46?source=collection_archive---------0-----------------------#2021-07-11">https://medium.com/analytics-vidhya/k-means-algorithm-in-4-parts-4-4-42bc6c781e46?source=collection_archive---------0-----------------------#2021-07-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d8b43b124ab6da7a7596d117d1107bca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-Ov4DLWjc7ejQOkj"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@edgr?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">edu·格兰德</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="319c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们处理高维数据集时，我们可能会遇到聚类方法的问题。特征选择是监督学习的一种众所周知的技术，但对于非监督学习(如聚类)方法来说却很少。这里我们将开发一个相对简单的贪婪算法来对Kaggle上的<a class="ae iu" href="https://www.kaggle.com/roshansharma/europe-datasets" rel="noopener ugc nofollow" target="_blank">欧洲数据集</a>执行变量选择。</p><p id="0ca3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该算法将具有以下步骤:</p><p id="d27c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">0.确保变量是数值型的且经过缩放，例如使用StandardScaler()及其fit_transform()方法</p><ol class=""><li id="b63f" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">选择要保留的最大变量(<em class="kc"> maxvars </em>)，最小和最大聚类数(<em class="kc"> kmin </em>和<em class="kc"> kmax </em>)并创建一个空列表:<em class="kc"> selected_variables </em>。</li><li id="ccee" class="jt ju hi ix b iy kd jc ke jg kf jk kg jo kh js jy jz ka kb bi translated">从kmin到kmax循环。然后，依次使用每个变量，使用K-Means记录变量和聚类数的每个组合的轮廓值(从kmin到kmax)。</li><li id="b7e2" class="jt ju hi ix b iy kd jc ke jg kf jk kg jo kh js jy jz ka kb bi translated">选择给出最大轮廓值的变量，将其添加到<em class="kc"> selected_variables </em>中，并将其从测试变量列表中移除。</li><li id="ae44" class="jt ju hi ix b iy kd jc ke jg kf jk kg jo kh js jy jz ka kb bi translated">通过使用<em class="kc"> selected_variables </em>列表并依次添加每个剩余变量，重复2和3中的过程，直到达到某个停止标准(在这种情况下是要保留的变量数量，<em class="kc"> maxvars </em>)。</li></ol><p id="f0a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，对于第一步，我们定义和初始化一些变量。</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="253f" class="kr ks hi kn b fi kt ku l kv kw">maxvars=3<br/>kmin=2<br/>kmax=8</span><span id="4d5a" class="kr ks hi kn b fi kx ku l kv kw">kmeans_kwargs = {"init": "random","n_init": 20,"max_iter": 1000,"random_state": 1984}<br/>cut_off=0.5</span><span id="b7cb" class="kr ks hi kn b fi kx ku l kv kw"># We also define a cols variables containing a list of all features:<br/>cols=list(df.columns)<br/></span><span id="7d54" class="kr ks hi kn b fi kx ku l kv kw"># We also set a list and a dictionary to store the silhouette values<br/># for each number of clusters tested so we can choose the k value<br/># maximising the silhouette score, with its corresponding features</span><span id="d7dd" class="kr ks hi kn b fi kx ku l kv kw">results_for_each_k=[]<br/>vars_for_each_k={}</span></pre><p id="1e43" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，我们创建三个嵌套循环，外部的一个循环遍历集群数量<em class="kc"> k </em>的值。然后我们有一个while循环来检查保留变量的数量是否低于由<em class="kc"> maxvars </em>设置的阈值。<em class="kc">选择变量</em>列表将保存保留的特征名称。<em class="kc">结果</em>列表将保存每个变量的轮廓值。</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="8f60" class="kr ks hi kn b fi kt ku l kv kw">for k in range(kmin,kmax+1):<br/>    selected_variables=[]<br/>    while(len(selected_variables)&lt;maxvars):<br/>        results=[]</span><span id="5e0a" class="kr ks hi kn b fi kx ku l kv kw">    selected_variables=[]<br/>    print(k)<br/>    while(len(selected_variables)&lt;maxvars):<br/>        results=[]</span></pre><p id="fb33" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">内部循环逐一检查所有特征，将它们添加到已选择的变量中(如果有)，并评估轮廓值。然后，它选择获得最高值的变量，并将其添加到<em class="kc"> selected_variables </em>列表中。</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="99bc" class="kr ks hi kn b fi kt ku l kv kw">for col in cols:<br/>            scols=[]<br/>            scols.extend(selected_variables)<br/>            scols.append(col) <br/>            kmeans = KMeans(n_clusters=k, **kmeans_kwargs)<br/>            kmeans.fit(df[scols])<br/>            results.append(silhouette_score(df[scols],   kmeans.predict(s)))</span><span id="d0ca" class="kr ks hi kn b fi kx ku l kv kw"># We identify the best variable, add it to our list and remove it <br/># from the list of variables to be tested on the next iteration<br/>        selected_var=cols[np.argmax(results)]<br/>        selected_variables.append(selected_var)<br/>        cols.remove(selected_var)</span></pre><p id="4317" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，我们可以在我们的循环中更新这个特定k值的变量列表和分数。</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="d02e" class="kr ks hi kn b fi kt ku l kv kw">results_for_each_k.append(max(results))<br/>vars_for_each_k[k]=selected_variables</span></pre><p id="8a5e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，在三个循环运行之后，我们可以确定k和变量的最佳组合，拟合模型并绘制它。</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="8615" class="kr ks hi kn b fi kt ku l kv kw">best_k=np.argmax(results_for_each_k)+kmin<br/>selected_variables=vars_for_each_k[best_k]<br/>kmeans = KMeans(n_clusters=best_k, **kmeans_kwargs)<br/>kmeans.fit(df_[selected_variables])<br/>clusters=kmeans.predict(df[selected_variables])</span></pre><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ky"><img src="../Images/3d143273c374f15a7104873f7a9ae725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4X_oM8j3BNYAbvX1I6lwQQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">基于人口、财富和犯罪率的2组国家的最终结果</figcaption></figure><p id="10e5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们选择3个集群，我们会得到不同的选择</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ky"><img src="../Images/b824c4e1ca0fec7539decce171591d5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nb_EDrgS2Qmik-ZXz_CNLw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">对于3个集群，我们有不同的变量选择</figcaption></figure><p id="5902" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">每组国家的一些例子:</p><p id="87eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">冰岛、瑞士、比利时、德国、卢森堡、荷兰、奥地利和英国</p><p id="3d4c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">集群2: </strong>希腊、西班牙、法国、克罗地亚、意大利、塞浦路斯、拉脱维亚、立陶宛、匈牙利、马耳他、波兰、葡萄牙</p><p id="e80d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">第三组:</strong>挪威、丹麦、芬兰和瑞典</p><p id="8dc7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是完整的代码，包括3d绘图。</p><figure class="ki kj kk kl fd ij"><div class="bz dy l di"><div class="kz la l"/></div></figure><p id="9fac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当然，我们可以将变量选择与受约束的K-Means算法相结合，以强制均匀聚类，如前面的<a class="ae iu" href="https://fmarthoz.medium.com/k-means-algorithm-in-4-parts-6f44dc21d119" rel="noopener">和</a>所示。</p></div></div>    
</body>
</html>