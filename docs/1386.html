<html>
<head>
<title>How to write a Neural Network in Tensorflow from scratch (without using Keras)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在Tensorflow中从头开始编写神经网络(不使用Keras)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-write-a-neural-network-in-tensorflow-from-scratch-without-using-keras-e056bb143d78?source=collection_archive---------1-----------------------#2021-02-27">https://medium.com/analytics-vidhya/how-to-write-a-neural-network-in-tensorflow-from-scratch-without-using-keras-e056bb143d78?source=collection_archive---------1-----------------------#2021-02-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/535bddc0dd7612b43bd37520b6f9ec56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*m7YV3mL3aq6S-lVs.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">来源:<a class="ae iq" href="https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/" rel="noopener ugc nofollow" target="_blank">图片搜索</a></figcaption></figure></div><div class="ab cl ir is gp it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hb hc hd he hf"><h1 id="539c" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">介绍</h1><p id="ff45" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">编写一个简单的前馈神经网络可能是你掌握深度学习之旅的第一步。今天，有无数的库和框架可用于开发机器学习模型。开发机器学习模型的两个最著名和最常用的框架是Tensorflow和PyTorch。Keras已经集成到Tensorflow2.0中，这无疑使以Tensorflow为主干编写任何机器学习代码变得更加容易。但是，如果您是该领域的初学者，您可能希望从头开始编写代码，以便了解模型的来龙去脉。我在互联网上搜索可以帮助我在TensorFlow中编写简单前馈神经网络的基本代码的资源，但没有找到任何资源。所以，我想到了自己写一个。以下是完成它的方法！</p><h1 id="c40b" class="iy iz hi bd ja jb ku jd je jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv bi translated"><strong class="ak">为什么不用Keras？</strong></h1><h2 id="b701" class="kz iz hi bd ja la lb lc je ld le lf ji kh lg lh jm kl li lj jq kp lk ll ju lm bi translated">1.keras模型和基本模型之间的学习差异</h2><p id="ece4" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">Keras是一个非常简单的库，可以帮助你毫不费力地用python编写机器学习代码。它又快又简单又干净！然而，在引擎盖下，Keras做了大量的工作来为您提供这种无缝的体验。如果你在Tensorflow的github知识库上查看像<code class="du ln lo lp lq b">model.fit()</code>或<code class="du ln lo lp lq b">model.train()</code>这样的函数的后端代码，你会发现它们包含了许多优化，如热启动和硬件层面上的函数优化实现。这些技巧肯定会让你的代码运行得更快，但这并不总是意味着你的模型正在学习它应该学习的方式。作为比较，如果你写一个简单的2-3层神经网络代码，使用和不使用keras，你会发现两个程序的损耗曲线有很大的不同。不使用keras编写的基本代码的损耗曲线将指示损耗值从高数值下降到低数值，而keras代码的损耗值将首先从低数值开始，并且与前一编码器不同，不会显著下降到较低值。这并不意味着keras模型不学习，而是意味着在基本代码中发生的学习的幅度大于在keras代码中发生的学习的幅度。优化keras代码就像剥一缕头发。</p><p id="e61f" class="pw-post-body-paragraph jw jx hi jy b jz lr kb kc kd ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt hb bi translated"><strong class="jy hj"> 2。更好地控制架构的执行<br/> </strong>有时候，你的架构可能不是一个简单的序列神经网络。它可能包含剩余/跳过连接或多个子神经网络。在这种情况下，您需要对您的架构的执行进行更多的控制。拥有一个定制代码对于这样的用例来说无疑是非常方便的。</p><p id="ea2a" class="pw-post-body-paragraph jw jx hi jy b jz lr kb kc kd ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt hb bi translated">用于比较的一些其他来源:<br/> 1。<a class="ae iq" href="https://www.javatpoint.com/tensorflow-vs-keras" rel="noopener ugc nofollow" target="_blank">https://www.javatpoint.com/tensorflow-vs-keras</a>T10】2。<a class="ae iq" href="https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2019/10/21/keras-vs-TF-keras-whats-the-difference-in-tensor flow-2-0/</a></p><h1 id="5740" class="iy iz hi bd ja jb ku jd je jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="790a" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">下面是一个简单的前馈神经网络的例子，它包含两个隐藏层，学习使用梯度下降优化来预测<code class="du ln lo lp lq b">mnist</code>数字。</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="er es lw"><img src="../Images/69127c92648fb278270b5cdcea21a64d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SL8FESMwzSy6QTrcIzcRYw.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">简单前馈神经网络</figcaption></figure><blockquote class="mf mg mh"><p id="9db8" class="jw jx mi jy b jz lr kb kc kd ls kf kg mj lt kj kk mk lu kn ko ml lv kr ks kt hb bi translated">我假设您已经下载了mnist数据集</p></blockquote><figure class="lx ly lz ma fd ij"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="6f26" class="pw-post-body-paragraph jw jx hi jy b jz lr kb kc kd ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt hb bi translated">在上面的代码片段中，我为一个网络创建了一个类并初始化它。<code class="du ln lo lp lq b">n_layers</code>包含网络中每层的节点数<br/>使用<code class="du ln lo lp lq b">tf.Variable()</code>函数初始化每个权重和偏差矩阵<br/>我们保存<code class="du ln lo lp lq b">self.params</code>中的所有参数以备将来使用</p><figure class="lx ly lz ma fd ij"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="1b09" class="pw-post-body-paragraph jw jx hi jy b jz lr kb kc kd ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt hb bi translated">在正向传递中，我们对矩阵执行线性代数。<br/>您应该在predict()函数中的激活函数之后返回完整的输出。在上面的代码片段中，Z3是logits，Y是最终的预测输出。</p><figure class="lx ly lz ma fd ij"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="a160" class="pw-post-body-paragraph jw jx hi jy b jz lr kb kc kd ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt hb bi translated">如上所述，你可以使用<code class="du ln lo lp lq b">tf.compat.v1.losses.sigmoid_cross_entropy()</code>或<code class="du ln lo lp lq b">tf.compat.v1.losses.softmax_cross_entropy()</code>，他们所做的就是在逻辑上应用sigmoid或softmax激活函数，然后计算损耗。为了使用这些功能，标签应该是一个热点矢量格式。更多信息，你可以参考以下链接，<br/> 1。<a class="ae iq" href="https://mmuratarat.github.io/2018-12-21/cross-entropy" rel="noopener ugc nofollow" target="_blank">https://mmuratarat.github.io/2018-12-21/cross-entropy</a>T21【2】。<a class="ae iq" href="https://gombru.github.io/2018/05/23/cross_entropy_loss/" rel="noopener ugc nofollow" target="_blank">https://gombru.github.io/2018/05/23/cross_entropy_loss/</a></p><figure class="lx ly lz ma fd ij"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="ae46" class="pw-post-body-paragraph jw jx hi jy b jz lr kb kc kd ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt hb bi translated">上述函数可用于执行反向传播。这可能是整个计划中最重要的部分。上述函数可作为TensorFlow中<code class="du ln lo lp lq b">model.fit()</code>的替代函数。</p><p id="94c6" class="pw-post-body-paragraph jw jx hi jy b jz lr kb kc kd ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt hb bi translated">这就对了。这就是如何在TensorFlow中构建一个非常基本的前馈神经网络，而无需使用任何像Keras这样的高级库。它的其余部分，如批处理加载器和评估指标的函数，很容易开发，因此我跳过了它们。</p><p id="4838" class="pw-post-body-paragraph jw jx hi jy b jz lr kb kc kd ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt hb bi translated">✌️和平</p></div></div>    
</body>
</html>