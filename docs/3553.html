<html>
<head>
<title>Less Known Applications of k-Means Clustering — Dimensionality Reduction, Anomaly Detection and Data Representation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-均值聚类的鲜为人知的应用——降维、异常检测和数据表示</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/less-known-applications-of-k-means-clustering-dimensionality-reduction-anomaly-detection-and-908f4bee155f?source=collection_archive---------2-----------------------#2021-07-09">https://medium.com/analytics-vidhya/less-known-applications-of-k-means-clustering-dimensionality-reduction-anomaly-detection-and-908f4bee155f?source=collection_archive---------2-----------------------#2021-07-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1502" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">k-Means是一种广泛用于聚类的数据划分技术。它有一些变种(比如小批量k-Means ),对于大量数据来说速度非常快。它的聚类结果也很容易解释。然而，k-Means还有很多应用没有被提及。它们是:</p><ol class=""><li id="55fe" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj">非线性降维</strong> —可用于仅使用几个“转换”特征来表示数千个特征，这些特征可用作ML管道中的工程特征或用于数据可视化。</li><li id="0d81" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">多元异常值/异常检测</strong></li><li id="34ff" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">数据表示(用于其他算法的输入)</strong>—通常，k-means只能检测球形簇。然而，可以将k-means的输出提供给分层聚类算法，以检测复杂形状的非凸聚类。</li></ol><p id="2f3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将通过示例和Python代码详细讨论上述应用。我们将使用<strong class="ih hj"> <em class="jr"> scikit-learn。</em> </strong>让我们先加载数据。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="0fa2" class="kb kc hi jx b fi kd ke l kf kg">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from sklearn import linear_model, preprocessing, model_selection, pipeline, ensemble, tree, datasets, cluster<br/>sns.set(style = 'white', font_scale = 1.4)</span><span id="56db" class="kb kc hi jx b fi kh ke l kf kg">######## Load the Data<br/>data  =  pd.DataFrame(datasets.load_boston().data, columns = datasets.load_boston().feature_names)<br/>y = datasets.load_boston().target</span><span id="5a5e" class="kb kc hi jx b fi kh ke l kf kg">features = ['CRIM', 'LSTAT', 'RM', 'AGE', 'INDUS', 'NOX', 'DIS']<br/>data = data[features]<br/>data.head()</span></pre><h1 id="b4eb" class="ki kc hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">基于K-均值的非线性降维</h1><p id="de7b" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">这个想法是使用k-Means来计算聚类中心，将聚类的数量设置为我们在转换后的数据中需要的维数。在我们的例子中，由于我们想将数据减少到2维，我们将聚类数设置为2。<strong class="ih hj">变换空间中的新特征是每个点到每个聚类中心的距离。</strong></p><p id="c128" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是我们如何使用K-Means进行非线性维数计算:</p><ol class=""><li id="7181" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">将k-Means拟合到数据。将分类数设置为我们想要将数据转换到的维数，在本例中为2。</li><li id="65d6" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">计算每个数据点到每个聚类中心的距离。这样，我们为每个数据点获得2个特征，即-(点与聚类1的距离，点与聚类2的距离)。如果我们有<strong class="ih hj"> k </strong>个聚类中心，我们将获得每个数据点的<strong class="ih hj"> k </strong>个特征。这些代表了我们转变后的特征。幸运的是，拟合K-means对象的<strong class="ih hj"> <em class="jr"> transform() </em> </strong>方法为我们做到了这一点。</li></ol><p id="2b8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑波士顿住房数据集中的7个特征。我们将尝试使用K-Means将它们减少到2维以便可视化。</p><p id="fa6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据如下所示:</p><figure class="js jt ju jv fd ll er es paragraph-image"><div class="er es lk"><img src="../Images/9fd27e04f0ec9a3e41491347af24d879.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*sAfK9c2qgHHazRXYBl1LEQ.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">波士顿住房数据的7个特征</figcaption></figure><p id="f76c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用Scikit-Learn管道来执行缩放和拟合k均值。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="d93e" class="kb kc hi jx b fi kd ke l kf kg">#### Scale and Fit KMeans to data as part of the pipeline ####<br/>kmeans = pipeline.make_pipeline(preprocessing.StandardScaler(), cluster.KMeans(n_clusters = 2)).fit(X_train)</span><span id="9127" class="kb kc hi jx b fi kh ke l kf kg"># Store transformed dimensions<br/>lower_dim = pd.DataFrame(kmeans.transform(X_train), columns = ['Comp 1', 'Comp 2'])</span><span id="ee67" class="kb kc hi jx b fi kh ke l kf kg">lower_dim.plot.scatter( x='Comp 1',y= 'Comp 2', grid = True, figsize = (10, 7))</span></pre><figure class="js jt ju jv fd ll er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es ls"><img src="../Images/1705b76439ea79354f2dea36032ffafa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lTkFNu48ZYYgAjUyDggp6A.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">2个降维——使用波士顿住房数据的K均值</figcaption></figure><p id="0ffc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，组件1测量每个点到聚类中心#1的距离。组件2测量每个点到聚类中心#2的距离。正如我们所见，右上角有一些孤立的点。正如我们将在后面讨论的，像这样的点可能被认为是异常值。</p><p id="50b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是使用变换要素的一些方法:</p><ol class=""><li id="1895" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj">数据可视化:</strong>对于具有数百个特征的数据，这允许我们在2D屏幕上对数据进行可视化和编码。可以根据各种感兴趣的量对数据进行颜色编码。</li><li id="664b" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">对于预测建模:</strong>这里，我们使用转换后的特征作为模型的输入。这对于线性模型特别有用，因为K-Means创建的要素是非线性变换，有助于说明数据中的非线性关系。</li><li id="b1f8" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">异常检测:</strong>一个点离聚类中心的距离越大，它成为异常的几率就越大。</li></ol><p id="83c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看如何使用它进行预测建模。我们使用sklearn管道。让我们首先对原始数据使用线性回归，而不执行转换:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="5edf" class="kb kc hi jx b fi kd ke l kf kg">### Train Test Split<br/>X_train, X_test, y_train, y_test = model_selection.train_test_split(data, y, test_size = .2, random_state = 10)</span><span id="9365" class="kb kc hi jx b fi kh ke l kf kg">### Fit a Linear Model<br/>model = linear_model.LinearRegression()</span><span id="8233" class="kb kc hi jx b fi kh ke l kf kg">score = model_selection.cross_val_score(model, X_train, y_train, cv = 10, scoring = 'r2')<br/>print(f'Average r2: {np.mean(score)}')</span><span id="274b" class="kb kc hi jx b fi kh ke l kf kg">Average r2: 0.6534847470004668</span></pre><p id="22df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们使用K-Means执行变换，并在变换后的特征上训练线性模型。我们使用sklearn管道和转换数据来实现5个功能:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="73f5" class="kb kc hi jx b fi kd ke l kf kg">km = pipeline.make_pipeline(preprocessing.StandardScaler(), cluster.KMeans(n_clusters = 5),<br/>                           linear_model.LinearRegression())</span><span id="ce2d" class="kb kc hi jx b fi kh ke l kf kg">score = model_selection.cross_val_score(km, X_train, y_train, cv = 10, scoring = 'r2')<br/>print(f'Average r2: {np.mean(score)}')</span><span id="c527" class="kb kc hi jx b fi kh ke l kf kg">Average r2: 0.6751640572278654</span></pre><p id="cdb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种情况下，模型性能通过使用5个变换的特征得到了提高。然而，<strong class="ih hj">在一般情况下，必须通过交叉验证找到或根据领域/业务专业知识决定转换特征的数量。</strong></p></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><h1 id="bdf2" class="ki kc hi bd kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la mi lc ld le bi translated"><strong class="ak">使用K均值的异常检测</strong></h1><p id="da3c" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">由k-Means识别的聚类中心可以被认为是数据的“代表”。聚类中心和聚类内方差汇总数据。<strong class="ih hj">使用k-Means的异常检测基于离所有聚类中心非常远的点更加异常的想法。</strong>这是有意义的，因为远离聚类中心的点平均来说远离整个数据。<strong class="ih hj">这是一种异常检测的多变量方法</strong>，不同于tukey的方法或z-score方法，它们是单变量方法。</p><p id="5596" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是我们计算异常分数的方法:</p><ol class=""><li id="3cd8" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">使用k-Means计算聚类中心。</li><li id="b96c" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">使用<strong class="ih hj">聚类的变换方法计算每个点到每个聚类中心的距离。KMeans() </strong>对象。</li><li id="318d" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">对每个点取这些距离的最小值，即我们计算每个点到其最近的聚类中心的距离。这个距离就是点的异常得分。<strong class="ih hj">异常值越大，表示该点越异常。</strong>也可以将每个聚类的每个点的平均距离视为异常得分。</li></ol><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="dfd3" class="kb kc hi jx b fi kd ke l kf kg">## Fit KMeans after standardizing data<br/>km = pipeline.make_pipeline(preprocessing.StandardScaler(), cluster.KMeans(n_clusters = 5)).fit(X_train)</span><span id="8951" class="kb kc hi jx b fi kh ke l kf kg">### Calculate distance of each point from each cluster center<br/>transformed = pd.DataFrame(km.transform(X_train))</span><span id="cc35" class="kb kc hi jx b fi kh ke l kf kg"># Calculate the Anomaly score - as the minimum distance of a point to any cluster center<br/>anomaly_score = lower_dim.min(axis = 1)</span><span id="c229" class="kb kc hi jx b fi kh ke l kf kg">plt.figure( figsize = (12, 7))<br/>plt.scatter(x=lower_dim['Comp 1'],y= lower_dim['Comp 2'], c = anomaly_score, cmap = 'coolwarm')<br/>plt.colorbar(label = 'Anomaly Score')<br/>plt.xlabel('Component 1')<br/>plt.ylabel('Component 2')<br/>plt.grid()</span></pre><figure class="js jt ju jv fd ll er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mj"><img src="../Images/0f5671f8d78c85e629fd71e1bf8524b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIaYsXqOhpVPxLt7dlDN9w.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">正如我们所见，红点是最不规则的——这种方法可以正确识别。</figcaption></figure><p id="0622" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的红点被检测为异常，因为它们具有更高的异常分数。与许多其他方法一样，这种方法将数据转换到新的空间，并使用这种表示将点标记为异常。<strong class="ih hj">关于这类异常检测方法的更多分析，请查看我的文章:<br/></strong><a class="ae mk" rel="noopener" href="/analytics-vidhya/anomaly-detection-in-python-part-1-basics-code-and-standard-algorithms-37d022cdbcff"><strong class="ih hj">https://medium . com/analytics-vid hya/anomaly-detection-in-python-part-1-basics-code-and-standard-algorithms-37d 022 CDB CFF</strong></a></p><p id="7d90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用k-Means执行异常检测的另一种方法是选择大量的聚类——足够大以至于一些聚类只有很少的数据点。数据点非常少的聚类可能被怀疑是异常的，必须进一步分析。</p></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><h1 id="dc35" class="ki kc hi bd kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la mi lc ld le bi translated">其他算法的输入</h1><p id="f2e5" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">K-means可用于通过聚类中心表示数据来构建数据的“汇总”版本。聚类中心又可以用作其他算法的输入。以下是k-means聚类中心如何用作其他算法的输入的一些示例:</p><p id="a8f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.K-Means检测“球形”聚类。然而，有时我们需要检测非凸形状的聚类，这是不可能通过k-means单独正确检测的。在这种情况下，我们通常使用单链层次聚类或其他能够检测此类聚类的方法。然而，像分层聚类这样的方法需要很长时间才能完成——这使得它们对于大量数据来说不切实际。在这些情况下，可以使用第一个<strong class="ih hj">使用k-means来使用大量聚类对数据进行聚类，并使用聚类中心作为分层聚类算法(或任何其他可以检测非凸形聚类的聚类算法)的输入。</strong>这显著减少了聚类数据所需的时间，并且还能够检测非凸聚类。</p><p id="3f30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.K-Means聚类中心可以用作GMM(高斯混合模型)的初始化。GMM可用于聚类，并可用于构建数据的生成模型。</p></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><h1 id="9b22" class="ki kc hi bd kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la mi lc ld le bi translated">摘要</h1><p id="715a" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">综上所述，k均值可以用于多种用途。我们可以使用它来执行维数缩减，其中每个变换的特征是该点与聚类中心的距离<strong class="ih hj">。<strong class="ih hj">在使用它执行异常检测时，我们测量每个点到其最近的聚类中心的距离。如果这个距离足够大，我们就称之为异常。</strong>最后，我们看到了如何将聚类中心用作分层聚类和GMM等算法的输入。在描述性分析和半监督学习的领域中，k-means有更多的用途。</strong></p><p id="7414" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望你喜欢我的文章——如果你有任何反馈，请随时告诉我，并查看我写的关于DS和ML的其他文章<a class="ae mk" href="https://nitishkthakur.medium.com/" rel="noopener">这里</a></p></div></div>    
</body>
</html>