<html>
<head>
<title>Gaussian Naive Bayes Classifier in C++</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">C++中的高斯朴素贝叶斯分类器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gaussian-naive-bayes-classifier-in-c-846e9a22aefc?source=collection_archive---------9-----------------------#2021-06-21">https://medium.com/analytics-vidhya/gaussian-naive-bayes-classifier-in-c-846e9a22aefc?source=collection_archive---------9-----------------------#2021-06-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/34299e35e06c44940645c47869af1f8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*6iJO3i8m0maPRS_UrYTHxA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">贝叶斯统计</figcaption></figure><h1 id="f6f3" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">目标</h1><p id="1a3c" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">基于条件概率的贝叶斯定理在C++中从头构建一个分类器，而不需要使用Eigen这样的外部第三方库！从头开始纯粹而有趣的编码。</p></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><h1 id="3bd8" class="iq ir hi bd is it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn bi translated">介绍</h1><p id="a72a" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">高斯朴素贝叶斯(GNB)是一种使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Conditional_probability" rel="noopener ugc nofollow" target="_blank">条件概率确定结果的概率方法。</a>顾名思义，它是“幼稚的”,因为它强烈假设“特征”都是相互独立的，并且具有由给定类别“c”和特征“x”的高斯分布定义的可能性。</p></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><h1 id="0607" class="iq ir hi bd is it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn bi translated">该算法</h1><p id="df42" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">这一节首先讨论条件概率的理论，然后讨论它的实现</p><h2 id="3afe" class="kz ir hi bd is la lb lc iw ld le lf ja jz lg lh je kd li lj ji kh lk ll jm lm bi translated">理论</h2><p id="cdf6" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">因为GNB是基于条件概率模型的，所以首先用简单的词语，然后用数学符号来表达手头的问题是非常简单的。</p><p id="a37f" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">基本上，</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/5d30d525e1a3c39a4e9dbc3b87c80d91.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/1*OKs4gRquA1DSur7290ysGw.gif"/></div></figure><p id="260c" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">让我们定义一些变量。</p><p id="1f57" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">CK—K个可能类别中的每一个<br/> n —特征数量<br/> x —特征</p><p id="43e7" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">利用这一点，我们可以定义要分类的问题，</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/ab07062728b2e4ba600d3e1743b6b675.png" data-original-src="https://miro.medium.com/v2/resize:fit:230/1*57Pz4yJO7vSR5GDHE6GyYA.gif"/></div></figure><p id="f65b" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">其中，每个“x”是独立的特征。这是条件概率的朴素贝叶斯定理的基本假设。利用这一点，基本方程可以用数学符号表示。[ <a class="ae ky" href="https://link.springer.com/chapter/10.1007/978-0-85729-495-1_4" rel="noopener ugc nofollow" target="_blank"> 1 </a></p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/3af268345fe5ddadfb893ea555c2e470.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/1*sYnE6vV10QiWdWVonpCqDg.gif"/></div></figure><p id="9dc9" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">分数的分子很重要，因为分母只是一个归一化项。因此，它可以写成:</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/7fa1af33b43b2ffb9b429497877cbca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/1*gdDrbTfRCdGydvk6LvV-TA.gif"/></div></figure><p id="4848" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">使用链式法则，它可以扩展为:</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="er es ma"><img src="../Images/e58c6a8a293d3e93eec954f233622d8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*zovf51AIojEujkqxhVA7Eg.gif"/></div></div></figure><p id="eaf0" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">天真的条件独立性假设导致:</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/93009002b0d1e33bd0c8934b7712aa7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/1*DlpIkdpzz8yxJBvNHCfcNQ.gif"/></div></figure><p id="06ed" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">因此，联合概率模型简化为:</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/8b36ba9ebb3ea08640a42b9024afb7fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/1*vYjcsuTzNMvAYp-7PYmWvQ.gif"/></div></figure><p id="94a5" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">其中N是归一化因子</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/e8092d662c2d515426bc757fad42d8f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/1*447CILfw7VrWLEWVDlykbw.gif"/></div></figure><p id="4c15" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated"><strong class="jq hj">分类</strong></p><p id="6318" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">为了建立分类器，使用计算出的概率的最大值来确定预测的类别。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/e0cb14f95b5c7be8a45ede8fcee66e81.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/1*LuCKb0cHFUaGQg1Oxy45eg.gif"/></div></figure><p id="4726" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated"><strong class="jq hj">培训</strong></p><p id="d7d1" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">当训练数据以连续属性“x”为特征时，假设分布为<strong class="jq hj">正态</strong>。所以下一步就是计算每一类中<strong class="jq hj"> x </strong>的方差(σ _k)和均值(μ_k)。哪里；</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/430e6782a59ff9bfecade7f131511135.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/1*lrBcz7bpE44ROJdgykbukQ.gif"/></div></figure><p id="e7bf" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">计算出的值将导致每个类别的<strong class="jq hj">先验</strong>概率。[ <a class="ae ky" href="https://link.springer.com/chapter/10.1007/978-0-85729-495-1_4" rel="noopener ugc nofollow" target="_blank"> 1 </a></p><h2 id="4941" class="kz ir hi bd is la lb lc iw ld le lf ja jz lg lh je kd li lj ji kh lk ll jm lm bi translated">履行</h2><p id="d1ce" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">实现可以在这个<a class="ae ky" href="https://github.com/Aparajith-S/Gaussian_Naive_Bayes" rel="noopener ugc nofollow" target="_blank">库</a>中找到。</p><p id="130d" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated"><strong class="jq hj">注意:</strong>在存储库中，可以找到分别在“classifier.h”和“classifier.cpp”中定义了<strong class="jq hj"> GNB </strong>的源代码。从应用的角度来看，只需要<strong class="jq hj"> GNB::训练(…) </strong>和<strong class="jq hj"> GNB::预测(…) </strong>就可以了，但这不是我们的目标。目标是理解为了实现GNB分类器已经做了什么。</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="mk ml l"/></div></figure><p id="38bd" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">其中的函数</p><pre class="lt lu lv lw fd mm mn mo mp aw mq bi"><span id="d588" class="kz ir hi mn b fi mr ms l mt mu">void CalcStatistics(const vector&lt;vector&lt;double&gt;&gt;&amp; data,                               const vector&lt;string&gt;&amp; labels);</span></pre><p id="cd52" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">负责计算先验概率。通过存储每类(即标签)C_k的标准差(σ_k)和均值(μ_k)</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="mk ml l"/></div></figure><p id="c861" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated"><strong class="jq hj"> GNB::预测(…) </strong>函数接收测试样本并预测标签。</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="mk ml l"/></div></figure><p id="b602" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">这将计算测试样本相对于所有标签的概率。</p><p id="7277" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">这当然需要使用之前讨论过的高斯分布即正态分布来计算概率。</p><pre class="lt lu lv lw fd mm mn mo mp aw mq bi"><span id="e0f7" class="kz ir hi mn b fi mr ms l mt mu">double GNB::gaussian(double x, double mu, double sigma) const                       {                           double expr1 = 1.0 / sqrt(2 * M_PI);                             double expr2 = pow((x - mu) / sigma, 2)*-0.5;<br/>return (expr1 / sigma) * exp(expr2);   <br/>}</span></pre><p id="1d8f" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">下一个重要的步骤是采用argmax。这将完成分类器的实现</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="mk ml l"/></div></figure></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><h1 id="2ce9" class="iq ir hi bd is it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn bi translated">数据</h1><p id="4e0c" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">GNB适用于连续的要素输入。因此，使用不同车道中的车辆的轨迹数据，在道路上进行发现目标车辆的车道变换意图的实验。如图所示，有三种可能的标签。</p><p id="aa73" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">‘保持’—保持车道，即不变道(黑色)<br/>‘右’—向右变道(红色)<br/>‘左’—向左变道(蓝色)</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="er es mv"><img src="../Images/54790207034567730f0d3f5d83e32b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2m4eBez8SI6cxHY8ybdGXQ.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">图1:样本生成的数据</figcaption></figure><p id="d75b" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">这些数据被分成两个文件，正如您在<a class="ae ky" href="https://github.com/Aparajith-S/Gaussian_Naive_Bayes/tree/main/data" rel="noopener ugc nofollow" target="_blank">存储库/数据</a>中看到的:</p><pre class="lt lu lv lw fd mm mn mo mp aw mq bi"><span id="74ea" class="kz ir hi mn b fi mr ms l mt mu">// load the train and test data<br/>vector&lt;vector&lt;double&gt; &gt; X_train = Load_State("../data/train_states.txt");                         <br/>vector&lt;vector&lt;double&gt; &gt; X_test  = Load_State("../data/test_states.txt");                         <br/>vector&lt;string&gt; Y_train = Load_Label("../data/train_labels.txt");                         vector&lt;string&gt; Y_test  = Load_Label("../data/test_labels.txt");</span><span id="63a5" class="kz ir hi mn b fi mw ms l mt mu">//Training<br/>GNB gnb = GNB();                                                  gnb.train(X_train, Y_train);</span><span id="c9d5" class="kz ir hi mn b fi mw ms l mt mu">//Predictions<br/>int score = 0;                         <br/>for (int i = 0; i &lt; X_test.size(); ++i) {<br/>vector&lt;double&gt; coords = X_test[i];<br/>string predicted = gnb.predict(coords);<br/>//find accuracy.<br/>if (predicted.compare(Y_test[i]) == 0) <br/>{<br/>score += 1; <br/>}                         <br/>}                                                 <br/>float fraction_correct = float(score) / Y_test.size();                        </span></pre><p id="9c71" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated"><strong class="jq hj"> fraction_correct </strong>给出分类器的精确度。</p><p id="14ae" class="pw-post-body-paragraph jo jp hi jq b jr ln jt ju jv lo jx jy jz lp kb kc kd lq kf kg kh lr kj kk kl hb bi translated">在构建和运行项目之后，人们应该可以看到分类器在准确性方面的性能。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/51d8b899834ca5671959728f3bf98fc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*L5S5wsGT-2EanlShtjdxtw.png"/></div></figure></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><h1 id="3663" class="iq ir hi bd is it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn bi translated">摘要</h1><p id="0e56" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">因此，详细说明了简单高斯朴素贝叶斯分类器的设计和实现，并简要讨论了其在发现车道变换意图中的应用。</p></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><h1 id="95d3" class="iq ir hi bd is it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn bi translated">参考</h1><ol class=""><li id="b04d" class="my mz hi jq b jr js jv jw jz na kd nb kh nc kl nd ne nf ng bi translated">穆尔蒂·纳拉辛哈；德维诉苏谢拉。<em class="nh">模式识别:算法方法</em>。斯普林格科学&amp;商业媒体，2011年。</li><li id="f523" class="my mz hi jq b jr ni jv nj jz nk kd nl kh nm kl nd ne nf ng bi translated">朴素贝叶斯文本分类的事件模型比较。参加:<em class="nh"> AAAI-98文本分类学习研讨会</em>。1998.第41至48条。</li><li id="1234" class="my mz hi jq b jr ni jv nj jz nk kd nl kh nm kl nd ne nf ng bi translated">数据由一门课程提供<a class="ae ky" href="https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013" rel="noopener ugc nofollow" target="_blank">https://www . uda city . com/course/自驾-汽车-工程师-纳米学位- nd013 </a></li></ol></div></div>    
</body>
</html>