<html>
<head>
<title>How do I reduce Memory footprint of my Machine Learning Model!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我如何减少我的机器学习模型的内存占用！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-do-i-reduce-memory-footprint-of-my-machine-learning-model-8af8f4258974?source=collection_archive---------3-----------------------#2021-02-12">https://medium.com/analytics-vidhya/how-do-i-reduce-memory-footprint-of-my-machine-learning-model-8af8f4258974?source=collection_archive---------3-----------------------#2021-02-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a5e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嵌入式平台上的DNN推理</p><p id="ca5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度神经网络(DNNs)在许多计算机视觉任务中是成功的。明显的废话！然而，最精确的DNNs需要数百万个参数和操作，这使得它们是能量、计算和存储器密集型的。[戈尔]</p><p id="133f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">VGG-16需要150亿次运算来对单幅图像进行图像分类。</p><p id="cf7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">YOLOv3执行390亿次运算来处理一幅图像。</p><p id="7738" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要在小型嵌入式计算机上部署这样的dnn，需要进行更多的优化。因此，追求深度学习的低功耗改进以实现高效推理是值得的，也是一个不断增长的研究领域[Alyamkin]。</p><p id="e1be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与image net竞赛类似，现在有许多人正在努力寻找能够同时实现计算机视觉高精度和高能效的最佳视觉解决方案。[LPIRC-WEB]</p><h1 id="8b13" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">挑战</h1><p id="04c5" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">假设你想在边缘设备上进行准确快速的图像识别。这需要几个步骤。</p><ol class=""><li id="94ec" class="kg kh hi ih b ii ij im in iq ki iu kj iy kk jc kl km kn ko bi translated">首先，需要建立和训练神经网络模型来识别和分类图像。</li><li id="9773" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">然后，模型应该尽可能准确和快速地运行。</li><li id="9205" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">大多数神经网络都是在浮点模型上训练的，通常需要转换为定点才能在边缘设备上高效运行</li><li id="de3b" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">控制功耗</li></ol><p id="0693" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">令人着迷的是，已经做了这么多让我们处于这样一个位置的工作，我们将有人工智能模型在你的微波炉里运行等等。在资源受限环境中部署ML模型的最新解决方案可分为以下几大类[GOEL]</p><ol class=""><li id="9aca" class="kg kh hi ih b ii ij im in iq ki iu kj iy kk jc kl km kn ko bi translated"><strong class="ih hj">参数量化和修剪</strong>:通过减少用于存储DNN模型参数的位数，降低内存和计算成本。</li><li id="a411" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated"><strong class="ih hj">压缩卷积滤波器和矩阵分解</strong>:将较大的DNN层分解成较小的层，以减少内存需求和冗余矩阵运算的数量。</li><li id="8a66" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated"><strong class="ih hj">网络架构搜索</strong>:自动构建不同层组合的DNN，寻找达到期望性能的DNN架构。</li><li id="522e" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated"><strong class="ih hj">知识蒸馏:</strong>训练一个紧凑的DNN，模仿一个计算量更大的DNN的输出、特征和激活。</li></ol><p id="34ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们更深入地了解其中的每一项</p><h1 id="37cc" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">参数量化和修剪</strong></h1><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/7f301ec9739f1f1ef31baf960a9f3d53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EdHT8iT4PvCStrtZfT09Vw.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">XNOR —拉斯特加里</figcaption></figure><p id="fc61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们举一个例子，50个卷积层的ResNet-50模型在处理一个图像时需要超过95MB的内存来存储和超过38亿次浮点数乘法。但在丢弃一些冗余权值后，网络仍然照常工作，但节省了75%以上的参数和50%的计算时间。使用的技术范围从对参数值应用k均值标量量化到权重共享，然后对量化的权重和码本应用霍夫曼编码[YUCHENG]</p><p id="3096" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将此发挥到极致的是每个权重的1位表示，即二进制权重神经网络。主要思想是在模型训练期间直接学习<strong class="ih hj">二进制权重或激活。</strong></p><p id="e95e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那是带b的二进制。</p><p id="71dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有几个作品直接用二进制权重训练CNN，例如，BinaryConnect [Courbariaux]，BinaryNet和XNOR [Rastegari]。</p><p id="8006" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">二进制权重，即被限制为仅两个可能值(例如-1或1)的权重，通过用简单的累加代替许多乘法-累加操作，将会给专用DL硬件带来很大的好处，因为乘法器是神经网络的数字实现中最需要空间和功率的组件。与其他丢弃方案一样，作者表明BinaryConnect充当正则化子，并且他们在置换不变MNIST、CIFAR-10和SVHN上使用BinaryConnect获得了接近最新水平的结果。</p><h1 id="74a2" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">压缩卷积滤波器</strong></h1><p id="c0fd" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">这些方法基于关键的观察，即学习卷积滤波器的权重通常是平滑和低频的，我们首先用离散余弦变换(DCT)将滤波器权重转换到频域，并使用低成本哈希函数将频率参数随机分组到哈希桶中。被分配相同散列桶的所有参数共享用标准反向传播学习的单个值。</p><h1 id="dfc2" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">网络架构搜索</strong></h1><p id="3693" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">在尝试Keras或Torch等深度学习框架时，首先要做的是指定网络架构。你会同意这很大程度上是武断的，至少感觉是如此。我们把它也自动化怎么样？我们肯定会失业，但这难道不令人兴奋吗？</p><p id="095c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">神经架构搜索(NAS)，自动化架构工程的过程，因此是自动化机器学习的合乎逻辑的下一步。到目前为止，NAS方法在一些任务上已经优于手动设计的架构，例如图像分类(Zoph等人，2018；Real等，2019)、物体检测(Zoph等，2018)或语义分割(陈等，2018)。NAS可以被视为AutoML的子领域(Hutter等人，2019年)，与超参数优化(福雷尔和Hutter，2019年)和元学习(Vanschoren，2019年)有显著重叠。[埃尔斯肯]</p><p id="d63a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">NAS的早期版本在架构搜索阶段从头开始训练每个候选的神经架构，导致计算量激增。ENAS建议使用参数共享策略来加速架构搜索过程。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lk"><img src="../Images/032cca852243d9e37430aa3815ffaa6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xJEXUEI2ubfusImmfA-3hg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">任调查</figcaption></figure><p id="081d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">人们已经尝试了许多不同的方法，从强化学习到进化算法，来构建这些NAS解决方案。[彭真任]</p><p id="089e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你肯定听说过Auto-Pytorch？还没有！去看看这个吧，显然是以后了。</p><h1 id="db1f" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">知识蒸馏</strong></h1><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lm"><img src="../Images/867d35897a49a7ec63d52ab9f32dc0ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qWFshB7HkzMy_cozGMbzRw.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated"><a class="ae ll" href="https://arxiv.org/pdf/2006.05525.pdf" rel="noopener ugc nofollow" target="_blank">郭建平等人</a></figcaption></figure><p id="b424" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在知识提炼中，一个小的学生模型一般由一个大的老师模型来监督。其主要思想是学生模型模仿教师模型，以获得有竞争力的甚至更好的表现。</p><p id="f6c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们谈论知识提炼时，我们想到的一个问题是，当学习参数的空间有限时，我们如何压缩模型，很难看到我们如何改变模型的形式，但保持相同的知识。但是为了解决这个问题，上帝自己——Geoffery hint on非常简洁地说，需要对知识本身有一个更抽象的看法，一个把它从任何特定的实例化中解放出来的看法，也就是说，它应该被看作是从输入向量到输出向量的习得性映射。</p><p id="b21d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">软目标作为正则化子</strong></p><p id="1f63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将教师模型的概括能力转移到学生模型的一个明显的方法是使用由教师模型产生的类概率作为训练小模型的“软目标”。对于这个转移阶段，我们可以使用相同的训练集或单独的“转移”集。当教师模型是较简单模型的大集合时，我们可以使用它们各自预测分布的算术或几何平均值作为软目标。当软目标具有高熵时，它们在每个训练案例中提供比硬目标多得多的信息，并且在训练案例之间的梯度中提供少得多的变化，因此学生模型通常可以在比原始笨重模型少得多的数据上训练，并且使用高得多的学习率。[辛顿]</p><p id="0afd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们关于使用软目标而不是硬目标的一个主要主张是，软目标可以携带许多有用的信息，而这些信息不可能用单个硬目标进行编码。</p><h1 id="c6b0" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">结论</h1><p id="c366" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">哦，天哪，这原来是一篇相当长的文章。但有趣的是，各种不同的方法是如何被追求并导致一种非常理想的状态，在这种状态下，我们最终不仅节省了我们有限的资源，而且反过来能够在可以进一步加速人工智能系统的增长和传播的情况下部署我们的模型。</p><p id="e288" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我知道你对此的想法<a class="ae ll" href="https://www.linkedin.com/in/abhijeet-pokhriyal-18a7649a/" rel="noopener ugc nofollow" target="_blank">这里</a></p><h1 id="22fd" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">来源</h1><p id="c74b" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">https://arxiv.org/pdf/2003.11066.pdf(<a class="ae ll" href="https://arxiv.org/pdf/2003.11066.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="a4bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[Alyamkin](Alyamkin，s .，Ardi，m .，Brighton，a .，Berg，A. C .，Chen，b .，Chen，y .，… Zhuo，S. (2019)。低功耗计算机视觉:现状、挑战、机遇。IEEE电路与系统新兴和精选专题期刊，1–1。doi:10.1109/jetcas . 1911899)</p><p id="6744" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[LP IRC-WEB](【https://rebootingcomputing.ieee.org/lpirc/2019】T4)</p><p id="4722" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【禹城】(<a class="ae ll" href="https://arxiv.org/pdf/1710.09282.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1710.09282.pdf</a>)</p><p id="f88b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[Courbariaux](<a class="ae ll" href="https://arxiv.org/pdf/1511.00363.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1511.00363.pdf</a></p><p id="9a7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[拉斯特加里](<a class="ae ll" href="https://arxiv.org/pdf/1603.05279.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1603.05279.pdf</a>)</p><p id="33a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">https://arxiv.org/pdf/1808.05377.pdf</p><p id="c6ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【https://arxiv.org/pdf/2006.02903.pdf任】(<a class="ae ll" href="https://arxiv.org/pdf/2006.02903.pdf" rel="noopener ugc nofollow" target="_blank"/>)</p><p id="3519" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【辛顿】(<a class="ae ll" href="https://arxiv.org/pdf/1503.02531.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1503.02531.pdf</a>)</p></div></div>    
</body>
</html>