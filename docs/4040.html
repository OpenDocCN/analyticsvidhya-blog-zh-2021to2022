<html>
<head>
<title>Implementing Transformer Paper (Google T5 Transformer from Scratch and using it to create a Chatbot)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实现Transformer Paper(从头开始使用Google T5 Transformer并使用它创建聊天机器人)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/googles-t5-transformer-theory-ffd0acc738d2?source=collection_archive---------2-----------------------#2021-08-20">https://medium.com/analytics-vidhya/googles-t5-transformer-theory-ffd0acc738d2?source=collection_archive---------2-----------------------#2021-08-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="c212" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文全面论述了利用Tensorflow使用Google<a class="ae jc" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">T5</a>变压器。</p><p id="2de7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要在运行中直接使用它，请参考我的文章- <br/> ' <a class="ae jc" rel="noopener" href="/@ar9avg/create-chatbot-using-chatformer-t5-based-chat-bot-1b3445f87d72"> <strong class="ig hi">使用T5</strong>'</a>-w<em class="jd">这是一个关于如何使用我的库创建上下文聊天机器人的</em> <strong class="ig hi"> <em class="jd"> 3步教程</em> </strong> <em class="jd">(不需要深入学习，因为我已经为您完成了)并将其部署在Reddit/Telegram/Mobile应用程序上</em></p><h1 id="6b3f" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">T5简介</strong></h1><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/fe1bbc11ecec2700a88527c4f4684f25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KM24KDPqfRaVJcl4.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">摘自T5变压器论文</figcaption></figure><p id="0c62" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank"> T5变压器的纸链接</a></p><h2 id="18b9" class="ks jf hh bd jg kt ku kv jk kw kx ky jo ip kz la js it lb lc jw ix ld le ka lf bi translated">什么是变压器？</h2><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lg"><img src="../Images/dfc63c19dc214b6598e13e2ed3e98ddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/0*ziSWaQWf1LJd0rH2.png"/></div></figure><p id="51de" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Transformer是一种语言模型，它是位置感知的前馈神经网络。</p><p id="0e32" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">只要你明白注意力计算向量之间的相似性，注意力的确切内部工作方式就无关紧要了。</p><p id="2965" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">他们一下子“读”完了整个句子。每个单词都有自己的位置，句子中所有其他的单词都有自己的位置。</p><p id="954b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意，默认实现假设最大序列长度(与RNNs不同)。对于无限/非常长的序列，需要不同的架构(Transformer-XL)。</p><p id="e10f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总的来说，转换器实际上也知道单词的顺序，它们被编码在一个单独的位置向量中。然后，将位置向量与表示每个单词之间相似性的向量合并，然后将其传递给编码器中的前馈层。</p><p id="8144" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我不会详细介绍它的工作原理和细微差别，你肯定能很容易地理解它(http://www.peterbloem.nl/blog/transformers)这是我找到的最好的解释。</p><p id="2eea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">T5变压器是做什么的？</strong></p><p id="dcbf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最近几年，大量的预训练模型，如乌尔姆菲特、伯特、GPT等被开源到NLP社区。<br/>最新的SOTA之一是T5:文本到文本传输变压器模型，该模型于2019年12月开源。</p><p id="710d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">T5转换器可以适应多个文本类，因为它将所有的NLP任务重新组织成一个统一的文本到文本格式(T25 ),其中输入和输出总是文本串。</p><p id="542c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该模型在清理版本<a class="ae jc" href="https://commoncrawl.org/" rel="noopener ugc nofollow" target="_blank">通用抓取</a>数据集大小超过700GB的情况下进行预训练。</p><p id="4eb2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">和BERT一样，T5也是掩蔽语言模型。</p><p id="d1ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是BERT和T5的关键区别在于:<br/> — BERT将单个令牌替换为单个屏蔽令牌<br/> — T5将多个令牌替换为单个屏蔽令牌</p><p id="f16e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，作为输出，我们期望在T5的情况下，令牌上有一个序列。</p><p id="e79d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">T5变压器可以微调到任何任务，如聊天机器人，翻译，文本摘要，句子相似性等。</p></div><div class="ab cl lh li go lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ha hb hc hd he"><h1 id="6967" class="je jf hh bd jg jh lo jj jk jl lp jn jo jp lq jr js jt lr jv jw jx ls jz ka kb bi translated"><strong class="ak">变形金刚NMT聊天机器人</strong></h1><p id="0d34" class="pw-post-body-paragraph ie if hh ig b ih lt ij ik il lu in io ip lv ir is it lw iv iw ix lx iz ja jb ha bi translated"><strong class="ig hi">关注层</strong></p><p id="2f14" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意，我们基本上采用两个单词嵌入(x和y)，通过查询变换矩阵(Q)传递一个单词，通过密钥变换矩阵(K)传递第二个单词，并通过它们的点积来比较结果查询和密钥向量有多相似。</p><p id="a3b9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们开始创建一个多头注意力层(MHA)</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ly"><img src="../Images/f7f2a5cea4a8e91946838c7f24fcdb6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/0*pbDUjSQRZQAFoPhJ.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated"><a class="ae jc" href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/2017/file/3f 5ee 243547 dee 91 FBD 053 C1 C4 a 845 aa-paper . pdf</a></figcaption></figure><p id="d42b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用Tensorflow Keras创建多头注意力类</p><p id="52ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里的<em class="jd"> V，K </em>和<em class="jd"> Q </em>描述了值、键和查询。</p><p id="9c0d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jd"> Q </em>来自目标序列，<em class="jd"> K，V </em>来自源序列</p><p id="9500" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这三个参数在结构上是相似的，序列中的每个单词都由一个向量表示。</p><p id="49e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据图像，我们接受V，K和Q输入标记，并构建一个尺寸向量。</p><p id="6152" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将<em class="jd"> q，k，v </em>分割成与q中的令牌数量相等的大小。</p><p id="f57a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">split函数将最后一个维度拆分为(数量头，深度)。<br/>转置结果，使形状为(batch_size，num_heads，seq_len，depth)</p><p id="224c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">成比例的点产品注意事项</strong></p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="ab fe cl lz"><img src="../Images/31573030d1a65a61fbd5e375cc6c1f55.png" data-original-src="https://miro.medium.com/v2/format:webp/1*b31hiO4ynbDLRrXWEFF4aQ.png"/></div></figure><p id="7898" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里我们计算注意力权重</p><p id="d1c8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">q，k，v必须有匹配的前导尺寸。</p><p id="061e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">k，v必须具有匹配的倒数第二个维度，即:seq_len_k = seq_len_v。掩码根据其类型(填充或前瞻)具有不同的形状，但它必须是可扩展的，以便添加。</p><p id="3da8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们通过应用上面的公式得到这个。</p><p id="2125" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">公式中的dk是k或v的倒数第二个维度，即:seq_len_k。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ma"><img src="../Images/44eedea44db09fb0791972bab96add87.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/0*2JctDhTnqRL6kQlr.png"/></div></figure><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="4ed1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，在多头注意力的构建之后，我们调用<strong class="ig hi"><em class="jd">【scaled _ dot _ product _ Attention】</em></strong>来获得权重以获得scaled_attention，attention_weights使用q、k和v。</p><p id="4984" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们进一步转置和缩放它，以获得我们的连接注意力向量。</p><p id="94fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">并将其通过一个线性层，该层给出了序列的<strong class="ig hi">注意力输出向量</strong>表示。</p><p id="1bcb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">MHA的最终实施</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="mb mc l"/></div></figure><h2 id="7c2a" class="ks jf hh bd jg kt ku kv jk kw kx ky jo ip kz la js it lb lc jw ix ld le ka lf bi translated">定义编码层</h2><p id="342e" class="pw-post-body-paragraph ie if hh ig b ih lt ij ik il lu in io ip lv ir is it lw iv iw ix lx iz ja jb ha bi translated">现在我们为T5变压器建立一个编码器层。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es md"><img src="../Images/2847a97d43be3a62ff6abe801e8ba495.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2FkIdOyxcnFrWrK8ma3kxw.png"/></div></div></figure><p id="60e7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于编码器层，</p><p id="7f4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们接受文本的编码表示，并通过MHA传递，以获得我们上面设计的注意力输出。</p><p id="3b7e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们添加一些漏失，并通过一个前馈网络对注意力输出和插入向量的总和进行归一化，该网络是2个连续的线性层，并在其上重复漏失和归一化部分。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="c767" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">定位编码</strong></p><p id="67a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以这个事情一直让我很困惑。</p><p id="6a9a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">位置嵌入是更可取的添加到单词嵌入，而不是连接它们。我们已经知道单词嵌入的维度与语义有关。那么，为什么要将位置嵌入语义空间，而不是增加额外的维度来表示位置呢？</p><p id="0b5a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我们有一个单词列表。我们选择任意两个。</p><p id="4d11" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设选择了x和y。设x和y的位置分别是e和f。</p><p id="af93" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">问题:<br/> 1)给定单词y，我们应该对单词x给予多大的关注？</p><p id="942f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2)给定单词y的位置f，我们应该对单词x给予多大的关注？</p><p id="16f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3)给定单词x的位置e，我们应该对y给予多大的关注？</p><p id="6829" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4)给定单词y的位置f，我们应该对单词x的位置e给予多大的关注？</p><p id="ccdc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在论坛上读到这些后，我得到了这些。</p><p id="6596" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">具有位置编码的学习变换矩阵Q’k必须同时完成所有这四项任务。这可能是看起来效率低下的部分，因为直觉上，Q'K同时做好四项任务的能力应该有所取舍。</p><p id="d2a3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">串联将确保位置维度与单词维度正交，但我的猜测是，因为这些嵌入空间是如此高维，所以即使在添加时，您也可以免费获得近似正交性，而没有串联的成本(需要学习更多参数)。考虑到非线性，增加层只会对此有所帮助。</p><p id="8508" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们最终也希望e和f有一些好的表现，所以在向量表示中，对于位置的微小变化，有一些“接近”。sin和cos表示很好，因为附近的位置在它们的位置编码中具有很高的相似性，这使得学习“保持”这种期望的接近性的变换更容易。</p><p id="bc59" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> TLDR: </strong></p><p id="2ba8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">直观上可能的是，在高维空间中，单词向量在整个嵌入空间内形成较小维度的子空间，并且位置向量形成不同的较小维度的子空间，该子空间近似正交于单词向量所跨越的子空间。因此，尽管矢量相加，这两个子空间可以通过一些单一的学习转换基本上彼此独立地操纵。因此，串联并没有增加多少，但是极大地增加了学习参数的成本。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="3856" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">编码器</strong></p><p id="da2d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在使用位置嵌入和令牌嵌入，我们通过多个编码器层传递它。</p><p id="a12e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们最终得到了序列的编码表示。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="964e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">解码层</strong></p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es me"><img src="../Images/41af9408dc53d85fe1b02df16895f239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kb-keb6ymCFXHwjYs7XTcg.png"/></div></div></figure><p id="02bf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于解码器层，</p><p id="bdc1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与编码器类似，我们将目标序列输入到输出中，进行嵌入和位置编码，从而产生编码表示。</p><p id="464d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里的变化是:</p><p id="e578" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们需要一个<strong class="ig hi">前瞻掩码</strong>,因为当在解码器生成目标序列时，由于转换器使用自关注，它倾向于包括来自解码器输入的所有单词。</p><p id="4585" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是，实际上这是不正确的。只有当前单词之前的单词可以有助于下一个单词的生成。MHA确保了这一点。下图解释了前瞻掩码的工作原理。</p><p id="70d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在解码器中，我们有两个MHA层。</p><p id="cd37" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先采用具有前瞻掩码的上述向量，其次采用定位目标向量和编码输出向量的归一化形式。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="8682" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">解码器</strong></p><p id="94a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，使用目标的位置嵌入和令牌嵌入，我们将它与前瞻掩码一起通过多个解码器层。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="f3be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">变压器</strong></p><p id="68f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们定义我们的转换器，它基本上是编码器和解码器层的叠加。</p><p id="c984" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在顶部，我们添加了一个输出所需序列的密集层。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="f83f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">加载创建标记器:</p><p id="9d9a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于记号赋予器，我们使用load <a class="ae jc" href="https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/SubwordTextEncoder" rel="noopener ugc nofollow" target="_blank"> Tensorflow子字编码器</a></p><p id="d021" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">子词标记化器的主要优点是它在基于词和基于字符的标记化之间进行插值。普通单词在词汇表中有一个位置，但是对于未知单词，标记器可以退回到单词片段和单个字符。</p><p id="03f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们从给定的训练数据中构建输入和输出标记器。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="7eeb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">自定义调度器</strong></p><p id="b1c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们为我们的任务使用一个定制的学习率调度器。它考虑了预热步骤，然后是步骤数平方根倒数的最小值或arg2(预热步骤),如下所示。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="8f42" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">配置加载程序</p><p id="709e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于训练参数，我们制作如下yml文件并指定参数。我们在这里输入所有可配置的参数。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="7741" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">配置文件:</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="mb mc l"/></div></figure><blockquote class="mf mg mh"><p id="731f" class="ie if jd ig b ih ii ij ik il im in io mi iq ir is mj iu iv iw mk iy iz ja jb ha bi translated">我们建造了自己的变压器。</p></blockquote><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ml"><img src="../Images/6894a1fc94569f8bd4cda987bb4f2c2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/0*JgcvHrwHnJMvXL0O.gif"/></div></figure><p id="bf4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">培训司机</strong></p><p id="8b39" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们已经完成了模型构建。</p><p id="172e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们把所有的东西都拿走，把司机包起来。</p><p id="cb91" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们根据自己创建的CustomScheduler设置学习率。</p><p id="0bcc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Optimizer = &gt;根据我的经验，在Transformer风格的模型上，Adam确实看起来比SGD用momentum训练得更快，即使在很好地调整momentum之后。参见<a class="ae jc" href="https://arxiv.org/abs/1910.05446" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1910.05446</a>中图2的第四个面板。</p><p id="00c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用<em class="jd"> tf.keras.metrics.Mean </em>来测量<em class="jd"> train_loss。</em></p><p id="d0de" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用<em class="jd">TF . keras . metrics . sparsecategoricalaccuracy</em>来度量<em class="jd"> train_accuracy。</em></p><p id="fe9c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jd"> @tf.function </em>将train_step跟踪编译成tf图，以便更快地执行。该函数专用于自变量张量的精确形状。为了避免由于可变序列长度或可变批次大小(最后一个批次较小)而导致的重新跟踪，请使用input_signature指定更通用的形状。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="204b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jd">疑惑，建议，联系我。</em> </strong></p><p id="590e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://www.linkedin.com/in/ar9av/" rel="noopener ugc nofollow" target="_blank">我的LinkedIn </a></p></div></div>    
</body>
</html>