<html>
<head>
<title>Regression Regularization Techniques — Ridge and Lasso</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回归正则化技术—脊和套索</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/regression-regularization-techniques-ridge-and-lasso-c464f24a4ed1?source=collection_archive---------0-----------------------#2022-01-20">https://medium.com/analytics-vidhya/regression-regularization-techniques-ridge-and-lasso-c464f24a4ed1?source=collection_archive---------0-----------------------#2022-01-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/d9c090c776ac9dbd22cdf5c318e76573.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w5bdQGgrHibp5o0mMcNUqw.png"/></div></div></figure><p id="4478" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi jn translated">无论走到哪里，我们都会听到很多关于T2、回归、T4和T5的故事。所以让我们跳过这一点。在本文中，我们将假设我们是<em class="jw">回归</em>的大师，并且我们已经在给定的数据集上完成了<em class="jw">线性回归模型</em>的构建。</p><p id="63c6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，从模型中，您会注意到它有<strong class="ir hi"> <em class="jw">过度拟合</em> </strong> <em class="jw"> —过度拟合</em>是数据科学中使用的一个术语，用于描述统计模型完美地拟合其训练数据，而算法无法针对未知数据准确执行，从而否定了该方法的目标。现在这个<em class="jw">过度拟合</em>可能是因为模型太复杂了。必须降低这种复杂性，以改进模型并消除<em class="jw">过拟合</em>。</p><blockquote class="jx jy jz"><p id="9628" class="ip iq jw ir b is it iu iv iw ix iy iz ka jb jc jd kb jf jg jh kc jj jk jl jm ha bi translated">为此，我们可以降低某些<em class="hh">回归系数</em>的数值，和/或我们可以丢弃一些对最终预测没有显著价值的特征。我们将应用的过程叫做<strong class="ir hi"> <em class="hh">规则化</em> </strong>。</p></blockquote><p id="08fa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以我们来到了这篇文章的结尾。谢谢你。</p><p id="b545" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Lol，开个玩笑。好吧，反正大多数关于这个话题的文章都是这样的。但相信我，这次会不同的。让我带你经历完整的体验——从理解我们为什么和在哪里需要它，它的统计重要性，以及我们如何在Python上实现它开始。</p><h1 id="7ab1" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">理解偏差和方差</strong></h1><p id="6555" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">让我们再一次回到基础——偏差<em class="jw">和方差<em class="jw"/>。</em></p><p id="c4f0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当模型在训练数据集上表现不佳时，我们说模型的<em class="jw">偏差</em>高。当模型在测试数据集上表现不佳时，可以说<em class="jw">方差</em>很高。我举个例子让你更清楚。</p><h2 id="3789" class="lg ke hh bd kf lh li lj kj lk ll lm kn ja ln lo kr je lp lq kv ji lr ls kz lt bi translated"><strong class="ak">场景1: </strong></h2><figure class="lv lw lx ly fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lu"><img src="../Images/98cd83538d238d578ccf04d14f016625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l4O-8VR7dgWBUSKvmNfgAQ.png"/></div></div></figure><p id="d5bf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设你正在准备考试(是的，请假设。)而你在这一章的最后只看了5个问题的答案。有了这个，如果考试有和你准备的一样的5道题，你就能答得很好。但是你将无法回答考试中已经问过的其他问题。这是一个<strong class="ir hi"> <em class="jw">低偏差</em> </strong>和<strong class="ir hi"> <em class="jw">高偏差</em> </strong>的例子。</p><h2 id="517b" class="lg ke hh bd kf lh li lj kj lk ll lm kn ja ln lo kr je lp lq kv ji lr ls kz lt bi translated">场景2:</h2><figure class="lv lw lx ly fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lu"><img src="../Images/774726e1b07430dbf79e82b4c4901aa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gPNzPWy1bdWuXlg6J6tOdg.png"/></div></div></figure><p id="5c47" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但在另一种情况下，让我们说，你已经通过实际浏览整个章节并理解概念而不是记住5个问题的答案来准备考试。有了这个，你将能够回答考试中出现的任何问题，但你不能完全按照书中给出的答案来回答。这是一个<strong class="ir hi"> <em class="jw">高偏差</em></strong><strong class="ir hi"><em class="jw">低方差</em> </strong>的例子。</p><p id="406c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，回到D <em class="jw">数据科学</em>术语——重要的是要明白，在你构建的任何模型中，总是要在<em class="jw">偏差</em>和<em class="jw">方差</em>之间进行权衡。看看这个图表，它显示了不同<strong class="ir hi"><em class="jw"/></strong>模型复杂度的<em class="jw">方差</em>和<em class="jw">偏差</em>。具有<em class="jw">低复杂度的模型或简单模型</em>将具有<em class="jw">高偏差</em>和<em class="jw">低方差</em>，而<em class="jw">高度复杂的</em> <em class="jw">模型</em>通常将具有<em class="jw">低偏差</em>和<em class="jw">高方差。(试着把这个和我上面给出的考试例子联系起来。)还要注意，无论是非常简单的模型还是高度复杂的模型，模型的<em class="jw">总误差(偏差+方差)</em>都将是<em class="jw">最大</em>。</em></p><figure class="lv lw lx ly fd ii er es paragraph-image"><div class="er es lz"><img src="../Images/699d2654a150233f961db4d332152efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*kADA5Q4al9DRLoXck6_6Xw.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">偏差-方差权衡</figcaption></figure><p id="e8f9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，现在可以有把握地说，我们将需要建立一个具有<em class="jw">最低总误差</em>的模型——一个能够识别来自<em class="jw">训练数据</em>的所有模式，并且在它之前没有见过的数据(即<em class="jw">测试数据</em>上也表现良好的模型。</p><h1 id="9df1" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">正则化对模型复杂性有什么帮助？</h1><p id="3322" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated"><em class="jw">正则化</em>在我们需要考虑模型复杂性以平衡<em class="jw">偏差-方差时出现。</em>正则化有助于将模型系数的大小降低至0(当系数变为0时，也有助于从模型中完全移除特征)，从而降低模型的复杂性。这反过来将减少模型的过度拟合，并降低总误差，这正是我们想要实现的。</p><p id="fb51" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">考虑<em class="jw">普通最小二乘法</em> (OLS回归)——<em class="jw">残差平方和</em>或<em class="jw">成本函数</em>由下式给出:</p><figure class="lv lw lx ly fd ii er es paragraph-image"><div class="er es me"><img src="../Images/20dd52f7512821d11860092aae5f16a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*1p_Z5_cHLLUrSvoRZIvOoA.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">残差平方和RSS —回归成本函数</figcaption></figure><p id="902c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当我们建立回归模型时，我们以成本函数或RSS最小的方式构造特征系数。请注意，该RSS仅考虑模型产生的<em class="jw">偏差</em>，而不考虑<em class="jw">方差</em>。因此，模型可能会尝试减少<em class="jw">偏差</em>并过度拟合训练数据集——这将导致模型具有<em class="jw">高方差</em>。</p><p id="bc11" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，<em class="jw">正则化</em>通过稍微修改模型的<em class="jw">成本函数</em>来帮助我们降低模型复杂度，从而减少<em class="jw">过拟合</em>。</p><h2 id="b571" class="lg ke hh bd kf lh li lj kj lk ll lm kn ja ln lo kr je lp lq kv ji lr ls kz lt bi translated">那么我们实际上是怎么做的呢？</h2><p id="a5ca" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">在<em class="jw">正则化</em>中，我们将<em class="jw">惩罚</em>项添加到<em class="jw">成本函数</em>中，这将帮助我们控制模型的复杂性。</p><blockquote class="jx jy jz"><p id="6a73" class="ip iq jw ir b is it iu iv iw ix iy iz ka jb jc jd kb jf jg jh kc jj jk jl jm ha bi translated">在<em class="hh">正则化之后，代价函数</em>变成了<strong class="ir hi"> <em class="hh"> RSS +惩罚</em> </strong> <em class="hh">，即我们在代价函数中给正则RSS增加了一个惩罚项。</em></p></blockquote><h1 id="107a" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">岭回归</strong></h1><blockquote class="jx jy jz"><p id="42e0" class="ip iq jw ir b is it iu iv iw ix iy iz ka jb jc jd kb jf jg jh kc jj jk jl jm ha bi translated">在<em class="hh">岭回归中，</em>我们添加了一个惩罚项，它是<strong class="ir hi"><em class="hh">lambda(</em></strong>λ)<strong class="ir hi"><em class="hh">乘以权重(模型系数)的平方和。</em> </strong></p></blockquote><figure class="lv lw lx ly fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mf"><img src="../Images/8fda07df4b348369daf14fee785e5361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JAPnU3B4Fqqc3y-jz4R5ig.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">岭回归方程</figcaption></figure><p id="8cff" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">注意，惩罚项(称为<strong class="ir hi"> <em class="jw">收缩惩罚</em> </strong>)具有权重的平方和。所以在<em class="jw">回归</em>中，我们通过尝试降低<em class="jw">成本函数来建立模型。</em>因此，这里的模型将试图降低模型系数(权重)以降低成本函数，从而降低模型复杂度。</p><p id="d8fb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在让我们明白<strong class="ir hi"><em class="jw">λ</em></strong>(<strong class="ir hi">λ)</strong>在这里做什么。λ是<strong class="ir hi">调谐参数。</strong>将0赋给λ将使整个惩罚项为0，模型系数不会减少(无收缩),导致<em class="jw">过拟合</em>。但是当λ达到非常高的值时，惩罚项也会增加，从而降低模型系数，并可能导致<em class="jw">欠拟合</em>。</p><p id="8fc7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这就是为什么我们必须选择正确的λ值，以确保模型复杂性得到有效降低，但不会出现<em class="jw">过拟合</em>或<em class="jw">欠拟合</em>。在岭回归中选择合适的λ值可以通过<strong class="ir hi"> <em class="jw">超参数调谐来完成。</em>T59】</strong></p><p id="16b1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请注意，在执行<em class="jw">岭回归</em>之前，数据集必须经过<em class="jw">标准化</em>。这是因为我们正在处理模型的系数，如果它们在同一尺度上，这将是有意义的。</p><h1 id="b3c1" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">套索回归</h1><p id="8269" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">岭回归和套索回归的重要区别在于惩罚项。</p><blockquote class="jx jy jz"><p id="955d" class="ip iq jw ir b is it iu iv iw ix iy iz ka jb jc jd kb jf jg jh kc jj jk jl jm ha bi translated">在<em class="hh"> Lasso回归中，</em>我们添加了一个惩罚项，它是<strong class="ir hi"><em class="hh">λ乘以权重(模型系数)的绝对值之和。</em> </strong></p></blockquote><figure class="lv lw lx ly fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mf"><img src="../Images/4a16ede6daf1adafe34a6a016e3152ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G6wmWy-VabTLlCzMc7iGOQ.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">拉索回归方程</figcaption></figure><p id="69b2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">其余的原则和惩罚项降低模型系数以降低模型复杂性的方式与<em class="jw">岭回归</em>相似。</p><blockquote class="jx jy jz"><p id="2bf7" class="ip iq jw ir b is it iu iv iw ix iy iz ka jb jc jd kb jf jg jh kc jj jk jl jm ha bi translated">但是这里必须注意的一个区别是，在<em class="hh">套索回归</em>的情况下，<em class="hh">收缩项(惩罚项)</em>迫使一些模型系数<em class="hh">恰好变为0 </em>，从而<em class="hh">从模型中移除整个特征</em>(假定λ值足够大)。这给出了套索回归的一个全新应用— <strong class="ir hi"> <em class="hh">特征选择</em> </strong>。<em class="hh">在岭回归的情况下，这是不可能的。</em></p></blockquote><p id="b82b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请注意，与<em class="jw">岭回归</em>类似，在执行<em class="jw">套索回归</em>之前，数据集必须<em class="jw">标准化</em>。这是因为我们正在处理模型的系数，如果它们在同一尺度上，这将是有意义的。</p><h1 id="b445" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">正则化技术的Python实现——山脊和套索</strong></h1><p id="949a" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">岭和套索回归的整个实施，以及从探索性数据分析开始的数据集的详细分析，多元线性回归，包括多重共线性、VIF分析等。可以在这里找到。</p><p id="d103" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">本实施中使用的数据集是“<strong class="ir hi"> <em class="jw">惊喜住房案例研究</em> </strong>”的数据集。</p><p id="1279" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你可以在这里找到到Python实现的<strong class="ir hi"> <em class="jw"> Github </em> </strong>链接:</p><div class="mg mh ez fb mi mj"><a href="https://github.com/Adhithia/RegressionWithRegularization" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">GitHub-Adhithia/regressionwithregonization:多元线性回归与…</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">多重线性回归与正则化技术一起执行，包括山脊和套索。惊喜住房…</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">github.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx in mj"/></div></div></a></div><p id="832f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您可以在Kaggle上编辑和协作同一个项目:</p><div class="mg mh ez fb mi mj"><a href="https://www.kaggle.com/adhithia/regression-with-ridge-and-lasso-regularization" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">使用岭和套索正则化的回归</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">使用Kaggle笔记本探索和运行机器学习代码|使用来自无附加数据源的数据</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">www.kaggle.com</p></div></div><div class="ms l"><div class="my l mu mv mw ms mx in mj"/></div></div></a></div><h2 id="bc1b" class="lg ke hh bd kf lh li lj kj lk ll lm kn ja ln lo kr je lp lq kv ji lr ls kz lt bi translated">片段—岭回归实现</h2><figure class="lv lw lx ly fd ii"><div class="bz dy l di"><div class="mz na l"/></div></figure><h2 id="342c" class="lg ke hh bd kf lh li lj kj lk ll lm kn ja ln lo kr je lp lq kv ji lr ls kz lt bi translated">片段—套索回归实现</h2><figure class="lv lw lx ly fd ii"><div class="bz dy l di"><div class="mz na l"/></div></figure></div><div class="ab cl nb nc go nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ha hb hc hd he"><p id="fab2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这就是<em class="jw">回归正则化技术——脊和套索</em>的结束。你可以在这里查看我的其他作品:</p><div class="mg mh ez fb mi mj"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/spotting-anomalies-in-chest-x-ray-scans-using-deep-learning-4b8195a7b7bb"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">使用深度学习发现胸部X射线扫描中的异常</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">使用卷积神经网络将扫描分类为“渗出”或“正常”。</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">medium.com</p></div></div><div class="ms l"><div class="ni l mu mv mw ms mx in mj"/></div></div></a></div><div class="mg mh ez fb mi mj"><a href="https://adhithia.medium.com/skin-cancer-detection-using-convolutional-neural-networks-38f386cdc6d7" rel="noopener follow" target="_blank"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">使用卷积神经网络的皮肤癌检测</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">黑色素瘤是一种癌症，如果不及早发现，可能会致命。它占皮肤癌死亡的75%。一个…</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">adhithia.medium.com</p></div></div><div class="ms l"><div class="nj l mu mv mw ms mx in mj"/></div></div></a></div></div></div>    
</body>
</html>