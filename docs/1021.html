<html>
<head>
<title>Deploy your machine learning models to production</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将您的机器学习模型部署到生产中</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-c8f820e094e5?source=collection_archive---------8-----------------------#2021-02-10">https://medium.com/analytics-vidhya/introduction-c8f820e094e5?source=collection_archive---------8-----------------------#2021-02-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><figure class="im in io ip fd iq er es paragraph-image"><div role="button" tabindex="0" class="ir is di it bf iu"><div class="er es il"><img src="../Images/6a3265b0b4298677843525200c3b2e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nqaz98QjbFA44baGdyn5LA.jpeg"/></div></div><figcaption class="ix iy et er es iz ja bd b be z dx translated">布拉登·科拉姆在<a class="ae jb" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><h1 id="fb6c" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">介绍</h1><p id="2765" class="pw-post-body-paragraph ka kb hh kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ha bi translated">机器学习每天都受到越来越多的关注。因此，对ML模型的生产就绪部署的需求急剧增加。<br/>在本文中，我们将讨论如何将您的机器学习模型部署到生产中。<br/>我们将从简单的方法开始，这不是在生产中要做的事情，<br/>增加解决方案的复杂性，最后，在用所有这些噩梦折磨你之后，我们将以部署和服务你的模型的正确方式结束。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><h1 id="08ce" class="jc jd hh bd je jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv lc jx jy jz bi translated">先决条件</h1><ul class=""><li id="b26d" class="ld le hh kc b kd ke kh ki kl lf kp lg kt lh kx li lj lk ll bi translated">我们使用linux</li><li id="e117" class="ld le hh kc b kd lm kh ln kl lo kp lp kt lq kx li lj lk ll bi translated">Python 3</li><li id="1a46" class="ld le hh kc b kd lm kh ln kl lo kp lp kt lq kx li lj lk ll bi translated"><a class="ae jb" href="https://flask.palletsprojects.com/en/1.1.x/" rel="noopener ugc nofollow" target="_blank">烧瓶</a></li><li id="67c3" class="ld le hh kc b kd lm kh ln kl lo kp lp kt lq kx li lj lk ll bi translated"><a class="ae jb" href="https://uwsgi-docs.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> uWSGI </a></li><li id="ba39" class="ld le hh kc b kd lm kh ln kl lo kp lp kt lq kx li lj lk ll bi translated">码头工人</li><li id="80c4" class="ld le hh kc b kd lm kh ln kl lo kp lp kt lq kx li lj lk ll bi translated"><a class="ae jb" href="https://ray.io/" rel="noopener ugc nofollow" target="_blank">雷</a></li></ul></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="7f53" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">网上大部分可用资源建议你在Flask API后面部署ML模型并为其服务。大概是这样的:</p><p id="8b5b" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">(<a class="ae jb" href="https://mlinproduction.com/" rel="noopener ugc nofollow" target="_blank">感谢Luigi Patruno的精彩系列文章</a>)</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="ba96" class="mb jd hh lx b fi mc md l me mf">from flask import Flask, request</span><span id="1fef" class="mb jd hh lx b fi mg md l me mf">app = Flask(__name__)<br/>model = None<br/></span><span id="98c8" class="mb jd hh lx b fi mg md l me mf">def some_fancy_loading_logic():<br/>    # Load your model here<br/>    return None<br/></span><span id="b5a6" class="mb jd hh lx b fi mg md l me mf">@app.before_first_request<br/>def load_model():<br/>    global model<br/>    model = some_fancy_loading_logic()<br/></span><span id="1bef" class="mb jd hh lx b fi mg md l me mf">@app.route('/predict', methods=['POST'])<br/>def predict():<br/>    global model<br/>    data = request.get_json()<br/>    prediction = model.predict(data)<br/>    inp_out = {'input': data, 'prediction': prediction}<br/>    return inp_out</span><span id="3955" class="mb jd hh lx b fi mg md l me mf">if __name__ == '__main__':<br/>    app.run(host='0.0.0.0', port=80, debug=True)</span></pre><p id="f559" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">并在docker中打包:</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="1dd1" class="mb jd hh lx b fi mc md l me mf">FROM python:3.6</span><span id="3b65" class="mb jd hh lx b fi mg md l me mf">COPY ./requirements.txt /tmp<br/>RUN pip3 install --upgrade pip \<br/>  &amp;&amp; pip3 install -r /tmp/requirements.txt</span><span id="0491" class="mb jd hh lx b fi mg md l me mf">COPY ./app /app<br/>CMD ["python", "/app/main.py"]</span></pre><p id="004f" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">太不可思议了。不是吗？简答:没有！</p><p id="bfac" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">长回答:如果你开始这个并查看日志，它会对你大喊:</p><blockquote class="mh mi mj"><p id="2b8f" class="ka kb mk kc b kd lr kf kg kh ls kj kk ml lt kn ko mm lu kr ks mn lv kv kw kx ha bi translated">"警告:这是一个开发服务器。不要在生产部署中使用它。</p></blockquote><p id="99df" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">这里有一篇短文描述为什么:<a class="ae jb" href="https://build.vsupalov.com/flask-web-server-in-production/" rel="noopener ugc nofollow" target="_blank"> Flask不是您的生产服务器</a>和<a class="ae jb" href="https://flask.palletsprojects.com/en/1.1.x/tutorial/deploy/#run-with-a-production-server" rel="noopener ugc nofollow" target="_blank">官方文档</a>明确声明不使用Flask内部服务器，而是使用生产就绪的WSGI服务器。所以我们从这个开始。</p><p id="af2f" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">为了完成这个任务，请牢记黄金法则(如果有人已经做到了——使用它，不要发明它！)我们将使用<a class="ae jb" href="https://github.com/tiangolo/uwsgi-nginx-flask-docker" rel="noopener ugc nofollow" target="_blank">这个</a>作为父docker图像(感谢<a class="ae jb" href="https://github.com/tiangolo" rel="noopener ugc nofollow" target="_blank">塞巴斯蒂安·拉米雷斯</a>)。这个图像满足了我们所有的需求。这里是我们将用来创建图像的Dockerfile文件:</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="2a1f" class="mb jd hh lx b fi mc md l me mf">FROM tiangolo/uwsgi-nginx-flask:python3.6</span><span id="64ca" class="mb jd hh lx b fi mg md l me mf">COPY ./requirements.txt /tmp<br/>RUN pip3 install --upgrade pip \<br/>  &amp;&amp; pip3 install -r /tmp/requirements.txt</span><span id="9ac3" class="mb jd hh lx b fi mg md l me mf">COPY ./app /app</span></pre><p id="9399" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">dockerfile - /app目录的肉。这是所有奇迹发生的地方。这是将在WSGI中运行的应用程序，为我们的模型提供服务。要理解在用nginx和wsgi部署flask应用程序时发生的奇妙事情，请看<a class="ae jb" href="https://flask.palletsprojects.com/en/1.0.x/deploying/uwsgi/" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><p id="cbec" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">首先，让我们用以下内容创建一个简单的main.py:</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="c6e6" class="mb jd hh lx b fi mc md l me mf">from flask import Flask</span><span id="ab4d" class="mb jd hh lx b fi mg md l me mf">app = Flask(__name__)<br/></span><span id="f263" class="mb jd hh lx b fi mg md l me mf">@app.route('/predict', methods=['POST'])<br/>def predict():<br/>    return "Still want to continue?"</span></pre><p id="bc84" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">添加uwsgi ini文件:</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="7144" class="mb jd hh lx b fi mc md l me mf">[uwsgi]<br/>module = main<br/>callable = app</span></pre><p id="c9c5" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">您可以在<a class="ae jb" href="https://github.com/hbeybutyan/articles/tree/main/model_deployment" rel="noopener ugc nofollow" target="_blank"> github </a>中查看项目结构</p><p id="8157" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">构建…</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="67aa" class="mb jd hh lx b fi mc md l me mf">docker build -t simple_app .</span></pre><p id="a04a" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">运行…</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="78a2" class="mb jd hh lx b fi mc md l me mf">docker run -p 8080:8080 -it simple_app:latest</span></pre><p id="051d" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">请求…</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="666e" class="mb jd hh lx b fi mc md l me mf">curl -X POST -d {\"text\":\"tadaaam\"} 127.0.0.1:80/predict -H "Content-Type: application/json"</span></pre><p id="01ef" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">这个太简单了，不好评论什么。现在让我们创建一个类来代表我们训练好的模型。创建实例后，我们将加载模型，并根据用户请求进行预测。如果翻译成Python:</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="5b36" class="mb jd hh lx b fi mc md l me mf">from time import sleep<br/></span><span id="e475" class="mb jd hh lx b fi mg md l me mf">class FancyModel:<br/>    def __init__(self):<br/>        # Here we load pretrained model. It lasts somewhat long.<br/>        sleep(0.05)</span><span id="f904" class="mb jd hh lx b fi mg md l me mf">    def predict(self, text):<br/>        sleep(0.005)<br/>        return "Can't tell anything smart about: {}".format(text)</span></pre><p id="c533" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">这篇文章不是关于任何特定的模型或ML问题。这就是为什么我们用这个简单的类模拟了一个漫长的不眠之夜的结果，你做了大量的工作来获得一个最终表现有点合理的模型。在现实生活中，您将使用您最喜欢的库来加载预训练模型。可能还会进行一些输入验证。这里我们保持简单。我们假设加载模型需要50毫升，进行预测需要5毫升。</p><p id="e54a" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">把所有东西粘在一起，我们就有了这个:</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="0300" class="mb jd hh lx b fi mc md l me mf">from flask import Flask, request<br/>from FancyModel import FancyModel</span><span id="8418" class="mb jd hh lx b fi mg md l me mf">app = Flask(__name__)<br/></span><span id="6435" class="mb jd hh lx b fi mg md l me mf">@app.route('/predict', methods=['POST'])<br/>def predict():<br/>    fnc = FancyModel()<br/>    data = request.get_json()<br/>    return fnc.predict(data["text"])</span></pre><p id="31d7" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">再次-构建…运行…请求…</p><p id="8ebd" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">但是等等，这太难看了。对于每个传入的请求，我们加载模型。这就像你打电话给电视供应商的技术支持，他们会让你在线一个小时。我打赌他们会错过你在明年合同上的签字。所以我们再进一步，做另一件丑陋的事。</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="d950" class="mb jd hh lx b fi mc md l me mf">from flask import Flask, request<br/>from FancyModel import FancyModel</span><span id="6511" class="mb jd hh lx b fi mg md l me mf">app = Flask(__name__)<br/>fnc = FancyModel()<br/></span><span id="e5da" class="mb jd hh lx b fi mg md l me mf">@app.route('/predict', methods=['POST'])<br/>def predict():<br/>    data = request.get_json()<br/>    return fnc.predict(data["text"])<br/></span></pre><p id="dad4" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">再次-构建…运行…请求…</p><p id="8726" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">耶哈哈。现在不需要为每个请求加载模型。这样不好吗？肯定没有！问题是你可能使用的一些框架可能不是线程安全的。比如尝试谷歌“tensorflow叉安全”。这可能不仅仅是张量流和分叉的情况。所以让我们更进一步。我们需要每个模型生活在它自己的过程中。这里我们有多种选择。例如，继续使用纯python。创建一个长生命进程池，关注它们的生命周期等。我们不会这样做。相反，我们将使用一个uWSGI的居民，如<a class="ae jb" href="https://uwsgi-docs.readthedocs.io/en/latest/Mules.html" rel="noopener ugc nofollow" target="_blank">骡子</a>。下面是现在服务请求的方式:</p><figure class="im in io ip fd iq er es paragraph-image"><div role="button" tabindex="0" class="ir is di it bf iu"><div class="er es mo"><img src="../Images/45934abb9012fd179791a2943a89beba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vi7Jwdu2qi0vgoVcvIV5Cg.jpeg"/></div></div></figure><p id="076b" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">一旦http服务器收到请求并将其转发给我们的应用程序，我们将:</p><ol class=""><li id="5781" class="ld le hh kc b kd lr kh ls kl mp kp mq kt mr kx ms lj lk ll bi translated">为请求生成一个uuid，并将请求放入队列。</li><li id="701f" class="ld le hh kc b kd lm kh ln kl lo kp lp kt lq kx ms lj lk ll bi translated">通知骡子。如果所有的任务都完成了，那么骡子就在等待一个确认，即有一个任务要服务。</li><li id="7cdf" class="ld le hh kc b kd lm kh ln kl lo kp lp kt lq kx ms lj lk ll bi translated">等待结果出现在缓存中。</li></ol><p id="2c89" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">依次，骡子:</p><ol class=""><li id="d4fb" class="ld le hh kc b kd lr kh ls kl mp kp mq kt mr kx ms lj lk ll bi translated">收听确认消息。</li><li id="2e3b" class="ld le hh kc b kd lm kh ln kl lo kp lp kt lq kx ms lj lk ll bi translated">一旦收到请求，就轮询任务队列。</li><li id="2afd" class="ld le hh kc b kd lm kh ln kl lo kp lp kt lq kx ms lj lk ll bi translated">使用请求uuid作为关键字进行预测并将结果添加到缓存中。</li></ol><p id="2b13" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">让我们在项目中添加一头骡子。</p><p id="d005" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">首先，我们现在需要对uwsgi进行一些配置更改。这是新的配置。</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="396f" class="mb jd hh lx b fi mc md l me mf">[uwsgi]<br/>module = main<br/>callable = app<br/>mule=hard_working_mule.py<br/>mule=hard_working_mule.py<br/>mule=hard_working_mule.py<br/>mule=hard_working_mule.py</span><span id="1b22" class="mb jd hh lx b fi mg md l me mf">master = true<br/>queue = 100<br/>queue-blocksize = 2097152<br/>cache2 = name=mcache,items=10,blocksize=2097152</span></pre><p id="eeef" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">让我们一行一行来。</p><p id="842a" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">uWSGI服务器需要知道在哪里可以找到应用程序的callable，前两行就是关于这个的。接下来，我们指定每个mule将运行什么，正如您所看到的，我们希望初始化4个mule。每头骡子运行的代码很快就会出现。接下来，uwsgi <a class="ae jb" href="https://uwsgi-docs.readthedocs.io/en/latest/Queue.html" rel="noopener ugc nofollow" target="_blank">队列</a>和<a class="ae jb" href="https://uwsgi-docs.readthedocs.io/en/latest/Caching.html" rel="noopener ugc nofollow" target="_blank">缓存</a>被初始化。好吧，我知道你注意到了。我们漏了一行</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="0e10" class="mb jd hh lx b fi mc md l me mf">master = true</span></pre><p id="b7bb" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">这样我们就启动了一个名为“缓存清理器”的线程。它的目的是从缓存中删除过期的密钥。如果我们错过了什么，就有点警惕了。</p><p id="26df" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">正如所承诺的，这是每头骡子将要做的:</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="ab95" class="mb jd hh lx b fi mc md l me mf">from FancyModel import FancyModel<br/>import uwsgi<br/>import json</span><span id="2a36" class="mb jd hh lx b fi mg md l me mf">if __name__ == '__main__':<br/>    fnc = FancyModel()<br/>    while True:<br/>        uwsgi.mule_get_msg()<br/>        req = uwsgi.queue_pull()<br/>        if req is None:<br/>            continue<br/>        json_in = json.loads(req.decode("utf-8"))<br/>        text = json_in["text"]<br/>        # to store transliterations<br/>        json_out = {"res": fnc.predict(text)}<br/>        uwsgi.cache_update(json_in.get("id"), json.dumps(json_out, ensure_ascii=False), 0, "mcache")</span></pre><p id="46d1" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">主要逻辑是:</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="7ee0" class="mb jd hh lx b fi mc md l me mf">from flask import Flask, request, Response<br/>import uuid<br/>import json<br/>import uwsgi</span><span id="ddb5" class="mb jd hh lx b fi mg md l me mf">CACHE_NAME = "mcache"</span><span id="3907" class="mb jd hh lx b fi mg md l me mf">def process_request(json_in):<br/>    uid = str(uuid.uuid4())<br/>    json_in["id"] = uid<br/>    uwsgi.queue_push(json.dumps(json_in))<br/>    # Actual content of message does not really matter<br/>    # This is just to triger mule execution<br/>    uwsgi.mule_msg("s")<br/>    while not uwsgi.cache_exists(uid, CACHE_NAME):<br/>        continue<br/>    res = uwsgi.cache_get(uid, CACHE_NAME)<br/>    uwsgi.cache_del(uid, CACHE_NAME)<br/>    return Response(response=res,<br/>                            status=200,<br/>                            mimetype="application/json")<br/></span><span id="7eae" class="mb jd hh lx b fi mg md l me mf">app = Flask(__name__)<br/></span><span id="4b69" class="mb jd hh lx b fi mg md l me mf">@app.route('/predict', methods=['POST'])<br/>def predict():<br/>    data = request.get_json()<br/>    return process_request(data)</span></pre><p id="714c" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">看起来不错，不是吗？再说一次-不。</p><p id="acba" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">让我们看看是怎么回事。首先:骡子代码中丑陋的“虽然是真的”:但是让我们假设我们是优秀的、迂腐的程序员，这不是问题。按照规定，我们有4头骡子来满足我们的需求。但是等等。谁说4骡子是个好选择？我们是怎么得出这个数字的。如果我们有一个非常大的请求率，那么4只骡子不能及时满足所有的请求怎么办？在这种情况下，队列将填满，我们将开始丢失请求。这意味着我们也需要仔细选择队列的大小。但是怎么做呢？这个决定背后的基本原理是什么？除了实践，没有任何理由可以改变和摧毁一切。</p><p id="4fcb" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">“但是等等”——你可以说——“我们可以扩展应用程序来满足更多的请求。”</p><p id="4c57" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">好的。将所有这些乱七八糟的东西打包到docker中，并通过复制进行扩展。在这种情况下，我们的扩展粒度是4骡子，加上运行主代码的进程，等等。这是典型的资源浪费。如果我们有许多必须服务的模型(也许我们提供45种语言的翻译，并为每一对训练了单独的模型)，那该怎么办？策略必须是什么？把所有东西都装在一只骡子里，可能会跑出队列？每个型号再养4头骡子，尽可能浪费资源？</p><p id="c1ad" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">另一件要注意的事情是:如果mule接受了请求并在服务过程中死去了怎么办。我们等待结果出现在缓存中。更糟糕的是，每个特定的解决方案都会带来新的问题。</p><p id="0ffe" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">那怎么办呢？</p><p id="f48c" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">认识一下<a class="ae jb" href="https://docs.ray.io/en/master/index.html" rel="noopener ugc nofollow" target="_blank">雷</a>。让我们不要谈论它有多好，开始使用它。我不打算描述部署Ray的所有步骤。在文档中有很好的描述。相反，我们将研究一个简单的用例，涵盖我们之前在ngnix和uwsgi方面的内容。顺便说一句，雷有令人难以置信的响应社区。如果有的话，试着在Slack频道上提问。</p><p id="d9e8" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">我们假设您已经配置了kubernates和kubectl来与之交互。如果没有，可以尝试一下<a class="ae jb" href="https://minikube.sigs.k8s.io/docs/start/" rel="noopener ugc nofollow" target="_blank"> minikube </a>。它很好地记录了你如何<a class="ae jb" href="https://docs.ray.io/en/master/cluster/kubernetes.html" rel="noopener ugc nofollow" target="_blank">在你的kubernetes </a>上部署Ray cluster。长话短说:</p><ol class=""><li id="159c" class="ld le hh kc b kd lr kh ls kl mp kp mq kt mr kx ms lj lk ll bi translated">从<a class="ae jb" href="https://github.com/ray-project/ray/tree/master/doc/kubernetes" rel="noopener ugc nofollow" target="_blank"> repo </a>下载配置文件:</li><li id="4884" class="ld le hh kc b kd lm kh ln kl lo kp lp kt lq kx ms lj lk ll bi translated">使用以下命令在kubernates中创建名称空间</li></ol><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="528c" class="mb jd hh lx b fi mc md l me mf">kubectl create -f ray/doc/kubernetes/ray-namespace.yaml</span></pre><p id="0722" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">3.使用以下工具部署射线群集:</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="3a7a" class="mb jd hh lx b fi mc md l me mf">kubectl apply -f ray/doc/kubernetes/ray-cluster.yaml</span></pre><p id="ecd3" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">显然，您需要更深入地了解这个yaml文件，以了解发生了什么。但是这些都是不言自明的，即使没有经验的kubernates用户也可以通过谷歌了解那里发生了什么。</p><p id="0aad" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">我们完了。雷已经在你的kubernates上运行了。如果需要，请检查它:</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="347b" class="mb jd hh lx b fi mc md l me mf">kubectl get pods -n ray</span></pre><p id="2bed" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">您可以在那里看到3个工作节点。想换吗？在上面的yaml中更改worker节点副本的数量，重新应用配置，并根据需要找出尽可能多的worker。更进一步，杀死一个工人——它会再次上升。配置自动缩放——甚至有一个用于自动缩放的kubernates操作符。此外，kubernates还有更多的细节功能。</p><p id="20f5" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">现在，假设已经启动了ray cluster，部署我们的模型将类似于:</p><pre class="im in io ip fd lw lx ly lz aw ma bi"><span id="0793" class="mb jd hh lx b fi mc md l me mf">import ray<br/>from ray import serve<br/>from FancyModel import FancyModel<br/># connect to Ray cluster<br/>ray.init(address="auto")<br/>client = serve.start(detached=True, http_options={"host": "0.0.0.0"})<br/>client.create_backend("lr:v1", FancyModel)<br/>client.create_endpoint("fancy_predictor", backend="lr:v1", route="/predict")</span></pre><p id="1162" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">耶哈哈。它在那里。尝试请求它。你知道吗？它附带了糖果:缩放、增量部署、在后端之间分割流量(比如训练模型的不同版本)、会话亲和性、监控等等。</p><p id="9a66" class="pw-post-body-paragraph ka kb hh kc b kd lr kf kg kh ls kj kk kl lt kn ko kp lu kr ks kt lv kv kw kx ha bi translated">上面说一点是怎么回事吧。首先，我们初始化射线运行时。这假设已经有一个长寿命的射线束可以到达，并且我们可以连接到它。随后我们开始用<a class="ae jb" href="https://docs.ray.io/en/master/serve/index.html" rel="noopener ugc nofollow" target="_blank">射线在上面发球</a>。<a class="ae jb" href="https://docs.ray.io/en/master/serve/index.html" rel="noopener ugc nofollow" target="_blank">服务</a>是我们甜甜圈的精华。它是在<a class="ae jb" href="https://docs.ray.io/en/master/actors.html#actor-guide" rel="noopener ugc nofollow" target="_blank"> Ray actors </a>之上创建的，它是框架不可知的，它是python优先的，它是Rayish。我们在这里使用的一些关键概念是:后端——这是应用程序的业务逻辑。在我们的例子中，这是FancyModel及其所有的结构和接口。端点——这是允许我们通过HTTP与后端交互的东西。端点可以有一个或多个后端(例如，在一个端点下可以提供从亚美尼亚语到英语的多种翻译模式)。如果你注意到在启动Ray Serve时我们指定了“detached=True”。这是因为Ray Serve的生命周期是和从serve.start()返回的客户端耦合在一起的。一旦超出范围，服务实例将被销毁。但是正如我们所记得的那样，我们需要长期有效的模型，所以这些模型并不是每个请求都加载的。分离式服务解决了这个问题。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><h1 id="4c46" class="jc jd hh bd je jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv lc jx jy jz bi translated">摘要</h1><p id="a927" class="pw-post-body-paragraph ka kb hh kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ha bi translated">在本文中，我们研究了将机器学习模型部署到生产中的问题。我们从在玩具、非生产级烧瓶服务中部署ML模型的简单选项开始。然后将它发展成一个更健壮的、生产就绪的、仍然非常混乱的解决方案。最后，我们使用了Ray，正如它所宣传的那样——用于构建分布式应用程序的简单、通用的API。有了它提供的所有工具，现在将您的模型部署到生产环境就容易多了。配合其他ML R&amp;D、Ops、工具(如<a class="ae jb" href="https://www.mlflow.org/docs/latest/tracking.html" rel="noopener ugc nofollow" target="_blank"> MLFlow </a>、<a class="ae jb" href="https://github.com/aimhubio/aim" rel="noopener ugc nofollow" target="_blank"> aimhubio </a>等)..)现在，管理您的ML产品的生命周期变得更加容易，从培训、评估您的模型到将其部署到生产环境。</p></div></div>    
</body>
</html>