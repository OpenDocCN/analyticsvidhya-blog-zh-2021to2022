<html>
<head>
<title>Using NLP to get inside Warren Buffet mind part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用自然语言处理进入沃伦巴菲特的思想第一部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/using-nlp-to-get-inside-warren-buffet-mind-part-i-666d717d0c2e?source=collection_archive---------10-----------------------#2021-04-27">https://medium.com/analytics-vidhya/using-nlp-to-get-inside-warren-buffet-mind-part-i-666d717d0c2e?source=collection_archive---------10-----------------------#2021-04-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/93ffbea37cdb9d82724f615b4c998343.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QF225R8ZbA_bfP4nDo9ocw.png"/></div></div></figure><div class=""/><p id="b68e" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">沃伦·巴菲特是一位美国投资者、慈善家，在《T2》福布斯富豪榜上排名第六。从1965年到2020年，他的总收益为<strong class="ir ht">2810526%</strong>，而<a class="ae jn" href="https://www.investopedia.com/terms/s/sp500.asp" rel="noopener ugc nofollow" target="_blank"> S &amp; P500 </a>(美国500大上市公司指数)上涨了23454%。他被认为是有史以来最好的投资者，是包括我在内的许多人的灵感来源。</p></div><div class="ab cl jo jp go jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="ha hb hc hd he"><h1 id="c8f0" class="jv jw hs bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">介绍</h1><p id="8afe" class="pw-post-body-paragraph ip iq hs ir b is kt iu iv iw ku iy iz ja kv jc jd je kw jg jh ji kx jk jl jm ha bi translated">为了结合我对<strong class="ir ht">技术</strong>和<strong class="ir ht">金融市场</strong>的热情，我将写一系列帖子，通过回答以下3个问题，看看人工智能(AI)的使用是否能帮助我理解巴菲特的想法。</p><ol class=""><li id="53b7" class="ky kz hs ir b is it iw ix ja la je lb ji lc jm ld le lf lg bi translated">机器学习模型可以回答关于金融和经济的问题吗？</li></ol><p id="2eea" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">2.自助餐最常用的术语是什么？这些术语是否会随着时间的推移而改变？</p><p id="97a3" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">3.这些年来，他对经济和股市的感觉如何？</p><p id="1d82" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了回答这些问题，我将使用自然语言处理(<a class="ae jn" href="https://machinelearningmastery.com/natural-language-processing/" rel="noopener ugc nofollow" target="_blank"> NLP </a>)技术和他每年写给伯克希尔·哈撒韦股东的<a class="ae jn" href="https://www.berkshirehathaway.com/letters/letters.html" rel="noopener ugc nofollow" target="_blank">信件</a>中的文字，他是这家公司的首席执行官。在这篇文章中，我将回答第一个问题。</p></div><div class="ab cl jo jp go jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="ha hb hc hd he"><h1 id="9c5b" class="jv jw hs bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">机器学习模型可以回答关于金融和经济的问题吗？</h1><p id="d8aa" class="pw-post-body-paragraph ip iq hs ir b is kt iu iv iw ku iy iz ja kv jc jd je kw jg jh ji kx jk jl jm ha bi translated">为了回答这个问题，我将使用变革人工智能领域的transformer架构。</p><p id="0878" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">论文“<a class="ae jn" href="https://arxiv.org/pdf/1706.03762v5.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ir ht">关注你所需要的一切</strong> </a>”从Google引入了变压器技术。考虑到单词在句子中的重要性，他们使用了一个<strong class="ir ht">编码器</strong>来为每个单词创建嵌入，并使用了一个<strong class="ir ht">解码器</strong>来将嵌入转换回文本。</p><h2 id="de17" class="lh jw hs bd jx li lj lk kb ll lm ln kf ja lo lp kj je lq lr kn ji ls lt kr lu bi translated">变压器架构</h2><p id="9b11" class="pw-post-body-paragraph ip iq hs ir b is kt iu iv iw ku iy iz ja kv jc jd je kw jg jh ji kx jk jl jm ha bi translated">编码器和解码器架构由具有多头注意力和前馈层的堆叠模块组成。</p><figure class="lw lx ly lz fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lv"><img src="../Images/0206a70290144ee5ee573f2a3fc63fe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hyQ5FSA0R6dtvVXu"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">变压器-模型架构。</figcaption></figure><p id="ddd4" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">与其他NLP方法不同，transformer不使用循环，而是使用堆叠的注意层。在每个注意力层中，模型都在观察句子的不同部分，并试图了解更多关于单词的信息。</p><figure class="lw lx ly lz fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es me"><img src="../Images/5c74760a85bc6f927ea7e8f5ad336ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IkiyTWp9QiZ3scy3"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">多头注意力由几个并行运行的注意力层组成。</figcaption></figure><p id="9525" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">仔细观察注意机制，您会发现它由两部分组成，一部分是使用softmax学习权重的点积注意，另一部分是并行调用点积操作数次的多头注意。</p><p id="0440" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后有一个前馈层，实现了给定句子中每个元素的线性变换。这些注意力层试图观察句子的不同部分，以发现单词的语义或句法信息。</p><p id="4259" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想了解更多关于变形金刚的信息，你可以查看Cathal Horan的这个<strong class="ir ht"> <em class="mf"> </em> </strong> <a class="ae jn" href="https://neptune.ai/blog/bert-and-the-transformer-architecture-reshaping-the-ai-landscape" rel="noopener ugc nofollow" target="_blank"> <strong class="ir ht"> <em class="mf">帖子。</em></strong></a></p><h2 id="bbe4" class="lh jw hs bd jx li lj lk kb ll lm ln kf ja lo lp kj je lq lr kn ji ls lt kr lu bi translated">问题回答</h2><p id="0f18" class="pw-post-body-paragraph ip iq hs ir b is kt iu iv iw ku iy iz ja kv jc jd je kw jg jh ji kx jk jl jm ha bi translated">QA(问答)是当模型只接收一个上下文和一个问题时，它输出一个答案(其中答案在上下文中)和一个置信度得分。QA是一项使用transformers架构的任务。</p><p id="920b" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用预先训练好的变换来完成QA任务很容易，就像我们在下面的例子中从<a class="ae jn" href="https://huggingface.co/transformers/task_summary.html#question-answering" rel="noopener ugc nofollow" target="_blank"> huggingface </a>看到的那样:</p><figure class="lw lx ly lz fd hj"><div class="bz dy l di"><div class="mg mh l"/></div></figure></div><div class="ab cl jo jp go jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="ha hb hc hd he"><p id="f937" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，我使用这个经过训练的模型和2008年的Buffet信件，只是删除了特殊字符，作为上下文来查看该模型是否可以做出正确的预测。</p><figure class="lw lx ly lz fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mi"><img src="../Images/2e430092b94fd7b985bc1746e071bd39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sc_QMITWF2_grK5oefzOZw.jpeg"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">问题、答案和显示正确答案的片段</figcaption></figure><p id="6fba" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如您所见，该模型能够正确理解所有问题，并对所有场景给出了满意的答案。对于第一个问题，答案应该是这样的:“信贷危机，伴随着房价和股价的暴跌”，但这个模型回答了“信贷危机”，部分是正确的。</p><p id="694b" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我提出的第二个问题是检查模型是否会给出正确的答案，即使我将该单词改为同义词，因此我将单词<strong class="ir ht">产生的</strong>改为单词<strong class="ir ht">产生的</strong>，答案是相同的。</p><p id="fbf9" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后在第三个问题中，我用“Who”而不是“What”来查看具有不同问题代词the的模型行为，模型给出了置信度为0.98的正确答案。最后一题，除了信心分低，回答也是对的。<br/>然后我去问那些回答不在正文里的问题，看看模型表现如何。</p><figure class="lw lx ly lz fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mj"><img src="../Images/81c48a02c18ea54680e4c41b46e3a08a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PwRG4OajNY1n5ivbG9w6bg.jpeg"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">模型给出的问题和答案</figcaption></figure><p id="85c3" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该模型对问题4给出了一个毫无意义的答案，但这是意料之中的，因为信中没有提到任何熊市。但是这个模型对问题5和6给出了有趣的答案。</p><p id="08b4" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该模型认为，当市场崩溃时，你应该投资于衍生品交易，当市场崩溃时，这确实是一个很好的投资，正如你在2020年市场崩溃时看到的那样，当Universa Tail基金在其投资组合中有衍生品时，<a class="ae jn" href="https://www.bloomberg.com/news/articles/2020-04-08/taleb-advised-universa-tail-risk-fund-returned-3-600-in-march" rel="noopener ugc nofollow" target="_blank"> <strong class="ir ht">在2020年3月</strong> </a>回报了3，600%。</p><p id="e6e1" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于问题6，虽然模型没有回答具体的股票购买，但输出是“定价过低的证券”,这是一个很好的回答，因为巴菲特以购买定价过低的证券而闻名，而定价过低的证券几乎总是一个好的投资。</p></div><div class="ab cl jo jp go jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="ha hb hc hd he"><h1 id="868f" class="jv jw hs bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">结论</h1><p id="39f9" class="pw-post-body-paragraph ip iq hs ir b is kt iu iv iw ku iy iz ja kv jc jd je kw jg jh ji kx jk jl jm ha bi translated">我们可以看到，预训练的transformer模型能够在新的上下文中获得正确的答案，甚至能够根据其答案不在该上下文中的问题给出有意义的答案。当然，这个模型还不是一个预言，但是我很期待看到NLP的发展，以及它将引领我们走向未来。</p><p id="2f8c" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你可以在我的<a class="ae jn" href="https://github.com/jairNeto/warren_buffet_letters" rel="noopener ugc nofollow" target="_blank"> Github库</a>查看写这篇文章的代码。如果你对我的Linkedin账户有任何意见，请随时联系我，非常感谢你阅读这篇文章。</p><p id="9b0b" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你喜欢你所读的，一定要👏下面，分享给你的朋友，关注我，不要错过这一系列的帖子。在下一篇文章中，我将尝试回答这些问题:</p><ul class=""><li id="4962" class="ky kz hs ir b is it iw ix ja la je lb ji lc jm mk le lf lg bi translated">自助餐最常用的术语是什么？这些术语是否会随着时间的推移而改变？</li><li id="fc21" class="ky kz hs ir b is ml iw mm ja mn je mo ji mp jm mk le lf lg bi translated">这些年来，他对经济和股市的感觉如何？</li></ul><p id="6c65" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir ht">敬请期待！</strong></p><h1 id="bc01" class="jv jw hs bd jx jy mq ka kb kc mr ke kf kg ms ki kj kk mt km kn ko mu kq kr ks bi translated">参考</h1><ul class=""><li id="15c6" class="ky kz hs ir b is kt iw ku ja mv je mw ji mx jm mk le lf lg bi translated"><a class="ae jn" href="https://www.berkshirehathaway.com/letters/letters.html" rel="noopener ugc nofollow" target="_blank"><strong class="ir ht">https://www.berkshirehathaway.com/letters/letters.html</strong></a></li><li id="043f" class="ky kz hs ir b is ml iw mm ja mn je mo ji mp jm mk le lf lg bi translated"><a class="ae jn" href="https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671" rel="noopener" target="_blank">https://towards data science . com/10分钟内情绪分析-与伯特拥抱脸294e8a04b671 </a></li><li id="39af" class="ky kz hs ir b is ml iw mm ja mn je mo ji mp jm mk le lf lg bi translated"><a class="ae jn" href="https://www.tensorflow.org/tutorials/text/classify_text_with_bert" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/tutorials/text/classify _ text _ with _ Bert</a></li><li id="f490" class="ky kz hs ir b is ml iw mm ja mn je mo ji mp jm mk le lf lg bi translated"><a class="ae jn" href="https://huggingface.co/transformers/model_doc/bert.html" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/transformers/model_doc/bert.html</a></li><li id="7cf6" class="ky kz hs ir b is ml iw mm ja mn je mo ji mp jm mk le lf lg bi translated"><a class="ae jn" href="https://neptune.ai/blog/bert-and-the-transformer-architecture-reshaping-the-ai-landscape" rel="noopener ugc nofollow" target="_blank">https://Neptune . ai/blog/Bert-and-the-transformer-architecture-shaping-the-ai-landscape</a></li></ul></div></div>    
</body>
</html>