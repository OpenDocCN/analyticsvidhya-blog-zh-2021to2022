<html>
<head>
<title>Getting Started with PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark入门</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/getting-started-with-pyspark-251a0af0f2c4?source=collection_archive---------3-----------------------#2021-10-11">https://medium.com/analytics-vidhya/getting-started-with-pyspark-251a0af0f2c4?source=collection_archive---------3-----------------------#2021-10-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><h2 id="f66e" class="hf hg hh bd b fp hi hj hk hl hm hn dx ho translated" aria-label="kicker paragraph">Spark分布式计算</h2><div class=""/><div class=""><h2 id="2e40" class="pw-subtitle-paragraph in hq hh bd b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je dx translated">使用Python连接到Spark集群</h2></div><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jf"><img src="../Images/41da82813b0aae026af3cee55a9aa0e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LCA73KkeJyTqjUl_"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">沃伦·王在<a class="ae jv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="0d06" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi ks translated"><span class="l kt ku kv bm kw kx ky kz la di">在过去的十年里，人们对快速可靠的工具产生了前所未有的需求，以处理海量数据。解决这个问题的方法之一是<a class="ae jv" href="https://en.wikipedia.org/wiki/MapReduce#:~:text=MapReduce%20is%20a%20programming%20model,distributed%20algorithm%20on%20a%20cluster.&amp;text=MapReduce%20libraries%20have%20been%20written,with%20different%20levels%20of%20optimization." rel="noopener ugc nofollow" target="_blank">MapReduce</a>——然而，这不仅花费大量时间，而且无法处理实时数据。火花来救援了！Spark由加州大学伯克利分校实验室推出，并于2010年开源。它提供了一个既快速又通用的解决方案。Spark的一个主要优势是它在内存中运行计算。另一个是<a class="ae jv" href="https://data-flair.training/blogs/apache-spark-lazy-evaluation/" rel="noopener ugc nofollow" target="_blank">惰性计算</a>机制，简单地说就是除非被明确要求，否则不采取任何行动。这有助于更快的数据处理，从而大大减少时间。</span></p><p id="08b2" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">Spark的神奇之处在于它的rdd——弹性分布式数据集。如果你熟悉熊猫数据框，这可能会更容易。RDDs的一个抽象层次是Spark DataFrame，它使用起来更简单，并且可以自动优化。</p><h2 id="f5c9" class="lb lc hh bd ld le lf lg lh li lj lk ll kf lm ln lo kj lp lq lr kn ls lt lu hn bi translated">首先，什么是火花？</h2><p id="fd02" class="pw-post-body-paragraph jw jx hh jy b jz lv ir kb kc lw iu ke kf lx kh ki kj ly kl km kn lz kp kq kr ha bi translated">Spark是一个集群计算框架，它将一个任务划分到一个称为节点的计算机集群中，以实现快速高效的处理。这种数据分割使得处理大型数据集变得更加容易。每个节点处理并行分配的计算，这些计算稍后被组合以返回最终结果。</p><p id="1cb4" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated"><strong class="jy hr">那PySpark是什么？</strong></p><p id="32de" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">很高兴你问了——这是Python <strong class="jy hr"> </strong>中Apache Spark的一个<strong class="jy hr"> </strong>接口，允许你使用Python APIs编写Spark应用程序。简单来说，PySpark是一种使用Python连接Spark集群的方式。</p><p id="cb8e" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated"><strong class="jy hr">有熊猫数据框为什么还要Spark数据框？</strong></p><p id="1288" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">问得好——如果你必须思考你是否需要火花，或许你根本不需要火花。老实说，除非你正在处理非常大的数据集，熊猫工作得很好。这两者之间的主要区别是pandas数据帧不是分布式的，而是在单个节点上运行。Spark DataFrame接口的优点之一是可以在Spark集群中的表上运行SQL查询。</p><h2 id="79f5" class="lb lc hh bd ld le lf lg lh li lj lk ll kf lm ln lo kj lp lq lr kn ls lt lu hn bi translated">熊猫数据框列与Spark数据框列</h2><p id="6ae1" class="pw-post-body-paragraph jw jx hh jy b jz lv ir kb kc lw iu ke kf lx kh ki kj ly kl km kn lz kp kq kr ha bi translated">值得注意的是，更新Spark数据帧与在<code class="du ma mb mc md b"><em class="me">pandas</em></code>中工作有些不同，因为Spark数据帧<em class="me">是不可变的</em>。这意味着它不能被改变，因此列不能被就地更新。</p></div><div class="ab cl mf mg go mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ha hb hc hd he"><h2 id="0097" class="lb lc hh bd ld le lf lg lh li lj lk ll kf lm ln lo kj lp lq lr kn ls lt lu hn bi translated">装置</h2><p id="0741" class="pw-post-body-paragraph jw jx hh jy b jz lv ir kb kc lw iu ke kf lx kh ki kj ly kl km kn lz kp kq kr ha bi translated">你可以在这里安装Spark<a class="ae jv" href="https://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank"/>。值得一提的是，spark是用Scala编写的，使用py4j与JVM接口交互。因此，要运行PySpark，您还需要将<a class="ae jv" href="https://www.java.com/en/download/" rel="noopener ugc nofollow" target="_blank"> Java </a>与Python和Apache Spark一起安装。</p><p id="2271" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">为了让您熟悉spark生态系统，我建议在配置spark在集群上运行之前，在本地运行spark，因为它可以独立运行。</p><p id="cbce" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">既然我们已经看到了如何获得<em class="me"> Spark </em>，我们现在可以连接到一个集群并完成一些基础工作。事不宜迟，让我们继续—</p><ul class=""><li id="34a7" class="mm mn hh jy b jz ka kc kd kf mo kj mp kn mq kr mr ms mt mu bi translated"><strong class="jy hr">使用PySpark </strong>连接到Spark集群——首先，我们需要一个到集群的连接。这是通过创建一个<code class="du ma mb mc md b">SparkContext</code>类的实例来完成的。验证您运行的是哪个版本的Spark。试试这个:<code class="du ma mb mc md b">print(SparkContext.version)</code></li><li id="e021" class="mm mn hh jy b jz mv kc mw kf mx kj my kn mz kr mr ms mt mu bi translated"><strong class="jy hr">使用Spark数据帧</strong>——一旦我们连接到集群，我们就可以使用它从您的<code class="du ma mb mc md b">SparkContext</code>创建<code class="du ma mb mc md b">SparkSession</code>对象。您可以将<code class="du ma mb mc md b">SparkContext</code>视为您与集群的连接，将<code class="du ma mb mc md b">SparkSession</code>视为您与该连接的接口。为了避免多个连接和会话，最好使用<strong class="jy hr">spark session . builder . getor create()</strong></li></ul><pre class="jg jh ji jj fd na md nb nc aw nd bi"><span id="a8e8" class="lb lc hh md b fi ne nf l ng nh">#Import SparkSession<br/>from pyspark.sql import SparkSession<br/>#Create Session<br/>spark = SparkSession.builder.getOrCreate()</span></pre><ul class=""><li id="742e" class="mm mn hh jy b jz ka kc kd kf mo kj mp kn mq kr mr ms mt mu bi translated">您可以使用<code class="du ma mb mc md b">spark.catalog.listTables()</code>查看集群中的所有表。要列出表格内容，可以使用show()。Ex- <code class="du ma mb mc md b">Table1.show()</code>将显示表1中的20行和所有列。</li><li id="934e" class="mm mn hh jy b jz mv kc mw kf mx kj my kn mz kr mr ms mt mu bi translated"><code class="du ma mb mc md b">.createDataFrame()</code>方法接受一个<code class="du ma mb mc md b">pandas</code>数据帧并返回一个Spark数据帧。请注意，创建的数据框将不会保留在会话目录中，因为它们存储在本地。这意味着您可以在它上面使用所有的Spark DataFrame方法，但是不能在其他上下文中访问数据。</li><li id="3878" class="mm mn hh jy b jz mv kc mw kf mx kj my kn mz kr mr ms mt mu bi translated"><code class="du ma mb mc md b">.select()</code>方法——用于从Spark数据帧中选择列。此外，当您使用<code class="du ma mb mc md b">df.colName</code>符号选择列时，您可以执行任何列操作，它将返回转换后的列。例如，见下文。</li></ul><pre class="jg jh ji jj fd na md nb nc aw nd bi"><span id="9b1c" class="lb lc hh md b fi ne nf l ng nh"># Weekly Salary when you have monthly salary</span><span id="8419" class="lb lc hh md b fi ni nf l ng nh">Week_Salary = (df.select(df.salary/4)).alias(“weekly salary”)</span></pre><ul class=""><li id="1663" class="mm mn hh jy b jz ka kc kd kf mo kj mp kn mq kr mr ms mt mu bi translated"><code class="du ma mb mc md b">.filter()</code>方法——正如您已经猜到的，这是SQL的<code class="du ma mb mc md b">WHERE</code>子句的Spark等价物。将SQL表达式的<code class="du ma mb mc md b">WHERE</code>子句之后的表达式作为字符串，或者作为布尔(<code class="du ma mb mc md b">True</code> / <code class="du ma mb mc md b">False</code>)值的火花列。</li></ul><pre class="jg jh ji jj fd na md nb nc aw nd bi"><span id="382d" class="lb lc hh md b fi ne nf l ng nh">#SQL<br/>SELECT * FROM table WHERE column &gt; 120</span><span id="107a" class="lb lc hh md b fi ni nf l ng nh">#PySpark<br/>table<!-- -->.filter("<!-- -->column<!-- --> &gt; value").show()</span></pre><ul class=""><li id="ce4f" class="mm mn hh jy b jz ka kc kd kf mo kj mp kn mq kr mr ms mt mu bi translated"><code class="du ma mb mc md b">.createTempView()</code> Spark DataFrame方法可以帮助你建立一个临时表，并接受你想要注册的临时表的名称作为参数。该方法将数据帧注册为目录中的一个表格，但由于该表格是临时的，只能从用于创建Spark数据帧的特定<code class="du ma mb mc md b">SparkSession</code>中访问。</li><li id="ffb6" class="mm mn hh jy b jz mv kc mw kf mx kj my kn mz kr mr ms mt mu bi translated"><code class="du ma mb mc md b">spark.read.csv(file_path)</code>可以用来读取PySpark中的文件。如果你的文件有标题，你可以传递一个参数告诉spark第一行是标题。<code class="du ma mb mc md b">spark.read.csv(file_path,header = True)</code></li><li id="8fda" class="mm mn hh jy b jz mv kc mw kf mx kj my kn mz kr mr ms mt mu bi translated"><code class="du ma mb mc md b">.withColumn()</code>采用两个参数的方法，可用于在数据集中创建新列。首先是一个包含新列名称的字符串，其次是新列本身。例如，将价格提高100。<code class="du ma mb mc md b">df.withColumn("price",col("price")+100).show()</code></li><li id="ec8c" class="mm mn hh jy b jz mv kc mw kf mx kj my kn mz kr mr ms mt mu bi translated">有时，使用像<code class="du ma mb mc md b">pandas</code>这样的工具在本地使用该表是有意义的。使用<code class="du ma mb mc md b">.toPandas()</code>火花数据帧使这变得简单</li><li id="f6ba" class="mm mn hh jy b jz mv kc mw kf mx kj my kn mz kr mr ms mt mu bi translated">另一个常见的数据库任务是聚合。也就是说，通过将数据分成几部分并对每一部分进行汇总来减少数据。也可以通过<code class="du ma mb mc md b">(.groupBy())</code>对多列进行分组。如果group by中有两列，它们的唯一组合将出现在输出中。</li></ul><p id="d24f" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">就这样了，伙计们！</p></div><div class="ab cl mf mg go mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ha hb hc hd he"><h1 id="5a80" class="nj lc hh bd ld nk nl nm lh nn no np ll iw nq ix lo iz nr ja lr jc ns jd lu nt bi translated">外卖食品</h1><p id="6111" class="pw-post-body-paragraph jw jx hh jy b jz lv ir kb kc lw iu ke kf lx kh ki kj ly kl km kn lz kp kq kr ha bi translated">在本文中，我们通过一些常用的基本查询来了解PySpark。本质上，PySpark是让Python与Spark集群对话的一种方式。如果您对SQL和Python有一点了解，您可以跳到PySpark ship上🚢很快。</p><p id="4a60" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">Spark已被广泛采用，并作为大规模分析处理引擎广泛用于数据科学和机器学习社区。该堆栈支持Spark流、Spark机器学习库(MLLib)和Spark SQL的实时和批量数据处理。在这里阅读更多关于他们的信息！</p><p id="00dd" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">根据你的喜好和问题，也许值得看一看<a class="ae jv" rel="noopener" href="/swlh/parallel-processing-in-python-using-dask-a9a01739902a"> Dask </a>。有任何问题，请喊我一声！</p><p id="32b9" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">我很高兴你能走到这一步！谢谢你的时间。不管你在忙什么，祝你过得愉快😄</p></div></div>    
</body>
</html>