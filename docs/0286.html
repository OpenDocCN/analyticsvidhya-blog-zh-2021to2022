<html>
<head>
<title>House Price Prediction using Linear Regression from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始用线性回归预测房价</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/house-price-prediction-using-linear-regression-from-scratch-b2b48fd73689?source=collection_archive---------5-----------------------#2021-01-11">https://medium.com/analytics-vidhya/house-price-prediction-using-linear-regression-from-scratch-b2b48fd73689?source=collection_archive---------5-----------------------#2021-01-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="08bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">今天，让我们从零开始，尝试用线性回归算法解决经典的房价预测问题。</em></p><p id="08ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">想了解更多关于<em class="jd">线性回归</em>的信息，别忘了看看我之前的博客——<a class="ae je" rel="noopener" href="/analytics-vidhya/everything-you-need-to-know-about-linear-regression-750a69a0ea50"><strong class="ih hj"><em class="jd">关于线性回归</em> </strong> </a>你需要知道的一切</p><p id="b304" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将需要<a class="ae je" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="jd">房价—高级回归技术</em></strong></a><strong class="ih hj"><em class="jd"/></strong>数据集来完成这项工作。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jf"><img src="../Images/a257d53796b8cac413bb94998f6a7aeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YMZOAO8QE4bZ4_Rk.jpg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">图片来源:Ritu Yadav (GitHub)</figcaption></figure><p id="6455" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">一定要看看下面GitHub要点中的数据描述:</em></p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="jv jw l"/></div></figure><p id="8604" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">通过Kaggle临时令牌导入Kaggle数据集(在Google Colab上):</em></p><blockquote class="jx jy jz"><p id="7a38" class="if ig jd ih b ii ij ik il im in io ip ka ir is it kb iv iw ix kc iz ja jb jc hb bi translated">关于Google Colab的更多信息— <a class="ae je" rel="noopener" href="/analytics-vidhya/a-beginners-guide-for-getting-started-with-machine-learning-7ba2cd5796ae"> <strong class="ih hj">机器学习入门指南</strong> </a></p></blockquote><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="a469" class="ki kj hi ke b fi kk kl l km kn"><strong class="ke hj">from</strong> <strong class="ke hj">google.colab</strong> <strong class="ke hj">import</strong> files <br/><em class="jd">"""upload your Kaggle temporary token downloaded from your Kaggle account onto your local device"""<br/></em>files.upload() </span><span id="ea77" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">Out:<br/></strong>Saving kaggle.json to kaggle.json<br/>{'kaggle.json': b'{"username":"xxx","key":"yyy"}'}</span></pre><p id="0b5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">下载您的数据集:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="a002" class="ki kj hi ke b fi kk kl l km kn">!mkdir -p ~/.kaggle<br/>!cp kaggle.json ~/.kaggle/<br/>!chmod 600 ~/.kaggle/kaggle.json</span><span id="3d91" class="ki kj hi ke b fi ko kl l km kn">!kaggle competitions download -c house-prices-advanced-regression-techniques</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es kp"><img src="../Images/d76289a9bcd36384805116cc34c79c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*m5Z3vRO_hAVeTBKa6FcbfA.png"/></div></figure><p id="ecae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">检查您的文件是否已下载:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="5ff0" class="ki kj hi ke b fi kk kl l km kn">!ls</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es kq"><img src="../Images/1eedfa85cc96b8e03f70809934bcf540.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*ZtHH6SyRV6QQIWPnDZyiyg.png"/></div></figure><p id="e000" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">导入库:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="9744" class="ki kj hi ke b fi kk kl l km kn"><strong class="ke hj">import</strong> <strong class="ke hj">pandas</strong> <strong class="ke hj">as</strong> <strong class="ke hj">pd</strong> <br/><strong class="ke hj">import</strong> <strong class="ke hj">numpy</strong> <strong class="ke hj">as</strong> <strong class="ke hj">np</strong> <br/><strong class="ke hj">from</strong> <strong class="ke hj">sklearn.preprocessing</strong> <strong class="ke hj">import</strong> MinMaxScaler, LabelEncoder</span></pre><p id="b8a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">读取数据集:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="102d" class="ki kj hi ke b fi kk kl l km kn">data = pd.read_csv("train.csv")<br/>data</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es kr"><img src="../Images/1c3efd7fc917c53259a6973f2b922c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nsdcTIhIWHrAVQjxPZ69lw.png"/></div></div></figure><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="2e16" class="ki kj hi ke b fi kk kl l km kn">data.describe()</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ks"><img src="../Images/3b5de4415ce87c90cbffcba7c1da61c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*In4aKrrJct3F9s8izked_g.png"/></div></div></figure><p id="d50b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">混洗数据:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="d6c6" class="ki kj hi ke b fi kk kl l km kn">data = data.sample(frac=1).reset_index(drop=<strong class="ke hj">True</strong>)<br/>data</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es kt"><img src="../Images/a12faddfd65ee1e874f6f404b5d77251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3HM3PvLL3vIZcMJy9UvqFQ.png"/></div></div></figure><p id="ec4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">检查NaN值:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="f3c6" class="ki kj hi ke b fi kk kl l km kn">data.isna().sum()</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ku"><img src="../Images/83d3cc6c54fa8bd98bc668bfd29e8068.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*XLp5tvC2dS0-yakW-HV18Q.png"/></div></figure><p id="bf8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">逐个检查NaN值计数:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="aefb" class="ki kj hi ke b fi kk kl l km kn">print(data['Alley'].isna().sum())<br/>print(data['FireplaceQu'].isna().sum())<br/>print(data['PoolQC'].isna().sum())<br/>print(data['Fence'].isna().sum())<br/>print(data['MiscFeature'].isna().sum())</span><span id="c52d" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">Out:<br/></strong>1369 <br/>690 <br/>1453 <br/>1179 <br/>1406</span><span id="be88" class="ki kj hi ke b fi ko kl l km kn">data.shape</span><span id="9d4a" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">Out:</strong><br/>(1460, 81)</span></pre><p id="5827" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">您也可以选择删除具有多余NaN值的列或估算它们:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="ea57" class="ki kj hi ke b fi kk kl l km kn"><em class="jd">#data.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1, inplace=True)<br/></em><br/><em class="jd">#data.drop(['LotFrontage'], axis=1, inplace=True)</em></span></pre><p id="77e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">删除Id，因为它没有任何意义:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="9702" class="ki kj hi ke b fi kk kl l km kn">data.drop(['Id'], axis=1, inplace=<strong class="ke hj">True</strong>)<br/>data.shape</span><span id="47a1" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">Out:</strong><br/>(1460, 80)</span></pre><p id="b85d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">输入缺失值:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="12ed" class="ki kj hi ke b fi kk kl l km kn"><em class="jd"># Imputing Missing Values</em><br/><strong class="ke hj">from</strong> <strong class="ke hj">sklearn.base</strong> <strong class="ke hj">import</strong> TransformerMixin<br/><strong class="ke hj">class</strong> <strong class="ke hj">DataFrameImputer</strong>(TransformerMixin):<br/><br/>    <strong class="ke hj">def</strong> __init__(self):<br/>        <em class="jd">"""Columns of dtype object are imputed with the most frequent value in column. Columns of other types are imputed with mean of column."""</em><br/>    <strong class="ke hj">def</strong> fit(self, X, y=<strong class="ke hj">None</strong>):<br/>        self.fill = pd.Series([X[c].value_counts().index[0]<br/>            <strong class="ke hj">if</strong> X[c].dtype == np.dtype('O') <strong class="ke hj">else</strong> X[c].mean() <strong class="ke hj">for</strong> c <strong class="ke hj">in</strong> <br/>            X],index=X.columns)<br/>        <strong class="ke hj">return</strong> self<br/>    <strong class="ke hj">def</strong> transform(self, X, y=<strong class="ke hj">None</strong>):<br/>        <strong class="ke hj">return</strong> X.fillna(self.fill)</span><span id="3dfb" class="ki kj hi ke b fi ko kl l km kn">X = pd.DataFrame(data) <br/>data = DataFrameImputer().fit_transform(X)</span><span id="f0d7" class="ki kj hi ke b fi ko kl l km kn">data.isna().sum()</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es kv"><img src="../Images/662c02c062b4813bd9766368152ac9fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*QZLwxnRv7xZkdEetLsEO1w.png"/></div></figure><p id="f2a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">编码分类变量的标签:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="feb9" class="ki kj hi ke b fi kk kl l km kn">LE = LabelEncoder()<br/>CateList = data.select_dtypes(include="object").columns<br/>print(CateList)</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es kw"><img src="../Images/47215fc28b3fa03ac6b27cca2e8f2349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-MAG3FV8TPP02bwDQjTsg.png"/></div></div></figure><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="f64e" class="ki kj hi ke b fi kk kl l km kn">data.head()</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es kx"><img src="../Images/552e69093b6c9cd03a78fef57e12da02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oxuX9jiB3uxxBE1c3w8Grw.png"/></div></div></figure><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="64bd" class="ki kj hi ke b fi kk kl l km kn"><strong class="ke hj">for</strong> i <strong class="ke hj">in</strong> CateList:<br/>    data[i] = LE.fit_transform(data[i])<br/>data.head()</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ky"><img src="../Images/181065dd62fb53efb64686d3153e2748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gMxHXUkXdj3ZpDBADrp1mw.png"/></div></div></figure><p id="5c7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">使用最小最大缩放器或标准缩放器缩放数值:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="f67c" class="ki kj hi ke b fi kk kl l km kn"><em class="jd">#from sklearn.preprocessing import StandardScaler</em><br/>df = data.iloc[:,:-1]<br/>mm = MinMaxScaler()<br/>df[:]= mm.fit_transform(df[:])</span><span id="8528" class="ki kj hi ke b fi ko kl l km kn">df.head()</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es kz"><img src="../Images/a17ba954d9133c5b8a4b5c514becaa7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*euubsOJuKJmIq4QHgi-9kA.png"/></div></div></figure><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="d235" class="ki kj hi ke b fi kk kl l km kn">X = df.values<br/>y = data['SalePrice'].values</span><span id="85f8" class="ki kj hi ke b fi ko kl l km kn">X_shape = X.shape<br/>X_type  = type(X)<br/>y_shape = y.shape<br/>y_type  = type(y)</span><span id="cca3" class="ki kj hi ke b fi ko kl l km kn">print(f'X: Type-<strong class="ke hj">{</strong>X_type<strong class="ke hj">}</strong>, Shape-<strong class="ke hj">{</strong>X_shape<strong class="ke hj">}</strong>') <br/>print(f'y: Type-<strong class="ke hj">{</strong>y_type<strong class="ke hj">}</strong>, Shape-<strong class="ke hj">{</strong>y_shape<strong class="ke hj">}</strong>')</span><span id="bf64" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">Out:<br/></strong>X: Type-&lt;class 'numpy.ndarray'&gt;, Shape-(1460, 79) <br/>y: Type-&lt;class 'numpy.ndarray'&gt;, Shape-(1460,)</span></pre><p id="5942" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">将我们的数据分为训练和测试数据:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="f5c3" class="ki kj hi ke b fi kk kl l km kn"><strong class="ke hj">from</strong> <strong class="ke hj">sklearn.model_selection</strong> <strong class="ke hj">import</strong> train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)</span><span id="a256" class="ki kj hi ke b fi ko kl l km kn">print(X_train.shape, X_test.shape)<br/>print(y_train.shape, y_test.shape)</span><span id="cf80" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">Out:<br/></strong>(1095, 79) (365, 79)<br/>(1095,) (365,)</span></pre><p id="f30c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">编写我们的预测函数，返回我们的</em> <strong class="ih hj"> <em class="jd">假设:</em> </strong></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="c906" class="ki kj hi ke b fi kk kl l km kn"><strong class="ke hj">def</strong> predict(X, weights):<br/>    y_pred = np.dot(X, weights)<br/>    <strong class="ke hj">assert</strong> (y_pred.shape==(X.shape[0],1))<br/>    <strong class="ke hj">return</strong> y_pred</span></pre><p id="7fb6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">定义在给定训练示例中返回损失\成本函数值的均方误差函数:</em></p><blockquote class="jx jy jz"><p id="0eee" class="if ig jd ih b ii ij ik il im in io ip ka ir is it kb iv iw ix kc iz ja jb jc hb bi translated"><strong class="ih hj">损失函数:</strong>当你只考虑单个训练例子时。</p><p id="1caf" class="if ig jd ih b ii ij ik il im in io ip ka ir is it kb iv iw ix kc iz ja jb jc hb bi translated"><strong class="ih hj">成本函数:</strong>当你考虑整批/小批时。</p></blockquote><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="f660" class="ki kj hi ke b fi kk kl l km kn"><strong class="ke hj">def</strong> mean_squared_error(y_true, y_pred): <br/>    loss = (1/(2*y_true.shape[0])*np.sum(y_true-y_pred)**2)<br/>    <strong class="ke hj">return</strong> loss</span></pre><p id="0f71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">定义我们的梯度(梯度矩阵初始化为0): </em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="547e" class="ki kj hi ke b fi kk kl l km kn"><strong class="ke hj">def</strong> gradient(X, y_true, y_pred):<br/>    grad = np.zeros((len(X[1]),1))<br/>    diff = y_pred-y_true<br/>    <strong class="ke hj">for</strong> i <strong class="ke hj">in</strong> range(len(X[1])):<br/>      grad[i][0] = (2/X.shape[0])*np.sum(np.dot(X[:,i],(diff)))<br/>    <strong class="ke hj">return</strong> grad</span></pre><p id="1601" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">定义我们的梯度下降函数(将我们的权重初始化为随机数——也可以初始化为0): </em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="106e" class="ki kj hi ke b fi kk kl l km kn"><strong class="ke hj">def</strong> gradient_descent(X, y, learning_rate=0.01, max_iterations=100):<br/><br/>    weights = np.random.rand(len(X[1]),1)<br/>    losses  = []<br/>  <br/>    y_true = y.reshape(-1,1)<br/>    <strong class="ke hj">for</strong> i <strong class="ke hj">in</strong> range(max_iterations):<br/>        y_pred = predict(X,weights)<br/>        losses.append(mean_squared_error(y_true,y_pred))<br/>        grad = gradient(X,y_true,y_pred)<br/><br/>        <strong class="ke hj">for</strong> i <strong class="ke hj">in</strong> range(len(X[1])):<br/>          weights[i][0] = weights[i][0] - learning_rate*grad[i][0]<br/>    <br/>    <strong class="ke hj">return</strong> weights, losses</span></pre><p id="b801" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">让我们看看我们的模型根据我们的训练数据学习到的最佳权重:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="3653" class="ki kj hi ke b fi kk kl l km kn">optimal_weights, losses = gradient_descent(X_train, y_train, 0.001, 200)</span></pre><blockquote class="jx jy jz"><p id="444f" class="if ig jd ih b ii ij ik il im in io ip ka ir is it kb iv iw ix kc iz ja jb jc hb bi translated">调整您的超参数——Alpha(学习率)和最大迭代次数，查看其对准确性的影响)</p></blockquote><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="47da" class="ki kj hi ke b fi kk kl l km kn">print("Root mean-squared error:", losses[-1]**(1/2))</span><span id="b6d6" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">Out:<br/></strong>Root mean-squared error: 5116.780901005974</span></pre><p id="3d86" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">检查你的梯度下降是否有效(如果损失在减少):</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="71e7" class="ki kj hi ke b fi kk kl l km kn"><strong class="ke hj">for</strong> i <strong class="ke hj">in</strong> range(len(losses)):<br/>  print(losses[i]**(1/2))</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es la"><img src="../Images/b6f6b9168d564c1266007c76e50c59b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*-xA5fvLNR5sw_PGW5DXe7A.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx">………</figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lb"><img src="../Images/7b3a7fb0d94ca4c408ea8bbc7f2e86f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*0mwvOx5i5ueQ633HCpU31w.png"/></div></figure><blockquote class="jx jy jz"><p id="86a4" class="if ig jd ih b ii ij ik il im in io ip ka ir is it kb iv iw ix kc iz ja jb jc hb bi translated">正如你所看到的，我们的损失在每一次迭代中不断减少，这意味着我们的梯度下降运行良好！</p></blockquote><p id="828d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">在我们的训练数据上看到我们的预测:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="4691" class="ki kj hi ke b fi kk kl l km kn">train_pred = predict(X_train, optimal_weights)<br/>train_pred</span><span id="f088" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">Out:</strong><br/>array([[191931.99404727],<br/>       [172596.37961212],<br/>       [211588.65076176],<br/>       ...,<br/>       [204967.81704443],<br/>       [166346.22710897],<br/>       [192415.28850303]])</span><span id="45b0" class="ki kj hi ke b fi ko kl l km kn"><em class="jd">#actual values<br/></em>y_train</span><span id="ece5" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">Out:<br/></strong>array([196000, 147500, 253293, ..., 235000, 167500, 250000])</span></pre><p id="e62a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">在测试集上推断我们训练好的模型:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="03cc" class="ki kj hi ke b fi kk kl l km kn">test_pred = predict(X_test, optimal_weights)<br/>test_pred</span><span id="9bb3" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">Out:<br/></strong>array([[162556.88304305],<br/>       [195496.11789844],<br/>       [180261.96458508],<br/>       ...,<br/>       [163731.79340151],<br/>       [201160.3300839 ],<br/>       [175285.52648482]])</span></pre><p id="fcea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">绘制我们的损失曲线:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="722d" class="ki kj hi ke b fi kk kl l km kn"><strong class="ke hj">import</strong> <strong class="ke hj">matplotlib.pyplot</strong> <strong class="ke hj">as</strong> <strong class="ke hj">plt</strong><br/>plt.plot([i <strong class="ke hj">for</strong> i <strong class="ke hj">in</strong> range(len(losses))], losses)<br/>plt.title("Loss curve")<br/>plt.xlabel("Iteration num")<br/>plt.ylabel("Loss")<br/>plt.show()</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lc"><img src="../Images/941f6de5cb0617732d82b2c31ff9207f.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*BlnjN0Wpt52H6OkfIMznVw.png"/></div></figure><p id="6558" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">你可以用Sklearn内置的函数来比较同一个问题:</em></p><pre class="jg jh ji jj fd kd ke kf kg aw kh bi"><span id="fbf2" class="ki kj hi ke b fi kk kl l km kn"><strong class="ke hj">from</strong> <strong class="ke hj">sklearn.linear_model</strong> <strong class="ke hj">import</strong> LinearRegression</span><span id="2c48" class="ki kj hi ke b fi ko kl l km kn">model = LinearRegression().fit(X_train, y_train)<br/>pred = model.predict(X_train)<br/>r2_score(y_train, pred)</span><span id="5cd0" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">Out:<br/></strong>0.8540160463212708</span><span id="e76b" class="ki kj hi ke b fi ko kl l km kn">pred2 = model.predict(X_test)<br/>r2_score(y_test, pred2)</span><span id="6487" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">Out:<br/></strong>0.8431209747429238</span></pre><p id="26ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">您可以根据不同的回归指标评估您的模型，如MSE、RMSE、MAE、R2分数等。</em></p><blockquote class="jx jy jz"><p id="116c" class="if ig jd ih b ii ij ik il im in io ip ka ir is it kb iv iw ix kc iz ja jb jc hb bi translated">想了解更多，请查看我之前的博客— <a class="ae je" rel="noopener" href="/analytics-vidhya/everything-you-need-to-know-about-linear-regression-750a69a0ea50"> <strong class="ih hj">关于线性回归你需要知道的一切</strong> </a></p></blockquote><p id="1c90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">要获得完整的代码，请查看我的GitHub库— </em> </strong></p><div class="ld le ez fb lf lg"><a href="https://github.com/tanvipenumudy/Winter-Internship-Internity/blob/main/Day%2007/Day-7%20Notebook%20%28Linear%20Regression%29.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="lh ab dw"><div class="li ab lj cl cj lk"><h2 class="bd hj fi z dy ll ea eb lm ed ef hh bi translated">tanvipenumudy/Winter-实习-实习</h2><div class="ln l"><h3 class="bd b fi z dy ll ea eb lm ed ef dx translated">存储库跟踪每天分配的工作-tanvipenumudy/Winter-实习-实习</h3></div><div class="lo l"><p class="bd b fp z dy ll ea eb lm ed ef dx translated">github.com</p></div></div><div class="lp l"><div class="lq l lr ls lt lp lu jp lg"/></div></div></a></div><blockquote class="jx jy jz"><p id="8d95" class="if ig jd ih b ii ij ik il im in io ip ka ir is it kb iv iw ix kc iz ja jb jc hb bi translated">如果你喜欢这篇文章，别忘了鼓掌并跟随:</p></blockquote></div></div>    
</body>
</html>