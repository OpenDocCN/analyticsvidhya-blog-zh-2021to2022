<html>
<head>
<title>Fine-tune GPT-2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微调GPT-2</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fine-tune-gpt-2-225c09400cb6?source=collection_archive---------5-----------------------#2021-03-09">https://medium.com/analytics-vidhya/fine-tune-gpt-2-225c09400cb6?source=collection_archive---------5-----------------------#2021-03-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4a61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将尝试展示GPT-2的简单用法和训练。我假设你对GPT-2有基本的了解。GPT是一个自回归的语言模型。它可以用它巨大的预训练模型为我们生成文本。我想微调GPT-2，以便它为我的任务生成更好的文本。为此，我从维基百科下载了关于日本的页面，并创建了一个包含4万个句子的文件。我希望经过一些训练后，model能为我生成更好的关于日本的句子。github上的代码。(<a class="ae jd" href="https://github.com/mcelikkaya/medium_articles/blob/main/gtp2_training.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>)</p><p id="2dc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于训练需要大量的资源，没有GPU很难训练。建议试试Colab。但是当我在尝试的时候，我用GPU太多了，Colab拒绝给我新的GPU 2天。所以我放弃了修改代码。尝试时要小心。</p><p id="295d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GPT-2是一个语言模型，意味着我们将创造新的文本。因此，对于互联网上的样本，你可以看到给语言模型的莎士比亚文本样本，它生成类似莎士比亚的文本。所以对于一个基本的测试，我将为一个特定的任务训练我的网络。我会下载维基百科中与日本相关的页面，并尝试生成关于日本的有意义的句子。</p><p id="b140" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于语言模型，我们检查输出句子的概率。它包括语义，语法…在这个简单的任务中，我将检查生成的句子是否变得更有意义。如果你想尝试不同的设置，改变代码中的<strong class="ih hj">【all _ sentences】</strong>，你可以尝试任何你想做的事情。您还必须为不同的序列更改“eval_keywords”。</p><p id="9d03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如何生成文本？</strong></p><p id="4a51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GPT-2记号赋予器为我们编码文本，但是根据参数我们得到不同的结果。在下面的代码中，你可以看到一个非常简单的循环。我们用tokenizer对文本进行编码(第2行)。我们给模型的输入张量加上一些参数(第4行)。和模型生成文本，我们需要将生成的张量转换回单词。(第16行)使用truncate参数提前停止。GPT-2为我们生成1024个令牌序列。GPT-2不会停止生成，因此在生成函数中使用truncate参数，以便GPT-2在生成结束令牌时停止。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><strong class="ak">文本生成的简单循环</strong></figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jp"><img src="../Images/9a85b1e3cd511c26e3ab2f1b4606cde7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8lHqw_BE8Rcl_qSMyHwXhw.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><strong class="bd jw">样本代码输出</strong></figcaption></figure><p id="aca2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">培训是如何进行的？</strong></p><p id="80a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">语言模型是一个自回归模型。意思是我们给x1，它生成x2，然后我们给[x1，x2]，它生成x3。然后我们给[x1，x2，x3]…这样我们就生成了新的文本。训练以相反的顺序进行。Huggingface事实上使一切变得如此简单，我们不必像在其他神经网络问题中那样给出输入、目标。我们只输入句子，huggingface生成输入，这是训练所必需的目标。</p><p id="a4f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常在训练集中，我们准备x(特征)和y(目标，目的)变量，GPT-2 hugginface自动为我们做这项工作。所以我们只给x，因为target也是x。(我们是从x生成x，就像我上面解释的那样。)所以数据集如果只是句子的编码，如下。没有目标变量。如果您检查代码片段“process_one_batch ”,我会将该值直接提供给hugginface。正如你在下面第15行看到的，我通常只在一个数据集(x，y)中对句子(输入)进行编码，这里只有x。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="a4a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以训练的主要循环如下。在下面的代码(第10行)你可以看到1个训练时期。我们从数据加载器获得一批数据。我们将这一批交给模型，模型返回输出。事实上，不需要更多的解释，正如你所看到的，这是神经网络训练的正常循环。我正在给出与“日本”相关的句子，网络学习生成关于<strong class="ih hj">“日本】、</strong>的句子，就这么简单。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="947e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">经过一些训练后，我可以看到如下输出，你可以看到它正在生成更好的句子。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jx"><img src="../Images/471b87372fbafdb0bb58ee91a57a84de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7QoAB5cNgHDgDPTRPRM3BA.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">培训后的输出</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jy"><img src="../Images/5d3f84012977089c5a6b9cd2531e6381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yz5dULoIFiTKx3bghLokPQ.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jz"><img src="../Images/bf0edd56c6208fda5f86edf3785d5799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WLL3fSGJausATU7NBIlNdg.png"/></div></div></figure><p id="8fca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我进行更多的训练，我会有更好的结果，但正如我之前所说，Colab拒绝给我几天的GPU。可能我这一周消耗了所有的资源:(。如果你有样本输入，并尝试足够长的时间，你会看到非常好的结果。这是一个非常简单的总结如何训练GPT-2为我们的特定任务。</p></div></div>    
</body>
</html>