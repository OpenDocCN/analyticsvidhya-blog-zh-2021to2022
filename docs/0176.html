<html>
<head>
<title>Text Classification — From Bag-of-Words to BERT — Part 5( Recurrent Neural Network)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本分类—从词袋到BERT —第五部分(递归神经网络)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-5-recurrent-neural-network-b825ffc8cb26?source=collection_archive---------5-----------------------#2021-01-07">https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-5-recurrent-neural-network-b825ffc8cb26?source=collection_archive---------5-----------------------#2021-01-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/ba1a45d449a4bfae6e65344a227d8acc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-A6bbN3liJGKv3o-"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae it" href="https://unsplash.com/@tine999?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tine ivani</a>拍摄的照片</figcaption></figure><p id="f110" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个故事是一系列文本分类的一部分——从词袋到BERT在名为“<a class="ae it" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge" rel="noopener ugc nofollow" target="_blank"> <em class="js">有毒评论分类挑战”</em> </a> <strong class="iw hi"> <em class="js">的Kaggle比赛上实施多种方法。</em> </strong>在这场比赛中，我们面临的挑战是建立一个多头模型，能够检测不同类型的毒性，如<em class="js">威胁、淫秽、侮辱和基于身份的仇恨。如果你还没看过之前的报道，那就去看看吧</em></p><p id="7fbe" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9" rel="noopener">第一部分(BagOfWords) </a></p><p id="56b1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-2-word2vec-35c8c3b34ee3" rel="noopener">第二部分(Word2Vec) </a></p><p id="e4f7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-3-fasttext-8313e7a14fce" rel="noopener">第三部分(快速文本)</a></p><p id="d3ad" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-4-convolutional-neural-network-53aa63941ade" rel="noopener">第四部分(卷积神经网络)</a></p><p id="5c2c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在更早的故事中(<a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-4-convolutional-neural-network-53aa63941ade" rel="noopener">第4部分(卷积神经网络)</a>)，我们使用Keras库(TensorFlow上的一个包装器)为输出变量——有毒、严重有毒、淫秽、威胁、侮辱、身份仇恨——的多标签文本分类创建一维CNN。</p><p id="93bb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这一次，我们将使用相同的Keras库来创建长短期记忆(LSTM ),这是对用于多标签文本分类的常规rnn的改进。我们将首先通过对RNNs和LSTM如何工作的一点直觉，然后使用用于多标签分类的最小化单输出层(具有6个神经元)来实现它(而不是为每种类型的毒性创建6个单独的网络或创建具有6个输出层的多输出层网络)。我们将只使用一个LSTM层，在一个时期内，它在排行榜上给出了大约96 AUC</p><h1 id="496a" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">直觉</h1><h2 id="1afd" class="kr ju hh bd jv ks kt ku jz kv kw kx kd jf ky kz kh jj la lb kl jn lc ld kp le bi translated">我们为什么需要注册护士？</h2><p id="6ded" class="pw-post-body-paragraph iu iv hh iw b ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn lj jp jq jr ha bi translated">在传统的神经网络中，我们假设所有的输入(和输出)都是相互独立的。它们不共享在文本的不同位置学到的特征。对于序列信息(如文本数据或时序数据)来说，这可能是一个问题，因为每个实例都依赖于前一个实例。rnn被称为递归的，因为它们对序列的每个元素执行相同的任务，输出依赖于先前的计算。另一种思考rnn的方式是，它们有一个“记忆”，可以捕获到目前为止已经计算过的信息。</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lk"><img src="../Images/acfc7e5053290aab3992d265ef1f3c8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qyKDVjKFOC71AaIu23N1BQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank">https://Stanford . edu/~ shervine/teaching/cs-230/cheat sheet-recurrent-neural-networks</a></figcaption></figure><h2 id="05c3" class="kr ju hh bd jv ks kt ku jz kv kw kx kd jf ky kz kh jj la lb kl jn lc ld kp le bi translated"><strong class="ak">无线网络的架构是什么？</strong></h2><p id="e319" class="pw-post-body-paragraph iu iv hh iw b ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn lj jp jq jr ha bi translated">RNN的整体架构取决于手头的任务。对于这个分类任务，我们将使用第三种方法:多对一。但是出于直觉的目的，让我们看看第五个，它是RNNs的一个更一般化的符号。如果我们知道第五记谱法是如何工作的，那么只需改变一小部分就可以了。</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lp"><img src="../Images/52245d449ed0f23766504fbd987aed1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zad_IWHzl4xb-9aVUIn17g.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://www.di.ens.fr/~lelarge/dldiy/slides/lecture_8/images/rnn_variants_4.png" rel="noopener ugc nofollow" target="_blank">https://www . di . ens . fr/~ le large/dldiy/slides/lecture _ 8/images/rnn _ variants _ 4 . png</a></figcaption></figure><p id="7fb5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">输入向量用红色表示，输出向量用蓝色表示，绿色向量表示RNN状态。从左到右:(1)没有RNN的普通处理模式(例如图像分类)(2)序列输出(例如图像字幕)(3)序列输入(例如情感分析)(4)序列输入和序列输出(例如机器翻译)(5)同步序列输入和输出(例如视频分类——标记视频的每一帧)</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lq"><img src="../Images/e78014fd18f7f81290702694649d8c9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ii9RmveqIBiGEcI1gAGtbA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">RNNs中的前向和反向传播(<a class="ae it" href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" rel="noopener ugc nofollow" target="_blank">http://www . wild ml . com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-RNNs/</a>)</figcaption></figure><h2 id="873c" class="kr ju hh bd jv ks kt ku jz kv kw kx kd jf ky kz kh jj la lb kl jn lc ld kp le bi translated">什么是消失渐变？</h2><p id="84fd" class="pw-post-body-paragraph iu iv hh iw b ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn lj jp jq jr ha bi translated">消失梯度问题出现在非常深的神经网络中，通常是rnn，其使用梯度趋于小(在0-1的范围内)的激活函数。因为这些小梯度在反向传播过程中会成倍增加，所以它们往往会在所有层中“消失”或减少到0，从而阻止网络学习长程相关性。随着序列变长，传递到先前状态的梯度/导数变得越来越小。这个问题有很多解决方法。其中之一是使用一种叫做LSTMs的变种。</p><h2 id="6daa" class="kr ju hh bd jv ks kt ku jz kv kw kx kd jf ky kz kh jj la lb kl jn lc ld kp le bi translated">什么是LSTM？</h2><p id="c8ec" class="pw-post-body-paragraph iu iv hh iw b ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn lj jp jq jr ha bi translated">长短期记忆网络——通常简称为“lstm”——是一种特殊的RNN，能够学习长期依赖性。所有的rnn都具有神经网络重复模块链的形式。LSTM也有这种链状结构，但我们没有隐藏层，而是有一种叫做LSTM单元的东西，我们还有另一种连接，它贯穿所有时间步骤和隐藏状态。这就是所谓的“单元状态”向量，可以根据需要从其中检索和删除信息。</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/81e858f4d38e52c11d70530d4ceabc84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*XNyENfJKXfR0VisEYvH6uw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></figcaption></figure><p id="9148" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们看看这6个步骤:</p><ol class=""><li id="6d86" class="ls lt hh iw b ix iy jb jc jf lu jj lv jn lw jr lx ly lz ma bi translated">这是<strong class="iw hi">遗忘门</strong>，它负责遗忘多少，由于它通过一个sigmoid函数，它将给出0到1的值，这是从先前单元状态保留的存储量</li><li id="3607" class="ls lt hh iw b ix mb jb mc jf md jj me jn mf jr lx ly lz ma bi translated">这是<strong class="iw hi">输入门</strong>，它负责向单元状态添加多少新信息。类似于遗忘门，这也将给出0到1的值，这是要添加的新内存量</li><li id="c5cf" class="ls lt hh iw b ix mb jb mc jf md jj me jn mf jr lx ly lz ma bi translated">这是新的候选向量/单元状态的创建</li><li id="10e8" class="ls lt hh iw b ix mb jb mc jf md jj me jn mf jr lx ly lz ma bi translated">这是更新单元状态的地方，该单元状态是先前单元状态和当前单元状态的组合，分别使用<strong class="iw hi">遗忘门</strong>和<strong class="iw hi">输入门</strong>控制每个单元状态的贡献。</li><li id="e2ff" class="ls lt hh iw b ix mb jb mc jf md jj me jn mf jr lx ly lz ma bi translated">这是<strong class="iw hi">输出门</strong>,其负责在具有0和1之间的值的隐藏状态中记忆更新的单元状态的哪一部分</li><li id="2624" class="ls lt hh iw b ix mb jb mc jf md jj me jn mf jr lx ly lz ma bi translated">这是更新后的隐藏状态，将作为下一个单元的输入，并基于由<strong class="iw hi">输出门</strong>控制的当前单元状态</li></ol><h2 id="3db6" class="kr ju hh bd jv ks kt ku jz kv kw kx kd jf ky kz kh jj la lb kl jn lc ld kp le bi translated">LSTM如何解决消失梯度？</h2><ul class=""><li id="001a" class="ls lt hh iw b ix lf jb lg jf mg jj mh jn mi jr mj ly lz ma bi translated">LSTM架构使得RNN更容易在多个时间步长上保存信息，例如，如果遗忘门被设置为在每个时间步长上记住所有内容，则单元中的信息被无限期保存。相比之下，香草RNN更难学习一个递归的权重矩阵，以隐藏状态保存信息</li><li id="4706" class="ls lt hh iw b ix mb jb mc jf md jj me jn mf jr mj ly lz ma bi translated">LSTM并不保证没有消失/爆炸梯度，但它确实为模型学习长距离依赖性提供了一种更简单的方法</li></ul><p id="5197" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在变形金刚诞生之前，LSTMs统治着NLP的世界。即使在今天，它仍被广泛使用</p><p id="c34c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2015年，谷歌在谷歌语音中使用了LSTM，这将转录错误减少了49%。</p><p id="8566" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2016年，谷歌使用LSTM在Allo conversation应用程序中建议消息。谷歌使用LSTMs进行谷歌翻译，减少了60%的翻译错误。苹果宣布，将开始在iPhone和Siri中使用quick type LSTM。亚马逊发布了波利，Alexa背后的声音，使用双向LSTM。</p><p id="53c3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2017年，脸书每天使用LSTMs进行约45亿次自动翻译</p><p id="c45f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">足够的上下文，让我们深入代码👨‍💻对于对完整代码感兴趣的人来说，这里是它现在的<a class="ae it" href="https://www.kaggle.com/anirbansen3027/jtcc-multilabel-lstm-keras" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="19aa" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">履行</h1><h2 id="8834" class="kr ju hh bd jv ks kt ku jz kv kw kx kd jf ky kz kh jj la lb kl jn lc ld kp le bi translated">1.读取数据集</h2><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mk"><img src="../Images/90759cb3db6cdcb6e9cf373b4502526b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZCYfgXTmjnb1kYgL.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">提醒一下，这是训练数据的样子</figcaption></figure><h2 id="fd28" class="kr ju hh bd jv ks kt ku jz kv kw kx kd jf ky kz kh jj la lb kl jn lc ld kp le bi translated">2.文本预处理</h2><p id="4ffa" class="pw-post-body-paragraph iu iv hh iw b ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn lj jp jq jr ha bi translated">LSTM模型的预处理与CNN非常相似。</p><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="b6db" class="kr ju hh mm b fi mq mr l ms mt"><em class="js">#Initializing the class</em><br/>tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)<br/><em class="js">#Updates internal vocabulary based on a list of texts.</em><br/>tokenizer.fit_on_texts(train_texts)<br/><em class="js">#Transforms each text in texts to a sequence of integers.</em><br/>train_sequences = tokenizer.texts_to_sequences(train_texts)<br/>test_sequences = tokenizer.texts_to_sequences(test_texts)<br/>word_index = tokenizer.word_index<br/>print("Length of word Index:", len(word_index))<br/>print("First 5 elements in the word_index dictionary:", dict(list(word_index.items())[0: 5]) )<br/>print("First comment text in training set:<strong class="mm hi">\n</strong>", train_sequences[0])</span><span id="4c7a" class="kr ju hh mm b fi mu mr l ms mt"><em class="js">#Pad tokenized sequences</em><br/>trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)<br/>test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)<br/>print("Shape of padded sequence list:<strong class="mm hi">\n</strong>", trainvalid_data.shape)<br/>print("First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:<strong class="mm hi">\n</strong>", trainvalid_data[0][-50:])</span></pre><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es mv"><img src="../Images/35e1f27aae4d72f17e801e04d7cc30e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*lJxwNIQ3jsBLx6Vo4Gw3-w.jpeg"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><strong class="bd jv">顶部</strong>:原字符串<strong class="bd jv">中间</strong>:标记文本<strong class="bd jv">底部</strong>:填充文本</figcaption></figure><p id="4309" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们使用Keras中的Tokenizer类，通过基于频率将每个单词映射到一个数字，将字符串标记为一个数字序列。我们还使用Keras中的pad_sequences填充标记化的整数序列，使所有序列的大小相同，以便进行矢量化计算。我建议仔细阅读<a class="ae it" href="https://www.kaggle.com/anirbansen3027/jtcc-cnn#3.-Text-Preprocessing" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><p id="9f88" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js">我们将按照这些步骤使用LSTM进行多标签文本分类:</em> <strong class="iw hi">输入字符串- &gt;标记化- &gt;填充- &gt;嵌入- &gt; LSTM - &gt;分类器</strong></p><h2 id="60cd" class="kr ju hh bd jv ks kt ku jz kv kw kx kd jf ky kz kh jj la lb kl jn lc ld kp le bi translated">3.定义多标签LSTM模型</h2><p id="29ec" class="pw-post-body-paragraph iu iv hh iw b ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn lj jp jq jr ha bi translated">在Keras中，定义模型最简单的方法是启动一个顺序模型类，并不断添加所需的层。在这个神经网络模型中，使用了一个称为“下降”的新参数:</p><p id="1e52" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">剔除:</strong>剔除是一种解决过度拟合问题的技术。关键思想是在训练期间从神经网络中随机丢弃单元(连同它们的连接)。这可以防止单位之间过度的相互适应。引入该超参数是为了指定该层的输出被丢弃的概率。</p><p id="c638" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">循环丢失</strong>屏蔽(或“丢弃”)循环单元之间的连接。</p><p id="d7f6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">重要提示:一般情况下，</strong></p><p id="2316" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于<strong class="iw hi">二进制</strong>分类，我们可以有1个输出单元，在输出层使用sigmoid激活并使用二进制交叉熵损失</p><p id="1163" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于<strong class="iw hi">多类</strong>分类，我们可以有N个输出单元，在输出层使用softmax激活，并使用分类交叉熵损失</p><p id="3cd4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于<strong class="iw hi">多标签</strong>分类，我们可以有N个输出单元，在输出层使用sigmoid激活并使用二进制交叉熵损失</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es mw"><img src="../Images/faec5d370b0241661fca3dca908522df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*8uaBTixWsYxIJsFX9pCL5Q.png"/></div></figure><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="d5f5" class="kr ju hh mm b fi mq mr l ms mt">rnn_model = Sequential()<br/>rnn_model.add(Embedding(MAX_NUM_WORDS, 128))<br/>rnn_model.add(LSTM(units = 128, dropout = 0.2, recurrent_dropout = 0.2))<br/>rnn_model.add(Dense(units = 6, activation = 'sigmoid'))<br/>print(rnn_model.summary()</span></pre></div><div class="ab cl mx my go mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="ha hb hc hd he"><h2 id="75e4" class="kr ju hh bd jv ks kt ku jz kv kw kx kd jf ky kz kh jj la lb kl jn lc ld kp le bi translated">5.编译和训练LSTM模型</h2><p id="49ec" class="pw-post-body-paragraph iu iv hh iw b ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn lj jp jq jr ha bi translated">编译和训练/拟合代码也与CNN模型非常相似。在开始训练模型之前，我们需要对其进行配置。我们需要提到损失函数，它将用于计算每次迭代的误差，优化器将指定如何更新权重，以及模型在训练和测试期间评估的指标。我在之前的博客(<a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-4-convolutional-neural-network-53aa63941ade" rel="noopener">第四部分(卷积神经网络)</a>)中已经深入讨论了训练参数</p><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="5603" class="kr ju hh mm b fi mq mr l ms mt"><em class="js">#Configures the model for training.</em><br/>rnn_model.compile(loss = "binary_crossentropy", optimizer = "adam", metrics = ["AUC"])<br/><em class="js">#Split the dataset into train and validation set for training and evaludating the model</em><br/>X_train, X_val, y_train, y_val = train_test_split(trainvalid_data, train_labels, shuffle = True, random_state = 123)<br/>print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)<br/><em class="js">#Trains the model for a fixed number of epochs (iterations on a dataset)</em><br/>history = rnn_model.fit(X_train, y_train, batch_size = 128, epochs = 1, validation_data = (X_val, y_val))</span></pre><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ne"><img src="../Images/1b4a2053bc3733a6af55c93ee046ab57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X_xxY41Uac1DvBRiXKVJtw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Val_AUC仅在一个时期内为0.98，具有单个LSTM层并且几乎没有调谐</figcaption></figure><h2 id="9854" class="kr ju hh bd jv ks kt ku jz kv kw kx kd jf ky kz kh jj la lb kl jn lc ld kp le bi translated">5.改进的结果和范围</h2><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/913c79309f7bf20b1dd4b9d8422afa0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*MR90hVnh68aU7TbtuuVTpA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Kaggle排行榜分数(相当惊人的一个时代吧？)</figcaption></figure><ul class=""><li id="0117" class="ls lt hh iw b ix iy jb jc jf lu jj lv jn lw jr mj ly lz ma bi translated">堆叠1个以上的LSTM层</li><li id="970b" class="ls lt hh iw b ix mb jb mc jf md jj me jn mf jr mj ly lz ma bi translated">时期、学习率、批量大小、提前停止的超参数调整</li></ul><p id="34d8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是关于LSTMs的。下一个将是关于强大的BERT，它是建立在变压器网络上的，目前正统治着NLP的世界。同样，这个博客的全部代码都在这里<a class="ae it" href="https://www.kaggle.com/anirbansen3027/jtcc-multilabel-lstm-keras" rel="noopener ugc nofollow" target="_blank">(这里)</a>。请以回答和鼓掌的形式提供您的反馈:)</p></div></div>    
</body>
</html>