<html>
<head>
<title>What is Residual Network or ResNet? — Idiot Developer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是剩余网络或ResNet？—白痴开发者</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-is-residual-network-or-resnet-idiot-developer-6a1daa7c3b09?source=collection_archive---------0-----------------------#2021-09-25">https://medium.com/analytics-vidhya/what-is-residual-network-or-resnet-idiot-developer-6a1daa7c3b09?source=collection_archive---------0-----------------------#2021-09-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="7b99" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">深度神经网络因其在现实世界应用中的高性能而变得流行，如<a class="ae jc" href="https://idiotdeveloper.com/dog-breed-classification-using-transfer-learning-in-tensorflow/" rel="noopener ugc nofollow" target="_blank">图像分类</a>，语音识别，机器翻译等等。随着时间的推移，深度神经网络变得越来越深，以解决更复杂的任务。向深度神经网络添加更多层可以提高其性能，但在一定程度上。之后，训练网络变得困难，性能开始下降。为了解决这些挑战，研究人员引入了剩余网络。</p><p id="4831" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们将了解更多关于剩余网络及其工作原理，并帮助解决上述挑战。</p><p id="1aa7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">目录:</strong></p><ol class=""><li id="d750" class="jd je hh ig b ih ii il im ip jf it jg ix jh jb ji jj jk jl bi translated">什么是剩余网络</li><li id="8615" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">为什么我们需要一个剩余网络？</li><li id="78ea" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">什么是残块？</li><li id="596b" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">残余块如何帮助</li><li id="60c7" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">ResNet架构</li><li id="6770" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">摘要</li><li id="c665" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">阅读更多</li></ol><h1 id="81c4" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">什么是残余网络？</h1><p id="aada" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">残差网络(ResNet)是由、何、、、任、于2015年在他们的论文“<a class="ae jc" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习</a>”中提出的。</p><p id="3b5a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些ResNet模型的成功可以从以下几点看出:</p><ul class=""><li id="1ac6" class="jd je hh ig b ih ii il im ip jf it jg ix jh jb ku jj jk jl bi translated">ResNet模型以3.57%的前5名错误率在ILSVRC 2015分类竞赛中获得第一名。</li><li id="a6ad" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ku jj jk jl bi translated">该网络在多个ILSVRC和COCO 2015竞赛中获得第一名。这些竞赛包括— ImageNet检测和定位、COCO检测和分割。</li><li id="76bb" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ku jj jk jl bi translated">通过用ResNet-101替换更快的R-CNN中的VGG-16层，观察到28%的改进。</li></ul><h1 id="2440" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">为什么我们需要一个剩余网络？</h1><p id="6079" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">我们知道神经网络是通用函数逼近器，当给定一个好的数据集时，它们可以学习将输入X映射到输出Y。神经网络的性能随着层数的增加而提高。添加更多层的原因是这些层将学习更复杂的功能。最初的层将学习检测线条、边缘等，随后的层在最后可以检测可识别的对象，如狗或猫。</p><p id="1012" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">向神经网络添加层数是有限制的。达到该阈值后，模型的精度开始饱和，然后下降。这是由于消失/爆炸梯度，这导致梯度成为0或太大。因此，当我们增加层数时，训练和测试错误率也会增加。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es kv"><img src="../Images/1b9702c0525b5b858808515de2899557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JD6gKRqfibBJDUvv"/></div></div></figure><p id="89d9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上图中，我们可以说浅网络比深网络表现更好。我们可以看到20层网络在训练和测试中的错误率都小于56层网络。</p><p id="447b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可能还会认为56层模型是过度拟合的。在过拟合的情况下，模型的训练性能急剧增加，而测试性能显著下降。这里，56层网络的性能在训练和测试中都较差。</p><p id="4d29" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于网络的深度和维数灾难，网络不能被适当地训练。因此，浅层(20层)网络比深层(56层)网络表现更好。</p><h1 id="3d67" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">什么是残块？</h1><p id="b3be" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">剩余网络由作为网络主要组成部分的剩余单元或块组成。在深入细节之前，这里是残差块的示意图。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lh"><img src="../Images/ae32c4a8e20d7be9440861a4164871ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jDEs1BzL9n28hTaR.png"/></div></div></figure><p id="5ef6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在以前的方法中，每一层向其下一层馈送，并且同样的事情随后继续。在剩余网络中，每一层都连接到下一层，并直接连接到下一层的2-3层。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es li"><img src="../Images/8e1c7dd55b4d21eb2a29c47294b7b497.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/0*e72Gp9_sroN-a66J.png"/></div></figure><p id="6b16" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">残差块由两个3×3卷积层和一个标识映射(也称为快捷连接)组成。每个卷积层后面都有一个批量标准化层和一个ReLU(校正线性单位)激活函数。在标识映射与最后一批规范化的输出之间执行元素式加法。</p><p id="872b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于两个特征的元素相加，输出通道(特征通道)的数量应该相同。在输出通道的数量不相同的情况下，我们对身份映射执行1×1卷积和批量归一化。</p><h1 id="1a86" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">残余块如何帮助</h1><p id="7482" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">残余块帮助研究人员建立和训练更深的网络，而没有消失/爆炸梯度的问题。剩余块中存在的身份映射或快捷连接在以下方面有所帮助:</p><p id="df32" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果正常流程中的层没有学到任何东西，那么身份映射基本上是从更早的层复制信息。这有助于神经网络即使在更深的架构中也能更好地执行。</p><p id="a286" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用残差网络或ResNet可以极大地提高神经网络的性能，尽管有更多的层。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lj"><img src="../Images/d124af6eb371623f361cfe64129e7139.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GfaGSNrwwp6lP9fp.png"/></div></div></figure><p id="3fe3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上图清楚地表明，与plain-34相比，ResNet-34的错误率要低得多。此外，我们可以看到plain-18和ResNet-18的错误率几乎相同。</p><h1 id="8fc2" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">ResNet架构</h1><p id="08e2" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">有许多ResNet体系结构是用网络中不同数量的层构建的。这些网络是:</p><p id="0628" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">剩余网络采用受VGG-19协议启发的34层平面网络架构，然后添加快捷连接，从而形成34层剩余网络，如下图所示:</p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es lk"><img src="../Images/f6954f72c392396b7214b2ccf6db81c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/0*FA4cOuEVvGsQmpBE.png"/></div></figure><h1 id="89c3" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">摘要</h1><p id="aa36" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">在本文中，您将了解剩余网络及其相关内容。</p><ol class=""><li id="04d7" class="jd je hh ig b ih ii il im ip jf it jg ix jh jb ji jj jk jl bi translated">现有的神经网络有什么问题，残差网络是如何解决的？</li><li id="a242" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">什么是剩余块，它是如何工作的？</li><li id="3536" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">剩余块如何帮助。</li></ol><p id="d214" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">仍然，有一些问题或疑问？就在下面评论吧。更多更新。跟我来。</p><h1 id="0faf" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">阅读更多</h1><ul class=""><li id="309e" class="jd je hh ig b ih kp il kq ip ll it lm ix ln jb ku jj jk jl bi translated"><a class="ae jc" href="https://www.geeksforgeeks.org/introduction-to-residual-networks/" rel="noopener ugc nofollow" target="_blank">剩余网络简介</a></li><li id="9042" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ku jj jk jl bi translated"><a class="ae jc" href="https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec" rel="noopener" target="_blank">剩余积木ResNet的积木</a></li><li id="aff9" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ku jj jk jl bi translated"><a class="ae jc" href="https://www.mygreatlearning.com/blog/resnet/" rel="noopener ugc nofollow" target="_blank">Resnet或残差网络简介</a></li></ul></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><p id="b42d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lv">原载于2021年9月25日https://idiotdeveloper.com</em><em class="lv"/><a class="ae jc" href="https://idiotdeveloper.com/what-is-residual-network-or-resnet/" rel="noopener ugc nofollow" target="_blank"><em class="lv">。</em></a></p></div></div>    
</body>
</html>