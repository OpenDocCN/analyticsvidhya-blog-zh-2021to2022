<html>
<head>
<title>Naive Bayes Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯分类器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/naive-bayes-classifier-cca65df81e83?source=collection_archive---------33-----------------------#2021-04-03">https://medium.com/analytics-vidhya/naive-bayes-classifier-cca65df81e83?source=collection_archive---------33-----------------------#2021-04-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="aa76" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">一点背景</h1><p id="56ef" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">正如我们所知，朴素贝叶斯定理是一种机器学习分类器模型，它是从<strong class="je hi">贝叶斯定理</strong>中派生出来的，该定理指出:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es ka"><img src="../Images/4911a20cfc5a3e9925b6aed22a9bb0f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/0*AGskBvwBCS0w6oR8"/></div><figcaption class="ki kj et er es kk kl bd b be z dx translated"><a class="ae km" href="https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c" rel="noopener" target="_blank">来源</a></figcaption></figure><p id="8413" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">看起来很复杂，其实很简单。贝叶斯定理表明，我们可以根据B已经发生的证据找到事件A发生的概率。</p><p id="0d91" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">现在，在开始讨论如何找到它之前，让我们简单介绍一下哪一个是证据，哪一个是假设:</p><p id="98c9" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">这里有一个<strong class="je hi">假设</strong>，因为它是在证据的基础上提出的概率。</p><p id="cdcc" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">B是证据<strong class="je hi"/>,因为事件已经发生，并且与A是否会发生无关，这意味着B是预测项。</p><p id="4bc8" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">让我们考虑一个学生的例子。考虑到这一点，一个学生可以聪明也可以不聪明。如果他/她进行考试，聪明的学生可能会通过考试，也可能不会。因此，它仍然是一个假设或假说，“假定一个学生是聪明的，他/她将通过考试”，相应的替代假说也需要考虑。</p><p id="eac4" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">因此，为了找出它是否成立，我们需要借助这样一个条件概率来展示“<em class="ks">学生通过考试的概率，前提是他/她是聪明的</em>”。</p><p id="b304" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">我希望你有一点贝叶斯定律的先决条件，其中，<strong class="je hi"> P(A) </strong>表示先验概率，项<strong class="je hi"> P(B|A) </strong>表示后验概率。我们考虑在两种情况下相等的联合概率的概念，它符合我们的方程。</p><h1 id="f61d" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">为什么幼稚？</strong></h1><p id="84d2" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">每当我们谈到朴素贝叶斯定理时，这是一个我们经常遇到的问题！原因真的很明显。</p><p id="54fd" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">该定理基于特征相互独立的假设。背后的一个重要原因是因为如果一个特性隐含了另一个特性的某个值，那么预测我们需要的值就没有用了。</p><p id="0c8a" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">这就像你吃了过量的糖，它肯定会增加你的体重到一个很高的水平。这里的体重取决于糖的摄入量。所以，这自动暗示你超重了，你不需要两种特征都吃！你确实需要控制你的糖摄入量。</p><p id="0811" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">因此，这使得我们不必依赖训练数据集中的重复来进行特定的分类成为可能。因此，将结果与单个领域进行比较将是合适的，只要我们有这些单个领域在某种程度上彼此不相关。</p><p id="fbcf" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">这就是为什么有时候天真是件好事的原因！这无疑使我们在没有重复计算证据的情况下进行分类成为可能。老实说，朴素贝叶斯定理比贝叶斯定理更聪明，完全相反！</p><h1 id="a237" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">跳回到朴素贝叶斯分类器</strong></h1><ul class=""><li id="1845" class="kt ku hh je b jf jg jj jk jn kv jr kw jv kx jz ky kz la lb bi translated">让我们考虑具有<br/>特征的数据集:x1，x2，x3，x4，x5 <br/>输出:y</li><li id="d32a" class="kt ku hh je b jf lc jj ld jn le jr lf jv lg jz ky kz la lb bi translated">所以在这里，我们需要找到y的概率，即单独给定每个特定特征的输出，即P(x1|y)，P(x2|y)，等等。</li><li id="6fbc" class="kt ku hh je b jf lc jj ld jn le jr lf jv lg jz ky kz la lb bi translated">现在，我们需要找到y的概率，给定独立特征x1、x2、x3、x4和x5的某一组值。</li></ul><p id="dcb8" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">正如我们在上面看到的，它将是P(y|X ),其中X代表这5个特征的集合或元组。</p><ul class=""><li id="ac23" class="kt ku hh je b jf kn jj ko jn lh jr li jv lj jz ky kz la lb bi translated">因此，我们可以继续乘以所有5个特征的概率:P(xi|y ),这5个特征将代表我们等式中的项:P(B|A)！</li></ul><p id="8d69" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">因此在这里，P(B | A)= P(x1 | y)* P(x2 | y)* P(x3 | y)* P(x4 | y)* P(X5 | y)</p><p id="4f71" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">我们可以用圆周率记法把它变成一个术语，如下所示:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lk"><img src="../Images/0780fc84e0e18dc34bc2b84fd3846d1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/0*5aSn3uyVoLO19oNX"/></div><figcaption class="ki kj et er es kk kl bd b be z dx translated"><a class="ae km" href="https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c" rel="noopener" target="_blank">来源</a></figcaption></figure><p id="71be" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">我们现在可能遗漏了两个重要的术语:分母P(B)和分子P(A)。</p><ul class=""><li id="79c2" class="kt ku hh je b jf kn jj ko jn lh jr li jv lj jz ky kz la lb bi translated">这里的P(B)代表P(x1)*P(x2)*P(x3)*P(x4)*P(x5)所以，这里我们不想考虑。原因是这是一个常数项，不会改变。最重要的是，这里我们需要的是结果y，而不是概率P(y|X)。</li></ul><p id="00d8" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">所以，我们会根据P(y|X)的比例来考虑y，也就是说，</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es ll"><img src="../Images/5329cfaaa46e75db037cd88837630c8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*f9J7w13eS6YsGtPb"/></div><figcaption class="ki kj et er es kk kl bd b be z dx translated"><a class="ae km" href="https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c" rel="noopener" target="_blank">来源</a></figcaption></figure><p id="3674" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">现在，让我们专注于我们的目标:找到y项！</p><ul class=""><li id="5c66" class="kt ku hh je b jf kn jj ko jn lh jr li jv lj jz ky kz la lb bi translated">我们知道，y是一个分类特征。在这种情况下，让我们考虑一个二元分类问题。所以y只有两种可能的结果。因此，我们需要考虑我们获得的关于结果的特征的最高概率。</li></ul><p id="cb31" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">因此，我们简单地使用上面获得的argmax项:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es ll"><img src="../Images/724dab83a72882e0e9b384f5f70cb5d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PGxG2FUN_5kzdinW"/></div><figcaption class="ki kj et er es kk kl bd b be z dx translated"><a class="ae km" href="https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c" rel="noopener" target="_blank">来源</a></figcaption></figure><p id="3098" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">我们完成了朴素贝叶斯定理的预测模型，就在这里！</p><ul class=""><li id="219a" class="kt ku hh je b jf kn jj ko jn lh jr li jv lj jz ky kz la lb bi translated">如果你想知道如何找到P(xi|y ),可以这样想:这将是最大数量的结果，其中x=0假设y=1，x=1假设y=1。(这是针对单个特征x的，设想其他独立特征也是如此。)</li></ul><p id="4696" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">让我们跳到一个例子的代码中，让它更清楚。</p><p id="ea40" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated"><strong class="je hi">预测虹膜数据集的类别</strong></p><p id="3456" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">众所周知，Iris数据集是一个非常著名的分类数据集。因此，我们将在这里使用它。你可以在这里找到它:<a class="ae km" href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.names" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a></p><p id="d27e" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">让我们开始吧:</p><p id="e944" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">1]导入所需的库和数据集</p><pre class="kb kc kd ke fd lm ln lo lp aw lq bi"><span id="dfc3" class="lr if hh ln b fi ls lt l lu lv"><strong class="ln hi">import</strong> math<br/><strong class="ln hi">import</strong> random<br/><strong class="ln hi">import</strong> pandas <strong class="ln hi">as</strong> pd</span><span id="0a30" class="lr if hh ln b fi lw lt l lu lv"><strong class="ln hi">import</strong> numpy as np<br/>from sklearn.preprocessing <strong class="ln hi">import</strong> LabelEncoder<br/>from sklearn.model_selection <strong class="ln hi">import</strong> train_test_split</span><span id="9407" class="lr if hh ln b fi lw lt l lu lv">df = pd.read_csv(‘iris.csv’)</span></pre><p id="8c5d" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">2]编码花卉类</p><pre class="kb kc kd ke fd lm ln lo lp aw lq bi"><span id="e7d6" class="lr if hh ln b fi ls lt l lu lv">label_encoder = LabelEncoder()<br/>df[‘class’] = label_encoder.fit_transform(df[‘class’])</span></pre><p id="eb1d" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">3]成型箱</p><pre class="kb kc kd ke fd lm ln lo lp aw lq bi"><span id="fb2f" class="lr if hh ln b fi ls lt l lu lv">bins = [4.3, 5.5, 6.7, 7.9]<br/>labels = [0, 1, 2]<br/>df[‘sepal_l’] = pd.cut(df[‘sepal length in cm’], bins=bins, labels=labels)<br/>bins = [2.0, 2.8, 3.6, 4.4]<br/>df[‘sepal_w’] = pd.cut(df[‘sepal width in cm’], bins=bins, labels=labels)<br/>bins = [1.0, 3.0, 5.0, 7.0]<br/>df[‘petal_l’] = pd.cut(df[‘petal length in cm’], bins=bins, labels=labels)<br/>bins = [0.1, 0.9, 1.7, 2.5]<br/>df[‘petal_w’] = pd.cut(df[‘petal width in cm’], bins=bins, labels=labels)</span></pre><p id="4090" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">4]删除前面的列</p><pre class="kb kc kd ke fd lm ln lo lp aw lq bi"><span id="17c8" class="lr if hh ln b fi ls lt l lu lv">df = df.drop([‘sepal length <strong class="ln hi">in</strong> cm’, ‘sepal width <strong class="ln hi">in</strong> cm’, ‘petal length <strong class="ln hi">in</strong> cm’, ‘petal width <strong class="ln hi">in</strong> cm’], axis=1)</span><span id="c8bc" class="lr if hh ln b fi lw lt l lu lv">5] Replacing the missing values</span><span id="8538" class="lr if hh ln b fi lw lt l lu lv">df[‘sepal_l’].fillna(df[‘sepal_l’].mode().values[0], inplace=True)<br/>df[‘sepal_w’].fillna(df[‘sepal_w’].mode().values[0], inplace=True)<br/>df[‘petal_l’].fillna(df[‘petal_l’].mode().values[0], inplace=True)<br/>df[‘petal_w’].fillna(df[‘petal_w’].mode().values[0], inplace=True)</span></pre><p id="fc13" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">⑹列车测试数据的拆分</p><pre class="kb kc kd ke fd lm ln lo lp aw lq bi"><span id="f1af" class="lr if hh ln b fi ls lt l lu lv">X = df.drop([‘class’], axis=1)<br/>y = df[[‘class’]]<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)</span></pre><p id="e244" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">7]检查先验概率</p><pre class="kb kc kd ke fd lm ln lo lp aw lq bi"><span id="acac" class="lr if hh ln b fi ls lt l lu lv"># Check Priori Probability</span><span id="2e1f" class="lr if hh ln b fi lw lt l lu lv">prob_0 = len(<strong class="ln hi">y_train</strong>[y_train[‘class’]==0])/len(<strong class="ln hi">y_train</strong>)<br/>prob_1 = len(<strong class="ln hi">y_train</strong>[y_train[‘class’]==1])/len(<strong class="ln hi">y_train</strong>)<br/>prob_2 = len(<strong class="ln hi">y_train</strong>[y_train[‘class’]==2])/len(<strong class="ln hi">y_train</strong>)</span></pre><p id="2d10" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">8]创建组合的训练数据集并找到后验概率作为字典pxy</p><pre class="kb kc kd ke fd lm ln lo lp aw lq bi"><span id="bda5" class="lr if hh ln b fi ls lt l lu lv">X_train_trail = X_train.copy()<br/>X_train_trail[‘y’] = y_train<br/>pxy = {}<br/>pxy[‘sepal_l_0’] = [len(X_train_trail[X_train_trail[‘sepal_l’]==0])/len(X_train_trail[X_train_trail[‘y’]==0]), len(X_train_trail[X_train_trail[‘sepal_l’]==1])/len(X_train_trail[X_train_trail[‘y’]==0]), len(X_train_trail[X_train_trail[‘sepal_l’]==2])/len(X_train_trail[X_train_trail[‘y’]==0])]<br/>pxy[‘sepal_l_1’] = [len(X_train_trail[X_train_trail[‘sepal_l’]==0])/len(X_train_trail[X_train_trail[‘y’]==1]), len(X_train_trail[X_train_trail[‘sepal_l’]==1])/len(X_train_trail[X_train_trail[‘y’]==1]), len(X_train_trail[X_train_trail[‘sepal_l’]==2])/len(X_train_trail[X_train_trail[‘y’]==1])]<br/>pxy[‘sepal_l_2’] = [len(X_train_trail[X_train_trail[‘sepal_l’]==0])/len(X_train_trail[X_train_trail[‘y’]==2]), len(X_train_trail[X_train_trail[‘sepal_l’]==1])/len(X_train_trail[X_train_trail[‘y’]==2]), len(X_train_trail[X_train_trail[‘sepal_l’]==2])/len(X_train_trail[X_train_trail[‘y’]==2])]</span></pre><p id="8fde" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">这是一个示例专栏，我们需要一个接一个地为所有的特性做这件事，以防你想写更多的代码。或者，您可以对所有特性使用相同的for循环。我们形成一个字典pxy，代表我们的后验概率。</p><p id="38d4" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">它给出了输出表:</p><figure class="kb kc kd ke fd kf"><div class="bz dy l di"><div class="lx ly l"/></div></figure><p id="1209" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">9]最后，创建预测函数:</p><pre class="kb kc kd ke fd lm ln lo lp aw lq bi"><span id="a200" class="lr if hh ln b fi ls lt l lu lv">def pred(X):<br/>    sepal_l = X[0]<br/>    sepal_w = X[1]<br/>    petal_l = X[2]<br/>    petal_w = X[3]<br/>    sepal_l_col = ‘sepal_l_’+str(sepal_l)<br/>    sepal_w_col = ‘sepal_w_’+str(sepal_w)<br/>    petal_l_col = ‘petal_l_’+str(petal_l)<br/>    petal_w_col = ‘petal_w_’+str(petal_w)<br/>    prob0 = pxy[sepal_l_col][0]*pxy[sepal_w_col][0]*pxy[petal_l_col][0]*pxy[petal_w_col][0]*prob_0<br/>    prob1 = pxy[sepal_l_col][1]*pxy[sepal_w_col][1]*pxy[petal_l_col][1]*pxy[petal_w_col][1]*prob_1<br/>    prob2 = pxy[sepal_l_col][2]*pxy[sepal_w_col][2]*pxy[petal_l_col][2]*pxy[petal_w_col][2]*prob_2<br/>    x = max([prob0, prob1, prob2])<br/>    if x==prob0:<br/>        return 0<br/>    elif x==prob1:<br/>        return 1<br/>    else:<br/>        return 2</span></pre><p id="adff" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">10]然后，检查你的结果！试试你那端，让我们看看你表现如何。</p><h1 id="ca4f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">短暂的结束</strong></h1><p id="3855" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">由于我们在这里将几个连续值捆绑到类中，这被称为多项式朴素贝叶斯。</p><p id="ddd7" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">不过，这很有可能通过高斯朴素贝叶斯模型得到更准确的解决。原因是，它对连续变量有用！</p><p id="5bf7" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">我们不会讨论它是如何制定的，因为它的公式有点复杂。但是，探索它到底是什么是好的。</p><h1 id="0d83" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">结论</strong></h1><p id="88bd" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">朴素贝叶斯定理相对来说比声称的要简单，但在预测方面，它是一种非常有效的算法。这完全取决于证据在概率方面的表现。这对于分类问题以及对数据进行情感分析非常有用。尽管如此，模型的独立性标准在现实生活的数据集上经常失效，这些数据集通常包含因变量和多重共线性变量。然而，它被用于一些值得注意的领域，并且在行业中执行小规模预测时确实有很好的性能。</p></div></div>    
</body>
</html>