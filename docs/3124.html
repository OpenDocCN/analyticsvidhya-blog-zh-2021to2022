<html>
<head>
<title>All you need to know about Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于梯度下降，你需要知道的是</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/all-you-need-to-know-about-gradient-descent-f0178c19131d?source=collection_archive---------3-----------------------#2021-06-09">https://medium.com/analytics-vidhya/all-you-need-to-know-about-gradient-descent-f0178c19131d?source=collection_archive---------3-----------------------#2021-06-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/31a083c3e61a77fe671dedb0388cbfa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*ufj8HuDOC694mQ3bKCsmFw.jpeg"/></div></figure><div class="il im ez fb in io"><a href="https://www.buymeacoffee.com/mustisid13" rel="noopener  ugc nofollow" target="_blank"><div class="ip ab dw"><div class="iq ab ir cl cj is"><h2 class="bd hi fi z dy it ea eb iu ed ef hg bi translated">Mustafa Sidhpuri正在使用Flutter撰写博客和开发应用程序。</h2><div class="iv l"><h3 class="bd b fi z dy it ea eb iu ed ef dx translated">嘿，我叫穆斯塔法，是一名Flutter开发者，也是一名兼职博主。</h3></div><div class="iw l"><p class="bd b fp z dy it ea eb iu ed ef dx translated">www.buymeacoffee.com</p></div></div><div class="ix l"><div class="iy l iz ja jb ix jc ij io"/></div></div></a></div><h1 id="d7de" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">介绍</h1><p id="a8b2" class="pw-post-body-paragraph kb kc hh kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ha bi translated">你好，在这篇博客中，我将谈论梯度下降。是学习机器学习的人必须知道的基本课题之一。我会试着用一种非常简单的方式，和不同类型的梯度下降，用数学方程来解释。我们开始吧！</p><h1 id="255c" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">什么是梯度下降？</h1><p id="e6d9" class="pw-post-body-paragraph kb kc hh kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ha bi translated">梯度下降是一种通用优化算法，能够找到各种问题的最优解。梯度下降的一般思想是迭代地更新权重参数以最小化成本函数。</p><p id="be22" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">假设你在浓雾中迷失在群山之中；你只能感觉到脚下地面的坡度。快速到达谷底的一个好策略是朝着坡度最陡的方向下山。这正是梯度下降所做的:它测量误差函数关于参数向量θ的局部梯度，并且它沿着梯度下降的方向前进。一旦梯度为零，你就达到了最小值！</p><p id="43a8" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">通常，首先，我们随机初始化θ值，然后逐步最小化成本函数(例如MSE或RMSE ),直到达到全局最小值。</p><p id="0f77" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">学习率是梯度下降中最重要的参数。它决定了台阶的大小。如果学习率太小，那么算法就要经过多次迭代才能收敛，耗时较长。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es le"><img src="../Images/d343f4c1f57faac339eb44bdb31e679f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rodvIAWS-CWnLZGmsbpA3Q.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">学习率太低。</figcaption></figure><p id="b46f" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">另一方面，如果学习率太高，你可能会跳过山谷，到达另一边，甚至可能比以前更高。这可能会使算法发散，值越来越大，无法找到好的解决方案。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es lr"><img src="../Images/a58ec92e51e49401c93ea2ea6e61073d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BN-LuzkVWPI2Fm_Wq6dACg.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">学习率太高。</figcaption></figure><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ls"><img src="../Images/d2aad82e8079c59456a04b2508fa138d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f2EEsc2JXYJg4DROj2l74Q.png"/></div></div></figure><p id="c14a" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">左边是学习率太低:算法最终会达到解，但需要很长时间。在中间，学习率看起来相当不错:在短短的几次迭代中，它已经收敛到解。右边，学习率太高:算法发散，跳得到处都是，实际上每一步都离解越来越远。</p><p id="59a9" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">最后，并不是所有的成本函数看起来都像一个漂亮的普通碗。可能有洞、山脊、高原和各种不规则的地形，这使得很难找到一个全局最小值。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es lt"><img src="../Images/0f1fe8e352316cd6b8851b2a7fbd935c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sbJO1Dz-dXKFMKsyaQ8pvQ.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">梯度下降的挑战。</figcaption></figure><p id="82bb" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">上图显示了梯度下降的两个主要挑战:如果随机初始化在左侧启动算法，那么它将收敛到一个局部最小值，这不如全局最小值。如果它从右边开始，那么它将需要很长的时间来穿越平台，如果你停止得太早，你将永远不会达到全局最小值。</p><p id="4c64" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">为了实现梯度下降，我们需要计算如果我们稍微改变θj，成本函数会改变多少。这叫做偏导数。以下等式仅适用于一行中的一个特征。我们必须对每个特征的每一行进行计算。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lu"><img src="../Images/71e8cb9c984755211bc59618bd63c99c.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*Bh_K_q8r0PYILvt1ACHRmg.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">偏导数</figcaption></figure><h1 id="a015" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">梯度下降的类型</h1><h2 id="3911" class="lv je hh bd jf lw lx ly jj lz ma mb jn km mc md jr kq me mf jv ku mg mh jz mi bi translated">批量梯度下降</h2><p id="86ab" class="pw-post-body-paragraph kb kc hh kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ha bi translated">在简单梯度下降法中，我们单独计算每个实例的偏导数，并取平均值，如上式所示。批量梯度下降一次性计算这些偏导数，即我们仅计算梯度下降的一个值来更新θ，如下所示。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mj"><img src="../Images/a29f7e9d7e0fce659927c46a6ea2c2b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*QAlGxyOnrFPlKOGRVo8c6g.png"/></div></figure><p id="9552" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">请注意，每个特征都有自己的θ值，并通过梯度下降进行更新。该公式包括在每个梯度下降步骤对整个训练<br/>集合X的计算！这就是为什么该算法被称为批量梯度下降:它在每一步都使用整批训练数据。因此，它在非常大的训练集上非常慢。</p><p id="2f52" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">计算梯度下降后，是时候使用下面的等式更新θ值了。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mk"><img src="../Images/5a6aaa3bfe84fdc23429c3c8abf3c6fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*wqc5XzebPgUoY7Eprk_gHA.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">梯度下降步骤</figcaption></figure><p id="cc12" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">现在，让我们使用python实现批量梯度下降:</p><pre class="lf lg lh li fd ml mm mn mo aw mp bi"><span id="4c2c" class="lv je hh mm b fi mq mr l ms mt">import numpy as np<br/># generating random data<br/>X = 2 * np.random.rand(100, 1)<br/>y = 4 + 3 * X + np.random.randn(100, 1)</span><span id="6ca7" class="lv je hh mm b fi mu mr l ms mt">eta = 0.1 # learning rate<br/>n_iterations = 1000<br/>m = 100 # size of dataset<br/>theta = np.random.randn(2,1) # random initialization</span><span id="4a92" class="lv je hh mm b fi mu mr l ms mt">for iteration in range(n_iterations):<br/>    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) # batch GD<br/>    theta = theta - eta * gradients # step<br/>print(theta)</span><span id="e549" class="lv je hh mm b fi mu mr l ms mt"># array([[4.21509616],<br/>#       [2.77011339]])</span></pre><h2 id="2a85" class="lv je hh bd jf lw lx ly jj lz ma mb jn km mc md jr kq me mf jv ku mg mh jz mi bi translated">随机梯度下降</h2><p id="a90c" class="pw-post-body-paragraph kb kc hh kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ha bi translated">随机梯度下降(SGD)在每一步的训练集中选取一个随机实例，并仅基于该单个实例计算梯度。SGD克服了批量GD问题，即它使用整个训练集来计算每一步的梯度，这使得当训练集很大时它非常慢。</p><p id="a681" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">与批量GD相比，SGD算法的规则性要差得多，成本函数将上下跳动，仅平均下降，而不是缓慢下降到最小值。所以使用SGD我们每次都能得到好的值，而不是最优值。</p><p id="e64e" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">为了获得最佳值并减少SGD中的上下跳动，我们可以逐渐降低学习速率。步长从大开始，然后变得越来越小，允许算法停留在全局最小值。决定每次迭代学习速率的函数被称为<em class="mv">学习时间表</em>。</p><p id="4b45" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">请注意，如果我们过快地降低学习率，它可能会陷入局部最小值，甚至最终冻结在最小值的中途。如果学习率降低得太慢，它可能会在最小值附近跳很长时间，如果我们过早停止训练，最终会得到一个次优的解决方案。</p><p id="b5dd" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">让我们深入研究一下SGD的实现:</p><pre class="lf lg lh li fd ml mm mn mo aw mp bi"><span id="b202" class="lv je hh mm b fi mq mr l ms mt">n_epochs = 50</span><span id="69ec" class="lv je hh mm b fi mu mr l ms mt">t0, t1 = 5, 50 # learning schedule hyperparameters</span><span id="f91f" class="lv je hh mm b fi mu mr l ms mt">def learning_schedule(t):<br/>    return t0 / (t + t1)</span><span id="2664" class="lv je hh mm b fi mu mr l ms mt">theta = np.random.randn(2,1) # random initialization</span><span id="9fd4" class="lv je hh mm b fi mu mr l ms mt">for epoch in range(n_epochs):<br/>    for i in range(m):</span><span id="6c75" class="lv je hh mm b fi mu mr l ms mt"># getting a random instance       <br/>        random_index = np.random.randint(m)<br/>        xi = X_b[random_index:random_index+1]<br/>        yi = y[random_index:random_index+1]</span><span id="22b8" class="lv je hh mm b fi mu mr l ms mt"># calulating gradient on single instance        <br/>        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)</span><span id="3881" class="lv je hh mm b fi mu mr l ms mt"># getting learning rate         <br/>        eta = learning_schedule(epoch * m + i)</span><span id="736e" class="lv je hh mm b fi mu mr l ms mt"># updating theta        <br/>        theta = theta - eta * gradients<br/>print(theta)<br/># array([[4.21076011],<br/>#       [2.74856079]])</span></pre><div class="il im ez fb in io"><a href="https://www.buymeacoffee.com/mustisid13" rel="noopener  ugc nofollow" target="_blank"><div class="ip ab dw"><div class="iq ab ir cl cj is"><h2 class="bd hi fi z dy it ea eb iu ed ef hg bi translated">Mustafa Sidhpuri正在使用Flutter撰写博客和开发应用程序。</h2><div class="iv l"><h3 class="bd b fi z dy it ea eb iu ed ef dx translated">嘿，我叫穆斯塔法，是一名Flutter开发者，也是一名兼职博主。</h3></div><div class="iw l"><p class="bd b fp z dy it ea eb iu ed ef dx translated">www.buymeacoffee.com</p></div></div><div class="ix l"><div class="iy l iz ja jb ix jc ij io"/></div></div></a></div><h2 id="9b50" class="lv je hh bd jf lw lx ly jj lz ma mb jn km mc md jr kq me mf jv ku mg mh jz mi bi translated">小批量梯度下降</h2><p id="f06b" class="pw-post-body-paragraph kb kc hh kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ha bi translated">我们要看的最后一个梯度下降算法叫做小批量梯度下降。小批量GD在称为小批量的小随机实例集上计算梯度。与随机GD相比，小批量GD的主要优势在于，您可以从矩阵运算的硬件优化中获得性能提升，尤其是在使用GPU时。小批量GD可能面临逃离局部极小值的困难。</p><p id="0cc8" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">下图显示了三种GD算法在训练期间在参数空间中采用的路径。都是在最小值附近结束的，但是批量GD的路径实际上是停在最小值，而随机GD和迷你批量GD都是继续走来走去。但是，不要忘记，批量GD需要花费大量时间来完成每一步，如果你使用良好的学习计划，随机GD和小批量GD也会达到最小值。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mw"><img src="../Images/da46df7f99952c3b5937aae0dc1a736d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*il0AY5DKIt5wvTiO6quQTQ.png"/></div></div></figure><p id="5cee" class="pw-post-body-paragraph kb kc hh kd b ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky ha bi translated">这就是梯度下降。如果我错过了任何重要的观点，请在下面分享你的观点和评论。谢谢:)</p><div class="il im ez fb in io"><a href="https://www.buymeacoffee.com/mustisid13" rel="noopener  ugc nofollow" target="_blank"><div class="ip ab dw"><div class="iq ab ir cl cj is"><h2 class="bd hi fi z dy it ea eb iu ed ef hg bi translated">Mustafa Sidhpuri正在使用Flutter撰写博客和开发应用程序。</h2><div class="iv l"><h3 class="bd b fi z dy it ea eb iu ed ef dx translated">嘿，我叫穆斯塔法，是一名Flutter开发者，也是一名兼职博主。</h3></div><div class="iw l"><p class="bd b fp z dy it ea eb iu ed ef dx translated">www.buymeacoffee.com</p></div></div><div class="ix l"><div class="iy l iz ja jb ix jc ij io"/></div></div></a></div></div></div>    
</body>
</html>