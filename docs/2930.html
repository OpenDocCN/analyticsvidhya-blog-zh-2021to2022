<html>
<head>
<title>Visualizing CNN and Automated Bounding Boxes using Saliency Maps (Hands -on)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用显著图可视化CNN和自动包围盒(动手操作)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/automated-bounding-boxes-using-saliency-maps-ae83f4c0f071?source=collection_archive---------2-----------------------#2021-05-26">https://medium.com/analytics-vidhya/automated-bounding-boxes-using-saliency-maps-ae83f4c0f071?source=collection_archive---------2-----------------------#2021-05-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="843b" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">可解释CV和弱监督目标定位</h2></div><p id="cca2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">显著图</strong>简单来说基本上是一个显示<strong class="iy hi">输入图像像素的变化对该类</strong>输出的影响的图像。如果图像像素有很小的变化，输出类会受到什么影响，我们逐个查找每个像素的w.r.t。需要记住的重要一点是，输出类值不是softmax值，而是应用softmax之前的值。</p><p id="29f3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">包围盒</strong>主要用于物体检测。它包含图像中聚焦对象的信息<strong class="iy hi"> (x0，y0，width，height) </strong>，但是<strong class="iy hi">标记</strong>这些框是一项<strong class="iy hi"> </strong>繁琐的任务，需要大量的手工劳动。市场上很少有工具可以让你画出一个盒子，并且信息会自动存储在一个文件中，但是你仍然需要浏览所有的图像并手动完成。但是如果有一种自动的方法<strong class="iy hi"/><strong class="iy hi">来标记</strong>呢，给定图像分类模型可用于这些类别。</p><p id="4986" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">反过来，这些边界框也可以被视为模型在图像中学习到的内容或模型关注的地方。</p><p id="bf49" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我已经采取了一个<strong class="iy hi">预先训练的VGG16 imagenet模型</strong>和样本图像来解释这个概念。</p><div class="js jt ju jv fd ab cb"><figure class="jw jx jy jz ka kb kc paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><img src="../Images/eb9e0e883f3ce873d2abb2c698156f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*vYbowBqFUIOsePhVDdVbag.jpeg"/></div></figure><figure class="jw jx kj jz ka kb kc paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><img src="../Images/60c268d5edfa5aae2c80139370fcff7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*IND4deES2jOtGsAUCMb6vw.jpeg"/></div></figure><figure class="jw jx kk jz ka kb kc paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><img src="../Images/f50d9b5a25601706d5eea19fab5c1f90.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*_vJighAzH_YoUjNkKFgQHQ.jpeg"/></div><figcaption class="kl km et er es kn ko bd b be z dx kp di kq kr translated">(左)黄貂鱼-指数6 |(中)澳大利亚梗-指数193 |(右)喜鹊-指数18</figcaption></figure></div><p id="ee94" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">导入所有需要的库</strong></p><pre class="js jt ju jv fd ks kt ku kv aw kw bi"><span id="127f" class="kx ky hh kt b fi kz la l lb lc">import cv2<br/>import numpy as np<br/>from time import time<br/>import tensorflow as tf<br/>import matplotlib.pyplot as plt</span></pre><p id="49ff" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">加载异常模型并将最后一层从softmax输出转换为raw输出(在softmax之前)</strong></p><pre class="js jt ju jv fd ks kt ku kv aw kw bi"><span id="9618" class="kx ky hh kt b fi kz la l lb lc">model = tf.keras.applications.vgg16.VGG16()   ### loaded pre-trained</span><span id="0eab" class="kx ky hh kt b fi ld la l lb lc">### Get raw values (before softmax)<br/>dense_weights = model.get_weights()[-2]<br/>predictions_weights = model.get_weights()[-1]</span><span id="acfc" class="kx ky hh kt b fi ld la l lb lc">x = model.layers[-2].output<br/>x = tf.keras.layers.Dense(1000, kernel_initializer=tf.constant_initializer(dense_weights), bias_initializer=tf.constant_initializer(predictions_weights))(x)</span><span id="7b5f" class="kx ky hh kt b fi ld la l lb lc">model = tf.keras.models.Model(inputs=model.input, outputs=x)</span></pre><p id="855e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，模型被加载，现在我们必须<strong class="iy hi">写一个函数来计算显著图</strong>。显著图是为每个单独的通道(红色、绿色和蓝色)计算的，然后我们取最大值得到最终的图。</p><pre class="js jt ju jv fd ks kt ku kv aw kw bi"><span id="02cd" class="kx ky hh kt b fi kz la l lb lc">## class_number is the index of imagenet class<br/>## image_size is input size of the model<br/>## preprocess is preprocessing function</span><span id="002b" class="kx ky hh kt b fi ld la l lb lc"><strong class="kt hi">def saliencyMaps(model, img_path, class_number, image_size, preprocess):</strong><br/>    img = tf.keras.preprocessing.image.load_img(img_path, target_size=image_size)<br/>    x = tf.keras.preprocessing.image.img_to_array(img)<br/>    x = np.expand_dims(x, axis=0)<br/>    x = preprocess(x)<strong class="kt hi"><br/></strong>    x = tf.cast(x, tf.float32)<br/>    <br/>    with tf.GradientTape() as tape:<br/>        tape.watch(x)<br/>        preds = model(x)<br/>        preds = preds[0][class_number]<br/>        <br/>    grads = tape.gradient(preds, x)<strong class="kt hi"><br/>    <br/>    </strong>red = grads[0,:,:,0]<strong class="kt hi"><br/>    </strong>green = grads[0,:,:,1]<strong class="kt hi"><br/>    </strong>blue = grads[0,:,:,2]<br/>    <br/>    return img, red, green, blue</span></pre><p id="bce9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">让我们在一张图片上运行这个代码</strong></p><pre class="js jt ju jv fd ks kt ku kv aw kw bi"><span id="75c6" class="kx ky hh kt b fi kz la l lb lc">img_path =     ## path_to_img<br/>class_number = ## class_index<br/>image_size =   ## Input size of model<br/>preprocess_func = tf.keras.applications.vgg16.preprocess_input</span><span id="a43f" class="kx ky hh kt b fi ld la l lb lc">img, red, green, blue = \<strong class="kt hi"><br/> saliencyMaps(model, img_path, class_number=class_number, image_size=image_size, preprocess=preprocess_func)</strong></span><span id="959d" class="kx ky hh kt b fi ld la l lb lc">plt.figure(1, figsize=(15, 15))</span><span id="94a0" class="kx ky hh kt b fi ld la l lb lc">plt.subplot(330 + 2)<br/>plt.axis('off')<br/>plt.imshow(img)</span><span id="b920" class="kx ky hh kt b fi ld la l lb lc">plt.subplot(330 + 4)<br/>plt.axis('off')<br/>plt.imshow(red, cmap='gray')</span><span id="f034" class="kx ky hh kt b fi ld la l lb lc">plt.subplot(330 + 5)<br/>plt.axis('off')<br/>plt.imshow(green, cmap='gray')</span><span id="43a9" class="kx ky hh kt b fi ld la l lb lc">plt.subplot(330 + 6)<br/>plt.axis('off')<br/>plt.imshow(blue, cmap='gray')</span><span id="8fcd" class="kx ky hh kt b fi ld la l lb lc">plt.subplot(330 + 8)<br/>plt.axis('off')<br/>plt.imshow(<strong class="kt hi">np.maximum(np.absolute(red), np.absolute(green), np.absolute(blue))</strong>, cmap='gray')</span><span id="cdd5" class="kx ky hh kt b fi ld la l lb lc">plt.show()</span></pre><p id="b122" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">输出:</strong></p><figure class="js jt ju jv fd jx er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es le"><img src="../Images/041f60c32289c3c173e1388a7d457faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MJ3RuD3cvNzk2m9o-apctw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">(上)原始图像|(中)红色、绿色和蓝色显著性图|(下)通过最大化红色、绿色和蓝色图的最终图</figcaption></figure><p id="1d97" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下一步是找到一个自动包围盒(弱监督对象定位)。基本概念是找出图像的<strong class="iy hi">前景和背景。高于95%分位数的每个像素值都是我们的前景，低于30%分位数的所有像素值(【https://bit.ly/3wCyaNQ】)都是背景。然后我们用<strong class="iy hi"> cv2.grabCut </strong>做一个<strong class="iy hi">蒙版图像</strong>突出前景，接着找到<strong class="iy hi">最大连通分量</strong>。参考资料:<a class="ae lf" href="https://arxiv.org/pdf/1312.6034.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1312.6034.pdf</a></strong></p><pre class="js jt ju jv fd ks kt ku kv aw kw bi"><span id="bd51" class="kx ky hh kt b fi kz la l lb lc">img = cv2.imread(img_path)<br/>img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/>img = cv2.resize(img, image_size)</span><span id="0347" class="kx ky hh kt b fi ld la l lb lc">mask = np.zeros(img.shape[:2], np.uint8)<br/>mask_img = np.maximum(np.absolute(red), np.absolute(green), np.absolute(blue))</span><span id="f2ec" class="kx ky hh kt b fi ld la l lb lc">fquantile = np.quantile(mask_img.ravel(), .95)<br/>bquantile = np.quantile(mask_img.ravel(), .30)</span><span id="6289" class="kx ky hh kt b fi ld la l lb lc">mask[mask_img&gt;=fquantile] = 1<br/>mask[mask_img&lt;=bquantile] = 0<br/>mask[(mask_img&lt;fquantile) &amp; (mask_img&gt;bquantile)] = 2</span><span id="1390" class="kx ky hh kt b fi ld la l lb lc"><strong class="kt hi">bgdModel </strong>= np.zeros((1, 65),np.float64)<br/><strong class="kt hi">fgdModel </strong>= np.zeros((1, 65),np.float64)</span><span id="34c7" class="kx ky hh kt b fi ld la l lb lc">mask, bgdModel, fgdModel = <strong class="kt hi">cv2.grabCut</strong>(img, mask, None, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_MASK)</span><span id="e8be" class="kx ky hh kt b fi ld la l lb lc">mask = np.where((mask==2)|(mask==0), 0, 1).astype('uint8')</span><span id="e722" class="kx ky hh kt b fi ld la l lb lc">plt.axis('off')<br/>plt.imshow(mask, cmap='gray')</span></pre><figure class="js jt ju jv fd jx er es paragraph-image"><div class="er es lg"><img src="../Images/50cf71c4537dba515b30ec8e00ead298.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*i4RBQpzZKc_vLtez3Fcw0w.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">前景背景减法</figcaption></figure><p id="c0e1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">最大连通分量</strong></p><pre class="js jt ju jv fd ks kt ku kv aw kw bi"><span id="640d" class="kx ky hh kt b fi kz la l lb lc">new_mask = np.zeros_like(mask)<br/>for val in np.unique(mask)[1:]:<br/>    temp_mask = np.uint8(mask == val)<br/>    labels, stats = <strong class="kt hi">cv2.connectedComponentsWithStats</strong>(temp_mask, 4)[1:3]<br/>    largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])<br/>    new_mask[labels == largest_label] = val</span><span id="4ec0" class="kx ky hh kt b fi ld la l lb lc">plt.axis('off')<br/>plt.imshow(new_mask, cmap='gray')</span></pre><figure class="js jt ju jv fd jx er es paragraph-image"><div class="er es lg"><img src="../Images/2d94e71f3beafaa744d345d173245417.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*FVwgOYEl3BW1K01PUiz30g.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">最大连通分量</figcaption></figure><p id="94af" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">扩张(可选步骤)</strong></p><pre class="js jt ju jv fd ks kt ku kv aw kw bi"><span id="f252" class="kx ky hh kt b fi kz la l lb lc">kernel = np.ones((5, 5), np.uint8)<br/>dilation = <strong class="kt hi">cv2.dilate</strong>(new_mask, kernel, iterations = 10)</span><span id="bee5" class="kx ky hh kt b fi ld la l lb lc">plt.axis('off')<br/>plt.imshow(dilation, cmap='gray')</span></pre><figure class="js jt ju jv fd jx er es paragraph-image"><div class="er es lg"><img src="../Images/6d9ea972257a3b1ec0452dd0a2f0130a.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*Rj8oWJSejEr0N5N0Parfvw.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">膨胀5x5</figcaption></figure><p id="d339" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">包围盒</strong></p><pre class="js jt ju jv fd ks kt ku kv aw kw bi"><span id="caeb" class="kx ky hh kt b fi kz la l lb lc">white_pixels = np.array(np.where(dilation==1))</span><span id="90ae" class="kx ky hh kt b fi ld la l lb lc">yMin = min(white_pixels[0])<br/>yMax = max(white_pixels[0])</span><span id="ad32" class="kx ky hh kt b fi ld la l lb lc">xMin = min(white_pixels[1])<br/>xMax = max(white_pixels[1])</span><span id="1eb2" class="kx ky hh kt b fi ld la l lb lc">plt.figure(figsize=(10, 10))<br/>plt.subplot(131)<br/>plt.axis('off')<br/>plt.imshow(img)</span><span id="181d" class="kx ky hh kt b fi ld la l lb lc">plt.subplot(132)<br/>plt.axis('off')<br/>plt.imshow(cv2.rectangle(img.copy(), (xMin+1, yMin+1), (xMax, yMax), (255, 255, 0), 3))</span><span id="0bbb" class="kx ky hh kt b fi ld la l lb lc">plt.subplot(133)<br/>plt.axis('off')<br/>plt.imshow(img[yMin:yMax, xMin:xMax])</span></pre><figure class="js jt ju jv fd jx er es paragraph-image"><div class="er es lh"><img src="../Images/03c50517ade12ae2831144d5b9920c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*cotF47B1YGFVqP-E8Q0reQ.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">(左)原始图像|(中间)边界框|(右)裁剪图像</figcaption></figure><h1 id="c5cb" class="li ky hh bd lj lk ll lm ln lo lp lq lr in ls io lt iq lu ir lv it lw iu lx ly bi translated"><strong class="ak">其他结果</strong></h1><div class="js jt ju jv fd ab cb"><figure class="jw jx lz jz ka kb kc paragraph-image"><img src="../Images/2270a7c997a64741645c63195362aa00.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*xdFOhJLujnH8gg-kxS9-VQ.jpeg"/></figure><figure class="jw jx lz jz ka kb kc paragraph-image"><img src="../Images/d93f6eeb28423a8140e7c9f8d62ac92d.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*SttrudizMJFyPAWF8ENFvQ.png"/></figure><figure class="jw jx lz jz ka kb kc paragraph-image"><img src="../Images/3e273e8bbfe9093cf0157f1c6d637a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*trbBF_DZniXfS8iJ7kf82g.png"/></figure></div><div class="ab cb"><figure class="jw jx lz jz ka kb kc paragraph-image"><img src="../Images/d63d29bc33bda9591d0152f698227874.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*PPSjVRD0mDpxZHX9_Hczlw.jpeg"/></figure><figure class="jw jx lz jz ka kb kc paragraph-image"><img src="../Images/ed1ab47f1f2de560e35ac44057837102.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*jFKD6Z5uMX1i3wNQCV6cnw.png"/></figure><figure class="jw jx lz jz ka kb kc paragraph-image"><img src="../Images/37650cdd3c2709ca6585c79f013925a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*bpbb8jdeAWTAA493nkGkIw.png"/><figcaption class="kl km et er es kn ko bd b be z dx ma di mb kr translated">(左)原始图像|(中间)前景背景提取贴图|(右)边界框</figcaption></figure></div><h1 id="31b3" class="li ky hh bd lj lk ll lm ln lo lp lq lr in ls io lt iq lu ir lv it lw iu lx ly bi translated">结论</h1><p id="8dcc" class="pw-post-body-paragraph iw ix hh iy b iz mc ii jb jc md il je jf me jh ji jj mf jl jm jn mg jp jq jr ha bi translated">最终的结果似乎相当惊人。无需人工操作，就可以得到精确的边界框。膨胀是一个可选但有用的步骤，它有助于更好地增强对象。您肯定应该在数据集上尝试这种方法，然后可以直观地看到模型是否正确地预测了焦点对象。</p></div></div>    
</body>
</html>