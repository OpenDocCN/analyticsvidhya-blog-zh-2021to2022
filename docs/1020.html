<html>
<head>
<title>Processing Large Data with Dask Dataframe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Dask数据框架处理大数据</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/processing-large-data-with-dask-dataframe-42b095d73bd0?source=collection_archive---------7-----------------------#2021-02-10">https://medium.com/analytics-vidhya/processing-large-data-with-dask-dataframe-42b095d73bd0?source=collection_archive---------7-----------------------#2021-02-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="202c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在工作中，我们通常会可视化和分析非常大的数据。在典型的一天中，这相当于6500万条记录和20 GB的数据。在许多天的时间范围内分析大量的数据是一项挑战。数据的规模迫使我们在比预期更短的时间内进行分析。</p><p id="e50b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我最近发现了Dask库，因此我想为任何想开始使用这个神奇工具的人写一篇关于它的文章。</p><p id="3d1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用典型的Python数据工具包来完成ETL工作。庞大的数据量对于我们的标准工具<code class="du jd je jf jg b">numpy</code> / <code class="du jd je jf jg b">pandas</code>来说太大了。有像Spark这样的分布式计算框架来处理繁重的工作。虽然Spark可以处理这项工作，但是从Python数据工具包迁移到Spark是一个彻底的改变。</p><p id="7bd6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">达斯克来了！</p><h1 id="c5ac" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">Dask是什么？</h1><p id="1580" class="pw-post-body-paragraph if ig hi ih b ii kf ik il im kg io ip iq kh is it iu ki iw ix iy kj ja jb jc hb bi translated">Dask的设计是为了扩展<code class="du jd je jf jg b">numpy</code>和<code class="du jd je jf jg b">pandas</code>包来处理那些太大而无法保存在内存中的数据处理问题。它将较大的处理任务分解成许多较小的任务，由<code class="du jd je jf jg b">numpy</code>或<code class="du jd je jf jg b">pandas</code>处理，然后将结果重新组合成一个连贯的整体。这发生在一个无缝接口的背后，该接口被设计成模仿<code class="du jd je jf jg b">numpy</code> / <code class="du jd je jf jg b">pandas</code>接口。</p><p id="d7aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是它的一些属性:-</p><ul class=""><li id="11cb" class="kk kl hi ih b ii ij im in iq km iu kn iy ko jc kp kq kr ks bi translated"><code class="du jd je jf jg b">dask.bag</code>:一个无序集合，有效地分布式替代Python迭代器，从文本/二进制文件或任意<code class="du jd je jf jg b">Delayed</code>序列中读取</li><li id="3bdb" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated"><code class="du jd je jf jg b">dask.array</code>:具有类似<code class="du jd je jf jg b">numpy</code>界面的分布式阵列，非常适合扩展大型矩阵运算</li><li id="ea7b" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated"><code class="du jd je jf jg b">dask.dataframe</code>:类似于<code class="du jd je jf jg b">pandas</code>的分布式数据帧，用于高效处理表格化、组织化的数据</li><li id="de0a" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated"><code class="du jd je jf jg b">dask_ml</code>:围绕<code class="du jd je jf jg b">scikit-learn</code>的分布式包装器——类似机器学习工具</li></ul><p id="a199" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您有兴趣了解更多关于Dask的不同用例，请查看此链接:<a class="ae ky" href="https://stories.dask.org/en/latest/" rel="noopener ugc nofollow" target="_blank">https://stories.dask.org/en/latest/</a></p><p id="cbc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将展示一个如何用dask加载数据集的例子，在本教程中，我使用的是一个10GB大小的CSV文件，pandas在读取它时崩溃了。开始了。</p><h1 id="019e" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">安装DASK</h1><pre class="kz la lb lc fd ld jg le lf aw lg bi"><span id="9e20" class="lh ji hi jg b fi li lj l lk ll">pip install dask[complete]</span></pre><h1 id="4c21" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">加载数据集</h1><p id="631c" class="pw-post-body-paragraph if ig hi ih b ii kf ik il im kg io ip iq kh is it iu ki iw ix iy kj ja jb jc hb bi translated">因此，我们将加载一个10 GB的数据，非常容易与Dask！</p><figure class="kz la lb lc fd lm"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="3e99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于<code class="du jd je jf jg b">read_csv</code>函数需要注意的一点是，它实际上并没有将数据加载到内存中。相反，它创建了一个将数据加载到内存中所需完成的工作的<a class="ae ky" href="http://docs.dask.org/en/latest/graphs.html" rel="noopener ugc nofollow" target="_blank">任务图</a>。任务图在程序的后面部分执行。我们可以通过<code class="du jd je jf jg b">print</code>来快速浏览一下<code class="du jd je jf jg b">dask.dataframe</code>的样子:</p><pre class="kz la lb lc fd ld jg le lf aw lg bi"><span id="33c2" class="lh ji hi jg b fi li lj l lk ll">Dask DataFrame Structure: datetime open high low close volume instrument npartitions=1 object float64 float64 float64 float64 int64 object … … … … … … … Dask Name: from-delayed, 43 tasks</span></pre><p id="ea5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这一点上，我们看到<code class="du jd je jf jg b">dataframe</code>知道它将加载的数据的结构，并且已经将工作划分为任务。数据包括列Dask将加载数据的工作分成43个任务。然而，它还没有完成任何任务。</p><h1 id="85bd" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">打印数据</h1><p id="b674" class="pw-post-body-paragraph if ig hi ih b ii kf ik il im kg io ip iq kh is it iu ki iw ix iy kj ja jb jc hb bi translated">使用典型的pandas命令实际查看dask数据帧。这样，您应该能够像往常一样访问数据集。</p><pre class="kz la lb lc fd ld jg le lf aw lg bi"><span id="de70" class="lh ji hi jg b fi li lj l lk ll">print(ticker_data.head())</span></pre><figure class="kz la lb lc fd lm"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="9dd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您对dask还不太熟悉，您可以用很少的语句将数据集转换成pandas dataframe。</p><pre class="kz la lb lc fd ld jg le lf aw lg bi"><span id="fd51" class="lh ji hi jg b fi li lj l lk ll"># Converting dask dataframe into pandas dataframe<br/>result_df=df.compute()<br/>type(result_df)</span><span id="e785" class="lh ji hi jg b fi lp lj l lk ll">pandas.core.series.Series</span></pre><h1 id="4e56" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">结论</h1><p id="13ae" class="pw-post-body-paragraph if ig hi ih b ii kf ik il im kg io ip iq kh is it iu ki iw ix iy kj ja jb jc hb bi translated">Dask是将数据处理工作负载从单台机器扩展到分布式集群的绝佳选择。对于标准Python数据科学工具包的用户来说，它似乎很熟悉，并且允许向分布式处理发展。你可以在这里了解更多关于dask <a class="ae ky" href="https://examples.dask.org/dataframe.html" rel="noopener ugc nofollow" target="_blank">的信息。</a></p></div></div>    
</body>
</html>