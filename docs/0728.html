<html>
<head>
<title>Principal Component Analysis- A Brief Understanding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析——一个简单的理解</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/principal-component-analysis-a-brief-understanding-2ee133c3179c?source=collection_archive---------12-----------------------#2021-01-28">https://medium.com/analytics-vidhya/principal-component-analysis-a-brief-understanding-2ee133c3179c?source=collection_archive---------12-----------------------#2021-01-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="71ca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi jc translated">主成分分析(PCA)是一种无监督的数据降维技术。PCA背后的思想是在低维空间中寻求最精确的数据表示。换句话说，在一个较低的维度中表示数据，这样当我们沿着维度移动时，我们将沿着数据移动。高维数据中存在的不确定性的数量和变化的数量应该完全在较低的维度中被捕获。然而，在许多情况下，这可能是不可能的，因此，表示不能成为数据中的所有类型的维度。PCA试图保留数据中最大的方差。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es jl"><img src="../Images/cd637c164eeacffafa96a8cfb2e8d6f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*XcCinrKzw2jJ1HSd3Vi2sg.png"/></div></figure><p id="3ff1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">考虑变量的线性组合:</p><p id="57ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">C = w1 * y1 + w2 * y2 + w3 * y3</p><p id="e7d9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在哪里</p><p id="536b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">c:特性(组件)的综合表示</p><p id="9f3e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">w1、w2、w3:五氯苯甲醚成分含量</p><p id="de8f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y1、y2、y3:缩放特征</p><p id="e76c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据中的大部分可变性由PC1(主成分1)捕获，剩余可变性由PC2(主成分2)捕获，PC2与PC1正交(独立)。PC2试图捕获数据集中的异常变化。PC1和PC2的相关性为0。</p><p id="05c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">执行PCA的步骤:</strong></p><p id="fada" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1.数据标准化，即数据应以原点为中心。</p><p id="a6c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.生成所有维度的协方差/相关矩阵。</p><p id="a3eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">协方差/相关矩阵捕捉原始维度中不同变量之间的变化。</p><p id="d944" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.协方差/相关矩阵分解为旋转数据集的坐标轴。它确保旋转的版本捕获了其中的大部分可变性。这些旋转轴被称为特征向量，相应的特征值是捕获的方差的大小。</p><p id="0de6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.按照特征值的降序对特征对进行排序，并选择具有最大值的特征对。这是包含原始数据中最大信息量的PC1。</p><p id="17e5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">5.最后，使用Scree Plot可以看到有多少PC是有用的。PC越多，解释的方差就越多。PC越小，数据的降维和压缩就越多。</p><p id="5dfa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">我们什么时候应用PCA？:</strong></p><p id="c9fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1.减少数据的维度或特征</p><p id="490a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.基于数据特征的模式识别</p><p id="9fe5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.要解决多重共线性问题</p><p id="5135" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当自变量之间高度相关时，系数失去了稳定性和可解释性，这就是多重共线性问题。</p><p id="1043" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">考虑以下等式:</p><p id="8d1c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Y = β0 + β1 * PC1 + β2 * PC2</p><p id="accd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中PC1和PC2是独立的。因此，根据定义，PCA通过创建两个相互独立的要素来解决多重共线性问题。</p><p id="86e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">信噪比:</strong></p><p id="a7cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">捕捉可变性的沿线变化称为信号，该信号周围的一切都是噪声。噪声代表信号无法拾取的数据的不同方面。就PC1而言，这是由于随机因素造成的，但这被认为是发送给PC2的信号。</p><p id="4979" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">换句话说，PCA是从数据中提取信号的连续方法。随着我们不断从噪声中提取信号，我们不断一个接一个地提取PCA。提取的信号越多，PCA性能越好。</p><p id="16c7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">信号提取的质量称为信噪比(SNR)。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jt"><img src="../Images/8ab9bdbc60529c12702028b47fedcbc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*xl8WjQZfc4Mer0rdg2fqsA.png"/></div></div></figure><p id="8c67" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">更大的SNR意味着PCA能够从具有更少维度的数据中提取信号。</p><p id="d94f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">通过PCA改善SNP:</strong></p><p id="6385" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">重要的是，我们将方差居中，即，从和两个维度上的所有点中减去平均值这将空间的原点从(x̅，ȳ)转换为(0，0)。所以坐标系的(0，0)成为数据的中心。所以现在即使我们绕着新的坐标系旋转它，中心也不会改变，仍然是(0，0)。因此，“居中”是至关重要的，这样旋转就不会损害数值本身，并允许我们捕捉变化，减少表示的总误差。这意味着信噪比最大化。</p><p id="5a6b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">PCA的性能问题:</strong></p><p id="0d6f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1.主成分分析的有效性取决于属性的尺度。如果所考虑的属性具有不同的标度，PCA将挑选具有最高方差的变量，而不是挑选基于相关性的属性。</p><p id="efcc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.改变变量的标度，就改变了主成分分析。</p><p id="794a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.由于离散数据的存在，解释PCA可能变得具有挑战性。缩放离散数据变得困难。</p><p id="a508" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.具有长粗尾的数据中的偏斜度的存在会影响PCA的有效性。方差是标准差的平方，并且是对称的。偏斜不代表对称。因此，偏斜度影响方差的概念，从而影响主成分分析的概念。</p><p id="d37e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">5.一般来说，PCA假设属性之间是线性关系，当关系是非线性时，它是无效的。PCA有多种版本，可以捕捉非线性关系，但标准PCA不能。</p></div></div>    
</body>
</html>