<html>
<head>
<title>Solving Optimization Problems in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解决机器学习中的优化问题</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/solving-optimization-problems-in-machine-learning-4573c436bbc9?source=collection_archive---------2-----------------------#2021-11-07">https://medium.com/analytics-vidhya/solving-optimization-problems-in-machine-learning-4573c436bbc9?source=collection_archive---------2-----------------------#2021-11-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="8eb7" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">使用梯度下降和随机梯度下降算法</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/4dbbdc03176f02964fd629a9ecf5dda6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3HHFKKZJ7ULziXw_xaxh8g.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">来源:作者图片</figcaption></figure><h1 id="5411" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated"><strong class="ak">什么是优化？:</strong></h1><p id="f55a" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">优化，简单来说就是通过各种算法使成本或损失函数最小化的过程。这是通过找到算法正在优化的损失函数(或成本函数)的最小值来实现的。</p><p id="f237" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">在本文中，我们将讨论如何使用两种流行的算法优化损失函数——梯度下降和随机梯度下降。但在此之前，让我们了解一些概念，如微分，极大值和极小值。</p><h1 id="c131" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated"><strong class="ak">微分&amp;最大值/最小值:</strong></h1><p id="c86a" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">早期，当数据较少且损失函数易于求解时，我们使用微分和链式法则来优化损失函数，以解决优化问题。微分是我们在第11节数学课上学到的概念</p><blockquote class="lf lg lh"><p id="1637" class="ke kf li kg b kh la ii kj kk lb il km lj lc kp kq lk ld kt ku ll le kx ky kz ha bi translated">通常，99%的机器学习优化依赖于微分&amp;最大值/最小值</p></blockquote><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lm"><img src="../Images/3b6607aa3125e62cf09318c6b4d91f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lQ0qE8NhrRGCqltPtV1ISg.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">来源:作者图片</figcaption></figure><p id="0779" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">dy/dx是x变化时y的变化率，即x变化时y变化多少。</p><p id="9600" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">链式法则是机器学习的另一个重要法则</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lm"><img src="../Images/564ad518030d3738e50e8e3500bd8b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_-7gD_33K7FNMpHBqmxDaw.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">来源:作者图片</figcaption></figure><p id="025b" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated"><strong class="kg hi">在机器学习中，求导有助于计算丢失率wrt相对于权重变化率的变化率</strong></p><p id="7ed6" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">最大值/最小值:最大值和最小值是函数的极值，最大值是最大的，而最小值是最小的。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lm"><img src="../Images/152651ba46ed3b6670f3ae124650187c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AFAD7TUcO9SI4W5Ucorlyw.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">来源:作者图片</figcaption></figure><p id="e435" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated"><strong class="kg hi">最大值和最小值时斜率= 0</strong></p><p id="349d" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">在某些情况下，没有最大值或最小值。在某些情况下，可能有多个最小值和最大值。</p><p id="7fe4" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">可以有多个局部最小值和最大值，但只有一个全局最小值/最大值。</p><h1 id="33db" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated"><strong class="ak">需要优化算法？:</strong></h1><p id="8750" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">对于某些函数，很难求解微分，即df/dx = 0，因为数据很大或函数太复杂。所以在这里，我们将使用类似<strong class="kg hi">梯度下降的计算机算法。</strong></p><p id="b203" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">例如，解决逻辑损失方程是不容易的。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lm"><img src="../Images/ab8f1f55e6683486908927ab0dbdf1b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JiaORNyXvRY9GmRWJ65WoA.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">来源:作者图片</figcaption></figure><h1 id="ccea" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">梯度下降:</h1><p id="c0fc" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">这是一种<strong class="kg hi">迭代</strong>优化算法，很容易在现代计算机中实现。在机器学习中，梯度下降的目标是最小化损失函数。</p><p id="8642" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated"><strong class="kg hi">几何直觉:</strong></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lm"><img src="../Images/ce89b23b45483ddc0c0cfe6b6e3103c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7-qLYOEchDvW43cH5Q1NCg.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">来源:作者图片</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lm"><img src="../Images/6d717ea69c2d5be45413826d2bb185fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RZ56lmJkK-B5dxJzIje43g.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">来源:作者图片</figcaption></figure><p id="4698" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">斜率在最小值时从+ve变为-ve。其次，随着我们越来越接近最小值，即x*，斜率不断减小。</p><p id="1e47" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">在机器学习中，我们的目标是实现最大精度和最小误差，因此梯度下降通过帮助我们到达损失最小的最小值点来实现这一点。</p><p id="72c5" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">让我们看看梯度下降实际上是如何工作的？</p><ol class=""><li id="4046" class="ln lo hh kg b kh la kk lb kn lp kr lq kv lr kz ls lt lu lv bi translated">随机选取初始值Xo。</li><li id="4e80" class="ln lo hh kg b kh lw kk lx kn ly kr lz kv ma kz ls lt lu lv bi translated">计算局部最小值的公式如下:</li></ol><p id="006d" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">Xᵢ = (Xᵢ-₁)-r[df/dx)在Xᵢ-₁</p><p id="5533" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">这是更新功能</p><p id="faa9" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">3.我们会在X₀.找到X₁ = X₀-r[df/dx](r是这里的学习率，下面我们会了解。现在，让我们假设r=1。这里df/dx无非就是梯度<em class="li">。</em></p><p id="1777" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">4.现在X₁算出来会比Xo少</p><p id="a4c7" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">5.当差异(Xᵢ-Xᵢ-₁)很小时，即当xᵢ-₁、xᵢ收敛时，我们将停止迭代，并宣布X* = Xᵢ</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lm"><img src="../Images/b0e550401568ac714dd04f226c1871ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zuqLfnHaE75vVLxFrEZdHQ.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">来源:作者图片</figcaption></figure><h1 id="2c57" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">学习率:</h1><p id="83e1" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">学习率是一个超参数，必须仔细选择以达到最小点。它必须随着每次迭代而改变，否则如果保持不变，我们就会陷入振荡。我们可以在下面的图片中看到一个例子。这里，当r =1时，我们可以看到它在X = -0.5和X = 0.5之间振荡。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lm"><img src="../Images/7c8d55bd2eaf7d7746029d6ccad81370.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jhSQ-RoT325AUqcoot8hjQ.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">来源:作者图片</figcaption></figure><p id="84c8" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">处理它的技术之一是在每次迭代中减少“r”。</p><h1 id="230b" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">线性回归的梯度下降；</h1><p id="5c32" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">在线性回归中，我们的任务是找到最符合数据点的最佳直线/平面。我们通过一个数学公式找到它，这是一个优化问题。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lm"><img src="../Images/efe0a44e1b853828522002739461e711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1pPL4HxNGm5bPr8Ror_E5g.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">来源:作者图片</figcaption></figure><p id="c3a0" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">我们这里假设没有截距项xo，也没有使用正则化。</p><p id="654d" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">现在对线性回归应用梯度下降算法:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lm"><img src="../Images/243345f1171fa4ab9df81f8cc562f74b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LvssgHJCIg9gMf9xLbk3gQ.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">来源:作者图片</figcaption></figure><p id="0e93" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">这里也是一样，我们不断更新权重“w ”,直到差异太小，在那里我们得到我们的最优w*。</p><p id="d757" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">此外，我们将像上面一样在步骤3中更改“r”。</p><p id="e2a8" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">但是，当n很大时，我们面临一个问题，因为这个过程太耗时。完成一次迭代和更新w需要很多时间，此外，当n较大时，数据可能无法放入RAM，因此我们可能会遇到内存问题。</p><h1 id="79ef" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">随机梯度下降；</h1><p id="3790" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">它是机器学习中最重要的优化算法。由于梯度下降算法的缺点，将其改进为随机梯度下降算法。</p><p id="36d2" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">在SGD中，我们选择随机数量的点，而不是整个数据集。这节省了我们的时间，也避免了内存问题。</p><p id="04c8" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated"><strong class="kg hi">线性回归中的新币:</strong></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lm"><img src="../Images/fee661e0f5021d14cec7b47981c45506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9BnCsZX5QzjfsKEGqLkaMw.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">来源:作者图片</figcaption></figure><p id="8101" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">当我们从n个点中随机选择k个点时，这是无替换采样，我们将数据集分成批次，并将这些批次输入到模型中。</p><p id="3d2b" class="pw-post-body-paragraph ke kf hh kg b kh la ii kj kk lb il km kn lc kp kq kr ld kt ku kv le kx ky kz ha bi translated">唯一的区别是SGD以批处理方式加载数据和更新权重。GD一次搞定。</p><h1 id="49f5" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">结论:</h1><p id="eccb" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">总之，优化是通过各种算法最小化成本或损失函数的过程。最重要的算法是随机梯度下降法。在深度学习中，它被扩展到Adam，Adagrad，我们将在即将到来的博客中探索这一点。</p></div></div>    
</body>
</html>