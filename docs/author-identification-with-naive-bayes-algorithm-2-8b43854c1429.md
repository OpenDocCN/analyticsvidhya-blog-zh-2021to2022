# 基于朴素贝叶斯算法的作者识别——建模

> 原文：<https://medium.com/analytics-vidhya/author-identification-with-naive-bayes-algorithm-2-8b43854c1429?source=collection_archive---------16----------------------->

## 托马斯·贝叶斯能区分史蒂夫·罗杰斯和托尼·斯塔克吗？

本文是三篇系列文章中的第二篇，旨在以端到端的方式构建朴素贝叶斯分类算法。在[之前的帖子](/swlh/author-identification-using-naive-bayes-algorithm-1-abeeb88eb862)中，我们查看了这个问题的数据:我们分别从《美国队长:第一复仇者联盟》、《钢铁侠》和《复仇者联盟 4：终局之战》的剧本中搜集了《美国队长》和《钢铁侠》的对话。然后，我们对它们进行处理，删除所有动作/场景描述线索，只保留我们两个角色说的话，并创建了一个数据帧，并将它们保存为 csv 文件。

![](img/d0909179b36da26d8f8adf544b0c7a8c.png)

图片来自 Pixabay 演职员表: [vinsky2002](https://pixabay.com/users/vinsky2002-1151065/)

你可以参考[这篇文章](/swlh/author-identification-using-naive-bayes-algorithm-1-abeeb88eb862)来了解上面解释的所有操作。我们现在将集中于构建一个朴素贝叶斯分类器，并从头开始为之工作。在这篇文章中，我们将关注

1.  理解条件概率
2.  理解贝叶斯定理
3.  用于作者识别的朴素贝叶斯
4.  评估分类模型

事不宜迟，让我们开始吧！

# 理解条件概率

我们在之前的帖子里看到了对话的分布，其中大约 40%是美国队长说的，60%是钢铁侠说的。我们可以用以下方式想象同样的情况:

![](img/518bdf9f1e213060448d0fff2422ec6e.png)

图片由 Vinayak 提供

为了简单起见，我们假设有 25 个对话，其中 15 个来自钢铁侠，用红色显示，10 个来自美国队长，用蓝色显示；这是为了使图形代表我们的语料库。现在，我们可以从这张图中看出:

p(美国队长)= 10 / 25 = 2 / 5

p(钢铁侠)= 15 / 25 = 3 / 5

现在，让我们看看对话，我们发现贾维斯这个词在钢铁侠的对话中出现了 7 次，而在美国队长的对话中只出现了两次。它可以在下图中显示

![](img/09f64b1283081f16d774559912446aa5.png)

图片由 Vinayak 提供

这里黄色代表 Jarvis 这个词出现在对话中。我们也可以在维恩图中看到同样的情况。

如果我只告诉你任何给定的对话有 40%的几率是美国队长说的，有 60%的几率是钢铁侠说的，然后问你一个给定的句子是钢铁侠说的还是美国队长说的，你很可能会说钢铁侠比美国队长多 1.5 倍。

但是如果我告诉你这个句子包含单词 Jarvis，现在你就有了一条额外的信息。你从上面的句子分布知道，贾维斯在钢铁侠的对话中出现的次数比在美国队长的对话中多。更具体地说，我们看到所有包含贾维斯(7)的对话中，有 5 个是钢铁侠说的，只有 2 个是美国队长说的。这意味着你很容易猜出钢铁侠的次数是美国队长的 2.5 倍，你猜对了。

刚才发生的事情是条件概率在起作用的结果。仅仅通过提到一个句子包含 Jarvis 的事实，我们就将概率计算的语料库从 25 个减少到 7 个。其中，我们然后看到哪个角色说了多少次这个词，并计算了概率。简而言之，我们的条件是句子中有单词 Jarvis。

p(美国队长|贾维斯)= 2 / 7

p(钢铁侠|贾维斯)= 5 / 7

因此，简而言之，我们可以从文氏图中看到，以特定证据为条件减少了我们的样本空间/语料库空间，我们可以更好地估计事件的概率。我们可以正式声明

P(A|B) = P(A ∩ B) / P(B)

这在我们的例子中转化为以下两个表达式

P(美国队长|贾维斯)= P(美国队长∩贾维斯)/ P(贾维斯)

P(钢铁侠|贾维斯)= P(钢铁侠∩贾维斯)/ P(贾维斯)

# 理解贝叶斯定理

使用上述条件概率的定义，我们可以做一些代数运算来得出一个方程，这就是俗称的贝叶斯规则或贝叶斯定理。

P(A|B) = P(A ∩ B) / P(B)

P(B|A) = P(B ∩ A) / P(A)

我们知道 P(B ∩ A) = P(A ∩ B)。

所以，我们可以把方程分开，最后得到

P(A | B) / P(B | A) = P(A) / P(B)

或者重组后

> P(A | B) = P(B | A) × P(A) / P(B)

在上面的等式中，每个术语都有特定的含义:

> p(A)—A 类的先验概率
> 
> P(B) —预测值 B 的先验概率
> 
> p(B | A)-似然性，即给定类别的预测值的概率
> 
> P(A | B) —给定预测值类别的后验概率

当涉及到条件概率时，贝叶斯规则是一个非常有用的构造，因为在大多数情况下，我们知道条件概率的一种方式，但不知道另一种方式(出于许多实际原因)，这个规则帮助我们数字/现实地估计反向条件概率。

例如，在疾病诊断实验中，我们可以通过对实际上正在遭受痛苦的人进行测试来估计 P(阳性|疾病),但发现 P(阳性|疾病)实际上是无法计算的，因为测试整个人群的成本太高，而且是徒劳的；尤其是如果这种疾病不太流行的话。

在我们的情况下，规则将是

P(美国队长|贾维斯)= P(贾维斯|美国队长)× P(美国队长)/ P(贾维斯)

RHS 条件概率可以通过使用可用的文档语料库来估计，然后这可以在测试/推断时间期间用于找出哪个对话更可能是由哪个超级英雄说出的。

# 但是什么是朴素贝叶斯算法，它有什么幼稚之处？

我们已经在上面看到了贝叶斯规则或贝叶斯定理，即

P(A | B) = P(B | A) × P(A) / P(B)

在许多情况下，B 实际上不是单个事件，而是一起发生的一系列事件。例如，在我们的例子中，我们可以将一个句子分解成单个的元素/标记/量子，在英语中是单词(在其他一些语言中可能是短语甚至字符)。该公式如下所示

![](img/2f1f722bca3ca863b2f6bb93698cc272.png)

如果我们假设事件 B1，B2，B3，… Bn 是独立的，我们可以简化上面的等式并写成如下

![](img/92616f579ca7012f22bb436f083e8ee5.png)

所以，我们可以看到

![](img/c3b1dc7a6013d3fab84be1a3704eaac5.png)

上面的比例可以被用于在给定一组预测值的情况下识别最可能的类别。注意独立性假设是如何将这个复杂的问题变成非常模块化和相对简单的问题的。

在我们的例子中，我们可以取一个比率，得出这样的结果

![](img/6ec9b03096451f5919cca239fc85e401.png)

图片由 Vinayak 提供

如果比率大于 1，我们可以说叙述者比 Cap 更可能是 Stark，反之亦然。上述表示可以被简化并表示为

![](img/61e0887d1cb3a18d8d9916cfe1de9155.png)

图片由 Vinayak 提供

其中 Bi 是句子 B 中的单个标记

句子中记号/单词的独立性是朴素贝叶斯算法中的一个朴素假设；因此得名。一般来说，在任何语言中，不同的单词和它们在句子中出现的顺序都是相互依赖的。在英语中，冠词总是与名词联系在一起，这意味着如果你有一个冠词，那么接下来的单词成为名词的可能性比动词或介词的可能性更大。

尽管有天真的假设，但该算法为不同的任务提供了一个强大的基线，并且它的时间和空间复杂度远低于其他算法。这就是为什么在大多数分类问题中，数据科学家首先使用这种方法来获得基线，然后通过使用逐步复杂的方法来改进基线，如逻辑回归、随机森林、多层感知器等…

# 用于作者识别的朴素贝叶斯

在我们之前工作的基础上，我们现在将看看如何使用朴素贝叶斯建立分类模型来识别对话的叙述者。如果你还没有，读读这篇前一篇文章会帮助你跟上我们正在做的事情。

在阅读训练数据后，我们可以看到它看起来像这样:

![](img/ed79310fd0be2e145763e51e88b8e651.png)

图片由 Vinayak 提供

由于一个句子是不同单词的某种排列，我们需要首先找到一种方法来解析这些单词。为此，我们使用了所谓的标记化。让我们来看看给定句子标记的一种方法。

## 标记化

要将一个句子/文档用于任何任务，我们首先需要将其分解成各个组成部分，然后将其数字化，因为计算机只能处理数字。我们决定在特定任务中有意义的最小量，然后将句子分解成这些量。在英语中，对于我们的应用程序来说，在单词级别上分解句子是最有意义的(在其他语言中，您也可以根据相应语言的工作情况将其分解为短语/字符级别)。

我们使用正则表达式来执行标记化过程。正如我们在上面看到的，我们首先使用了`compile`方法来定义我们的表达式，然后使用这些方法来`sub`(简称)那些带有各自标记的表达式。最终，我们将所有这些小写，并在空格上进行拆分，以获得我们的令牌。

这是一个非常基本的方式来标记我们的句子；为了从一句话中提取出最多的内容，我们可以做得更好。例如，在英语中，如果你想表达当你打出相应的单词/句子时你在大喊大叫，那么用全大写字母写东西已经成为一种口语化的趋势。因此，所有大写的单词都可以在前面加上一个特殊的标记，比如 MAJ，来表示这个意思，尽管句子中的每个单词都是小写的。类似地，所有专有名词都是标题大小写的，这可以通过在标题大小写的名词前添加一个特殊的标记来捕获。瑞秋·托马斯在[的这组视频](https://www.youtube.com/playlist?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9)中对此做了很好的解释。你也可以参考[这本笔记本](https://github.com/fastai/course-nlp/blob/master/3-logreg-nb-imdb.ipynb)以及特别的代币部分来了解更多。

现在，我们将求助于上面定义的简单方法来进行标记化。我们正在捕捉标点符号，所有的单词和一些特殊的字符组合，如' nt，' ll 等。表示否定、占有等。(即使它们在技术上不是单词，它们也有不同的含义，因此将它们分开并像那样使用是有意义的。)

## 条件概率的计算

一旦我们有了代币，我们需要找出两个主要类别(美国队长和钢铁侠)的先验概率以及从他们的对话中提取的代币的条件概率。我们可以这样做

![](img/e16461eecec90002a9fd719131f7d27e.png)

图片由 Vinayak 提供

对句子进行标记后，我们找出两个类别(即钢铁侠和美国队长)中的单个标记的数量，并填充如上所示的表格。接下来，我们计算每个标记的条件概率，方法是将特定类别的标记数除以该类别的标记总数。对所有的类重复这个过程。

如果我们使用上面讨论的修正的比率公式，我们可能会遇到两个潜在的问题

## 被零除

在包含 P(word|class)等项的分母中，有可能一些概率项的值为零，就像上面的例子中我们看到的 P(need | Iron Man) = 0。

为了不遇到这样的问题，我们可以使用拉普拉斯平滑法，它将条件概率定义修改如下:

![](img/5be2b9430d6fe74828c55108a73bc22e.png)

图片由 Vinayak 提供

我们在分子和术语 vocab_size 上加了 1，vocab _ size 指的是我们语料库中的单词总数。这将确保所有概率都是正的非零数量。

## 乘法下溢

当绝对值小于 1 的数相乘时，它们的结果在数量上继续递减。如果我们有一个有很多单词的长句，这可能会发生；原则上来说，如果我们用手计算东西，这没什么大不了的，但是计算机不是这样工作的。

与数学不同，计算机是离散的和有限的；将许多小数字相乘将导致下溢，计算机无法准确计算结果。

我们知道乘积的对数是对数的和，这就是我们用来把概率乘积转换成概率对数和的方法。这有助于规避这个问题。如果我们对上面讨论的修正方程取一个对数，我们最终得到

![](img/fde30ebc594fa585e2df2b85e81ae68e.png)

图片由 Vinayak 提供

如果 RHS 评估为正数，这意味着 LHS 上的比率大于 1，这意味着对话更有可能来自 Stark 的颈静脉。

另一方面，如果 RHS 是负的，这意味着 LHS 上的比率小于 1，并且更有可能是 cap 叙述了句子 b

现在我们已经清楚了所有的术语和理论，是时候看看朴素贝叶斯算法了。

我们遵循上述概念中讨论的相同步骤，并将条目存储在字典的字典中，如下所示:

![](img/63f4fa0b5974d561a2e224f3e7932fdd.png)

图片由 Vinayak 提供

该词汇词典包含作为关键字的每个单词/词汇项，并且它映射到如上所示的所考虑的类别的条件概率和对数似然性的词典。一旦我们有了这本字典，我们就可以训练模型了。先验概率的可能性还有一个组成部分，是这本字典的一部分。它只是映射到一个表示 log(P(A) / P(B))的浮点值

在朴素贝叶斯中，我们只需要这个字典映射就可以对任何给定的句子或文本片段进行推理。下面的函数描述了我们如何使用上面的概率进行预测。

*   我们首先对句子进行标记，把它分解成独立的成分。
*   我们查找句子中每个标记的对数可能性；如果令牌不存在，我们使用一个特殊字段 xxunk，并从概率字典中找到它的对数似然性。
*   我们将所有这些概率相加得出最终得分，并加上先验概率的对数似然。
*   如果结果得分为正，我们将该对话归类为属于托尼·斯塔克，否则我们说它是由史蒂夫·罗杰斯叙述的。

一旦我们写了这个函数，我们将在相同的数据上评估它的性能，然后在复仇者联盟结局的脚本中评估它的性能。

![](img/e929dd2c903801f1297b8879dbc57423.png)

图片由 Vinayak 提供

训练和测试数据的准确率分别为 88%和 57%，两个数据集的混淆矩阵如上所示。我们可以看到算法对钢铁侠对话非常敏感，善于正确识别。这是意料之中的，因为我们有一个不平衡的数据集，更多的例子来自美国队长的对话，而较少来自钢铁侠的对话。

最后，我们现在有了朴素贝叶斯算法的工作实现。我们将存储上面显示的词汇表，并在下一篇文章中使用它进行推理，在下一篇文章中，我们将构建一个 API 端点并对其进行 dockerize 以进行部署。

我希望你能像我写这篇文章时一样喜欢阅读上面的文章。请继续关注下一篇关于使用 Flask、Flasgger 和 Dockers 部署该模型的文章。

# 参考

1.  [带分类的 NLP&向量空间— Deeplearning.ai](https://www.coursera.org/learn/classification-vector-spaces-in-nlp)
2.  [上述帖子的 Github 代码](https://github.com/ElisonSherton/rogers-stark-classification)
3.  [朴素贝叶斯&瑞切尔·托马斯的正则表达式](https://www.youtube.com/watch?v=Q1zLqfnEXdw)
4.  [上一篇关于使用朴素贝叶斯进行作者识别的数据收集的文章](/swlh/author-identification-using-naive-bayes-algorithm-1-abeeb88eb862)