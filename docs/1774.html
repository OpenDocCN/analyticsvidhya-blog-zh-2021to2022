<html>
<head>
<title>Implementing SRResnet/SRGAN Super-Resolution with Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Tensorflow实现SRResnet/SRGAN超分辨率</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementing-srresnet-srgan-super-resolution-with-tensorflow-89900d2ec9b2?source=collection_archive---------4-----------------------#2021-03-17">https://medium.com/analytics-vidhya/implementing-srresnet-srgan-super-resolution-with-tensorflow-89900d2ec9b2?source=collection_archive---------4-----------------------#2021-03-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="abe4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原文:<a class="ae jd" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank">使用生成式对抗网络的照片级单幅图像超分辨率</a></p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><h1 id="20c2" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><strong class="ak">简介</strong></h1><p id="08f3" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">上面的论文提出了一种基于残差块的神经网络来超分辨率图像，一种VGG损失来改善MSE损失，该MSE损失经常不能实施精细的SR图像生成。来自该论文的SRGAN方法还涉及利用对抗损失以及上下文损失来训练模型，以进一步提高图像重建质量。</p><p id="3434" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在之前的文章中总结了论文的概念和方法[2]。在这篇文章中，我们将实现本文提出的方法的网络架构、损耗和训练过程。本帖中使用的完整代码可以在这里查看<a class="ae jd" href="https://colab.research.google.com/drive/15MGvc5h_zkB9i97JJRoy_-qLtoPEU2sp?usp=sharing" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><h1 id="e4ed" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><strong class="ak">加载数据</strong></h1><p id="b5fa" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">论文通过著名的ImageNet图像识别数据集的裁剪来训练他们的网络。虽然在大量数据中训练模型是有益的，但数据集太重了，我决定使用<a class="ae jd" href="https://www.tensorflow.org/datasets/catalog/tf_flowers" rel="noopener ugc nofollow" target="_blank"> tf_flowers </a>数据集，它由3670张图像组成，看起来似乎太小，但对于一个玩具数据集来说，足以评估和比较论文中每种训练方法的性能。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="653b" class="kx jm hi kt b fi ky kz l la lb">data=tfds.load('tf_flowers')</span><span id="bc00" class="kx jm hi kt b fi lc kz l la lb">train_data=data['train'].skip(600)<br/>test_data=data['train'].take(600)</span></pre><p id="2ce5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用“tensorflow_datasets”模块加载tf_flowers数据集，并将前600幅图像作为验证数据集。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="c8e1" class="kx jm hi kt b fi ky kz l la lb">@tf.function<br/>def build_data(data):<br/>  cropped=tf.dtypes.cast(tf.image.random_crop(data['image'] / 255,(128,128,3)),tf.float32)<br/>  lr=tf.image.resize(cropped,(32,32))<br/>  return (lr,cropped * 2 - 1)</span><span id="b3b4" class="kx jm hi kt b fi lc kz l la lb">train_dataset_mapped = train_data.map(build_data,num_parallel_calls=tf.data.AUTOTUNE)<br/>for x in train_dataset_mapped.take(1):<br/>  plt.imshow(x[0].numpy())<br/> plt.show()</span><span id="d23d" class="kx jm hi kt b fi lc kz l la lb">  plt.imshow(bicubic_interpolate(x[0].numpy(),(128,128)))<br/>  plt.show()</span><span id="01a9" class="kx jm hi kt b fi lc kz l la lb">  plt.imshow(x[1].numpy())<br/>  plt.show()</span></pre><p id="70bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们定义一个函数，将数据集中的每幅图像映射到(128，128)裁剪及其(32，32)低分辨率副本。我们可以通过<code class="du ld le lf kt b">train_data.map(build_data, …)</code>将这个函数应用到我们的数据集。这将在每个训练时段之前执行。</p><h1 id="6adb" class="jl jm hi bd jn jo lg jq jr js lh ju jv jw li jy jz ka lj kc kd ke lk kg kh ki bi translated">模型定义</h1><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es ll"><img src="../Images/ffd9b92f19534ced6e26c44e16dfa34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cXI0XE82OhbM_g3p.png"/></div></div></figure><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="f7c6" class="kx jm hi kt b fi ky kz l la lb">def residual_block_gen(ch=64,k_s=3,st=1):<br/>  model=tf.keras.Sequential([<br/>  tf.keras.layers.Conv2D(ch,k_s,strides=(st,st),padding='same'),<br/>  tf.keras.layers.BatchNormalization(),<br/>  tf.keras.layers.LeakyReLU(),<br/>  tf.keras.layers.Conv2D(ch,k_s,strides=(st,st),padding='same'),<br/>  tf.keras.layers.BatchNormalization(),<br/>  tf.keras.layers.LeakyReLU(),<br/>  ])<br/>  return model</span><span id="e0d3" class="kx jm hi kt b fi lc kz l la lb">def Upsample_block(x, ch=256, k_s=3, st=1):<br/>  x = tf.keras.layers.Conv2D(ch,k_s, strides=(st,st),padding='same')(x)<br/>  x = tf.nn.depth_to_space(x, 2) # Subpixel pixelshuffler<br/>  x = tf.keras.layers.LeakyReLU()(x)<br/> return x</span><span id="2f6d" class="kx jm hi kt b fi lc kz l la lb">input_lr=tf.keras.layers.Input(shape=(None,None,3))<br/>input_conv=tf.keras.layers.Conv2D(64,9,padding='same')(input_lr)<br/>input_conv=tf.keras.layers.LeakyReLU()(input_conv)<br/>SRRes=input_conv</span><span id="9cd2" class="kx jm hi kt b fi lc kz l la lb">for x in range(5):<br/>  res_output=residual_block_gen()(SRRes)<br/>  SRRes=tf.keras.layers.Add()([SRRes,res_output])</span><span id="48f0" class="kx jm hi kt b fi lc kz l la lb">SRRes=tf.keras.layers.Conv2D(64,9,padding='same')(SRRes)<br/>SRRes=tf.keras.layers.BatchNormalization()(SRRes)</span><span id="dbe3" class="kx jm hi kt b fi lc kz l la lb">SRRes=tf.keras.layers.Add()([SRRes,input_conv])</span><span id="b1ef" class="kx jm hi kt b fi lc kz l la lb">SRRes=Upsample_block(SRRes)<br/>SRRes=Upsample_block(SRRes)<br/>output_sr=tf.keras.layers.Conv2D(3,9,activation='tanh',padding='same')(SRRes)</span><span id="75b6" class="kx jm hi kt b fi lc kz l la lb">SRResnet=tf.keras.models.Model(input_lr,output_sr)</span></pre><p id="7039" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用张量流定义剩余生成器架构。定义了构建整个残差块的函数，并且还实现了逐元素的sum skip连接。连接5个剩余块，并通过在<code class="du ld le lf kt b">Upsample_block</code>函数中实现的像素混洗器方法对最终图像进行上采样。该模型被构建如下。因为这个网络是完全卷积组成的，我们不必定义输入形状，因此，该模型也可以处理任何大小的图像。</p><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es lt"><img src="../Images/069d2ab6f93e6d558530c9b3d469d8fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ou7RQ-a8D2K5ska2mMJQQ.png"/></div></div></figure><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="e53d" class="kx jm hi kt b fi ky kz l la lb">def residual_block_disc(ch=64,k_s=3,st=1):<br/>  model=tf.keras.Sequential([<br/>  tf.keras.layers.Conv2D(ch,k_s,strides=(st,st),padding='same'),<br/>  tf.keras.layers.BatchNormalization(),<br/>  tf.keras.layers.LeakyReLU(),<br/>  ])<br/>  return model</span><span id="ac8f" class="kx jm hi kt b fi lc kz l la lb">input_lr=tf.keras.layers.Input(shape=(128,128,3))<br/>input_conv=tf.keras.layers.Conv2D(64,3,padding='same')(input_lr)<br/>input_conv=tf.keras.layers.LeakyReLU()(input_conv)</span><span id="0e0d" class="kx jm hi kt b fi lc kz l la lb">channel_nums=[64,128,128,256,256,512,512]<br/>stride_sizes=[2,1,2,1,2,1,2]</span><span id="a641" class="kx jm hi kt b fi lc kz l la lb">disc=input_conv</span><span id="df56" class="kx jm hi kt b fi lc kz l la lb">for x in range(7):<br/>  disc=residual_block_disc(ch=channel_nums[x],st=stride_sizes[x])(disc)</span><span id="2947" class="kx jm hi kt b fi lc kz l la lb">disc=tf.keras.layers.Flatten()(disc)<br/>disc=tf.keras.layers.Dense(1024)(disc)<br/>disc=tf.keras.layers.LeakyReLU()(disc)disc_output=tf.keras.layers.Dense(1,activation='sigmoid')(disc)</span><span id="7570" class="kx jm hi kt b fi lc kz l la lb">discriminator=tf.keras.models.Model(input_lr,disc_output)</span></pre><p id="b2be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">鉴别器架构也是基于论文的规范实现的。该网络是传统的CNN，它输入图像并判断图像的真实性。</p><h1 id="77d5" class="jl jm hi bd jn jo lg jq jr js lh ju jv jw li jy jz ka lj kc kd ke lk kg kh ki bi translated">损失实现</h1><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="0f2f" class="kx jm hi kt b fi ky kz l la lb">def PSNR(y_true,y_pred):<br/>  mse=tf.reduce_mean( (y_true - y_pred) ** 2 )<br/>  return 20 * log10(1 / (mse ** 0.5))</span><span id="7feb" class="kx jm hi kt b fi lc kz l la lb">def log10(x):<br/>  numerator = tf.math.log(x)<br/>  denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))<br/>  return numerator / denominator</span><span id="7f88" class="kx jm hi kt b fi lc kz l la lb">def pixel_MSE(y_true,y_pred):<br/>  return tf.reduce_mean( (y_true - y_pred) ** 2 )</span></pre><p id="eac9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们定义了用于训练和评估的像素级MSE损失和PSNR度量。这些损失公式在关于本文概念的前一篇文章中有更多的解释。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="200b" class="kx jm hi kt b fi ky kz l la lb">VGG19=tf.keras.applications.VGG19(weights='imagenet',include_top=False,input_shape=(128,128,3))<br/></span><span id="6cef" class="kx jm hi kt b fi lc kz l la lb">VGG_i,VGG_j=2,2</span><span id="e562" class="kx jm hi kt b fi lc kz l la lb">def VGG_loss(y_hr,y_sr,i_m=2,j_m=2):<br/>  i,j=0,0<br/>  accumulated_loss=0.0<br/>  for l in VGG19.layers:\<br/>    cl_name=l.__class__.__name__<br/>    if cl_name=='Conv2D':<br/>      j+=1<br/>    if cl_name=='MaxPooling2D':<br/>      i+=1<br/>      j=0<br/>    if i==i_m and j==j_m:<br/>      break<br/>    y_hr=l(y_hr)<br/>    y_sr=l(y_sr)<br/>    if cl_name=='Conv2D':<br/>      accumulated_loss+=tf.reduce_mean((y_hr-y_sr)**2) * 0.006<br/>  return accumulated_loss</span><span id="7cda" class="kx jm hi kt b fi lc kz l la lb">def VGG_loss_old(y_true,y_pred):<br/>  accumulated_loss=0.0<br/>  for l in VGG19.layers:<br/>    y_true=l(y_true)<br/>    y_pred=l(y_pred)<br/>    accumulated_loss+=tf.reduce_mean((y_true-y_pred)**2) * 0.006<br/>  return accumulated_loss</span></pre><p id="9711" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">论文中提出的VGG损失比较了预测图像时预训练的VGG-19网络的中间激活。我们通过VGG模型的每一层逐一向前传播，并比较每个中间输出。我们将直观的VGG损耗定义为<code class="du ld le lf kt b">VGG_loss_old</code>，精确的损耗定义为<code class="du ld le lf kt b">VGG_loss</code>。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="f180" class="kx jm hi kt b fi ky kz l la lb">cross_entropy = tf.keras.losses.BinaryCrossentropy()<br/>def discriminator_loss(real_output, fake_output):<br/>  real_loss = cross_entropy(tf.ones_like(real_output), real_output)<br/>  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)<br/>  total_loss = real_loss + fake_loss<br/>  return total_loss<br/>def generator_loss(fake_output):<br/>  return cross_entropy(tf.ones_like(fake_output), fake_output)</span></pre><p id="bd37" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对抗性损失的定义如上。与对抗训练程序相关的代码主要参考Tensorflow DCGAN教程[3]。</p><h1 id="6e81" class="jl jm hi bd jn jo lg jq jr js lh ju jv jw li jy jz ka lj kc kd ke lk kg kh ki bi translated">培养</h1><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="c21b" class="kx jm hi kt b fi ky kz l la lb">generator_optimizer=tf.keras.optimizers.Adam(0.001)<br/>discriminator_optimizer=tf.keras.optimizers.Adam(0.001)<br/>adv_ratio=0.001<br/>evaluate=['PSNR']</span><span id="394f" class="kx jm hi kt b fi lc kz l la lb"># MSE<br/>loss_func,adv_learning = pixel_MSE,False</span><span id="081c" class="kx jm hi kt b fi lc kz l la lb"># VGG2.2<br/>loss_func,adv_learning = lambda y_hr,h_sr:VGG_loss(y_hr,y_sr,i_m=2,j_m=2),False</span><span id="c84f" class="kx jm hi kt b fi lc kz l la lb"># VGG 5.4<br/>loss_func,adv_learning = lambda y_hr,h_sr:VGG_loss(y_hr,y_sr,i_m=5,j_m=4),False</span><span id="2c54" class="kx jm hi kt b fi lc kz l la lb"># SRGAN-MSE<br/>loss_func,adv_learning = pixel_MSE,True</span><span id="e592" class="kx jm hi kt b fi lc kz l la lb"># SRGAN-VGG 2.2<br/>loss_func,adv_learning = lambda y_hr,h_sr:VGG_loss(y_hr,y_sr,i_m=2,j_m=2),True</span><span id="d6e9" class="kx jm hi kt b fi lc kz l la lb"># SRGAN-VGG 5.4<br/>loss_func,adv_learning = lambda y_hr,h_sr:VGG_loss(y_hr,y_sr,i_m=5,j_m=4),True</span><span id="f5cb" class="kx jm hi kt b fi lc kz l la lb">loss_func,adv_learning = lambda y_hr,h_sr:VGG_loss(y_hr,y_sr,i_m=5,j_m=4),True</span><span id="6e00" class="kx jm hi kt b fi lc kz l la lb">#Real loss<br/>loss_func,adv_learning = pixel_MSE,False</span></pre><p id="450f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们首先为要优化的模型定义超参数和损失函数。摘录提供了论文中提出的损失的一些配置。</p><p id="3654" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练步骤基于Tensorflow DCGAN教程，训练循环可以概括所有可能的损失。对抗训练只有在<code class="du ld le lf kt b">adv_learning=True</code>的情况下才会进行。我们使用生成器模型超解析图像，用给定的度量测量损失，并记录梯度。如果下面的代码看起来过于复杂，我强烈建议看一下DCGAN教程。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="e1ca" class="kx jm hi kt b fi ky kz l la lb">@tf.function()<br/>def train_step(data,loss_func=pixel_MSE,adv_learning=True,evaluate=['PSNR'],adv_ratio=0.001):<br/>  logs={}<br/>  gen_loss,disc_loss=0,0<br/>  low_resolution,high_resolution=data<br/>  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:<br/>    super_resolution = SRResnet(low_resolution, training=True)<br/>    gen_loss=loss_func(high_resolution,super_resolution)<br/>    logs['reconstruction']=gen_loss<br/>    if adv_learning:<br/>      real_output = discriminator(high_resolution, training=True)<br/>      fake_output = discriminator(super_resolution, training=True)<br/>      <br/>      adv_loss_g = generator_loss(fake_output) * adv_ratio<br/>      gen_loss += adv_loss_g<br/>      <br/>      disc_loss = discriminator_loss(real_output, fake_output)</span><span id="760d" class="kx jm hi kt b fi lc kz l la lb">      logs['adv_g']=adv_loss_g<br/>      logs['adv_d']=disc_loss</span><span id="e5a6" class="kx jm hi kt b fi lc kz l la lb">  gradients_of_generator = gen_tape.gradient(gen_loss, SRResnet.trainable_variables)<br/>  generator_optimizer.apply_gradients(zip(gradients_of_generator, SRResnet.trainable_variables))<br/>  <br/>  if adv_learning:<br/>    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)<br/>    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))<br/>  <br/>  for x in evaluate:<br/>    if x=='PSNR':<br/>      logs[x]=PSNR(high_resolution,super_resolution)<br/>  return logs</span></pre><p id="4537" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练由以下代码执行，该代码循环数据集并为每批调用预定义的<code class="du ld le lf kt b">train_step</code>函数。如上所述，图像在每个时期之前再次被裁剪。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="ab70" class="kx jm hi kt b fi ky kz l la lb">for x in range(50):<br/>  train_dataset_mapped = train_data.map(build_data,num_parallel_calls=tf.data.AUTOTUNE).batch(128)<br/>  val_dataset_mapped = test_data.map(build_data,num_parallel_calls=tf.data.AUTOTUNE).batch(128)<br/>  for image_batch in tqdm.tqdm(train_dataset_mapped, position=0, leave=True):<br/>    logs=train_step(image_batch,loss_func,adv_learning,evaluate,adv_ratio)<br/>  for k in logs.keys():<br/>    print(k,':',logs[k],end='  ')<br/>    print()</span></pre><h1 id="6ec8" class="jl jm hi bd jn jo lg jq jr js lh ju jv jw li jy jz ka lj kc kd ke lk kg kh ki bi translated">估价</h1><p id="2551" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">我们通过训练的模型可视化一些超分辨率的示例图像。第一个图像是原始HR图像，第二个图像是SR图像，第三和第四个图像是低分辨率和双三次插值图像。虽然每个模型都没有训练足够长的时间，但我们可以比较每个模型的性能。用VGG和对抗性损失训练的模型生成的图像似乎具有更好的质量。仔细观察第一张图中重建的木材纹理。</p><p id="0f5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我没有测试出所有建议的损失。如果你能在训练更多方法后分享结果，并用我的<a class="ae jd" href="https://colab.research.google.com/drive/15MGvc5h_zkB9i97JJRoy_-qLtoPEU2sp?authuser=3#scrollTo=jBD0Rsn9FmHf" rel="noopener ugc nofollow" target="_blank"> COLAB链接</a>中提供的代码评估性能，并尝试在更大的数据集(如ImageNet数据集)上训练模型，那就太好了。此外，我确信随着训练次数的增加，这个模型会表现得更好。在训练的当前阶段，由于不成熟的ESPCN重建层，我们可以在重建图像中看到人工滤波器。这可以通过更多的训练迭代来解决，尽管该模型在感知上仍然优于基于MSE的模型。</p><figure class="ko kp kq kr fd lm er es paragraph-image"><div class="er es lu"><img src="../Images/991204187284f0018c0c02c0c9833213.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*t-Hg--ZW8QLLqkmblf2QgQ.png"/></div></figure><ul class=""><li id="6f5e" class="lv lw hi ih b ii ij im in iq lx iu ly iy lz jc ma mb mc md bi translated">SRResNet + MSE</li></ul><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/d04c58b0f0510dadae7bb3d7b603a836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BIC5bPgPXEqQCJoYCy7Kqw.png"/></div></div></figure><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/60e86193154d7a1c938121753d584f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O34Hl_AYg-YWj0TgQ4el9g.png"/></div></div></figure><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/0be7f159446b96af5c1e162fccacc40c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ls6SxhBfp560rGdap-ZQA.png"/></div></div></figure><ul class=""><li id="c364" class="lv lw hi ih b ii ij im in iq lx iu ly iy lz jc ma mb mc md bi translated">SRResNet + VGG 2.2</li></ul><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/0a00646c0cb85e2d40832ffda0a6630d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OkL6J9zYRIKhV369cVDloQ.png"/></div></div></figure><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/3980887cb599bf92c8b84eec0c868760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*57HAVtjeDeHH2MSL6TeBSQ.png"/></div></div></figure><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/8f1c8ff3e19590b540593997b53f2d3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-mH5u0MlXCuuLlDv0WyUzw.png"/></div></div></figure><ul class=""><li id="4609" class="lv lw hi ih b ii ij im in iq lx iu ly iy lz jc ma mb mc md bi translated">SRResNet + VGG 5.4</li><li id="2c99" class="lv lw hi ih b ii mf im mg iq mh iu mi iy mj jc ma mb mc md bi translated">SRGAN 0.001 + MSE</li></ul><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/81c908b87ed49b05dc05d667f6d1c42d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QWfrWcaVJMlYNWVfU1q6rA.png"/></div></div></figure><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/b1b776a21f71627c53de467400fed57e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tHcTtcEBYroV52GDwioGYw.png"/></div></div></figure><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/76287243ea8614700999a636abdfbc6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Y30MQ6CJFN7k7_XvHbvyA.png"/></div></div></figure><ul class=""><li id="2b05" class="lv lw hi ih b ii ij im in iq lx iu ly iy lz jc ma mb mc md bi translated">斯尔甘0.001 + VGG 2.2</li><li id="1b73" class="lv lw hi ih b ii mf im mg iq mh iu mi iy mj jc ma mb mc md bi translated">斯尔甘0.001 + VGG 5.4</li></ul><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/cd45b7c145756a2ba52e33ef961d8b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2dZkSHQRmq9Ww9uXBA2s_w.png"/></div></div></figure><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/432d5be364577461e3b270cf11d3ae88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aZWkF5JgHYxMDuleDDg24w.png"/></div></div></figure><figure class="ko kp kq kr fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/df1bf03cd66fdf576ff88182fe7af8f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FJDa3b5Vo5xLCe2wagKhZw.png"/></div></div></figure><h1 id="0a95" class="jl jm hi bd jn jo lg jq jr js lh ju jv jw li jy jz ka lj kc kd ke lk kg kh ki bi translated">参考</h1><p id="58b8" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">我的COLAB实现SRResnet/SRGAN:<a class="ae jd" href="https://colab.research.google.com/drive/15MGvc5h_zkB9i97JJRoy_-qLtoPEU2sp?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://COLAB . research . Google . com/drive/15 mgvc 5h _ zkb 9i 97 jjroy _-qltopeu2 sp？usp =共享</a></p><p id="cf8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[1] Ledig，Christian等，“使用生成式对抗网络的照片级单幅图像超分辨率”<em class="mk">IEEE计算机视觉和模式识别会议论文集</em>。2017.</p><p id="10ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]采用SRResnet、SRGAN的超分辨率。<a class="ae jd" rel="noopener" href="/analytics-vidhya/super-resolution-with-srresnet-srgan-2859b87c9c7f">https://medium . com/analytics-vid hya/super-resolution-with-srresnet-srgan-2859 b 87 c 9 c 7 f</a></p><p id="82d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3] Tensorflow DCGAN教程:<a class="ae jd" href="https://www.tensorflow.org/tutorials/generative/dcgan" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/generative/dcgan</a></p></div></div>    
</body>
</html>