<html>
<head>
<title>Understanding the building blocks of transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解变压器的构建模块</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-the-building-blocks-of-transformers-c28484788d5a?source=collection_archive---------0-----------------------#2021-08-17">https://medium.com/analytics-vidhya/understanding-the-building-blocks-of-transformers-c28484788d5a?source=collection_archive---------0-----------------------#2021-08-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="35c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">变压器作为解决NLP问题的一种架构选择已经变得无处不在。本文将介绍一些从零开始理解变压器如何工作所需的基本背景知识。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/71e29d18dff43303d2adb7125b8c36fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*jFSeBSZ2WOTl-cUM2PfAow.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><a class="ae jo" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">“你所需要的只是关注”</a>中提出的变压器架构</figcaption></figure><h1 id="97ec" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">以前的建筑:</strong></h1><p id="128c" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">为了理解为什么有人会有这样的想法，我们可以回顾一下变形金刚出现之前的一些架构。</p><h2 id="e604" class="ks jq hh bd jr kt ku kv jv kw kx ky jz ip kz la kd it lb lc kh ix ld le kl lf bi translated"><strong class="ak">编解码RNN </strong></h2><p id="6a7f" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">在2014年，题为“<a class="ae jo" href="https://arxiv.org/pdf/1409.3215.pdf" rel="noopener ugc nofollow" target="_blank">序列到序列学习与神经网络</a>”的论文介绍了“编码器-解码器”结构，该结构具有RNN来编码序列以获得<strong class="ig hi">固定向量长度表示</strong>(以英语为例)。类似地，RNN解码器会接收固定的向量表示，并将其逐字转换成法语。这很有用，因为两个序列都可以是<strong class="ig hi">可变长度</strong>，这在语言翻译过程中很常见。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lg"><img src="../Images/2a05c3c199c76a1948bbd4d887229774.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/0*nKosSEf9_6Ramb5x.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">用于NMT的编码器-解码器结构的例子</figcaption></figure><p id="6570" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上面的例子中，我们可以看到“RNN的真正摇滚”是如何被转换成法语的。<strong class="ig hi">最后一个“隐藏状态”作为句子整个上下文的固定编码</strong>。然后，解码器获取该上下文，即开始标记，并预测timestep_0的字分布。那么同一个单词(上图中的“Les”)将作为下一个时间步长的输入。</p><p id="c303" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们已经可以尝试找出一些可以尝试和改善这一点的调整。</p><p id="b54d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为什么我们应该将整个输入句子<strong class="ig hi">表示为一个固定的表示？</strong>当然，如果有一个很长的句子，<strong class="ig hi">一个固定的向量不能很好地处理整个上下文</strong>。这就是注意力集中的RNNs带着论文<a class="ae jo" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">“通过联合学习对齐进行神经机器翻译&amp;翻译</a>”进来的地方。</p><h2 id="d267" class="ks jq hh bd jr kt ku kv jv kw kx ky jz ip kz la kd it lb lc kh ix ld le kl lf bi translated"><strong class="ak">编解码RNNs关注:</strong></h2><p id="6425" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">这里<strong class="ig hi">不是使用固定的上下文表示，而是通过搜索输入句子在每个时间步</strong>计算上下文向量。因此，有效地，给定解码器的隐藏状态，模型决定输入序列的哪一部分对它翻译(或生成)当前单词是重要的。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lh"><img src="../Images/e29ea599f3ab4296626044ca9d2e6d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*ZAcpXcpZc-igGEYh2nDMqA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">注意力集中的编码器-解码器</figcaption></figure><p id="e28d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们试着弄清楚上图是怎么回事。我们的输入序列由单词(X1，X2 …XT)给出，其中每个单词通过双向RNN馈入，其中h1，h2..是隐藏的状态。在没有注意的情况下，我们将向量<strong class="ig hi"> h_T </strong>作为我们的上下文向量。但是在这种情况下，我们通过一些权重α对我们所有的隐藏状态和解码器<strong class="ig hi"> S_t-1 </strong>的先前状态进行线性组合，以预测<strong class="ig hi"> S_T </strong>以及随后我们当前时间步长的翻译后的字<strong class="ig hi"> Y_t </strong>。</p><p id="da0e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">针对单个预测的注意力模型的步骤如下:</p><ol class=""><li id="ec18" class="li lj hh ig b ih ii il im ip lk it ll ix lm jb ln lo lp lq bi translated"><strong class="ig hi">编码器&amp;解码器</strong>:编码器保持不变。给定时间步长t处的输入，新的隐藏状态被计算为输入和先前隐藏状态的函数。解码器曾经有一个固定的上下文向量c，<strong class="ig hi">，但是现在在解码器的每个时间步长I，我们使用不同的向量</strong>。</li></ol><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/0c4f0c0884db922915dbb0fbdf14434c.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/0*PclImV37UoHncNYZ.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">解码器</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ls"><img src="../Images/fa8e1ce4a6cb83a37ed8398d9f0f7f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/0*8yaLH0_a7E8Z72XG.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">编码器</figcaption></figure><p id="706d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.<strong class="ig hi">相似度得分:</strong>我们如何知道各种隐藏状态与预测解码器中当前单词的相关程度？我们计算所有隐藏状态和先前解码器状态之间的<strong class="ig hi">对准分数</strong>。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lt"><img src="../Images/182c19945bef5e5240f34f8411326c8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/0*c5MV4delMQ2Xl2Lk.png"/></div></figure><p id="70ca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里<strong class="ig hi"> e </strong>是每个先前的解码器隐藏状态和编码器隐藏状态之间的相似性分数。F_att可以是MLP，甚至是点积，这将有助于确定分数。其想法是，<em class="lu"> t </em> <strong class="ig hi"> <em class="lu">他的分数越高，隐藏状态(以及因此的输入)对当前翻译的单词</em> </strong>的贡献就越大。</p><p id="ae4e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.<strong class="ig hi">注意力权重——转换成概率:</strong>我们简单地通过softmax运行所有相似性得分，将所有得分转换成概率分布。根据上图，这些是我们的权重α。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lv"><img src="../Images/1f62535c47b52af15bb224207e99254f.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/0*lpGFWpuShxa7qEXQ.png"/></div></figure><p id="c775" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.<strong class="ig hi">计算上下文向量:</strong>一旦我们获得了与编码器的每个隐藏状态相关联的这些概率，我们计算上下文向量如下</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lw"><img src="../Images/f84b3796bd73369c89e6c02bdd7626a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/0*NPvkELODDNEz20M2.png"/></div></figure><p id="37a6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">5.<strong class="ig hi">使用<strong class="ig hi">上下文向量C_i </strong>、先前预测的单词<strong class="ig hi"> Y_(i-1) </strong>和先前解码器隐藏状态<strong class="ig hi"> s_(i-1) </strong>来预测翻译后的世界</strong></p><pre class="jd je jf jg fd lx ly lz ma aw mb bi"><span id="896e" class="ks jq hh ly b fi mc md l me mf">Example: English to French</span><span id="8f80" class="ks jq hh ly b fi mg md l me mf">Source Sentence: "I am eating pizza"</span><span id="930e" class="ks jq hh ly b fi mg md l me mf">Translated sentence: "je mange des pizzas"</span><span id="0128" class="ks jq hh ly b fi mg md l me mf">The intuition would be that while predicting the word "eating" the similarity score of "mange" would be higher.</span></pre><p id="5d17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最令人惊讶的是，这不是一个受监督的过程。我们没有明确地告诉模型关注序列中的特定部分。由于整个函数是可微的，在回推期间，模型<strong class="ig hi">自动收敛到序列的最佳部分，以关注</strong>。</p><h2 id="889a" class="ks jq hh bd jr kt ku kv jv kw kx ky jz ip kz la kd it lb lc kh ix ld le kl lf bi translated"><strong class="ak">使用RNNs的缺点:</strong></h2><p id="6138" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">不知何故，我们想摆脱顺序计算。对于RNNs，每个隐藏状态都依赖于前一个状态。借助现代GPU，我们希望通过一次巨大的矩阵乘法一次性处理整个输入序列。此外，我们还必须确保能够处理更长的序列。</p><h2 id="2df5" class="ks jq hh bd jr kt ku kv jv kw kx ky jz ip kz la kd it lb lc kh ix ld le kl lf bi translated"><strong class="ak">概括#1 —单个查询向量</strong></h2><ol class=""><li id="1f82" class="li lj hh ig b ih kn il ko ip mh it mi ix mj jb ln lo lp lq bi translated">我们有i <strong class="ig hi"> nput向量X </strong>(形状:Nx，Dq)，它可以被认为是前面例子中编码器的隐藏状态。</li><li id="2095" class="li lj hh ig b ih mk il ml ip mm it mn ix mo jb ln lo lp lq bi translated">我们引入了<strong class="ig hi">查询向量q </strong> (Shape: Dq)，它可以被认为类似于我们用来计算相似性得分的先前解码器状态。实际上，这可以看作是我们搜索最相似输入的查询。</li><li id="a264" class="li lj hh ig b ih mk il ml ip mm it mn ix mo jb ln lo lp lq bi translated">作为我们的相似性函数，我们使用一个<strong class="ig hi">缩放的点积</strong>，其中我们将查询向量与输入相乘，该输入由查询向量<strong class="ig hi">的维度的平方缩放。</strong>X中第I个输入的得分如下所示:</li></ol><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mp"><img src="../Images/af9fd2f407915a28cbf33bd762ba07bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/0*kP3kIOtYhBUcLIFo.png"/></div></figure><pre class="jd je jf jg fd lx ly lz ma aw mb bi"><span id="fdc3" class="ks jq hh ly b fi mc md l me mf">Side note: Why do we use the square root?</span><span id="fc41" class="ks jq hh ly b fi mg md l me mf">A higher dot product would cause a higher softmax for that input, and a very low probability for all others. This causes very low gradients during back-propagation. Hence a scaling factor, the square root of the query vector dimension is used to prevent this.</span></pre><h2 id="2ddb" class="ks jq hh bd jr kt ku kv jv kw kx ky jz ip kz la kd it lb lc kh ix ld le kl lf bi translated"><strong class="ak">泛化#2多个查询向量&amp;关注层</strong></h2><ol class=""><li id="cb10" class="li lj hh ig b ih kn il ko ip mh it mi ix mj jb ln lo lp lq bi translated">我们可以有Nq个查询向量，并用矩阵乘法计算相同的相似性得分，而不是只有一个查询向量q。所以现在我们有了<strong class="ig hi"> Q </strong>(形状:Nq，Dq)</li><li id="cffd" class="li lj hh ig b ih mk il ml ip mm it mn ix mo jb ln lo lp lq bi translated"><strong class="ig hi">输入向量X </strong>(形状:Nx，Dq)</li></ol><p id="79b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">计算:</strong></p><ol class=""><li id="d2aa" class="li lj hh ig b ih ii il im ip lk it ll ix lm jb ln lo lp lq bi translated"><strong class="ig hi">相似性:</strong>我们用一次矩阵乘法来计算相似性</li></ol><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mq"><img src="../Images/b61860041040d3e98e5213c5038560bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/0*-wD1GZPvR-wgi4Pg.png"/></div></figure><p id="1a5e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该矩阵的每个元素由下式给出:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mr"><img src="../Images/606dccbfc57cafd7b0327dd83e4cb6b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/0*LHeaSsonjAGTDDpS.png"/></div></figure><p id="cc01" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其本质上是第<strong class="ig hi">个查询向量</strong> r和第j <strong class="ig hi">个输入</strong>之间的点积</p><p id="1098" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.<strong class="ig hi">注意力权重:</strong>正如我们在前面的例子中所做的，我们通过softmax运行我们的矩阵E，以获得每个查询向量的所有输入的概率分布。</p><p id="ef16" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> A= Softmax(E，dim=1) </strong></p><p id="e3bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于3个查询向量和3个输入向量</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ms"><img src="../Images/79267c3153aab795c97dc6c7d61a3eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/0*wcDpB24g_DsRIwKH.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">相似矩阵</figcaption></figure><p id="705a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">归一化后，该矩阵的每一列的总和将为1</p><p id="6c54" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.<strong class="ig hi">输出向量:</strong>我们将输出向量定义为注意力权重的线性组合，类似于我们在前面的例子中如何找到上下文向量。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mt"><img src="../Images/b0378b748c56370e7a391596118724dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/0*b_Mn_Sl8kQ1g-0o5.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mu"><img src="../Images/6301f23717a8a87374f08981534df599.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/0*eanVDaxEH3LD_Mzl.png"/></div></figure><p id="46a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">输出的每个元素由关注矩阵的列与输入的线性组合给出。</p><p id="a3ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们差不多有了<strong class="ig hi"> <em class="lu">一个自给自足的层</em> </strong>，有输入(X1，X2…)和输出(Y1，Y2)...).</p><h2 id="2f34" class="ks jq hh bd jr kt ku kv jv kw kx ky jz ip kz la kd it lb lc kh ix ld le kl lf bi translated"><strong class="ak">概括#3键和值矩阵</strong></h2><p id="6c72" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">在之前的归纳中，我们使用了两次输入向量，一次是计算相似度，另一次是计算输出。</p><p id="a01d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了区分这两个任务，我们创建了两个可学习的矩阵:</p><p id="2a09" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">键矩阵&amp;值矩阵:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/751a2c5b8bdef0f773489fe52252b6a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/0*hvWxS0M0ET6UuiwY.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">关键矩阵</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/bfbcde2396a1dcafcac5b847a5b79a98.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/0*PaaX1kHKNYPb27xI.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">价值矩阵</figcaption></figure><p id="1c73" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个想法是将我们的输入转换到两个空间，一个是当我们想要计算我们与查询的相似性时。另一个是当我们检索与查询相关的值时。</p><p id="0986" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">所有涉及的计算:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mv"><img src="../Images/8294135bc51a14d18a327635c44a2649.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/0*V6NrlRgTWII1RrBP.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">关键向量</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mw"><img src="../Images/185974f4321726e68c5efe9e22b88498.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/0*fazC4VFjAfVy73U5.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">价值向量</figcaption></figure><p id="515e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意我们是如何输入两次来计算键和值向量的。现在，我们必须计算我们与键的相似性和与值的输出。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mx"><img src="../Images/a9d3f55d130409a2e006c03d84bb39d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/0*d7oAPyCq8lpbPa2E.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">查询和关键字之间的相似性矩阵</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es my"><img src="../Images/e8a942027607ac7c5216bf9a125c54ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/0*HdMpHrzme7nLa0hn.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">注意力权重</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mz"><img src="../Images/e57f08ac767b7a88da762f1c570e8181.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/0*kGyZXLcx69NGUguF.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">以值的线性组合形式输出</figcaption></figure><p id="a995" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">就是它，这就是我们的“<strong class="ig hi">注意力层</strong>”，我们可以用它来比较我们的查询和输入！</p><h2 id="6233" class="ks jq hh bd jr kt ku kv jv kw kx ky jz ip kz la kd it lb lc kh ix ld le kl lf bi translated"><strong class="ak">泛化#4自我关注</strong></h2><p id="025a" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">我们正在慢慢进入这些变形金刚的一个组成部分。在前面的概括中，我们有单独的查询向量。但是如果你必须在一个序列中找出每个单词所指的是什么呢？或者换句话说，给定一个单词，我们想要计算出在序列的其余部分的注意力权重。比如你有一句“我过了河就到了银行”之类的话。<strong class="ig hi">银行是指河岸还是真正的银行？为了表示“银行”，我们需要整个句子的上下文。与RNNs不同的是，自我关注机制正是以并行方式做到这一点的。这是变压器的主要部分。对于输入-输出依赖，我们使用自我关注，而不是递归或卷积。</strong></p><p id="c7bf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">transformer架构以3种不同的方式使用自我关注。让我们看一下在论文演示中给出的例子。输入的句子是“动物没有过马路是因为它太累了”:</p><ol class=""><li id="9965" class="li lj hh ig b ih ii il im ip lk it ll ix lm jb ln lo lp lq bi translated">Input-Input:输入中的所有单词相互照应。</li></ol><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es na"><img src="../Images/9c8573207be9cfc218ed6e0a12391e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*4Y3kupnOz5jh79whtI5xFA.png"/></div></figure><p id="26a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.输入-输出:就像我们的RNN编码器-解码器结构一样，在翻译过程中，我们希望知道输入序列中的哪些单词对应于翻译。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nb"><img src="../Images/2c112950a6afde518e28da3b13bcc58b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*25ZJCBuYKDpfqS7BWyhPxg.png"/></div></figure><p id="fa5c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3)输出-输出:我想在相应的图中显示权重，而不是连接。你注意到什么不同了吗？</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nc"><img src="../Images/ed6054fbe546f56d850af57fdf290c29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*qo0HHSA_83O1Ww2dfnhe8A.png"/></div></figure><p id="026d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在翻译的输出中，好像网络没有在翻译的序列中向前看。这是合乎逻辑的，因为在翻译过程中，你不希望你的网络已经注意到它还没有翻译的单词。在上面的例子中，我们看到了在“die”之前的主动权重词。这是一个很好的技巧，当你把相似性矩阵推到一个大的负数时，那么post softmax那些概率会变成0。它被称为<strong class="ig hi">被掩盖的自我关注。</strong></p><p id="eb19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">自我关注的计算:</strong></p><ol class=""><li id="52cb" class="li lj hh ig b ih ii il im ip lk it ll ix lm jb ln lo lp lq bi translated">输入向量X(形状:Nx，Dx)</li><li id="07be" class="li lj hh ig b ih mk il ml ip mm it mn ix mo jb ln lo lp lq bi translated">关键矩阵:W_k(形状:Dx，Dq)</li><li id="7e8d" class="li lj hh ig b ih mk il ml ip mm it mn ix mo jb ln lo lp lq bi translated">价值矩阵:W_v(形状:Dx，Dv)</li><li id="5504" class="li lj hh ig b ih mk il ml ip mm it mn ix mo jb ln lo lp lq bi translated">查询矩阵:W_q ( Shape: Dx，Dq):唯一新的可学习矩阵，它将我们的输入转换为查询</li></ol><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mx"><img src="../Images/e15998e9c12d81e520ebb609f762c140.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/0*la9O2mfNpKaTr6qx.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">查询向量</figcaption></figure><p id="e7bc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是唯一的区别，我们发现键、值、相似性和输出与归纳#3类似。</p><p id="eda1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">自我注意是排列同变:</strong></p><p id="4cba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这意味着输入中的任何排列都会产生完全相同的输出，但排列方式相似。这意味着如果你改变句子的顺序，模型不会真的知道。这就是作者在输入中嵌入<strong class="ig hi">额外位置编码的原因。</strong></p><h2 id="92b6" class="ks jq hh bd jr kt ku kv jv kw kx ky jz ip kz la kd it lb lc kh ix ld le kl lf bi translated">概括#5多头自我关注</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nd"><img src="../Images/2ccecb8fded1b0c21071f2502973447a.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*zbV4VMYEofZ26Akzr25mMA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">论文中描述的多头自我关注</figcaption></figure><p id="9ac9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将这种计算重复8次，而不是只有一个自我关注模块。不同组的键、值和查询矩阵用于转换我们的输入。我们还降低了每个头部的维度，以保持它在计算上类似于单个注意力头部。例如，如果我们先前使用512作为我们的模型尺寸，并且如果我们必须分成8个头。我们将使用64作为每个键、值和查询矩阵的维度。最后，当我们连接所有这8个头时，我们将得到一个输出维度(*Nx，64x8)。但是我们的输入是(*Nx，512)的，应该和我们的输出大小一样。因此需要另一个形状为(64x8，512)的矩阵<strong class="ig hi"> W_o </strong>，将我们注意力的输出转换为(*Nx，512)。</p><pre class="jd je jf jg fd lx ly lz ma aw mb bi"><span id="3200" class="ks jq hh ly b fi mc md l me mf"><strong class="ly hi">Example:</strong></span><span id="74bf" class="ks jq hh ly b fi mg md l me mf">We have a sentence which needs to be translated:</span><span id="8bdf" class="ks jq hh ly b fi mg md l me mf">"The boy plays on the field" </span><span id="3f3c" class="ks jq hh ly b fi mg md l me mf">We assume an embedding dimension of 512 for each word.</span><span id="0ef6" class="ks jq hh ly b fi mg md l me mf">Therefore our input X is of size (5,512)</span><span id="e0f8" class="ks jq hh ly b fi mg md l me mf">Now, let's assume 8 attention heads and an query dim of 64.</span><span id="cb96" class="ks jq hh ly b fi mg md l me mf">Therefore each of our matrices (key,query&amp;value) would be of size (512,64).</span><span id="8dab" class="ks jq hh ly b fi mg md l me mf">Post applying the self-attention mechnaism each of these 8 heads would give us an output of (5,64).</span><span id="61d7" class="ks jq hh ly b fi mg md l me mf">Now if we concatenate 8 of these matrices we would get one giant matrix of size (5,64*8) which would finally be transformed by the W_0 matrix to the same size as our input vector (5,512)</span></pre><h2 id="5713" class="ks jq hh bd jr kt ku kv jv kw kx ky jz ip kz la kd it lb lc kh ix ld le kl lf bi translated"><strong class="ak">其他图层</strong></h2><p id="da16" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">贴出这个自我关注层，就有了层正常化之后的全连通网络。此外，在所有这些层之间的输入和输出之间存在剩余连接。注意力标准化前馈网络的同一块被重复。它们被用于编码器和解码器。我打算在另一篇文章中用一个实际的编码示例更深入地研究它们。</p></div></div>    
</body>
</html>