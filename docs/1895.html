<html>
<head>
<title>All About Linear Regression Analysis In Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的线性回归分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/all-about-linear-regression-analysis-in-machine-learning-b64677d9d7fc?source=collection_archive---------9-----------------------#2021-03-24">https://medium.com/analytics-vidhya/all-about-linear-regression-analysis-in-machine-learning-b64677d9d7fc?source=collection_archive---------9-----------------------#2021-03-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="dc8a" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">使用Scikit-learn及其背后的数学知识构建您自己的线性回归模型</h2></div><figure class="iw ix iy iz fd ja"><div class="bz dy l di"><div class="jb jc l"/></div><figcaption class="jd je et er es jf jg bd b be z dx translated">只是另一个数学GIF</figcaption></figure><p id="715f" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">我们先从最基本也最容易理解的回归说起:线性回归。更简单地说，线性回归试图通过将线性方程拟合到观察到的数据来模拟两个变量之间的关系。</p><p id="501f" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">更一般地，线性回归模型通过简单地计算输入要素+常数的加权和来进行预测，该加权和称为偏差项(也称为截距项)。</p></div><div class="ab cl kd ke go kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="ha hb hc hd he"><p id="cb65" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi kk translated">数学上:</p><figure class="iw ix iy iz fd ja er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es kt"><img src="../Images/8dc67b39cfa359da50f8d11199949063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nHm-mCmnylXGRqb6VS1tug@2x.jpeg"/></div></div><figcaption class="jd je et er es jf jg bd b be z dx translated">线性回归模型预测方程</figcaption></figure><figure class="iw ix iy iz fd ja er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es la"><img src="../Images/ac51a0508d73e48fd475b07dba1acbfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tuktYAQUPXHLcrxlfPB8TA@2x.jpeg"/></div></div><figcaption class="jd je et er es jf jg bd b be z dx translated">向量形式的线性回归模型预测方程</figcaption></figure><p id="82a1" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">这是我们的线性回归模型。但是为了训练它，我们需要设置参数，使我们的模型最适合训练集。我们有一个称为<strong class="jj hi"> <em class="lb"> RMSE或均方根误差的性能指标。</em> </strong></p><p id="6c0f" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">因此，我们需要以这样的方式训练我们的模型，使得<strong class="jj hi">θ</strong>的值最小化RMSE。</p><figure class="iw ix iy iz fd ja er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lc"><img src="../Images/af04fab085797a27850d4d73f152332e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*acr6PcX7MEB85eeAq4SQVw@2x.jpeg"/></div></div><figcaption class="jd je et er es jf jg bd b be z dx translated">线性回归模型的MSE成本函数</figcaption></figure><p id="c07d" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">我们试着用一个例子来理解这一点。</p><p id="46bc" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">首先，进口所有需要的东西:</p><pre class="iw ix iy iz fd ld le lf lg aw lh bi"><span id="28aa" class="li lj hh le b fi lk ll l lm ln">import numpy as np<br/>import pandas as pd<br/>import sklearn<br/>import matplotlib as mpl<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span></pre><p id="adc8" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">设置一些随机X特征值:</p><pre class="iw ix iy iz fd ld le lf lg aw lh bi"><span id="454b" class="li lj hh le b fi lk ll l lm ln"># this will return an array of 100 values between 0 to 2 <br/>X = 2 * np.random.rand(100, 1)</span></pre><p id="2622" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">设置一些相应的随机y标签值:</p><pre class="iw ix iy iz fd ld le lf lg aw lh bi"><span id="945a" class="li lj hh le b fi lk ll l lm ln">y = 4 + (3 * X + np.random.rand(100, 1))</span></pre><p id="f047" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">使用matplotlib绘图和可视化:</p><pre class="iw ix iy iz fd ld le lf lg aw lh bi"><span id="f4a0" class="li lj hh le b fi lk ll l lm ln">from matplotlib import style<br/>style.use("fivethirtyeight")<br/>plt.plot(X, y, "b.")<br/>plt.xlabel("X")<br/>plt.ylabel("y")<br/>plt.axis([0, 2, 0, 12])<br/>plt.show()</span></pre><figure class="iw ix iy iz fd ja er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lo"><img src="../Images/6f3a50430edb8fc6d7a61a21c07e1b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mH-aWzt8Rb-tcLX7y118-Q.png"/></div></div><figcaption class="jd je et er es jf jg bd b be z dx translated">用matplotlib绘制一些随机值</figcaption></figure><p id="6c48" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">数据似乎是线性的，它应该是我们采用的线性方程，但在分析真实数据时，我们一开始不知道这一点。</p><pre class="iw ix iy iz fd ld le lf lg aw lh bi"><span id="df43" class="li lj hh le b fi lk ll l lm ln">from sklearn.linear_model import LinearRegression</span><span id="a1fe" class="li lj hh le b fi lp ll l lm ln">lin_reg = LinearRegression()<br/>lin_reg.fit(X, y)</span><span id="664e" class="li lj hh le b fi lp ll l lm ln">print(lin_reg.intercept_)<br/>print(lin_reg.coef_)</span></pre><figure class="iw ix iy iz fd ja er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lq"><img src="../Images/095af34f5adddd2bcf2141ee857976d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tk7UDJXdCTt0N-hZ9nTCfQ@2x.jpeg"/></div></div><figcaption class="jd je et er es jf jg bd b be z dx translated">使用正态方程或Sklearn线性回归模型的最佳拟合线</figcaption></figure><p id="5619" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">这适用于较小的训练数据。但是随着训练数据越来越大，计算X转置与X  的点积的<strong class="jj hi"> <em class="lb">倒数的时间复杂度变得非常高。因此，通常，我们使用<strong class="jj hi"> SGDRegressor </strong>，它使用<strong class="jj hi">梯度下降</strong>的概念来寻找最佳拟合线。</em></strong></p><blockquote class="lr ls lt"><p id="d0cb" class="jh ji lb jj b jk jl ii jm jn jo il jp lu jr js jt lv jv jw jx lw jz ka kb kc ha bi translated"><em class="hh">梯度下降</em>是一种通用优化算法，能够找到各种问题的最优解。梯度下降的一般思想是迭代地调整参数，以便最小化成本函数。</p></blockquote><p id="c330" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated"><strong class="jj hi">目测:</strong></p><figure class="iw ix iy iz fd ja er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lx"><img src="../Images/7fb00274a86db25acf0def6a474bf629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FC2HpGnlhYp5fZEUZRZ7Wg@2x.jpeg"/></div></div><figcaption class="jd je et er es jf jg bd b be z dx translated">梯度下降的描述，模型参数被随机初始化并反复调整以最小化成本函数</figcaption></figure><p id="083d" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated"><em class="lb">学习步长</em>与成本函数的斜率成比例，因此随着参数接近最小值，步长逐渐变小。</p><p id="c81c" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">梯度下降中的一个重要参数是由学习率超参数确定的步长。</p><p id="4dfd" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">如果学习率<strong class="jj hi"> <em class="lb">太小</em> </strong>，那么算法将不得不经历大量迭代，这将花费很长时间来最小化代价函数。</p><figure class="iw ix iy iz fd ja er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ly"><img src="../Images/fcfe8c9f4f2f4df9b2e0b082571f7f3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sj5TxOdbs84Deon1pP-0eQ@2x.jpeg"/></div></div><figcaption class="jd je et er es jf jg bd b be z dx translated">学习率太小了</figcaption></figure><p id="0719" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">另一方面，如果学习率<strong class="jj hi"><em class="lb"/></strong>太高，你可能会跳过山谷，到达另一边，甚至可能比以前更高。</p><figure class="iw ix iy iz fd ja er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lz"><img src="../Images/ad60f3111ff05a3b74f05cbb76bdeae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GoUhnFxMXWMrTip8PHmz4A@2x.jpeg"/></div></div><figcaption class="jd je et er es jf jg bd b be z dx translated">学习率太大</figcaption></figure></div><div class="ab cl kd ke go kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="ha hb hc hd he"><p id="891c" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">要使用SGD执行线性回归或使用Scikit-Learn执行随机梯度下降，我们只需遵循以下简单步骤:</p><pre class="iw ix iy iz fd ld le lf lg aw lh bi"><span id="ee66" class="li lj hh le b fi lk ll l lm ln">from sklearn.linear_model import SGDRegressor<br/>sgd_reg = SGDRegressor()<br/>sgd_reg.fit(X, y.ravel())</span><span id="96d0" class="li lj hh le b fi lp ll l lm ln">print(sgd_reg.intercept_)<br/>print(sgd_reg.coef_)</span></pre><figure class="iw ix iy iz fd ja er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ma"><img src="../Images/770f7dbc5a8d14e23d2346483b098e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uojawtPAWLBVvdL-iyOuKA@2x.jpeg"/></div></div><figcaption class="jd je et er es jf jg bd b be z dx translated">SGD的最初几步</figcaption></figure><p id="42c0" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">我们用SGD得到的解非常接近正规方程。</p><p id="9cde" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">今天到此为止。我将在下一篇文章中分享多项式分析。</p><p id="cf00" class="pw-post-body-paragraph jh ji hh jj b jk jl ii jm jn jo il jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated"><strong class="jj hi">如果你有任何困难，欢迎在下面评论。干杯！</strong></p></div></div>    
</body>
</html>