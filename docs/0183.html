<html>
<head>
<title>Explain How Your Model Works Using Explainable AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用可解释的人工智能解释你的模型是如何工作的</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/explain-how-your-model-works-using-explainable-ai-c7cefc42fde?source=collection_archive---------12-----------------------#2021-01-07">https://medium.com/analytics-vidhya/explain-how-your-model-works-using-explainable-ai-c7cefc42fde?source=collection_archive---------12-----------------------#2021-01-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/b582f7303754df1bdc2b73fd3e58c030.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wy57Q1Cs7KPE4_QI.jpeg"/></div></div></figure><h1 id="b702" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">你能解释一下你的模型是如何工作的吗？</h1><p id="7a27" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">人工智能技术被用来解决现实世界的问题。我们获取数据，执行一些操作以使其干净，并为后续流程做好准备。</p><p id="e5f0" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我们基本上是从这个世界中挑选东西，然后由T2把它们带入机器的世界，用数字来表示，然后输入给一堆模型。尝试改进它们，最终<em class="kq">【获胜者模型】</em>得到测试数据。一个至关重要的问题浮现在脑海中:</p><blockquote class="kr ks kt"><p id="e2da" class="jn jo kq jp b jq kl js jt ju km jw jx ku kn ka kb kv ko ke kf kw kp ki kj kk ha bi translated">"我们如何把这个结果带回现实世界？"</p></blockquote><p id="d960" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">可解释的人工智能(有一个更酷的名字:XAI)</p><p id="af12" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">正式定义</strong>:根据维基百科，可解释的AI是指人工智能技术应用中的方法和技术，使得解决方案的结果可以被人类理解。[1]</p><p id="8f8e" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在人工智能采用的早期阶段，不理解模型以某种方式预测什么是没关系的，只要它给出正确的输出。解释它们是如何工作的并不是首要任务。现在，焦点转向建造<strong class="jp hi"><em class="kq"/></strong>的人类可解释模型。</p><blockquote class="kr ks kt"><p id="8719" class="jn jo kq jp b jq kl js jt ju km jw jx ku kn ka kb kv ko ke kf kw kp ki kj kk ha bi translated"><em class="hh">模型解释的三个重要方面是:<br/> 1。透明度<br/> 2。质疑的能力<br/> 3。容易理解。[2] </em></p></blockquote><p id="9ece" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">模型的可解释性可以从两个层面进行检查:</p><ul class=""><li id="86a8" class="kx ky hh jp b jq kl ju km jy kz kc la kg lb kk lc ld le lf bi translated"><strong class="jp hi">全局解读:</strong>从更广阔的视角审视模型。例如，假设我们正在处理一个房价数据集，并且我们实现了一个神经网络。全局解释可能会说“您的模型使用平方英尺数作为一个重要特征来获得预测”</li><li id="91bc" class="kx ky hh jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated"><strong class="jp hi">局部解读:</strong>顾名思义，这种方法是聚焦于某一个观测/数据点。让我们继续我们的例子。对一栋非常小的房子的预测变成了大房子。当地的解释着眼于其他特征，它可能会说“你的模型是这样预测的，因为房子的位置非常靠近市中心。”</li></ul><figure class="lm ln lo lp fd ii er es paragraph-image"><div class="er es ll"><img src="../Images/fb2f9e786eb80bf55d899086e505836e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*Akgrdt9Xyq9DmCjw.jpeg"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">来源:<a class="ae lu" href="https://www.slideshare.net/0xdata/get-handson-with-explainable-ai-at-machine-learning-interpretabilitymli-gym" rel="noopener ugc nofollow" target="_blank">斯里安巴蒂，抓住MLI </a></figcaption></figure><h1 id="bb0f" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">准确性和可解释性之间的权衡</h1><p id="14ca" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">在行业中，您会经常听到<strong class="jp hi">业务利益相关者倾向于更易于解释的模型</strong>，如<strong class="jp hi">线性模型(线性\逻辑回归)</strong>和<strong class="jp hi">树</strong>，这些模型直观、易于验证，并向非数据科学专家解释。[2]</p><p id="58a4" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">相比之下，当我们查看真实数据的复杂结构时，在模型构建和选择阶段，人们的兴趣主要转移到更高级的模型上。这样，我们更有可能获得更好的预测。</p><p id="82ed" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">像这样的模型(集成、神经网络等。)被称为<strong class="jp hi">黑箱</strong>模型。随着模型越来越先进，解释它的工作原理变得越来越困难。输入神奇地进入一个盒子，瞧！我们得到了惊人的结果。</p><p id="9732" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><em class="kq">但是，怎么会呢？</em></p><p id="3aa2" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">当我们向涉众推荐这个模型时，他们会完全信任它并立即开始使用它吗？<strong class="jp hi">否</strong>。他们会问问题，我们应该准备好回答他们。</p><blockquote class="kr ks kt"><p id="0c87" class="jn jo kq jp b jq kl js jt ju km jw jx ku kn ka kb kv ko ke kf kw kp ki kj kk ha bi translated">我为什么要相信你的模型？</p><p id="01a1" class="jn jo kq jp b jq kl js jt ju km jw jx ku kn ka kb kv ko ke kf kw kp ki kj kk ha bi translated"><em class="hh">为什么模型要做某个决定？</em></p><p id="2b0d" class="jn jo kq jp b jq kl js jt ju km jw jx ku kn ka kb kv ko ke kf kw kp ki kj kk ha bi translated"><em class="hh">是什么驱动了模型预测？</em></p></blockquote><p id="bd8c" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我们既要考虑提高模型的准确性，又不能迷失在解释中。两者之间应该有一个平衡。</p><figure class="lm ln lo lp fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lv"><img src="../Images/03c5d2fc8a615e7a164d8f47b80f6e2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9TjYyRzoWOcymXuH.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">来源:<a class="ae lu" href="https://dphi.tech/lms/learn/ml-bootcamp-advanced/687" rel="noopener ugc nofollow" target="_blank">dpi高级ML训练营——可解释AI </a> [2]</figcaption></figure><p id="c6a1" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在这里，我想分享迪潘詹·萨卡尔<a class="ae lu" href="https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476" rel="noopener" target="_blank">中帖</a>中关于可解释AI的一句话:</p><blockquote class="kr ks kt"><p id="150f" class="jn jo kq jp b jq kl js jt ju km jw jx ku kn ka kb kv ko ke kf kw kp ki kj kk ha bi translated">任何机器学习模型的核心都有一个响应函数，它试图映射和解释自变量(输入)和因变量(目标或响应)之间的关系和模式。[3] </p></blockquote><p id="28cd" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">因此，模型接受输入，并对其进行处理以获得输出。<em class="kq">如果我们的数据有偏差怎么办？</em>也会让我们的<strong class="jp hi">模型偏向</strong>并因此<strong class="jp hi">不可信</strong>。了解&amp;能够向我们的模型解释很重要，这样我们也可以相信他们的预测，甚至可以在向他人展示之前发现问题并解决它们。</p><p id="6a77" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">为了提高我们模型的可解释性，有各种各样的技术，其中一些我们已经知道并实现了。传统技术是探索性数据分析、可视化和模型评估指标。在他们的帮助下，我们可以了解模型的策略。然而，它们有一些局限性。要了解更多关于传统方式及其局限性的信息，请查阅Dipanjan Sarkar的这篇令人惊叹的文章。[4]</p><p id="86a0" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">已经开发了其他模型解释技术和库来克服限制。其中一些是:</p><ul class=""><li id="069b" class="kx ky hh jp b jq kl ju km jy kz kc la kg lb kk lc ld le lf bi translated"><strong class="jp hi"> LIME </strong>(本地可解释的与模型无关的解释)</li><li id="0244" class="kx ky hh jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated"><strong class="jp hi"> SHAP </strong>(沙普利补充说明)</li><li id="9bda" class="kx ky hh jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated"><strong class="jp hi"> ELI5 </strong>(像我5岁一样解释)</li><li id="ec13" class="kx ky hh jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated"><strong class="jp hi">滑冰者</strong></li></ul><p id="a7ea" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">这些库使用特征重要性、部分相关图、个体条件期望图来解释不太复杂的模型，如线性回归、逻辑回归、决策树等。</p><p id="9aa4" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">特征重要性</strong>显示了特征对模型的重要性。换句话说，当我们从模型中删除特征时，我们的误差如何变化？如果误差增加很多，这意味着特征对于我们的模型预测目标变量是重要的。</p><figure class="lm ln lo lp fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lw"><img src="../Images/7f697373337be379bf1d226f2d504f08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VIYF09-4CvgExdvF.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">来源:机器学习掌握，<a class="ae lu" href="https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/" rel="noopener ugc nofollow" target="_blank"> XGBoost特征重要性条形图</a></figcaption></figure><p id="ed4e" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">部分相关图</strong>在其他一切保持不变的情况下，可视化某个特性的变化效果(用一个更酷的说法:其他条件不变)。在这些的帮助下，我们可以看到一个可能的极限值，当超过这个值时，它会将模型预测引向另一个方向。当我们可视化部分依赖图时，我们是在全局地检查模型。</p><figure class="lm ln lo lp fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lx"><img src="../Images/6278d17b90a4841917ab46816785e6be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7ZZvuZSqYkvGYslT.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">来源:Dipanjan (DJ) Sarkar，<a class="ae lu" href="https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739" rel="noopener" target="_blank">模型解释策略</a></figcaption></figure><p id="4db0" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">单个条件期望图</strong>显示了某个特性变化的影响，就像部分依赖图一样。但这一次，观点是地方性的。我们有兴趣看到某个特性<strong class="jp hi">的变化对我们数据</strong>中所有实例的影响。部分相关图是ICE图中各条线的平均值。[5]</p><figure class="lm ln lo lp fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ly"><img src="../Images/8af8a2b715bffa72712e0a0ef125a61a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*g87bQ372ECySDCwC.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">来源:Christoph Molnar，<a class="ae lu" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">可解释的机器学习——让黑盒模型变得可解释的指南</a></figcaption></figure><p id="ccaa" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在解释更高级的模型时，使用了模型不可知(不依赖于模型)技术。</p><p id="6ed5" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">全球代理</strong>模型采用原始输入和你的黑盒机器学习预测。当这个新数据集用于训练和测试适当的全局代理模型(更可解释的模型，例如线性模型、决策树等)时。)，它基本上试图<em class="kq">模仿你的黑盒模型的预测</em>。通过解释和可视化这个<em class="kq">“更容易”</em>的模型，我们可以更好地理解我们的实际模型是如何以某种方式进行预测的。</p><p id="c14c" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">其他可解释性工具有<strong class="jp hi"> LIME、SHAP、ELI5 </strong>和<strong class="jp hi"> SKATER </strong>库。我们将在下一篇文章中通过<em class="kq">引导的实现</em>来讨论它们。在那之前，我会分享一些我用来写这篇文章的惊人资源以及一些额外的链接。敬请关注下一篇帖子，到时见！</p><p id="0b34" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><em class="kq">快乐学习！</em></p><h1 id="6bc8" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">参考</h1><p id="7ffd" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">[1]维基百科，可解释AI，<a class="ae lu" href="https://en.wikipedia.org/wiki/Explainable_artificial_intelligence" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/explable _ artificial _ intelligence</a></p><p id="bff5" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">[2]dpi Tech，可讲解的人工智能课程，<a class="ae lu" href="https://dphi.tech/lms/learn/explainable-ai/563" rel="noopener ugc nofollow" target="_blank">https://dphi.tech/lms/learn/explainable-ai/563</a></p><p id="d6c6" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">[3] Dipanjan (DJ) Sarkar，人类可解释机器学习的重要性，<a class="ae lu" href="https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476" rel="noopener" target="_blank">https://towardsdatascience . com/Human-Interpretable-Machine-Learning-part-1-The-need-and-Importance-of-model-interpretation-2ed 758 F5 f 476</a></p><p id="a4ac" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">[4]迪潘詹(DJ)萨卡尔，模型解释策略，<a class="ae lu" href="https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739" rel="noopener" target="_blank">https://towards data science . com/explable-artificial-intelligence-part-2-Model-Interpretation-Strategies-75d 4 afa6b 739</a></p><p id="ced5" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">[5] Christoph Molnar，可解释的机器学习——让黑盒模型变得可解释的指南，2019，<br/><a class="ae lu" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">https://christophm.github.io/interpretable-ml-book/</a></p></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="120b" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><em class="kq">原载于2021年1月7日https://www.analyticsvidhya.com</em><em class="kq">的</em> <a class="ae lu" href="https://www.analyticsvidhya.com/blog/2021/01/explain-how-your-model-works-using-explainable-ai/" rel="noopener ugc nofollow" target="_blank"> <em class="kq">。</em></a></p></div></div>    
</body>
</html>