<html>
<head>
<title>Sales Analytics: Churn Analysis and Prediction with PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">销售分析:使用PySpark进行流失分析和预测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sales-analytics-churn-analysis-and-prediction-with-pyspark-98fffc169f36?source=collection_archive---------5-----------------------#2021-06-21">https://medium.com/analytics-vidhya/sales-analytics-churn-analysis-and-prediction-with-pyspark-98fffc169f36?source=collection_archive---------5-----------------------#2021-06-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="1442" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">最后用保存的模型和管道进行基础数据可视化和预测</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/9a05994fd3cd1060fdfcc3eaf2288d32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZUFYnGZREvJm0H_t8X4yWw.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">约翰·莱德曼在<a class="ae jm" href="https://unsplash.com/@johenredman?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="4807" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">客户流失是销售和市场营销的一个重要话题。这对于提供订阅服务的公司尤其重要，比如Apple Music或Amazon Prime，以及提供长期服务的公司，比如银行的储蓄账户或投资服务。</p><p id="e5bf" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">然而，什么是流失分析呢？为什么它如此重要？我们如何利用Python和大数据做到这一点？</p><h1 id="4b20" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">客户流失分析</h1><p id="0683" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">客户流失分析的目的是为了更好地了解客户流失的深层原因，从而减少客户流失，提高销售额。通过分析，一般来说，我们会发现是什么原因导致了客户的离开，谁可能会离开，以及我们可以做些什么来留住客户。正如您可能已经看到的，最重要的部分是分析的可操作的见解，通常您公司的CRM团队可能需要这些来改进他们当前的计划。</p><p id="d1d9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如今有了大数据和数据科学，我们甚至可以预测谁会流失，因此公司可以启动CRM计划来减少流失。有些人甚至会将LTV(客户终身价值)分析纳入模型构建中，以使其预测更加准确。虽然一些研究表明<strong class="jp hi">获得新客户的成本</strong> (CAC)比留住现有客户的成本高<strong class="jp hi"> 5倍，但流失分析是你应该添加到你的武器库中的秘密武器，以提高你的销售并降低成本。</strong></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lg"><img src="../Images/d0a2167bad247a7baab2a54f4fc2b563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uogTxM0a-lVa__VkjFBjew.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">照片由<a class="ae jm" href="https://unsplash.com/@hiteshchoudhary?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Hitesh Choudhary </a>在<a class="ae jm" href="https://unsplash.com/s/photos/python?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="72b0" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">使用PySpark进行客户流失分析和预测</h1><p id="3101" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">在本教程中，我们将建立一个机器学习模型来预测哪个客户端正在搅动。除了构建模型，我们还将为那些渴望了解更多信息的人演示一些基本的EDA技术。</p><p id="a82e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">用于分析的数据和Jupyter笔记本可在此处找到。</p><h1 id="0cc5" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">关于数据</h1><p id="1ca7" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">数据集是人工的，因此不会有任何侵犯或泄露隐私的问题。数据文件命名为Sales_new.csv，可以在这里下载。</p><p id="afce" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">以下是字段及其定义:</p><ol class=""><li id="5cfc" class="lh li hh jp b jq jr jt ju jw lj ka lk ke ll ki lm ln lo lp bi translated">名称:客户名称</li><li id="bbdd" class="lh li hh jp b jq lq jt lr jw ls ka lt ke lu ki lm ln lo lp bi translated">年龄:客户年龄</li><li id="ff3a" class="lh li hh jp b jq lq jt lr jw ls ka lt ke lu ki lm ln lo lp bi translated">Total_Purchase:采购金额</li><li id="1e8c" class="lh li hh jp b jq lq jt lr jw ls ka lt ke lu ki lm ln lo lp bi translated">account _ Manager:Binary 0 =无经理，1=由客户经理跟进</li><li id="efcc" class="lh li hh jp b jq lq jt lr jw ls ka lt ke lu ki lm ln lo lp bi translated">年数:作为客户的总年数</li><li id="54f1" class="lh li hh jp b jq lq jt lr jw ls ka lt ke lu ki lm ln lo lp bi translated">Num_sites:使用公司服务的客户网站的数量。可以把它看作是一种AWS服务。</li><li id="c95d" class="lh li hh jp b jq lq jt lr jw ls ka lt ke lu ki lm ln lo lp bi translated">Onboard_date:成为公司客户的年份</li><li id="d936" class="lh li hh jp b jq lq jt lr jw ls ka lt ke lu ki lm ln lo lp bi translated">地点:客户地址</li><li id="e47b" class="lh li hh jp b jq lq jt lr jw ls ka lt ke lu ki lm ln lo lp bi translated">公司:客户公司的名称</li></ol><p id="6ff6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">好了，开始编码吧！</p><h1 id="d541" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">系统设置</h1><p id="c734" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">首先，在使用PySpark之前，我们已经设置了系统。大多数数据分析师在第一次使用PySpark时都会遇到问题，因为他们没有设置好JAVA_HOME和SPARK_HOME。如果你运气不好，也遇到了这个错误，请不要担心，你可以试试这个<a class="ae jm" href="https://stackoverflow.com/questions/48260412/environment-variables-pyspark-python-and-pyspark-driver-python" rel="noopener ugc nofollow" target="_blank">帖子</a>看看是否能解决你的问题。</p><p id="2e43" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">请记住Python的版本也很重要，因为通常最新版本的Python与最新版本的Java不兼容。经过漫长的研究，我发现我的MacBook Pro 2015版本最稳定的版本是Java版本8。所以，我最后的设置是:</p><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="e3c7" class="ma kk hh lw b fi mb mc l md me">spark-2.4.8-bin-hadoop2.7</span><span id="d392" class="ma kk hh lw b fi mf mc l md me">Python 3.7.10</span></pre><p id="ff7e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">设置好环境后，我们将加载必要的库。</p><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="aa28" class="ma kk hh lw b fi mb mc l md me">import findspark<br/>findspark.init()</span><span id="bf2e" class="ma kk hh lw b fi mf mc l md me">from pyspark import SparkContext<br/>sc = SparkContext.getOrCreate()</span><span id="c30d" class="ma kk hh lw b fi mf mc l md me">import pyspark<br/>from pyspark.sql import SparkSession</span><span id="ceaf" class="ma kk hh lw b fi mf mc l md me">spark = SparkSession.builder.appName(‘lorentzyeung-logistic-regression-churn’).getOrCreate()</span><span id="557c" class="ma kk hh lw b fi mf mc l md me">spark</span></pre><h1 id="7bf9" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">加载数据</h1><p id="c36d" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">然后我们用<em class="mg"> spark.read.csv </em>函数加载数据。我们将告诉spark我们的数据集有头，并让它为我们推断模式。然后我们将检查dataframe的前5行，并设置<em class="mg"> truncate=True </em>，这意味着我们不希望内容在显示时被截断。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mh"><img src="../Images/6ec92197d5ce3e9ae791204a946d2877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3TZfOpKEVasebqomP2in0w.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">导入的数据帧将如下所示，模式显示在数据帧下方。</figcaption></figure><p id="6638" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在数据模式中，我们可以看到有10列数据及其类型。用于监督机器学习的列/标签在最后，并命名为“搅动”。</p><h1 id="3b13" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">数据探索</h1><p id="fc66" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">首先，我们将运行<em class="mg">描述()。show() </em>，用于查看数据帧的基本统计细节，如<strong class="jp hi">平均值、标准差、最大值、最小值和计数</strong>。当这个方法应用于一系列字符串时，它返回一个没有平均值和标准差的输出..</p><p id="5d3d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">然后，我们使用一个非常有用的for循环语法来只返回数字列。这里看起来没有必要，但是当你有一个巨大的数据集时，这是非常必要的。</p><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="7730" class="ma kk hh lw b fi mb mc l md me">numeric_col = [i[0] for i in data.dtypes if i[1] == “int”]</span><span id="97c7" class="ma kk hh lw b fi mf mc l md me">data.select(numeric_col).describe().toPandas().transpose() # row to col</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mi"><img src="../Images/b3e3ba827a67d18b8065a71f52f7a7cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*obRy0FWymRgH5ONXWtrnMA.jpeg"/></div></div></figure><p id="0ef8" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">然后我们将可视化客户流失的分布，以及Account_Manager的分布。Account_Manager几乎一半一半平均分布。但是我们在流失方面存在阶级不平衡，这将影响模型。通常，我们会采用其他一些方法，如K-fold交叉验证来解决这个问题。但是我们将在本教程中保持我们的工作简单，我们现在将跳过它。</p><div class="ix iy iz ja fd ab cb"><figure class="mj jb mk ml mm mn mo paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><img src="../Images/6803ebca83a65f0d5280ecaeae2e3c42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*HA3_dMijvZKW4v80kYuH_Q.png"/></div></figure><figure class="mj jb mk ml mm mn mo paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><img src="../Images/dd598cddcea8d1433ad3074cda21e191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*XQIoQ3DzfP9EAGdlshm2RQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx mp di mq mr translated">数据可视化</figcaption></figure></div><h1 id="93b5" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">检查不适用值</h1><p id="1106" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">每当我们看到一个新的数据集，我们需要检查空值。虽然Spark数据帧不同于pandas数据帧，seaborn只接受Pandas数据帧，但是我们需要使用<em class="mg"> toPandas() </em>函数来转换我们的数据帧，以便被heatmap函数接受。</p><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="9d0a" class="ma kk hh lw b fi mb mc l md me">import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="1219" class="ma kk hh lw b fi mf mc l md me">plt.figure(figsize=(12,6))</span><span id="c41e" class="ma kk hh lw b fi mf mc l md me">sns.heatmap(data.toPandas().isnull(),cbar=False)</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ms"><img src="../Images/a5507bff2f807c50e64859472f540f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_uPb6r8oqe_jqLxumk1NSQ.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">我们没有空值。</figcaption></figure><p id="e703" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">很好，我们不需要处理任何空值。</p><h1 id="1942" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated"><strong class="ak">基础EDA </strong></h1><p id="16d0" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">探索性数据分析通常是用可视化的方法对数据集进行分析或总结。让我们在这里做一些简单的EDA。首先，我们将看看在有和没有任何客户经理照顾客户的情况下，我们的流失率如何。</p><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="08ce" class="ma kk hh lw b fi mb mc l md me">data.groupby(“Account_Manager”).avg(“Churn”).show()</span></pre><p id="64f4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">结果很有趣。一般来说，有了客户经理，客户应该更忠诚，不太可能流失。但在我们的情况下却完全相反。与没有客户经理的客户流失率相比，有客户经理的客户流失率更高。我们形象地总结一下。</p><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="4ac5" class="ma kk hh lw b fi mb mc l md me">group_by_am = data.groupby(“Account_Manager”).avg(“Churn”).toPandas()</span><span id="e902" class="ma kk hh lw b fi mf mc l md me">figure = px.bar(group_by_am, x=”Account_Manager”, y=”avg(Churn)”, template=”plotly_dark”, title=”Chrun Rate if Account Manager is Assigned Graph”, color=”Account_Manager”)</span><span id="1c32" class="ma kk hh lw b fi mf mc l md me">figure.show()</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mt"><img src="../Images/4e1edc7535da09a555842a6c99bf8d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ujF2N3hDJ8AfsCvjczZHHw.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">这使我们在结论中分享一些有价值的见解。</figcaption></figure><p id="57b8" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">然后，我们将可视化我们的预测者的相关性。多重共线性是一个必须解决的问题，如果有的话。但是，如果变量之间不幸存在多重共线性，我们可以不处理它吗？是的，因为多重共线性只会影响系数和p值，但不会对模型预测因变量的能力产生太大影响。</p><p id="7939" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">幸运的是，我们这次没有任何多重共线性要处理。</p><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="f1b9" class="ma kk hh lw b fi mb mc l md me">from pyspark.mllib.stat import Statistics<br/>import pandas as pd</span><span id="e27d" class="ma kk hh lw b fi mf mc l md me">def compute_correlation_matrix(df, method=’pearson’):</span><span id="546e" class="ma kk hh lw b fi mf mc l md me">    features = df.rdd.map(lambda row: row[0:])<br/>    corr_mat = Statistics.corr(features, method=method)<br/>    corr_mat_df = pd.DataFrame(corr_mat,<br/>    columns=df.columns,<br/>    index=df.columns)</span><span id="bffa" class="ma kk hh lw b fi mf mc l md me">    return corr_mat_df</span><span id="595c" class="ma kk hh lw b fi mf mc l md me">compute_correlation_matrix(data[“Age”, “Total_Purchase”, “Account_Manager”, “Years”, “Num_Sites”, “Churn”])</span><span id="74cf" class="ma kk hh lw b fi mf mc l md me">plt.figure(figsize=(12,12))</span><span id="2a04" class="ma kk hh lw b fi mf mc l md me">sns.heatmap(compute_correlation_matrix(data[“Age”, “Total_Purchase”, “Account_Manager”, “Years”, “Num_Sites”, “Churn”]));</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mu"><img src="../Images/61cb5d3f3e7cdaef6ec3d95a6fcb93b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hvhMUyxGauVnUXKxKmzHLQ.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">使用的方法是皮尔逊。</figcaption></figure><h1 id="b8f2" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">我们的最终数据</h1><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="411c" class="ma kk hh lw b fi mb mc l md me">final = data.select([‘Age’, ‘Total_Purchase’, ‘Account_Manager’, ‘Years’, ‘Num_Sites’]) </span></pre><p id="548f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">然后我们把它分为训练集和测试集。原始数据的70%将是我们的训练数据，而30%将是我们的测试数据。</p><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="508b" class="ma kk hh lw b fi mb mc l md me">train,test = final_data.randomSplit([0.7,0.3])</span></pre><h1 id="6975" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">训练我们的模型</h1><p id="f986" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">现在让我们用LogisticRegression函数来训练我们的模型。</p><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="3e4f" class="ma kk hh lw b fi mb mc l md me">from pyspark.ml.classification import LogisticRegression</span><span id="542f" class="ma kk hh lw b fi mf mc l md me">lr_churn = LogisticRegression(featuresCol=”features”, labelCol=’churn’)</span><span id="7ab4" class="ma kk hh lw b fi mf mc l md me">lr_churn = lr_churn.fit(train) # train with the train data set</span><span id="8f79" class="ma kk hh lw b fi mf mc l md me">training_sum = lr_churn.summary</span><span id="96eb" class="ma kk hh lw b fi mf mc l md me">training_sum.predictions.show()</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mv"><img src="../Images/e202a02354bb75f4d4a93df78313124b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AplfNq_WQ5wvPYerOUpwRA.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">显示预测的摘要表。</figcaption></figure><h1 id="24d8" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">估价</h1><p id="bcb6" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">让我们用测试数据集来评估结果。</p><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="1866" class="ma kk hh lw b fi mb mc l md me">from pyspark.ml.evaluation import BinaryClassificationEvaluator</span><span id="7b59" class="ma kk hh lw b fi mf mc l md me">pred_and_labels = lr_churn.evaluate(test)</span><span id="6d40" class="ma kk hh lw b fi mf mc l md me">pred_and_labels.predictions.show(3)</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mw"><img src="../Images/b429f87cb2101e56c746069c1a1209c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QH9s_EKd2aGiDMOhdWMFSQ.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">第二个不正确，但是第一个和第三个是正确的，还不错。</figcaption></figure><p id="8337" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">然后用AUC对模型进行评价。AUC代表“ROC曲线下的面积”它告诉我们这个模型在多大程度上能够区分不同的类。该区域始终表示为0到1之间的值。1表示模型是完美的，没有任何错误。值0.5表示模型一般，只是胡乱猜测。因此，我们的目标是最大化这个价值。越大越好。但我们很可能不想要1，因为这意味着存在过度拟合。</p><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="4f93" class="ma kk hh lw b fi mb mc l md me">churn_eval = BinaryClassificationEvaluator(rawPredictionCol=’prediction’, labelCol=’churn’)</span><span id="6944" class="ma kk hh lw b fi mf mc l md me">auc = churn_eval.evaluate(pred_and_labels.predictions)</span></pre><p id="ee59" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们的AUC值是0.76。所以，我们的模型做得不错，不会有什么过拟合的问题。</p><h1 id="5302" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">在一个全新的数据集上预测</h1><p id="6c16" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">我们准备了另一个数据集，名为<em class="mg"> Sales_new.csv </em>。首先，我们必须将数据加载到系统中。然后我们需要使用VectorAssembler再次转换我们的数据。将转换后的数据拟合到训练好的模型中，然后我们检查结果。</p><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="4c7c" class="ma kk hh lw b fi mb mc l md me">new_customers = spark.read.csv(‘Sales_new.csv’,inferSchema=True, header=True)</span><span id="b812" class="ma kk hh lw b fi mf mc l md me">test_new_customers = assembler.transform(new_customers)</span><span id="5088" class="ma kk hh lw b fi mf mc l md me">final_results = lr_churn.transform(test_new_customers)</span><span id="10dc" class="ma kk hh lw b fi mf mc l md me">final_results.select(‘Company’,’prediction’).show()</span></pre><p id="e11e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">太棒了。你做到了！下图显示了对每家公司是否会流失的预测。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mx"><img src="../Images/23953dc278bf117586e6758123e77d59.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*irdTxOUNIhv960ve7nrFcg.jpeg"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">坎农-本森、巴伦-罗伯逊、塞克斯顿-戈尔登和帕克斯-罗宾斯已经准备好了。CRM部门的新CRM项目需要这个结果。</figcaption></figure><h1 id="2645" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated"><strong class="ak">管道创建</strong></h1><p id="2afd" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">现在我们继续建设管道。通常，您不必将数据分为训练和测试。在这里分割数据，因为我们想向您展示它将实现相同的预测结果。</p><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="0019" class="ma kk hh lw b fi mb mc l md me">from pyspark.ml import Pipeline<br/>from pyspark.ml.classification import LogisticRegression</span><span id="13c1" class="ma kk hh lw b fi mf mc l md me">lr_churn = LogisticRegression(featuresCol="features", labelCol='Churn')</span><span id="46c3" class="ma kk hh lw b fi mf mc l md me">pipeline = Pipeline(stages=[assembler, lr_churn])</span><span id="4df6" class="ma kk hh lw b fi mf mc l md me">model = pipeline.fit(train_churn)</span><span id="5381" class="ma kk hh lw b fi mf mc l md me">model.save("saved_model.model")</span></pre><h1 id="c068" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">加载并应用保存的管道</h1><pre class="ix iy iz ja fd lv lw lx ly aw lz bi"><span id="1344" class="ma kk hh lw b fi mb mc l md me">from pyspark.ml import PipelineModel</span><span id="39a3" class="ma kk hh lw b fi mf mc l md me">model_loaded = PipelineModel.load(“saved_model.model”)</span><span id="c3cc" class="ma kk hh lw b fi mf mc l md me">prediction = model_loaded.transform(test_churn)</span><span id="e158" class="ma kk hh lw b fi mf mc l md me">results.select('Company','prediction').show()</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es my"><img src="../Images/d003dfdc6610aec6f310562481122531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZgDBzd1RDuB_J3BdrauAmA.jpeg"/></div></div></figure><p id="fb67" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">看，预测和上面一样。</p><h1 id="8b48" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">结论</h1><p id="f0c2" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">我们已经建立了一个机器学习模型来预测客户的流失。虽然它是在一个虚假的数据集上训练的，但本教程的目的是向观众展示通常需要的过程和常见的解释性分析。正如我们已经提到的，最重要的部分是分析的可操作的见解。在EDA课程中，我们发现客户经理的流失率稍高。那么作为一名数据科学家，我们应该用假设检验等其他方法来交叉检查。在进一步调查后，如果结果是显著的，那么我们可以得出结论，我们的客户经理可能出了问题，可能需要通知部门主管。在与部门主管的讨论中，可能会发现一些绩效差距和劳动力差距。太棒了。</p><p id="2e7b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">虽然预测客户流失率表是钱，但它应该与CRM或客户经理部门共享。让这些部门知道谁会流失可以节省数千美元的促销费用，并可以提高销售额。这也可以减轻销售部门完成目标的压力。</p><p id="3f4d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">他们可以推出一些特殊的套餐来留住这些客户。也可能有一个新的电子邮件营销活动，特别是针对这些客户。这些电子邮件中包含的物品可以是优惠券、礼品、实体活动邀请卡等。</p><p id="ade6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">你可能也会对这篇文章感兴趣:<a class="ae jm" href="https://towardsdatascience.com/fundamental-marketing-analytics-f875018391d5?source=your_stories_page-------------------------------------" rel="noopener" target="_blank">基础营销分析</a>，和熊猫<a class="ae jm" href="https://towardsdatascience.com/pandas-data-wrangling-cheat-sheet-2021-cf70f577bcdd" rel="noopener" target="_blank">数据争论小抄</a>。</p><p id="8b9a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">非常感谢您阅读我的文章，我一直在为数字营销、数据分析、分析和Python写作。请点击下面的链接来看看。</p><p id="ba8a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://elartedm.com/marketing-blog/" rel="noopener ugc nofollow" target="_blank">数字营销</a>:<a class="ae jm" href="https://elartedm.com/marketing-blog/" rel="noopener ugc nofollow" target="_blank">https://elartedm.com/marketing-blog/</a></p><p id="f91d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://positivehk.com/category/%E5%B0%88%E6%AC%84/digital-marketing-blog/" rel="noopener ugc nofollow" target="_blank">另一个数字营销和机器学习</a>:【https://positivehk.com/category/%E5%B...】T2</p><p id="ce68" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">数据科学:【https://lorenzoyeung-io.medium.com/ T4】</p></div></div>    
</body>
</html>