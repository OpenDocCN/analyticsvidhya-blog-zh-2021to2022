<html>
<head>
<title>Stop just using Machine Learning and learn how to build it. Linear Regression and Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">停止仅仅使用机器学习，学习如何构建它。线性回归和梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/stop-just-using-machine-learning-and-learn-how-to-build-it-linear-regression-and-gradient-descent-3653de24c6d5?source=collection_archive---------5-----------------------#2021-07-12">https://medium.com/analytics-vidhya/stop-just-using-machine-learning-and-learn-how-to-build-it-linear-regression-and-gradient-descent-3653de24c6d5?source=collection_archive---------5-----------------------#2021-07-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/d19de431c09024564be28ebedb3d804f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tfz0XR4WEbsT3nTDGLJKjg.jpeg"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">派拉蒙电影公司</figcaption></figure><div class=""/><p id="f92d" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">今天对我们来说，训练和使用机器学习模型真的很容易。许多编程语言中的许多库为我们实现了机器学习模型，我们只需调用一个函数，瞧，我们正在使用复杂的机器学习模型。</p><p id="8ee5" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">但是我们知道这些模型是如何工作的吗？他们的目标是什么？如果特定模型的库实现不符合我们的需求，我们知道如何修改它吗？</p><p id="8107" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">想着这些问题，我有了写这些系列文章的动机。</p><p id="3dea" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在这篇文章中，你将学到:</p><ol class=""><li id="9583" class="jr js hw iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated"><strong class="iv hx">什么是线性回归？</strong></li><li id="05d5" class="jr js hw iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hx">什么是梯度下降？</strong></li><li id="fca4" class="jr js hw iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hx">如何使用NumPy实现线性回归？</strong></li></ol></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="0b76" class="km kn hw bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">介绍</h1><p id="4c47" class="pw-post-body-paragraph it iu hw iv b iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm lo jo jp jq ha bi translated">从最简单的机器学习模型(如一元线性回归)到复杂的深度神经网络，所有机器学习模型都有相同的目标，即找到可以回答特定问题的方程。</p><p id="8c3e" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">因此，作为这一系列文章的开始，我将向您展示如何仅使用Python Numpy从头开始构建线性回归模型。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="24ea" class="km kn hw bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">线性回归</h1><p id="5e18" class="pw-post-body-paragraph it iu hw iv b iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm lo jo jp jq ha bi translated">在线性回归中，我们希望使用一些“Xs”值作为输入来预测连续值。</p><p id="3eb6" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">简单的线性回归公式是</p><p id="1009" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">Y = aX + b</p><p id="3a24" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">“Y”是因变量，“X”是解释变量，“b”是直线的斜率，“a”是截距变量，即“X”为零时“Y”的值。在线性回归中，我们希望找到使预测误差最小化的“a”和“b”的值。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="5a8d" class="km kn hw bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">我如何知道我可以使用线性回归？</h1><p id="af3d" class="pw-post-body-paragraph it iu hw iv b iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm lo jo jp jq ha bi translated">判断线性回归是否是一个好的模型的最简单的方法之一是建立一个数据散点图。散点图是一个简单的图形，它使用笛卡尔坐标来绘制(X，Y)点。</p><p id="6ec5" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">如果该图显示了因变量和解释变量之间的线性关系，如下图所示，您可以尝试拟合线性模型。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lp"><img src="../Images/7bd2f343ceae6ed126a42bf82c40e62c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-gQKfuDQcedyZ_QyQ6-b9g.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">散点图的例子点，我们可以使用线性回归，我们可以看到一个线性回归模式与一些离群点。</figcaption></figure><p id="6960" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在此图中，我们可以看到因变量和解释变量之间的正线性关联，下图的情况并非如此。我们无法画出一条简单的线来很好地拟合这些数据。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lu"><img src="../Images/58e2b2ac455c16e6550915c84aab4808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s-o-1R0UxTagqeQfir02qQ.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">线性回归不是最佳模型的散点图示例。</figcaption></figure><p id="1fc1" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">但是我们怎么知道我们的线性回归是正确的呢？因为知道我们需要计算<strong class="iv hx">成本函数</strong>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="a9cb" class="km kn hw bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">价值函数</h1><p id="b4d9" class="pw-post-body-paragraph it iu hw iv b iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm lo jo jp jq ha bi translated">直观地说，要找出线性回归函数的误差，我们只需要得到预测的y值(ÿ)和实际点(y)之间所有差异的总和，然后我们可以用这个值除以点的数量(n)来得到平均值。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lv"><img src="../Images/f013d525086af72ee17a115062795fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*X3Ng6QnbHgXK2IuP41nvlQ.png"/></div></div></figure><p id="59e8" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">但是如果我们的'ÿ'值小于y值，误差就会减小，这不是我们想要的结果。因此，为了避免这种行为，我们可以得到方程的绝对值或方程的平方。</p><p id="7288" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">因为我们要对这个函数求导，稍后，平方函数的导数更容易求解，我们将对方程求平方。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es lw"><img src="../Images/c334282bf1b6e7672e8ffb5cc3130895.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*mBvRSPvvahh99LPgP7MNUA.png"/></div></figure><p id="6953" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">这个公式叫做均方差(MSE)。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lx"><img src="../Images/2cc007c8c0837d2a3bd672ccd43da4c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S6Kizsiwku9QMH90eh9iTg.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">该图以蓝色显示线性回归模型，以紫色显示基础真值点，以红色显示误差</figcaption></figure><p id="0eb2" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">但是我们如何改进我们的线性回归，降低成本函数的值呢？使用一种叫做<strong class="iv hx">梯度下降</strong>的技术。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="6012" class="km kn hw bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">梯度下降</h1><p id="8abe" class="pw-post-body-paragraph it iu hw iv b iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm lo jo jp jq ha bi translated">梯度下降是一种用于寻找函数最小值的技术。在线性回归中，我们将使用梯度下降来找到使成本函数最小化的“a”和“b”的值。</p><p id="dc12" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">下图描述了梯度下降。我们首先从随机的“a”和“b”值开始，初始误差是球的位置。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ly"><img src="../Images/12909aee309640741b509de4c66b896d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gCWwzetib5az8OxLTYTaZA.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">梯度下降的例子</figcaption></figure><p id="af70" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在那之后，我们想要向这条曲线的底部前进。但我们不想迈出大步，因为那样做我们会花太长时间到达底部，甚至更糟的是永远达不到底部。</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es lz"><img src="../Images/9888aed7c2a48d284860ca3d4041aca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*sSINprxpsNafZDrlKx5fMg.png"/></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">当步长太大时会发生什么</figcaption></figure><p id="25aa" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">此外，我们不希望小步前进，否则会花太长时间达到底部。所以，我们需要一个大小合适的台阶。这个步长由一个称为学习速率(L)的因子来调整。通常，L是一些小值，如0.0001。</p><p id="a9a9" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">为了优化成本函数，我们需要通过下面的步骤得到它的导数。</p><p id="fce7" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">1-替换ax + b的ÿ</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es ma"><img src="../Images/ff6818193ff5e5c33316b5d1c997a07d.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*k5FcbhvNgf0PFew9L_2BEQ.png"/></div></figure><p id="8b02" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">2-计算关于“a”的偏导数</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es mb"><img src="../Images/0e03ec77bf5488349e3fc996ced9c1a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*iuVrNUMFX-dHIHB5IPz9cA.png"/></div></figure><p id="aef6" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">3-计算b的偏导数</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es mc"><img src="../Images/d56ce922944497a3ab4a3367bf3b8306.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*LqC_luTnotA8UctudNfWFw.png"/></div></figure><p id="7ab4" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">4-在每次迭代中更新“a”和“b”的值</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div class="er es md"><img src="../Images/dc47fb91be22ffb3fa2b7f359d168509.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*yHZDC43itht9SHvIwPQkbw.png"/></div></figure><p id="d3e2" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">最后，重复这个过程m次或直到误差小于某个公差。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="f144" class="km kn hw bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">在Python中实现</h1><p id="f887" class="pw-post-body-paragraph it iu hw iv b iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm lo jo jp jq ha bi translated">导入所需的库。</p><pre class="lq lr ls lt fd me mf mg mh aw mi bi"><span id="1bb9" class="mj kn hw mf b fi mk ml l mm mn">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from warnings import warn</span></pre><p id="2cf1" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">函数来实现线性回归</p><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mo"><img src="../Images/0dda6a2604d1aba651b1b311e174cc38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zy_bGCHb7KdP3wqzlFh-Qw.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">线性回归的实现</figcaption></figure><p id="d720" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">为了实现线性回归，我们</p><ol class=""><li id="08f1" class="jr js hw iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated">首先将“a”和“b”设置为零，然后得到点数“n”。</li><li id="c9bb" class="jr js hw iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">获得预测</li><li id="791d" class="jr js hw iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">得到关于a和b的导数</li><li id="63c6" class="jr js hw iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">更新“a”和“b”系数</li><li id="cbed" class="jr js hw iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">得到错误</li><li id="5846" class="jr js hw iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">重复这个过程，直到I小于epoch或误差小于公差</li><li id="b166" class="jr js hw iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">返回更新后的“a”和“b”</li></ol><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mp"><img src="../Images/989327c274ba19825fc9cfa1d662dc24.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*10j8IqrR18H_iY7-398_8A.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">检查梯度下降是否发散的代码</figcaption></figure><p id="0525" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">这段代码检查梯度下降是否发散。</p><p id="4e49" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">为了测试这段代码是否如预期的那样工作，我使用了第二次世界大战Kaggle数据集中的<a class="ae mq" href="https://www.kaggle.com/smid80/weatherww2/data" rel="noopener ugc nofollow" target="_blank">天气条件，看看我是否可以拟合线性回归来解释MinTemp和MaxTemp之间的关系。</a></p><figure class="lq lr ls lt fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lp"><img src="../Images/936fe4b00f254cd59a8a77832e4daa4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3VUK7aKsFE-XXyClg2rL9A.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">最小温度与最大温度图，蓝色为点，红色为线性回归结果</figcaption></figure></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><p id="634a" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">你可以在这个<a class="ae mq" href="https://colab.research.google.com/drive/15SoZ2cqa3LwHn_qjEXDx17v_kC2oLuA1?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>里查看写这篇文章用的代码。如果你对我的<a class="ae mq" href="https://www.linkedin.com/in/jair-guedes-ferreira-neto/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>账户有任何意见，请随时联系我，感谢你阅读这篇文章。</p><p id="5bdb" class="pw-post-body-paragraph it iu hw iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">如果你喜欢你所读的，一定要👏下面，分享给你的朋友，关注我，不要错过这一系列的帖子。</p><h1 id="fe24" class="km kn hw bd ko kp mr kr ks kt ms kv kw kx mt kz la lb mu ld le lf mv lh li lj bi translated">参考</h1><p id="6dab" class="pw-post-body-paragraph it iu hw iv b iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm lo jo jp jq ha bi translated"><a class="ae mq" href="https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931" rel="noopener" target="_blank">https://towards data science . com/linear-regression-using-gradient-descent-97a6c 8700931</a></p></div></div>    
</body>
</html>