<html>
<head>
<title>[Papers Xplained]— Neural Machine Translation using Bahdanau Attention</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[论文解释]—使用Bahdanau注意力的神经机器翻译</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/papers-xplained-neural-machine-translation-using-bahdanau-attention-7e3274c41e14?source=collection_archive---------1-----------------------#2021-06-11">https://medium.com/analytics-vidhya/papers-xplained-neural-machine-translation-using-bahdanau-attention-7e3274c41e14?source=collection_archive---------1-----------------------#2021-06-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="b9f0" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi">*******************************************************************</p><p id="9bc9" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated"><strong class="ih hi"> <em class="jd">【论文解释系列】:这一系列帖子背后的直觉是解释著名深度学习研究论文的主旨。</em> </strong></p><p id="cbb1" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi">*******************************************************************</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/1e36a5616d693449427f5c6ca486a4e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4SuSa8RI0ddQcX32"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">杰弗里·布兰德杰斯在<a class="ae ju" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h1 id="5d7a" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">论文:联合学习对齐和翻译的神经机器翻译，2014</h1><h1 id="0b02" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">论文的重要性:</h1><p id="286e" class="pw-post-body-paragraph if ig hh ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc ha bi translated">正在讨论的论文是<strong class="ih hi">【联合学习对齐和翻译的神经机器翻译】</strong>作者<em class="jd"> Dzmitry Bahdanau，kyung hyun Cho&amp;Yoshua beng io</em>。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ky"><img src="../Images/b490f9296eabde4df6e8806a58bb79cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aSIzpMS2W9jHMSqrSjcJ3A.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">来源:原文:<a class="ae ju" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1409.0473.pdf</a></figcaption></figure><p id="aa30" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">这篇论文在2014年介绍了现在著名的<strong class="ih hi"> <em class="jd">【注意机制】</em> </strong>。尽管注意的概念有了一些发展，但本文介绍的机制仍然被称为“<strong class="ih hi"> <em class="jd">【巴丹瑙注意】或“附加注意”</em> </strong>。</p><h2 id="3bd7" class="kz jw hh bd jx la lb lc kb ld le lf kf iq lg lh kj iu li lj kn iy lk ll kr lm bi translated">论文链接:<a class="ae ju" href="https://arxiv.org/pdf/1409.0473" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1409.0473.pdf</a></h2><h1 id="4543" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">论文要点:</h1><ul class=""><li id="113e" class="ln lo hh ih b ii kt im ku iq lp iu lq iy lr jc ls lt lu lv bi translated">神经机器翻译(NMT)是使用神经网络将句子从源语言翻译成目标语言的概念。</li><li id="5533" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">在这篇论文之前，这样的NMT模型使用了多个网络，分别对它们进行训练。</li><li id="c739" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">本文提出建立和训练一个单一的大型神经网络，该网络读取句子并输出正确的翻译。这是当今使用的编码器-解码器架构的所有序列到序列模型的基础。</li><li id="517d" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">机器翻译，从概率的角度来看，类似于寻找一个使<code class="du mb mc md me b">p(y|x)</code>的条件概率最大化的目标句<code class="du mb mc md me b">y</code>，其中<code class="du mb mc md me b">x</code>是源句。</li><li id="d67f" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><strong class="ih hi"><em class="jd">NMT任务的目标:</em> </strong>使用平行训练语料库最大化句子对的条件概率。将使用参数化模型来模拟这种关系，并且该模型将使用反向传播来学习参数权重。</li><li id="f12e" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">NMT任务利用编码器-解码器模型(由Sutskever等人介绍，2014；Cho等人，2014年a)。编码器和解码器组件都是神经网络。</li></ul><h2 id="9360" class="kz jw hh bd jx la lb lc kb ld le lf kf iq lg lh kj iu li lj kn iy lk ll kr lm bi translated">编码器-解码器架构:</h2><ul class=""><li id="5bb1" class="ln lo hh ih b ii kt im ku iq lp iu lq iy lr jc ls lt lu lv bi translated">编码器接收源句子并将其编码成固定长度的向量。</li><li id="6c7a" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">解码器输出来自编码向量的翻译(目标句子)。</li><li id="7b0a" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">编码器-解码器系统被联合训练以最大化给定源-目标句子对的正确翻译的条件概率。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mf"><img src="../Images/b03fc6c04e301680bb73caf5cbd1dfc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B8QspC43qMaggvO3CmtT1Q.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">资料来源:Seq2Seq文件—<a class="ae ju" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1409.3215</a></figcaption></figure><h2 id="9abd" class="kz jw hh bd jx la lb lc kb ld le lf kf iq lg lh kj iu li lj kn iy lk ll kr lm bi translated">编码器-解码器架构的局限性:</h2><ul class=""><li id="3fb2" class="ln lo hh ih b ii kt im ku iq lp iu lq iy lr jc ls lt lu lv bi translated">解码器仅依赖于最后编码的固定长度向量来获得关于源句子的信息。</li><li id="a80d" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">特别是当源句子相当长时，编码器很难将所有信息压缩到一个向量中。</li><li id="5093" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">经验证明，随着源句子长度的增加，基本编码器-解码器的性能会迅速恶化(Cho等人(2014年b))。</li></ul><p id="0395" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">那么，论文提出了什么来克服这些局限性呢？</p><h2 id="fcae" class="kz jw hh bd jx la lb lc kb ld le lf kf iq lg lh kj iu li lj kn iy lk ll kr lm bi translated">学会联合调整和翻译:</h2><ul class=""><li id="6b59" class="ln lo hh ih b ii kt im ku iq lp iu lq iy lr jc ls lt lu lv bi translated">该论文提出了对编码器-解码器模型的扩展，该模型学习联合“对齐”和“翻译”。</li><li id="b4f3" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">每当NMT模型生成一个翻译单词时，它将软搜索源句子中的一组位置，并寻找最相关信息集中的位置。这类似于挑选对最终翻译更有意义的单词。</li><li id="c7b4" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">这违背了将整个源句子编码成单个固定长度的上下文向量的概念。</li><li id="1214" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">然后，NMT模型基于与这些源位置相关联的上下文向量以及先前生成的翻译输出来预测目标翻译。</li><li id="1a6a" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">和之前的方法有什么不同？这种方法将源句子编码成一系列向量，然后解码器在输出翻译时选取这些向量的子集。</li><li id="4b34" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">那么好处是什么呢？它使NMT模型能够避免将所有信息压缩到一个向量中，而是允许模型理解长句并基于上下文重要性进行选择性搜索。</li></ul><h2 id="9fac" class="kz jw hh bd jx la lb lc kb ld le lf kf iq lg lh kj iu li lj kn iy lk ll kr lm bi translated">编码器-解码器框架背后的数学原理:</h2><p id="d666" class="pw-post-body-paragraph if ig hh ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc ha bi translated">编码器将源句子作为向量序列<code class="du mb mc md me b">x = (x1, · · · , xTx )</code>处理成上下文向量<code class="du mb mc md me b">c</code>。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mg"><img src="../Images/fc921ee42a26de37dc5cb7f202c02d1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*j04y9sgiy8pOqWWWW9v-ZA.png"/></div></figure><p id="98da" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">在哪里，</p><ul class=""><li id="c163" class="ln lo hh ih b ii ij im in iq mh iu mi iy mj jc ls lt lu lv bi translated"><code class="du mb mc md me b">h </code>是潜州，</li><li id="ddbb" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><code class="du mb mc md me b">c</code>是从隐藏状态序列中生成的上下文向量</li><li id="101d" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><code class="du mb mc md me b">f, q </code>是非线性函数</li></ul><p id="2cc6" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">然后，在给定上下文向量<code class="du mb mc md me b">c</code>和所有先前预测的单词<code class="du mb mc md me b">{y1,y2....yt-1}</code>的情况下，训练解码器来预测下一个单词<code class="du mb mc md me b">yt</code></p><p id="41fb" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">这不过是给定<code class="du mb mc md me b">y</code>输出向量和<code class="du mb mc md me b">c</code>上下文向量的预测<code class="du mb mc md me b">yt</code>的最大似然估计。那么<code class="du mb mc md me b">p(y) </code>给出为</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mk"><img src="../Images/d05c4d4834587c90e6d4c59790f1e2d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*-sbxB4D-X2Zt2uf7JBx1_g.png"/></div></figure><p id="b3fb" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">使用递归神经网络，每个条件概率被建模为，</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ml"><img src="../Images/5dad837fa615e35a0f4a057a4ee52268.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*JtRrusLcFNK3dBdSjR37uQ.png"/></div></figure><p id="ad5b" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">在哪里，</p><ul class=""><li id="ec39" class="ln lo hh ih b ii ij im in iq mh iu mi iy mj jc ls lt lu lv bi translated"><code class="du mb mc md me b">g</code>是一个非线性的、潜在的多层函数，它输出<code class="du mb mc md me b">p(yt)</code>，</li><li id="87e3" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><code class="du mb mc md me b">st</code>是RNN的隐藏状态。</li></ul><h1 id="2a1b" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">该论文中提出的编码器-解码器结构:</h1><ul class=""><li id="97e5" class="ln lo hh ih b ii kt im ku iq lp iu lq iy lr jc ls lt lu lv bi translated">编码器模块是双向RNN。</li><li id="232b" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">解码器是“搜索和对齐”模型的组合，该模型考虑来自源序列的所有隐藏状态，然后挑选最相关的向量。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mm"><img src="../Images/1e4180e09530999332d731f3cee09d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*EidgTLR3gfu32WglgPn7hA.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">来源:原文:【https://arxiv.org/pdf/1409.0473.pdf T21】</figcaption></figure><p id="e762" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">这里，</p><ul class=""><li id="eab1" class="ln lo hh ih b ii ij im in iq mh iu mi iy mj jc ls lt lu lv bi translated"><code class="du mb mc md me b">X1, X2,…. XT</code>是源句标记。</li><li id="94c1" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">指向隐藏状态两个方向的箭头— <code class="du mb mc md me b">h1, h2, h3</code>表示RNN块的双向性质。</li><li id="4931" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">然后，每个隐藏状态连同相应的权重<code class="du mb mc md me b">αt1, αt2..αtT</code>一起被传递给一个<strong class="ih hi"> <em class="jd">【加法】</em> </strong>函数。</li></ul><h2 id="e159" class="kz jw hh bd jx la lb lc kb ld le lf kf iq lg lh kj iu li lj kn iy lk ll kr lm bi translated"><strong class="ak"> <em class="ie">为什么巴赫达瑙注意力被称为“加法注意力”？</em> </strong></h2><p id="fc79" class="pw-post-body-paragraph if ig hh ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc ha bi translated">由于注意力上下文向量是通过将源句子的所有隐藏状态相加得到的，因此Bahdanau注意力也被称为“加法注意力”。</p><h1 id="5045" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">Bahdanau注意力机制背后的数学原理:</h1><p id="fde5" class="pw-post-body-paragraph if ig hh ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc ha bi translated">现在，让我们从数学的角度来看—</p><p id="62a1" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">解码器模块负责预测序列中的下一个目标字，这由给定先前预测<code class="du mb mc md me b">yi-1</code>和输入字<code class="du mb mc md me b">x</code>的下一个预测<code class="du mb mc md me b">yi</code>的条件概率给出。</p><p id="ec1c" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">根据由附加注意机制计算的上下文向量<code class="du mb mc md me b">ci</code>，<code class="du mb mc md me b">p(yi|y1,...yi-1, x)</code>是以下的函数—</p><ul class=""><li id="0546" class="ln lo hh ih b ii ij im in iq mh iu mi iy mj jc ls lt lu lv bi translated">先前预测→ <code class="du mb mc md me b">yi-1</code></li><li id="5149" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">当前RNN隐藏状态(或简称为当前状态)→ <code class="du mb mc md me b">si</code></li><li id="9103" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">由附加注意机制导出的上下文向量→ <code class="du mb mc md me b">ci</code></li></ul><p id="6463" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">用数学的方式表达出来，</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mn"><img src="../Images/b7b03023bb744c8f608a256cdf04069a.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*hPzI23CyFl22nkckACD1VA.png"/></div></figure><p id="20ef" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">其中<code class="du mb mc md me b">si</code>是RNN隐藏状态，</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mo"><img src="../Images/c6f927f551d90aa82803bcd1061ddc0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*aPXWngiYvLsVMXUp6ncQBg.png"/></div></figure><p id="4b82" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">即，概率取决于每个目标单词<code class="du mb mc md me b">y</code>的不同上下文向量<code class="du mb mc md me b">ci </code>。</p><ul class=""><li id="2dca" class="ln lo hh ih b ii ij im in iq mh iu mi iy mj jc ls lt lu lv bi translated">上下文向量<code class="du mb mc md me b">ci </code>取决于编码器将输入句子映射到的注释序列<code class="du mb mc md me b">(h1, · · · , hTx )</code>。</li><li id="d154" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">然后，上下文向量<code class="du mb mc md me b">ci </code>被计算为这些注释<code class="du mb mc md me b">hj</code>的加权和，</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mp"><img src="../Images/80ede3622e3aa60f38f4ceac658e8aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*qOTyQIbhsOHzFm-0GP6RAw.png"/></div></figure><p id="5575" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">每个注释<code class="du mb mc md me b">hj </code>的权重<code class="du mb mc md me b">αij </code>计算如下:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mq"><img src="../Images/e22d11e2a53862161170acea06f2aae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9j_j-Sd9lzox6KVVJj8cg.png"/></div></div></figure><p id="cb04" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">这个等式有印象吗？是的，这与在所有多类分类问题中广泛使用的“Softmax”激活相同。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mr"><img src="../Images/f3465dd9c51483518529058e4cdf991b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/0*DMYdtOOx0IXq5ERr.png"/></div></figure><p id="1e4e" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">使用softmax进行额外关注可以确保以下几点-</p><ul class=""><li id="0b86" class="ln lo hh ih b ii ij im in iq mh iu mi iy mj jc ls lt lu lv bi translated">考虑“所有”隐藏状态有助于计算上下文向量<code class="du mb mc md me b">ci</code></li><li id="3141" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">它充当对齐模型，对位置<code class="du mb mc md me b">j</code>周围的输入单词和位置<code class="du mb mc md me b">i</code>处的输出单词的匹配程度进行评分。该分数基于rnn的隐藏状态。</li></ul><h2 id="0745" class="kz jw hh bd jx la lb lc kb ld le lf kf iq lg lh kj iu li lj kn iy lk ll kr lm bi translated">训练Bahdanau注意力模型:</h2><ul class=""><li id="dc1e" class="ln lo hh ih b ii kt im ku iq lp iu lq iy lr jc ls lt lu lv bi translated">对齐模型(这是作者最初提到的注意力模型)是一个参数化的前馈神经网络。</li><li id="845a" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">使用反向传播机制，它与系统的所有其他组件一起被联合训练，即，成本函数的梯度将被用于重复地更新权重向量，直到收敛。</li><li id="0f7e" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">因此，除了RNN权重之外，甚至与每个隐藏状态相关联的注意力权重也将被“学习”。</li><li id="e983" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">权重将以这样的方式更新，即在翻译中具有最大重要性的单词将具有最大的权重值，这意味着在上下文向量中有更多的表示。</li></ul><p id="debe" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">我们可以理解将所有注释的加权和作为计算期望注释的方法，其中期望在可能的比对之上。</p><h1 id="f34a" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">注意机制如何有利于翻译质量？</h1><p id="ad2a" class="pw-post-body-paragraph if ig hh ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc ha bi translated">如前所述，上下文向量<code class="du mb mc md me b">ci</code>作为所有隐藏状态<code class="du mb mc md me b">hj</code>的加权和给出。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mp"><img src="../Images/80ede3622e3aa60f38f4ceac658e8aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*qOTyQIbhsOHzFm-0GP6RAw.png"/></div></figure><p id="1de2" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">其中权重<code class="du mb mc md me b">αij</code>由下式给出</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mq"><img src="../Images/e22d11e2a53862161170acea06f2aae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9j_j-Sd9lzox6KVVJj8cg.png"/></div></div></figure><p id="224d" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">假设<code class="du mb mc md me b">αij </code>是目标单词<code class="du mb mc md me b">yi </code>与源单词<code class="du mb mc md me b">xj </code>对齐或从源单词<code class="du mb mc md me b">xj </code>翻译的概率。</p><p id="5c3c" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">然后，第<code class="du mb mc md me b">i</code>个上下文向量<code class="du mb mc md me b">ci </code>是所有具有概率<code class="du mb mc md me b">αij </code>的注释中的预期注释。</p><p id="bacc" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated">概率<code class="du mb mc md me b">αij </code>或其相关能量<code class="du mb mc md me b">eij </code>反映了注释<code class="du mb mc md me b">hj </code>相对于前一隐藏状态<code class="du mb mc md me b">si−1 </code>在决定下一状态<code class="du mb mc md me b">si </code>和生成<code class="du mb mc md me b">yi </code>中的重要性。</p><p id="aacf" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated"><em class="jd">直观地说，这在解码器中实现了一种注意机制。</em></p><blockquote class="ms"><p id="f693" class="mt mu hh bd mv mw mx my mz na nb jc dx translated">解码器学习决定源句子中需要注意的部分。通过让解码器具有注意机制，编码器从必须将源句子中的所有信息编码成固定长度向量的负担中解放出来。</p></blockquote><p id="e694" class="pw-post-body-paragraph if ig hh ih b ii nc ik il im nd io ip iq ne is it iu nf iw ix iy ng ja jb jc ha bi translated">因此，注意机制使解码器能够从源句子中进行“选择性检索”,而不考虑序列长度。到目前为止，我们已经看到了加法注意力机制背后的数学原理，以及编码器和解码器模块的内部工作原理。让我们继续理解并向涉众解释模型的预测(或翻译)。这是使用一个名为“<strong class="ih hi"> <em class="jd">注意力地图</em> </strong>”的可视化概念来完成的。</p><h1 id="1368" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">可视化注意力地图:</h1><p id="ec19" class="pw-post-body-paragraph if ig hh ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc ha bi translated">注意力地图是一种可视化的方法，可以让解码器“注意”源句子中的哪些单词。两个词之间的关注度越高，单元格值越高，反之亦然。它在视觉上类似于热图矩阵，其中相关的词对以较高的颜色梯度突出显示。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es nh"><img src="../Images/9f30a339ffdaa3ee526290b13a7fc46e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hfFhnKuQCmZgdIb_WIs-SQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">来源:原文:<a class="ae ju" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1409.0473.pdf</a></figcaption></figure><h1 id="4598" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">在我们结束之前…</h1><p id="b83b" class="pw-post-body-paragraph if ig hh ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc ha bi translated">Bahdanau注意也称为“附加注意”,是注意机制的一种类型，自从本文介绍以来，已经取得了一些进展。</p><h2 id="cb1a" class="kz jw hh bd jx la lb lc kb ld le lf kf iq lg lh kj iu li lj kn iy lk ll kr lm bi translated">几个有用的链接:</h2><ul class=""><li id="c636" class="ln lo hh ih b ii kt im ku iq lp iu lq iy lr jc ls lt lu lv bi translated">注意机制的类型(Bahdanau &amp; Luong注意)→<a class="ae ju" href="https://blog.floydhub.com/attention-mechanism/" rel="noopener ugc nofollow" target="_blank">https://blog.floydhub.com/attention-mechanism/</a></li><li id="8800" class="ln lo hh ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">变形金刚原纸——注意力是你所需要的→<a class="ae ju" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1706.03762.pdf</a></li></ul><p id="feb1" class="pw-post-body-paragraph if ig hh ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc ha bi translated"><strong class="ih hi"> <em class="jd">感谢关注:-) </em> </strong></p></div></div>    
</body>
</html>