<html>
<head>
<title>Deep Learning: Perceptron and Multi-Layered Perceptron</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习:感知器和多层感知器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-learning-perceptron-and-multi-layered-perceptron-ac8ca69c2c36?source=collection_archive---------12-----------------------#2021-01-24">https://medium.com/analytics-vidhya/deep-learning-perceptron-and-multi-layered-perceptron-ac8ca69c2c36?source=collection_archive---------12-----------------------#2021-01-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="63b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我将解释什么是感知器和多层感知器及其背后的数学。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/554d1704132cccbdc45876be6531f232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kyz6jwocEqaKTWrrgj5QvA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">感知器。</figcaption></figure><p id="ca96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上图是整个深度学习的积木。感知器与神经元具有相似性，因为其结构非常相似。感知器也像神经元一样接受输入和输出。因此，神经网络这个名称通常用于命名深度学习中的模型。</p><blockquote class="js"><p id="9295" class="jt ju hh bd jv jw jx jy jz ka kb jb dx translated">感知器是深度学习中所有架构的构建模块。</p></blockquote><p id="7db2" class="pw-post-body-paragraph ie if hh ig b ih kc ij ik il kd in io ip ke ir is it kf iv iw ix kg iz ja jb ha bi translated">给感知器的输入是权重和输入的点积。该函数接受这个输入并给出一些输出。如果输出大于0，则最终的output(y^为1，否则为0。您可以选择任何功能作为激活功能。例如:乙状结肠、tanh、relu等。</p><h1 id="6d21" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak">多层感知器(MLP): </strong></h1><p id="2fb2" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">顾名思义，在MLP我们有多层感知器。MLPs是前馈人工神经网络。在MLP，我们至少有3层。第一层称为输入层，接下来的层称为隐藏层，最后一层称为输出层。输入层中的节点没有激活，事实上，输入层中的节点表示数据点。如果使用d维向量表示数据点，则输入图层将具有d个节点。下图将使这一点更加清楚。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lk"><img src="../Images/4c88de560a965ca2e6b8676a24a6be7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HgIFlvpPhKzd7z0IgjhRxA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">多层感知器。</figcaption></figure><p id="95a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上图中，我们有一个输入层，两个隐藏层，和最后一个最终层。所有层都是完全连接的。这意味着当前节点与前一层的节点相连接。我们在每一层都有一个权重矩阵来存储该层的所有权重。这基本上就是训练结束后我们所得到的。所有这些权重在训练期间使用<strong class="ig hi">返回</strong> - <strong class="ig hi">传播</strong>进行更新。不要担心反向传播。我们将在下一篇文章中详细了解这一点。请熟悉图表中使用的符号和下标。这将有助于你理解这篇文章。</p><p id="9785" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在节点中使用的函数是非线性的。所谓非线性iI是指输出不会线性地依赖于函数的输入。这意味着，如果投入增加10%，并不意味着产出也会增加10%，产出可能会增加10%以上或10%以下。诸如sigmoid、tanh等激活函数是非线性激活函数的例子。在我的文章中，我将主要使用sigmoid作为激活函数。有一点值得一提的是<strong class="ig hi"> sigmoid永远只会给出0到1之间的输出。</strong></p><p id="23cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上图中，x1和x2代表数据点的矢量表示的两个值。这里我取了一个二维数据点。在上面的几行中，我已经提到MLPs是前馈神经网络。前馈意味着节点之间的连接不是循环的，而是单向的。从左到右。</p><p id="a61d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们将了解输入是如何馈入网络的。下图是第一个隐藏层的输入。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ll"><img src="../Images/abae89386c7d236fad98d0dacf97861c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Gdw2H673Hzm3OwZzfIM3Q.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">输入到第一个隐藏层。</figcaption></figure><p id="2881" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">类似地，另外两个神经元将获得输入a12和a13。这里输入和输出的下标与神经元相同。现在让我们看看下一个隐藏层得到什么输入。在这里，我们将考虑第二个隐藏层的两个神经元。从下面给出的图表可以清楚地看出这一点。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lm"><img src="../Images/ebf877e999753d84ad98859bd16802d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mc0V0atKEpd5Z4MGOlc4Fw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">显示输入如何提供给第二个隐藏层的图表。</figcaption></figure><p id="c57e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们将移动到最后一层，即输出层。这一层将给出最终输出。我们将以二元分类为例，在这里我们只给出两个值。1或0。如果最后一层的输出大于0.5，则为1，否则为0。下图将使这一点更加清楚。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ln"><img src="../Images/990fae9d113b4a83e72c6250be13edca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VP-brL2vhjUJPOJf4UGehg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">显示从输出层生成输出的图表。</figcaption></figure><p id="19cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">整个过程显示到现在为止，其中我们在向前的方向上将输入馈送到MLP或神经网络。因此，使用的术语是前馈神经网络。现在，由于我们有了给定数据点的预测标签，我们可以计算损失。我们可以使用均方误差或对数损失(也称为交叉熵)公式来计算损失。如果你想更好地理解什么是交叉熵，那就看看这篇<a class="ae lo" href="https://towardsdatascience.com/cross-entropy-log-loss-and-intuition-behind-it-364558dca514" rel="noopener" target="_blank"> <strong class="ig hi">文章</strong> </a>。</p><p id="d0e0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你不理解损失函数，不用担心。当我们学习如何使用反向传播在MLP更新权重时，我将介绍这一点。</p><p id="b3c3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我希望到目前为止，您可能已经清楚地了解了我们是如何为MLP提供输入的。在接下来的文章中，我们将了解当我们给出输入时，MLP是如何被训练的。</p><h1 id="83f8" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">序列中的文章。这些文章很快就会出现。</h1><p id="2437" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated"><strong class="ig hi">反向传播及其背后的数学</strong>。</p><p id="9ad9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">编写你的第一个感知机。</p><p id="60ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">编写你的第一个MLP </strong>。</p><p id="a3ac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi lp translated"><span class="l lq lr ls bm lt lu lv lw lx di">如果</span>你是ML和DL的新手，想知道如何防止模特<strong class="ig hi">过拟合和欠拟合</strong>，请浏览这个<a class="ae lo" href="https://towardsdatascience.com/overfitting-and-underfitting-in-machine-learning-89738c58f610" rel="noopener" target="_blank"> <strong class="ig hi">博客</strong> </a>。</p><div class="ly lz ez fb ma mb"><a href="https://towardsdatascience.com/overfitting-and-underfitting-in-machine-learning-89738c58f610" rel="noopener follow" target="_blank"><div class="mc ab dw"><div class="md ab me cl cj mf"><h2 class="bd hi fi z dy mg ea eb mh ed ef hg bi translated">机器学习中的过拟合和欠拟合</h2><div class="mi l"><h3 class="bd b fi z dy mg ea eb mh ed ef dx translated">在这篇文章中，你将了解什么是过度拟合和欠拟合。您还将学习如何防止模型…</h3></div><div class="mj l"><p class="bd b fp z dy mg ea eb mh ed ef dx translated">towardsdatascience.com</p></div></div><div class="mk l"><div class="ml l mm mn mo mk mp jm mb"/></div></div></a></div><p id="928f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">参考资料:</p><ol class=""><li id="8c10" class="mq mr hh ig b ih ii il im ip ms it mt ix mu jb mv mw mx my bi translated"><a class="ae lo" href="https://en.wikipedia.org/wiki/Multilayer_perceptron" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Multilayer_perceptron</a></li><li id="365b" class="mq mr hh ig b ih mz il na ip nb it nc ix nd jb mv mw mx my bi translated"><a class="ae lo" href="https://machinelearningmastery.com/neural-networks-crash-course/" rel="noopener ugc nofollow" target="_blank">https://machinelingmastery . com/neural-networks-crash-course/</a></li><li id="1450" class="mq mr hh ig b ih mz il na ip nb it nc ix nd jb mv mw mx my bi translated">【http://d2l.ai/chapter_multilayer-perceptrons/mlp.html T4】</li></ol></div></div>    
</body>
</html>