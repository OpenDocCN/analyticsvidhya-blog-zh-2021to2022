# 线性回归背后的数学

> 原文：<https://medium.com/analytics-vidhya/the-maths-behind-linear-regression-12bb72431b00?source=collection_archive---------5----------------------->

考虑一个由两个变量 **x** 和 **y** 表征的现象。

观察完这一现象后，我们现在处理一个样本数据集。

![](img/4a772b73d2f82fa54e160f82d44af5a9.png)

# **目标:我们想找到一个将 x 和 y 联系起来的数学公式**

![](img/3f5120542d254de007c78ac05cf656e0.png)

这个公式将允许我们预测 **x** 的未观测值的 **y** 的值

合理的开始是创建数据集的散点图，以了解 **y** 相对于 **x.** 如何变化

![](img/41a4e2c75fa7e6a8813220f5d5f4c7ef.png)

散点图表明 **x** 和 **y** 之间的关系几乎是**线性的，因此，我们假设:**

![](img/b2a90fc5a416ccfbeb0e424d264effe7.png)

其中 **a** 和 **b** 为待定常数。

**问题:**这个公式可以根据 **a** 和 **b.** 的值表示无限多条线

![](img/8e3e4cb87f814223a7c88604cd153da5.png)

**解**:找出 **a** 和 **b** 的值，其中 **y(x)** 最符合数据集。

这就是线性回归发挥作用的地方。

线性回归是一种模型，用于在自变量 **x** 和因变量 **y** 之间建立线性关系。

> **我们可以认为线性回归是一种优化方法。**

# **最小二乘法:**

最小二乘法是可用于根据数据集确定两个参数 **a** 和 **b** 值的几种方法之一。

这个想法是计算 **a** 和 **b** 的值，它们对应于**误差平方和**的最小值。

错误的定义如下:

![](img/809d330630d6ea990b8d580e5a818deb.png)

其中:

![](img/64f009bba1b44ece06925be9cb6c608f.png)

y 的精确值

![](img/fe1998240d252b4ef438b34b0260baeb.png)

y 的预测值

![](img/ab7f7394b3c7824b46888629a7f13928.png)

误差平方和定义为:

![](img/221607b6f096ab84ae567520a7e30995.png)

其中 **n** 是数据集中样本的**数量，在我们的例子中是 4。**

我们想找出函数 S(a，b)最小的 a 和 b 的值。

函数的极值(最小值或最大值)可以通过将导数设置为 0 来找到。*(只要函数在* ***R*** *数集中可微)*。

在我们的例子中，误差函数 S(a，b)是一个多项式，因此在 **R** 中可微，所以我们写为:

![](img/f7d8ad98e03118ec0db96fd1088ec972.png)![](img/a4171ed021d0994e42cd01df611d91d9.png)

我们最终得到一个齐次方程组。

求解这些方程得到:

![](img/65c255158142b09536d1126e236f7927.png)![](img/472a9c1936032a8e89ec72c095e972a1.png)

其中:

![](img/64be57afe3214a02fcfe12080c6d6c94.png)

x 的平均值

![](img/cce1948a32e0d4dadb074bdb5d70685d.png)

y 的平均值

使用我们分析得出的那些结果，我们计算出 **a** 和 **b** 的最佳值:

![](img/5e6b25662b2cd8189633a4f1c4418c15.png)![](img/25763b76f84dd4ee46ed741236a03691.png)

最后，我们的回归模型看起来像这样:

![](img/06bc85aca6a1165cc2e467cef7f7ab38.png)

根据数据集绘制的回归模型

# **为什么是最小二乘法？**

为什么要在最小化误差函数之前先平方误差？为什么不把所有的误差加起来呢？

直接对误差求和的问题是误差可能是正的，也可能是负的。正数和负数往往会相互抵消。

假设我们有一个误差 **e1=100** 和另一个误差**E2 =-100；**两个误差之和 **(e1+e2=0)** 表明我们的模型是 100%准确的，但事实显然并非如此，因为误差太大了。

因此，在创建误差函数之前，我们需要假设所有的误差都是正的，因此，将误差函数定义为误差的总和是行不通的。

要把负数转换成正数，我们一般取数的绝对值，或者我们就平方。那么为什么不取误差的绝对值呢？

可以使用这种方法，称为**最小绝对偏差**方法。尽管如此，由于绝对值微分带来的复杂性，我们往往倾向于在回归分析中使用最小二乘法。

例如，看一下下面的函数:

![](img/b586d13ea40478b6c6c19edaa9e3d3bf.png)

这个函数在 **x=0** 中显然是不可微的。

使用最小二乘法只是让这个过程更容易。

# **就是这样，这就是线性回归的工作原理。**