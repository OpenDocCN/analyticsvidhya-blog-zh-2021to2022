<html>
<head>
<title>Supervised Learning: Logistic Regression from basics to expert</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">监督学习:从基础到专家的逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/supervised-learning-logistic-regression-from-basics-to-expert-9f710a755857?source=collection_archive---------8-----------------------#2021-05-10">https://medium.com/analytics-vidhya/supervised-learning-logistic-regression-from-basics-to-expert-9f710a755857?source=collection_archive---------8-----------------------#2021-05-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="bfea" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">什么是监督学习？</h1><p id="a503" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">当我们的数据被标记时，监督学习机器学习算法就完成了。我们有两种监督机器学习算法</p><ol class=""><li id="4ee4" class="ka kb hh je b jf kc jj kd jn ke jr kf jv kg jz kh ki kj kk bi translated"><strong class="je hi">回归。</strong></li><li id="6560" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated"><strong class="je hi">分类。</strong></li></ol><h1 id="1da6" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak"> 1。回归</strong></h1><p id="c3ae" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">本博客中有解释:<a class="ae kq" href="https://sruthils22.medium.com/linear-regression-in-machine-learning-783bb5e58944" rel="noopener"> <strong class="je hi">点此</strong> </a></p><p id="c761" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">线性回归的超参数调整:<a class="ae kq" href="https://sruthils22.medium.com/hyperparameter-tuning-in-linear-regression-e0e0f1f968a1" rel="noopener"> <strong class="je hi">点击此处</strong> </a></p><h1 id="7b28" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak"> 2。分类</strong></h1><p id="2261" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">我们什么时候用分类监督学习？</strong></p><p id="f241" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">分类是根据独立特征预测目标列中的<strong class="je hi">分类变量</strong>或因变量的方法。分类问题的输出将是二进制类或多类。它属于监督技术。它是一种统计工具，用于找出结果变量、因变量和一个或多个变量(通常称为自变量)之间的关系。</p><h2 id="4dd3" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">我们在分类下有许多算法</h2><ol class=""><li id="f16e" class="ka kb hh je b jf jg jj jk jn li jr lj jv lk jz kh ki kj kk bi translated"><strong class="je hi">逻辑回归。</strong></li></ol><p id="6caa" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi"> 2。决策树。</strong></p><p id="3f8c" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi"> 3。随机森林。</strong></p><p id="3dcb" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi"> 4。KNN </strong></p><p id="48d1" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi"> 5。纳维·拜尔斯。</strong></p><p id="202a" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi"> 6。Ada助推</strong></p><p id="35cf" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi"> 7。XG升压</strong></p><p id="906b" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi"> 8。梯度增强</strong></p><p id="7346" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">这些是分类问题下最常见和最流行的机器学习算法</p><p id="822c" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">下面讨论分类问题下最简单也是最流行的算法。</p><h1 id="928f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak"> 1。逻辑回归</strong></h1><p id="de70" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这是因为它建立在线性回归的基础上。逻辑回归是另一种流行的人工智能算法，能够提供二进制结果。这意味着该模型既可以预测结果，又可以指定两类价值中的一类。该函数也基于改变算法的权重，但是它不同，因为非线性逻辑函数用于转换结果。该函数可以表示为一条<strong class="je hi"> S形线(或)S形曲线</strong>，用于区分真值和假值。</p><p id="1086" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">成功的要求与线性回归相同——移除相同值的输入样本并减少噪声(低值数据)的数量。这是一个相当简单的函数，可以相对快速地掌握，非常适合执行二元分类。<strong class="je hi">逻辑回归的目标是利用对数损失最小化误差。</strong></p><p id="70b9" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">逻辑回归方程是从直线方程推导出来的。</p><h2 id="e9f6" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">Y=C+B1*X1+ B2*X2+…… + Bn*Xn</h2><p id="da79" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">范围从-inf到+inf。但是在逻辑回归中，我们需要预测0到1之间的值，所以我们要转换y。</p><p id="ec17" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi">如果Y=0，则0 </strong></p><p id="ea2a" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi">如果Y是1-Y那么Y=1那么无穷大</strong></p><p id="f95e" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">让我们进一步变换，得到-inf到+inf之间的范围</p><p id="eb17" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi">Log(Y/1-Y)=&gt;Y = C+B1 * X1+B2 * X2+……+Bn * Xn</strong></p><p id="d99e" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">使概率范围从0到1。我们将使用sigmoid曲线函数。</p><h2 id="b724" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">s形曲线</h2><p id="70e4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">sigmoid曲线具有您想要的所有属性，开始时值非常低，结束时值非常高，中间值在中间，这是对事件概率值建模的好选择。<strong class="je hi">如果估计概率大于50%，则模型预测该实例属于该类(称为正类，标记为“1”)，否则预测它不属于该类(即，它属于负类，标记为“0”)</strong>。这使它成为一个二元分类器。</p><figure class="lm ln lo lp fd lq er es paragraph-image"><div class="er es ll"><img src="../Images/32bb6f87e532e4d2e5203e07e9eca4b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*DwhBgdHK_oayyoxQKGNx2A.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">Sigmoid曲线公式</figcaption></figure><figure class="lm ln lo lp fd lq er es paragraph-image"><div class="er es lx"><img src="../Images/3e9aacee3d2d6d8dfbaa8ce1ee2cd53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*T4_Q0YFw6pXkpP7TV-wIwg.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">s形曲线</figcaption></figure><p id="03ac" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">中间切割的红线是<strong class="je hi">门槛线</strong></p><figure class="lm ln lo lp fd lq er es paragraph-image"><div class="er es ly"><img src="../Images/37eae99b66c147e1573963eb04e67b96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*WfB2jEgskeu4oE6irwEhsw.png"/></div></figure><h2 id="1e90" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated"><strong class="ak">日志丢失</strong></h2><p id="4210" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在逻辑回归中，输出可以是概率可以是yes(或等于1)。这个概率是0到1之间的一个值。对数损失(对数损失)衡量分类器的性能，其中预测输出是介于0和1之间的概率值。如果我们的模型预测是正确的，那么我们的对数损失将是0，否则将大于0。请记住，对数损失值越低，我们模型的准确性越高。</p><p id="ebae" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi">Log loss = y Log(p)+(1-y)Log(1-p)</strong></p><p id="9144" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">其中p是y的概率值</p><figure class="lm ln lo lp fd lq er es paragraph-image"><div class="er es lz"><img src="../Images/0392c0a599c14a1c1f86812863e1ddd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*yopZa7lWCD1lTKdGprlBjA.png"/></div></figure></div><div class="ab cl ma mb go mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ha hb hc hd he"><h1 id="c784" class="ie if hh bd ig ih mh ij ik il mi in io ip mj ir is it mk iv iw ix ml iz ja jb bi translated">逻辑回归的假设</h1><p id="9417" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">1.误差的独立性，即所有样本组的结果都是相互独立的(即没有重复的回答)</p><p id="67a8" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">2.任何连续自变量的对数线性</p><p id="1b88" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">3.多重共线性缺失</p><p id="c553" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">4.缺乏强有力的有影响力的离群值。</p><p id="9d9a" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi">在进入超参数调优之前，我们需要了解一下所有分类问题的性能指标。</strong></p></div><div class="ab cl ma mb go mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ha hb hc hd he"><h2 id="a31f" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">分类问题的性能度量的博客:<a class="ae kq" href="https://sruthils22.medium.com/performance-measures-for-supverised-classification-problem-33bf9ccfdace" rel="noopener">点击这里</a></h2></div><div class="ab cl ma mb go mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ha hb hc hd he"><p id="d276" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">我们开始了解准确性、回忆性、精确性、特异性、敏感性等。我们可以对逻辑回归进行超参数调整。</p><h1 id="f524" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">超参数调谐</strong></h1><p id="2f69" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在逻辑回归中，进行调整是为了调整曲线的阈值。点是否属于这个类。它减少或增加最佳截止值以识别最佳截止值。进行参数调整的原因是为了减少模型的对数损失。这两种调整阈值的方法。</p><p id="9009" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi"> 1。基于成本的方法。</strong></p><p id="41e0" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi"> 2。尤登指数法。</strong></p><h2 id="e674" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated"><strong class="ak"> 1。</strong> <strong class="ak">成本法</strong></h2><p id="915e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">成本分析是确定最佳临界值的方法之一。当假阳性和假阴性的成本已知时，以不同的截止值计算成本，以实现假阳性和假阴性之间的合理平衡。<strong class="je hi">在逻辑回归建模中，分界点是决策者决定接受还是拒绝的点。</strong></p><p id="b331" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">完全逻辑回归模型对于假阳性(FP)和假阴性(FN)具有不同的值。因此，我们可以使用基于成本的方法来计算最佳临界值。在这种方法中，我们找到了总成本最小的最优截止值。总成本由以下公式给出:</p><p id="dffc" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi">总成本= FN x C1+FP x C2</strong></p><p id="2da8" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">其中，<br/>C1:是假阴性的代价<br/>C2:是假阳性的代价</p><p id="eba9" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">成本值可以使用商业知识来决定。</p><h2 id="430f" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated"><strong class="ak"> 2。</strong> <strong class="ak">尤登指数</strong></h2><p id="79c1" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这是为我们的模型计算阈值的另一种方法。尤登指数是最大化<strong class="je hi">(灵敏度+特异性-1)的分类截止概率。</strong></p><p id="1c11" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi">尤登指数=最大值(灵敏度+特异性— 1) </strong></p><p id="6fef" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi">(或)</strong></p><p id="4597" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi"> =最大值(TPR + TNR — 1) </strong></p><p id="b70a" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi">(或)</strong></p><p id="8021" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated"><strong class="je hi"> =最大值(TPR — FPR) </strong></p><p id="c22e" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">调整逻辑回归的参数后，我们需要选择<strong class="je hi">特征，从模型</strong>中移除不足的特征。因此，它降低了模型的复杂性。</p><h1 id="99d6" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">特征选择</h1><ol class=""><li id="5900" class="ka kb hh je b jf jg jj jk jn li jr lj jv lk jz kh ki kj kk bi translated">反向消除</li><li id="a5e1" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">预选</li><li id="2153" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">递归特征消除(RFE)。</li></ol><h1 id="bc0f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">逻辑回归的优势</h1><ol class=""><li id="32f8" class="ka kb hh je b jf jg jj jk jn li jr lj jv lk jz kh ki kj kk bi translated">对于未知记录来说非常快</li><li id="1282" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">它在线性可分数据中表现良好。</li><li id="df97" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">我们的模型中没有方差误差</li><li id="d577" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">这是一个简单易行的算法</li></ol><h1 id="bbf3" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">逻辑回归的缺点</h1><ol class=""><li id="8bd8" class="ka kb hh je b jf jg jj jk jn li jr lj jv lk jz kh ki kj kk bi translated">简单的问题模型不会学习复杂的问题模型会在低性能上失败。</li><li id="f3c7" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">它构建了线性边界。</li></ol><h1 id="8bfb" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">脚注</h1><p id="4411" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们逻辑回归的目标是最小化模型的对数损失，我们的模型不应该有更多的方差误差和偏差误差。我们必须相应地调整模型，以平衡偏差和方差的权衡。</p><p id="6bec" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">是的，这是一个很长的，但最常用的行业，流行和简单的分类问题下的算法。</p><p id="30ca" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">希望你喜欢这个博客</p><p id="5688" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn kr jp jq jr ks jt ju jv kt jx jy jz ha bi translated">感谢阅读:)</p></div></div>    
</body>
</html>