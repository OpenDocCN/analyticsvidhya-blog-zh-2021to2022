<html>
<head>
<title>Linear Discriminant Analysis in Python: Step by Step Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的线性判别分析:逐步指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-discriminant-analysis-in-python-step-by-step-guide-56b3e534d41b?source=collection_archive---------14-----------------------#2021-03-04">https://medium.com/analytics-vidhya/linear-discriminant-analysis-in-python-step-by-step-guide-56b3e534d41b?source=collection_archive---------14-----------------------#2021-03-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/ecb1ff0b6df8ff882d2bcf0fe0441bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_Y9rHBxUi6K5S4vP.jpg"/></div></div></figure><p id="d360" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你在寻找关于<strong class="ir hi">线性判别分析Python的完整指南吗？</strong>。如果是，那么你来对地方了。这里我将讨论与<strong class="ir hi">线性判别分析相关的所有细节，以及如何在Python </strong>中实现线性判别分析。所以，请花几分钟时间阅读这篇文章，以便获得关于<strong class="ir hi">线性判别分析</strong> <strong class="ir hi"> Python </strong>的所有细节。</p><h1 id="b704" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">线性判别分析Python</h1><p id="c579" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">线性判别分析用于降维。现在你可能在想，“什么是降维？”。所以在进入线性判别分析之前，先了解一下降维。</p><h1 id="bd67" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">什么是降维？</h1><p id="b999" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">降维是在模式分类和机器学习应用中使用的预处理步骤。</p><p id="c32d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我简化一下，</p><p id="554f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您收集用于处理的数据量很大。因此，处理大量数据是复杂的。它需要更多的处理能力和空间。因此，降维应运而生。它降低了数据的维度。</p><p id="74de" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">那么，你所说的降低维度是什么意思呢？</p><p id="ef64" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设，这是我们分散在二维空间的数据集。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kq"><img src="../Images/4830bc7d8fac4e396ebdb9d75ebf7d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2wSiaNQS4tqzRERF.png"/></div></div></figure><p id="0960" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，我们可以通过应用降维在1维空间中表示这些数据项。应用降维后，数据点看起来会像这样</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kq"><img src="../Images/ed6324cca50c0ccdb5b91341046f5de0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hHhNXAfNfqbpRhDg.png"/></div></div></figure><p id="6f68" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，降维是一种减少维数的技术。在这个例子中，我们从二维减少到一维。我希望现在你理解了降维。</p><p id="9d49" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">主成分分析也是降维方法之一。我已经写了一篇关于PCA的文章。可以在这里看这篇文章- <a class="ae kv" href="https://www.mltut.com/what-is-principal-component-analysis-in-machine-learning-complete-guide/" rel="noopener ugc nofollow" target="_blank">什么是机器学习中的主成分分析？完全指南！</a></p><p id="f2f6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们进入线性判别分析-</p><h1 id="26c7" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">什么是线性判别分析？</h1><p id="8ad9" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">线性判别分析是一种降维方法。LDA的目标是将数据集投影到低维空间。听起来和PCA差不多。对吗？</p><p id="5f76" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是LDA不同于PCA。线性判别分析寻找使多个类别之间的分离最大化的区域。这在常设仲裁院是做不到的。</p><p id="4d95" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，LDA的定义是- <strong class="ir hi"> LDA将特征空间(N维数据)投影到更小的子空间k( k &lt; = n-1)上，同时保持类别区分信息。</strong></p><p id="1a90" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">PCA被称为<strong class="ir hi">无监督</strong>，但是LDA是<strong class="ir hi">有监督</strong>的，因为它与因变量有关。</p><p id="1a10" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们看看LDA是如何工作的-</p><h1 id="e3ed" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">线性判别分析是如何工作的？</h1><p id="bff8" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">LDA的工作步骤如下-</p><h2 id="d263" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">第一步</h2><p id="47cd" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">计算数据集中不同类别的d维均值向量。</p><h2 id="a81f" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">第二步</h2><p id="d3f0" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">在类别散布矩阵(Sw)内计算<strong class="ir hi">。</strong></p><p id="42bd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设我们有一个二维数据集C1和C2。因此，要计算二维数据集的Sw，Sw的公式为-</p><p id="2ecf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Sw = S1+ S2</p><p id="805e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">S1是C1类别的协方差矩阵，S2是C2类别的协方差矩阵。</p><p id="6749" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，<strong class="ir hi">协方差矩阵</strong> S1的公式是——</p><p id="fcc1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">S1 =σ(x-u1)。(x-u1)^T</p><p id="e3ea" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">其中u1是C1类的<strong class="ir hi">均值</strong>。同样，你可以计算S2和C2。</p><h2 id="0150" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">第三步-</h2><p id="3006" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">计算<strong class="ir hi">类间散布矩阵(Sb) </strong></p><p id="f2bf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">计算Sb的公式是-</p><p id="22f6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Sb= (u1-u2)。(u1-u2)^T</p><h2 id="2516" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">第四步-</h2><p id="587e" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">计算散布矩阵的特征向量(e1，e2，e3，…ed)和相应的特征值(λ1，λ2，… λd)。</p><h2 id="9222" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">第5步–</h2><p id="7004" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">按特征值递减对特征向量进行排序，选择k个特征值最大的特征向量，形成d×k维矩阵w。</p><h2 id="97f4" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">第六步-</h2><p id="1089" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">减少维度</p><p id="e697" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">y= W^T. X</p><p id="c23c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">其中W^T是投影向量，x是输入数据样本。这里，投影向量对应于最高特征值。</p><p id="b8d8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，让我们想象一下LDA的整个运作-</p><p id="76d6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设，这条黑线是最高的特征向量，红点和绿点是两个不同的类。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lk"><img src="../Images/e996082e4b6c44e2cac608f01d7a16a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b4iVIkCOeDBnR0RP.jpg"/></div></div></figure><p id="c20c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当数据点被投影到这个向量上时，维数被降低，并且类别之间的区别也被可视化。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lk"><img src="../Images/a197c05ea73c4b9e3bba142b8a583301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*o0yjAZa70C6k-jz4.jpg"/></div></div></figure><p id="d87b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在该图像中，红色代表一个类别，绿色代表第二个类别。因此，通过应用LDA，维数被降低，并且两个类别之间的分离也被最大化。</p><p id="9187" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我希望，现在你明白了LDA的整个运作。现在，我们来看看如何用Python实现线性判别分析。</p><h1 id="d9fd" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">线性判别分析在Python中的实现</h1><p id="83c2" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">对于这个实现，我将使用Wine数据集。你可以从<a class="ae kv" href="https://www.kaggle.com/mltuts/wine-dataset" rel="noopener ugc nofollow" target="_blank">这里</a>下载数据集。</p><p id="f0e3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们的目标是根据几种葡萄酒的特性来识别不同的客户群。因此，酒店的店主可以根据客户群推荐葡萄酒。</p><p id="edff" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们从第一步开始-</p><h2 id="3a9b" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">1-导入重要的库</h2><pre class="kr ks kt ku fd ll lm ln lo aw lp bi"><span id="733e" class="kw jo hh lm b fi lq lr l ls lt">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span></pre><h2 id="aa64" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">2-加载数据集</h2><pre class="kr ks kt ku fd ll lm ln lo aw lp bi"><span id="fff7" class="kw jo hh lm b fi lq lr l ls lt">dataset = pd.read_csv('Wine.csv')<br/>X = dataset.iloc[:, 0:13].values<br/>y = dataset.iloc[:, 13].values</span></pre><p id="114a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">将数据集分成X和Y后，我们会得到类似这样的结果-</p><p id="6edd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，X是自变量，Y是因变量。y是依赖的，因为y的预测依赖于X值。</p><p id="2c13" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">X-</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/2b6153d9d3453aa5ab96472c9b93e9a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FLDha9wA3VEyh7it.png"/></div></div></figure><p id="97ae" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你-</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es lu"><img src="../Images/e950bdb25cadb9a423fe7957609b82d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/0*N6tQnOSwb7XzBT74.jpg"/></div></figure><p id="27cc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，下一步是-</p><h2 id="f4ce" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">3-将数据集分成训练集和测试集</h2><pre class="kr ks kt ku fd ll lm ln lo aw lp bi"><span id="eb16" class="kw jo hh lm b fi lq lr l ls lt">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)</span></pre><p id="a8c2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，我们将数据集分为训练集和测试集。也就是说，我们用最大数据来训练模型，分离一些数据来测试。</p><h2 id="64fa" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">4-应用特征缩放</h2><pre class="kr ks kt ku fd ll lm ln lo aw lp bi"><span id="2d31" class="kw jo hh lm b fi lq lr l ls lt">from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span></pre><p id="49c1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">特征缩放是执行的重要步骤。应用特征缩放后，我们将获得以下形式的数据-</p><p id="3758" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">x列车-</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/a4107f94778a2d2b648238d3a615943e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F1P8jw0ygfYAgjjP.jpg"/></div></div></figure><p id="7037" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里的值被缩放。应用特征缩放后，就该应用线性判别分析(LDA)了。</p><h2 id="9b10" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">5.应用LDA</h2><pre class="kr ks kt ku fd ll lm ln lo aw lp bi"><span id="3820" class="kw jo hh lm b fi lq lr l ls lt">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA<br/>lda = LDA(n_components = 2)<br/>X_train = lda.fit_transform(X_train, y_train)<br/>X_test = lda.transform(X_test)</span></pre><p id="8055" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，<strong class="ir hi"> n_components = 2 </strong>表示提取的特征的数量。这意味着我们只使用了所有特性中的2个特性。而这两个特性会给出最好的结果。</p><p id="ef2a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，在应用LDA之后，我们会得到X_train和X_test之类的东西-</p><p id="227b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">x _火车-</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es lv"><img src="../Images/9f411c8a0eaeffddf7621c3d32900b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*eFIxzYlFdlCfNLQQ.jpg"/></div></figure><p id="3c6c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">X_测试-</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es lv"><img src="../Images/ce803aa007b670c50544cc57a948b553.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*p7yVP-dY-v6_ESAJ.jpg"/></div></figure><p id="2b95" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">应用LDA之后，现在是应用任何分类算法的时候了。这里我用的是逻辑回归。但是您可以使用任何其他分类算法并检查准确性。</p><h2 id="effd" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">6.将逻辑回归拟合到定型集</h2><pre class="kr ks kt ku fd ll lm ln lo aw lp bi"><span id="c425" class="kw jo hh lm b fi lq lr l ls lt">from sklearn.linear_model import LogisticRegression<br/>classifier = LogisticRegression(random_state = 0)<br/>classifier.fit(X_train, y_train)</span></pre><p id="94c8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">注意-在应用分类算法之前，总是首先应用LDA。</p><p id="2786" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，是时候预测结果了。</p><h2 id="52e3" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">7.预测测试集结果</h2><pre class="kr ks kt ku fd ll lm ln lo aw lp bi"><span id="9fe2" class="kw jo hh lm b fi lq lr l ls lt">y_pred = classifier.predict(X_test)</span></pre><p id="817d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">运行这段代码后，我们会得到Y_Pred类似的东西-</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es lv"><img src="../Images/a4af3f492c478477468cc2c1243b798d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*vVDzRvb61nkneKXU.jpg"/></div></figure><h2 id="880c" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">8.用混淆矩阵检查准确性</h2><pre class="kr ks kt ku fd ll lm ln lo aw lp bi"><span id="0196" class="kw jo hh lm b fi lq lr l ls lt">from sklearn.metrics import confusion_matrix, accuracy_score<br/>cm = confusion_matrix(y_test, y_pred)<br/>print(cm)<br/>accuracy_score(y_test,y_pred)</span></pre><p id="850c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们得到了这个混淆矩阵和准确度分数，太棒了！我们得到了<strong class="ir hi"> 100% </strong>的准确率。</p><p id="40de" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">[14 0 0]<br/>【0 16 0】<br/>【0 0 6】<br/>Out:1.0</p><p id="b5b6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们来看一下测试集的结果</p><h2 id="f66c" class="kw jo hh bd jp kx ky kz jt la lb lc jx ja ld le kb je lf lg kf ji lh li kj lj bi translated">9.可视化测试集结果</h2><pre class="kr ks kt ku fd ll lm ln lo aw lp bi"><span id="dfbf" class="kw jo hh lm b fi lq lr l ls lt">from matplotlib.colors import ListedColormap<br/>X_set, y_set = X_test, y_test</span><span id="30ad" class="kw jo hh lm b fi lw lr l ls lt">X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))</span><span id="021d" class="kw jo hh lm b fi lw lr l ls lt">plt.contourf(X1,X2,classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))</span><span id="f5df" class="kw jo hh lm b fi lw lr l ls lt">plt.xlim(X1.min(), X1.max())<br/>plt.ylim(X2.min(), X2.max())</span><span id="9e4f" class="kw jo hh lm b fi lw lr l ls lt">for i, j in enumerate(np.unique(y_set)):<br/>    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],<br/>                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)</span><span id="deea" class="kw jo hh lm b fi lw lr l ls lt">plt.title('Logistic Regression (Test set)')<br/>plt.xlabel('LD1')<br/>plt.ylabel('LD2')<br/>plt.legend()<br/>plt.show()</span></pre><p id="1da2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，运行这段代码后，我们将得到</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es lx"><img src="../Images/0d9c7b6be3ea7c2e20b5593b7be2ef41.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/0*QOMd5do1kCv-_c2D.png"/></div></figure><p id="c6b1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这里，您可以看到所有三个类别都属于正确的区域。没有不正确的结果。</p><p id="6bb5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我希望你了解LDA的整个工作程序。现在是总结的时候了。</p><h1 id="432d" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结论</h1><p id="832a" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">在本文中，您学习了与<strong class="ir hi">线性判别分析Python </strong>相关的一切。</p><p id="5765" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">具体来说，你学到了-</p><ol class=""><li id="faac" class="ly lz hh ir b is it iw ix ja ma je mb ji mc jm md me mf mg bi translated">什么是降维，线性判别分析？</li><li id="d1cb" class="ly lz hh ir b is mh iw mi ja mj je mk ji ml jm md me mf mg bi translated">线性判别分析是如何工作的？</li><li id="a868" class="ly lz hh ir b is mh iw mi ja mj je mk ji ml jm md me mf mg bi translated">以及如何用Python实现线性判别分析。</li></ol><p id="bc1a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我试图让这篇文章对你来说简单易懂。但是，如果你有任何疑问，欢迎在评论区问我。我将尽力消除你的疑虑。</p><p id="910b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">万事如意！</p><p id="bfb7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">快乐学习！</p></div></div>    
</body>
</html>