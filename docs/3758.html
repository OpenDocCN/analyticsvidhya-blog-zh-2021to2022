<html>
<head>
<title>Understanding Word2Vec</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解Word2Vec</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-word2vec-39fabe660705?source=collection_archive---------3-----------------------#2021-07-23">https://medium.com/analytics-vidhya/understanding-word2vec-39fabe660705?source=collection_archive---------3-----------------------#2021-07-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="edfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">“从一个人交的朋友就可以知道一个字”——(</em><a class="ae je" href="https://en.wikipedia.org/wiki/John_Rupert_Firth" rel="noopener ugc nofollow" target="_blank"><em class="jd">弗斯，J. R. 1957:11 </em> </a> <em class="jd"> ) </em></p><p id="9eff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jf translated">这是英国语言学家约翰·鲁伯特·弗斯的一句名言。他广为人知的是让人们注意到这样一个事实，即你可以通过在任何给定的句子中查看相同上下文中的其他单词来判断一个单词的意思。这意味着在一个句子中可以互换使用的单词有相似的意思。</p><p id="08e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当你看到<strong class="ih hj"> <em class="jd">同音异义词时，单词的“上下文相关”含义的想法更有意义。</em> </strong>这些<strong class="ih hj"> <em class="jd"> </em> </strong>是在不同语境下有不同含义的词。下面句子中的“树皮”就是一个例子；</p><p id="dc8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望她的狗不会在我敲门的时候叫<strong class="ih hj"><em class="jd"/></strong><em class="jd"><br/>树</em> <strong class="ih hj"> <em class="jd">树皮</em> </strong> <em class="jd">摸起来很粗糙。<br/>我爱吃裹着杏仁</em> <strong class="ih hj"> <em class="jd">树皮</em> </strong> <em class="jd">的椒盐卷饼。</em></p><p id="1537" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这三句话中，你可以通过观察树皮周围的单词立即判断出树皮的意思。在实践中，你通常会选择一个周围单词的窗口，并通过查看所选单词来推断原始单词的意思。例如，在上面的句子中，我们可以看到树皮的意思因周围的词而异，这些词被称为<strong class="ih hj">【上下文词】</strong>，我们将“树皮”<strong class="ih hj"> </strong>称为<strong class="ih hj">【中心词】</strong></p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jo"><img src="../Images/ad815995d1d38ceb5e719e3ad32cd4f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WsWWXjDkoXVe7o_sHh_Jpw.png"/></div></div></figure><p id="43b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的同义词库图像显示了单词“bark”在不同上下文中的同义词。这意味着在上面的第一句话中；我们可以很容易地用<em class="jd">、</em>、<em class="jd">、</em>、<strong class="ih hj">来代替<em class="jd">、</em>这个词，但不能用</strong>、<em class="jd">、【外壳】、</em>来代替，因为尽管它们是<em class="jd">、【树皮】的同义词，但意思却完全不同。</em></p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es ka"><img src="../Images/c79fef88c46bad85589cb1270abe919c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9pZunjWuF03a7dceBdf1ug.png"/></div></div><figcaption class="kb kc et er es kd ke bd b be z dx translated"><a class="ae je" href="https://www.freethesaurus.com/bark" rel="noopener ugc nofollow" target="_blank">https://www.freethesaurus.com/bark</a>中“树皮”的词库</figcaption></figure><p id="6844" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此<em class="jd">，</em>当将单词建模为向量时，以反映单词出现的上下文中的含义的方式对它们进行编码是很重要的，这就是<strong class="ih hj"> Word2Vec </strong>算法背后的直觉。</p></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><p id="e68c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jf translated"><span class="l jg jh ji bm jj jk jl jm jn di"> W </span> ord2Vec是一种NLP算法，它将单词的含义编码成简短、密集的向量(单词嵌入),可用于下游NLP任务，如问答、信息检索、机器翻译、语言建模等。该向量通过查看语料库中单词周围的单词，将任何给定语料库中单词的含义置于上下文中。该算法由Mikolov等人于2013年在他们的论文(“<a class="ae je" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank"> <em class="jd">向量空间中单词表示的有效估计</em> </a>”)中引入。</p><p id="f1a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在引入word2vec之前，单词被表示为<strong class="ih hj">稀疏长向量</strong>，其维数为训练语料库中存在的整个词汇的大小(<em class="jd">单词总数</em>)。这些传统向量的示例包括<em class="jd"> one-hot向量、count向量、Tf-Idf向量、</em> e.t.c。将单词表示为稀疏向量的主要缺点之一是在单词之间建立任何形式的关系。这是因为这些向量不包含足够的关于单词的信息来证明这种句法或语义关系，例如，独热向量是正交的(<em class="jd">垂直并且具有0 </em>的点积)，因此不能用于测量任何形式的相似性。</p><p id="c72f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如前所述，word2vec背后的直觉是确保句子中存在于相似上下文中的单词被映射到同一个向量空间。这意味着语料库中具有相似邻近/周围/上下文词的词具有相似的向量(<em class="jd">具有高余弦相似度</em>)。更令人印象深刻的是，单词嵌入的相似性超越了句法规则；使用简单的代数运算，我们可以显示单词之间更复杂的关系。例如，作者能够确定<strong class="ih hj">向量(' King') —向量(' Man') +向量(' Woman') </strong>产生具有最接近“Queen”的向量表示的相似性的向量(<strong class="ih hj">矢(</strong>皇后)<strong class="ih hj"> ) </strong>)。</p><p id="594d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以使用谷歌的G <a class="ae je" href="https://github.com/RaRe-Technologies/gensim" rel="noopener ugc nofollow" target="_blank"> ensim </a>库和<a class="ae je" href="https://drive.google.com/u/0/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&amp;export=download" rel="noopener ugc nofollow" target="_blank">预训练单词嵌入</a>来展示这一点。</p><pre class="jp jq jr js fd km kn ko kp aw kq bi"><span id="18fd" class="kr ks hi kn b fi kt ku l kv kw">from gendim.models import KeyedVectors</span><span id="d8fc" class="kr ks hi kn b fi kx ku l kv kw"><strong class="kn hj"># load the google Word2Vec model</strong><br/>filename = 'GoogleNews-vectors-negative300.bin'<br/>model = KeyedVectors.load_word2vec_format(filename, binary=True)</span><span id="3710" class="kr ks hi kn b fi kx ku l kv kw"><strong class="kn hj"># vector algebra</strong><br/>result = model.most_similar(positive=['woman', 'king'], negative= ['man'], topn=1)</span><span id="9ed1" class="kr ks hi kn b fi kx ku l kv kw">print(result)</span></pre><p id="f21a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结果</strong> : <em class="jd"> [('queen '，0.7118192315101624)] </em></p></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><p id="ea71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文作者提出了两种学习单词表征的体系结构。<strong class="ih hj">连续字包</strong>和<strong class="ih hj">跳格图，下面的</strong>是用于训练word2vec嵌入的两种架构。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es ky"><img src="../Images/fffd1a44567beb375cdd26af3df93dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c8sQXfRie4nZkB7WZ7ElYA.png"/></div></div><figcaption class="kb kc et er es kd ke bd b be z dx translated"><a class="ae je" href="https://www.researchgate.net/figure/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations_fig1_326588219" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/figure/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations _ fig 1 _ 326588219</a></figcaption></figure><p id="820d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jf translated"><span class="l jg jh ji bm jj jk jl jm jn di"> C </span>连续单词包通过从选择的上下文单词窗口中预测中心单词来学习单词表示。在CBOW中，我们对特定单词周围的上下文单词窗口进行采样，将其提供给模型，并预测中心单词。在这个特定的架构中，输入和投影层之间的权重矩阵在所有单词之间共享。我们将输入单词<strong class="ih hj">(上下文单词)</strong>的一键向量映射到投影层<strong class="ih hj">(嵌入层)。n维的</strong>嵌入层乘以另一个权重矩阵得到输出层。我们在输出层运行softmax操作，以获得词汇表中单词的概率分布。</p><p id="036e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jf translated"><span class="l jg jh ji bm jj jk jl jm jn di"> S </span> kip gram是word2vec的不同变体。与CBOW不同，在CBOW中，我们基于词汇表中的上下文单词来预测中心单词，在这里，我们试图通过预测特定单词周围的上下文单词来学习单词向量表示。该模型试图基于句子中的另一个单词来最大化一个单词的分类。理想情况下，依存窗口越长，单词向量的质量越好。作者还发现，这增加了复杂性，有时遥远的单词与正在建模的当前单词不太相关。作者在他们的原始论文中使用10的窗口大小进行训练，结果显示skip-gram模型在几个实验中优于CBOW。</p><p id="968e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在另一篇名为<a class="ae je" href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="jd">“单词和短语的分布式表示及其组合性</em> </a> <em class="jd">”的论文中，</em> Mikolov等人介绍了一种不同的方法来训练skip-gram模型，这显著提高了单词向量表示的训练速度&amp;质量。俗称<strong class="ih hj">负抽样。</strong>最初的skip-gram模型在3.2亿单词和82k词汇量的语料库上训练了8周！但是有了负采样，训练时间和计算复杂度就大大减少了。</p><p id="1ca2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么什么是负抽样呢？</p><p id="1ec6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在原始模型中，我们有维数为(vocab_size x word_vector_dim)的训练权重；这意味着根据词汇表的大小，模型的大小会以十亿计。因此，我们将在每个训练步骤期间更新这些参数中的每一个，这在计算上是昂贵的。使用否定采样，对于每个<strong class="ih hj">训练示例(输入单词)，</strong>我们将采样否定单词列表<strong class="ih hj">(例如，在所选输入单词的上下文中不存在的5个词汇单词)。</strong></p><p id="bd63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们在输出单词旁边使用这“5”个否定单词来学习分类器，该分类器为输出单词输出1，为否定单词输出0。这样，我们只更新这6个单词的训练权重，同时在每次迭代期间学习输入单词的单词向量。</p></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><h2 id="2a40" class="kr ks hi bd kz la lb lc ld le lf lg lh iq li lj lk iu ll lm ln iy lo lp lq lr bi translated">摘要</h2><p id="32c1" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq lu is it iu lv iw ix iy lw ja jb jc hb bi translated">虽然Word2Vec在NLP方面已经取得了很大的成功，但还有更近的架构，如<strong class="ih hj"> ELMo、</strong> <a class="ae je" href="https://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank"> <em class="jd">“深度上下文化的单词表示</em> </a>”<em class="jd"> </em>和<strong class="ih hj"> BERT </strong>、<em class="jd"> </em> <a class="ae je" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> <em class="jd"> BERT:用于语言理解的深度双向变形金刚前期训练</em></a>；这些模型能够创建高度特定于上下文的单词表示，使得可以为存在于同一语料库中的多个上下文中的单词生成适当的表示，而不是像word2vec那样的静态单词向量。</p><p id="4b11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来是一个PyTorch实现，使用负采样的skip-gram…</p><h2 id="8d2b" class="kr ks hi bd kz la lb lc ld le lf lg lh iq li lj lk iu ll lm ln iy lo lp lq lr bi translated">参考资料:</h2><ol class=""><li id="c163" class="lx ly hi ih b ii ls im lt iq lz iu ma iy mb jc mc md me mf bi translated"><a class="ae je" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="jd">对向量空间中单词表示的高效估计</em> </a></li><li id="b8c4" class="lx ly hi ih b ii mg im mh iq mi iu mj iy mk jc mc md me mf bi translated"><a class="ae je" href="https://www.researchgate.net/figure/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations_fig1_326588219" rel="noopener ugc nofollow" target="_blank"><em class="jd">https://www . research gate . net/figure/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations _ fig 1 _ 326588219</em></a></li><li id="69dd" class="lx ly hi ih b ii mg im mh iq mi iu mj iy mk jc mc md me mf bi translated"><a class="ae je" href="https://www.freethesaurus.com/bark" rel="noopener ugc nofollow" target="_blank"><em class="jd">https://www.freethesaurus.com</em></a></li></ol></div></div>    
</body>
</html>