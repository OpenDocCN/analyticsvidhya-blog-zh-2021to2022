<html>
<head>
<title>These football clubs don’t exist — Sharing my experience with StyleGAN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这些足球俱乐部并不存在——分享我和StyleGAN的经历</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/these-football-clubs-dont-exist-sharing-my-experience-with-stylegan-7d02e4b34914?source=collection_archive---------6-----------------------#2021-06-14">https://medium.com/analytics-vidhya/these-football-clubs-dont-exist-sharing-my-experience-with-stylegan-7d02e4b34914?source=collection_archive---------6-----------------------#2021-06-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="2fb2" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">使用约1K幅英国足球俱乐部标志图像的训练风格</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/67bbc6dd549b0bdbfb45382a911aa5a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRqw-DjtbC_DDKx18KItDQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">图片由作者提供。StyleGAN设计的足球俱乐部标志。</figcaption></figure><h1 id="ac05" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">这篇文章包括</h1><ul class=""><li id="8729" class="ke kf hh kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">通过网络搜集准备自定义数据集</li><li id="b3be" class="ke kf hh kg b kh kw kj kx kl ky kn kz kp la kr ks kt ku kv bi translated">一个简单但实用的使用自定义数据集训练风格的方法</li><li id="6d58" class="ke kf hh kg b kh kw kj kx kl ky kn kz kp la kr ks kt ku kv bi translated">如何在StyleGAN中获得潜在向量并操纵它们</li></ul><h1 id="05ec" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">介绍</h1><p id="5a61" class="pw-post-body-paragraph lb lc hh kg b kh ki ii ld kj kk il le kl lf lg lh kn li lj lk kp ll lm ln kr ha bi translated">生成对抗网络(GAN)是一种用于无监督学习的生成模型，自2014年由<a class="ae lo" href="https://arxiv.org/pdf/1406.2661" rel="noopener ugc nofollow" target="_blank"> Ian Goodfellow </a>首次开发以来，发展迅速。GAN的主要目标是学习真实世界数据的分布，并合成与原始真实世界数据难以区分的真实“假”数据。</p><p id="e408" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated"><a class="ae lo" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwi5r4u2ovDwAhUuzIsBHY75DzwQFjABegQIBBAD&amp;url=https%3A%2F%2Farxiv.org%2Fabs%2F1812.04948&amp;usg=AOvVaw1zXrhtPzBsKVrNqpiKiBgV" rel="noopener ugc nofollow" target="_blank">NVIDIA在2018年开发的一个基于风格的GANs </a> (StyleGAN)生成器架构，现在是最知名的GANs之一。StyleGAN不仅能够生成逼真的高分辨率图像，而且在解缠结方面也很强，因此能够进行精细的风格控制。关于原论文的详细解释，请参考本帖:</p><div class="lu lv ez fb lw lx"><a href="https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431" rel="noopener follow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">解释:一个基于风格的GANs生成器架构-生成和调整现实…</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">NVIDIA针对生成式对抗网络的新型架构</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">towardsdatascience.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml jg lx"/></div></div></a></div><p id="6b5b" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">由于StyleGAN最初是使用人脸数据训练的，因此在其他非人脸图像上微调StyleGAN很困难，因为很难建立一个大型的自定义数据集。StyleGAN有一些更新，在撰写本文时最新的版本是<a class="ae lo" href="https://arxiv.org/abs/2006.06676" rel="noopener ugc nofollow" target="_blank"> StyleGAN2，带有自适应鉴别器增强(ADA) </a>。最新版本的意义在于，你可以用相对有限的数据来训练网络，而不会过度拟合。</p><p id="f73b" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">幸运的是，NVIDIA research提供了StyleGAN所有版本的Github库，包括StyleGAN2-ADA的Tensorflow实现(最新版本是StyleGAN2-ADA的Pytorch实现):</p><div class="lu lv ez fb lw lx"><a href="https://github.com/NVlabs/stylegan2-ada" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">NVlabs/stylegan2-ada</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">用有限数据训练生成性对抗网络</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">github.com</p></div></div><div class="mg l"><div class="mm l mi mj mk mg ml jg lx"/></div></div></a></div><p id="5678" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">因此，我想在足球俱乐部徽标图像上试用StyleGAN2-ADA，并与那些希望用相对较小的自定义数据集训练StyleGAN的人分享我的经验。</p><p id="aa5c" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">让我们从克隆StyleGAN2-ADA存储库开始:</p><pre class="ix iy iz ja fd mn mo mp mq aw mr bi"><span id="bfdf" class="ms jn hh mo b fi mt mu l mv mw">git clone https://github.com/NVlabs/stylegan2-ada.git</span></pre><p id="bc3f" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">这些是我用过的包:</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mx my l"/></div></figure><h1 id="c39d" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">来自维基百科的网络抓取</h1><p id="7cb9" class="pw-post-body-paragraph lb lc hh kg b kh ki ii ld kj kk il le kl lf lg lh kn li lj lk kp ll lm ln kr ha bi translated">由于我找不到任何公开的数据集，我决定从互联网上收集图片。虽然世界各地都有足球俱乐部，但我还是选择了英格兰的俱乐部，以便更容易地浏览网页。在撰写本文时(2020-2021赛季)，根据<a class="ae lo" href="https://en.wikipedia.org/wiki/List_of_football_clubs_in_England" rel="noopener ugc nofollow" target="_blank">这个维基百科页面</a>，有1056个俱乐部在英格兰足球联赛系统内竞争。我注意到这些俱乐部的每个维基百科页面都在HTML类“infobox-image”下包含一个徽标图像，所以网络抓取并不困难。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mz"><img src="../Images/5d73100c341fd2bdb55d56e407534724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5AYSrdC_rcKKnotpWkfbYg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">摘自<a class="ae lo" href="https://en.wikipedia.org/wiki/Manchester_City_F.C." rel="noopener ugc nofollow" target="_blank">维基百科</a></figcaption></figure><p id="7d18" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">我使用<code class="du na nb nc mo b">requests</code> + <code class="du na nb nc mo b">bs4</code> (BeautifulSoup)+ <code class="du na nb nc mo b">urllib</code>从维基百科页面中提取俱乐部标志图片的源URL。我不打算详细介绍代码，因为它是我的数据集特有的。我用于网页抓取的代码如下:</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mx my l"/></div></figure><h1 id="455c" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">正在为StyleGAN准备数据集</h1><p id="ed0e" class="pw-post-body-paragraph lb lc hh kg b kh ki ii ld kj kk il le kl lf lg lh kn li lj lk kp ll lm ln kr ha bi translated">在删除了来自同一地区的一些带有相同标志的俱乐部和那些在维基百科页面上没有标志图像的俱乐部后，我最终得到了总共1030个俱乐部标志。大多数下载的图像是4通道的PNG文件，它们的宽度和高度从100像素到400像素不等。</p><p id="eb19" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">现在，对于StyleGAN要使用的输入数据有一些要求。</p><ul class=""><li id="a5bb" class="ke kf hh kg b kh lp kj lq kl nd kn ne kp nf kr ks kt ku kv bi translated">必须有3个通道(RGB)</li><li id="b827" class="ke kf hh kg b kh kw kj kx kl ky kn kz kp la kr ks kt ku kv bi translated">必须具有相同的宽度和高度</li><li id="6e3f" class="ke kf hh kg b kh kw kj kx kl ky kn kz kp la kr ks kt ku kv bi translated">宽度和高度必须是2的幂(256，512等。)</li><li id="1438" class="ke kf hh kg b kh kw kj kx kl ky kn kz kp la kr ks kt ku kv bi translated">宽度和高度至少应为128(使用预训练模型)</li></ul><p id="aaf9" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">首先，我必须在边界添加白色像素，使图像呈方形，并将透明像素(PNG的第四通道)转换为白色。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mx my l"/></div></figure><p id="d415" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">接下来，我选择将图片尺寸调整为128×128。然而，由于有小于128像素的图像，调整这些图像的大小会产生非常差的图像质量。因此，我使用了<a class="ae lo" href="https://arxiv.org/abs/1802.08797" rel="noopener ugc nofollow" target="_blank">剩余密集网络模型</a>(谢天谢地，一个预先训练的模型是<a class="ae lo" href="https://github.com/idealo/image-super-resolution" rel="noopener ugc nofollow" target="_blank">可用的</a>！)从调整大小前的小图像中创建<strong class="kg hi"> <em class="ng">超分辨率</em> </strong>图像。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mx my l"/></div></figure><p id="fd09" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">数据集现在准备好了！</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nh"><img src="../Images/b667556e8f5588b234c7733d56e71c7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ex9f_Zve4e36HTG1j8AWbg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">数据集中的例子。</figcaption></figure><h1 id="bab7" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">培养</h1><p id="5692" class="pw-post-body-paragraph lb lc hh kg b kh ki ii ld kj kk il le kl lf lg lh kn li lj lk kp ll lm ln kr ha bi translated">我使用了NVIDIA的官方TensorFlow实现StyleGAN2-ADA ,我的超参数配置也是基于这个库中的例子。</p><p id="fa8e" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">在定型之前，请将数据集转换为TFRecords。</p><pre class="ix iy iz ja fd mn mo mp mq aw mr bi"><span id="b247" class="ms jn hh mo b fi mt mu l mv mw">python dataset_tool.py create_from_images ./datasets/logo ../YOUR_DIRECTORY</span></pre><p id="5156" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">我的训练分为两个阶段:</p><pre class="ix iy iz ja fd mn mo mp mq aw mr bi"><span id="201c" class="ms jn hh mo b fi mt mu l mv mw">#1<br/>python train.py --outdir=./training-runs --gpus=1 --data=./datasets/logo --res=128 --kimg=1000 --mirror=True --gamma=16 --augpipe=bgcfnc --freezed=2 --resume=ffhq256</span><span id="c78a" class="ms jn hh mo b fi ni mu l mv mw">#2<br/>python train.py --outdir=./training-runs --gpus=1 --data=./datasets/logo --res=128 --kimg=2000 --mirror=True --gamma=16 --augpipe=bgcfnc --resume=PATH_LAST_NETWORK_FROM_1</span></pre><ol class=""><li id="164b" class="ke kf hh kg b kh lp kj lq kl nd kn ne kp nf kr nj kt ku kv bi translated">从FFHQ转移学习培训256 +冻结</li><li id="101e" class="ke kf hh kg b kh kw kj kx kl ky kn kz kp la kr nj kt ku kv bi translated">从1开始恢复，不冻结</li></ol><p id="8bbd" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated"><a class="ae lo" href="https://arxiv.org/abs/2002.10964" rel="noopener ugc nofollow" target="_blank">冻结</a>是一种利用冻结的鉴别器低层进行微调的技术，它被认为是有效的，尤其是在数据有限的情况下。</p><p id="00f7" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">该培训是在一个具有1 Tesla V100 GPU的GCP虚拟机实例上进行的。第一阶段耗时5.5小时，第二阶段耗时11小时。为了监控训练过程，您可以定期检查FID指标或StyleGAN创建的样本图像(由<code class="du na nb nc mo b">--snap</code>控制)。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nk"><img src="../Images/76962acdae88a023c4dfd33a7a07cee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AqRA0pD4BTsbI2ujlP9hSg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">图片由作者提供。FID度量。左:ffhq 256+冻结，右:从左开始恢复整个图层。</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/43d8200f7a1a3d683ce5c8042a23ec7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vgrmiakLpNtF87TJmaDuaQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">图片由作者提供。我训练过程的最后一个例子。</figcaption></figure><p id="d5b3" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">训练甘可能非常棘手。如果您使用自己的数据集，您应该使用不同的超参数尝试不同的方法。我就是这样做的。请在<a class="ae lo" href="https://github.com/NVlabs/stylegan2-ada" rel="noopener ugc nofollow" target="_blank">官方风格库</a>或本帖中查看更好的微调建议:</p><div class="lu lv ez fb lw lx"><a href="https://towardsdatascience.com/stylegan-v2-notes-on-training-and-latent-space-exploration-e51cf96584b3" rel="noopener follow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">StyleGAN v2:训练和潜在空间探索笔记</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">一个笔记和结果收集的集合，同时训练多种风格和探索学习的潜在模型</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">towardsdatascience.com</p></div></div><div class="mg l"><div class="nl l mi mj mk mg ml jg lx"/></div></div></a></div><h1 id="ef71" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">探索潜在空间</h1><p id="7d07" class="pw-post-body-paragraph lb lc hh kg b kh ki ii ld kj kk il le kl lf lg lh kn li lj lk kp ll lm ln kr ha bi translated">现在我们有了一个训练有素的生成高质量图像的StyleGAN。为了控制生成图像的风格，您需要知道哪个输入(在GAN中称为潜在向量或潜在代码)映射到哪个输出。在GAN中操纵输出的基本概念是，如果你从点A到B“在潜在空间中行走”，你也将获得从G(A)到G(B)的输出的平滑过渡。</p><p id="94c6" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">与大多数以前的GANs不同，StyleGAN利用了传统潜在向量和输出之间的“中间向量”,众所周知，这有助于StyleGAN中出色的特征解缠。你实际上只需要中间的潜在向量来控制风格。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nm"><img src="../Images/fffcc8b0630db09cf585bcd16395bc7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Di9C0OZFqvPJbDulBLQ2g.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated"><a class="ae lo" href="https://arxiv.org/pdf/1812.04948.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>。注意中间的潜在空间<strong class="bd jo"> W </strong>。</figcaption></figure><p id="d832" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">有两种方法可以获得中间潜在向量:</p><ol class=""><li id="8517" class="ke kf hh kg b kh lp kj lq kl nd kn ne kp nf kr nj kt ku kv bi translated">使用随机输入生成图像，检查生成的图像，并选择您想要处理的图像</li><li id="b691" class="ke kf hh kg b kh kw kj kx kl ky kn kz kp la kr nj kt ku kv bi translated">首先选择要操作的图像，并找到尽可能靠近这些图像的映射到输出的输入</li></ol><h2 id="a833" class="ms jn hh bd jo nn no np js nq nr ns jw kl nt nu jy kn nv nw ka kp nx ny kc nz bi translated">如何生成随机图像</h2><p id="fab2" class="pw-post-body-paragraph lb lc hh kg b kh ki ii ld kj kk il le kl lf lg lh kn li lj lk kp ll lm ln kr ha bi translated">像任何其他GANs一样，您可以在StyleGAN中从潜在空间中的随机向量生成输出。然而，在将随机潜在向量映射到中间向量之后，StyleGAN并不直接使用中间向量来生成新的图像，而是使用“<strong class="kg hi">截断技巧”</strong>。为了避免在训练数据的分布之外生成过于随机的图像，StyleGAN只对位于训练数据的平均中间向量周围的特定范围内的中间向量进行采样。确定的范围由原论文和代码中的一个变量<strong class="kg hi"> <em class="ng"> truncation_psi ψ </em> </strong>决定。鉴于此，您可以生成随机图像，并确保您保留中间向量。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mx my l"/></div></figure><h2 id="804f" class="ms jn hh bd jo nn no np js nq nr ns jw kl nt nu jy kn nv nw ka kp nx ny kc nz bi translated">如何将图像投射到潜在空间</h2><p id="8dbd" class="pw-post-body-paragraph lb lc hh kg b kh ki ii ld kj kk il le kl lf lg lh kn li lj lk kp ll lm ln kr ha bi translated">您可以反其道而行之，先获取图像，然后找到相应的输入，而不是先对随机输入进行采样并检查输出。官方的StyleGAN库提供了一个投影仪代码，它通过一个迭代过程来完成这项工作。</p><pre class="ix iy iz ja fd mn mo mp mq aw mr bi"><span id="21f7" class="ms jn hh mo b fi mt mu l mv mw">python projector.py --outdir=out --target=targetimg.png --save_video=False --network=YOUR_NETWORK_PATH</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es oa"><img src="../Images/9c342ccd240ea9f061cf096d27041d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*PVCSA_NF7BkCNbu6UuAQLQ.gif"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">图片由作者提供。将图像投影到潜在空间1000次迭代的例子。</figcaption></figure><p id="717c" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">我选择了第二个选项——将图像投射到潜在空间——因为在许多随机输出中进行选择很麻烦。然而，投影仪的缺点是速度慢，有时不准确。你可以使用一个编码器模型作为替代，但我不会在这篇文章中详细介绍。如果您感兴趣:</p><div class="lu lv ez fb lw lx"><a href="https://arxiv.org/abs/2008.00951" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">风格编码:一种用于图像到图像翻译的风格编码器</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">我们提出了一个通用的图像到图像的翻译框架，像素2样式2像素(pSp)。我们的pSp框架是基于…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">arxiv.org</p></div></div></div></a></div><h1 id="5bce" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">插入文字</h1><p id="e240" class="pw-post-body-paragraph lb lc hh kg b kh ki ii ld kj kk il le kl lf lg lh kn li lj lk kp ll lm ln kr ha bi translated">如果您使用官方StyleGAN存储库中的<code class="du na nb nc mo b">projector.py</code>，输出目录将包含一个<code class="du na nb nc mo b">dlatents.npz</code>文件以及目标和投影的图像。<code class="du na nb nc mo b">dlatents.npz</code>是具有投影的中间潜在向量的文件，所以你现在可以用它操纵潜在空间。</p><p id="5513" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">一件事是你可以在潜在空间的两个点之间插值，你也可以在图像之间得到平滑的插值。编码相对简单。你只需要在两个潜在向量之间进行线性插值，然后运行生成模型。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mx my l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ob"><img src="../Images/f02c4c3e34cde1fcebf51a62a3864e4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/1*Tt20YQAjlq3aqTLCdKaqkQ.gif"/></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">图片由作者提供。插值的例子。</figcaption></figure><h1 id="2cac" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">风格混合</h1><p id="6544" class="pw-post-body-paragraph lb lc hh kg b kh ki ii ld kj kk il le kl lf lg lh kn li lj lk kp ll lm ln kr ha bi translated">插值很酷，但是中点的两个潜在向量可能不会像你想要的那样与两幅图像的混合版本相关联。这是因为特征是有层次的:从粗糙的(如姿势、形状)到精细的(如配色方案、微观结构)。正如在简介中提到的，StyleGAN以其在特性解缠方面的优势而闻名。</p><p id="20c1" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">中间向量变量<code class="du na nb nc mo b">dlatents</code>的一个元素具有<code class="du na nb nc mo b">(1,12,512)</code>的形状，实际上是12个相同的512大小的中间向量的堆栈。你可以用<code class="du na nb nc mo b">all(dlatent[0][i]==dlatent[0][j])</code>来检查这个，对于0~11中的任何一个(I，j)。这种结构与StyleGAN的音阶特异性控制能力有关。您可以根据所操纵的层来控制高级到低级的属性。风格混合的代码如下:</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mx my l"/></div></figure><p id="6133" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">上面代码的关键一行是:</p><pre class="ix iy iz ja fd mn mo mp mq aw mr bi"><span id="a838" class="ms jn hh mo b fi mt mu l mv mw">mix[0][:6] = dlatents[t][0][:6]</span></pre><p id="0dbd" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">它混合了两幅图像的风格。您可以根据想要控制的样式级别来更改要切换的图层。</p><p id="55d0" class="pw-post-body-paragraph lb lc hh kg b kh lp ii ld kj lq il le kl lr lg lh kn ls lj lk kp lt lm ln kr ha bi translated">我创建了一个样式混合的交叉表，用六个图像作为源和目标，这样我可以看到控制粗略或精细样式的效果。注意同一行中的混合图像与目标具有相同的颜色组合，同一列中的混合图像与源具有相同的形状。我很惊讶的是，即使使用大约1K的训练图像，特定比例的控制风格GAN也显示出如此好的效果。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es oc"><img src="../Images/1e985e6c8df1c684a9d03a32418aef1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6szQUgddLGr8E_LMvJgfjw.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx translated">图片由作者提供。注意风格混合:源的形状+目标的颜色。</figcaption></figure><h1 id="5519" class="jm jn hh bd jo jp jq jr js jt ju jv jw in jx io jy iq jz ir ka it kb iu kc kd bi translated">结论</h1><p id="d857" class="pw-post-body-paragraph lb lc hh kg b kh ki ii ld kj kk il le kl lf lg lh kn li lj lk kp ll lm ln kr ha bi translated">你可以训练StyleGAN2-ADA，这是目前NVIDIA最新版本的StyleGAN，允许用有限的数据进行微调，具有约1K图像的自定义数据集。操纵潜在空间是可能的，并且在定制数据集上训练的StyleGAN的结果是令人满意的。</p></div></div>    
</body>
</html>