<html>
<head>
<title>Simple CNN using NumPy Part IV (Back Propagation Through Fully Connected Layers)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用NumPy的简单CNN第四部分(通过全连接层的反向传播)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/simple-cnn-using-numpy-part-iv-back-propagation-through-fully-connected-layers-c5035d678307?source=collection_archive---------16-----------------------#2021-06-20">https://medium.com/analytics-vidhya/simple-cnn-using-numpy-part-iv-back-propagation-through-fully-connected-layers-c5035d678307?source=collection_archive---------16-----------------------#2021-06-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div class="er es hf"><img src="../Images/69c29457efab1e78ebcba171c589b704.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*noL5Bt0f-OHu-9D4RmKIzw.png"/></div></figure><div class=""/><p id="b267" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在前面的章节中，我们介绍了</p><ol class=""><li id="ae77" class="jj jk ho in b io ip is it iw jl ja jm je jn ji jo jp jq jr bi translated"><a class="ae js" rel="noopener" href="/@PAdhokshaja/simple-cnn-using-numpy-part-i-introduction-data-processing-b6652615604d">数据处理</a></li><li id="a672" class="jj jk ho in b io jt is ju iw jv ja jw je jx ji jo jp jq jr bi translated"><a class="ae js" rel="noopener" href="/@PAdhokshaja/simple-cnn-using-numpy-part-ii-convolution-operation-b8c5a02b0844">卷积</a></li><li id="4772" class="jj jk ho in b io jt is ju iw jv ja jw je jx ji jo jp jq jr bi translated"><a class="ae js" rel="noopener" href="/@PAdhokshaja/simple-cnn-using-numpy-part-iii-relu-max-pooling-softmax-c03a3377eaf2"> ReLU，最大汇集&amp;软最大功能</a></li></ol><p id="662e" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在CNN这个系列的第四部分，我们将尝试通过网络中完全连接的层来覆盖反向传播。</p><p id="e9c0" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">简单回顾一下，我们的网络从头到尾有以下一系列操作</p><p id="412e" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hp"> <em class="jy">设N为图像总数</em> </strong></p><ul class=""><li id="15ce" class="jj jk ho in b io ip is it iw jl ja jm je jn ji jz jp jq jr bi translated">它接受大小为(<strong class="in hp"> N，1，28，28 </strong>)的输入矩阵</li><li id="af69" class="jj jk ho in b io jt is ju iw jv ja jw je jx ji jz jp jq jr bi translated">随后是大小为(<strong class="in hp"> 2，1，5，5 </strong>)的第一个卷积滤波器</li><li id="4242" class="jj jk ho in b io jt is ju iw jv ja jw je jx ji jz jp jq jr bi translated">卷积运算导致矩阵转换为大小(<strong class="in hp"> N，2，24，24 </strong>)。这个结果被传递给一个ReLU函数。</li><li id="bdd7" class="jj jk ho in b io jt is ju iw jv ja jw je jx ji jz jp jq jr bi translated">跨距为2的(2x2)过滤器的最大池操作。这产生了一个大小为(<strong class="in hp"> N，2，12，12 </strong>)的矩阵</li><li id="1443" class="jj jk ho in b io jt is ju iw jv ja jw je jx ji jz jp jq jr bi translated">然后我们将它展平成一个大小为(<strong class="in hp"> 288，N </strong>)的数组</li><li id="2587" class="jj jk ho in b io jt is ju iw jv ja jw je jx ji jz jp jq jr bi translated">接下来是矩阵乘法，将数组转换成形状(<strong class="in hp"> 60，N </strong>)。这个结果被提供给ReLU函数。</li><li id="1a2d" class="jj jk ho in b io jt is ju iw jv ja jw je jx ji jz jp jq jr bi translated">最后的操作包括将前一层的结果乘以另一个矩阵，该矩阵将形状更改为(<strong class="in hp"> 10，N </strong>)。该结果被馈送到软最大值函数。</li></ul><figure class="kb kc kd ke fd hj er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es ka"><img src="../Images/fd879d698f8d06d100cff7e014a2bdc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FSHmitYkMsqog0pSg5GqQw.png"/></div></div><figcaption class="kj kk et er es kl km bd b be z dx translated">我们CNN的草图</figcaption></figure><p id="0336" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们初始化的模型参数将导致不正确的估计。估计值和实际标签之间的偏差通过称为交叉熵的度量来量化。交叉熵通过一个称为梯度下降的过程迭代地减少。</p><figure class="kb kc kd ke fd hj er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es kn"><img src="../Images/48f42643e4ae0f06291bc1a77c3caab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E_7SxMKYzIP6IUnGkOJ_OQ.png"/></div></div><figcaption class="kj kk et er es kl km bd b be z dx translated">交叉熵方程</figcaption></figure><p id="e525" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hp">交叉熵背后的直觉</strong></p><p id="1735" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">让我们举一个简单的例子，我们有三个类。让实际标签的一个热编码表示为[1，0，0]。让我们来看几个场景。</p><ol class=""><li id="daf9" class="jj jk ho in b io ip is it iw jl ja jm je jn ji jo jp jq jr bi translated"><strong class="in hp">设预测输出为【1，0，0】</strong>。交叉熵误差将是(-1 * log(1))+(-0 * log(0))+(-0 * log(0))，即0。</li><li id="b2f8" class="jj jk ho in b io jt is ju iw jv ja jw je jx ji jo jp jq jr bi translated"><strong class="in hp">设预测输出为【0，1，0】</strong>。这里的交叉熵误差将是(-1 * log(0))+(-1 * log(1))+(-0 * log(0))。由于log(0)是一个无限大的负值&amp;，所以误差会无限大。</li><li id="3791" class="jj jk ho in b io jt is ju iw jv ja jw je jx ji jo jp jq jr bi translated"><strong class="in hp">设预测输出为[0，0，1] </strong>。交叉熵误差将是(-1 * log(0))+(-0 * log(0))+(-0 * log(1))。这将导致无限高的误差。</li></ol><p id="64ff" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">交叉熵惩罚给出高置信度错误输出的分类器。</p><p id="1070" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">所以在学习过程中，我们需要减少交叉熵。这是通过一个叫做梯度下降的过程完成的。梯度下降是一个迭代过程，它调整模型的参数，使输出尽可能接近实际输出</p><figure class="kb kc kd ke fd hj er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es ko"><img src="../Images/11988f1b38d69733920994501106e7d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2QplH6jO_NV9YlAc41o0A.png"/></div></div><figcaption class="kj kk et er es kl km bd b be z dx translated">梯度下降</figcaption></figure><p id="09c1" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们将其扩展到完全连接的层中的模型参数。</p><figure class="kb kc kd ke fd hj er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es kp"><img src="../Images/f7a6a06de3cf55eac6d2537562da07c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*spKFmHIj8KZt9_YB1laiuQ.png"/></div></div><figcaption class="kj kk et er es kl km bd b be z dx translated">全连接层中的梯度下降</figcaption></figure><p id="f779" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这里，希腊字母alpha指的是学习率。这决定了模型收敛到最小值需要多长时间。</p><h1 id="1838" class="kq kr ho bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">通过完全连接的层的梯度下降</h1><h2 id="b289" class="lo kr ho bd ks lp lq lr kw ls lt lu la iw lv lw le ja lx ly li je lz ma lm mb bi translated">计算交叉熵相对于W2和B1的导数</h2><figure class="kb kc kd ke fd hj er es paragraph-image"><div class="er es mc"><img src="../Images/90a68affe6a2ddd87e5fd3082fa62e39.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*rItWIdau59DQuXVzgmeSTw.png"/></div></figure><figure class="kb kc kd ke fd hj er es paragraph-image"><div class="er es md"><img src="../Images/39690eb90641e561ffd1df753c74e1a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*WyOvHBJbUUwuiZwbSPwO3Q.png"/></div></figure><figure class="kb kc kd ke fd hj er es paragraph-image"><div class="er es me"><img src="../Images/3c21fedc1b6717029d979c51093b58c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*hDdHs5OCKKFfSorarGHYVw.png"/></div></figure><figure class="kb kc kd ke fd hj er es paragraph-image"><div class="er es md"><img src="../Images/b1b161ec385c85becd7651efab14e0f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*wgcJBJvN07teYUSccnteuw.png"/></div><figcaption class="kj kk et er es kl km bd b be z dx translated">关于W2的推导误差</figcaption></figure><p id="457d" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">下面的代码片段实现了上述功能</p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="9a81" class="lo kr ho mg b fi mk ml l mm mn">delta_2 = (final_fc-y_batch)</span><span id="ac74" class="lo kr ho mg b fi mo ml l mm mn">dW2 = delta_2@fc1.T<br/>dB1 = np.sum(delta_2,axis=1,keepdims=True)</span></pre><h2 id="9759" class="lo kr ho bd ks lp lq lr kw ls lt lu la iw lv lw le ja lx ly li je lz ma lm mb bi translated">计算交叉熵相对于W1和B0的导数</h2><figure class="kb kc kd ke fd hj er es paragraph-image"><div class="er es mp"><img src="../Images/f553b4577e1c7495fa0ae09ed57aabbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*YOo3-MRKQpWA_zrf5_50Og.png"/></div><figcaption class="kj kk et er es kl km bd b be z dx translated">损失函数关于W1和B0的导数的求导。</figcaption></figure><p id="0369" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">下面的代码片段计算损失函数相对于W1和B0的导数。</p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="f94d" class="lo kr ho mg b fi mk ml l mm mn">delta_1 = np.multiply(W2.T@delta_2,dReLU(W1@X_maxpool_flatten+B0))<br/>dW1 = delta_1@X_maxpool_flatten.T<br/>dB0 = np.sum(delta_1,axis=1,keepdims=True)</span></pre><p id="3adc" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">上面的dReLU()函数是ReLU函数的派生物。ReLU函数的导数将每个正值转换为1，而其余的值则为零。该函数编写如下</p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="de85" class="lo kr ho mg b fi mk ml l mm mn">def dReLU(x):<br/>    return (x&gt;0)*1.0</span></pre><h2 id="a19e" class="lo kr ho bd ks lp lq lr kw ls lt lu la iw lv lw le ja lx ly li je lz ma lm mb bi translated">计算相对于X0的导数</h2><p id="4d3e" class="pw-post-body-paragraph il im ho in b io mq iq ir is mr iu iv iw ms iy iz ja mt jc jd je mu jg jh ji ha bi translated">我们需要计算损失函数对X0的导数。这将用于计算卷积滤波器的导数。其推导过程如下。</p><figure class="kb kc kd ke fd hj er es paragraph-image"><div class="er es mv"><img src="../Images/634ed298e760eda6d6417812c2d18a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*mhhAu63mwfcUMatHbg7BKg.png"/></div><figcaption class="kj kk et er es kl km bd b be z dx translated">损失函数关于X0的导数</figcaption></figure><p id="af20" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">下面的代码片段找到了损失函数相对于X0的导数</p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="e39c" class="lo kr ho mg b fi mk ml l mm mn">delta_0 = np.multiply(W1.T@delta_1,1.0)</span></pre><p id="a7dc" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">鉴于每个模型参数相对于损失函数的误差是从网络的末端到前端计算的，这个过程也称为“反向传播”。</p><p id="958f" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在下一部分中，我们将尝试计算卷积滤波器损失函数的误差。</p><h1 id="af93" class="kq kr ho bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">资源</h1><ul class=""><li id="d386" class="jj jk ho in b io mq is mr iw mw ja mx je my ji jz jp jq jr bi translated"><a class="ae js" href="https://www.youtube.com/watch?v=5-rVLSc2XdE" rel="noopener ugc nofollow" target="_blank">用交叉熵和softmax寻找图层错误的视频教程</a></li><li id="27c1" class="jj jk ho in b io jt is ju iw jv ja jw je jx ji jz jp jq jr bi translated"><a class="ae js" href="http://neuralnetworksanddeeplearning.com/" rel="noopener ugc nofollow" target="_blank">迈克尔·尼尔森关于神经网络的博客</a></li></ul><h1 id="76eb" class="kq kr ho bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">反馈</h1><p id="d1f7" class="pw-post-body-paragraph il im ho in b io mq iq ir is mr iu iv iw ms iy iz ja mt jc jd je mu jg jh ji ha bi translated">感谢您的阅读！如果您有任何反馈/建议，请随时在下面评论，或者您可以发电子邮件到padhokshaja@gmail.com给我</p><h1 id="74c2" class="kq kr ho bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">下一篇文章</h1><p id="1a71" class="pw-post-body-paragraph il im ho in b io mq iq ir is mr iu iv iw ms iy iz ja mt jc jd je mu jg jh ji ha bi translated"><a class="ae js" rel="noopener" href="/@PAdhokshaja/simple-cnn-using-numpy-part-v-back-propagation-through-max-pool-layer-convolutional-filter-7c434a7addd4">通过max pooling滤波器的反向传播，卷积输出层&amp;卷积滤波器。</a></p></div></div>    
</body>
</html>