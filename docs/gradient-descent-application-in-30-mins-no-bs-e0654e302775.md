# 30 åˆ†é’Ÿå†…æ¢¯åº¦ä¸‹é™åº”ç”¨(æ—  BS)

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/gradient-descent-application-in-30-mins-no-bs-e0654e302775?source=collection_archive---------2----------------------->

è®©æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªç®€å•çš„å¤šé¡¹å¼æ¨¡å‹:

![](img/37820ae6edd52b34e3b357358eed6e1c.png)

æˆ‘ä»¬çš„æ¨¡å‹ç¤ºä¾‹

```
def sin_model(x, theta):
    """
    Predict the estimate of y given x, theta_1, theta_2 Keyword arguments:
    x -- the vector of values x
    theta -- a vector of length 2
    """ theta_1 = theta[0]
    theta_2 = theta[1]
    return theta_1 * x + np.sin(theta_2 * x)
```

**æ¢¯åº¦ä¸‹é™èƒŒåçš„ç›´è§‰:**

1.  æœ€ä½³Î¸å€¼æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚
2.  æˆ‘ä»¬æ‰¾åˆ°Î¸æœ€å°å€¼çš„æ–¹æ³•æ˜¯**é€šè¿‡å¯¹æŸå¤±å‡½æ•°å¯¹Î¸**æ±‚å¯¼

ç¬¬ä¸€æ­¥æ˜¯ä¸ºæˆ‘ä»¬çš„æ¨¡å‹é€‰æ‹©ä¸€ä¸ªæŸå¤±å‡½æ•°ã€‚

å‡è®¾æˆ‘ä»¬ä½¿ç”¨å‡æ–¹æŸå¤±å‡½æ•°ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå› æ­¤:((y_hat â€” y_obs) ** 2) / n

```
def sin_MSE(theta, x, y):
    """
    Compute the numerical value of the l2 loss of our sinusoidal model given thetaKeyword arguments:
    theta -- the vector of values theta
    x     -- the vector of x values
    y     -- the vector of y values
    """ y_hat = sin_model(x, theta)
    return np.mean((y - y_hat)** 2)
```

æˆ‘ä»¬å°†è¿™ä¸ªæŸå¤±å‡½æ•°å¯¹Î¸_ 1 å’ŒÎ¸_ 2 æ±‚å¯¼ï¼Œæˆ‘ä»¬å¾—åˆ°å¦‚ä¸‹ç»“æœ:

![](img/db052b0e3b683adebbb6073007bcda3b.png)

```
def sin_MSE_dt1(theta, x, y):
    """
    Compute the numerical value of the partial of l2 loss with respect to theta_1Keyword arguments:
    theta -- the vector of values theta
    x     -- the vector of x values
    y     -- the vector of y values
    """

    theta_1 = theta[0]
    theta_2 = theta[1]

    dt_1 = -2 * (y - theta_1 * x - np.sin(theta_2 * x)) * x
    return np.mean(dt_1)

def sin_MSE_dt2(theta, x, y):
    """
    Compute the numerical value of the partial of l2 loss with respect to theta_2Keyword arguments:
    theta -- the vector of values theta
    x     -- the vector of x values
    y     -- the vector of y values
    """
    theta_1 = theta[0]
    theta_2 = theta[1]

    dt_2 = -2 * (y - theta_1 * x - np.sin(theta_2 * x)) * x * np.cos(theta_2 * x)
    return np.mean(dt_2)
```

æœ€åï¼Œæˆ‘ä»¬å°†å®ƒä»¬æ”¾åœ¨ä¸€ä¸ªæ•´ä½“æ¢¯åº¦æŸå¤±å‡½æ•°ä¸­

```
def sin_MSE_gradient(theta, x, y):
    """
    Returns the gradient of l2 loss with respect to vector thetaKeyword arguments:
    theta -- the vector of values theta
    x     -- the vector of x values
    y     -- the vector of y values
    """
    return np.array([sin_MSE_dt1(theta, x, y), sin_MSE_dt2(theta, x, y)])
```

## å®ç°æ¢¯åº¦ä¸‹é™åŠŸèƒ½

æœ‰å¾ˆå¤šæ–¹æ³•å¯ä»¥å†™å‡ºä½ çš„æ¢¯åº¦ä¸‹é™å‡½æ•°ï¼›è¯·è®°ä½ï¼Œå¯¹æ‚¨æ¥è¯´ï¼Œæœ€é‡è¦çš„äº‹æƒ…æ˜¯ç†è§£æ‰€æœ‰è¿™äº›å®ç°èƒŒåçš„é€»è¾‘ï¼Œå¹¶åœ¨æ‚¨è‡ªå·±çš„åˆ†æä¸­æ‰¾åˆ°å¯¹æ‚¨æ¥è¯´æœ€æœ‰æ„ä¹‰çš„å®ç°ã€‚

**éœ€è¦è®°ä½çš„ä¸€äº›ç›´è§‰:**

*   æˆ‘ä»¬è¯•å›¾æ‰¾åˆ°å¯¼è‡´æŸå¤±å‡½æ•°æ•´ä½“æœ€å°å€¼çš„Î¸_ 1 å’ŒÎ¸_ 2 çš„ç»„åˆ
*   æ¢¯åº¦ä¸‹é™åœ¨è´Ÿæ¢¯åº¦æ–¹å‘è½»æ¨Î¸ï¼Œç›´åˆ°Î¸æ”¶æ•›ã€‚

æˆ‘ä»¬å¼€å§‹å§ï¼

1.  **å®æ–½æ¢¯åº¦ä¸‹é™çš„é€æ­¥æŒ‡å—**

*   å…¶ä¸­`df`æ˜¯æˆ‘ä»¬è¦æœ€å°åŒ–çš„å‡½æ•°çš„æ¢¯åº¦(åˆåã€‚æŸå¤±å‡½æ•°)å’Œ`initial_guess`æ˜¯è¯¥å‡½æ•°çš„èµ·å§‹å‚æ•°(ä»»æ„çš„)ã€‚

```
**def** gradient_descent(df, initial_guess, alpha, n):
    guesses = [initial_guess]
    guess = initial_guess **while** len(guesses) < n:
        guess = guess - alpha * df(guess)
        guesses.append(guess)
    **return** np.array(guesses)
```

æ­£å¦‚æ‚¨å¯èƒ½æ³¨æ„åˆ°çš„ï¼Œæˆ‘ä»¬çš„ sin_MSE_gradient å‡½æ•°æ¥å— 3 ä¸ªå‚æ•°(thetaï¼Œxï¼Œy)ï¼Œå¦‚æœæˆ‘ä»¬æƒ³åœ¨ gradient_descent å‡½æ•°ä¸­ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›å¯¹å®ƒè¿›è¡Œå¦‚ä¸‹ä¼˜åŒ–ï¼Œä»¥ä¾¿å®ƒåªæ¥å— 1 ä¸ªå‚æ•°(theta):

```
**def** mse_loss_derivative_single_arg(theta):
    x = data['a', 'b']
    y_obs = data['y']

    **return** sin_MSE_gradient(theta, x, y_obs)
```

ç„¶åè®©æˆ‘ä»¬å°è¯•è¿è¡Œæ¢¯åº¦ä¸‹é™å‡½æ•°ï¼Œåˆå§‹Î¸å‘é‡[0ï¼Œ0]ï¼Œalpha = 0.001ï¼Œmax_iteration = 100:

```
gradient_descent(mse_loss_derivative_single_arg, np.array([0, 0]), 0.0001, 100)
```

2.**æœ‰é™æ¬¡è¿­ä»£çš„æ¢¯åº¦ä¸‹é™**

![](img/f0a647f53daa40e05026e2ca59d4d210.png)

æ›´æ–°åŠŸèƒ½

åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œä½¿ç”¨æ¢¯åº¦å’Œ`alpha`æ¥æ›´æ–°ä½ å½“å‰çš„`theta`ã€‚æˆ‘ä»¬è¿˜å°†ç”µæµ`theta`ä¿å­˜åœ¨`theta_history`ä¸­ï¼ŒåŒæ—¶å°†å¹³å‡å¹³æ–¹æŸè€—(ç”¨ç”µæµ`theta`è®¡ç®—)ä¿å­˜åœ¨`loss_history`ä¸­

*   è®©æˆ‘ä»¬é¦–å…ˆå®šä¹‰åˆå§‹Î¸å€¼ï¼Œå®ƒå®é™…ä¸Šæ˜¯å›¾ä¸Šçš„ä»»æ„ç‚¹

```
def init_theta():
    """Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent""" return np.zeros((2,))
```

æ¢¯åº¦ä¸‹é™å‡½æ•°:

```
def grad_desc(loss_f, gradient_loss_f, theta, data, num_iter=20, alpha=0.1):
    """
    Run gradient descent update for a finite number of iterations and static learning rateKeyword arguments:
    loss_f -- the loss function to be minimized (used for computing loss_history)
    gradient_loss_f -- the gradient of the loss function to be minimized
    theta -- the vector of values theta to use at first iteration
    data -- the data used in the model 
    num_iter -- the max number of iterations
    alpha -- the learning rate (also called the step size)

    Return:
    theta -- the optimal value of theta after num_iter of gradient descent
    theta_history -- the series of theta values over each iteration of gradient descent
    loss_history -- the series of loss values over each iteration of gradient descent
    """ theta_history = []
    loss_history = []

    #initial x and y values
    x = part_1_data['x']
    y = part_1_data['y']

    #append the original theta and loss     
    theta_history.append(theta)
    loss_history.append(loss_f(theta, x, y))

    #main update function for iteration in range (num_iter):

        #update and append theta
        theta_cur = theta - alpha * gradient_loss_f(theta, x, y)
        theta_history.append(theta_cur)

        #calculate and append loss using cur_theta
        cur_loss = loss_f(theta_cur, x, y)
        loss_history.append(cur_loss)

        #reset theta
        theta = theta_curreturn theta, theta_history, loss_history#running gradient descent 
theta_start = init_theta()
theta_hat, thetas_used, losses_calculated = grad_desc(
    sin_MSE, sin_MSE_gradient, theta_start, part_1_data, num_iter=20, alpha=0.1
)
for b, l in zip(thetas_used, losses_calculated):
    print(f"theta: {b}, Loss: {l}")
```

# E ä½“éªŒæ›´å¤šçš„æ¢¯åº¦ä¸‹é™

## **è¡°å‡é˜¿å°”æ³•(å­¦ä¹ ç‡)æ¢¯åº¦ä¸‹é™**

**ç›´è§‰:**

é€šè¿‡è¡°å‡çš„å­¦ä¹ é€Ÿç‡ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ•°å­—ğ›¼ï¼Œå­¦ä¹ ç°åœ¨åº”è¯¥æ˜¯ğ›¼/(ğ‘¡+1)å…¶ä¸­ğ‘¡æ˜¯æ¢¯åº¦ä¸‹é™çš„å½“å‰è¿­ä»£çš„æ•°å­—ã€‚

ä½ å‡ ä¹å¯ä»¥å›æ”¶ä¸Šé¢çš„å¤§éƒ¨åˆ†ä»£ç ï¼Œè€Œä¸æ˜¯æ¯æ¬¡è¿­ä»£åªæ›´æ–°ğ›¼åˆ°ğ›¼/(iteration+1ã€‚

## éšæœºæ¢¯åº¦ä¸‹é™

**ç›´è§‰:**

![](img/691995071a950b4841190d8f178de8b8.png)

æ¯æ‰¹å¤§å°çš„Î¸æ›´æ–°å‡½æ•°

åœ¨ä¸Šé¢çš„æ›´æ–°è§„åˆ™ä¸­ï¼Œğ‘(æ‰¹é‡å¤§å°)æ¯”ğ‘›(æ•°æ®çš„æ€»å¤§å°)å°å¾—å¤šã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé™æ€å­¦ä¹ ç‡å’Œä¸€ä¸ªå‚æ•°`batch_size`æ¥è¡¨ç¤ºæ¯æ¬¡è¿­ä»£çš„å°æ‰¹é‡æ ·æœ¬çš„å¤§å°ã€‚

ä¸Šé¢ä»£ç çš„ä¸»è¦å˜åŒ–æ˜¯æˆ‘ä»¬è¿­ä»£æ—¶çš„æ›´æ–°åŠŸèƒ½:

```
for iteration in range (num_iter):
        #sample batch size each iterations 
        sample = data.sample(batch_size)
        out_arr_x = sample['x']
        out_arr_y = sample['y']

        #update and append theta
        theta_cur = theta - alpha * gradient_loss_f(theta, out_arr_x, out_arr_y)
        theta_history.append(theta_cur)

        #calculate and append loss using cur_theta
        cur_loss = loss_f(theta_cur, out_arr_x, out_arr_y)
        loss_history.append(cur_loss)

        #reset
        theta = theta_cur
```

æ³¨æ„:å¯¹äº cur_lossï¼Œæˆ‘ä»¬è®¡ç®—å¹¶ä¿å­˜æ¯ä¸ªæ‰¹é‡çš„ loss_valï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ã€‚

## å¯è§†åŒ–å’Œæ¯”è¾ƒæ€§èƒ½

è®©æˆ‘ä»¬æƒ³è±¡ä¸€ä¸‹æˆ‘ä»¬çš„å‡½æ•°ï¼Œçœ‹çœ‹æ¯ä¸ªå‡½æ•°åœ¨æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼æ—¶çš„è¡¨ç°ã€‚

```
plt.plot(np.arange(len(loss)), loss, label='Static Alpha')
plt.plot(np.arange(len(loss)), loss_decay, label='Decaying Alpha')
plt.plot(np.arange(len(loss)), loss_stoch, label='SGD')
plt.xlabel('Iteration #')
plt.ylabel('Avg Loss')
plt.title('Avg Loss vs Iteration # vs Learning Rate Type')
plt.legend();
```

![](img/ec31e0f679b78138e9c55270ebd8bc7c.png)

```
#3D plot (gradient descent with static alpha)
plot_3d(thetas[:, 0], thetas[:, 1], loss, average_squared_loss, sin_model, x, y)
```

![](img/bb94415c6b84413a0f12498ce65f6515.png)

é™æ€é˜¿å°”æ³•æ¢¯åº¦ä¸‹é™

```
#3D plot (gradient descent with decaying alpha)
plot_3d(thetas_decay[:, 0], thetas_decay[:, 1], loss_decay, average_squared_loss, sin_model, x, y)
```

![](img/bfd35f15ea46b1f88ec3313c174f7aac.png)

é˜¿å°”æ³•è¡°å‡çš„æ¢¯åº¦ä¸‹é™

```
#3D plot (stochastic gradient descent)
plot_3d(thetas_stoch[:, 0], thetas_stoch[:, 1], loss_stoch, average_squared_loss, sin_model, x, y)
```

![](img/0bcfe3c88902367833305364262d7264.png)

éšæœºæ¢¯åº¦ä¸‹é™

G æ¢¯åº¦ä¸‹é™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§é€šç”¨çš„æ–¹æ³•ï¼Œå½“æˆ‘ä»¬æ— æ³•è§£æåœ°æ±‚è§£Î¸çš„æœ€å°å€¼æ—¶ï¼Œå¯ä»¥ä½¿æŸå¤±å‡½æ•°æœ€å°åŒ–ã€‚éšç€æˆ‘ä»¬çš„æ¨¡å‹å’ŒæŸå¤±å‡½æ•°è¶Šæ¥è¶Šå¤æ‚ï¼Œæˆ‘ä»¬å°†è½¬å‘æ¢¯åº¦ä¸‹é™ä½œä¸ºæ‹Ÿåˆæ¨¡å‹çš„å·¥å…·ã€‚

ç”±äºè®¡ç®—å¤§å‹æ•°æ®é›†å¯èƒ½æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶è¾ƒé•¿ï¼Œå› æ­¤éšæœº GD è®¡ç®—æ›´æ–°æ¯”æ‰¹é‡æ¢¯åº¦ä¸‹é™å¿«å¾—å¤šã€‚åˆ°æ‰¹é‡æ¢¯åº¦ä¸‹é™å®Œæˆä¸€æ¬¡æ›´æ–°æ—¶ï¼Œå®ƒå¯ä»¥æœç€æœ€ä¼˜Î¸å–å¾—æ˜¾è‘—è¿›å±•ã€‚

è€Œè¡°å‡çš„Î±GD è€ƒè™‘äº†æ›´æ–°å­¦ä¹ ç‡ï¼Œè¿™æœ‰åˆ©äºé˜²æ­¢è¿‡å†²ï¼Œå› æ­¤æ­¥é•¿æ›´ç²¾ç¡®ã€‚