<html>
<head>
<title>Create custom Object Detection without using tensorflow API</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不使用tensorflow API创建自定义对象检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/create-custom-object-detection-without-using-tensorflow-api-230159a58207?source=collection_archive---------1-----------------------#2021-11-08">https://medium.com/analytics-vidhya/create-custom-object-detection-without-using-tensorflow-api-230159a58207?source=collection_archive---------1-----------------------#2021-11-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="d6d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">目标检测是计算机视觉最广泛的应用之一。无论是特斯拉的自动驾驶模式还是简单的面具检测模型，物体检测和定位都是一条路要走。当向我介绍对象检测时，我同样感到惊讶，当我进行简单的google搜索时，我所能找到的只是tensorflow对象检测API。甚至当我在关键字中添加“不使用tensorflow API”时，它仍然显示使用tensorflow API的结果。这极大地激励我创建自定义对象检测模型，消除对使用高级API的依赖。</p><p id="3bc5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Tensorflow的对象检测API非常酷，但我发现它非常笨拙和耗时。此外，它的GitHub库一直在更新，这与可用的教程和博客不匹配。因此初学者很难应付。在这篇博客中，我将解释如何在不使用tensorflow API的情况下，使用CNN创建一个精确的对象检测模型。但是，我们肯定会导入keras和tensorflow库来创建模型。那么，让我们开始吧！</p><h1 id="ad74" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">这个概念</h1><p id="74e6" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">试着回忆一下我们传统的CNN图像分类模型——我们将一张RGB图像作为输入，比如200x200，然后预测它是一只猫还是一只狗。因此，我们的输入形状变成(None，200，200，3 ),经过一次热编码后，我们的输出形状变成(None，1 ),其中None表示批量大小。</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="640f" class="ko jd hh kk b fi kp kq l kr ks">model = Sequential()<br/>model.add(Conv2D(32, (3, 3), input_shape=(3, 200, 200)))<br/>model.add(Activation('relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/><br/>model.add(Conv2D(32, (3, 3)))<br/>model.add(Activation('relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))</span><span id="0d71" class="ko jd hh kk b fi kt kq l kr ks">model.add(Flatten())  <br/>model.add(Dense(64))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(1))<br/>model.add(Activation('sigmoid'))<br/><br/>model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])</span></pre><p id="9c4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将在这里使用相同的概念。对于对象检测，我们有一个RGB图像，我们的输出是2个坐标(x，y ),因此我们的模型总共有4个输出——xmin，ymin，xmax，ymax。因此，在这种情况下，我们的输出形状将是(None，4)</p><figure class="kf kg kh ki fd kv er es paragraph-image"><div class="er es ku"><img src="../Images/bad8c595589e660674b8b79a32087b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*aTT_oo1t8bTQh0midWsBhw.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">边界框坐标——来源:ResearchGate</figcaption></figure><h1 id="e3c0" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">数据准备</h1><p id="8af0" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">让我们把这个阶段分成两部分——X和Y。X是图像，Y是我们的坐标。</p><p id="1ba2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了训练，我们将不得不使用注释工具手动注释图像。为此，我们将使用LabelImg。这个过程非常简单，你可以按照它的<a class="ae lc" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank"> GitHub自述文件</a>上的步骤操作。</p><p id="6f66" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦我们在同一个文件夹中有了带有xml文件的图像。我们将使用xml_to_csv.py将xml文件转换成csv文件。你可以从<a class="ae lc" href="https://github.com/datitran/raccoon_dataset/blob/master/xml_to_csv.py" rel="noopener ugc nofollow" target="_blank">这里</a>下载这个文件</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="76d8" class="ko jd hh kk b fi kp kq l kr ks">import osimport glob<br/>import pandas as pd<br/>import xml.etree.ElementTree as ET  </span><span id="96f2" class="ko jd hh kk b fi kt kq l kr ks">def xml_to_csv(path):    <br/>xml_list = []    <br/>for xml_file in glob.glob(path + '/*.xml'):        <br/>     tree = ET.parse(xml_file)        <br/>     root = tree.getroot()        <br/>     for member in root.findall('object'):            <br/>           value = (root.find('filename').text, <br/>                  int(root.find('size')[0].text),<br/>                  int(root.find('size')[1].text),                      <br/>                  member[0].text,<br/>                  int(member[4][0].text),                     <br/>                  int(member[4][1].text),                     <br/>                  int(member[4][2].text),                     <br/>                  int(member[4][3].text))            <br/>           xml_list.append(value)    <br/>      column_name = ['filename', 'width', 'height', 'class',                    'xmin','ymin', 'xmax', 'ymax']    </span><span id="1ebe" class="ko jd hh kk b fi kt kq l kr ks">       xml_df = pd.DataFrame(xml_list, columns=column_name)    <br/>       return x xml_df  </span><span id="a172" class="ko jd hh kk b fi kt kq l kr ks">def main():    <br/>  image_path = os.path.join(os.getcwd(), 'foldername')    <br/>  xml_df = xml_to_csv(image_path)      <br/>  xml_df.to_csv('filename.csv', index=None)         <br/>  [xml to csv script]</span><span id="135d" class="ko jd hh kk b fi kt kq l kr ks">main()</span></pre><p id="39fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不要忘记更改上面脚本中的“foldername”和“filename.csv”。</p><p id="081d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，我们将有一个csv文件，其中包含图像及其在csv中的相应坐标。</p><h1 id="c762" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">数据生成程序</h1><p id="aed9" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">现在，因为我们有图像，即我们的X和csv包含坐标，即我们的Y准备好了，我们将生成模型训练的数据。</p><p id="f8dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">步骤1 </strong>:首先我们将导入我们刚刚使用xml_to_csv.py生成的csv文件</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="161f" class="ko jd hh kk b fi kp kq l kr ks">req_images = pd.read_csv('labels.csv')<br/>fin_images = req_images.drop_duplicates(subset='filename', keep="last")</span><span id="b6d5" class="ko jd hh kk b fi kt kq l kr ks">fin_images.head()</span></pre><p id="5d90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这将加载我们的csv，看起来像这样:</p><figure class="kf kg kh ki fd kv er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ld"><img src="../Images/e6290c4c4240428a7afc5ec67299aaae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iyoLW6H_gy59h1qP_f_AFA.png"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">xml到csv转换文件</figcaption></figure><p id="eaed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">步骤2 : </strong>编写用于从csv生成图像的代码，并将大小调整为96x96，因为这将减少训练时间和计算能力。请注意，我们将返回两个对象—调整大小的图像和source_images列表，我们将使用它们来进一步验证关键点。</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="ac2d" class="ko jd hh kk b fi kp kq l kr ks">def generate_images():<br/>    <br/>    source_images = fin_images['filename'].to_list()<br/>    <br/>    images = []<br/>   <br/>    for i in source_images:<br/>        path = 'train/'+i<br/>        input_image = cv2.imread(path)<br/>        input_image = cv2.resize(input_image,(96,96))<br/>        images.append(input_image)<br/>#         cv2.imwrite('img_t.jpg',input_image)<br/>    <br/>    images = np.array(images)<br/>    print('images_shape:',images.shape)<br/>    return images, source_images</span><span id="1254" class="ko jd hh kk b fi kt kq l kr ks">model_input_images, source_images = generate_images()</span></pre><p id="a169" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第三步:</strong>同样，我们将加载边界框的坐标，并根据我们的96x96大小的图像对它们进行归一化。</p><p id="d68f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为我们将图像的尺寸调整为96x96，所以边界框的点会相应地改变。为此，我们将找到宽高比，并将xmin和xmax乘以宽比，将ymin和ymax乘以高比。这将为我们提供用于模型训练的标准化坐标。</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="d0f7" class="ko jd hh kk b fi kp kq l kr ks">fin_images['width_ratio'] = fin_images['width']/96<br/>fin_images['height_ratio'] = fin_images['height']/96</span><span id="accd" class="ko jd hh kk b fi kt kq l kr ks">fin_images['xmin'] = fin_images['xmin']/fin_images['width_ratio']<br/>fin_images['xmax'] = fin_images['xmax']/fin_images['width_ratio']<br/>fin_images['ymin'] = fin_images['ymin']/fin_images['height_ratio']<br/>fin_images['ymax'] = fin_images['ymax']/fin_images['height_ratio']</span></pre><p id="b48a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了检查我们是否已经正确归一化，只需确保数据帧中的所有坐标应该小于96(显然！)</p><p id="d35d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第四步:</strong>就像图像一样，我们将生成Y ie归一化坐标。在这里，我们将通过检查文件名和source_images列表中的image_name来验证我们正在获取对应于图像的正确关键点。</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="15d5" class="ko jd hh kk b fi kp kq l kr ks">def generate_keypoints():<br/>    <br/>    keypoint_features = []<br/>    <br/>    for i in source_images:<br/>        try:<br/>            print(i)<br/>            image_name = i<br/>            mask = fin_images[fin_images['filename'] == image_name]<br/>            mask = mask.values.tolist()<br/>            print(mask)<br/>            keypoints = (mask[0][4:8])<br/>            #print(keypoints)<br/>            newList = [int(x) / 96 for x in keypoints]<br/>            #print(newList)<br/>            keypoint_features.append(newList)<br/>        except:<br/>            print('error !')<br/>    keypoint_features = np.array(keypoint_features, dtype=float)    <br/>    return keypoint_features</span></pre><p id="d6ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">只是为了确保到目前为止一切都做得正确，加载任何随机的X和Y，并在图像上绘制矩形。如果边界框显示正确，这意味着您已经正确执行了归一化。如果没有，喝杯咖啡&amp;再次尝试以上步骤！</p><figure class="kf kg kh ki fd kv er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ld"><img src="../Images/e1ea751951404a1ed7efc6b786cc37bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vU2DS0Rbe1cepWal5LQjMw.png"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">通过手工绘制边界框来测试我们生成的X和Y</figcaption></figure><p id="df77" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第五步:最后，我们将创建模型架构。您可以创建任何基于CNN的序列模型，并根据您的需要和要求调整其复杂性和优化器。</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="8c82" class="ko jd hh kk b fi kp kq l kr ks">input_shape = (96,96,3)<br/>no_of_keypoints = 4  #xmin, ymin, xmax, ymax</span><span id="edd2" class="ko jd hh kk b fi kt kq l kr ks">model = Sequential()</span><span id="8418" class="ko jd hh kk b fi kt kq l kr ks"># Input dimensions: (None, 96, 96, 3)<br/>model.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=input_shape))<br/>model.add(LeakyReLU(alpha = 0.1))<br/>model.add(BatchNormalization())<br/># Input dimensions: (None, 96, 96, 32)<br/>model.add(Convolution2D(32, (3,3), padding='same', use_bias=False))<br/>model.add(LeakyReLU(alpha = 0.1))<br/>model.add(BatchNormalization())<br/>model.add(MaxPool2D(pool_size=(2, 2)))</span><span id="7b57" class="ko jd hh kk b fi kt kq l kr ks"># Input dimensions: (None, 48, 48, 32)<br/>model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))<br/>model.add(LeakyReLU(alpha = 0.1))<br/>model.add(BatchNormalization())<br/># Input dimensions: (None, 48, 48, 64)<br/>model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))<br/>model.add(LeakyReLU(alpha = 0.1))<br/>model.add(BatchNormalization())<br/>model.add(MaxPool2D(pool_size=(2, 2)))</span><span id="dca8" class="ko jd hh kk b fi kt kq l kr ks"># Input dimensions: (None, 24, 24, 64)<br/>model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))<br/>model.add(LeakyReLU(alpha = 0.1))<br/>model.add(BatchNormalization())<br/># Input dimensions: (None, 24, 24, 96)<br/>model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))<br/>model.add(LeakyReLU(alpha = 0.1))<br/>model.add(BatchNormalization())<br/>model.add(MaxPool2D(pool_size=(2, 2)))</span><span id="1b40" class="ko jd hh kk b fi kt kq l kr ks"># Input dimensions: (None, 12, 12, 96)<br/>model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))<br/>model.add(LeakyReLU(alpha = 0.1))<br/>model.add(BatchNormalization())<br/># Input dimensions: (None, 12, 12, 128)<br/>model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))<br/>model.add(LeakyReLU(alpha = 0.1))<br/>model.add(BatchNormalization())<br/>model.add(MaxPool2D(pool_size=(2, 2)))</span><span id="a693" class="ko jd hh kk b fi kt kq l kr ks"># Input dimensions: (None, 6, 6, 128)<br/>model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))<br/>model.add(LeakyReLU(alpha = 0.1))<br/>model.add(BatchNormalization())<br/># Input dimensions: (None, 6, 6, 256)<br/>model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))<br/>model.add(LeakyReLU(alpha = 0.1))<br/>model.add(BatchNormalization())<br/>model.add(MaxPool2D(pool_size=(2, 2)))</span><span id="2556" class="ko jd hh kk b fi kt kq l kr ks"># Input dimensions: (None, 3, 3, 256)<br/>model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))<br/>model.add(LeakyReLU(alpha = 0.1))<br/>model.add(BatchNormalization())<br/># Input dimensions: (None, 3, 3, 512)<br/>model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))<br/>model.add(LeakyReLU(alpha = 0.1))<br/>model.add(BatchNormalization())</span><span id="1944" class="ko jd hh kk b fi kt kq l kr ks"># Input dimensions: (None, 3, 3, 512)<br/>model.add(Flatten())<br/>model.add(Dense(512, activation='linear'))<br/>model.add(Dropout(0.3))<br/>model.add(Dense(no_of_keypoints))<br/>model.summary()</span></pre><p id="5e96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不要忘记保持最后一个密集层为:</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="dcb1" class="ko jd hh kk b fi kp kq l kr ks">model.add(Dense(no_of_keypoints))) # 4 in our case </span></pre><p id="5964" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们的例子中是4，因为我们有4个关键点作为输出——xmin，xmax，ymin，ymax。</p><p id="ce72" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">步骤6 : </strong>一旦你的序列模型准备好了，只需使用生成的X(模型输入图像)和Y(模型输入关键点)编译和拟合模型</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="96b3" class="ko jd hh kk b fi kp kq l kr ks">from keras.callbacks import EarlyStopping, ReduceLROnPlateau</span><span id="de9c" class="ko jd hh kk b fi kt kq l kr ks">earlyStopping = EarlyStopping(monitor='loss', patience=30, mode='min', baseline=None)</span><span id="8dbb" class="ko jd hh kk b fi kt kq l kr ks">rlp = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=1e-15, mode='min', verbose=1)</span><span id="97d4" class="ko jd hh kk b fi kt kq l kr ks">model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])</span><span id="98a5" class="ko jd hh kk b fi kt kq l kr ks">history = model.fit(model_input_images, model_input_keypoints, epochs=500, batch_size=16, validation_split=0.15, callbacks=[earlyStopping])</span></pre><p id="2e8a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第7步:</strong>标志着我们的物体检测训练结束。现在是考验我们努力的时候了！让我们测试模型。请记住，我们必须执行与培训时相同的预处理步骤。</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="e570" class="ko jd hh kk b fi kp kq l kr ks">image_test = "final_test1.jpg"<br/>test_image = cv2.imread(image_test)<br/>test_image = cv2.resize(test_image,(96,96))<br/>cv2.imwrite('final_test.jpg',test_image)<br/>images = np.array(test_image)/255.0</span><span id="a9e2" class="ko jd hh kk b fi kt kq l kr ks">print(images.shape)<br/>test=np.expand_dims(images,axis=0)<br/>print(test.shape)</span><span id="1762" class="ko jd hh kk b fi kt kq l kr ks">ans = model.predict(test)<br/>print(abs(ans))</span><span id="ad61" class="ko jd hh kk b fi kt kq l kr ks">ans = ans * 96  #denormalizing<br/>print(ans)</span></pre><p id="d229" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">步骤8 : </strong>让我们在图像上绘制坐标，以可视化我们的结果。</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="d6a6" class="ko jd hh kk b fi kp kq l kr ks">test_image = cv2.imread('final_test.jpg')<br/>test_image = cv2.resize(test_image,(96,96))<br/>out = cv2.rectangle(test_image,(ans[0],ans[1]),(ans[2],ans[3]),    (255,0,0), 2)<br/>plt.imshow(out)<br/>cv2.imwrite('final_test.jpg',out)</span></pre><figure class="kf kg kh ki fd kv er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es li"><img src="../Images/97d4972a6ea46166e672951671735b31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SY6IBixR4Wjroal5A5BoMQ.png"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">目标检测模型输出</figcaption></figure><p id="ceab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用此管道进行对象检测的最大优点是，您可以根据自己的需求轻松调整管道的模型和其他参数。你也可以用这个试试迁移学习。只要确保在最后一个密集层保留4个nuerons就可以了。</p><h1 id="603b" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">未来的改进和范围</h1><ul class=""><li id="f634" class="lj lk hh ig b ih ka il kb ip ll it lm ix ln jb lo lp lq lr bi translated">需要弄清楚，如何为多个边界框修改它</li><li id="9f8d" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">再添加一个参数来预测置信度得分</li><li id="1a7c" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">如果您正在使用任何其他数据标注工具以Pascal-VOC以外的格式导出坐标，则需要调整数据生成管道</li></ul><p id="d34e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你遇到任何错误或问题，请随时写下你的评论。此外，如果您尝试了这种方法并获得了惊人的预测，请不要犹豫，在评论中炫耀您的结果。</p><p id="ac39" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于定制的计算机视觉和基于人工智能的产品和服务，请访问我们的<a class="ae lc" href="http://www.thinkinbytes.in" rel="noopener ugc nofollow" target="_blank"> Think In Bytes </a>或查看我们的<a class="ae lc" href="https://docs.google.com/presentation/d/1vewfvp7LZjp_-dVMBgJmxLXuNag6K7HwxA1j_ABcQKI/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">产品组合</a>！</p></div></div>    
</body>
</html>