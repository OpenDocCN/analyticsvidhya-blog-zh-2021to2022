<html>
<head>
<title>A sudden change to the encoder!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">编码器突然变了！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-sudden-change-to-the-encoder-368fd9a72bc7?source=collection_archive---------25-----------------------#2021-05-27">https://medium.com/analytics-vidhya/a-sudden-change-to-the-encoder-368fd9a72bc7?source=collection_archive---------25-----------------------#2021-05-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="e6ba" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><a class="jc jd ge" href="https://medium.com/u/8cc2ede908a6?source=post_page-----368fd9a72bc7--------------------------------" rel="noopener" target="_blank"> theamitnikhade </a></h1><p id="f2a4" class="pw-post-body-paragraph je jf hh jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb ha bi translated">游览:【amitnikhade.com T2】</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/a9d7cb4dfd7d45263c7f86322c2125f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PoVLy4YljZduLgW8uQE-6w.jpeg"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">[ <a class="ae kc" href="https://unsplash.com/photos/liAwyJ64wHE?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditShareLink" rel="noopener ugc nofollow" target="_blank">演职员表</a></figcaption></figure><blockquote class="kt ku kv"><p id="4fd5" class="je jf kw jg b jh kx jj jk jl ky jn jo kz la jr js lb lc jv jw ld le jz ka kb ha bi translated">《变形金刚》自问世以来，由于其重要的舞台表演而广受欢迎。他们统治了NLP和计算机视觉。基于变形金刚的模型一直是最受欢迎的。</p></blockquote><h2 id="52e0" class="lf if hh bd ig lg lh li ik lj lk ll io jp lm ln is jt lo lp iw jx lq lr ja ls bi translated">概观</h2><p id="f8d4" class="pw-post-body-paragraph je jf hh jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb ha bi translated"><a class="ae kc" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <strong class="jg hi">注意力是你所需要的一切</strong> </a>。本文描述了编码器和解码器堆叠在一起的变压器架构。这两种架构都包括<a class="ae kc" rel="noopener" href="/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029#:~:text=Normalization%20is%20a%20technique%20often,dataset%20does%20not%20require%20normalization.">规范化</a>、<a class="ae kc" href="https://en.wikipedia.org/wiki/Feedforward_neural_network" rel="noopener ugc nofollow" target="_blank">前馈</a>和<a class="ae kc" href="https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/#:~:text=The%20attention%20mechanism%20emerged%20as,natural%20language%20processing%20(NLP).&amp;text=The%20encoder%20LSTM%20is%20used,state%20of%20the%20LSTM%2FRNN." rel="noopener ugc nofollow" target="_blank">注意</a>层。转换器最大的优势是它们的并行性。注意力在变形金刚机制中起到了至关重要的作用，负责其整体优化</p><p id="4107" class="pw-post-body-paragraph je jf hh jg b jh kx jj jk jl ky jn jo jp la jr js jt lc jv jw jx le jz ka kb ha bi translated"><a class="ae kc" href="https://www.computerhope.com/jargon/p/parallelization.htm#:~:text=Parallelization%20is%20the%20act%20of,the%20next%2C%20then%20the%20next." rel="noopener ugc nofollow" target="_blank">并行化</a>取代了<a class="ae kc" href="https://www.analyticsvidhya.com/blog/2020/08/a-simple-introduction-to-sequence-to-sequence-models/" rel="noopener ugc nofollow" target="_blank"> seq2seq </a>机制，这是可行的，因为与早期的序列对序列模型相比，位置编码和注意力导致了更快的训练。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lt"><img src="../Images/e0f8e5915c17004a8cee8794c8e5b89c.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*JQRVLK-aVeMASaCF74jgtQ.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">变形金刚模型建筑[ <a class="ae kc" href="https://miro.medium.com/max/856/1*ZCFSvkKtppgew3cc7BIaug.png" rel="noopener">来源</a></figcaption></figure><h2 id="af56" class="lf if hh bd ig lg lh li ik lj lk ll io jp lm ln is jt lo lp iw jx lq lr ja ls bi translated">编码器</h2><p id="dd96" class="pw-post-body-paragraph je jf hh jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb ha bi translated">编码器由相同的<em class="kw"> n </em>层堆叠而成，其中编码器的每个子层由多头关注和一个具有一定归一化的前馈网络组成。作为输入传递给编码器的向量是单词和位置嵌入的。</p><h2 id="e74f" class="lf if hh bd ig lg lh li ik lj lk ll io jp lm ln is jt lo lp iw jx lq lr ja ls bi translated">解码器</h2><p id="bff5" class="pw-post-body-paragraph je jf hh jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb ha bi translated">解码器有点类似于编码器，但稍有修改，如双关注层，其中第二关注层接受来自编码器和前一解码器关注层的输入，该输入进一步通过前馈网络。</p><h2 id="0be0" class="lf if hh bd ig lg lh li ik lj lk ll io jp lm ln is jt lo lp iw jx lq lr ja ls bi translated">注意力来了</h2><p id="eeda" class="pw-post-body-paragraph je jf hh jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb ha bi translated">首先，由于图片在处理数据时对某些因素的关注，它引起了人们的注意。简单地说，注意力以这样的方式学习上下文向量，即它使用余弦相似性从输入序列中提取和映射重要的和相关的信息，并给它分配更高的权重，这导致更精确的预测。它解决了编解码机制中的<a class="ae kc" href="https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/" rel="noopener ugc nofollow" target="_blank">消失梯度</a>问题。</p><p id="d1de" class="pw-post-body-paragraph je jf hh jg b jh kx jj jk jl ky jn jo jp la jr js jt lc jv jw jx le jz ka kb ha bi translated">在变压器的编码器部分，<strong class="jg hi">自关注</strong>用于关注输入序列，以便从中提取重要数据。</p><h2 id="406d" class="lf if hh bd ig lg lh li ik lj lk ll io jp lm ln is jt lo lp iw jx lq lr ja ls bi translated">有许多头的野兽</h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lu"><img src="../Images/3db18cfaf72b53c182fc278e77e85c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*IiAtd383_nkVsELVfjwBwA.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">多头注意力和成比例的点积注意力[ <a class="ae kc" href="https://miro.medium.com/max/513/1*7Aye1P2QcS2wMhuSzQzZbA.png" rel="noopener">来源</a></figcaption></figure><p id="cf33" class="pw-post-body-paragraph je jf hh jg b jh kx jj jk jl ky jn jo jp la jr js jt lc jv jw jx le jz ka kb ha bi translated">提供给注意力的输入是三个参数的形式，称为查询、键和值，即<em class="kw"> (Q，K，V)。</em>所有三个值都是相同的向量。在编码器自我关注。注意力层多次并行循环计算。这些计算中的每一个都被称为注意头。所有这些注意力计算结合在一起产生一个最终分数。在变压器架构中有三个注意事项。编码器自关注处理编码器的输入序列并关注自身，解码器自关注关注解码器的目标序列，编码器-解码器自关注关注解码器的输入序列。</p><p id="288a" class="pw-post-body-paragraph je jf hh jg b jh kx jj jk jl ky jn jo jp la jr js jt lc jv jw jx le jz ka kb ha bi translated">这种关注被谷歌取代，因为它昂贵的计算需求以及训练模型所需的时间是巨大的。当他们用<a class="ae kc" href="https://en.wikipedia.org/wiki/Fast_Fourier_transform" rel="noopener ugc nofollow" target="_blank">快速傅立叶变换</a>做同样的尝试时，与自我注意相比，模型在更短的时间内得到训练，而精确度相似。</p><h2 id="144c" class="lf if hh bd ig lg lh li ik lj lk ll io jp lm ln is jt lo lp iw jx lq lr ja ls bi translated">2021年5月9日</h2><p id="015a" class="pw-post-body-paragraph je jf hh jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb ha bi translated">谷歌用傅立叶变换代替了伯特自我关注</p><p id="7e56" class="pw-post-body-paragraph je jf hh jg b jh kx jj jk jl ky jn jo jp la jr js jt lc jv jw jx le jz ka kb ha bi translated"><a class="ae kc" href="https://arxiv.org/abs/2105.03824" rel="noopener ugc nofollow" target="_blank"> FNet:将令牌与傅立叶变换混合</a>提出在编码器中用简单的标准非参数化傅立叶变换代替自我关注可以大规模地加速它，并且具有良好的准确性。快速傅立叶变换是离散傅立叶变换的优化版本。它从输入的信号中提取有用的特征。它在GPU上运行速度快7倍，在TPU上快2倍。这是混合令牌最有效的方法。此外，当通过快速傅立叶变换处理数据时，很少的信息丢失，这使得它更加有益。</p><p id="5559" class="pw-post-body-paragraph je jf hh jg b jh kx jj jk jl ky jn jo jp la jr js jt lc jv jw jx le jz ka kb ha bi translated">FFT首先由<a class="ae kc" href="https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm" rel="noopener ugc nofollow" target="_blank">库利和图基</a> (1965)讨论，它主要用于信号处理，将信号分解成其组成频率，也用于图像处理，将图像分解成其正弦和余弦分量。FFT对深度学习的贡献由来已久，它的一些应用是加速卷积、傅立叶RNN等。</p><p id="61df" class="pw-post-body-paragraph je jf hh jg b jh kx jj jk jl ky jn jo jp la jr js jt lc jv jw jx le jz ka kb ha bi translated"><strong class="jg hi">傅立叶变换</strong>子层对序列维度应用1D傅立叶变换，对隐藏维度应用1D变换。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lv"><img src="../Images/bce6f48545ef9f3058005e210426d560.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*jtkhaK7dK5WCj_Gb_I6XAg.png"/></div></figure><h2 id="5e6c" class="lf if hh bd ig lg lh li ik lj lk ll io jp lm ln is jt lo lp iw jx lq lr ja ls bi translated">FNET编码器架构</h2><blockquote class="kt ku kv"><p id="2b5b" class="je jf kw jg b jh kx jj jk jl ky jn jo kz la jr js lb lc jv jw ld le jz ka kb ha bi translated">FNet是具有多个层的层标准化ResNet架构，每个层由傅立叶混合子层和跟随其后的前馈子层组成。<a class="ae kc" href="https://arxiv.org/pdf/2105.03824.pdf" rel="noopener ugc nofollow" target="_blank">【来源】</a></p></blockquote><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lw"><img src="../Images/3dcefed6146c459d4e6f7af989749827.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*ta6_3OKd08mj31AJDxA9vw.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">FNet编码器架构[ <a class="ae kc" href="https://arxiv.org/pdf/2105.03824.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="bf2c" class="pw-post-body-paragraph je jf hh jg b jh kx jj jk jl ky jn jo jp la jr js jt lc jv jw jx le jz ka kb ha bi translated">上面的模型架构是带有傅立叶变换的<a class="ae kc" href="https://blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank">伯特模式</a> l</p><p id="cd18" class="pw-post-body-paragraph je jf hh jg b jh kx jj jk jl ky jn jo jp la jr js jt lc jv jw jx le jz ka kb ha bi translated">编码器的架构保持不变，只是通过用傅立叶变换代替自关注进行了小的改变。这只是混合输入标记的另一种很酷的方式，它为前馈层提供了最重要的细节。模型中的线性变换加速了训练，性能良好。在我们的神经网络中，不存在我们偶然看到的权重概念，可学习的权重在训练中发挥作用。这也是增加加速度和使模型轻量化的一个因素。</p><h2 id="7fca" class="lf if hh bd ig lg lh li ik lj lk ll io jp lm ln is jt lo lp iw jx lq lr ja ls bi translated">履行</h2><pre class="ke kf kg kh fd lx ly lz ma aw mb bi"><span id="93d8" class="lf if hh ly b fi mc md l me mf"><strong class="ly hi"># FNet Encoder</strong></span><span id="0f0a" class="lf if hh ly b fi mg md l me mf">import torch</span><span id="a830" class="lf if hh ly b fi mg md l me mf">from torch import nn</span><span id="c8c7" class="lf if hh ly b fi mg md l me mf">from torch.nn import functional as F</span><span id="6b00" class="lf if hh ly b fi mg md l me mf">class ff(nn.Module):<br/>    def __init__(self, dim, hidden_dim, dropout):</span><span id="ba0a" class="lf if hh ly b fi mg md l me mf">        super().__init__()</span><span id="326c" class="lf if hh ly b fi mg md l me mf">        self.net = nn.Sequential(</span><span id="2a9d" class="lf if hh ly b fi mg md l me mf">        nn.Linear(dim, hidden_dim),</span><span id="8401" class="lf if hh ly b fi mg md l me mf">        nn.GELU(),</span><span id="19e6" class="lf if hh ly b fi mg md l me mf">        nn.Dropout(dropout),</span><span id="e79e" class="lf if hh ly b fi mg md l me mf">        nn.Linear(hidden_dim, dim),</span><span id="1b19" class="lf if hh ly b fi mg md l me mf">        nn.Dropout(dropout)</span><span id="21e0" class="lf if hh ly b fi mg md l me mf">        )</span><span id="0a57" class="lf if hh ly b fi mg md l me mf">    def forward(self, x):</span><span id="752b" class="lf if hh ly b fi mg md l me mf">        return self.net(x)</span><span id="7963" class="lf if hh ly b fi mg md l me mf">class FNetLayer(nn.Module):</span><span id="523d" class="lf if hh ly b fi mg md l me mf">    def __init__(self, dim, hidden_dim, dropout):</span><span id="ea9b" class="lf if hh ly b fi mg md l me mf">        super().__init__()</span><span id="5dc1" class="lf if hh ly b fi mg md l me mf">        self.norm = nn.LayerNorm(dim)</span><span id="1ce2" class="lf if hh ly b fi mg md l me mf">        self.feedForward = ff(dim, hidden_dim, dropout)</span><span id="abc9" class="lf if hh ly b fi mg md l me mf">    def forward(self, x):</span><span id="dedf" class="lf if hh ly b fi mg md l me mf">        residual = x</span><span id="68b1" class="lf if hh ly b fi mg md l me mf"><strong class="ly hi">        x = torch.fft.fft2(x, dim=(-1, -2)).real # Here it is</strong></span><span id="1446" class="lf if hh ly b fi mg md l me mf">        x = self.norm(x+residual)</span><span id="53b7" class="lf if hh ly b fi mg md l me mf">        x = self.feedForward(x)</span><span id="41f1" class="lf if hh ly b fi mg md l me mf">        x = self.norm(x+residual)</span><span id="a119" class="lf if hh ly b fi mg md l me mf">        return x</span><span id="c831" class="lf if hh ly b fi mg md l me mf">class FNet(nn.Module):</span><span id="e374" class="lf if hh ly b fi mg md l me mf">    def __init__(self, dim, hidden_dim, dropout, layers):</span><span id="27cd" class="lf if hh ly b fi mg md l me mf">        super().__init__()</span><span id="e9cf" class="lf if hh ly b fi mg md l me mf">        self.Encoder = FNetLayer(dim, hidden_dim, dropout)</span><span id="f2d5" class="lf if hh ly b fi mg md l me mf">        self._layers_e = nn.ModuleList()</span><span id="306a" class="lf if hh ly b fi mg md l me mf">        for i in range(layers):</span><span id="fe6f" class="lf if hh ly b fi mg md l me mf">            layer = self.Encoder</span><span id="70f6" class="lf if hh ly b fi mg md l me mf">            self._layers_e.append(layer)</span><span id="2ac3" class="lf if hh ly b fi mg md l me mf">    def forward(self, x):</span><span id="7f13" class="lf if hh ly b fi mg md l me mf">        for e in self._layers_e:</span><span id="d557" class="lf if hh ly b fi mg md l me mf">            x = e.forward(x)</span><span id="0ea0" class="lf if hh ly b fi mg md l me mf">        return x</span><span id="6f2d" class="lf if hh ly b fi mg md l me mf">model = FNet(dim=256, hidden_dim=512, dropout=.5, layers=2)</span><span id="e84e" class="lf if hh ly b fi mg md l me mf">print(model)</span><span id="81f5" class="lf if hh ly b fi mg md l me mf">x = torch.randint(1, 20, size=(20, 256))</span><span id="977c" class="lf if hh ly b fi mg md l me mf">output = model(x)</span><span id="34be" class="lf if hh ly b fi mg md l me mf">print(output)</span></pre><div class="mh mi ez fb mj mk"><a href="https://github.com/AmitNikhade/FNet_Encoder" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab dw"><div class="mm ab mn cl cj mo"><h2 class="bd hi fi z dy mp ea eb mq ed ef hg bi translated">AmitNikhade/FNet _编码器</h2><div class="mr l"><h3 class="bd b fi z dy mp ea eb mq ed ef dx translated">快速傅立叶变换的变压器编码器。为AmitNikhade/FNet_Encoder开发做出贡献，创建一个…</h3></div><div class="ms l"><p class="bd b fp z dy mp ea eb mq ed ef dx translated">github.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my kn mk"/></div></div></a></div><p id="ccf1" class="pw-post-body-paragraph je jf hh jg b jh kx jj jk jl ky jn jo jp la jr js jt lc jv jw jx le jz ka kb ha bi translated">本质上，傅立叶变换只是将输入编码为嵌入的线性组合(我们将其作为位置和单词嵌入提供给编码器)。这些组合嵌入进一步与前馈网络中的非线性混合。与创造奇迹的自我关注相比，这只是一个简单的算法。</p><h2 id="642f" class="lf if hh bd ig lg lh li ik lj lk ll io jp lm ln is jt lo lp iw jx lq lr ja ls bi translated">输出</h2><p id="8ea8" class="pw-post-body-paragraph je jf hh jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb ha bi translated">下面是带有输出的模型摘要。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mz"><img src="../Images/1b854269f438a570ac7416f7a347b6fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iSf7-5ZVWCUot_xdtApwOQ.png"/></div></div></figure><p id="68df" class="pw-post-body-paragraph je jf hh jg b jh kx jj jk jl ky jn jo jp la jr js jt lc jv jw jx le jz ka kb ha bi translated">在下面的python代码中，我们只实现了没有位置嵌入的编码器和最终的密集层。从torch库中导入的快速傅立叶变换层将矢量化的输入序列变换为一维离散傅立叶张量。其中<em class="kw"> dim </em>是序列长度，<em class="kw"> hidden_dim </em>是模型隐藏维数离散傅立叶变换由以下公式定义。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es na"><img src="../Images/7c5b2b6f959a95c0ff5182276945636a.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*gbEyItT1pTghtAKJD9rtrQ.png"/></div></figure><blockquote class="kt ku kv"><p id="8cba" class="je jf kw jg b jh kx jj jk jl ky jn jo kz la jr js lb lc jv jw ld le jz ka kb ha bi translated">如果我们在GPT使用FFT会怎么样？请在下面评论意见。</p></blockquote><p id="ed67" class="pw-post-body-paragraph je jf hh jg b jh kx jj jk jl ky jn jo jp la jr js jt lc jv jw jx le jz ka kb ha bi translated">在<a class="ae kc" href="https://gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank"> GLUE benchmark </a>中，带自我关注的谷歌BERT仅比带快速傅立叶变换的Bert精确8%,但FFT在GPU中的计算速度快7倍</p><p id="4dfd" class="pw-post-body-paragraph je jf hh jg b jh kx jj jk jl ky jn jo jp la jr js jt lc jv jw jx le jz ka kb ha bi translated">简单来说，<a class="ae kc" href="https://en.wikipedia.org/wiki/Fourier_transform" rel="noopener ugc nofollow" target="_blank">傅立叶变换</a>是一种将复杂的时间信号变换成由频率定义的更简单的子分量的算法。</p><h2 id="86b1" class="lf if hh bd ig lg lh li ik lj lk ll io jp lm ln is jt lo lp iw jx lq lr ja ls bi translated">关于我</h2><div class="mh mi ez fb mj mk"><a href="https://www.linkedin.com/in/theamitnikhade/" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab dw"><div class="mm ab mn cl cj mo"><h2 class="bd hi fi z dy mp ea eb mq ed ef hg bi translated">Amit Nikhade - JSPM拉贾什胡莎工程学院-马哈拉施特拉邦阿科拉</h2><div class="mr l"><h3 class="bd b fi z dy mp ea eb mq ed ef dx translated">在我的杯子里装了很多咖啡，为普通智力革命而工作。我是一个热爱享受的人…</h3></div><div class="ms l"><p class="bd b fp z dy mp ea eb mq ed ef dx translated">www.linkedin.com</p></div></div><div class="mt l"><div class="nb l mv mw mx mt my kn mk"/></div></div></a></div><h2 id="c170" class="lf if hh bd ig lg lh li ik lj lk ll io jp lm ln is jt lo lp iw jx lq lr ja ls bi translated">参考</h2><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="nc nd l"/></div></figure><div class="mh mi ez fb mj mk"><a href="https://arxiv.org/abs/1706.03762" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab dw"><div class="mm ab mn cl cj mo"><h2 class="bd hi fi z dy mp ea eb mq ed ef hg bi translated">你需要的只是关注</h2><div class="mr l"><h3 class="bd b fi z dy mp ea eb mq ed ef dx translated">主导序列转导模型是基于复杂的递归或卷积神经网络在一个…</h3></div><div class="ms l"><p class="bd b fp z dy mp ea eb mq ed ef dx translated">arxiv.org</p></div></div></div></a></div><h2 id="0757" class="lf if hh bd ig lg lh li ik lj lk ll io jp lm ln is jt lo lp iw jx lq lr ja ls bi translated">结论</h2><p id="a2e8" class="pw-post-body-paragraph je jf hh jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb ha bi translated">希望这些文章能让你理解Google Research and team发布的确切研究，我试着让它更直截了当、更简单，这样就容易理解了。谢谢你。</p></div></div>    
</body>
</html>