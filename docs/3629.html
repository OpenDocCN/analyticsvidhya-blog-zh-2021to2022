<html>
<head>
<title>Imbalanced Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不平衡数据</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/imbalanced-data-2887d92951fd?source=collection_archive---------2-----------------------#2021-07-14">https://medium.com/analytics-vidhya/imbalanced-data-2887d92951fd?source=collection_archive---------2-----------------------#2021-07-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="ddf8" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">python中管理不平衡数据的一些技巧</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/808ed06abec67df7afe98fdfeb709f75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WC-ulpbLB4cxzrahKqMaUA.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">Elena Mozhvilo 在<a class="ae jn" href="https://unsplash.com/s/photos/balance?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="0732" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated"><strong class="ak">简介</strong></h1><p id="0660" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">通常情况下，我们要处理的数据集在类之间的分布不均匀，这也称为不平衡数据。为什么要关注这个呢？因为当我们尝试将分类模型应用于这种数据时，它会影响分类器的性能，因为存在代表性不足的数据。在本文中，我们将评估一些用于处理不平衡数据的方法，如评估指标、抽样技术和成本敏感方法。</p><p id="3538" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">通过<strong class="ki hj">评估指标</strong>，我们指的是评估模型如何对样本进行分类的方法。我们将在下面更详细地应用和描述的指标是:<strong class="ki hj">准确度、精确度、召回率、F1分数、G均值和马修斯相关系数。</strong></p><p id="7337" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">另一方面，通过<strong class="ki hj">采样方法</strong>我们指的是通过平衡类的分布来修改数据结构的技术。换句话说，这些技术中的大多数人为地增加少数样本的数量(过采样)或减少多数样本的数量(欠采样)。我们要应用的方法有:<strong class="ki hj"> SMOTE，ADASYN，邻域清洗规则，单侧选择，SMOTEENN和SMOTE+Tomek。</strong></p><p id="2b84" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">最后但同样重要的是，我们将看到改变分类器每次错误地对代表性不足的类别进行分类时的惩罚方式的效果，而不是修改训练数据的类别分布。这也被称为<strong class="ki hj">成本敏感学习，</strong>它表示模型将增加被错误分类的少数类中的惩罚权重。</p><h1 id="c538" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated"><strong class="ak">数据</strong></h1><p id="c123" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">使用的数据集是可以在<a class="ae jn" href="https://archive.ics.uci.edu/ml/datasets/bank+marketing" rel="noopener ugc nofollow" target="_blank"> UCI </a>找到的银行营销数据，本文使用的完整代码可以在我的<a class="ae jn" href="https://github.com/Michelpayan/Imbalanced_data_techniques" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="cd82" class="lm jp hi li b fi ln lo l lp lq">import pandas as pd<br/>import numpy as np<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.metrics import confusion_matrix<br/>from sklearn.metrics import plot_confusion_matrix</span><span id="a3bd" class="lm jp hi li b fi lr lo l lp lq">from sklearn.preprocessing import OneHotEncoder<br/>from sklearn.compose import ColumnTransformer</span><span id="4aed" class="lm jp hi li b fi lr lo l lp lq">from sklearn.model_selection import StratifiedKFold<br/>from sklearn.model_selection import train_test_split</span><span id="24b9" class="lm jp hi li b fi lr lo l lp lq">from imblearn.pipeline import Pipeline</span><span id="5417" class="lm jp hi li b fi lr lo l lp lq">from sklearn.model_selection import cross_validate<br/>from sklearn.metrics import make_scorer<br/>from sklearn.metrics import fbeta_score, matthews_corrcoef<br/>from imblearn.metrics import geometric_mean_score</span><span id="6419" class="lm jp hi li b fi lr lo l lp lq">from sklearn.ensemble import RandomForestClassifier</span><span id="493a" class="lm jp hi li b fi lr lo l lp lq">#Sampling methods<br/>from imblearn.over_sampling import SMOTE, ADASYN<br/>from imblearn.under_sampling import NeighbourhoodCleaningRule,OneSidedSelection<br/>from imblearn.combine import SMOTEENN, SMOTETomek</span><span id="04a3" class="lm jp hi li b fi lr lo l lp lq">data=pd.read_csv(r'bank-additional\bank-additional\bank-additional-full.csv',sep=";")</span><span id="6ea2" class="lm jp hi li b fi lr lo l lp lq">train, test = train_test_split(data, test_size=0.2, random_state=0, stratify=data[['y']])</span><span id="0f38" class="lm jp hi li b fi lr lo l lp lq">#As a suggestion in <a class="ae jn" href="https://archive.ics.uci.edu/ml/datasets/bank+marketing" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/datasets/bank+marketing</a>, the column "duration" is dropped.<br/>train=train.drop(columns='duration')<br/>test=test.drop(columns='duration')</span><span id="bbce" class="lm jp hi li b fi lr lo l lp lq">train_x=train.iloc[:,0:-1]<br/>train_y=pd.DataFrame(train['y'])<br/>train_y.y.replace('no',0,inplace=True)<br/>train_y.y.replace('yes',1,inplace=True)</span><span id="d680" class="lm jp hi li b fi lr lo l lp lq">test_x=test.iloc[:,0:-1]<br/>test_y=pd.DataFrame(test['y'])<br/>test_y.y.replace('no',0,inplace=True)<br/>test_y.y.replace('yes',1,inplace=True)</span><span id="c51c" class="lm jp hi li b fi lr lo l lp lq">numerical_ix = train_x.select_dtypes(include=['int64', 'float64']).columns<br/>categorical_ix = train_x.select_dtypes(include=['object', 'bool']).columns</span><span id="9040" class="lm jp hi li b fi lr lo l lp lq">kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=100)</span><span id="9d9d" class="lm jp hi li b fi lr lo l lp lq">scoring = {"Acc":"accuracy",<br/>           'F1': 'f1', <br/>           'Prec': 'precision',<br/>           'Recall':'recall',<br/>           'MC':make_scorer(matthews_corrcoef),<br/>          'GM':make_scorer(geometric_mean_score)}</span><span id="9e97" class="lm jp hi li b fi lr lo l lp lq">rf=RandomForestClassifier(random_state=0)</span></pre><h1 id="a27c" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated"><strong class="ak">评估指标</strong></h1><p id="c41f" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">处理分类问题时最常用的度量之一是准确性，它等于:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ls"><img src="../Images/4dfd62a094c3ac2700d9c20e1e148b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/0*v-MAxaNgczHT04qs"/></div></figure><p id="a564" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">然而，当我们处理不平衡数据时，使用准确性来评估分类器的性能可能会产生误导，因为它对模型在多数类上的能力提供了过于乐观的估计。</p><p id="cbbe" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">为了演示这一陈述，让我们在预测不平衡数据集的类之后，可视化混淆矩阵:</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="fa08" class="lm jp hi li b fi ln lo l lp lq">t0 = [('cat', OneHotEncoder(handle_unknown = "ignore"), categorical_ix)]<br/>col_transform0 = ColumnTransformer(transformers=t0)</span><span id="de9f" class="lm jp hi li b fi lr lo l lp lq">pipeline = Pipeline(steps=[('prep', col_transform0),('m',rf)])<br/>cv_results_none=cross_validate(pipeline, train_x, train_y.values.ravel(), scoring=scoring, cv=kfold)</span><span id="6d13" class="lm jp hi li b fi lr lo l lp lq">pipeline.fit(train_x,train_y.values.ravel())<br/>predict_y=pipeline.predict(test_x)<br/>plot_confusion_matrix(pipeline, test_x,test_y,display_labels=["No","Yes"], cmap=plt.cm.Blues) ;</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lt"><img src="../Images/42e32938f9cb23fba94776d1331fcd54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*PujtrDP1aoF1Qx38JPeSnw.png"/></div></figure><p id="f47f" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">通过查看上面的混淆矩阵，我们看到96%的多数类被正确分类，而77%的少数类被错误分类。但是，总的准确率是88.68%。</p><p id="fff6" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">由于准确性的缺点，在处理不平衡数据时，还有其他流行的评估指标:</p><p id="788c" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> <em class="lu"> 1。Precision: </em> </strong>这是一个度量标准，用于计算正类的正确预测百分比。它的计算方法是正确预测的正例数除以预测的正例总数。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lv"><img src="../Images/c0494c6a6e881a599fa1489834dcf041.png" data-original-src="https://miro.medium.com/v2/resize:fit:156/0*XiqAAuKVlnphgAkl"/></div></figure><p id="c7cd" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj">2<em class="lu">。回忆:</em> </strong>它计算所有可能做出的肯定预测中，肯定类别的正确预测的百分比。它的计算方法是正确预测的正例数除以可预测的正例总数。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lw"><img src="../Images/f62d97518f4fbbbb3f78b774bcb74332.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/0*Fx00ICj_TiqLYq9a"/></div></figure><p id="36a9" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> <em class="lu"> 3。F1得分:</em> </strong> it <strong class="ki hj"> <em class="lu"> </em> </strong>计算为精度和召回率的调和平均值，给每个相同的权重。它允许使用单个分数来评估模型，同时考虑精确度和召回率。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lx"><img src="../Images/138dd33896ecaa665fd7c221ddd827be.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/0*_bY1AsMNBSp71XMN"/></div></figure><p id="5c39" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> <em class="lu"> 4。几何平均值:</em> </strong>该度量试图最大化每个类的精度，同时保持这些精度的平衡。它是灵敏度/回忆率和特异性/真阴性率乘积的平方根。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/ec523f395f8c87632252fb560ab3017f.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/0*HhdzZK-kQNh05zZj"/></div></figure><p id="30c6" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> <em class="lu"> 5。马修斯相关系数:</em> </strong>是观测值和预测值标签之间的相关系数，其值在-1和+1之间。系数+1表示完美预测，0表示不比随机预测好，1表示预测和观察完全不一致。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lz"><img src="../Images/5c8be82673dc01dc834bce67d3706c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*YJVQ6-SUPfr-MONm"/></div></figure><h1 id="cee6" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">方法/技术:</h1><p id="0280" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">下面您将看到本文中用于处理不平衡数据集的每种方法的描述，以及所应用的评估指标的结果:</p><p id="5693" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> 1。使用原始数据集(不平衡数据)进行分类:</strong>首先，我们将使用原始数据计算评估指标:</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="34a5" class="lm jp hi li b fi ln lo l lp lq">results = pd.DataFrame(columns=scoring.keys())<br/>results.loc['No sampling']=  [np.mean(cv_results_none['test_{}'.format(score)]) for score in scoring.keys()] <br/>results</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ma"><img src="../Images/5dcd3940377c2d6bef33efa016c9edc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ATpg5OAgPuaMHg5ScHrWRg.png"/></div></div></figure><p id="2af4" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> 2。合成少数过采样技术(SMOTE): </strong>这是一种过采样技术，通过使用K最近邻(KNN)算法创建人工示例来增加少数类的数量。基本上来说，该方法采用所有少数样本，计算每个样本的KNN(默认K为5)，随机选择一个或多个计算的K个最近邻，然后取考虑中的样本与其最近邻的差，并将该差乘以0到1之间的随机数，最后将其添加到考虑中的原始样本。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="6198" class="lm jp hi li b fi ln lo l lp lq">t = [('cat', OneHotEncoder(handle_unknown = "ignore"), categorical_ix), ('num', StandardScaler(), numerical_ix)]<br/>col_transform = ColumnTransformer(transformers=t)</span><span id="4a59" class="lm jp hi li b fi lr lo l lp lq">pipeline = Pipeline(steps=[('prep', col_transform),('imbalance',SMOTE()) ,('m',rf)])<br/>cv_results_smote=cross_validate(pipeline, train_x, train_y.values.ravel(), scoring=scoring, cv=kfold)</span><span id="99d0" class="lm jp hi li b fi lr lo l lp lq">results = pd.DataFrame(columns=scoring.keys())<br/>results.loc['SMOTE']=  [np.mean(cv_results_smote['test_{}'.format(score)]) for score in scoring.keys()] <br/>results</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/ea52e5f064ae5b90978e09657309ff64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iMbbjquvpI3oaBPLpRHjQA.png"/></div></div></figure><p id="6b56" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> 3。自适应合成(ADASYN): </strong>这是另一种使用KNN的过采样技术。然而，ADASYN并没有像SMOTE那样创建相同数量的合成样本，而是计算一个密度分布函数，并将其作为一个标准来决定需要为原始数据的每个少数样本生成的人工样本的数量。换句话说，它根据与其他少数民族样本相比的学习难度，对不同的少数民族类别样本应用加权分布。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="98a2" class="lm jp hi li b fi ln lo l lp lq">pipeline = Pipeline(steps=[('prep', col_transform),('imbalance',ADASYN()) ,('m',rf)])<br/>cv_results_adasyn=cross_validate(pipeline, train_x, train_y.values.ravel(), scoring=scoring, cv=kfold)</span><span id="fec3" class="lm jp hi li b fi lr lo l lp lq">results = pd.DataFrame(columns=scoring.keys())<br/>results.loc['ADASYN']=  [np.mean(cv_results_adasyn['test_{}'.format(score)]) for score in scoring.keys()] <br/>results</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mc"><img src="../Images/12d777e30593775f42e01f34cfd35407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JK0KNReH8MoEe6pR1b4PKg.png"/></div></div></figure><p id="abce" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> 4。邻域清理规则:</strong>这是一种欠采样方法，计算每个多数和少数类的KNN。当它的K个最近邻(默认K是3)属于其他少数类时，它基本上丢弃了多数类的例子。此外，如果为少数类计算的K个邻居属于多数类，则属于多数类的那些最近邻居被移除。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="7505" class="lm jp hi li b fi ln lo l lp lq">pipeline = Pipeline(steps=[('prep', col_transform),('imbalance',NeighbourhoodCleaningRule()) ,('m',rf)])<br/>cv_results_ncr=cross_validate(pipeline, train_x, train_y.values.ravel(), scoring=scoring, cv=kfold)</span><span id="af34" class="lm jp hi li b fi lr lo l lp lq">results = pd.DataFrame(columns=scoring.keys())<br/>results.loc['NCR']=  [np.mean(cv_results_ncr['test_{}'.format(score)]) for score in scoring.keys()] <br/>results</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es md"><img src="../Images/146e49a0282c849423ad43d0c328a240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*foKZIiZPI7nti0yGxELJrw.png"/></div></div></figure><p id="4f1e" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> 5。单边选择:</strong>这种技术方法结合了两种欠采样，即Tome链接和压缩最近邻规则，以便从多数类中去除距离判定线或边界线太近和太远的样本。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="12eb" class="lm jp hi li b fi ln lo l lp lq">pipeline = Pipeline(steps=[('prep', col_transform),('imbalance',OneSidedSelection()) ,('m',rf)])<br/>cv_results_oss=cross_validate(pipeline, train_x, train_y.values.ravel(), scoring=scoring, cv=kfold)</span><span id="f1be" class="lm jp hi li b fi lr lo l lp lq">results = pd.DataFrame(columns=scoring.keys())<br/>results.loc['OSS']=  [np.mean(cv_results_oss['test_{}'.format(score)]) for score in scoring.keys()] <br/>results</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es me"><img src="../Images/5449c06ec7c9a399617fc20756148a14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*THiwEgtWgU634V3nadS21g.png"/></div></div></figure><p id="4723" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> 6。这种技术是一种过采样和欠采样方法相结合的结果。该方法首先使用SMOTE进行过采样，然后使用被称为编辑最近邻规则(ENN)的欠采样技术，该欠采样技术移除其类标签与其三个最近邻中的至少两个的类不同的任何样本。</strong></p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="06a5" class="lm jp hi li b fi ln lo l lp lq">pipeline = Pipeline(steps=[('prep', col_transform),('imbalance',SMOTEENN()) ,('m',rf)])<br/>cv_results_smeenn=cross_validate(pipeline, train_x, train_y.values.ravel(), scoring=scoring, cv=kfold)</span><span id="c2c0" class="lm jp hi li b fi lr lo l lp lq">results = pd.DataFrame(columns=scoring.keys())<br/>results.loc['SMEENN']=  [np.mean(cv_results_smeenn['test_{}'.format(score)]) for score in scoring.keys()] <br/>results</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mc"><img src="../Images/8e74b44d71ed09f2131e52808be43ce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WrrEXexAWRwYMC1ktRnznw.png"/></div></div></figure><p id="85f2" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> 7。SMOTE + TOMEK: </strong>这是过采样方法的另一种组合，在这种情况下是SMOTE，欠采样是Tome Links。在为少数类创建合成样本之后，它移除太靠近边界线或少数样本的多数类样本。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="7082" class="lm jp hi li b fi ln lo l lp lq">pipeline = Pipeline(steps=[('prep', col_transform),('imbalance',SMOTETomek()) ,('m',rf)]) </span><span id="5bb7" class="lm jp hi li b fi lr lo l lp lq">cv_results_smt=cross_validate(pipeline, train_x, <br/>train_y.values.ravel(), scoring=scoring, cv=kfold)  </span><span id="3e75" class="lm jp hi li b fi lr lo l lp lq">results = pd.DataFrame(columns=scoring.keys()) results.loc['SMT']=  [np.mean(cv_results_smt['test_{}'.format(score)]) for score in scoring.keys()]  </span><span id="0ee0" class="lm jp hi li b fi lr lo l lp lq">results</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mf"><img src="../Images/089b6b1efbb85bdb1f016f8cb9faeae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQJ4M1HNn7NKMmX9YnemAg.png"/></div></div></figure><p id="0434" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> 8。成本敏感学习:</strong>处理不平衡数据的另一种方法是通过改变other中分类器的结构，使其适合处理这种数据。由于模型的目标是最小化误差，相对于负类(多数)，成本敏感学习为来自正类(少数)的样本的错误分类引入了更高的成本。</p><p id="45ab" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">在下面的代码中，你会看到超参数<em class="lu"> class_weight=balanced。</em>术语“平衡的”是指模型将调整<em class="lu">“y”</em>中值的权重，与它们的频率成反比:n _ samples/(n _ classes * NP . bin count(y))。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="df01" class="lm jp hi li b fi ln lo l lp lq">pipeline = Pipeline(steps=[('prep', col_transform0),('m',RandomForestClassifier(class_weight='balanced'))])<br/>cv_results_cost=cross_validate(pipeline, train_x, train_y.values.ravel(), scoring=scoring, cv=kfold)</span><span id="7840" class="lm jp hi li b fi lr lo l lp lq">results = pd.DataFrame(columns=scoring.keys())<br/>results.loc['cost_sen']=  [np.mean(cv_results_cost['test_{}'.format(score)]) for score in scoring.keys()] <br/>results</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mg"><img src="../Images/0002e1592cc27d9b31b689f8c93d2448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i095ASEF_jyRjR2_y_V7Pg.png"/></div></div></figure><h1 id="f912" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated"><strong class="ak">方法间的最终比较</strong></h1><p id="c261" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">在下表中，您将看到上述所有方法的评估指标:</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="8ca3" class="lm jp hi li b fi ln lo l lp lq">cv_result=[cv_results_none,cv_results_smote,cv_results_adasyn,cv_results_ncr,<br/>         cv_results_oss,cv_results_smeenn,cv_results_smt,cv_results_cost]</span><span id="c7e9" class="lm jp hi li b fi lr lo l lp lq">methods=["No sampling","smote","adasyn","ncr","oss","smeenn","smt","Cost sensitive"]<br/>results = pd.DataFrame(columns=scoring.keys())</span><span id="8fcd" class="lm jp hi li b fi lr lo l lp lq">for i,j in zip(methods,cv_result):<br/>    results.loc[i]=  [np.mean(j['test_{}'.format(score)]) for score in scoring.keys()]</span><span id="7423" class="lm jp hi li b fi lr lo l lp lq">results</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mh"><img src="../Images/5f701d336f81689baacfede7f01d6b6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HmkVgIJHusbNMwEhpjjkVw.png"/></div></div></figure><p id="6e1c" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">尽管单侧选择产生了最好的准确度分数(89.11%)，但我们可以得出结论，最好地处理这个不平衡数据集的方法是SMOTEENN，因为它具有最高的F1分数(47.94%)、最高的Matthews系数(40.88%)和几何平均分数(72.56%)。</p><p id="1e23" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">这只是对一些用于处理不平衡数据的方法的简单介绍，然而，重要的是要记住，它们不是解决机器学习中问题的理想方法，因为它主要是关于试错。我真的希望你和我一样喜欢阅读这篇文章并从中学习新的东西。</p><h1 id="1f32" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">参考</h1><p id="2379" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">阿尔贝托·费尔南德斯、萨尔瓦多·加西亚、米克尔·加拉尔、罗纳尔多·普拉蒂、巴托什·科劳兹克、弗朗西斯科·埃雷拉——从不平衡的数据集中学习——施普林格国际出版公司(2018年)</p><p id="af50" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">巴蒂斯塔，普拉蒂，R. C .，&amp;莫纳德，M. C. (2004)。平衡机器学习训练数据的几种方法的行为研究。<em class="lu"> ACM SIGKDD探索新闻简报</em>，<em class="lu"> 6 </em> (1)，20–29。</p><p id="2afb" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">何海辉，白，杨，贾亚东，李，等(2008年6月)。ADASYN:用于不平衡学习的自适应合成采样方法。在<em class="lu"> 2008年IEEE国际神经网络联合会议(IEEE世界计算智能大会)</em>(第1322-1328页)。IEEE。</p><p id="244d" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">舒拉、鲍耶、K. W .、霍尔、L. O .、凯格尔迈耶、W. P. (2002年)。SMOTE:合成少数过采样技术。<em class="lu">人工智能研究杂志</em>，<em class="lu"> 16 </em>，321–357。</p></div></div>    
</body>
</html>