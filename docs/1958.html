<html>
<head>
<title>Movie Review Sentiment Analysis w/ RNNs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带RNNs的电影评论情感分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/movie-review-sentiment-analysis-w-rnns-5227e7b52f8c?source=collection_archive---------18-----------------------#2021-03-28">https://medium.com/analytics-vidhya/movie-review-sentiment-analysis-w-rnns-5227e7b52f8c?source=collection_archive---------18-----------------------#2021-03-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/9746be2924b5ed58257d3bcf5e4cd69e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*b3yB6XdwfZU1ouew"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">照片由<a class="ae it" href="https://unsplash.com/@chuklanov?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Avel Chuklanov </a>在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><p id="0d1d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">据IMDb称，它有超过50万部电影和近550万条用户评论，每条评论都是1-10分的评分和文字评论；我想用这些评论来训练一个模型，可以猜测未来评论的情绪。自然语言处理的类型称为文本分类。这里的大部分工作来自FastAI课程的第8课，我不能向任何对深度学习感兴趣的人推荐。跟随用于训练模型的<a class="ae it" href="https://github.com/jacKlinc/movie_review_sentiment/blob/main/notebooks/1_mdl_nlp_train_sentiment.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><h1 id="04d1" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">预处理</h1><p id="9b19" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">对于一台只能理解1和0的计算机来说，文本是一个非常抽象的概念——它需要努力让它为理解人类语言做好准备。语料库中的每个词就像一个有限级别的分类变量；语料库级别被称为词汇库。</p><p id="9512" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有两个步骤:</p><ol class=""><li id="3f12" class="kv kw hh iw b ix iy jb jc jf kx jj ky jn kz jr la lb lc ld bi translated"><em class="le">标记化</em>:为语料库创建一个单词/子单词列表。</li><li id="4727" class="kv kw hh iw b ix lf jb lg jf lh jj li jn lj jr la lb lc ld bi translated"><em class="le">数字化</em>:将单词转换成数字，并将它们的位置转换成索引。</li></ol><h2 id="adb5" class="lk jt hh bd ju ll lm ln jy lo lp lq kc jf lr ls kg jj lt lu kk jn lv lw ko lx bi translated">符号化</h2><p id="0571" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">看起来很简单:将文本传递给标记器并开始训练？不幸的是，我们需要处理像标点和连字符这样的事情。将文本拆分成单词的方法有很多种，但我在这里重点介绍的是基于单词的方法。幸运的是，有一些单词分词器——其中一个叫做<code class="du ly lz ma mb b">spacy</code>。它知道如何处理单词，比如“它的”。</p><pre class="mc md me mf fd mg mb mh mi aw mj bi"><span id="da80" class="lk jt hh mb b fi mk ml l mm mn">first(spacy(["It's really sunny on Monday"]))<br/>&gt;&gt; ['It', "'s", 'really', 'sunny', 'on', 'Monday']</span></pre><p id="6bc7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">FastAI添加了自己的特殊标记规则来为单词添加上下文，比如评论的开头或大写字母。</p><pre class="mc md me mf fd mg mb mh mi aw mj bi"><span id="d0e1" class="lk jt hh mb b fi mk ml l mm mn">tok = Tokenizer(spacy)(["It's really sunny on Monday"]); tok<br/>&gt;&gt; ['xxbos', 'xxmaj', 'it', "'s", 'really', 'sunny', 'on', 'xxmaj', monday']</span></pre><p id="ecf7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><code class="du ly lz ma mb b">xxbos</code>表示流(bos)的开始，<code class="du ly lz ma mb b">xxmaj</code>是大写字母。像“它”和“It”这样的词对我们来说是同一个词，但是对模型来说，它们是独特的。这些特殊的记号解析它们是一样的，但是用一个大写字母记号来区分它们。</p><h2 id="7b19" class="lk jt hh bd ju ll lm ln jy lo lp lq kc jf lr ls kg jj lt lu kk jn lv lw ko lx bi translated">数值化</h2><p id="1c18" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">该模型现在知道如何拆分评论中的每个单词，但我们仍然需要将它们转换成计算机可以理解的数字。数值化的工作方式类似于one-hot编码变量，其中<code class="du ly lz ma mb b">Numericalize</code>为分类变量列出所有可能的级别，用vocab中的索引替换每个级别。</p><pre class="mc md me mf fd mg mb mh mi aw mj bi"><span id="0c5c" class="lk jt hh mb b fi mk ml l mm mn">num = Numericalize(min_freq=3, max_vocab=60000)<br/>num.setup(tok)<br/>num(tok)<br/>&gt;&gt; tensor([ 4, 11, 435, 434, ... ])</span></pre><p id="edb7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">存储所有单词是不实际的，因为vocab会太大，<code class="du ly lz ma mb b">max_vocab</code>属性限制了它，而<code class="du ly lz ma mb b">min_freq</code>要求它在被添加之前出现几次。这个张量现在可以直接输入网络的嵌入层。</p><h1 id="3a32" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">RNNs</h1><p id="0c86" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">递归神经网络用于NLP，因为它们对文本在序列中的位置进行加权。基本思想是网络的第一层使用第一个字的嵌入；第二种使用第二个单词的嵌入和第一个单词的输出激活相结合；第三种使用第三个单词的嵌入和第二个单词的激活。前一个单词的激活被称为<strong class="iw hi"> <em class="le">隐藏状态</em> </strong>。</p><figure class="mc md me mf fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mo"><img src="../Images/be128daff77db8251501bf7509764ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5Oqn9ZNOJsgfcaNC"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">来自FastAI 的RNN</figcaption></figure><p id="4a61" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">他们没有显式声明每一层，而是将它们重构到一个for循环中——这是经常性的。<code class="du ly lz ma mb b">for</code>循环的极限是层数。</p><figure class="mc md me mf fd ii"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="cea4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上面的第一个类<code class="du ly lz ma mb b">LanguageModel</code>，明确地向每个网络的下一层提供先前的激活；<code class="du ly lz ma mb b">LanguageModelRecurrent</code>使用一个循环来实现同样的功能。上面的模型产生了大约50%的精确度。</p><h2 id="5a6e" class="lk jt hh bd ju ll lm ln jy lo lp lq kc jf lr ls kg jj lt lu kk jn lv lw ko lx bi translated">状态和信号</h2><p id="83c7" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated"><code class="du ly lz ma mb b">forward()</code>为隐藏状态，<code class="du ly lz ma mb b">h</code> <em class="le">，</em>存储以前的激活；然后，这被重置为零，丢弃关于给评论更多上下文的单词的潜在重要信息。将重置移入<code class="du ly lz ma mb b">init()</code>通过仅在对象实例化时重置来解决这个问题。这保持了状态，但打开了另一个问题:梯度爆炸。维护状态会为语料库中的每个标记创建一个层，可能有60，000个，需要对每个层进行梯度计算，这导致训练速度很慢。丢弃除了前三个梯度之外的所有梯度解决了这个问题；这被称为时间截断反向传播(tBPTT)。</p><figure class="mc md me mf fd ii"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="17d5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上面的第二个RNN实现通过循环序列长度<code class="du ly lz ma mb b">sl</code>而不是仅仅三个字来添加信号。增加句子长度会给出更多的上下文；潜在地提高准确性——这被称为<strong class="iw hi"> <em class="le">发信号</em> </strong>。引入状态将准确度提高到<strong class="iw hi">的57% </strong>，添加更多信号将准确度提高到<strong class="iw hi">的64% </strong>。</p><h2 id="60a5" class="lk jt hh bd ju ll lm ln jy lo lp lq kc jf lr ls kg jj lt lu kk jn lv lw ko lx bi translated">多层和LSTMs</h2><p id="f9cc" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">多层rnn将第一层的输出输入到下一层，为模型提供更长的学习时间范围，并创建对文本的更好理解。考虑到这一点，精度应该会提高很多，但反而下降到<strong class="iw hi"> 48% </strong>，比我们上次的单层模型下降了16。这种下降的原因是所谓的爆炸和消失梯度现象。由于每个浮点数只有32位，重复相乘矩阵会导致其精度下降；当浮点数偏离1时，它们会失去精确性，因为需要更多的位来存储它们的值。通过两层传递矩阵会导致每次乘法的数值偏离实际值。如果梯度太小，算法不会更新；太大了，他们的更新太快了。</p><p id="e3ae" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">长期短期记忆(LSTM)架构通过引入另一种称为<strong class="iw hi"> <em class="le">单元状态</em> </strong>的隐藏状态来保留更多的句子记忆，从而解决了这一问题。隐藏状态关注当前单词标记，而单元格状态考虑单词序列中较早的单词激活。</p><figure class="mc md me mf fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mr"><img src="../Images/1c673815e51ca4ee12f2cf508ec4c5e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6fB5oQIQgCD3o3ogwWKYw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">来自<a class="ae it" href="https://arxiv.org/abs/1808.05578" rel="noopener ugc nofollow" target="_blank">拉恩</a>的LSTM细胞</figcaption></figure><p id="4a8b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上图中的橙色方框代表网络中的层，<em class="le"> tanh </em>是双曲线tan，另一个是<em class="le"> sigmoid </em>。将<em class="le"> sigmoid </em>的输出从0调整到1，将<em class="le"> tanh </em>的输出从-1调整到+1，解决了爆炸梯度问题。<strong class="iw hi"> <em class="le">单元格状态</em> </strong> <em class="le"> </em>控制LSTM，<em class="le"> </em>使用图中的黄色圆圈更新；他们输入的乘积决定了状态:更新那些接近1的；丢弃接近0的部分。网络现在可以保持对单词的长期记忆，使较长的句子更容易理解。</p><figure class="mc md me mf fd ii"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="3f7d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><code class="du ly lz ma mb b">init()</code> <em class="le"> </em>定义了LSTM使用的各个门，<code class="du ly lz ma mb b">forward()</code>实现了它们。很方便的是，PyTorch有一个内置的<a class="ae it" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html" rel="noopener ugc nofollow" target="_blank">类</a>，所以不需要写那么多。有了LSTM，多层模型的精确度从<strong class="iw hi"> 48% </strong>上升到<strong class="iw hi">81%</strong>——相当大的一个飞跃！模型的验证损失远远高于训练损失，表明数据过度拟合。</p><h2 id="df01" class="lk jt hh bd ju ll lm ln jy lo lp lq kc jf lr ls kg jj lt lu kk jn lv lw ko lx bi translated">规范化</h2><p id="b184" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">在传统的ML模型中，这一步相当简单，只需在损失函数后附加一个正则化项:</p><figure class="mc md me mf fd ii er es paragraph-image"><div class="er es ms"><img src="../Images/fd13e8aef56655671a2430259462f1a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/0*zyBBHgv0wAgYjE82"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">重量衰减参数</figcaption></figure><p id="38d0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在NLP中，使用数据扩充的一些方法的过程稍微复杂一些，将文本翻译成另一种语言，然后以不同的方式再翻译回短语句子；这是一个开放的研究领域，超出了本文的范围。</p><p id="5913" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">辍学</strong></p><p id="83b2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">辍学背后的想法是随机丢弃网络中的随机神经元，以帮助网络朝着共同的目标努力。排除这些神经元会在网络中引入噪声，使模型更加健壮，并且不容易过度拟合。</p><figure class="mc md me mf fd ii er es paragraph-image"><div class="ab fe cl mt"><img src="../Images/b9fee55e20d55e30924c93dfcbbbfd3f.png" data-original-src="https://miro.medium.com/v2/format:webp/1*uxpH46OpTIj63j1MKQ-T2Q.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">被<a class="ae it" href="https://www.kdnuggets.com/2018/09/dropout-convolutional-networks.html" rel="noopener ugc nofollow" target="_blank"> KDNuggets </a>退学</figcaption></figure><p id="a6b6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">丢弃神经元由概率<code class="du ly lz ma mb b">p</code>控制，逐层变化，在复杂的网络层中适当丢弃。漏失遵循下面定义的伯努利分布。</p><figure class="mc md me mf fd ii er es paragraph-image"><div class="er es mu"><img src="../Images/b76a7026219402d67e074cc6a4ac2109.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/0*qY6lbxCgJPJnwOiu"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">二项分布</figcaption></figure><p id="76ad" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">PyTorch实施的辍学:</p><pre class="mc md me mf fd mg mb mh mi aw mj bi"><span id="b929" class="lk jt hh mb b fi mk ml l mm mn">class Dropout(Module):<br/>  def __init__(self):<br/>    self.p = p<br/>  <br/>  def forward(self, x):<br/>    if not self.training:<br/>      return x<br/>    mask.new(*x.shape).bernoulli_(1-p)<br/>    return x * mask.div_(1-p)</span></pre><p id="6ac6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">重量捆绑</strong></p><p id="a368" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">语言模型的输入嵌入(第一层)将英语单词映射到激活，并将输出激活映射到英语单词。将这些设置为相同可以提高准确性。这是论文。</p><pre class="mc md me mf fd mg mb mh mi aw mj bi"><span id="0551" class="lk jt hh mb b fi mk ml l mm mn">self.h_o.weight = self.i_h.weight</span></pre><p id="4460" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">FastAI提供了一个<code class="du ly lz ma mb b">TextLearner</code> <em class="le"> </em>类来为我们完成大部分工作:</p><pre class="mc md me mf fd mg mb mh mi aw mj bi"><span id="d9a5" class="lk jt hh mb b fi mk ml l mm mn">learn = TextLearner(dls, LanguageModelLSTM(len(vocab), 64, 2, 0.4),<br/>                loss_func=CrossEntropyLossFlat(), metrics=accuracy)<br/>learn.fit_one_cycle(15, 1e-2, wd=0.1)</span></pre><p id="e182" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">准确率从<strong class="iw hi"> 81% </strong>到<strong class="iw hi">87%</strong>；这在五年前会是世界上最好的！FastAI的创造者使用与上述相同的技术训练了一个<a class="ae it" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">模型</a>，达到了94%的准确率——只是最近才被打破。</p><h1 id="c0bb" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">预言；预测；预告</h1><p id="7bb8" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">FastAI提供了一个IMDb数据集<a class="ae it" href="https://course.fast.ai/datasets" rel="noopener ugc nofollow" target="_blank">和25，000条两极分化的评论，下面是非常简单的语法:</a></p><pre class="mc md me mf fd mg mb mh mi aw mj bi"><span id="2adc" class="lk jt hh mb b fi mk ml l mm mn">path = untar_data(URLs.IMDB)</span></pre><p id="6fcd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><code class="du ly lz ma mb b">DataBlock</code> <em class="le"> </em>使用<code class="du ly lz ma mb b">path</code>将实例加载到模型中。</p><pre class="mc md me mf fd mg mb mh mi aw mj bi"><span id="9631" class="lk jt hh mb b fi mk ml l mm mn">dls_clas = DataBlock(<br/>  blocks=(TextBlock.from_folder(path), CategoryBlock),<br/>  get_y=parent_label,<br/>  get_items=partial(get_text_files, folders=['train', 'test']),<br/>  splitter=GrandparentSplitter(valid_name='test')<br/>).dataloaders(path, path=path, bs=128, seq_len=72)</span></pre><p id="b4e9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">创建模型使用了<code class="du ly lz ma mb b">DataBlock</code> <em class="le">、</em> <code class="du ly lz ma mb b">AWD_LSTM</code>:正规化的LSTM建筑、<code class="du ly lz ma mb b">drop_multi</code>:全球化的辍学生。<code class="du ly lz ma mb b">to_fp16()</code> <em class="le"> </em>将所有32位浮点数转换为16位，从而加快训练速度。</p><pre class="mc md me mf fd mg mb mh mi aw mj bi"><span id="99d8" class="lk jt hh mb b fi mk ml l mm mn">l3 = text_classifier_learner(<br/>  dls_clas, <br/>  AWD_LSTM,<br/>  drop_mult=0.5,<br/>  metrics=accuracy<br/>).to_fp16()</span></pre><p id="1b89" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我建议将训练好的模型和vocab保存在<code class="du ly lz ma mb b"><a class="ae it" href="https://pypi.org/project/pickle5/" rel="noopener ugc nofollow" target="_blank">pickle</a></code>文件中，因为在Colab GPUs上训练它需要两个多小时。加载模型并进行预测的过程如下:</p><pre class="mc md me mf fd mg mb mh mi aw mj bi"><span id="59fa" class="lk jt hh mb b fi mk ml l mm mn">l3 = l3.load('/path/to/my_saved_model')<br/>l3.predict('That was terrible!')<br/> &gt;&gt; ('neg', tensor(0), tensor([0.8067, 0.1933]))</span></pre><h2 id="e45e" class="lk jt hh bd ju ll lm ln jy lo lp lq kc jf lr ls kg jj lt lu kk jn lv lw ko lx bi translated">主办；主持</h2><p id="a55a" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">我通常会将这个模型放在一个Streamlit web应用程序中进行推理，就像我在我的<a class="ae it" rel="noopener" href="/analytics-vidhya/mask-detector-w-fastai-and-streamlit-sharing-62448b4cb7b6">以前的</a>项目中所做的那样，但是因为保存的模型非常大，我将需要使用云托管。</p><h1 id="5a20" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">参考</h1><ul class=""><li id="a0a8" class="kv kw hh iw b ix kq jb kr jf mv jj mw jn mx jr my lb lc ld bi translated">【https://www.imdb.com/pressroom/stats/ T2】号</li><li id="5fd7" class="kv kw hh iw b ix lf jb lg jf lh jj li jn lj jr my lb lc ld bi translated">【https://course.fast.ai/videos/?lesson=8 T4】</li><li id="b38e" class="kv kw hh iw b ix lf jb lg jf lh jj li jn lj jr my lb lc ld bi translated"><a class="ae it" href="https://arxiv.org/abs/1808.05578" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1808.05578</a></li><li id="e465" class="kv kw hh iw b ix lf jb lg jf lh jj li jn lj jr my lb lc ld bi translated"><a class="ae it" href="https://www.kdnuggets.com/2018/09/dropout-convolutional-networks.html" rel="noopener ugc nofollow" target="_blank">https://www . kdnugges . com/2018/09/dropout-convolatile-networks . html</a></li></ul></div></div>    
</body>
</html>