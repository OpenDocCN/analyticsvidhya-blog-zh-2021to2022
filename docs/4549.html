<html>
<head>
<title>Akira’s Machine Learning News — Issue #33</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Akira的机器学习新闻—第33期</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/akiras-machine-learning-news-issue-33-a9866e366a0e?source=collection_archive---------15-----------------------#2021-11-10">https://medium.com/analytics-vidhya/akiras-machine-learning-news-issue-33-a9866e366a0e?source=collection_archive---------15-----------------------#2021-11-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="01ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本周特稿/新闻。</p><ul class=""><li id="450d" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><a class="ae jl" href="https://arxiv.org/abs/2109.13228" rel="noopener ugc nofollow" target="_blank">一个新的自我监督学习数据集</a>已经发布，可以用于商业目的，并且是肖像权友好的。作为业界的一员，我非常感谢这样的数据集，因为学术界经常使用的ImageNet这样的大规模数据通常是没有商用的！</li><li id="2a4e" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jh ji jj jk bi translated"><a class="ae jl" href="https://arxiv.org/abs/2109.10686" rel="noopener ugc nofollow" target="_blank">似乎前期训练的精度越高，下游任务的精度也不一定会变得越高</a>。因此，似乎下游任务的扩展策略需要与预培训的扩展策略分开考虑。</li></ul><p id="ca3e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — — — — — — — — — –</p><p id="6ebd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下面的章节中，我将介绍各种文章和论文，不仅仅是关于上述内容，还包括以下五个主题。</p><ol class=""><li id="c623" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jr ji jj jk bi translated">本周特稿/新闻</li><li id="945f" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jr ji jj jk bi translated">机器学习用例</li><li id="d6ea" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jr ji jj jk bi translated">报纸</li><li id="9f47" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jr ji jj jk bi translated">机器学习技术相关文章</li></ol><p id="4baf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="c5e0" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">1.本周特稿/新闻</h1><p id="9887" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2109.10686?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">培训前的绩效不一定与下游任务的绩效相匹配。</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://arxiv.org/abs/2109.10686" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">arxiv.org</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl la"><img src="../Images/fde682e57450b6d895a94917ec6f7e68.png" data-original-src="https://miro.medium.com/v2/0*JdUX1iVIiTkOlUc0"/></div></figure><p id="b216" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2109.10686]高效扩展:来自预训练和微调变压器的见解<br/>这是一项关于模型规模和下游任务准确性之间关系的研究。随着模型变大，预训练性能变得更好，但它不一定与下游任务的性能相匹配。作者提出了DeepNarrow策略，对模型进行了缩小和深化，成功地将训练速度提高了40%，同时保持了下游任务的性能。</p></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><p id="f635" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2109.13228?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">140万张图片的数据集，避免了版权和肖像权等问题</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://arxiv.org/abs/2109.13228" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">arxiv.org</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl la"><img src="../Images/a83fb9fe6be52999f01e0bc869b4079f.png" data-original-src="https://miro.medium.com/v2/0*tdYDafCCJ6neEZLe"/></div></figure><p id="49e6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2109.13228]PASS:ImageNet替代了没有人参与的自我监督预训练<br/>ImageNet等大型数据集在未经人们同意的情况下许可和使用他们的照片方面存在问题。为了解决这个问题，作者收集了CC-BY许可证下可用的数据，并发布了PASS，这是一个用于自我监督学习的数据集，将人排除在数据之外。他们证实，它避免了版权等问题，可以用MoCo、迪诺等进行训练。</p></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><p id="7a57" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="073a" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">2.机器学习用例</h1><p id="7e2f" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated"><a class="ae jl" href="https://thegradient.pub/sustainable-ai/?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">可持续人工智能系统</strong> </a></p><div class="lk ll ez fb lm ln"><a href="https://thegradient.pub/sustainable-ai/" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">可持续人工智能系统的必要性</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">这件作品赢得了首届渐变奖。简介人工智能系统是计算密集型的:人工智能…</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">thegradient.pub</p></div></div><div class="lw l"><div class="lx l ly lz ma lw mb lb ln"/></div></div></a></div><p id="2c15" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一篇讨论如何实现可持续的人工智能系统的文章。虽然计算量目前正在增加，但文章建议使用更小的模型，分散计算区域(碳排放区域)，并优化软件和硬件能源。</p></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><p id="5736" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="5e6b" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">3.机器学习论文</h1><p id="6d84" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2106.08265?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">异常检测方法使用计算效率高的预训练模型</strong></a><strong class="ig hi">—</strong><a class="ae jl" href="https://arxiv.org/abs/2106.08265" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">arxiv.org</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl la"><img src="../Images/7ad757e2fb0a05b1aeee6456ba4f4a3a.png" data-original-src="https://miro.medium.com/v2/0*wrpWPvGT3L0d7Ba4"/></div></figure><p id="27b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2106.08265]针对工业异常检测中的全面召回<br/>提出了PatchCore，它使用学习模型进行异常检测。其特征在于具有一个核心集，该核心集聚集了每一片训练样本的特征信息。在MVTech数据集上实现了SotA性能。</p></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><p id="6dde" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2109.14084?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">用文字和视频对比学习</strong></a><strong class="ig hi">——【arxiv.org】<a class="ae jl" href="https://arxiv.org/abs/2109.14084" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"/></a></strong></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es mc"><img src="../Images/31798192b670b87cf2784d41647415e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3_9SmXko3ex-w4YILacu2w.png"/></div></div></figure><p id="caf9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">【2109.14084】VideoCLIP:零镜头视频文本理解对比预训练<br/>提出Video clip，对文本和视频进行对比学习。视频的采样围绕采样文本的时间而变化，通过聚类对高难度样本进行对比学习。该方法在下游任务中的零镜头推理性能优于监督学习。</p></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><p id="e53e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2109.08141?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"/></a><strong class="ig hi">—</strong><a class="ae jl" href="https://arxiv.org/abs/2109.08141" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">【arxiv.org】</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl la"><img src="../Images/48f643d019eead107621b455f100f0aa.png" data-original-src="https://miro.medium.com/v2/0*QapFb85pKrZl2rZW"/></div></figure><p id="5e17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">【2109.08141】用于3D对象检测的端到端转换器模型<br/>作者提出了一种3D对象检测方法，3DETR，可以通过端到端进行训练。3DETR像DETR一样将点云的对象检测作为一个集对集的问题来处理，但与DETR不同，它只使用变压器，并消除了需要手动调整的参数。</p></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><p id="ef93" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2101.05224?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">医学影像自学学习</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://arxiv.org/abs/2101.05224" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">arxiv.org</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es mh"><img src="../Images/a07da8127efa2fc8caf2744b75aff160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AgEUi0GLJKmJD89dYZ-fwQ.png"/></div></div></figure><p id="00ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2101.05224]大型自我监督模型推进医学图像分类<br/>这是一项关于如何在ImageNet上进行自我监督学习，然后再次在医学图像上进行自我监督学习，以提高后续分类任务的性能的研究。由于医学图像通常是从多个角度拍摄的，作者提出了多示例对比学习，将它们视为相同的数据。</p></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><p id="9fcd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2110.04544?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">微调剪辑通过添加小网络和残余连接</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://arxiv.org/abs/2110.04544" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">【arxiv.org】</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl la"><img src="../Images/3301b97e78af41189b21f788f3d9b2c1.png" data-original-src="https://miro.medium.com/v2/0*l57F3-roTiIG0mhl"/></div></figure><p id="1eca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2110.04544] CLIP-Adapter:具有功能适配器的更好的视觉语言模型<br/>他们提出了CLIP-Adapter，可以使用更少的数据来微调剪辑。它在每个图像和语言分支的最后一层之后添加一个小网络，并对其进行微调。该结构的另一个特点是通过残留连接很容易保留原最终层的信息。用更少的数据就能获得好的性能。</p></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><p id="c863" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jl" href="https://arxiv.org/abs/2110.02178?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">结合Transformer和CNN构建高速运行的网络</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://arxiv.org/abs/2110.02178" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">arxiv.org</strong></a></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl la"><img src="../Images/efb2d1e0b9b7ebfe5fd762fd21e57bd0.png" data-original-src="https://miro.medium.com/v2/0*_ZQbPIUyGFS02w9u"/></div></figure><p id="67c8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2110.02178] MobileViT:轻量级、通用型和移动友好型视觉转换器<br/>作者提出了MobileViT，这是一种结合了转换器和CNN的移动设备高速网络。首先，CNN捕获本地信息，然后Transformer处理全局信息。比MobileNetv3精确5.7%。它可以用于分类、对象检测和分割。</p></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><p id="046e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="c95c" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">4.技术文章</h1><p id="7f58" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated"><a class="ae jl" href="https://nn.labml.ai/?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> Pytorch实现的著名算法</strong></a><strong class="ig hi">——</strong><a class="ae jl" href="https://nn.labml.ai/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">nn . labml . ai</strong></a></p><div class="lk ll ez fb lm ln"><a href="https://nn.labml.ai/" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">labml.ai注释PyTorch论文实现</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">这是神经网络和相关算法的简单PyTorch实现的集合。这些实现…</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">nn.labml.ai</p></div></div><div class="lw l"><div class="mi l ly lz ma lw mb lb ln"/></div></div></a></div><p id="c034" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个网站介绍了许多论文的核心技术的pytorch实现，包括gMLP、GAN和强化学习等新技术。如果你对某项技术感兴趣，你可以在这里查看一下。</p><p id="616a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="87b4" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">5.其他主题</h1><p id="d2d4" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">【aijourn.com】<strong class="ig hi">20艾人观看</strong><strong class="ig hi">——</strong><a class="ae jl" href="https://aijourn.com/20-ai-influencers-you-need-to-be-following" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"/></a></p><div class="lk ll ez fb lm ln"><a href="https://aijourn.com/20-ai-influencers-you-need-to-be-following" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">你需要关注的20个人工智能影响者——人工智能杂志</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">“人工智能将在2029年左右达到人类水平。如果继续这样下去，比如说，到2045年，我们将会…</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">aijourn.com</p></div></div><div class="lw l"><div class="mj l ly lz ma lw mb lb ln"/></div></div></a></div><p id="a620" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一篇介绍人工智能领域20位有影响力的人的文章，并附有对他们的Twitter和LinkedIn账户的描述。</p></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><h2 id="399b" class="mk jt hh bd ju ml mm mn jy mo mp mq kc ip mr ms kg it mt mu kk ix mv mw ko mx bi">— — — — — — — — — — — — — — — — — — –</h2><h1 id="3e31" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">其他博客</h1><div class="lk ll ez fb lm ln"><a href="https://towardsdatascience.com/machine-learning-2020-summary-84-interesting-papers-articles-45bd45c0d35b" rel="noopener follow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">机器学习2020摘要:84篇有趣的论文/文章</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">在这篇文章中，我总共展示了2020年发表的84篇我觉得特别有趣的论文和文章…</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">towardsdatascience.com</p></div></div><div class="lw l"><div class="my l ly lz ma lw mb lb ln"/></div></div></a></div><div class="lk ll ez fb lm ln"><a href="https://towardsdatascience.com/recent-developments-and-views-on-computer-vision-x-transformer-ed32a2c72654" rel="noopener follow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">计算机视觉x变形金刚的最新发展和看法</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">《变形金刚》和CNN的区别，为什么《变形金刚》很重要，它的弱点是什么。</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">towardsdatascience.com</p></div></div><div class="lw l"><div class="mz l ly lz ma lw mb lb ln"/></div></div></a></div><div class="lk ll ez fb lm ln"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/reach-and-limits-of-the-supermassive-model-gpt-3-5012a6ddff00"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">超大质量模型GPT-3的到达和极限</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">在这篇博文中，我将从技术上解释GPT 3号，GPT 3号取得了什么，GPT 3号没有取得什么…</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">medium.com</p></div></div><div class="lw l"><div class="na l ly lz ma lw mb lb ln"/></div></div></a></div><div class="lk ll ez fb lm ln"><a href="https://towardsdatascience.com/do-vision-transformers-see-like-convolutional-neural-networks-paper-explained-91b4bd5185c8" rel="noopener follow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">视觉变形器看起来像卷积神经网络吗？(论文解释)</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">我将仔细研究CNN和变形金刚之间的差异</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">towardsdatascience.com</p></div></div><div class="lw l"><div class="nb l ly lz ma lw mb lb ln"/></div></div></a></div><h2 id="7a33" class="mk jt hh bd ju ml mm mn jy mo mp mq kc ip mr ms kg it mt mu kk ix mv mw ko mx bi">— — — — — — — — — — — — — — — — — — –</h2><h1 id="0105" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">关于我</h1><p id="82bb" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">制造工程师/机器学习工程师/数据科学家/物理学硕士/<a class="ae jl" href="https://t.co/hjHHbG24Ph?amp=1&amp;utm_campaign=Akira%27s%20Machine%20Learning%20News%20%20%20&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank">http://github.com/AkiraTOSEI/</a></p><p id="17ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">推特，我贴一句纸评论。</p></div></div>    
</body>
</html>