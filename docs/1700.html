<html>
<head>
<title>Machine Learning Classification Algorithms with Codes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带代码的机器学习分类算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-classification-algorithms-with-codes-5a8af4491fcb?source=collection_archive---------3-----------------------#2021-03-14">https://medium.com/analytics-vidhya/machine-learning-classification-algorithms-with-codes-5a8af4491fcb?source=collection_archive---------3-----------------------#2021-03-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="b773" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大家好，今天我要讲的是分类算法。你可以在这里找到我关于回归和分类的区别的文章。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/54dc47528689b434543e75c65fe2c776.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*uQjXtB4cEKYXJ2DO7l0BLw.png"/></div></figure><p id="0ef4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们说我们用回归来预测数值数据比如价格预测，用分类来预测没有连续变量比如标注，是或否的问题，我不得不说有些算法可以应用，而且对回归和分类都很管用(比如K-NN，SVM，决策树和随机森林)。今天的话题:</p><ul class=""><li id="dc6f" class="jm jn hi ih b ii ij im in iq jo iu jp iy jq jc jr js jt ju bi translated">朴素贝叶斯分类器(基于概率)</li><li id="d7c7" class="jm jn hi ih b ii jv im jw iq jx iu jy iy jz jc jr js jt ju bi translated">k-最近邻分类器(基于组)</li><li id="2e48" class="jm jn hi ih b ii jv im jw iq jx iu jy iy jz jc jr js jt ju bi translated">逻辑回归(基于最大熵)</li><li id="4e05" class="jm jn hi ih b ii jv im jw iq jx iu jy iy jz jc jr js jt ju bi translated">决策树分类器(基于树)</li><li id="d6c6" class="jm jn hi ih b ii jv im jw iq jx iu jy iy jz jc jr js jt ju bi translated">随机森林分类器(基于集成的Bagging)</li><li id="2310" class="jm jn hi ih b ii jv im jw iq jx iu jy iy jz jc jr js jt ju bi translated">梯度增强分类器(基于集成的增强)</li><li id="c612" class="jm jn hi ih b ii jv im jw iq jx iu jy iy jz jc jr js jt ju bi translated">支持向量机(SVM)</li></ul></div><div class="ab cl ka kb gp kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="hb hc hd he hf"><h2 id="972a" class="kh ki hi bd kj kk kl km kn ko kp kq kr iq ks kt ku iu kv kw kx iy ky kz la lb bi translated">朴素贝叶斯分类器</h2><p id="3af0" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated"><strong class="ih hj">朴素贝叶斯算法</strong>是一种基于贝叶斯定理的分类技术，假设预测器之间相互独立。简而言之，朴素贝叶斯分类器假设一个类中特定特征的存在与任何其他特征的存在无关。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lh"><img src="../Images/66e0288939c12c0f94e09d5fd170622a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*O1wfWpbSw0iAcUSjARtQmw.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">贝叶斯定理</figcaption></figure><p id="0e52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">P(A|B):事件B发生时事件A发生的概率。</p><p id="8db6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个数据集中，我们将预测雇员是否会被雇用。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="8907" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将数据分为测试和训练。我们使用train来训练数据，使用test来测试我们的预测。</p><p id="3104" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它使用拟合函数从数据中学习，并使用预测函数进行预测。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lm ln l"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lo"><img src="../Images/b041560bbfef4c2cf02361be0cfbe0d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*BFzZd-XDS5XE68QMYCEVKg.png"/></div></figure><p id="174c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">准确性分数:它是被分类为正确的值的百分比。</p><p id="3246" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">精度分数:它显示了我们正面猜测的值中有多少实际上是正的。</p><p id="d71c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回忆分数:它显示“有多少真阳性被正确定义？”。</p><p id="00e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">F1得分:它是精确值和召回值的调和平均值。如果精确值和召回值对问题都很重要，F1分数也将变得很重要。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lm ln l"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lp"><img src="../Images/a04a482c2ad6cba9e03973f2e36eeb8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*2EFKoiiEnC1xK7ESQCgJQg.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">b检验的混淆矩阵</figcaption></figure><p id="7cd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">混淆矩阵中的数字是数据中TN、FP、FN、TP值的个数。这是我们的概率</p><p id="9435" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">准确度:(TP + TN) / (TN + FP + FN + TP) <br/>精密度:TP / (FP + TP) <br/>灵敏度:TP / (TP + FN) <br/>特异性:TN / (TN + FP)</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lq"><img src="../Images/a59edc34db913a612481f6a60e6340d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*g5zpskPaxO8uSl0OWT4NTQ.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">混淆矩阵</figcaption></figure><p id="9644" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们根据混淆矩阵来计算准确度:</p><p id="a782" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">(233+15) / (233+16+38+15) = 0.8211920529801</p><h2 id="9b0c" class="kh ki hi bd kj kk kl km kn ko kp kq kr iq ks kt ku iu kv kw kx iy ky kz la lb bi translated">k近邻分类器</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lr"><img src="../Images/c47039bb845d7c63160f130a9bb3ee63.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*ZaNAjZg-x6uszouMe6R1HQ.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">它被列入绿色类，因为它有更多的绿色邻居。</figcaption></figure><p id="2422" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KNN根据待预测值的自变量构成的向量的最近邻的类密度进行预测。计算要预测的点到其他点的距离。闵可夫斯基距离计算函数用于此目的。(K:我们告诉你要计算多少个最近邻。)</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="b6e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了在基于距离的算法(如KNN)中获得更成功的结果，数据被归一化。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lm ln l"/></div></figure><h2 id="52d1" class="kh ki hi bd kj kk kl km kn ko kp kq kr iq ks kt ku iu kv kw kx iy ky kz la lb bi translated">逻辑回归</h2><p id="46d6" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">逻辑回归试图找到区分这两个类别的最佳界限。它常用于线性分类问题。因为线性，所以和线性回归很像。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ls"><img src="../Images/721fec7555dd233ced26c0fdac1d765d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*IvifY_BB4N5oDy45gG-O2A.png"/></div></figure><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lm ln l"/></div></figure><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lm ln l"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lt"><img src="../Images/7d4e0d5c5ca92a55a615504a8fa866d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*1KCWF9gKMtVM8xHapqZiHw.png"/></div></figure><p id="8c38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ROC曲线描绘了假阳性率对正确阳性率。用函数roc_auc_score()找到该曲线下的面积。该字段越接近1，模型预测得越好。要查找ROC-AUC分数:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lm ln l"/></div></figure><h2 id="2871" class="kh ki hi bd kj kk kl km kn ko kp kq kr iq ks kt ku iu kv kw kx iy ky kz la lb bi translated">决策树分类器</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lu"><img src="../Images/e366563a6655b0667e5dd7cef6a62149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*DqXA5RZD7c-D8kJGPQaO-w.png"/></div></figure><p id="0969" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树<strong class="ih hj"> </strong>是一种监督学习算法，主要用于分类问题。它从单个节点开始，变成了树形结构。它通过从数据属性中提取简单的规则并学习这些规则(就像人一样)，来创建一个预测变量值的模型。</p><p id="a627" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们来预测一下Iris数据集中的花的种类。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lm ln l"/></div></figure><h2 id="3e34" class="kh ki hi bd kj kk kl km kn ko kp kq kr iq ks kt ku iu kv kw kx iy ky kz la lb bi translated">随机森林分类器(装袋)</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lv"><img src="../Images/ec459c7868ffaa6cc77bd487b0991761.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*oNJqSIOvENuqhsxqnza3AA.png"/></div></figure><p id="1326" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Bagging中，子集数据的每个集合都用于训练他们的决策树。结果，我们得到了不同模型的集合。使用来自不同树的大多数投票预测，这比单个决策树更健壮。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lm ln l"/></div></figure><h2 id="27bc" class="kh ki hi bd kj kk kl km kn ko kp kq kr iq ks kt ku iu kv kw kx iy ky kz la lb bi translated">梯度推进分类器</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lw"><img src="../Images/6fbc8e00dd723fa759593b2b68e236b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*NKNzHZebRQ3_OyCQyU0ohg.png"/></div></figure><p id="7f9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度推进=梯度下降+推进。它使用梯度下降算法，可以优化任何可微损失函数。一个系综树被一个接一个地构建，并且各个树被顺序地求和。下一个采油树试图恢复损失(实际值和预测值之间的差异)。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lm ln l"/></div></figure><h2 id="32b0" class="kh ki hi bd kj kk kl km kn ko kp kq kr iq ks kt ku iu kv kw kx iy ky kz la lb bi translated">支持向量机</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lx"><img src="../Images/2b5a8569edc3050f6c2c90dad215300c.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*VpISpxZaGY0BSkpBn9TBZg.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">Destek Vektö rleri表示支持向量</figcaption></figure><p id="6689" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像逻辑回归一样，支持向量机试图找到区分这两类的最佳直线。这条线1以内的绿色区域称为边距。边距越宽，两个或多个类的分离越好。SVM预测新样本会落在缺口的哪一边。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lm ln l"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">SVC:支持向量分类器(机器)</figcaption></figure><p id="da07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://github.com/esmabozkurt" rel="noopener ugc nofollow" target="_blank"> github账户/esmabozkurt </a></p></div></div>    
</body>
</html>