<html>
<head>
<title>Predicting the ratings of reviews of a hotel using Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习预测酒店评论的评级</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/predicting-the-ratings-of-reviews-of-a-hotel-using-machine-learning-bd756e6a9b9b?source=collection_archive---------2-----------------------#2021-02-14">https://medium.com/analytics-vidhya/predicting-the-ratings-of-reviews-of-a-hotel-using-machine-learning-bd756e6a9b9b?source=collection_archive---------2-----------------------#2021-02-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4182" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">机器学习(ML)是人工智能的一个领域，其中数据驱动的算法通过接触相关数据来学习模式。在自然语言处理(NLP)领域，即解释人类语言方面，ML已经获得了巨大的重要性。在这篇文章中，我们将关注ML在预测用户评论评分中的应用。本文中使用的数据来自Kaggle ( <a class="ae jd" href="https://www.kaggle.com/andrewmvd/trip-advisor-hotel-reviews" rel="noopener ugc nofollow" target="_blank"> Link </a>)，其中大约有20000条评论来自Trip Advisor。</p><p id="9b4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文中的所有代码都是使用Jupyter Notebook用Python 3编写的。</p><p id="255d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们必须导入我们将用于此任务的相关库。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="c796" class="jn jo hi jj b fi jp jq l jr js">import numpy as np <br/>import pandas as pd<br/>import re<br/>import spacy<br/>from nltk.corpus import stopwords<br/>from wordcloud import WordCloud,STOPWORDS<br/>from tensorflow.keras.utils import to_categorical<br/>import matplotlib.pyplot as plt<br/>import string</span></pre><p id="e360" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看数据中的一些行。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="c87a" class="jn jo hi jj b fi jp jq l jr js">data = pd.read_csv('input/trip-advisor-hotel-reviews/tripadvisor_hotel_reviews.csv')<br/>data.head()</span></pre><figure class="je jf jg jh fd ju er es paragraph-image"><div class="er es jt"><img src="../Images/fa92b3bec14260fd821d5c83b3a7cd1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*jLQP67-wa0xFs2JLhke1XA.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">数据的前5行。</figcaption></figure><p id="6dc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们来看一个数据的小描述。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="7425" class="jn jo hi jj b fi jp jq l jr js">data.describe()</span></pre><figure class="je jf jg jh fd ju er es paragraph-image"><div class="er es kb"><img src="../Images/8e92d4e021026ed4bbcbb722ebdbbf01.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*02SLbN5kiTqo6fZQXp05Ig.png"/></div></figure><p id="7903" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，数据中有20491个观察值。平均评分接近4。让我们看看是否有任何缺失的数据。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="fbb5" class="jn jo hi jj b fi jp jq l jr js">data.isnull().mean()</span></pre><p id="20a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出如下所示:</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="1099" class="jn jo hi jj b fi jp jq l jr js">Review    0.0<br/>Rating    0.0<br/>dtype: float64</span></pre><p id="ef9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不错！没有数据丢失。让我们来看看数据中的独特评级的数量。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="03e8" class="jn jo hi jj b fi jp jq l jr js">data['Rating'].unique()</span></pre><p id="a041" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出如下所示:</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="8086" class="jn jo hi jj b fi jp jq l jr js">array([4, 2, 3, 5, 1])</span></pre><p id="a423" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们只有5个可变评级的唯一值。所以我们会把这个问题当做一个softmax分类问题来处理。从形式上来说，我们的任务是训练一个模型，将酒店评论分为1、2、3、5星。</p><p id="fd8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来看看每个类在表中的分布。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="6f73" class="jn jo hi jj b fi jp jq l jr js">data['Rating'].value_counts(normalize=True)</span></pre><p id="5f23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到的输出是:</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="24bf" class="jn jo hi jj b fi jp jq l jr js">5    0.441853<br/>4    0.294715<br/>3    0.106583<br/>2    0.087502<br/>1    0.069348<br/>Name: Rating, dtype: float64</span></pre><p id="b632" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，大部分评分是5星评分，最少的是1星评分。因此，我们必须注意在足够的一星评级上训练我们的模型。因此，在训练数据时，我们必须对数据集和数据进行分层。这将确保所有类的分布在训练集、验证集和测试集之间是均匀的。这将有助于防止未知类出现在验证或测试集中。</p><p id="9260" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在将创建一个单词云来查看评论中最常出现的单词。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="40f8" class="jn jo hi jj b fi jp jq l jr js">def wordCloud_generator(data, title=None):<br/>    wordcloud = WordCloud(width = 800, height = 800,<br/>                          background_color ='black',<br/>                          min_font_size = 10<br/>                         ).generate(" ".join(data.values))                      <br/>    plt.figure(figsize = (8, 8), facecolor = None) <br/>    plt.imshow(wordcloud, interpolation='bilinear') <br/>    plt.axis("off") <br/>    plt.tight_layout(pad = 0) <br/>    plt.title(title,fontsize=30)<br/>    plt.show()</span><span id="f51c" class="jn jo hi jj b fi kc jq l jr js">wordCloud_generator(data['Review'], title="Top words in reviews")</span></pre><figure class="je jf jg jh fd ju er es paragraph-image"><div class="er es kd"><img src="../Images/1ed37d17a02e0d24ed950a86938937de.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*qHAv_AeH13Xk84G-ODVEMw.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">单词越大，出现的次数就越多</figcaption></figure><p id="d827" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将创建数据的副本来保存原始数据。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="2040" class="jn jo hi jj b fi jp jq l jr js">X = data['Review'].copy()<br/>y = data['Rating'].copy()</span></pre><p id="8c06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">文本数据包含许多无用的词，如停用词(is、an、the、cause等。)标点符号，同一单词的不同形式(如play，plays，playing，played等，其中词根是play。这个过程叫作词汇化)和对词的否定(像是没有而不是没有)。所有这些都干扰了我们模型的训练过程，增加了我们模型的词汇量。所以我们必须妥善处理它们。</p><p id="2eb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为此，我们将定义一个函数来清理我们的数据。为了有效地进行预处理，我们必须让所有的文本都是小写的。为了处理否定，我们将使用撇号词典，它列出了英语中所有常用的撇号词。我们可以在这里得到<a class="ae jd" href="https://drive.google.com/file/d/0B1yuv8YaUVlZZ1RzMFJmc1ZsQmM/view" rel="noopener ugc nofollow" target="_blank"/>。这本字典也是小写的。</p><p id="e487" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">仔细检查数据，我们可以发现数据中有拼写错误的单词(如下所示)。例如,“not”在某些情况下被拼写为“n't ”,而一些否定形式如“dot”被拼写为“dot”。我们也将在清洁过程中解决这些不一致的问题。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="96dc" class="jn jo hi jj b fi jp jq l jr js">"nice rooms not 4* experience hotel monaco seattle good hotel n't 4* level.positives large bathroom mediterranean suite comfortable bed pillowsattentive housekeeping staffnegatives ac unit malfunctioned stay desk disorganized, missed 3 separate wakeup calls, concierge busy hard touch, did n't provide guidance special requests.tv hard use ipod sound dock suite non functioning. decided book mediterranean suite 3 night weekend stay 1st choice rest party filled, comparison w spent 45 night larger square footage room great soaking tub whirlpool jets nice shower.before stay hotel arrange car service price 53 tip reasonable driver waiting arrival.checkin easy downside room picked 2 person jacuzi tub no bath accessories salts bubble bath did n't stay, night got 12/1a checked voucher bottle champagne nice gesture fish waiting room, impression room huge open space felt room big, tv far away bed chore change channel, ipod dock broken disappointing.in morning way asked desk check thermostat said 65f 74 2 degrees warm try cover face night bright blue light kept, got room night no, 1st drop desk, called maintainence came look thermostat told play settings happy digital box wo n't work, asked wakeup 10am morning did n't happen, called later 6pm nap wakeup forgot, 10am wakeup morning yep forgotten.the bathroom facilities great room surprised room sold whirlpool bath tub n't bath amenities, great relax water jets going,  "<br/>#sample review from the dataset</span></pre><p id="5d17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们前面得到的撇号字典将被修改以说明这一点，如下:</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="0775" class="jn jo hi jj b fi jp jq l jr js">apposV2 = {<br/>"are not" : "are not",<br/>"ca" : "can",<br/>"could n't" : "could not",<br/>"did n't" : "did not",<br/>"does n't" : "does not",<br/>"do n't" : "do not",<br/>"had n't" : "had not",<br/>"has n't" : "has not",<br/>"have n't" : "have not",<br/>"he'd" : "he would",<br/>"he'll" : "he will",<br/>"he's" : "he is",<br/>"i'd" : "I would",<br/>"i'd" : "I had",<br/>"i'll" : "I will",<br/>"i'm" : "I am",<br/>"is n't" : "is not",<br/>"it's" : "it is",<br/>"it'll":"it will",<br/>"i've" : "I have",<br/>"let's" : "let us",<br/>"might n't" : "might not",<br/>"must n't" : "must not",<br/>"sha" : "shall",<br/>"she'd" : "she would",<br/>"she'll" : "she will",<br/>"she's" : "she is",<br/>"should n't" : "should not",<br/>"that's" : "that is",<br/>"there's" : "there is",<br/>"they'd" : "they would",<br/>"they'll" : "they will",<br/>"they're" : "they are",<br/>"they've" : "they have",<br/>"we'd" : "we would",<br/>"we're" : "we are",<br/>"were n't" : "were not",<br/>"we've" : "we have",<br/>"what'll" : "what will",<br/>"what're" : "what are",<br/>"what's" : "what is",<br/>"what've" : "what have",<br/>"where's" : "where is",<br/>"who'd" : "who would",<br/>"who'll" : "who will",<br/>"who're" : "who are",<br/>"who's" : "who is",<br/>"who've" : "who have",<br/>"wo" : "will",<br/>"would n't" : "would not",<br/>"you'd" : "you would",<br/>"you'll" : "you will",<br/>"you're" : "you are",<br/>"you've" : "you have",<br/>"'re": " are",<br/>"was n't": "was not",<br/>"we'll":"we will",<br/>"did n't": "did not"<br/>}</span><span id="72b1" class="jn jo hi jj b fi kc jq l jr js">appos = {<br/>"aren't" : "are not",<br/>"can't" : "cannot",<br/>"couldn't" : "could not",<br/>"didn't" : "did not",<br/>"doesn't" : "does not",<br/>"don't" : "do not",<br/>"hadn't" : "had not",<br/>"hasn't" : "has not",<br/>"haven't" : "have not",<br/>"he'd" : "he would",<br/>"he'll" : "he will",<br/>"he's" : "he is",<br/>"i'd" : "I would",<br/>"i'd" : "I had",<br/>"i'll" : "I will",<br/>"i'm" : "I am",<br/>"isn't" : "is not",<br/>"it's" : "it is",<br/>"it'll":"it will",<br/>"i've" : "I have",<br/>"let's" : "let us",<br/>"mightn't" : "might not",<br/>"mustn't" : "must not",<br/>"shan't" : "shall not",<br/>"she'd" : "she would",<br/>"she'll" : "she will",<br/>"she's" : "she is",<br/>"shouldn't" : "should not",<br/>"that's" : "that is",<br/>"there's" : "there is",<br/>"they'd" : "they would",<br/>"they'll" : "they will",<br/>"they're" : "they are",<br/>"they've" : "they have",<br/>"we'd" : "we would",<br/>"we're" : "we are",<br/>"weren't" : "were not",<br/>"we've" : "we have",<br/>"what'll" : "what will",<br/>"what're" : "what are",<br/>"what's" : "what is",<br/>"what've" : "what have",<br/>"where's" : "where is",<br/>"who'd" : "who would",<br/>"who'll" : "who will",<br/>"who're" : "who are",<br/>"who's" : "who is",<br/>"who've" : "who have",<br/>"won't" : "will not",<br/>"wouldn't" : "would not",<br/>"you'd" : "you would",<br/>"you'll" : "you will",<br/>"you're" : "you are",<br/>"you've" : "you have",<br/>"'re": " are",<br/>"wasn't": "was not",<br/>"we'll":" will",<br/>"didn't": "did not"<br/>}</span></pre><p id="d632" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在将设置我们的清洗功能。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="23ad" class="jn jo hi jj b fi jp jq l jr js">nlp = spacy.load('en',disable=['parser','ner'])<br/>stop = stopwords.words('english')<br/>def cleanData(reviews):<br/>    all_=[]<br/>    for review in reviews:<br/>        lower_case = review.lower() #lower case the text<br/>        lower_case = lower_case.replace(" n't"," not") #correct n't as not<br/>        lower_case = lower_case.replace("."," . ")<br/>        lower_case = ' '.join(word.strip(string.punctuation) for word in lower_case.split()) #remove punctuation<br/>        words = lower_case.split() #split into words<br/>        words = [word for word in words if word.isalpha()] #remove numbers<br/>        split = [apposV2[word] if word in apposV2 else word for word in words] #correct using apposV2 as mentioned above<br/>        split = [appos[word] if word in appos else word for word in split] #correct using appos as mentioned above<br/>        split = [word for word in split if word not in stop] #remove stop words<br/>        reformed = " ".join(split) #join words back to the text<br/>        doc = nlp(reformed)<br/>        reformed = " ".join([token.lemma_ for token in doc]) #lemmatiztion<br/>        all_.append(reformed)<br/>    df_cleaned = pd.DataFrame()<br/>    df_cleaned['clean_reviews'] = all_<br/>    return df_cleaned['clean_reviews']</span><span id="d2b4" class="jn jo hi jj b fi kc jq l jr js">X_cleaned = cleanData(X)<br/>X_cleaned.head()</span></pre><p id="fce1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到以下输出:</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="fd21" class="jn jo hi jj b fi jp jq l jr js">0    nice hotel expensive parking get good deal sta...<br/>1    ok nothing special charge diamond member hilto...<br/>2    nice room experience hotel monaco seattle good...<br/>3    unique great stay wonderful time hotel monaco ...<br/>4    great stay great stay go seahawk game awesome ...<br/>Name: clean_reviews, dtype: object</span></pre><p id="6a9b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还将把我们的目标可变评级编码到一个热点向量中。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="c794" class="jn jo hi jj b fi jp jq l jr js">encoding = {1: 0,<br/>            2: 1,<br/>            3: 2,<br/>            4: 3,<br/>            5: 4<br/>           }</span><span id="575a" class="jn jo hi jj b fi kc jq l jr js">labels = ['1', '2', '3', '4', '5']<br/>           <br/>y = data['Rating'].copy()<br/>y.replace(encoding, inplace=True)<br/>y = to_categorical(y,5)</span></pre><p id="84f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是时候将我们的数据分成训练集和测试集了。我们将80%的数据用于训练，10%用于验证，10%用于测试。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="89d2" class="jn jo hi jj b fi jp jq l jr js">X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y, stratify=y, random_state=42,test_size=0.1)<br/>#validation split will done when fitting the model</span></pre><p id="67e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ML模型无法理解文本数据。为了给它们提供文本数据，我们将文本转换成序列，然后提供给模型。Keras为此提供了一个函数。它创建一个单词词汇表，并为每个单词分配一个索引，该索引用于以单词的顺序符号来表示该单词。每个句子的长度可能不同于其他句子。我们使用Keras填充它们，因为我们的模型期望每个句子长度相同。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="e3e6" class="jn jo hi jj b fi jp jq l jr js">from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences</span><span id="5381" class="jn jo hi jj b fi kc jq l jr js">tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts(X_train)</span><span id="5d40" class="jn jo hi jj b fi kc jq l jr js">X_train = tokenizer.texts_to_sequences(X_train)</span><span id="86f1" class="jn jo hi jj b fi kc jq l jr js">max_length = max([len(x) for x in X_train])<br/>vocab_size = len(tokenizer.word_index)+1 #add 1 to account for unknown word</span><span id="5ccd" class="jn jo hi jj b fi kc jq l jr js">print("Vocabulary size: {}".format(vocab_size))<br/>print("Max length of sentence: {}".format(max_length))</span><span id="9efb" class="jn jo hi jj b fi kc jq l jr js">X_train = pad_sequences(X_train, max_length ,padding='post')</span></pre><p id="a7db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到以下输出:</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="f38d" class="jn jo hi jj b fi jp jq l jr js">Vocabulary size: 41115<br/>Max length of sentence: 1800</span></pre><p id="7894" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里需要注意的一点是，永远不要在验证或测试数据上使用上面的记号化器。它必须只适合训练数据。一般来说，任何类型的拟合都只能在训练数据上进行。</p><p id="a063" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是时候开始建模了。让我们创建我们的模型。我们将创建一个顺序模型。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="42e3" class="jn jo hi jj b fi jp jq l jr js">from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import LSTM,Dense,Dropout<br/>from tensorflow.keras.layers import Bidirectional,Embedding,Flatten<br/>from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint</span><span id="bc4e" class="jn jo hi jj b fi kc jq l jr js">embedding_vector_length=32<br/>num_classes = 5<br/>model = Sequential()</span><span id="6d5e" class="jn jo hi jj b fi kc jq l jr js">model.add(Embedding(vocab_size,embedding_vector_length,input_length=X_train.shape[1]))<br/>model.add(Bidirectional(LSTM(250,return_sequences=True)))<br/>model.add(Dropout(0.2))<br/>model.add(Flatten())<br/>model.add(Dense(128,activation='relu'))<br/>model.add(Dropout(0.2))<br/>model.add(Dense(64,activation='relu'))<br/>model.add(Dense(32,activation='relu'))<br/>model.add(Dropout(0.2))<br/>model.add(Dense(16,activation='relu'))<br/>model.add(Dense(num_classes,activation='softmax'))</span><span id="6c1c" class="jn jo hi jj b fi kc jq l jr js">model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</span><span id="dba3" class="jn jo hi jj b fi kc jq l jr js">callbacks = [EarlyStopping(monitor='val_loss', patience=5),<br/>             ModelCheckpoint('../model/model.h5', save_best_only=True, <br/>                             save_weights_only=False)]<br/>model.summary()</span></pre><p id="bd2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们添加了一些脱落层和回调来防止过度拟合。我们得到以下输出。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="c621" class="jn jo hi jj b fi jp jq l jr js">Model: "sequential_1"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_1 (Embedding)      (None, 1800, 32)          1315680   <br/>_________________________________________________________________<br/>bidirectional_1 (Bidirection (None, 1800, 500)         566000    <br/>_________________________________________________________________<br/>dropout_3 (Dropout)          (None, 1800, 500)         0         <br/>_________________________________________________________________<br/>flatten_1 (Flatten)          (None, 900000)            0         <br/>_________________________________________________________________<br/>dense_5 (Dense)              (None, 128)               115200128 <br/>_________________________________________________________________<br/>dropout_4 (Dropout)          (None, 128)               0         <br/>_________________________________________________________________<br/>dense_6 (Dense)              (None, 64)                8256      <br/>_________________________________________________________________<br/>dense_7 (Dense)              (None, 32)                2080      <br/>_________________________________________________________________<br/>dropout_5 (Dropout)          (None, 32)                0         <br/>_________________________________________________________________<br/>dense_8 (Dense)              (None, 16)                528       <br/>_________________________________________________________________<br/>dense_9 (Dense)              (None, 5)                 85        <br/>=================================================================<br/>Total params: 117,092,757<br/>Trainable params: 117,092,757<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="8d13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们拟合我们的模型，并开始训练过程。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="cd94" class="jn jo hi jj b fi jp jq l jr js">history = model.fit(X_train, y_train, validation_split=0.11, <br/>                    epochs=15, batch_size=32, verbose=1,<br/>                    callbacks=callbacks)</span></pre><p id="bb06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的训练产生了以下结果。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="3669" class="jn jo hi jj b fi jp jq l jr js">Epoch 1/15<br/>513/513 [==============================] - 195s 368ms/step - loss: 1.6616 - accuracy: 0.4265 - val_loss: 0.9994 - val_accuracy: 0.5002<br/>Epoch 2/15<br/>513/513 [==============================] - 189s 369ms/step - loss: 0.9146 - accuracy: 0.5659 - val_loss: 0.9282 - val_accuracy: 0.5619<br/>Epoch 3/15<br/>513/513 [==============================] - 188s 367ms/step - loss: 0.7914 - accuracy: 0.6282 - val_loss: 0.9804 - val_accuracy: 0.5722<br/>Epoch 4/15<br/>513/513 [==============================] - 189s 368ms/step - loss: 0.6786 - accuracy: 0.7044 - val_loss: 0.9794 - val_accuracy: 0.6077<br/>Epoch 5/15<br/>513/513 [==============================] - 189s 368ms/step - loss: 0.5620 - accuracy: 0.7673 - val_loss: 1.0368 - val_accuracy: 0.5973<br/>Epoch 6/15<br/>513/513 [==============================] - 188s 367ms/step - loss: 0.4566 - accuracy: 0.8180 - val_loss: 1.2449 - val_accuracy: 0.5875<br/>Epoch 7/15<br/>513/513 [==============================] - 189s 369ms/step - loss: 0.3666 - accuracy: 0.8607 - val_loss: 1.3929 - val_accuracy: 0.5954</span></pre><p id="9df3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以注意到，尽管模型被设置为15个时期，但它在7个时期停止。这是因为我们的回调。一旦它观察到在5个时期后验证准确性没有提高，它就停止训练过程。此外，它还节省了模型的权重，最大限度地防止了过度拟合。</p><p id="aa37" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在验证集上达到了60%的准确率，在我们的训练集上达到了70%的准确率(第4个时期)。我们可以通过增加压差层来进一步降低过拟合。让我们画出我们的结果。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="790b" class="jn jo hi jj b fi jp jq l jr js">import matplotlib.pyplot as plt<br/><br/>plt.plot(history.history['loss'], label='Training')<br/>plt.plot(history.history['val_loss'], label='Validation')<br/>plt.legend()<br/>plt.title('Training and Validation Loss')<br/>plt.figure()<br/><br/>plt.plot(history.history['accuracy'],label='Training')<br/>plt.plot(history.history['val_accuracy'],label='Validation')<br/>plt.legend()<br/>plt.title('Training and Validation accuracy')</span></pre><figure class="je jf jg jh fd ju er es paragraph-image"><div class="er es ke"><img src="../Images/01f0b659bb1315dd92bc8b757d3db80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*9EJNgjqguLLvFWs2uNiydg.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">情节</figcaption></figure><p id="fb36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们在测试集上做一些预测。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="b7f9" class="jn jo hi jj b fi jp jq l jr js">X_test_token = tokenizer.texts_to_sequences(X_test)<br/>X_test_token = pad_sequences(X_test_token, max_length ,padding='post')<br/>pred = model.predict(X_test_token)<br/>pred = to_categorical(pred,5)</span></pre><p id="3f19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以得到如下的准确度分数和分类报告:</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="d9db" class="jn jo hi jj b fi jp jq l jr js">from sklearn.metrics import classification_report,accuracy_score<br/>print('Test Accuracy: {}'.format(accuracy_score(pred, y_test)))<br/>print(classification_report(y_test, pred, target_names=labels))</span></pre><p id="2041" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出如下:</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="bd52" class="jn jo hi jj b fi jp jq l jr js">Test Accuracy: 0.5936585365853658</span><span id="21ef" class="jn jo hi jj b fi kc jq l jr js">            precision    recall  f1-score   support<br/><br/>           1       0.61      0.64      0.63       142<br/>           2       0.40      0.31      0.35       179<br/>           3       0.45      0.32      0.38       219<br/>           4       0.47      0.57      0.52       604<br/>           5       0.75      0.72      0.73       906<br/><br/>   micro avg       0.59      0.59      0.59      2050<br/>   macro avg       0.54      0.51      0.52      2050<br/>weighted avg       0.59      0.59      0.59      2050<br/> samples avg       0.59      0.59      0.59      2050</span></pre><p id="247e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从我们的分类报告中，我们可以观察到我们的模型在1、4和5星评级上表现相当好，而在2和3星评级上表现相对较差。为了提高性能，我们可以更仔细地检查2星和3星评论。我们还可以实现其他预处理技术，如词干化而不是词汇化，并看看它如何影响性能。</p><p id="26ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文到此为止。回头见！</p></div></div>    
</body>
</html>