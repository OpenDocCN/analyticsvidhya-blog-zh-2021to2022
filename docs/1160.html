<html>
<head>
<title>Python Selenium: Scraping A Recipe Website</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python Selenium:抓取食谱网站</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/python-selenium-web-scraping-in-eight-steps-7d33b263f399?source=collection_archive---------2-----------------------#2021-02-17">https://medium.com/analytics-vidhya/python-selenium-web-scraping-in-eight-steps-7d33b263f399?source=collection_archive---------2-----------------------#2021-02-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="417d" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">逐步指南</h2><div class=""/><div class=""><h2 id="d42c" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">当你废弃数据科学实践平台时的数据科学见解。</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/020f7f4420da6d89e2cf23f7da26c679.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kz77FG4Q5qsjOUMvAS7IeA.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">麦克斯韦·尼尔森在T2的照片</figcaption></figure><h1 id="d724" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">引言。</h1><p id="7370" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">这次我将解释(用完整的代码示例)如何使用Selenium Python框架通过八个步骤创建一个web scraper。</p><p id="721f" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">我将带一个食谱网站【https://www.simplyrecipes.com/ T4】。这篇文章的主题可以是任何数据科学项目的基础部分:数据收集。</p><p id="45d8" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">所以我选择了这个网站，因为它正好包含了我的NLP形容词所需要的数据。此外，本教程的第3步、第5步和第7步将涉及一些在web爬行过程中可能出现的特定问题(selenium异常)。因此，在实现这个项目代码之后，您不需要检查Stackoverflow。</p><h1 id="dfd2" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">定义。</h1><p id="2d8e" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated"><a class="ae jw" href="https://en.wikipedia.org/wiki/Web_scraping" rel="noopener ugc nofollow" target="_blank"> <strong class="kr hs"> <em class="lq">网络抓取</em> </strong> </a>通常被称为网络爬行或网络蜘蛛，或“以编程方式浏览网页集合并提取数据”。这对任何数据科学家来说都是非常有益的实践。</p><p id="9909" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">使用web scraper，您可以挖掘关于一组产品的数据，获得用于NLP任务的大型文本语料库，获得用于电子商务分析的任何定量数据，或者收集用于计算机视觉目的的大型图片集。你甚至可以从没有官方API的网站上获取数据。</p><p id="76a5" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><a class="ae jw" href="https://en.wikipedia.org/wiki/Selenium_%28software%29" rel="noopener ugc nofollow" target="_blank"><strong class="kr hs"><em class="lq">Selenium</em></strong></a><em class="lq"/>是一个测试web应用的可移植框架。它自动化了web浏览器，您可以使用它来代表自己在浏览器环境中执行操作。Selenium还带有一个解析器，可以发送web请求。您可以像使用Javascript DOM API一样从HTML文档中提取数据。通常，如果人们需要只有在加载Javascript文件时才可用的数据，他们会使用Selenium。</p><h1 id="3185" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">第一步。安装。</h1><p id="0d74" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">Selenium文档提供了几种不同的安装包的方式。在我的例子中，我在Ubuntu OS的<strong class="kr hs"> Pycharm中的<strong class="kr hs">虚拟环境</strong>上应用安装。</strong></p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es lr"><img src="../Images/e3a28a174a5cd797a9a852527edc9951.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*moQKV5Hvh_sbwLK-fbhX2A.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图1在Pycharm的虚拟环境中安装Selenium包</figcaption></figure><p id="3a0e" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">使用pip，您可以像这样安装selenium:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ls"><img src="../Images/2ff8d76df18422aee9be560d718b6f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iwpv-MBu9V90rKLOkF5AOw.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图2通过pip安装硒包</figcaption></figure><p id="d675" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">安装Selenium后，必须下载一个<a class="ae jw" href="https://sites.google.com/a/chromium.org/chromedriver/downloads" rel="noopener ugc nofollow" target="_blank">驱动程序</a>。</p><blockquote class="lt lu lv"><p id="68f3" class="kp kq lq kr b ks ll is ku kv lm iv kx lw ln la lb lx lo le lf ly lp li lj lk hb bi translated">Selenium需要一个驱动程序来与所选的浏览器交互。例如，火狐需要<a class="ae jw" href="https://github.com/mozilla/geckodriver/releases" rel="noopener ugc nofollow" target="_blank">一只壁虎驱动</a>。它应该在您的路径中，例如，将它放在/usr/bin或/usr/local/bin中。</p></blockquote><p id="8dce" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">在本教程中，我拿一个<a class="ae jw" href="https://sites.google.com/a/chromium.org/chromedriver/downloads" rel="noopener ugc nofollow" target="_blank"> <strong class="kr hs"> Chrome驱动87版</strong> </a> <strong class="kr hs">给Ubuntu OS。</strong></p><p id="ec8c" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">现在我们已经安装了这个包，并在项目目录中解压了一个驱动程序。我们可以去数据所在的网页。让我们来探索一下HTML的结构。</p><h1 id="ae91" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">第二步。探索页面的HTML结构。</h1><p id="bbf3" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">这里有131页，每页有26个食谱文章的链接。我们可以收集超过三千个网址。我们必须记住最后一页没有写满。有些链接并不指向食谱信息。如何管理这种情况？您将在步骤6中看到它。将所有收集的URL保存到一个文件中。<br/>那么，我们在这个页面的HTML中有什么食谱链接收集呢？</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es lz"><img src="../Images/e276e218d7af9c4cff749d56bfeb21e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ivGcXDg-v-T1Fx52MI4_8Q.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图3我们很幸运，有两个地方(一个用红色圈起来，另一个用绿色圈起来)可以找到所需的食谱链接。</figcaption></figure><p id="f98b" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">如果一个地方有问题，还有其他地方可以尝试。页面导航呢？</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ma"><img src="../Images/8ef20bea2cd2737dd24d9ff90a1659d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q25UZGMS9diEBaKyygy3yg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图4页面底部有一个按钮“下一步”。</figcaption></figure><p id="9902" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">通过按钮“下一步”，我通过Python自动改变页面。</p><h1 id="73ad" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">第三步。获取与网页的连接。</h1><p id="138c" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">这里是第一个代码示例。这是一个为网页连接创建chromedriver的函数。为了更方便使用，我把它做成功能性表示。</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mb mc l"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jw" href="https://gist.github.com/Galina-Blokh/db95b44812175ac61e1992c2271a76e1" rel="noopener ugc nofollow" target="_blank">与网页建立连接的功能</a></figcaption></figure><p id="6cea" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">在每次连接请求后设置一个超时非常重要。否则，你将被阻止，你的程序将无法工作。<br/>如果您的互联网连接不好，或者页面有很多弹出窗口，或者页面负载过重，就会发生这种情况。在这些情况下，您可以捕获selenium.TimeoutException。只有第二次尝试失败，程序才会停止。</p><h1 id="7d21" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">第四步。从一个页面收集食谱链接。</h1><p id="eb3e" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">在我创建了一个chromedriver之后，是时候从页面获取链接了。使用硒有几种流行的方法:</p><ul class=""><li id="973a" class="md me hi kr b ks ll kv lm ky mf lc mg lg mh lk mi mj mk ml bi translated">使用XPath</li><li id="4132" class="md me hi kr b ks mm kv mn ky mo lc mp lg mq lk mi mj mk ml bi translated">使用CSS选择器</li><li id="fe15" class="md me hi kr b ks mm kv mn ky mo lc mp lg mq lk mi mj mk ml bi translated">按类别、id、标签</li><li id="0e6f" class="md me hi kr b ks mm kv mn ky mo lc mp lg mq lk mi mj mk ml bi translated">使用CSS或XPath进行部分搜索</li><li id="59ad" class="md me hi kr b ks mm kv mn ky mo lc mp lg mq lk mi mj mk ml bi translated">等等。</li></ul><p id="bcdd" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">我用XPath搜索完成了这一部分:</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mb mc l"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jw" href="https://gist.github.com/Galina-Blokh/c5175865109ed859086f31e5bd9cb3db" rel="noopener ugc nofollow" target="_blank">如何使用selenium python从一个页面收集每个食谱的链接</a></figcaption></figure><p id="9b59" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">这个函数接收一个chrome驱动对象作为参数。第一步是为链接收集创建一个空列表(第7行)。第二步—进入链接计数范围(我们记得这个网站每页包含26个链接)(第9行)，并使用XPath中的这个索引通过HTML对象进行迭代(第11–12行)。最后，将每个链接添加到一个列表中(第15行)，这个列表是在这个函数开始时创建的。<br/>如何获得网页上元素的XPath有三个简单的步骤:<br/> 1 .转到网页并按键盘上的F12键。<br/> 2。找到你想要得到元素，在HTML中点击鼠标右键。<br/> 3。复制XPath并将其作为搜索参数粘贴到函数中。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mr"><img src="../Images/e269f1bef5ba05e63b46b4df572b93f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eLRcgGNqezZQcNFmBBW8bg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图5如何获取网页中元素的XPath</figcaption></figure><p id="e4a7" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">用开发者模式(F12)真的不难做到。</p><h1 id="b760" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">第五步。逐页导航以收集链接。</h1><p id="a345" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">现在我可以一页一页地创建一个全局行走的函数。看看下面的代码:</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mb mc l"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jw" href="https://gist.github.com/Galina-Blokh/2eab0bfd1bb01c497bdf39d4a0ba5cfe" rel="noopener ugc nofollow" target="_blank">使用selenium python功能逐页收集所有链接</a></figcaption></figure><p id="5d11" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">同样，首先创建一个空列表(第11行)。该函数接收chromedriver作为参数以及它应该遍历的页数。<br/>然后我们进入一个页码范围内的for循环(第12行)。我们仍然记得该网站有131页的食谱网址。<br/>在第15–16行中，调用一个previous函数从一个页面获取链接，并将结果存储到列表中。<br/>在第19–22行中，查找“下一步”按钮并单击。如果没有更多的页面，您将得到一条关于它的消息(第24–26行)。<br/>该函数返回list [list[str]]。每个内部列表[str]是我们走过的每个页面的URL列表(第27行)。</p><h1 id="8e9b" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">第六步。将所有收集的URL保存到一个文件中。</h1><p id="f3ff" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">在这一步中，我们有三个常数:</p><ul class=""><li id="63fa" class="md me hi kr b ks ll kv lm ky mf lc mg lg mh lk mi mj mk ml bi translated">存储配方URL的文件路径(第2行)</li><li id="c189" class="md me hi kr b ks mm kv mn ky mo lc mp lg mq lk mi mj mk ml bi translated">文件的文件名/路径，其中将存储我们得到的没有食谱的链接(在我们浏览的每一页上，都有一些文章和评论的链接，而不是食谱的链接)(第3行)</li><li id="24c2" class="md me hi kr b ks mm kv mn ky mo lc mp lg mq lk mi mj mk ml bi translated">字符串模式(第4行)来区分这两个URL类别。</li></ul><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mb mc l"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jw" href="https://gist.github.com/Galina-Blokh/6f2165314b1416bb136b704cc096c3a7" rel="noopener ugc nofollow" target="_blank">将获取的链接保存到两个文件中的功能:链接到配方的文件和链接到没有配方的文件</a></figcaption></figure><p id="7f0e" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">该函数获取一个文件名作为参数，我们将在其中存储正确的链接。这次我们创建一个chromedriver(第14行)。<br/>调用前面的函数收集URL，并将结果写入一个变量(第17行)。<br/>在第20–29行，我们迭代结果变量中的元素。以“写”模式打开两个文件，并检查我们应该在哪个文件中写入元素的模式。写下一行并关闭两个文件。<br/>好了，这已经是我们做的大部分了。剩下的只是从网页上收集数据。</p><h1 id="c427" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">第七步。收集配方数据:配料和说明。</h1><p id="19e5" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">在HTML中寻找配方成分和配方说明的正确位置。使用与步骤2和4相同的方法:应用XPath搜索和按标记名搜索。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ms"><img src="../Images/61831b44e5663b8907435c17a69d83e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XxMHlXZB4LRuFrmj3QimHA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图HTML中包含所需数据的元素</figcaption></figure><p id="5f2d" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">现在说说下面的功能。它接收一个食谱页面链接作为参数。在第14–16行，我们定义了一个字典和两个列表(来收集我们的数据)。<br/>在第23行调用HTML元素搜索。它应该会返回给我们一个硒对象。使用它在第32行继续按标记名进行搜索。</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mb mc l"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jw" href="https://gist.github.com/Galina-Blokh/ab34f48a69dc745830b3458900503952" rel="noopener ugc nofollow" target="_blank">将一页中的配方成分和说明收集到字典中的功能</a></figcaption></figure><p id="bea4" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">那么在这两个步骤之间发生了什么:在第25–30行？".它预测由于警报而退出程序(在我们的例子中，ReCaptcha)。</p><p id="b536" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">有时警报不会立即显示。当您试图在它可见之前切换到它时，您会得到<em class="lq"> NoAlertPresentException </em>。要避免这种情况，只需等待警报出现。第25–30行的代码处理这种情况。</p><p id="3097" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">第34行:在下载进行到一半的时候，我突然意识到没有收集到所有的成分。其中一些表示为空列表。在HTML检查之后，我看到了另一个数据出现在页面上的<em class="lq"> div </em>元素。因此，第34行检查了<em class="lq"> div </em>元素的“为空”条件。如果search_by_tag_name返回一个空列表，程序将转到第35–36行。它应该在前面的<em class="lq"> div </em>中看到相同的id并收集数据。然后遍历搜索结果，将每个条目添加到一个列表中(第37–38行)。</p><p id="2fd0" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">说明书收集不同于配料收集。在第41–42行，我们通过XPath进行搜索。在第43行，我用标签名进行搜索。这里没有像检查配料一样检查警报(真是松了一口气)。但是，instruction中存在标签名称与文本数据相同的图片！因此，我检查“是段落文本，不是空字符串”(第44行)，并且仅在将文本追加到列表中之后(第45行)。否则，程序将转到下一个段落元素(第48–49行)。</p><p id="33e4" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">当所有搜索请求和迭代结束后，chromedriver应该关闭(第50行)。现在，您可以用收集的数据(53–54)更新我们的字典，并将其发送到return语句(第56行)</p><p id="b32d" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">此外，我编写了一个小的打印函数来监视加载的是哪种数据。它给出了控制台输出:一个. txt文件中页面链接的数量、链接本身以及收集的类似JSON表示的文本数据(关于成分和说明)。</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mb mc l"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jw" href="https://gist.github.com/Galina-Blokh/2d529fb1a1afb87a878b5506b46ff818" rel="noopener ugc nofollow" target="_blank">控制台输出监控下载数据的功能</a></figcaption></figure><h1 id="0a10" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">第八步。在main.py中运行所有</h1><p id="12f1" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">本教程的最后一部分—要从一个位置运行所有程序，请将收集的数据保存到一个CSV文件中。</p><p id="2e3f" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">我选择CSV是因为Python允许我们逐行写入这种文件格式。因此，如果你将停止中间的刮刀，并在休息后从同一位置继续，它将起作用。你不会丢失数据。</p><p id="514a" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">要继续下载，您必须更改的唯一内容是:</p><ol class=""><li id="9e86" class="md me hi kr b ks ll kv lm ky mf lc mg lg mh lk mt mj mk ml bi translated">注释行17和18，因为他们正在从网站收集链接。您已经将它们保存在一个. txt文件中；</li><li id="0946" class="md me hi kr b ks mm kv mn ky mo lc mp lg mq lk mt mj mk ml bi translated">并在第23行更改np.arrange()的start参数，从您停止的地方重新开始下载。监控功能将为您提供铲运机使用的最后一个链节的信息。</li></ol><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mb mc l"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jw" href="https://gist.github.com/Galina-Blokh/c2137ba02fb0cddd78f7504fb201d6a9" rel="noopener ugc nofollow" target="_blank">从一个地方运行抓取程序并将数据保存到csv文件</a></figcaption></figure><p id="bb0c" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><em class="lq"> main </em>打开一个包含所有URL的. txt文件来读取行(第21–22行)。然后从选择的页面收集数据(第26行)，将收集的数据存储到一个列表中，并将其作为一行写入CSV文件。</p><p id="518f" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">仅此而已。几个小时后，你将拥有一个2565行× 2列的大集合。剩下的只是对数据进行预处理:删除重复项，分成段落，删除标点符号，转换成数字序列，并训练模型。</p><h1 id="da14" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">结论。</h1><p id="4651" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">本教程应该足以学习如何用Selenium构建一个刮刀，并从任何网站收集数据。要将此代码应用到另一个网站，只需在所有爬行函数中更改URL-pattern和XPaths的常量。</p><p id="dbbe" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">你可以在Github 上找到完整的代码。</p></div></div>    
</body>
</html>