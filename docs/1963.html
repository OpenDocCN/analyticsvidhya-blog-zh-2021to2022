<html>
<head>
<title>Internal Covariate Shift: How Batch Normalization can speed up Neural Network Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">内部协变量移位:批量标准化如何加速神经网络训练</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/internal-covariate-shift-an-overview-of-how-to-speed-up-neural-network-training-3e2a3dcdd5cc?source=collection_archive---------1-----------------------#2021-03-29">https://medium.com/analytics-vidhya/internal-covariate-shift-an-overview-of-how-to-speed-up-neural-network-training-3e2a3dcdd5cc?source=collection_archive---------1-----------------------#2021-03-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/116d26a97a58e5701c644437d443c965.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/0*K7bi33azv-ejn3tc.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">来源:<a class="ae ip" href="https://becominghuman.ai/understanding-the-structure-of-neural-networks-1fa5bd17fef0" rel="noopener ugc nofollow" target="_blank">理解神经网络的结构</a>(如果你需要复习一下神经网络的内部工作原理，看看这个！！</figcaption></figure><p id="bc45" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">当开始构建你的第一个神经网络时，这个过程似乎类似于蒙着眼睛在大海捞针。有一百万个超参数——学习速率、层数、节点数、批量大小、激活函数——看起来找到最佳参数充其量只是在黑暗中摸索。</p><p id="6398" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">今天，我们将在<strong class="is hi"> Sergey Ioffe </strong>和<strong class="is hi"> Christian Szegedy的</strong>论文、<a class="ae ip" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank"> <em class="jo">批处理规范化:通过减少内部协变量移位</em> </a> <em class="jo">、</em>来加速深度网络训练的帮助下，有望揭示这个“黑箱”，以便更仔细地了解这些模糊的超参数中的至少一个——<em class="jo">批处理大小。</em></p><h1 id="a74d" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">快速概览</h1><figure class="ko kp kq kr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kn"><img src="../Images/3a006bc38a883188a383266a9c566443.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YGTzoMrmacMkNdBE.png"/></div></div></figure><p id="7e57" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">2015年，Ioffe &amp; Szegedy发表了这篇论文，提出了一种神经网络训练策略，经过彻底的实验，该策略被证明:</p><ul class=""><li id="e7c8" class="kw kx hh is b it iu ix iy jb ky jf kz jj la jn lb lc ld le bi translated">大幅<strong class="is hi">减少</strong>培训时间</li><li id="ddc4" class="kw kx hh is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">取消<a class="ae ip" href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/" rel="noopener ugc nofollow" target="_blank">退出</a>的必要性</li><li id="d9b4" class="kw kx hh is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">减少所需的调整量</li><li id="37c4" class="kw kx hh is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">考虑到<strong class="is hi">增加的学习率</strong></li></ul><p id="312e" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">他们将这种策略称为<strong class="is hi">批量标准化。</strong></p><p id="5013" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">使用<a class="ae ip" href="https://deepai.org/dataset/mnist" rel="noopener ugc nofollow" target="_blank"> MNIST数据集</a>，他们实验了一个具有大量卷积和池层的网络，以及一个softmax层来预测图像类别。</p><p id="f808" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">他们发现，他们的批量标准化数据集能够实现与控制模型相同的准确性(4.82%的测试误差)，而训练步骤减少了<strong class="is hi"> 14步。</strong></p><p id="5718" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">考虑到神经网络的计算成本接近MNIST数字分类领域，任何可以帮助网络更快学习的进步都可以对深度学习社区产生真正的积极影响。</p><p id="3e82" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">如果你想自己浏览一下<a class="ae ip" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">论文</a>，你可以参考这篇文章底部的<strong class="is hi">词典</strong>来跟踪他们在分析中使用的一些独立术语。</p></div><div class="ab cl lk ll go lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="ha hb hc hd he"><h1 id="3efe" class="jp jq hh bd jr js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km bi translated">那么，什么是<em class="lw">内部协变移位呢？？</em></h1><p id="a1f2" class="pw-post-body-paragraph iq ir hh is b it lx iv iw ix ly iz ja jb lz jd je jf ma jh ji jj mb jl jm jn ha bi translated">Ioffe &amp; Szegedy的定义如下:</p><blockquote class="mc md me"><p id="96f5" class="iq ir jo is b it iu iv iw ix iy iz ja mf jc jd je mg jg jh ji mh jk jl jm jn ha bi translated">"<strong class="is hi">内部协变量移位</strong>是由于训练期间网络参数的变化而导致的网络激活分布的变化。"</p></blockquote><figure class="ko kp kq kr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es mi"><img src="../Images/4e58869d365a78a551707220516b43a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uVNI4o9cQuD2O1OL.jpeg"/></div></div></figure><p id="9c63" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">你的关系网越深，越会引起<em class="jo">内部的混乱</em>的反变。让我们记住，神经网络通过电话的数学游戏来学习和调整它们的权重(链中的人或“层”越多，消息就越混乱)。作为神经网络的构建者，我们的工作是<strong class="is hi">稳定</strong>并改善输出层结果和每个隐藏层节点之间的联系。</p><p id="9e7b" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我们的作者推断，如果我们稳定每一层的输入值(定义为<strong class="is hi"> z = Wx + b，</strong>其中z是W权重/参数和偏差的线性变换)，我们可以防止我们的激活函数将我们的输入值放入我们的激活函数的最大/最小值中。为了说明这个概念，他们强调了下面所示的<strong class="is hi">乙状结肠激活功能</strong>。</p><p id="8a06" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">查看该图，我们可以看到，z<strong class="is hi">越大，函数就越接近他们所说的函数的“饱和状态”(或“区域”)。</strong></p><figure class="ko kp kq kr fd ii er es paragraph-image"><div class="er es mj"><img src="../Images/762ffe89ba4274675f89ba4455b2cadc.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*wJnwYi8AVJnaT0IK"/></div><figcaption class="il im et er es in io bd b be z dx translated">对于一篇解释<strong class="bd jr">饱和度的好文章，</strong>点击<a class="ae ip" href="https://jamesmccaffrey.wordpress.com/2017/07/06/neural-network-saturation/" rel="noopener ugc nofollow" target="_blank">这里</a>。</figcaption></figure><p id="ba24" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">为什么我们要置身于“饱和政体”之外？</p><p id="0f16" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">让我们来看看激活函数的<strong class="is hi">导数</strong>:</p><figure class="ko kp kq kr fd ii er es paragraph-image"><div class="er es mk"><img src="../Images/f10124644c7a084f8557ef1104d968f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*XT_wNAgbgaZu4srM"/></div><figcaption class="il im et er es in io bd b be z dx translated">关于神经网络梯度计算如何工作的回顾，请查看3Blue1Brown的惊人视频<a class="ae ip" href="https://www.youtube.com/watch?v=tIeHLnjs5U8" rel="noopener ugc nofollow" target="_blank">此处</a>。</figcaption></figure><p id="cd78" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">随着z的增加，导数迅速下降到零。为什么这个很重要？好吧，如果我们仍然从我们的成本函数中得到一个巨大的值，我们需要最小化，但是我们将只根据激活的导数(梯度)改变我们的权重，我们向收敛的“步骤”将变得真正<strong class="is hi">微小</strong>。</p><p id="a033" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这种激活函数输出持续膨胀(也称为“非线性”)可以通过<strong class="is hi">批处理规范化来防止。</strong></p><h1 id="d24a" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">批处理规范化是如何工作的？</h1><p id="fad5" class="pw-post-body-paragraph iq ir hh is b it lx iv iw ix ly iz ja jb lz jd je jf ma jh ji jj mb jl jm jn ha bi translated">我们已经知道<strong class="is hi">缩放我们的数据</strong>(平均值为0，标准差为1)对于我们正在构建的任何模型都是必不可少的。Ioffe &amp; Szegedy不仅要确保数据在进入训练之前<em class="jo">被缩放，而且要在</em>训练期间继续保持缩放。</p><p id="8132" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">但是，这究竟是如何实现的呢？</p><figure class="ko kp kq kr fd ii er es paragraph-image"><div class="er es ml"><img src="../Images/f8d18450fe79449f992e33496c2481ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*qiKbtf749In0MXdP5wpWkg.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">Ioffe &amp; Szegedy，第4页</figcaption></figure><p id="1bd6" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">经过反复试验(见第2页<a class="ae ip" href="http://proceedings.mlr.press/v37/ioffe15.pdf" rel="noopener ugc nofollow" target="_blank">和第3页</a>，他们发现网络输入可以通过<strong class="is hi">批量归一化变换来稳定。</strong>看一下左边的图片；新的变换值被表示为<strong class="is hi"> y，</strong>，其代表具有两个不熟悉的参数γ和β的<strong class="is hi"> x </strong>(也称为“非线性输入值”)的变换。</p><p id="d7f2" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这些参数作为一个“标准化器”，在整个培训过程中学习，以改变输入值(表示为<strong class="is hi"> x-hat </strong>)，这是标准化。</p><p id="2049" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">用他们的话说…</p><blockquote class="mc md me"><p id="3e21" class="iq ir jo is b it iu iv iw ix iy iz ja mf jc jd je mg jg jh ji mh jk jl jm jn ha bi translated">先前接收x作为输入的任何层现在接收BN(x)。</p></blockquote><p id="76ab" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">使用<strong class="is hi">小批量的</strong>平均值和方差对输入(x)进行标准化。</p><p id="e752" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi"> <em class="jo">此处的重要说明</em> </strong>:在使用这个额外的批量标准化(BN)步骤训练网络后，将使用<em class="jo">整个训练集的</em>均值和方差。</p><h2 id="2fa3" class="mm jq hh bd jr mn mo mp jv mq mr ms jz jb mt mu kd jf mv mw kh jj mx my kl mz bi translated">我们来看一些图表！</h2><figure class="ko kp kq kr fd ii er es paragraph-image"><div class="er es na"><img src="../Images/bc4fc3cc12f8c20c8cc260e21973a7aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*02FnXTKYsJ6hFdjKIURh5g.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">Ioffe &amp; Szegedy，第7页</figcaption></figure><p id="39ca" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi"> Inception </strong>是他们控制模型的术语(无批量规格化)。上图显示了每个模型在达到收敛之前的训练步数。在这里，批处理规范化如何大大减少训练时间变得非常清楚。除了Inception之外，其他行都显示了一个稍微调整过的批处理规范化网络版本:</p><ul class=""><li id="5fca" class="kw kx hh is b it iu ix iy jb ky jf kz jj la jn lb lc ld le bi translated">BN基线:与初始学习率相同。</li><li id="76bd" class="kw kx hh is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated"><strong class="is hi"> BN-x5 </strong>:初始学习率0.0075 (5倍Inception的学习率)。</li><li id="6612" class="kw kx hh is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated"><strong class="is hi"> BN-x30 </strong>:初始学习率0.045 ( <strong class="is hi"> <em class="jo">是初始</em> </strong>的30倍)。</li><li id="1d6e" class="kw kx hh is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated"><strong class="is hi"> BN-x5-Sigmoid </strong>:使用Sigmoid激活功能(非线性)代替ReLU。</li></ul><p id="f8b7" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我们看到<strong class="is hi"> BN-x5 </strong>胜出，只需要初始训练步骤的很小一部分(确切地说是6.7%)就可以达到<strong class="is hi"> 73% </strong>的准确度，而差的非规范化初始需要几乎<strong class="is hi">15倍</strong>的步骤数才能达到<strong class="is hi"> 72.2% </strong>的准确度。</p><p id="c7bc" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">为了获得我在开始提到的令人印象深刻的4.82%的错误率，他们使用了<strong class="is hi">集合分类</strong>，由基于BN-x30的6个网络组成，带有一些修改的超参数(见<a class="ae ip" href="http://proceedings.mlr.press/v37/ioffe15.pdf" rel="noopener ugc nofollow" target="_blank">第7页</a>)。</p><p id="e95a" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi"> <em class="jo">另一个重要的注意事项:</em> </strong>要使用带有<strong class="is hi">卷积神经网络</strong>的批量归一化，归一化必须发生在稍微不同的点，以尊重“卷积属性”(<a class="ae ip" href="http://proceedings.mlr.press/v37/ioffe15.pdf" rel="noopener ugc nofollow" target="_blank">第4页</a>:相同特征图的不同元素，在不同的位置，将以相同的方式归一化):</p><blockquote class="mc md me"><p id="09a9" class="iq ir jo is b it iu iv iw ix iy iz ja mf jc jd je mg jg jh ji mh jk jl jm jn ha bi translated"><em class="hh">“为了实现这一目标，我们在所有位置以小批量方式联合标准化所有激活。”(第4页)</em></p></blockquote><h1 id="d53f" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">外卖时间…</h1><p id="a822" class="pw-post-body-paragraph iq ir hh is b it lx iv iw ix ly iz ja jb lz jd je jf ma jh ji jj mb jl jm jn ha bi translated">简而言之，当你试图把你的神经网络串在一起时，你可能想尝试一些批量标准化。点击这里查看这篇伟大的文章<a class="ae ip" href="https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/" rel="noopener ugc nofollow" target="_blank">，它提供了关于这个主题的更多资源。查看</a><a class="ae ip" href="https://towardsdatascience.com/batch-normalization-in-neural-networks-code-d7c9b88da9f5" rel="noopener" target="_blank">这篇</a>帖子，学习如何用<strong class="is hi"> keras </strong>实现BN。网络快乐！</p></div><div class="ab cl lk ll go lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="ha hb hc hd he"><h1 id="26ba" class="jp jq hh bd jr js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km bi translated">一本小字典！</h1><ul class=""><li id="d830" class="kw kx hh is b it lx ix ly jb nb jf nc jj nd jn lb lc ld le bi translated"><strong class="is hi">批量标准化</strong>:对网络的隐藏层输入进行转换。</li><li id="43ae" class="kw kx hh is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated"><strong class="is hi">非线性</strong>(名词):给定激活函数<em class="jo">(例如:Sigmoid非线性== Sigmoid激活函数)</em></li><li id="9db8" class="kw kx hh is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated"><a class="ae ip" href="https://jamesmccaffrey.wordpress.com/2017/07/06/neural-network-saturation/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hi">饱和状态</strong> </a>:大部分隐藏节点的值接近-1.0或+1.0，输出节点的值接近0.0或1.0。预激活积和相对较大。当您的网络节点存在于此空间中时，训练会明显变慢，因为梯度值会降低。</li><li id="c209" class="kw kx hh is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated"><strong class="is hi">白化</strong>:去相关随机向量的各个分量<em class="jo">和</em>使方差为1</li><li id="100b" class="kw kx hh is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated"><strong class="is hi">雅可比矩阵</strong>:来自<em class="jo"> n </em>个变量中<em class="jo"> n </em>个方程的函数，其在任意一点的值都是在该点计算的那些方程的偏导数的<em class="jo">n×n</em>T38】行列式。</li><li id="2cc5" class="kw kx hh is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated"><strong class="is hi">学习仿射变换</strong>:一种保持共线性(即最初位于一条线上的所有点在变换后仍位于一条线上)和距离比的变换。</li></ul></div></div>    
</body>
</html>