<html>
<head>
<title>On the Importance of Data in Training Machine Learning Algorithms — Part Two</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论数据在训练机器学习算法中的重要性——第二部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/on-the-importance-of-data-in-training-machine-learning-algorithms-part-two-aeb86dba509?source=collection_archive---------10-----------------------#2021-07-02">https://medium.com/analytics-vidhya/on-the-importance-of-data-in-training-machine-learning-algorithms-part-two-aeb86dba509?source=collection_archive---------10-----------------------#2021-07-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e0a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这个博客系列的第一部分<a class="ae jc" href="https://krtk.medium.com/on-the-importance-of-data-in-training-machine-learning-algorithms-part-one" rel="noopener">中，我们讨论了探索数据特征的动机，并介绍了我们将要使用的数据集。我们在上一篇文章中看到了BestBuy数据集的数据分布和特征。</a></p><h1 id="6509" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">实验装置</h1><p id="0ccf" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">在本帖中，我们将更深入地探讨使用越来越多的记录来训练数据集的效果。如前所述，我们将保持分类器和测试记录不变，同时改变用于训练的记录数量。</p><p id="f641" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦我们为一个分类器做了这些，我们将改变分类器并重复这些步骤。这将确保我们可以使用不同的算法，确认可用于训练的记录数量对最终指标的影响。</p><p id="e3a0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我们的BestBuy数据集中有大约48000条记录，我们将重组数据集并保留大约8000条记录进行测试。我们可以通过下面的<em class="kg">熊猫</em>轻松实现这一点:</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="6ed7" class="kq je hh km b fi kr ks l kt ku">"""<br/>Splits a panda dataframe in two. Test split remains a constant.<br/>"""</span><span id="35fb" class="kq je hh km b fi kv ks l kt ku">dataset = dataset.sample(frac=1.0, random_state=1729)</span><span id="606f" class="kq je hh km b fi kv ks l kt ku">test_dataset = dataset[40000:]</span><span id="83ad" class="kq je hh km b fi kv ks l kt ku">train_dataset = dataset[:num_train]</span></pre><p id="6be0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过将<em class="kg"> random_state </em>的值设置为一个常数，我们可以确保测试集是随机的，而且每次通过这种方法加载数据集时都是相同的。你有什么不同的建议吗？</p><h1 id="d8b1" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">数据集上的训练</h1><p id="e2ae" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">正如上一篇文章中提到的，我们将使用TensorFlow决策树来训练不同的算法——随机森林、梯度提升决策树和CART模型。让我们快速看一下张量流决策树提供了什么:</p><blockquote class="kw kx ky"><p id="ce0a" class="ie if kg ig b ih ii ij ik il im in io kz iq ir is la iu iv iw lb iy iz ja jb ha bi translated"><strong class="ig hi"> TensorFlow决策森林</strong> ( <strong class="ig hi"> TF-DF </strong>)是TensorFlow中可用的决策森林(<strong class="ig hi"> DF </strong>)算法的集合。决策森林的工作方式不同于神经网络(<strong class="ig hi"> NN </strong> ): DFs通常不通过反向传播或小批量进行训练。因此，TF-DF管道与其他TensorFlow管道有一些不同。</p></blockquote><p id="b7cd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="kg">TF-DF框架的关键特征之一是它不是训练神经网络。</em>这自然意味着它不会因为过度参数化而陷入过度拟合数据的陷阱。这样做的结果是，数据不需要分成训练集、有效集和测试集，而只需要分成训练集和测试集。不需要用于验证的数据，因为框架不必监控它是否可能过度适应它正在被训练的数据。</p><p id="f19e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请记住，如果您自己使用TF-DF算法进行超参数优化，那么将数据分成训练集、有效集和测试集可能是合理的。</p><p id="a850" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用这一框架的下一个特点是不需要打乱数据集或改变批量大小。与传统的神经网络不同，这种框架不会批量处理数据以从中学习，而是将完整的数据集读入内存以从中学习。虽然这对于从更大的数据集进行学习可能是一个问题，但对于可以在训练时间方面适合内存的数据集来说，它提供了相当大的优势。</p><p id="b647" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="kg">TF-DF的作者建议，在大型数据集上进行训练的最佳实践是使用训练数据的子集来查看对性能和内存含义的影响。</em>这种方法背后的基本原理是增加数据集大小的收益递减。</p><p id="0448" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者指出的其他一些要点如下:</p><ul class=""><li id="1b8e" class="lc ld hh ig b ih ii il im ip le it lf ix lg jb lh li lj lk bi translated">不要使用要素列转换数据</li><li id="057f" class="lc ld hh ig b ih ll il lm ip ln it lo ix lp jb lh li lj lk bi translated">不要预处理特征</li><li id="830d" class="lc ld hh ig b ih ll il lm ip ln it lo ix lp jb lh li lj lk bi translated">不要标准化数字特征</li><li id="8e36" class="lc ld hh ig b ih ll il lm ip ln it lo ix lp jb lh li lj lk bi translated">不编码分类特征</li><li id="aa3f" class="lc ld hh ig b ih ll il lm ip ln it lo ix lp jb lh li lj lk bi translated">不要用幻值替换缺失的特性</li></ul><p id="b0db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然上述观点可能与机器学习课程中教授的内容完全矛盾，但作者在这里提出的原因是，该框架具有内置的功能，可以完成每个算法所需的所有转换、规范化和预处理，从而使预处理步骤变得多余。</p><h1 id="e3ac" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">TL；博士—给我看看结果</h1><p id="6ec7" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">如果你在这之前一直在阅读这篇文章，那么恭喜你。你已经等待了，会得到回报的。我们将查看结果，然后了解算法是如何实现这些结果的。</p><p id="6644" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将看到的第一个比较是记录数量对可以用TF-DF训练的三种算法的影响。</p><figure class="kh ki kj kk fd lr er es paragraph-image"><div class="er es lq"><img src="../Images/859a31a233e3fa8601b5f288aeeb0fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*6irxB8Maqdz9ggcXQTeOfw.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">记录数量对列车时间的影响</figcaption></figure><p id="513a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上面的图中，我们看到，随着更多数量的记录被训练，梯度提升决策树需要明显更长的时间来训练，而随机森林模型在训练时间方面是非常合理和线性的。另一方面，CART似乎具有相当低的训练时间值，如果性能指标也足够有竞争力，则使其成为对抗神经网络的可行候选。</p><figure class="kh ki kj kk fd lr er es paragraph-image"><div class="er es ly"><img src="../Images/669ec34933814de40d6b717fc6ce1433.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*DDGeRrIJgMAGf8btaLuOfw.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">记录数量对模型准确性的影响</figcaption></figure><p id="2e60" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上面的图中，我们看到了指标(这里是准确性)与记录数量的比较。注意到所有三种算法在记录数量非常低之前表现得非常相似，这确实令人着迷，因为我们知道我们可以多快地训练该模型，所以给了CART模型相当大的优势。随着记录数量的增加，我们可以看到GBDT和RF开始超越。当我们使用32000条记录时，随机森林似乎在精确度和所需训练时间方面胜出。</p><h1 id="276b" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">构建模型和地块</h1><p id="fa2c" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">在这篇文章中，我们已经用BestBuy数据集看了不同模型的图和分数。在下一篇文章中，我们将看看如何建立这些模型和情节。我们还将提出一个新的衡量标准，以评估机器学习模型在用这样的数据集进行训练时的可持续性。</p><p id="d113" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你已经有什么想法了吗？你想看看不同的图来评估这些算法的性能吗？让我知道你的想法。</p><h1 id="95dd" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">接下来呢？</h1><p id="6468" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">我将在下一篇博客中分享代码以及我是如何构建这些实验的。如果你有兴趣看这个，你可以去看看这个系列的第三部分<a class="ae jc" rel="noopener" href="/analytics-vidhya/on-the-importance-of-data-in-training-machine-learning-algorithms-part-three-db5c803e59e9?source=user_profile---------0----------------------------">。</a></p></div></div>    
</body>
</html>