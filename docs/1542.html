<html>
<head>
<title>Pedestrian detection and count for certain number of frames in a video</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">视频中特定数量帧的行人检测和计数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pedestrian-detection-and-count-for-certain-number-of-frames-in-a-video-f6563309ef78?source=collection_archive---------6-----------------------#2021-03-07">https://medium.com/analytics-vidhya/pedestrian-detection-and-count-for-certain-number-of-frames-in-a-video-f6563309ef78?source=collection_archive---------6-----------------------#2021-03-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5f1428085d762419702548ccca6a740e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xJItEXVASf2G6XQW.jpg"/></div></div></figure><h1 id="8c71" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">内容:</h1><ol class=""><li id="4622" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><em class="kg">文献调查</em></li><li id="a704" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated"><em class="kg">工作管道</em></li><li id="0ce3" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated"><em class="kg">数据准备</em></li><li id="2f0c" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated">预处理</li><li id="92d7" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated"><em class="kg">型号</em></li><li id="18a4" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated"><em class="kg">使用定制数据集上的预训练模型进行训练</em></li><li id="35d5" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated"><em class="kg">评估在定制数据集上训练的模型</em></li><li id="1c77" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated"><em class="kg">为模型进行砂箱展开</em></li><li id="3e9e" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated">输出</li><li id="000a" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated">未来的工作</li><li id="b62c" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated">参考</li></ol><h1 id="9476" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">文献调查:</h1><p id="f594" class="pw-post-body-paragraph km kn hi jq b jr js ko kp jt ju kq kr jv ks kt ku jx kv kw kx jz ky kz la kb hb bi translated">这个问题有许多解决方案，包括数据准备到模型部署在内的基本工作流程很大程度上取决于我们使用的数据类型和模型类型，这就是我将<em class="kg">“文献调查”</em>作为开始本问题陈述的第一步的原因。</p><h2 id="6d93" class="lb ir hi bd is lc ld le iw lf lg lh ja jv li lj je jx lk ll ji jz lm ln jm lo bi translated"><strong class="ak"> <em class="lp">了解物体检测和物体分割的区别:</em> </strong></h2><p id="7281" class="pw-post-body-paragraph km kn hi jq b jr js ko kp jt ju kq kr jv ks kt ku jx kv kw kx jz ky kz la kb hb bi translated"><strong class="jq hj"> <em class="kg">物体分割</em> </strong> <em class="kg"> : </em></p><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lq"><img src="../Images/7f5e91eccc691d589493a63be03dd8a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GjfFa4gQUmeRcCp5.png"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">对象分割</figcaption></figure><p id="4b46" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">在数字图像处理和计算机视觉中，图像分割是将数字图像分割成多个片段(像素组，也称为图像对象)的过程。分割的目标是简化和/或改变图像的表示，使其更有意义，更易于分析。</p><p id="07da" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj"> <em class="kg">物体检测:</em> </strong></p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es me"><img src="../Images/151262520baaf903219553ffe00eeddc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*sbovjViOnLsXc6k4.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">目标检测</figcaption></figure><p id="fb53" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">对象检测是一项与<a class="ae mf" href="https://en.wikipedia.org/wiki/Computer_vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a>和<a class="ae mf" href="https://en.wikipedia.org/wiki/Image_processing" rel="noopener ugc nofollow" target="_blank">图像处理</a>相关的计算机技术，处理在数字图像和视频中检测某类语义对象(如人、建筑物或汽车)的实例，并自动在这些对象上创建边界框。</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="ab fe cl mg"><img src="../Images/2aefff4720624fa6f91022adbc0b137e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*J1qpG6TUYsq43whAqezjyg.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">分割和检测的区别</figcaption></figure><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/40b2c1b6f64747a105291c8cb137c475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/0*Xi07VU5RDGVp6oNz.PNG"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">图像处理中检测与分割的关系</figcaption></figure><h2 id="ba6c" class="lb ir hi bd is lc ld le iw lf lg lh ja jv li lj je jx lk ll ji jz lm ln jm lo bi translated">FPS与精度</h2><p id="8517" class="pw-post-body-paragraph km kn hi jq b jr js ko kp jt ju kq kr jv ks kt ku jx kv kw kx jz ky kz la kb hb bi translated">当处理对象检测问题时，任何人都应该考虑的前两个约束是模型的每秒帧数(FPS)和准确性。每一个预先训练好的模型要么在准确性上很好，要么在FPS上很好，这归结为选择哪个模型的问题陈述和部署方法。</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/d7f079cf1c96ad87221e5119e6b3714d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*C0ZwmTD_cpD8tyBA"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">FPS与精度</figcaption></figure><p id="f83f" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">在上图中，该图显示了三个模型的比较，即YOLOv3、RetinaNET-50、RetinaNet-101，以及它们在coco-dataset上的推理时间(ms)和平均精度(MAP)方面的差异。</p><h2 id="7ec4" class="lb ir hi bd is lc ld le iw lf lg lh ja jv li lj je jx lk ll ji jz lm ln jm lo bi translated">我的问题陈述:</h2><p id="c326" class="pw-post-body-paragraph km kn hi jq b jr js ko kp jt ju kq kr jv ks kt ku jx kv kw kx jz ky kz la kb hb bi">“ “</p><p id="67ba" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">在这个案例研究中，我将研究行人检测问题，并将使用深度学习模型来实时设置阈值，以便如果某个区域的行人数量增加，输出将是“违规”，如果人数少于阈值，则输出将是“未违规”。</p><p id="a11a" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi">” ”</p><p id="64a0" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">因此，根据我的问题陈述，我必须制作一个模型，并在视频镜头上运行它，并给出每一帧中检测到的行人数量。</p><p id="e279" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">这里要注意的是，如果有一个行人行走的镜头，在一定数量的帧之后，行人的数量只会发生变化，在这些特定的帧中，我需要尽可能精确。</p><p id="a084" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">因此，根据问题陈述和我对模型部署的忽略，我需要比FPS更高的精度，根据上图中的图表，YOLOv3是最佳选择。</p><p id="3907" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">这就是FPS和准确性在决定我们的工作流程中扮演重要角色的原因。</p><p id="92d5" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">(<a class="ae mf" href="https://drive.google.com/file/d/18QZYmHS6WMwtlatrIS4VwUmkzapLxDIN/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">链接到我的文献调查</a>)</p><h1 id="7bfe" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">工作管道</h1><h2 id="a0bf" class="lb ir hi bd is lc ld le iw lf lg lh ja jv li lj je jx lk ll ji jz lm ln jm lo bi translated">整个过程可以总结为三个阶段:</h2><ol class=""><li id="c932" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">数据准备</li><li id="b548" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated">训练模型</li><li id="1274" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated">推理</li></ol><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/367ec04ec3bcb855f08c7183236006e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*B9_pKZOzXOkuea4c.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">工作用管道(参考:<a class="ae mf" href="https://nanonets.com/blog/how-to-automate-surveillance-easily-with-deep-learning/" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><blockquote class="mk ml mm"><p id="8b53" class="km kn kg jq b jr lz ko kp jt ma kq kr mn mb kt ku mo mc kw kx mp md kz la kb hb bi translated"><strong class="jq hj"> <em class="hi">我的数据</em> </strong> <em class="hi">:城镇中心数据集(</em> <a class="ae mf" href="https://academictorrents.com/details/35e83806d9362a57be736f370c821960eb2f2a01" rel="noopener ugc nofollow" target="_blank"> <em class="hi">链接</em> </a> <em class="hi"> ) </em></p><p id="a418" class="km kn kg jq b jr lz ko kp jt ma kq kr mn mb kt ku mo mc kw kx mp md kz la kb hb bi translated"><strong class="jq hj"> <em class="hi">我训练的机型</em> </strong> <em class="hi"> : Mobilenet-SSD，Centrenet-Hourglass，YOLOV3-Darknet。</em></p><p id="ea4a" class="km kn kg jq b jr lz ko kp jt ma kq kr mn mb kt ku mo mc kw kx mp md kz la kb hb bi translated"><strong class="jq hj"> <em class="hi">我的预测-推理</em> </strong> <em class="hi">:在flask app里，视频和预测都在flask服务器上运行。</em></p></blockquote><h1 id="f43b" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">数据准备</strong></h1><p id="8ebb" class="pw-post-body-paragraph km kn hi jq b jr js ko kp jt ju kq kr jv ks kt ku jx kv kw kx jz ky kz la kb hb bi translated"><em class="kg">城镇中心</em>数据由两部分组成，</p><ul class=""><li id="2347" class="jo jp hi jq b jr lz jt ma jv mq jx mr jz ms kb mt kd ke kf bi translated">TownCentreXVID.mp4:这是一个4分钟的关于市中心行人行走的视频。</li><li id="998a" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb mt kd ke kf bi translated">TownCentre-groundtruth:这是一个CSV格式的注释，用于3090帧，带有(x1，y1)，(x2，y2)边界框。(注意:(xmin，ymin) &amp; (xmax，ymax)未指定)。</li></ul><p id="f245" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">Mp4视频:</p><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="mu mv l"/></div></figure><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="mw mv l"/></div></figure><p id="91da" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">O/p</p><p id="3c29" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">总帧数:7502</p><p id="8100" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">Dummy_image的大小(1920，1080)= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/f6e2b8a1d0f52b7ad47871a88dfe8548.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RU5swpdzRc5C5Bm9Y4d-WA.png"/></div></div></figure><p id="67f9" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">市中心-地面真相</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/2d2580f48eaac0e2e187a7bdce1c9ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CtxO3T6g42f0F40nlV8cTw.png"/></div></div></figure><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="4037" class="lb ir hi na b fi ne nf l ng nh">Observation on Annotation :<br/><br/>    In the raw data the annotations are given in csv format.<br/><br/>    column[1] --&gt; frame_number<br/>    column[8] --&gt; x1_point_bb<br/>    column[11]--&gt; y1_point_bb<br/>    column[10]--&gt; x2_point_bb<br/>    column[9] --&gt; y2_point_bb</span></pre><p id="f41e" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">正如在数据集注释中一样，我们有7502帧中的3090帧的基本事实，因此我们可以使用这些帧来创建train和val图像，其余的用于测试图像。</p><p id="dd14" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj"> <em class="kg">存储列车和val的图像:</em> </strong></p><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="mw mv l"/></div></figure><p id="d53c" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj"> <em class="kg">存储注释:</em> </strong></p><p id="2746" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">我们将以两种格式存储注释，(为什么在对数据进行预处理和建模时会共享注释)</p><ol class=""><li id="415e" class="jo jp hi jq b jr lz jt ma jv mq jx mr jz ms kb kc kd ke kf bi translated">XML格式</li><li id="01d6" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated">逗号分隔值（csv）文件格式</li></ol><p id="6fb2" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">XML格式:</p><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="mw mv l"/></div></figure><p id="3aa7" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">转换为XML格式后，我们可以轻松地将其转换为CSV格式:</p><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="mw mv l"/></div></figure><p id="8cc1" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">检查每帧的行人数量:</p><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="mw mv l"/></div></figure><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/bf87f2c2b2681ce58548c8b1b683c964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*5RAeA_6ZtPlbhFPx--iQlg.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">每帧的行人数量</figcaption></figure><h1 id="24bf" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">预处理</h1><p id="ff6b" class="pw-post-body-paragraph km kn hi jq b jr js ko kp jt ju kq kr jv ks kt ku jx kv kw kx jz ky kz la kb hb bi translated">对于预处理，我们可以使用xml_df来创建我们的扩充数据和记录格式。</p><p id="60c0" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj"> <em class="kg">为什么要扩增数据？</em>T3】</strong></p><p id="2b59" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">—为了训练模型，如果图像的训练大小很小，我们可以使用imgaug库增加数据。</p><p id="1cc6" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj"> <em class="kg">如何保存数据-标注？</em> </strong></p><p id="aa9f" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">在我的例子中，我有3090张图像用于训练，这本身足以用于训练，但只是为了备份，我制作了增强数据和最终df，以便我可以使用我的自定义数据和训练任何模型。</p><p id="ead2" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">为了实现这一点，我们应该有3种格式的注释:</p><ul class=""><li id="e6cb" class="jo jp hi jq b jr lz jt ma jv mq jx mr jz ms kb mt kd ke kf bi translated">TF.record()格式</li><li id="ac17" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb mt kd ke kf bi translated">逗号分隔值（csv）文件格式</li><li id="92c3" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb mt kd ke kf bi translated">XML格式</li></ul><p id="0a7d" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj">创建备份数据帧和增强图像</strong></p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="3fa2" class="lb ir hi na b fi ne nf l ng nh">Steps : <br/>        1. Creating image dataframe.<br/>        2. Creating xml dataframe.<br/>        3. making the ultimate dataframe by merging the two.<br/>        4. Making augmented dir in the common_path.<br/>        5. Making XML for augmented images.<br/>        6. Similarly like 1,2,3 making ultimate aug ultimate DF </span></pre><p id="2e66" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">步骤1、2、3:</p><ol class=""><li id="fd5b" class="jo jp hi jq b jr lz jt ma jv mq jx mr jz ms kb kc kd ke kf bi translated">制作图像DF:</li></ol><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/c76142b3415611874eea055bdb8fc5ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*rkl0uXp_vWn7F02HFpk98g.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">图像_DF</figcaption></figure><p id="cefb" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">2.制作XML DF:</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nk"><img src="../Images/2ae5d1729947f542525894ccc46b6d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W3UtiwZyePZuGdHr8rAo9Q.png"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">XML_DF</figcaption></figure><p id="2805" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">3 .将两者合并为最终的数据帧(可选——我保留它用于备份4GB pandas DF)</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nl"><img src="../Images/78f5208bafc3055dc7c77e71f5482a9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s7FYfjA12o93rkYiDlcxKg.png"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">终极数据框架。</figcaption></figure><p id="8bd9" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">现在，使用XML DF，我们可以扩充数据集，并通过此函数使用扩充图像创建一个扩充图像目录:</p><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="mw mv l"/></div></figure><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nm"><img src="../Images/979d6dd61c99021ff514881802bb1499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h5yBK9uE6frPdo06ezAgwA.png"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">增强图像的增强测向。</figcaption></figure><p id="a97d" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">制作扩充的最终数据帧(可选)</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nn"><img src="../Images/8c2b53a85eaff1072ced02a998caef95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jG4Dtt2QridiUzYhf1vVVg.png"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">终极_扩充_数据帧</figcaption></figure><p id="f944" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">查看简单数据和扩充数据:</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es no"><img src="../Images/9b8068b37de4bede7c60d6091a281638.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*iwxo3ChJC84RlPSYq2kvSQ.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">简单图像和增强图像</figcaption></figure><p id="d3ba" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj">创建tf.record()格式</strong></p><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="mw mv l"/></div></figure><p id="c7e7" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">预处理前的基本目录:</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="2adc" class="lb ir hi na b fi ne nf l ng nh">base-dir-format :{<br/>        - train (contain all train images)<br/>        - test  (contain all test images)<br/>        - xml   (Annotations of all train images)<br/>        - csv file<br/>        - video in mp4 format<br/>        - .ci file<br/>        }</span></pre><p id="d378" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">预处理后的基本目录:</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="7dad" class="lb ir hi na b fi ne nf l ng nh">Got all the desired files in the common_path :[<br/>    'TownCentre-calibration.ci',        |<br/>    'TownCentre-groundtruth.top',       |--Town-Hall-Data<br/>    'TownCentreXVID.mp4',               |<br/><br/>    'train',       |<br/>    'test',        |-- All the images data.<br/>    'aug_images',  |<br/><br/>    'xmls',             |<br/>    'xmls_augmented_1', |-- All the annotation in XML Format.<br/>                        |<br/><br/>    'train.record',   |<br/>    'val.record'      |-- All tf.record format for training.<br/>                      |     (doesnt include augmnted data yet)<br/><br/>    ]<br/><br/>Stored the Ultimate_data.pickle and ultimate_augmented_data.pickle <br/>in the system(its 4GB each) for backup.<br/>    Structure of ultimate data :<br/>        columns : [filename    <br/>                  width<br/>                  height<br/>                  class    <br/>                  xmin    <br/>                  ymin    <br/>                  xmax<br/>                  ymax<br/>                  ids<br/>                  images]<br/>        Shape   : (47722 rows × 10 columns)</span></pre><h1 id="3e2c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">模型</h1><ol class=""><li id="6c24" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="jq hj"> <em class="kg">手机-网络-固态硬盘</em> </strong></li></ol><p id="82e6" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj">手机网研究论文链接:</strong><a class="ae mf" href="https://arxiv.org/pdf/1704.04861.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1704.04861.pdf</a><strong class="jq hj">SSD研究论文链接:</strong>T22】https://arxiv.org/pdf/1512.02325.pdf</p><p id="ea89" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj">架构(移动网络):</strong></p><p id="8ab5" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj">关键理论:</strong></p><ul class=""><li id="c132" class="jo jp hi jq b jr lz jt ma jv mq jx mr jz ms kb mt kd ke kf bi translated">深度方向可分离卷积；</li></ul><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es np"><img src="../Images/1e221de023f4758be9ed22ae65ebf196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*TbIsTBeeFLZoFxfDbhLx-w.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">深度方向卷积</figcaption></figure><p id="513b" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj">架构(固态硬盘):</strong></p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es nq"><img src="../Images/cffb9a99022a303770032d4901e7ccd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*9AgcXlFo98JdMhPdkPdEIg.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">固态硬盘的架构</figcaption></figure><p id="1f14" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj">关键理论:</strong></p><p id="1305" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">- <strong class="jq hj">数据集包含边界框，其结构形成了多个边界框，使用NMS我们得到了想要的结果。</strong></p><p id="a452" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj">移动网络固态硬盘:</strong></p><p id="a271" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">它由两部分组成:</p><p id="d44b" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">1.提取特征地图，以及</p><p id="a0e4" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">2.应用卷积滤波器来检测对象</p><p id="18ac" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">SSD被设计为独立于基础网络，因此它可以运行在任何基础网络之上，如VGG、YOLO、MobileNet。</p><p id="244c" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">为了进一步解决在实时应用中的低端设备上运行高资源和高功耗神经网络的实际限制，MobileNet被集成到SSD框架中。所以，当MobileNet被用作SSD中的基础网络时，它就成了<strong class="jq hj"> MobileNet SSD。</strong></p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es nr"><img src="../Images/83662145e522b860d50ab761789fc6b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*ObbXinVNoxRQL6HWVrQt_w.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">移动网络固态硬盘</figcaption></figure><p id="2451" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj">2<em class="kg">。YOLO V3</em>T7】</strong></p><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ns"><img src="../Images/47c5d2ab0d7f5448edd372a54c80e371.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2-M27AZdxMkoV4dV.png"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">YOLO-V3架构</figcaption></figure><p id="d316" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">解释YOLO ( <a class="ae mf" href="https://towardsdatascience.com/yolo-v3-explained-ff5b850390f" rel="noopener" target="_blank">环节</a>)</p><h1 id="a952" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">在自定义数据集上使用预训练模型进行训练</h1><p id="e490" class="pw-post-body-paragraph km kn hi jq b jr js ko kp jt ju kq kr jv ks kt ku jx kv kw kx jz ky kz la kb hb bi translated">在我的训练中，我使用了两种格式，</p><ul class=""><li id="ab12" class="jo jp hi jq b jr lz jt ma jv mq jx mr jz ms kb mt kd ke kf bi translated">TF.record()格式</li><li id="422a" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb mt kd ke kf bi translated">暗网的图像和文本格式</li></ul><p id="e277" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj"> <em class="kg">使用TF对象检测API </em> </strong></p><p id="0fc9" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">张量流对象检测API在著名数据集上有许多预训练模型，如COCO数据集、Image-Net数据集等等。</p><p id="3047" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">(<a class="ae mf" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank">链接</a>)</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es nt"><img src="../Images/db2d682a62bfaed89fcda2c4da77683e.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*hrIIHKtVFw0qm-scCpauhw.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">(<a class="ae mf" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md" rel="noopener ugc nofollow" target="_blank">链接</a>)</figcaption></figure><p id="27dd" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">从这些模型中，我使用Mobile-net-SSDv2模型和Centrenet(作为理解API使用的黑盒)在我的数据集上进行训练。</p><p id="3dc7" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj"> <em class="kg">如何在我们这样的自定义数据集上训练这些模型？</em> </strong></p><p id="900a" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">(<a class="ae mf" href="https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html" rel="noopener ugc nofollow" target="_blank">参考链接</a>)</p><ol class=""><li id="13dd" class="jo jp hi jq b jr lz jt ma jv mq jx mr jz ms kb kc kd ke kf bi translated">克隆TF对象检测API的git:</li></ol><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="4e6e" class="lb ir hi na b fi ne nf l ng nh">!git clone <a class="ae mf" href="https://github.com/tensorflow/models.git" rel="noopener ugc nofollow" target="_blank">https://github.com/tensorflow/models.git</a></span></pre><p id="df60" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">2.安装模型的所有依赖项。</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="9730" class="lb ir hi na b fi ne nf l ng nh">%cd /content/models/research/</span><span id="bd74" class="lb ir hi na b fi nu nf l ng nh">!protoc object_detection/protos/*.proto --python_out=.</span><span id="bfd3" class="lb ir hi na b fi nu nf l ng nh"># Install TensorFlow Object Detection API.</span><span id="604b" class="lb ir hi na b fi nu nf l ng nh">!cp object_detection/packages/tf2/setup.py .</span><span id="d89d" class="lb ir hi na b fi nu nf l ng nh">!python -m pip install .</span></pre><p id="5b09" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">3.测试模型构建器</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="aa0d" class="lb ir hi na b fi ne nf l ng nh">!python /content/models/research/object_detection/builders/model_builder_tf2_test.py</span></pre><p id="d047" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">注意(如果这个命令给出错误，所有的依赖项都没有正确设置，我正在使用google colab，所以像这样的命令！cp已经被设置)</p><p id="e919" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">4.创建标签映射，并将其作为label_map.pbtxt保存在您的基本目录中</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="bc17" class="lb ir hi na b fi ne nf l ng nh">item {<br/>  id: 1<br/>  name: 'pedestrian'<br/>}</span></pre><p id="ce72" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">5.现在，通过右击tf github链接并保存。tar文件。下载完<code class="du nv nw nx na b">*.tar.gz</code>文件后，使用您选择的解压缩程序(如7zip、WinZIP等)打开它。).接下来，打开压缩文件夹时看到的<code class="du nv nw nx na b">*.tar</code>文件夹，并提取其内容。</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="b1b0" class="lb ir hi na b fi ne nf l ng nh">%cd /content</span><span id="c559" class="lb ir hi na b fi nu nf l ng nh">!wget http://download.tensorflow.org/models/object_detection/classification/tf2/20200710/mobilenet_v2.tar.gz</span><span id="f464" class="lb ir hi na b fi nu nf l ng nh">!tar -xvf mobilenet_v2.tar.gz</span><span id="8609" class="lb ir hi na b fi nu nf l ng nh">!rm mobilenet_v2.tar.gz</span></pre><p id="8962" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">我们的模型目录如下所示:</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="2ee5" class="lb ir hi na b fi ne nf l ng nh">│  └─ mobile_net_ssd_v2_fpn_640x640_coco17_tpu-8/<br/>│     ├─ checkpoint/<br/>│     ├─ saved_model/<br/>│     └─ pipeline.config</span></pre><p id="d97e" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">6.定义培训参数:</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="f57e" class="lb ir hi na b fi ne nf l ng nh">num_classes = 1</span><span id="7882" class="lb ir hi na b fi nu nf l ng nh">batch_size = 16</span><span id="f7ca" class="lb ir hi na b fi nu nf l ng nh">num_steps = 6000</span><span id="6e9f" class="lb ir hi na b fi nu nf l ng nh">num_eval_steps = 1000</span><span id="f9cf" class="lb ir hi na b fi nu nf l ng nh">train_record_path = common_path + “/train.record”</span><span id="83cc" class="lb ir hi na b fi nu nf l ng nh">test_record_path = common_path + “/val.record”</span><span id="31b0" class="lb ir hi na b fi nu nf l ng nh">model_dir = ‘/content/training/’</span><span id="9de8" class="lb ir hi na b fi nu nf l ng nh">labelmap_path = ‘/content/label_map.pbtxt’</span><span id="2353" class="lb ir hi na b fi nu nf l ng nh">pipeline_config_path = ‘mobilenet_v2.config’</span><span id="0388" class="lb ir hi na b fi nu nf l ng nh">fine_tune_checkpoint = ‘/content/mobilenet_v2/mobilenet_v2.ckpt-1’</span></pre><p id="568f" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">7.现在使用这些变量更改模型目录中的配置文件，如下所示:</p><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="mw mv l"/></div></figure><p id="90b9" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">注意:(有时配置文件不同，更改没有反映出来，所以您可以直接打开配置文件自己进行更改，切记如果配置文件处理不当，培训将不会进行)</p><p id="14c5" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">8.设置培训:</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="ed1a" class="lb ir hi na b fi ne nf l ng nh">!python /content/models/research/object_detection/model_main_tf2.py \</span><span id="6600" class="lb ir hi na b fi nu nf l ng nh">--pipeline_config_path={pipeline_config_path} \</span><span id="6b6d" class="lb ir hi na b fi nu nf l ng nh">--model_dir={model_dir} \</span><span id="d0e2" class="lb ir hi na b fi nu nf l ng nh">--alsologtostderr \</span><span id="51d1" class="lb ir hi na b fi nu nf l ng nh">#\/ can ignore this if you dont want to overwrite \/</span><span id="665c" class="lb ir hi na b fi nu nf l ng nh">--num_train_steps={num_steps} \ </span><span id="2156" class="lb ir hi na b fi nu nf l ng nh">--sample_1_of_n_eval_examples=1 \</span><span id="deea" class="lb ir hi na b fi nu nf l ng nh">--num_eval_steps={num_eval_steps}</span></pre><p id="87c1" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">9.绘图张量板</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="befa" class="lb ir hi na b fi ne nf l ng nh">%load_ext tensorboard</span><span id="9560" class="lb ir hi na b fi nu nf l ng nh">%tensorboard — logdir ‘/content/training/’<br/></span></pre><p id="1d55" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj"> <em class="kg">结果为手机上网SSD </em> </strong></p><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ny"><img src="../Images/35ea0ca29813f9eb203149f180a5c6bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GWeGOa3fEXUBrpy8wdpDlA.png"/></div></div></figure><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nz"><img src="../Images/e73c6f3ce71f1c1b3b85fe9c5b5addc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B9glzPrZUt4g_MroIOfxKg.png"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">移动网络的结果</figcaption></figure><p id="29d3" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj"><em class="kg">Centrenet的结果:</em> </strong></p><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es oa"><img src="../Images/3caf378c1e64564c5e2dac9cddd2145c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ayh78A6Vl_V7Z8VqYodZug.png"/></div></div></figure><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ob"><img src="../Images/6acdb576c555e23ce16a57889299d94d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HekkweP-_XUE1nk0H_q5MQ.png"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">Centernet的结果</figcaption></figure><p id="b355" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><em class="kg">(对我来说有趣的是，在不知道centernet架构的情况下，我得到了比mobilenet更好的结果，因为我知道mobilenet的架构)</em></p><p id="5aff" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">10 .保存模型</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="6fd3" class="lb ir hi na b fi ne nf l ng nh">output_directory = 'inference_graph'</span><span id="b8c4" class="lb ir hi na b fi nu nf l ng nh">!python /content/models/research/object_detection/exporter_main_v2.py \</span><span id="2731" class="lb ir hi na b fi nu nf l ng nh">--trained_checkpoint_dir {model_dir} \</span><span id="780e" class="lb ir hi na b fi nu nf l ng nh">--output_directory {output_directory} \</span><span id="d443" class="lb ir hi na b fi nu nf l ng nh">--pipeline_config_path {pipeline_config_path}</span></pre><p id="6ab3" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated"><strong class="jq hj"> <em class="kg">在自定义数据集上训练暗网YOLO:</em></strong></p><p id="4b8d" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">为了制作暗网格式的数据，我们可以使用一个名为ROBOFLOW ( <a class="ae mf" href="https://roboflow.com/" rel="noopener ugc nofollow" target="_blank"> link </a>)的开源网站:</p><p id="467c" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">转换后，我们的数据集看起来像这样:</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es oc"><img src="../Images/d21e3aecba38c31db6fda9d1683ada6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hb3fxGsg20uRoQZBXn8-BQ.png"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">暗网的数据格式。</figcaption></figure><p id="b799" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">这里需要注意的重要一点是，通过将我们的注释格式保存为xml、csv和record，我们能够在自定义数据集上使用各种各样的模型。</p><p id="70e5" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">现在下载darknet模型并遵循以下步骤:</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="5f12" class="lb ir hi na b fi ne nf l ng nh">Steps:<br/><br/>    1. Adding obj folder under darket/data.<br/>    2. Saving all the images and annotations in darknet/data/obj.<br/>    3. making a train.txt and val.txt file.<br/>    4. making custom configeration file in cfg/ dir.<br/>    5. making obj.names and obj.data  for running the cfg.</span></pre><p id="5f4c" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">在完成这些步骤(<a class="ae mf" href="https://www.youtube.com/watch?v=zJDUhGL26iU" rel="noopener ugc nofollow" target="_blank"> video_link </a>)后，运行以下命令:</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="be69" class="lb ir hi na b fi ne nf l ng nh">%cd /content/darknet</span><span id="d0f3" class="lb ir hi na b fi nu nf l ng nh">!make</span><span id="3c21" class="lb ir hi na b fi nu nf l ng nh">!chmod +x ./darknet</span></pre><p id="9d6b" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">注: (我用的google colab就是这样！make已经配置好了，这是重要的一步，因为这会运行所有的演示文件并为yolo进行配置。)</p><p id="4afc" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">安装do2unix包并将给定的文件转换成unix格式。</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="67eb" class="lb ir hi na b fi ne nf l ng nh">!sudo apt install dos2unix</span><span id="3848" class="lb ir hi na b fi nu nf l ng nh">!dos2unix ./data/train.txt</span><span id="1be1" class="lb ir hi na b fi nu nf l ng nh">!dos2unix ./data/val.txt</span><span id="a26a" class="lb ir hi na b fi nu nf l ng nh">!dos2unix ./data/obj.data</span><span id="5d5f" class="lb ir hi na b fi nu nf l ng nh">!dos2unix ./data/obj.names</span><span id="4aee" class="lb ir hi na b fi nu nf l ng nh">!dos2unix ./cfg/yolov3_custom.cfg</span></pre><p id="571d" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">开始培训:</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="bb87" class="lb ir hi na b fi ne nf l ng nh">%cd /content/darknet</span><span id="3b49" class="lb ir hi na b fi nu nf l ng nh">!./darknet detector train data/obj.data cfg/yolov3_custom.cfg darknet53.conv.74</span></pre><p id="cba7" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">将整个Darknet保存到一个zip文件中:</p><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="829e" class="lb ir hi na b fi ne nf l ng nh">%cd /content/</span><span id="9331" class="lb ir hi na b fi nu nf l ng nh">!zip -r /content/drive/MyDrive/Townhall_dataset/OxfordTownCentre/darknet_trained.zip darknet/</span></pre><h1 id="c7db" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">评估在自定义数据集上训练的模型</h1><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="f8b4" class="lb ir hi na b fi ne nf l ng nh">Metrics of evaluation: <br/>            1. MAP<br/>            2. Object-count accuracy.<br/>            3. time-for-prediction.</span></pre><p id="6b23" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">对于移动网络:</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es od"><img src="../Images/8b5e6700f646a802e2968bce582c436e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*EbvFzRwIxQq8KQLGD50MYA.png"/></div></figure><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es oe"><img src="../Images/e78133839c053d39f2284bfb7dc43cc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*GAE2UR7QJ6FfHl92nxJkTQ.png"/></div></figure><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es of"><img src="../Images/9811b662d428f4e8648325b8d0b5f7db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*RtHIbHWDY68ERfF-goMlyw.png"/></div></figure><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="aee9" class="lb ir hi na b fi ne nf l ng nh">Observations:<br/>  Mobile net giving the no. of objects in a frame always 100.(solved by NMS)<br/>  MAP of the model is in the range 0.090 to 0.110<br/>  Time per frame: <br/>    max time : 0.019 sec<br/>    min time : 0.013 sec</span></pre><p id="00ea" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">对于Centernet:</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es og"><img src="../Images/f635135e1c98ba7e1e85d49c52164ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*4BJ2GL2XAgk8hnlgwPEcXg.png"/></div></figure><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es of"><img src="../Images/3e8139521c41ef0ba3c7189c38ab66b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*E0Z-R6UqdhUWcdjQGqki0w.png"/></div></figure><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es oh"><img src="../Images/f1469eb5913179ab0d03476a2d4d55d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*-PYFrtjuu8j2Wq3kzn0snw.png"/></div></figure><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="484e" class="lb ir hi na b fi ne nf l ng nh">Observations:<br/>  Mobile net giving the no. of objects in a frame always 100.(solved by NMS)<br/>  MAP of the model is in the range 0.090 to 0.110<br/>  Time per frame: <br/>    max time : 2.5 sec<br/>    min time : &lt; 0.0 sec</span></pre><p id="4286" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">对于YOLO-黑暗网络</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es oi"><img src="../Images/ed1439aab23e9eed68dac346eb229568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*cEz-AR8UUJ3zLaI5DFPtag.png"/></div></figure><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es oj"><img src="../Images/40c303cf6bcc2b7bcb06223d940169a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*9fq7ciPBR0niJudOEIajNg.png"/></div></figure><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es ok"><img src="../Images/fcef12f31f9c777642c86c68d301a7a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*cRBpq1AWLo4uYSHfJm2Ikw.png"/></div></figure><pre class="lr ls lt lu fd mz na nb nc aw nd bi"><span id="3046" class="lb ir hi na b fi ne nf l ng nh">Observations:<br/>  Giving the no. of objects in a frame with 14.(solved by NMS)<br/>  MAP of the model is in the range 0.00055 to 0.00070<br/>  Time per frame: <br/>    max time : 0.56 sec<br/>    min time : 0.42 sec</span><span id="9491" class="lb ir hi na b fi nu nf l ng nh">Personal-Observations :<br/>    1. Trainning time :<br/>      - mobile-net - 5 mins<br/>      - Center-net - 5 mins<br/>      - darknet-yolo- 5 hours<br/><br/>    Performing FLASK api model using YOLO-darknet or Center-net.</span></pre><h1 id="5a6e" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">对象检测的烧瓶实现；</h1><p id="12dd" class="pw-post-body-paragraph km kn hi jq b jr js ko kp jt ju kq kr jv ks kt ku jx kv kw kx jz ky kz la kb hb bi translated">我们可以使用cv2.dnn库来下载我们的暗网模型，并使用它来预测帧上的边界框。</p><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="mw mv l"/></div></figure><h1 id="79a9" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">输出:</h1><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="mu mv l"/></div></figure><h1 id="e0fd" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">未来工作:</h1><ol class=""><li id="85b6" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">可以建立一个模型，该模型可以计算包围盒之间的距离，并可以使违规更加精确。</li><li id="cdc9" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated">可以根据积累的或更多的数据训练模型</li></ol><h1 id="d732" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">参考资料:</h1><ol class=""><li id="e05a" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><a class="ae mf" href="https://arxiv.org/ftp/arxiv/papers/2012/2012.07072.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/ftp/arxiv/papers/2012/2012.07072.pdf</a></li><li id="dee6" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated"><a class="ae mf" href="https://arxiv.org/pdf/1704.02431v2.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1704.02431v2.pdf</a></li><li id="4d82" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated"><a class="ae mf" href="https://github.com/thatbrguy/Pedestrian-Detection" rel="noopener ugc nofollow" target="_blank">https://github.com/thatbrguy/Pedestrian-Detection</a></li><li id="d38a" class="jo jp hi jq b jr kh jt ki jv kj jx kk jz kl kb kc kd ke kf bi translated"><a class="ae mf" href="http://www.appliedaicourse.com" rel="noopener ugc nofollow" target="_blank">www.appliedaicourse.com</a></li></ol><p id="923a" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">(<a class="ae mf" href="https://github.com/Rag-hav385/pedestrian_detection" rel="noopener ugc nofollow" target="_blank"> GITHUB) </a></p><p id="3d7c" class="pw-post-body-paragraph km kn hi jq b jr lz ko kp jt ma kq kr jv mb kt ku jx mc kw kx jz md kz la kb hb bi translated">my-Linkedin:<a class="ae mf" href="https://www.linkedin.com/in/raghav-agarwal-127539170" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/raghav-agarwal-127539170</a></p></div></div>    
</body>
</html>