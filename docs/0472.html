<html>
<head>
<title>All about Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于决策树的一切</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/all-about-decision-trees-2e167d421002?source=collection_archive---------29-----------------------#2021-01-17">https://medium.com/analytics-vidhya/all-about-decision-trees-2e167d421002?source=collection_archive---------29-----------------------#2021-01-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/b35a2593b46a13a72071b9efd11e7c72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-7kq5mqwGe6_L3FSxo8SUQ.jpeg"/></div></div></figure><p id="2aad" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">21世纪的到来给数学家和统计学家带来了巨大的祝福，因为计算能力的提高使他们能够将曾经只存在于纸上的东西可视化并付诸实施。决策树就是这样一种算法。</p><p id="8846" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">决策树的概念并不是现成的，事实上它就是我们每天用来做决策的过程。你有没有在周一早上醒来，检查闹钟，然后在显示凌晨5点后继续睡觉？如果是(谁不会同意？)，那么你也使用了强大的决策树所使用的决策概念。</p><p id="7fb5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先从决策树的优势开始，我们有:</p><ul class=""><li id="9ac5" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">不需要重新缩放/归一化连续特征。</li><li id="b9b9" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">很容易理解为什么决策树会给出一个特定的预测。</li><li id="2912" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">它可用于分类和回归任务。</li><li id="d9f4" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">需要最少的数据预处理，因为丢失的值对决策树没有太大影响。</li><li id="31a8" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">DTs对数据的空间分布不做任何假设。</li></ul><p id="600a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">不同的现代和传统决策树算法是:</p><ul class=""><li id="668e" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">ID3(仅用于二元分类)</li><li id="e178" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">C4.5 — ID3的继任者(适用于离散和连续数据)</li><li id="ef89" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">CART(具有更高灵活性的动态学习回归和分类DTs系列，由scikit-learn使用；在一个节点产生0或2个分裂)</li><li id="989a" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">CHAID(分类/回归DT，但是特征必须是分类的；可以在一个节点上产生2个以上的分裂)</li><li id="d80b" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">MARS(对数值数据有更好支持的改进版本)</li></ul><p id="e74e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">由于CART提供的灵活性，本文通篇都在讨论它</strong></p><blockquote class="kc kd ke"><p id="8c37" class="ip iq jn ir b is it iu iv iw ix iy iz kf jb jc jd kg jf jg jh kh jj jk jl jm ha bi translated">这篇文章相当长，但就像各种ML算法中的权衡一样，我必须在保持文章尽可能短和同时添加尽可能多的信息之间做出权衡。所以，这里是我尝试击中甜蜜点。</p></blockquote></div><div class="ab cl ki kj go kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="ha hb hc hd he"><h1 id="b03f" class="kp kq hh bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak">分类决策树</strong></h1><p id="68a1" class="pw-post-body-paragraph ip iq hh ir b is ln iu iv iw lo iy iz ja lp jc jd je lq jg jh ji lr jk jl jm ha bi translated">决策树的创建过程包括一个递归步骤，在每个步骤中，数据根据“是-否”条件进行拆分。当节点是纯的时，即该节点处的所有样本属于同一类时，该递归过程终止，并且直观上它被称为<em class="jn">叶节点</em>。关于决策树的一个值得注意的事实是，这个递归过程一直持续到所有叶节点都是纯的，这就是为什么<strong class="ir hi">在被允许完全形成时，它总是会过度拟合训练数据。</strong></p><blockquote class="kc kd ke"><p id="ebb3" class="ip iq jn ir b is it iu iv iw ix iy iz kf jb jc jd kg jf jg jh kh jj jk jl jm ha bi translated">决策树天生就适应过度。</p></blockquote><p id="0b12" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在每一步中，都要进行分割，以便产生尽可能多的同类子节点。这个直观的任务主要使用两个概念来完成:基尼系数和熵。虽然，在引擎盖下，他们做非常相似的事情，基尼不纯在可用性方面领先，因为计算要求比熵低，因为熵使用对数运算，而基尼不纯不使用。</p><blockquote class="ls"><p id="200d" class="lt lu hh bd lv lw lx ly lz ma mb jm dx translated"><strong class="ak">基尼杂质:通俗地说，就是衡量一个样本的同质性。在一个批次中找到属于一个类别的样本的概率越大，与之相关的基尼系数就越小。目标是将基尼系数最小化。</strong></p></blockquote><figure class="md me mf mg mh ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mc"><img src="../Images/fd2d384ac7161014410a621681626bec.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*FhdR9eFklLY1pIRyvlJrpw.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">计算基尼系数的公式</figcaption></figure><blockquote class="ls"><p id="2a65" class="lt lu hh bd lv lw mm mn mo mp mq jm dx translated">用一个简单的例子来说明，假设我们有一个装有10个水果的篮子，其中4个是苹果，5个是芒果，一个是草莓。该样本的基尼系数为:</p><p id="8509" class="lt lu hh bd lv lw lx ly lz ma mb jm dx translated">GI(苹果)= 1 — [(4/10) + (6/10) ]</p><p id="7899" class="lt lu hh bd lv lw lx ly lz ma mb jm dx translated">GI(苹果)= 1 — (0.52) = 0.48</p></blockquote><blockquote class="kc kd ke"><p id="9d47" class="ip iq jn ir b is mr iu iv iw ms iy iz kf mt jc jd kg mu jg jh kh mv jk jl jm ha bi translated">基尼系数的范围是0-0.5(包括0和0)。scikit-learn API默认使用Gini杂质标准。</p></blockquote><blockquote class="ls"><p id="d691" class="lt lu hh bd lv lw mm mn mo mp mq jm dx translated">熵:顾名思义，熵是对样本随机性的度量。像基尼不纯一样，找到属于一个类别的样本的概率越大，与分布相关的熵就越小。同样，目标是最小化熵。熵有时与术语信息增益一起使用。熵越低，信息增益越高，这是有意义的。</p></blockquote><figure class="md me mf mg mh ii er es paragraph-image"><div class="er es mw"><img src="../Images/eabf4ec964e5682c04907ee45221601c.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*LVFBYNnDhUBRhRJ-d9_T4w.png"/></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">计算熵的公式</figcaption></figure><blockquote class="ls"><p id="cf95" class="lt lu hh bd lv lw mm mn mo mp mq jm dx translated">使用与之前相同的示例，熵得出为:</p><p id="091f" class="lt lu hh bd lv lw lx ly lz ma mb jm dx translated">熵(苹果)=-((4/10)* log2(4/10)+(6/10)* log2(6/10))</p><p id="5734" class="lt lu hh bd lv lw lx ly lz ma mb jm dx translated">熵(苹果)= 0.97</p></blockquote><figure class="md me mf mg mh ii er es paragraph-image"><div class="er es mx"><img src="../Images/f05e03e788a41d7ee57d0082b389f9d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*VFg_KfDezb54OSk-vGLKTA.png"/></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">熵与两倍的基尼系数不完全一致</figcaption></figure><p id="3870" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当使用两者中的任何一个时，精度没有显著差异。</p><p id="a970" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们来看看在使用基尼杂质和熵(使用经典的Iris数据集)的情况下，同一数据集上的模型训练时间出现了什么程度的差异。</p><figure class="mz na nb nc fd ii er es paragraph-image"><div class="er es my"><img src="../Images/cf09a6e5993a4b0a858c37fc56705fc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*eIqS6wT4bxdLD0YV9VvTNA.png"/></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">训练基于基尼系数杂质的决策树所花费的时间</figcaption></figure><figure class="mz na nb nc fd ii er es paragraph-image"><div class="er es nd"><img src="../Images/f651fb6ab1c80b4d2efd09e028134a7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*1CXouH4Q9Q9G32YguL_rqw.png"/></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">训练基于熵的决策树所花费的时间</figcaption></figure><p id="7ad3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">随着数据集规模的增加，基于熵和基尼系数的决策树之间的训练时间差异急剧增加。</p><p id="bc32" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">决策树迭代地遍历特征矩阵中的每个值，并基于关于特定特征的特定值的分割来计算基尼系数杂质/熵。然后，它首先根据最低基尼系数杂质/熵对它们进行排序。</p><figure class="mz na nb nc fd ii er es paragraph-image"><div class="er es ne"><img src="../Images/40cbd48f0f195faf052cc23b03e001ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*hRvJqmW9rhBKaIHmgQ_XYA.jpeg"/></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">每个条目(如红框中的条目)是一个块，决策树分割每个块的数据并计算其基尼系数杂质/熵</figcaption></figure><p id="e1f1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">分裂具有最小基尼系数杂质/熵的块被分配给根节点。</strong></p><p id="7200" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">第二个和第三个最小值被分配给根节点的左右子节点，依此类推。</p><blockquote class="kc kd ke"><p id="349e" class="ip iq jn ir b is it iu iv iw ix iy iz kf jb jc jd kg jf jg jh kh jj jk jl jm ha bi translated"><strong class="ir hi">由于基于平等-不平等的决策，决策树的决策边界明显是箱形的</strong></p></blockquote><figure class="mz na nb nc fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/84c3e3cf6eef9240809d55ee6f5116b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*lFsZLmK1r48k-hoP9RvieQ.png"/></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">DT分类器的独特的箱形决策边界(图片提供:堆栈溢出)</figcaption></figure><p id="8292" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">好处:</strong>在scikit-learn中，可以使用决策树instace的feature_importances_ attribute轻松检查每个特征在进行预测时的重要性，以便更好地理解数据本身。它可以很容易地可视化，以便快速解释。</p><figure class="mz na nb nc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ng"><img src="../Images/d5853e82941dee20fbebb7573d34315e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HSC9NMY2AIaLZsIgKQ0HgQ.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">虹膜数据集的特征重要性</figcaption></figure></div><div class="ab cl ki kj go kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="ha hb hc hd he"><h1 id="d7a0" class="kp kq hh bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">回归决策树</h1><p id="9c5b" class="pw-post-body-paragraph ip iq hh ir b is ln iu iv iw lo iy iz ja lp jc jd je lq jg jh ji lr jk jl jm ha bi translated">回归型决策树的工作方式与分类型决策树非常相似。大多数概念是相似的:</p><ol class=""><li id="995b" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm nh ju jv jw bi translated">基于条件的二进制拆分</li><li id="e81a" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm nh ju jv jw bi translated">天生适应过度</li><li id="b242" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm nh ju jv jw bi translated">箱形决策边界</li><li id="d7aa" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm nh ju jv jw bi translated">它计算关于每个特征的每个值的分割的损失函数。</li></ol><p id="1ae5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然而，有一个很小但非常重要的区别:</p><blockquote class="kc kd ke"><p id="b772" class="ip iq jn ir b is it iu iv iw ix iy iz kf jb jc jd kg jf jg jh kh jj jk jl jm ha bi translated">回归型决策树没有基尼杂质或熵的概念来进行节点分裂。他们用均方差来代替。</p></blockquote><figure class="mz na nb nc fd ii er es paragraph-image"><div class="er es ni"><img src="../Images/a019c0bf489faea3a0ccfd47473a670b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/1*7OSWyPTpvPJ-blIW5ULVkQ.gif"/></div></figure><p id="9f77" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">回归型决策树试图在节点处分割数据，使得在每个节点处，分配给该节点的值具有与该节点处包含的数据点最小的MSE。</p><p id="71af" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在调整的Iris数据集上测试决策树回归器—萼片宽度(连续)被指定为目标列，我们得到:</p><figure class="mz na nb nc fd ii er es paragraph-image"><div class="er es nj"><img src="../Images/9060ed50ef18f385652d75e3b401bc3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*zsj0cfUa1J30_s6x5uZVsw.png"/></div></figure><p id="24bf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">决策树回归器和线性回归之间的一个显著区别是，DTR中的预测值实际上是从训练数据本身中选取的。而线性回归在基于截距和系数计算最终值后给出预测。</p><blockquote class="kc kd ke"><p id="4ba5" class="ip iq jn ir b is it iu iv iw ix iy iz kf jb jc jd kg jf jg jh kh jj jk jl jm ha bi translated">回归型决策树给出的预测是从训练数据中选取的值。决策树回归器的一个很大的缺点是它不能外推，也就是说，它总是在被训练的数据范围内给出预测。</p></blockquote></div><div class="ab cl ki kj go kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="ha hb hc hd he"><h1 id="fdf0" class="kp kq hh bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak">修剪:减少过度拟合的简单方法</strong></h1><p id="ddc5" class="pw-post-body-paragraph ip iq hh ir b is ln iu iv iw lo iy iz ja lp jc jd je lq jg jh ji lr jk jl jm ha bi translated">修剪的概念包括限制决策树的深度，使得所有的叶节点不一定是纯的。这有助于我们避免过度拟合，因为模型不再试图记住整个数据集，而是捕捉数据的总体趋势，这才是更重要的。</p><p id="590c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">修剪决策树有两种方法:</p><ol class=""><li id="5dd4" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm nh ju jv jw bi translated">预修剪(在训练期间限制树的最大深度)</li><li id="fb46" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm nh ju jv jw bi translated">后期修剪(移除训练决策树的过度拟合叶节点)</li></ol><blockquote class="ls"><p id="95ed" class="lt lu hh bd lv lw mm mn mo mp mq jm dx translated">预修剪:这是一种早期停止技术，当我们怀疑过度拟合开始时，我们停止训练过程。在训练过程之前，我们简单地设定DT的最大深度。在scikit-learn中，我们使用参数“max_depth”来实现此目的。例如:</p></blockquote><blockquote class="kc kd ke"><p id="273b" class="ip iq jn ir b is mr iu iv iw ms iy iz kf mt jc jd kg mu jg jh kh mv jk jl jm ha bi translated">从sklearn.tree导入决策树分类器</p><p id="ef19" class="ip iq jn ir b is it iu iv iw ix iy iz kf jb jc jd kg jf jg jh kh jj jk jl jm ha bi translated">dt =决策树分类器(max_depth = 5)</p></blockquote><blockquote class="ls"><p id="c7d5" class="lt lu hh bd lv lw mm mn mo mp mq jm dx translated">后期修剪:修剪已训练好的决策树模型的过程称为后期修剪。它的实现比预修剪稍微复杂一点。</p><p id="af7c" class="lt lu hh bd lv lw lx ly lz ma mb jm dx translated">在scikit-learn中，后期修剪是使用成本复杂性参数(在sklearn中称为CCP _阿尔法)来完成的。</p><p id="bdf8" class="lt lu hh bd lv lw lx ly lz ma mb jm dx translated">CCPα的值越高，修剪的节点数越多。scikit-learn有一个名为cost_complexity_pruning_path的参数，它返回alpha值和与之关联的相应叶基尼杂质/熵。很明显，随着α的增加，杂质也会增加。</p><p id="1e68" class="lt lu hh bd lv lw lx ly lz ma mb jm dx translated">你可以在scikit-learn <a class="ae nk" href="https://github.com/dipspilani/Post_Pruning_DecisionTre/blob/master/plot_cost_complexity_pruning.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到一个详细的笔记本，里面有执行后期修剪的过程。</p></blockquote></div><div class="ab cl ki kj go kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="ha hb hc hd he"><h1 id="7ca9" class="kp kq hh bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak">系综决策树</strong></h1><p id="0051" class="pw-post-body-paragraph ip iq hh ir b is ln iu iv iw lo iy iz ja lp jc jd je lq jg jh ji lr jk jl jm ha bi translated">使用单个决策树有很多缺点，这就是集合决策树来拯救它的地方。</p><ul class=""><li id="5114" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">以有效的方式控制单个DT的过拟合特性并不容易。</li><li id="5068" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">单个DT对单个数据点非常敏感。移除一个样本可能会导致非常不同的DT。</li><li id="2d85" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">单个DT无法处理异常值。</li></ul><blockquote class="kc kd ke"><p id="28a5" class="ip iq jn ir b is it iu iv iw ix iy iz kf jb jc jd kg jf jg jh kh jj jk jl jm ha bi translated">集合决策树使用了一个非常强大的思想:一群普通人的智慧胜过一个聪明人。它使用大量弱的、不适合的分类器，并基于多数投票给出预测。</p></blockquote><ol class=""><li id="8eae" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm nh ju jv jw bi translated"><strong class="ir hi">随机森林:</strong>是一种使用n棵决策树进行预测的装袋技术。对于分类，它使用软投票，而对于回归，它给出每个DT预测的平均值作为结果。随机森林的每个决策树在一个数据集上训练，该数据集的大小等于原始数据集的大小，但是特征的数量会更少。</li></ol><blockquote class="kc kd ke"><p id="e522" class="ip iq jn ir b is it iu iv iw ix iy iz kf jb jc jd kg jf jg jh kh jj jk jl jm ha bi translated">Bagging技术包括基于多个单独的分类器/回归器的结果进行预测，这些分类器/回归器使用整个数据集的引导样本(替换样本)进行训练。</p></blockquote><p id="dbdf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">随机森林中的“随机”代表这样一个事实，即使用原始数据集的d个特征的随机子集来训练每个单独的DT。此外，作为自举抽样的结果，一个样本可能出现不止一次，而有些样本甚至可能一次也没有被选中。</p><p id="0e8a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">其中，d &lt; number of features in the original dataset. In scikit-learn, d is called as n_features and it defaults to square root of number of features in original dataset. n_features parameter determines the number of randomly selected features to be used in each Decision Tree of the Random Forest.</p><p id="d9f8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">If n_features = number of features in the original dataset, no randomness is induced in the individual DTs. Higher the value of n_features, more similar are the Individual DTs.</p><blockquote class="kc kd ke"><p id="4932" class="ip iq jn ir b is it iu iv iw ix iy iz kf jb jc jd kg jf jg jh kh jj jk jl jm ha bi translated">Non-zero importance is given to all features and it helps us to capture a much broader picture of the dataset.</p></blockquote><p id="55f2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">A major drawback of Random Forest is that it performs poorly on sparse datasets.</p><p id="2241" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">High Variance problem is easily tackled by Random Forest.</p><p id="c749" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 2。AdaBoost(自适应提升):</strong>这是一种提升技术，每个决策树都试图从前任决策树的错误中学习。因此，它是一系列正向训练的决策树。</p><p id="2262" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">AdaBoost中的每棵决策树深度为1，因此被称为决策树桩。每个树桩选择一个要素，并根据该要素的特定值分割数据集。</p><p id="a376" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">每个AdaBoost决策树都试图纠正前一个树所犯的错误。每个决策树的输出被用来训练下一个决策树的参数。这是通过给不正确预测的样本较高的权重和给那些处理得好的样本较低的权重来实现的。</p><figure class="mz na nb nc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nl"><img src="../Images/3491f75f399213c77487724f14476294.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KFEGM-m3D1z44L3gSXe4AA.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">图片来源:数据营</figcaption></figure><p id="ec57" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">注意不正确预测的(绿点)如何在下一个决策树桩训练过程中获得更高的权重。</p><p id="a87d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">每个预测器被分配一个系数α，该系数是预测器对训练数据执行得有多好的度量。α值越高，该预测值对AdaBoost最终预测的贡献就越大。</p><p id="9f8c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">AdaBoost的另一个重要参数是<em class="jn">学习率</em> η。它的值介于0和1之间(包括1)，用于缩小系数α。</p><p id="19b8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">因此，α := η * α </strong></p><p id="bca8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">η的小值必须用大量的估算器来补偿。</p><p id="64eb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 3。梯度推进:</strong>与AdaBoost类似，梯度推进是另一种推进技术，其中每个估计器试图纠正其前任的错误。</p><p id="847c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然而，与AdaBoost不同，训练实例的权重没有调整。<strong class="ir hi">取而代之的是，每个预测器都使用其前任的残差作为标签来训练。CART用作基础学习者。</strong></p><p id="dfab" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">它是最热门和最常用的分类算法之一，并已被证明是各种场景中最好的算法之一。</p><p id="4296" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在梯度推进机器中，每当一个新的弱估计量加入序列时，随着新层的引入，先前估计量的参数不变。这与AdaBoost策略相反，AdaBoost策略在每次添加新的估计器时更新权重。</p><p id="7fb4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">梯度推进机器试图最小化损失函数，因此它们非常类似于梯度下降。</p><blockquote class="kc kd ke"><p id="c539" class="ip iq jn ir b is it iu iv iw ix iy iz kf jb jc jd kg jf jg jh kh jj jk jl jm ha bi translated">一些特征通过梯度推进机器获得零重要性。</p></blockquote><figure class="mz na nb nc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nm"><img src="../Images/ade7593d175dfc2e5fb8c2e20ed721a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QafsvPz05UxHkK18GSkQew.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">图片来源:数据营</figcaption></figure><figure class="mz na nb nc fd ii er es paragraph-image"><div class="er es nn"><img src="../Images/147b076dbe607588dd7258c8cbf7345f.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*lKeTAzrQCnEYMnJOt7LLvQ.png"/></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">回归输出是单个估计量预测的加权平均值</figcaption></figure><p id="db71" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">每个预测器都使用其前一个预测器的残差作为样本来训练。</p><p id="8217" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">梯度推进机中的一个重要参数是收缩率(η)，其值为0 </p><p id="3859" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Hence, for each predictor, <strong class="ir hi"> r := η * r </strong></p><p id="4492" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">快乐学习！如果你发现它能提供信息，请鼓掌！</p></div></div>    
</body>
</html>