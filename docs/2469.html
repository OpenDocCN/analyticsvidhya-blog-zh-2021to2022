<html>
<head>
<title>Batch Normalization and ReLU for solving Vanishing Gradients</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于解决消失梯度的批量标准化和ReLU</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-batch-normalization-and-relu-solve-vanishing-gradients-3f1a8ace1c88?source=collection_archive---------1-----------------------#2021-04-26">https://medium.com/analytics-vidhya/how-batch-normalization-and-relu-solve-vanishing-gradients-3f1a8ace1c88?source=collection_archive---------1-----------------------#2021-04-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4910" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理解深度神经网络训练中的高级概念的逻辑和顺序路线图。</p><h1 id="851b" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">议程</h1><p id="62c5" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">我们将把我们的讨论分成4个相互依赖的逻辑部分。为了获得最佳阅读体验，请按顺序阅读:</p><p id="57c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.什么是消失渐变？为什么会有问题？为什么会这样？<br/> 2。什么是批量正常化？它是如何帮助渐变消失的？<br/> 3。ReLU如何帮助消失渐变？<br/> 4。内部协变量移位的批量标准化</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="1ecc" class="jd je hi bd jf jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka bi translated">消失梯度</h1><h2 id="bb7d" class="ks je hi bd jf kt ku kv jj kw kx ky jn iq kz la jr iu lb lc jv iy ld le jz lf bi translated">1.1什么是消失渐变？</h2><p id="91f4" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">首先，让我们理解消失是什么意思:</p><blockquote class="lg"><p id="eb9a" class="lh li hi bd lj lk ll lm ln lo lp jc dx translated">消失意味着它趋向于0，但永远不会真正成为0。</p></blockquote><blockquote class="lq lr ls"><p id="ad24" class="if ig lt ih b ii lu ik il im lv io ip lw lx is it ly lz iw ix ma mb ja jb jc hb bi translated">消失梯度是指在深度神经网络中，<strong class="ih hj">反向传播误差信号(梯度)通常作为与最后一层</strong>的距离的函数呈指数下降。</p></blockquote><p id="c5a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">换句话说，来自网络末端的有用梯度信息未能到达网络的起点。</p><figure class="md me mf mg fd mh er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mc"><img src="../Images/9ffa8ecfac6efe178953524b041ea509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UfUbqJfHIHX6i-3S.png"/></div></div><figcaption class="mo mp et er es mq mr bd b be z dx translated"><a class="ae ms" href="https://towardsdatascience.com/batch-normalization-the-greatest-breakthrough-in-deep-learning-77e64909d81d" rel="noopener" target="_blank">信号源</a></figcaption></figure><h2 id="3b86" class="ks je hi bd jf kt ku kv jj kw kx ky jn iq kz la jr iu lb lc jv iy ld le jz lf bi translated">1.2为什么是问题？</h2><p id="e2be" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">❓The:这一阶段的关键问题是:如果网络的初始层梯度很小，为什么会有问题？</p><p id="36f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要理解这一点，回忆一下"<em class="lt">渐变"的作用是什么？</em>嗯，梯度是对输入的微小变化导致输出变量发生多大变化的衡量。然后这个梯度被用于更新/学习模型参数——权重和偏差。以下是通常遵循的参数更新规则:</p><figure class="md me mf mg fd mh er es paragraph-image"><div class="er es mt"><img src="../Images/71e5b2b8cf70bb2cf321d55b03625639.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/0*90X7xJ1nOc7DQ_gf.png"/></div></figure><p id="31d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回到手头的问题——如果上面等式中的导数项太小，即几乎为零，会发生什么？我们可以看到，一个非常小的导数只会以极小的量更新或改变Wx的值，因此(新的)Wx*几乎等于(旧的)Wx。换句话说，模型权重没有改变。权重不变意味着没有学习。无论使用反向传播算法运行多少个时期，初始层的权重都将继续保持不变(或者仅发生可忽略不计的变化)。这就是渐变消失的问题！</p><p id="67c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们继续理解为什么消失梯度会发生的数学推理。</p><h2 id="f9c0" class="ks je hi bd jf kt ku kv jj kw kx ky jn iq kz la jr iu lb lc jv iy ld le jz lf bi translated">1.3为什么会发生渐变消失？</h2><p id="e318" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">❓The在这一阶段的关键问题是:为什么网络的初始起始层获得非常小的梯度？为什么当我们回到神经网络中时，梯度值会减少或消失？</p><p id="bc04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在隐藏层单元中使用Sigmoid或Tanh激活函数时，通常会出现消失渐变。查看下面的函数图，我们可以看到，当输入变得非常小或非常大时， <strong class="ih hj"> sigmoid函数在0和1处饱和，而tanh函数在-1和1处饱和。在这两种情况下，它们的导数都非常接近于0。</strong>让我们称函数的这些范围/区域为<strong class="ih hj"><em class="lt">饱和区域</em></strong>或<strong class="ih hj"><em class="lt">坏区域</em></strong>。</p><p id="446c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">因此，如果您的输入位于任何饱和区域，那么它几乎没有梯度通过网络传播回来。</strong></p><figure class="md me mf mg fd mh er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mu"><img src="../Images/87062c46f084c5a437fbb6718092950a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QbF2QbDvYQF2v3T5.png"/></div></div></figure></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="ea63" class="jd je hi bd jf jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka bi translated">2.批量标准化</h1><p id="6f3f" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">顾名思义，批处理规范化是某种类型的<strong class="ih hj">规范化技术</strong>，我们将它应用于输入(当前)批处理数据。忽略严格的数学细节，批量标准化可以简单地视为网络中的附加层，在将数据输入隐藏单元激活函数之前，对数据进行标准化(使用平均值和标准偏差)。</p><p id="c0c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是标准化输入如何防止渐变消失呢？现在是时候把这些点连接起来了！</p><h2 id="82f2" class="ks je hi bd jf kt ku kv jj kw kx ky jn iq kz la jr iu lb lc jv iy ld le jz lf bi translated">2.1消失梯度的批量标准化</h2><p id="4567" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">❓The在这个阶段的关键问题是:标准化的输入如何确保网络的初始层不会收到一个非常小的梯度？</p><p id="8074" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">批量标准化使输入标准化，并确保|x|位于“<em class="lt">良好范围</em>”(标记为绿色区域)内，并且不会到达sigmoid函数的外部边缘。<strong class="ih hj">如果输入在好的范围内，那么激活不会饱和，因此导数也保持在好的范围内</strong>，即-导数值不会太小。因此，批量标准化可防止梯度变得太小，并确保听到梯度信号。</p><figure class="md me mf mg fd mh er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mv"><img src="../Images/c659d5113957c4f6ec5d086aa4ed81b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EBWF-rWznpvGAKQt.png"/></div></div><figcaption class="mo mp et er es mq mr bd b be z dx translated">图2:s形激活函数的“有效范围”(<a class="ae ms" href="https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484" rel="noopener" target="_blank">来源</a>)</figcaption></figure><p id="07c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，虽然梯度已经被防止变得<em class="lt">太小</em>，但是梯度仍然<em class="lt">小</em>，因为它们总是位于[0，1]之间。具体来说，<strong class="ih hj">sigmoid的导数范围仅从[0，0.25]开始，tanh的导数范围仅从[0，1]开始。这可能意味着什么？</strong></p><p id="cb15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了得到答案，回想一下训练深度神经网络的步骤:</p><ul class=""><li id="a412" class="mw mx hi ih b ii ij im in iq my iu mz iy na jc nb nc nd ne bi translated">反向传播通过从最终层到初始层逐层移动来寻找网络的导数。</li><li id="49d7" class="mw mx hi ih b ii nf im ng iq nh iu ni iy nj jc nb nc nd ne bi translated">使用反向传播的任何层的梯度更新由从末端到当前层的层上累积的多个相乘的梯度(由于链规则)组成。</li><li id="eb09" class="mw mx hi ih b ii nf im ng iq nh iu ni iy nj jc nb nc nd ne bi translated">离网络的起点越远，这些梯度相乘得到的梯度更新就越多。</li><li id="dc78" class="mw mx hi ih b ii nf im ng iq nh iu ni iy nj jc nb nc nd ne bi translated">梯度值通常在范围[0，1]内。(如上所述)</li><li id="7715" class="mw mx hi ih b ii nf im ng iq nh iu ni iy nj jc nb nc nd ne bi translated">因此，<strong class="ih hj">如果我们将一组小于1的项相乘，项越多，梯度值越趋向于零。</strong></li><li id="eb94" class="mw mx hi ih b ii nf im ng iq nh iu ni iy nj jc nb nc nd ne bi translated">对于神经网络的初始层，这个问题被放大并且更加严重，因为许多这些小梯度已经在途中(从结束到开始)被相乘。</li></ul><p id="d2be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">因此，在使用sigmoid和tanh时，单独的批处理规范化无法解决渐变消失的问题。</strong></p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="d18e" class="jd je hi bd jf jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka bi translated">3.消失渐变的ReLU</h1><p id="7cbe" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">我们在上一节看到，批量归一化+ sigmoid或tanh不足以解决消失梯度问题。我们需要使用一个<em class="lt">更好的</em>激活函数——ReLU！</p><blockquote class="lg"><p id="5593" class="lh li hi bd lj lk ll lm ln lo lp jc dx translated">是什么让ReLU更好的解决了消失渐变？</p><p id="5c9f" class="lh li hi bd lj lk ll lm ln lo lp jc dx translated">a)它不会饱和</p><p id="5cb9" class="lh li hi bd lj lk ll lm ln lo lp jc dx translated">b)它具有恒定且更大的梯度(与sigmoid和tanh相比)</p></blockquote><p id="be44" class="pw-post-body-paragraph if ig hi ih b ii lu ik il im lv io ip iq lx is it iu lz iw ix iy mb ja jb jc hb bi translated">下面是sigmoid、tanh和ReLU的梯度比较。</p><figure class="md me mf mg fd mh er es paragraph-image"><div class="er es nk"><img src="../Images/e7e4ee331ba4a43b7f2186aa5ce09396.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/0*V4s_iyhAJxd2FaaB.png"/></div><figcaption class="mo mp et er es mq mr bd b be z dx translated">不同激活函数的梯度</figcaption></figure><p id="c59f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当输入&gt; 0时，ReLU具有梯度1，否则为零。因此，<strong class="ih hj">在反向投影方程中将一组ReLU导数相乘具有1或0的良好特性。</strong>没有渐变的“消失”或“递减”。梯度要么按原样传播到底层，要么在传播过程中正好变成0。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="89cc" class="jd je hi bd jf jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka bi translated">4.内部协变量移位的批量标准化</h1><p id="9a2c" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">批处理规范化起作用还有另一个原因。最初的批处理规范化论文声称批处理规范化在提高深度神经网络性能方面如此有效是因为一种叫做<strong class="ih hj"> <em class="lt">【内部协变量移位】</em> </strong>的现象。</p><p id="daa3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据这一理论，深度神经网络中隐藏层的输入的<strong class="ih hj">分布随着模型参数在反向投影</strong>期间的更新而无规律地变化。</p><p id="ed3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于一层的输出充当下一层的输入，并且权重也通过反向传播不断更新，这意味着每一层的输入数据分布也在不断变化。<br/>使用批量标准化，我们通过固定每层的平均值和方差来限制这种变化的输入数据分布的范围。换句话说，每个图层的输入现在都不允许移动太多-受平均值和方差的限制。<strong class="ih hj">这削弱了层间的耦合。</strong></p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="3a7c" class="jd je hi bd jf jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka bi translated">参考</h1><div class="nl nm ez fb nn no"><a href="https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks" rel="noopener  ugc nofollow" target="_blank"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hj fi z dy nt ea eb nu ed ef hh bi translated">在深度神经网络中ReLU比sigmoid函数有什么优势？</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">begingroup$使用ReLu的主要原因是因为它简单、快速，而且从经验上看，它似乎工作得很好…</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">stats.stackexchange.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc mm no"/></div></div></a></div><div class="nl nm ez fb nn no"><a href="https://cs231n.github.io/neural-networks-1/#actfun" rel="noopener  ugc nofollow" target="_blank"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hj fi z dy nt ea eb nu ed ef hh bi translated">用于视觉识别的CS231n卷积神经网络</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">目录:引入神经网络而不诉诸大脑类比是可能的。在…一节中</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">cs231n.github.io</p></div></div><div class="nx l"><div class="od l nz oa ob nx oc mm no"/></div></div></a></div><div class="nl nm ez fb nn no"><a href="https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks" rel="noopener  ugc nofollow" target="_blank"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hj fi z dy nt ea eb nu ed ef hh bi translated">什么是神经网络中的“垂死ReLU”问题？</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">begingroup$ ReLU神经元输出为零，并且对所有负输入的导数为零。所以，如果你的体重…</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">datascience.stackexchange.com</p></div></div><div class="nx l"><div class="oe l nz oa ob nx oc mm no"/></div></div></a></div><div class="nl nm ez fb nn no"><a href="https://towardsdatascience.com/batch-normalization-the-greatest-breakthrough-in-deep-learning-77e64909d81d" rel="noopener follow" target="_blank"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hj fi z dy nt ea eb nu ed ef hh bi translated">批量规范化:深度学习的最大突破</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">它是如何工作的——又是如何如此有效的？</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">towardsdatascience.com</p></div></div><div class="nx l"><div class="of l nz oa ob nx oc mm no"/></div></div></a></div><div class="nl nm ez fb nn no"><a href="https://www.quora.com/What-is-the-vanishing-gradient-problem" rel="noopener  ugc nofollow" target="_blank"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hj fi z dy nt ea eb nu ed ef hh bi translated">什么是消失梯度问题？</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">回答(9个中的第1个):消失梯度问题是在用…训练某些人工神经网络时发现的一个困难</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">www.quora.com</p></div></div><div class="nx l"><div class="og l nz oa ob nx oc mm no"/></div></div></a></div><div class="nl nm ez fb nn no"><a href="https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html" rel="noopener  ugc nofollow" target="_blank"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hj fi z dy nt ea eb nu ed ef hh bi translated">消失_ grad _示例</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">sigmoid函数将输入“压缩”到0和1之间。不幸的是，这意味着对于具有sigmoid的输入…</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">cs224d.stanford.edu</p></div></div></div></a></div></div></div>    
</body>
</html>