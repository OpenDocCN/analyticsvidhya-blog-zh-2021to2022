<html>
<head>
<title>SRCNN Paper Summary &amp; Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SRCNN论文摘要和实施</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/srcnn-paper-summary-implementation-ad5cea22a90e?source=collection_archive---------7-----------------------#2021-03-14">https://medium.com/analytics-vidhya/srcnn-paper-summary-implementation-ad5cea22a90e?source=collection_archive---------7-----------------------#2021-03-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="48ee" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">摘要</h1><p id="1d62" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">论文:<a class="ae ka" href="https://arxiv.org/abs/1501.00092" rel="noopener ugc nofollow" target="_blank">arxiv.org/abs/1501.00092</a></p><p id="73ac" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">SRCNN[1]提出了用于图像超分辨率的3层CNN。这是第一篇将深度神经网络应用于图像超分辨率的论文。SRCNN结构由三部分组成:特征提取器、非线性映射、重构。该模型被训练以最小化重建图像和地面真实图像之间的像素级MSE。本文测试了各种模型体系结构和超参数，并对性能和速度进行了权衡。</p><h1 id="c931" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">模型架构</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/3a9537c9b813c56c762a4d316aaf9a6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aXe4ptFFMUednX7e.png"/></div></div></figure><p id="b06a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">所提出的架构在概念上包括3个组件:特征提取器、非线性映射、重建。每一个都负责提取低分辨率特征，映射到高分辨率特征，重建。低分辨率图像被双三次插值到Y中，与高分辨率图像x具有相同的大小。该模型旨在学习映射F: Y-&gt;X。</p><p id="4b83" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">最后，每个组件都被表示为一个卷积层，从而形成一个3层卷积神经网络，其核大小为9–1–5。根据下图，每一层的中间输出似乎包含了期望它们计算的必要信息。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ks"><img src="../Images/d2c4f94d65f605314248f13f91bb5e5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/0*PS1lSNWtlYQUsJCT.png"/></div></figure><h1 id="efbd" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">失败</h1><p id="b5ec" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">损失函数被定义为重建图像F(Y)和地面真实图像x之间的像素MSE(均方误差)。这将导致训练最大化PSNR测量。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kt"><img src="../Images/16d76b0c6ded921a00cb5db31b244f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/0*0XFzBeVADsM7DsBQ.png"/></div></div></figure><h1 id="e959" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">实验</h1><p id="be04" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">本文试验了各种超参数设置以提高性能。下图显示了尺寸为9–1-5的网络通道如何优于其他设置，以及根据本文的3阶段方法增加非线性映射能力的更深网络是不必要的。虽然所提出的模型的概念具有一些已知的缺陷，这些缺陷通过深度学习的进一步研究被证明是错误的，但是实验显示了所提出的用于SR的3阶段方法的高性能。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ku"><img src="../Images/1be8db1aa5c94a2eefb43abb8a41d78f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/0*0NzW3ehHH4SX3324.png"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kv"><img src="../Images/4acbe117bd6854aa970e45c11dd82227.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/0*dpTkjr9uWbgCB7sr.png"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kv"><img src="../Images/a6ccca8d6a1c32e122f61a9b4a7ab6cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/0*wGKQ4n1ZVruJYene.png"/></div></figure><h1 id="fe8d" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">履行</h1><pre class="kh ki kj kk fd kw kx ky kz aw la bi"><span id="477f" class="lb if hh kx b fi lc ld l le lf">import tensorflow as tf<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import requests<br/>import tensorflow_datasets as tfds<br/>from tqdm import tqdm<br/>import os<br/>import shutil</span><span id="6be1" class="lb if hh kx b fi lg ld l le lf">data=tfds.load('tf_flowers')</span></pre><p id="267f" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">导入必要的库。我们将使用由3600幅花的图像组成的<code class="du lh li lj kx b">tf_flowers</code>数据集作为训练的小玩具数据集。</p><pre class="kh ki kj kk fd kw kx ky kz aw la bi"><span id="c232" class="lb if hh kx b fi lc ld l le lf">train_data=data['train'].skip(600)<br/>test_data=data['train'].take(600)</span><span id="5009" class="lb if hh kx b fi lg ld l le lf">@tf.function<br/>def build_data(data):<br/>  cropped=tf.dtypes.cast(tf.image.random_crop(data['image'] / 255,(128,128,3)),tf.float32)<br/><br/>  lr=tf.image.resize(cropped,(64,64))<br/>  lr=tf.image.resize(lr,(128,128), method = tf.image.ResizeMethod.BICUBIC)<br/>  return (lr,cropped)</span><span id="d8fb" class="lb if hh kx b fi lg ld l le lf">def downsample_image(image,scale):<br/>  lr=tf.image.resize(image / 255,(image.shape[0]//scale, image.shape[1]//scale))<br/>  lr=tf.image.resize(lr,(image.shape[0], image.shape[1]), method = tf.image.ResizeMethod.BICUBIC)<br/>  return lr</span></pre><p id="6080" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我们将测试数据拆分为数据集中的前600个图像，并定义一个函数build_data以(128，128)的大小随机裁剪给定的图像，并返回图像的低分辨率和高分辨率副本。低分辨率拷贝是通过双三次插值生成的。</p><pre class="kh ki kj kk fd kw kx ky kz aw la bi"><span id="c6f2" class="lb if hh kx b fi lc ld l le lf">for x in train_data.take(1):<br/>  plt.imshow(x['image'])<br/>  plt.show()</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lk"><img src="../Images/b231a6f35c5a46287c5f58346a9808a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/0*CyXB0tpVQE9SKK4q.png"/></div></figure><pre class="kh ki kj kk fd kw kx ky kz aw la bi"><span id="c69a" class="lb if hh kx b fi lc ld l le lf">train_dataset_mapped = train_data.map(build_data, num_parallel_calls = tf.data.AUTOTUNE)<br/>for x in train_dataset_mapped.take(1):<br/>  plt.imshow(x[0].numpy())<br/>  plt.show()<br/>  plt.imshow(x[1].numpy())<br/>  plt.show()</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ll"><img src="../Images/7f03069416acc8d44930aee772bd8ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/0*C-qwo1R4tRkw_y4Q.png"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ll"><img src="../Images/2970d6770c81ebdcec1de44cd88604df.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/0*MQXdnnifXrhdE87M.png"/></div></figure><pre class="kh ki kj kk fd kw kx ky kz aw la bi"><span id="8c53" class="lb if hh kx b fi lc ld l le lf">SRCNN_915=tf.keras.models.Sequential([<br/>    tf.keras.layers.Conv2D(64,9,padding='same',activation='relu'),<br/>    tf.keras.layers.Conv2D(64,1,padding='same',activation='relu'),<br/>    tf.keras.layers.Conv2D(3,5,padding='same',activation='relu')<br/>])</span><span id="2841" class="lb if hh kx b fi lg ld l le lf">def pixel_mse_loss(y_true,y_pred):<br/>  return tf.reduce_mean( (y_true - y_pred) ** 2 )</span><span id="ac93" class="lb if hh kx b fi lg ld l le lf">def PSNR(y_true,y_pred):<br/>  mse=tf.reduce_mean( (y_true - y_pred) ** 2 )<br/>  return 20 * log10(1 / (mse ** 0.5))<br/><br/>def log10(x):<br/>  numerator = tf.log(x)<br/>  denominator = tf.log(tf.constant(10, dtype=numerator.dtype))<br/>  return numerator / denominator</span><span id="beaa" class="lb if hh kx b fi lg ld l le lf">SRCNN_915.compile(optimizer=tf.keras.optimizers.Adam(0.001),loss=pixel_mse_loss)</span></pre><p id="f769" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我们定义了训练损失:pixel_mse_loss，也定义了一个PSNR函数来评估模型的PSNR损失。模型架构是一个3层CNN，内核大小为9–1–5。</p><pre class="kh ki kj kk fd kw kx ky kz aw la bi"><span id="244d" class="lb if hh kx b fi lc ld l le lf">for x in range(50):<br/>  train_dataset_mapped = train_data.map(build_data, num_parallel_calls = tf.data.AUTOTUNE).batch(128)<br/>  val_dataset_mapped = test_data.map(build_data, num_parallel_calls = tf.data.AUTOTUNE).batch(128)<br/>  <br/>  SRCNN_915.fit(train_dataset_mapped,epochs=1,validation_data=val_dataset_mapped)</span></pre><p id="5606" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">24/24[= = = = = = = = = = = = = = = = = = = = = = = = = =]—6s 215 ms/步—损耗:0.0413 — val_loss: 0.0138</p><p id="5879" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">24/24[= = = = = = = = = = = = = = = = = = = = = = = = = =]—5s 209 ms/步—损耗:0.0116 — val_loss: 0.0094</p><p id="2d09" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">24/24[= = = = = = = = = = = = = = = = = = = = = = = = = =]—5s 210 ms/步—损耗:0.0084 — val_loss: 0.0073</p><p id="a03c" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi">…</p><p id="e213" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">每个时期，图像被重新裁剪以生成图像的新样本。该模型可以训练更多的迭代以提高性能，但损失通常不会低于0.0032。</p><pre class="kh ki kj kk fd kw kx ky kz aw la bi"><span id="f510" class="lb if hh kx b fi lc ld l le lf">train_dataset_mapped = train_data.map(build_data,num_parallel_calls=tf.data.AUTOTUNE)<br/>for x in train_data.take(10):<br/>  fig=plt.figure(figsize=(12,4))<br/><br/>  plt.subplot(1,3,1)<br/>  plt.imshow(x['image'].numpy())<br/>  plt.axis('off')<br/>  plt.subplot(1,3,2)<br/>  lr=downsample_image(x['image'].numpy(),4)<br/>  plt.imshow(lr.numpy())  <br/>  plt.axis('off')<br/>  plt.subplot(1,3,3)<br/>  pred=SRCNN_915(np.array([lr]))<br/>  plt.imshow(pred[0].numpy())<br/>  plt.axis('off')<br/>  plt.show()</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lm"><img src="../Images/fde5a45eef1fbd39c0a92a8b2baadfde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/0*Nmv5B714DaoJJF_F.png"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lm"><img src="../Images/64ccadb4745a1ed8e696a8ad8fbb7d62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/0*_uDhG3IFW_UJMoT1.png"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lm"><img src="../Images/624d3367fd8080ac51bfed37d5c72941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/0*PmRK5xg_jIOSIrSe.png"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ln"><img src="../Images/37cffa9e41a591cbcac59909e2e980e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/0*405f2dnVMdantJ8u.png"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lm"><img src="../Images/a5db1df471e5dfc8256b7fddded4e298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/0*GXxPOT3KG5e5gjYO.png"/></div></figure><p id="78c3" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi">…</p><p id="e51e" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">第一幅图像是原始HR图像，第二幅图像是双三次插值图像，最后一幅图像是超分辨率图像。因为网络中唯一的权重是不依赖于图像大小的卷积滤波器，所以网络可以输入与被裁剪为(128，128)用于批量训练的训练数据大小不同的图像。</p><p id="831d" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">与双三次插值图像相比，训练的SRCNN在图像分辨率上没有表现出明显的提高。我计划在SR的最新高级论文中进行指导。我们最终将中间层的输出可视化。</p><pre class="kh ki kj kk fd kw kx ky kz aw la bi"><span id="36d3" class="lb if hh kx b fi lc ld l le lf">layers=SRCNN_915.layers<br/>train_dataset_mapped = train_data.map(build_data,num_parallel_calls=tf.data.AUTOTUNE)<br/>for x in train_dataset_mapped.take(1):<br/>  image=x[0].numpy().reshape(1,128,128,3)</span><span id="a523" class="lb if hh kx b fi lg ld l le lf">input_image_layer=layers[0].input<br/>for idx,l in enumerate(layers):<br/>  print("Output of layer",idx)<br/>  intermediate_model=tf.keras.models.Model(input_image_layer,l.output)<br/>  out=intermediate_model(image)<br/>  fig = plt.figure(figsize=(20,4))<br/>  for i in range( min(out.shape[-1], 20) ):<br/>      plt.subplot(2, 10, i+1)<br/>      plt.imshow(out[0, :, :, i] * 127.5 + 127.5, cmap='gray')<br/>      plt.axis('off')<br/>  plt.show()</span></pre><p id="2b6e" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">第0层的输出</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lo"><img src="../Images/4daa84b2fc3a990b00d6c0859f58d9ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eYPMieItmYZtT3gE.png"/></div></div></figure><p id="0150" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">第1层的输出</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lo"><img src="../Images/9d615ac3ae73129ca5637fad0f7c0057.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F9CVGOC_CZwwHD4D.png"/></div></div></figure><p id="e181" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">第二层的输出</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lp"><img src="../Images/324eb2b5a28910ad2a17b0a30191ce5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/0*nAjD3ydM6TBsyfXc.png"/></div></figure><h1 id="1ec7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">我的看法</h1><ul class=""><li id="0260" class="lq lr hh je b jf jg jj jk jn ls jr lt jv lu jz lv lw lx ly bi translated">我认为所提出的CNN的9–1–5结构不是SR的最佳模型架构，尽管作者对各种超参数设置进行了多次实验。</li><li id="80e8" class="lq lr hh je b jf lz jj ma jn mb jr mc jv md jz lv lw lx ly bi translated">像素方式的MSE也不是捕捉感知距离的最佳损失函数，如提出更多感知损失的进一步研究中所讨论的，例如GAN损失和VGG损失[2]。</li><li id="d08b" class="lq lr hh je b jf lz jj ma jn mb jr mc jv md jz lv lw lx ly bi translated">尽管这是将神经网络用于SR的第一批论文之一，但它并不包括许多最近的提高一般DL性能的进展(批处理规范化、将内核大小设置为3、优化器…)。</li></ul><p id="7fde" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">[1]董，晁，等.利用深度卷积网络实现图像超分辨率."<em class="me"> IEEE模式分析与机器智能汇刊</em>38.2(2015):295–307。</p><p id="2255" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">[2] Ledig，Christian等，“使用生成式对抗网络的照片级单幅图像超分辨率”<em class="me">IEEE计算机视觉和模式识别会议论文集</em>。2017.</p></div></div>    
</body>
</html>