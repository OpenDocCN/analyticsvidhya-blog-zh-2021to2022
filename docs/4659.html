<html>
<head>
<title>Loss functions to evaluate Regression Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">评估回归模型的损失函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/loss-functions-to-evaluate-regression-models-8dac47e327e2?source=collection_archive---------0-----------------------#2021-12-17">https://medium.com/analytics-vidhya/loss-functions-to-evaluate-regression-models-8dac47e327e2?source=collection_archive---------0-----------------------#2021-12-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d36718475ada61f9270add78c1c68d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rXejhcKLorK7JiXnQH-tg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://unsplash.com/photos/5fNmWej4tAA" rel="noopener ugc nofollow" target="_blank">图像来源</a></figcaption></figure><p id="c519" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">任何机器学习模型的目标都是从数据中理解和学习模式，这些模式可以进一步用于进行预测或回答问题，或者只是理解数据中的潜在模式，否则这些模式不是显而易见的。大多数时候，学习部分是迭代的。一个模型从数据中学习一些模式，我们根据模型在训练中没有遇到的一些新数据来测试它，我们看到它做得有多好或多差，我们调整和调整一些参数，然后我们再次测试它。这个过程一直重复，直到我们看到一个足够好的模型(虽然，一些真实世界的模型可能只是令人满意的，并使世界变得不同)。我们评估和测试模型的部分就是损失函数发挥作用的地方。</p><p id="e441" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">损失函数采用模型的预测值，并将它们与实际值进行比较。它根据模型在映射X(特征、自变量、预测变量)和Y(目标、因变量、响应变量)之间的关系的能力来评估模型的好坏。有时，仅仅知道模型的表现有多差可能还不够，我们可能还需要计算模型与实际值有多远。通过了解预测值和实际值之间的偏差量，我们可以相应地训练我们的模型。实际值和预测值之间的差异称为损失。高损失值意味着模型性能差。</p><p id="dd2a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">评估回归模型的损失函数有很多。</p><blockquote class="jt"><p id="2d9f" class="ju jv hi bd jw jx jy jz ka kb kc js dx translated">T <!-- -->这里没有<em class="kd">“一个函数统治所有人”。</em></p></blockquote><p id="8561" class="pw-post-body-paragraph iv iw hi ix b iy ke ja jb jc kf je jf jg kg ji jj jk kh jm jn jo ki jq jr js hb bi translated">选择合适的损失函数是非常关键的，什么使一个期望取决于手头的数据。每个函数都有自己的属性。有许多因素有助于损失函数的适当选择，如算法的选择、数据中的异常值、可微性等。</p><p id="ae8f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇文章的目的是给你一个回归损失函数的列表，以及它们的优缺点。尽管所有这些都可以使用诸如SciPy、PyTorch、Scikit Learn、Keras等库来实现，但我还是使用NumPy实现了代码，因为它有助于更好地理解幕后发生的事情。</p><p id="611a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">事不宜迟，我们开始吧。</p><h2 id="0d49" class="kj kk hi bd kl km kn ko kp kq kr ks kt jg ku kv kw jk kx ky kz jo la lb lc ld bi translated">目录</h2><ol class=""><li id="1e0b" class="le lf hi ix b iy lg jc lh jg li jk lj jo lk js ll lm ln lo bi translated">损失函数与成本函数</li><li id="6ada" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js ll lm ln lo bi translated">平均绝对误差</li><li id="c7a0" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js ll lm ln lo bi translated">平均偏差误差</li><li id="e9bf" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js ll lm ln lo bi translated">相对绝对误差(RAE)</li><li id="4b37" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js ll lm ln lo bi translated">平均绝对百分比误差(MAPE)</li><li id="c6d0" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js ll lm ln lo bi translated">均方误差</li><li id="cb7b" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js ll lm ln lo bi translated">均方根误差(RMSE)</li><li id="5067" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js ll lm ln lo bi translated">相对平方误差</li><li id="22bb" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js ll lm ln lo bi translated">归一化均方根误差(NRMSE)</li><li id="f398" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js ll lm ln lo bi translated">相对均方根误差(RRMSE)</li><li id="655a" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js ll lm ln lo bi translated">均方根对数误差</li><li id="a7a4" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js ll lm ln lo bi translated">胡伯损失</li><li id="8f68" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js ll lm ln lo bi translated">对数损失</li><li id="6aae" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js ll lm ln lo bi translated">分位数损失</li></ol><h2 id="fd8b" class="kj kk hi bd kl km kn ko kp kq kr ks kt jg ku kv kw jk kx ky kz jo la lb lc ld bi translated">损失函数与成本函数</h2><p id="644b" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">计算1个数据点损失的函数称为损失函数。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/a989a695b7ea241743b58b65d3d8ad8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*-bC7eNhPFbpcyBMElNuexw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">损失函数</figcaption></figure><p id="fdd8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">计算所使用的全部数据的损失的函数称为成本函数。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/01c232bdeeff3dda7882b2ceb31a026b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*1raWXWBwd5Z9AeEG2-XSKg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">价值函数</figcaption></figure><h2 id="0295" class="kj kk hi bd kl km kn ko kp kq kr ks kt jg ku kv kw jk kx ky kz jo la lb lc ld bi translated">平均绝对误差</h2><p id="f7a6" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">平均绝对误差，也称为L1损失，是最简单的损失函数之一。它的计算方法是取预测值和实际值之间的绝对差值，并在整个数据集中取平均值。从数学上讲，是绝对误差的算术平均值。MAE只测量误差的大小，并不关心它们的方向。MAE越低，模型的精确度越高。</p><p id="8448" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数学上，MAE可以表示如下:</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/334c4c88a21314ab4e6168f2846efb8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*ZMERG6iRzC4KOJ_2IoiRWw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">绝对平均误差</figcaption></figure><p id="5066" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中y_i =实际值，y_hat_i =预测值，n =样本量</p><pre class="ly lz ma mb fd me mf mg mh aw mi bi"><span id="a0dd" class="kj kk hi mf b fi mj mk l ml mm">def mean_absolute_error(true, pred):<br/>    abs_error = np.abs(true - pred)<br/>    sum_abs_error = np.sum(abs_error)<br/>    mae_loss = sum_abs_error / true.size<br/>    return mae_loss</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/11f35cad700d2f80a0d11426437f31f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*pbjTFttGG1ajIJ_qGXlNqw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">梅绘图</figcaption></figure><p id="6240" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">优点</strong></p><ul class=""><li id="986f" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">这很容易计算。</li><li id="235f" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">由于采用了绝对值，所有误差都在同一标度上加权。</li><li id="71dd" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">如果训练数据有异常值，这是有用的，因为MAE不会惩罚由异常值引起的高错误。</li><li id="3d21" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">它提供了一个衡量模型性能的平均标准。</li></ul><p id="15eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">缺点</strong></p><ul class=""><li id="cae2" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">有时，来自异常值的大误差最终会被视为小误差。</li><li id="bcb1" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">MAE遵循与尺度相关的精度测量，使用与被测数据相同的尺度。因此，它不能用于比较使用不同方法的序列。</li><li id="4819" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">MAE的一个主要缺点是它在零点不可微。许多优化算法倾向于使用微分来寻找参数的最佳值。</li><li id="58a6" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">在MAE中计算梯度可能具有挑战性。</li></ul><h1 id="c82d" class="ms kk hi bd kl mt mu mv kp mw mx my kt mz na nb kw nc nd ne kz nf ng nh lc ni bi translated">平均偏差误差</h1><p id="e620" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated"><em class="nj">“平均偏差误差”中的偏差</em>是测量过程高估或低估参数值的趋势。偏差只有方向，可以是正的，也可以是负的。正偏差意味着数据误差被高估，负偏差意味着误差被低估。平均偏差误差(MBE)是预测值和实际值之差的平均值。MBE量化了总体偏差，并捕获了预测中的平均偏差。它几乎类似于MAE，唯一的区别是这里不取绝对值。MBE应该小心处理，因为正负误差可以抵消。</p><p id="0320" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">MBE的公式，</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es nk"><img src="../Images/54c2c2504c648354ff509d830cde1b0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*lF98VEKnBzXpmOyOEyQqAA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">平均偏差误差</figcaption></figure><pre class="ly lz ma mb fd me mf mg mh aw mi bi"><span id="a248" class="kj kk hi mf b fi mj mk l ml mm">def mean_bias_error(true, pred):<br/>    bias_error = true - pred<br/>    mbe_loss = np.mean(np.sum(diff) / true.size)<br/>    return mbe_loss</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/5d017e556acc4e4d18038642873c6fc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*hHtqUbFlw2K23wY5cup80Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">MBE图</figcaption></figure><p id="59a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">优点</strong></p><ul class=""><li id="3a45" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">如果您想要检查模型的方向(即，是否存在正偏差或负偏差)并纠正模型偏差，MBE是一个很好的度量。</li></ul><p id="4278" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">缺点</strong></p><ul class=""><li id="3529" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">就幅度而言，这不是一个好的度量，因为误差往往会相互补偿。</li><li id="5cb7" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">它不是高度可靠的，因为有时高个体误差产生低MBE。</li><li id="0144" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">MBE在一个方向上可能是一贯错误的。例如，如果你试图预测交通模式，它总是显示低于实际观察到的交通。</li></ul><h1 id="68cc" class="ms kk hi bd kl mt mu mv kp mw mx my kt mz na nb kw nc nd ne kz nf ng nh lc ni bi translated">相对绝对误差(RAE)</h1><p id="63e6" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">相对绝对误差的计算方法是将总绝对误差除以平均值和实际值之间的绝对差值。</p><p id="cd54" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RAE表示为，</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es nl"><img src="../Images/e622f2fefd9877923156f66d7e65a52c.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*YR71DVW0H6_rrjK5kxar9Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">相对绝对误差</figcaption></figure><p id="8e01" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中y_bar是n个实际值的平均值。</p><p id="5263" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RAE衡量预测模型的性能，用比率表示。RAE的值可以从零到一。一个好的模型应该有接近于零的值，零是最好的值。该误差显示了平均残差与目标函数与其均值的平均偏差之间的关系。</p><pre class="ly lz ma mb fd me mf mg mh aw mi bi"><span id="f3ef" class="kj kk hi mf b fi mj mk l ml mm">def relative_absolute_error(true, pred):<br/>    true_mean = np.mean(true)<br/>    squared_error_num = np.sum(np.abs(true - pred))<br/>    squared_error_den = np.sum(np.abs(true - true_mean))<br/>    rae_loss = squared_error_num / squared_error_den<br/>    return rae_loss</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/04fe9346d4f07631260b02dc0e9fb2fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*Ilrhr5ui6x0ywg0QzPzIOA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">RAE图</figcaption></figure><p id="1e03" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">优点</strong></p><ul class=""><li id="6ec2" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">RAE可用于比较以不同单位测量误差的模型。</li><li id="cef2" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">在某些情况下，RAE是可靠的，因为它提供了对离群值的保护。</li></ul><p id="2220" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">缺点</strong></p><ul class=""><li id="edcb" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">RAE的一个主要缺点是，如果<a class="ae iu" href="https://en.wikipedia.org/wiki/Reference_class_forecasting" rel="noopener ugc nofollow" target="_blank">参考预测</a>等于地面实况，它可能是不确定的。</li></ul><h1 id="071c" class="ms kk hi bd kl mt mu mv kp mw mx my kt mz na nb kw nc nd ne kz nf ng nh lc ni bi translated">平均绝对百分比误差(MAPE)</h1><p id="f19e" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">平均绝对百分比误差的计算方法是取实际值和预测值之间的差值，然后除以实际值。将绝对百分比应用于该值，并在整个数据集中取平均值。MAPE也被称为平均绝对百分比偏差(MAPD)。它随着误差的增加而线性增加。MAPE越小，模型性能越好。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es nm"><img src="../Images/90019d86981004520c9438cc85ef047c.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*yFroyKz8HekimjsHGCfhLg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">平均绝对百分比误差</figcaption></figure><pre class="ly lz ma mb fd me mf mg mh aw mi bi"><span id="0aa6" class="kj kk hi mf b fi mj mk l ml mm">def mean_absolute_percentage_error(true, pred):<br/>    abs_error = (np.abs(true - pred)) / true<br/>    sum_abs_error = np.sum(abs_error)<br/>    mape_loss = (sum_abs_error / true.size) * 100<br/>    return mape_loss</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/d73d48b65346864b96285a030e506df2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*JI5S_dGfzetjYlRT6YBXnA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">MAPE图</figcaption></figure><p id="ae13" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">优点</strong></p><ul class=""><li id="c031" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">MAPE与变量的规模无关，因为它的误差估计是以百分比表示的。</li><li id="c572" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">所有的误差都在一个共同的尺度上标准化，这很容易理解。</li><li id="8af5" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">由于MAPE使用绝对百分比误差，避免了正值和负值相互抵消的问题。</li></ul><p id="eccd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">缺点</strong></p><ul class=""><li id="7f0e" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">MAPE的一个主要缺点是当分母值为零时。我们面临着“被零除”的问题，因为它没有被定义。</li><li id="32c0" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">MAPE对负面错误的惩罚多于正面错误。因此，当我们比较预测方法的准确性时，它是有偏差的，因为它会选择一个默认值过低的方法。</li><li id="bf9d" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">由于使用除法运算，对于相同的误差，实际值的变化将导致损失的差异。考虑一个场景，当实际值为100，预测值为75时，损失为25%。而实际值是50，预测值是75，损失将是50%。但是在这两种情况下，实际误差是相同的。即25岁。</li></ul><h1 id="1008" class="ms kk hi bd kl mt mu mv kp mw mx my kt mz na nb kw nc nd ne kz nf ng nh lc ni bi translated">均方误差</h1><p id="627f" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">MSE是最常见的回归损失函数之一。在均方误差(也称为L2损失)中，我们通过计算预测值和实际值之间的差值的平方，并在整个数据集内取平均值来计算误差。MSE也称为二次损失，因为损失与误差不成比例，而是与误差的平方成比例。平方误差给予异常值更高的权重，这导致小误差的平滑梯度。优化算法受益于这种对大误差的惩罚，因为它有助于找到参数的最佳值。因为误差是平方的，所以MSE永远不会是负的。误差值的范围从零到无穷大。MSE随着误差的增加呈指数增加。一个好的模型将具有接近于零的MSE值。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es nn"><img src="../Images/c311995a9123f78c0a6c7e83a5d61dd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*X5q-geTwO6Cfaj_WctiD7w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">均方误差</figcaption></figure><pre class="ly lz ma mb fd me mf mg mh aw mi bi"><span id="8f64" class="kj kk hi mf b fi mj mk l ml mm">def mean_squared_error(true, pred):<br/>    squared_error = np.square(true - pred) <br/>    sum_squared_error = np.sum(squared_error)<br/>    mse_loss = sum_squared_error / true.size<br/>    return mse_loss</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/0043b0ed454478185fcff318dcf4d144.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*XWTOBqGhBfHl64qtXba4kw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">MSE图</figcaption></figure><p id="db84" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">优点</strong></p><ul class=""><li id="dd6d" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">MSE值用二次方程表示。因此当我们画它的时候，我们得到一个只有一个全局最小值的梯度下降。</li><li id="f837" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">对于小误差，它有效地收敛到最小值。不存在局部最小值。</li><li id="8fb6" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">MSE通过平方误差来惩罚有巨大误差的模型。</li><li id="96a3" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">它特别有助于从模型中剔除误差较大的异常值，方法是增加它们的权重。</li></ul><p id="1b28" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">缺点</strong></p><ul class=""><li id="17ef" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">当有坏的预测时，MSE的优点之一变成了缺点。对异常值的敏感性通过平方它们放大了高误差。</li><li id="15c6" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">MSE对单个大误差的影响与对许多小误差的影响相同。但最重要的是，我们将寻找一种在整体水平上表现足够好的模式。</li><li id="6384" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">MSE依赖于尺度，因为其尺度取决于数据的尺度。这使得在不同的度量之间进行比较非常不理想。</li><li id="6953" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">当一个新的异常值被引入到数据中时，模型会尝试接受这个异常值。这样做将产生不同的最佳拟合线，这可能会导致最终结果出现偏差。</li></ul><h1 id="d579" class="ms kk hi bd kl mt mu mv kp mw mx my kt mz na nb kw nc nd ne kz nf ng nh lc ni bi translated">均方根误差(RMSE)</h1><p id="544e" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">通过取MSE的平方根来计算RMSE。RMSE也叫均方根<a class="ae iu" href="http://Deviation.It" rel="noopener ugc nofollow" target="_blank">偏差。它</a>测量误差的平均值，并与实际值的偏差有关。RMSE值为零表示模型非常适合。RMSE越低，模型及其预测就越好。较高的RMSE表明残差与地面真实值存在较大偏差。RMSE可用于不同的特征，因为它有助于确定该特征是否提高了模型的预测。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es no"><img src="../Images/dc22525545c06008da0b1bba022158a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*-yx7IsF8gEw7-mB7lvPzfQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">均方根误差</figcaption></figure><pre class="ly lz ma mb fd me mf mg mh aw mi bi"><span id="9957" class="kj kk hi mf b fi mj mk l ml mm">def root_mean_squared_error(true, pred):<br/>    squared_error = np.square(true - pred) <br/>    sum_squared_error = np.sum(squared_error)<br/>    rmse_loss = np.sqrt(sum_squared_error / true.size)<br/>    return rmse_loss</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/060f20e942d9a3da6c0b0c905e095968.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*J2h5qqAUoqEntg7KkHKHJw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">RMSE图</figcaption></figure><p id="c8a1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">优点</strong></p><ul class=""><li id="8ef4" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">RMSE很容易理解。</li><li id="2249" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">它作为训练模型的启发。</li><li id="a6a7" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">这是计算简单，容易微分，许多优化算法的愿望。</li><li id="a0d8" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">由于平方根的原因，RMSE不会像MSE那样惩罚错误。</li></ul><p id="8fc8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">缺点</strong></p><ul class=""><li id="acda" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">像MSE一样，RMSE依赖于数据的规模。如果误差范围增大，它的幅度也会增大。</li><li id="1f74" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">RMSE的一个主要缺点是它对异常值非常敏感，为了正常运行，必须删除异常值。</li><li id="a050" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">RMSE随着测试样本大小的增加而增加。当我们计算不同测试样本的结果时，这是一个问题。</li></ul><h1 id="7c4c" class="ms kk hi bd kl mt mu mv kp mw mx my kt mz na nb kw nc nd ne kz nf ng nh lc ni bi translated">相对平方误差</h1><p id="3fa4" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">为了计算相对平方误差，您需要将均方误差(MSE)除以实际数据与平均值之间的差值的平方。换句话说，我们将模型的MSE除以使用平均值作为预测值的模型的MSE。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es np"><img src="../Images/018909999f4978fdf4cb961731f1d06a.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*wtDMQdst08mIxKTmFQnv2g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">相对平方误差</figcaption></figure><pre class="ly lz ma mb fd me mf mg mh aw mi bi"><span id="587a" class="kj kk hi mf b fi mj mk l ml mm">def relative_squared_error(true, pred):<br/>    true_mean = np.mean(true)<br/>    squared_error_num = np.sum(np.square(true - pred))<br/>    squared_error_den = np.sum(np.square(true - true_mean))<br/>    rse_loss = squared_error_num / squared_error_den<br/>    return rse_loss</span></pre><p id="9240" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RSE的产值用比率表示。它的范围可以从零到无穷大。一个好的模型的值应该接近于零，而一个大于1的模型是不合理的。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/91f6fdb1229c4b0d6d3e73c3a0f117f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*LZ8zp7gR_doUD6tvaNSpqA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">RSE图</figcaption></figure><p id="79b3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">优点</strong></p><ul class=""><li id="4de5" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">RSE与规模无关。因此，它可以用来比较不同单位的误差模型。</li><li id="c6d2" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">RSE对预测的平均值和规模不敏感。</li></ul><h1 id="955d" class="ms kk hi bd kl mt mu mv kp mw mx my kt mz na nb kw nc nd ne kz nf ng nh lc ni bi translated">归一化均方根误差(NRMSE)</h1><p id="afec" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">归一化RMSE通常通过除以标量值来计算。它可以有不同的方式，</p><ul class=""><li id="487c" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">RMSE /系列中的最大值</li><li id="8dfa" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">RMSE /均值</li><li id="ba36" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">RMSE /最大值和最小值之间的差值(如果平均值为零)</li><li id="bbf4" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">RMSE /标准偏差</li><li id="cfbb" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">RMSE /四分位间距</li></ul><pre class="ly lz ma mb fd me mf mg mh aw mi bi"><span id="903b" class="kj kk hi mf b fi mj mk l ml mm"># implementation of NRMSE with standard deviation</span><span id="10a9" class="kj kk hi mf b fi nq mk l ml mm">def normalized_root_mean_squared_error(true, pred):<br/>    squared_error = np.square((true - pred))<br/>    sum_squared_error = np.sum(squared_error)<br/>    rmse = np.sqrt(sum_squared_error / true.size)<br/>    nrmse_loss = rmse/np.std(pred)<br/>    return nrmse_loss</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/ac197f03ffd453cb0a653dc9e1cc3a08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*0LBHUYmTluwUjUAfGGiB8A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">NRMSE图(标准偏差)</figcaption></figure><p id="71b6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有时选择四分位范围可能是最好的选择，因为其他方法容易出现异常值。当您想要比较不同因变量的模型或因变量被修改(对数转换或标准化)时，NRMSE是一个很好的度量。它克服了尺度依赖性，简化了不同尺度模型之间甚至数据集之间的比较。</p><h1 id="6533" class="ms kk hi bd kl mt mu mv kp mw mx my kt mz na nb kw nc nd ne kz nf ng nh lc ni bi translated">相对均方根误差(RRMSE)</h1><p id="47b7" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">RRMSE是RMSE的无量纲形式。相对均方根误差(RRMSE)是由均方根值归一化的均方根误差，其中每个残差与实际值成比例。虽然RMSE受到原始测量规模的限制，但RRMSE可以用来比较不同的测量技术。当你的预测不准确时，就会导致RRMSE增加。RRMSE用相对误差或百分数来表示误差。模型精度是，</p><ul class=""><li id="5b38" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">优秀当RRMSE &lt; 10%</li><li id="4d02" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">Good when RRMSE is between 10% and 20%</li><li id="5e2d" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">Fair when RRMSE is between 20% and 30%</li><li id="c9eb" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">Poor when RRMSE &gt; 30%</li></ul><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nr"><img src="../Images/27e230da994499555ba3bae9eedbf9e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*GSJvPw6ClWFGHnxj-NfCvw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">相对均方根误差</figcaption></figure><pre class="ly lz ma mb fd me mf mg mh aw mi bi"><span id="491f" class="kj kk hi mf b fi mj mk l ml mm">def relative_root_mean_squared_error(true, pred):<br/>    num = np.sum(np.square(true - pred))<br/>    den = np.sum(np.square(pred))<br/>    squared_error = num/den<br/>    rrmse_loss = np.sqrt(squared_error)<br/>    return rrmse_loss</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/c333fec5dabe354d0e31f8a4dbff2135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*amB6XX4Vw-zbr4pZ7ZUFXg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">RRMSE图</figcaption></figure><h1 id="b79e" class="ms kk hi bd kl mt mu mv kp mw mx my kt mz na nb kw nc nd ne kz nf ng nh lc ni bi translated">均方根对数误差</h1><p id="c9de" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">均方根对数误差的计算方法是将对数应用于实际值和预测值，然后取它们的差值。RMSLE对于异常值是稳健的，其中小误差和大误差被平均对待。</p><p id="3803" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果预测值小于实际值，它对模型的惩罚更大，而如果预测值大于实际值，模型的惩罚更小。它不会因为日志而惩罚高错误。因此，该模型对低估的惩罚比高估大。这在我们不会被高估困扰，但低估是不可接受的情况下会很有帮助。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es ns"><img src="../Images/125e781d81754bfb33b7da19535aa6d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*TQwlWzCcVuwDAfI9yUC17A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">均方根对数误差</figcaption></figure><pre class="ly lz ma mb fd me mf mg mh aw mi bi"><span id="91b9" class="kj kk hi mf b fi mj mk l ml mm">def root_mean_squared_log_error(true, pred):<br/>    square_error = np.square((np.log(true + 1) - np.log(pred + 1)))<br/>    mean_square_log_error = np.mean(square_error)<br/>    rmsle_loss = np.sqrt(mean_square_log_error)<br/>    return rmsle_loss</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/ca1cf6407ad94b4a87717d925fa569a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*G5lUcCZ5zT9F559lHYhetw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">RMSLE图</figcaption></figure><p id="d580" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">优点</strong></p><ul class=""><li id="de36" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">RMSLE不依赖于规模，并且在一系列规模上都是有用的。</li><li id="1fef" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">它不受大的异常值的影响。</li><li id="b74c" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">它只考虑实际值和预测值之间的相对误差。</li></ul><p id="f039" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">缺点</strong></p><ul class=""><li id="6622" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">它有偏见的惩罚，惩罚低估多于高估。</li></ul><h1 id="c7a7" class="ms kk hi bd kl mt mu mv kp mw mx my kt mz na nb kw nc nd ne kz nf ng nh lc ni bi translated">胡伯损失</h1><p id="4095" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">如果您想要一个既能了解异常值又能忽略它们的函数呢？Huber loss是你的。Huber loss是线性和二次评分法的结合。它有一个超参数增量(𝛿)，可以根据数据进行调整。对于高于delta的值，损耗将是线性的(L1损耗),对于低于delta的值，损耗将是二次的(L2损耗)。它平衡并结合了MAE(平均绝对误差)和MSE(均方误差)的良好特性。换句话说，对于小于δ的损失值，将使用MSE，而对于大于δ的损失值，将使用MAE。delta (𝛿)的选择非常关键，因为它定义了我们对异常值的选择。Huber损失通过使用MAE降低了我们对较大损失值的异常值的权重，而对于较小的损失值，它使用MSE保持二次函数。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es nt"><img src="../Images/2497eaffc29684d94af269c814213e4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*5zxb2i1BAM6wcfkm0DMirw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">胡伯损失</figcaption></figure><pre class="ly lz ma mb fd me mf mg mh aw mi bi"><span id="8717" class="kj kk hi mf b fi mj mk l ml mm">def huber_loss(true, pred, delta):<br/>    huber_mse = 0.5 * np.square(true - pred)<br/>    huber_mae = delta * (np.abs(true - pred) - 0.5 * (np.square(delta)))<br/>    return np.where(np.abs(true - pred) &lt;= delta, huber_mse, huber_mae)</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/8ddf8474ff096ccc370e096a2c29b530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*xmXTIRPvl_GhLO1r12ADdQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">具有不同(0.5，1，5)δ值的Huber图</figcaption></figure><p id="a584" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">优点</strong></p><ul class=""><li id="a407" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">它在零点是可微的。</li><li id="3bce" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">由于高于delta的线性度，异常值被适当地处理。</li><li id="7e7b" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">超参数，𝛿可以调整，以最大限度地提高模型的准确性。</li></ul><p id="f368" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">缺点</strong></p><ul class=""><li id="a9e0" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">对于大型数据集，额外的条件和比较使得Huber损失在计算上很昂贵。</li><li id="5927" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">为了最大化模型精度，需要优化𝛿，这是一个迭代过程。</li><li id="9466" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">它只可微一次。</li></ul><h1 id="1534" class="ms kk hi bd kl mt mu mv kp mw mx my kt mz na nb kw nc nd ne kz nf ng nh lc ni bi translated">对数损失</h1><p id="36f5" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">Log cosh计算误差的双曲余弦的对数。这个函数比二次损失更平滑。它像MSE一样工作，但是不受大的预测误差的影响。它与huber loss非常相似，因为它是线性和二次评分方法的组合。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es nu"><img src="../Images/f4c61e17712d81df6466ba0c61d4cdfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*disNhi7tbubxu1fdMC-DVw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">对数损失</figcaption></figure><pre class="ly lz ma mb fd me mf mg mh aw mi bi"><span id="e361" class="kj kk hi mf b fi mj mk l ml mm">def log_cosh(true, pred):<br/>    logcosh = np.log(np.cosh(pred - true))<br/>    logcosh_loss = np.sum(logcosh)<br/>    return logcosh_loss</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/0e88ba40329a21ca7f48142550bd220c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*UP3TJmL46ZhkPDqDrCQMuQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">对数余弦图</figcaption></figure><p id="9c8a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">优点</strong></p><ul class=""><li id="4682" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">它具有Huber损失的优点，同时又是处处两次可微的。像XGBoost这样的优化算法比像Huber这样只能微分一次的函数更喜欢双重可微。</li><li id="1bde" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">它比Huber需要更少的计算。</li></ul><p id="c7b2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">缺点</strong></p><ul class=""><li id="443b" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">它的适应性较差，因为它遵循固定的尺度。</li><li id="22a3" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">与Huber损失相比，推导更复杂，需要更深入的研究。</li></ul><h1 id="4daf" class="ms kk hi bd kl mt mu mv kp mw mx my kt mz na nb kw nc nd ne kz nf ng nh lc ni bi translated">分位数损失</h1><p id="1f6e" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">分位数回归损失函数用于预测分位数。分位数是确定组中有多少值低于或高于某个限制的值。它通过预测(独立)变量的值来估计响应(因变量)变量的条件中位数或<em class="nj">分位数</em>。损失函数是MAE的扩展，除了第50百分位，它是MAE。它甚至为具有非恒定方差的残差提供预测区间，并且它不假设响应的特定参数分布。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nv"><img src="../Images/99339b047f6e51194d7c54ddfb3adb84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GSs59BqcTEo0ELhJAGtEQQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">分位数损失</figcaption></figure><p id="13c0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">𝛾代表所需的分位数。分位数值是根据我们希望如何衡量正负误差来选择的。</p><p id="1a84" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的损失函数中，𝛾的值在0和1之间。当存在低估时，公式的第一部分将起主导作用，而对于高估，公式的第二部分将起主导作用。quantile(𝛾的选定值)对过预测和欠预测给出了不同的惩罚。当𝛾 = 0.5时，低估和高估由相同的因子惩罚，并获得中间值。当𝛾值较大时，高估比低估更不利。例如，当𝛾 = 0.75时，该模型将惩罚高估，其代价是低估的三倍。基于梯度下降的优化算法从分位数而不是平均值学习。</p><pre class="ly lz ma mb fd me mf mg mh aw mi bi"><span id="0bd9" class="kj kk hi mf b fi mj mk l ml mm">def quantile_loss(true, pred, gamma):<br/>    val1 = gamma * np.abs(true - pred)<br/>    val2 = (1-gamma) * np.abs(true - pred)<br/>    q_loss = np.where(true &gt;= pred, val1, val2)<br/>    return q_loss</span></pre><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/51d8b47d9eeb6a5492c117748093edea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*g_wN1NkPvMAN6KItw21dkg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">具有不同(0.25，0.5，0.75)伽马值的分位数损失图</figcaption></figure><p id="48ea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">优点</strong></p><ul class=""><li id="98ba" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">当我们预测区间而不是点估计时，它特别有用。</li><li id="fbdb" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">该函数也可用于计算神经网络和基于树的模型中的预测区间。</li><li id="c106" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">它对异常值是鲁棒的。</li></ul><p id="6764" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">缺点</strong></p><ul class=""><li id="94f6" class="le lf hi ix b iy iz jc jd jg mo jk mp jo mq js mr lm ln lo bi translated">分位数损失是计算密集型的。</li><li id="96fb" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated">如果我们使用平方损失来衡量效率，或者如果我们要估计均值，那么分位数损失会更糟。</li></ul><p id="70a8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢您一路阅读到这里！我希望这篇文章对你的学习之旅有所帮助。我很想在评论中听到我错过的其他损失函数。评价愉快！</p><p id="38f1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在LinkedIn 上与我联系</p><h2 id="ce17" class="kj kk hi bd kl km kn ko kp kq kr ks kt jg ku kv kw jk kx ky kz jo la lb lc ld bi translated">参考</h2><ul class=""><li id="c306" class="le lf hi ix b iy lg jc lh jg li jk lj jo lk js mr lm ln lo bi translated">数据挖掘算法，用R</li><li id="2295" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated"><a class="ae iu" href="https://www.sciencedirect.com/topics/engineering/mean-bias-error" rel="noopener ugc nofollow" target="_blank">https://www . science direct . com/topics/engineering/mean-bias-error</a></li><li id="d409" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated"><a class="ae iu" href="https://www.sciencedirect.com/science/article/abs/pii/S1364032115013258" rel="noopener ugc nofollow" target="_blank">https://www . science direct . com/science/article/ABS/pii/s 1364032115013258</a></li><li id="32f5" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated"><a class="ae iu" href="https://journals.plos.org/plosntds/article?id=10.1371/journal.pntd.0005295" rel="noopener ugc nofollow" target="_blank">https://journals.plos.org/plosntds/article?id = 10.1371/journal . pntd . 0005295</a></li><li id="2b21" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated"><a class="ae iu" href="https://support.sas.com/resources/papers/proceedings17/SAS0525-2017.pdf" rel="noopener ugc nofollow" target="_blank">https://support . SAS . com/resources/papers/proceedings 17/SAS 0525-2017 . pdf</a></li><li id="df81" class="le lf hi ix b iy lp jc lq jg lr jk ls jo lt js mr lm ln lo bi translated"><a class="ae iu" href="https://stats.stackexchange.com/questions/39002/when-is-quantile-regression-worse-than-ols" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/39002/when-is-quantile-regression-bad-than-ols</a></li></ul></div></div>    
</body>
</html>