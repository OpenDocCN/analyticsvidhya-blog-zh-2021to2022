<html>
<head>
<title>NLP Tutorial for Text Classification in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中文本分类的NLP教程</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e?source=collection_archive---------0-----------------------#2021-04-01">https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e?source=collection_archive---------0-----------------------#2021-04-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/71e7a259f22ad98b8f77ae189db6325d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rnko_Sy3iEQ-sUbzmU4A-A.png"/></div></div></figure><p id="cbea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">文本形式的非结构化数据:聊天记录、电子邮件、社交媒体、调查反馈如今无处不在。文本可以是丰富的信息来源，但由于其非结构化的性质，很难从中提取真知灼见。</p><p id="8dfc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">文本分类是有监督机器学习的重要任务之一。这是一个为文档分配标签/类别的过程，帮助我们以经济高效的方式自动快速构建和分析文本。它是自然语言处理的基本任务之一，具有广泛的应用，如情感分析、垃圾邮件检测、主题标注、意图检测等。</p><p id="e566" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">在这篇文章中，我将带你一步步了解我们如何使用Python </em> </strong>进行文本分类。我已经在GitHub上传了完整的代码:<a class="ae jp" href="https://github.com/vijayaiitk/NLP-text-classification-model" rel="noopener ugc nofollow" target="_blank">https://github.com/vijayaiitk/NLP-text-classification-model</a></p><p id="96d7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们将分类问题分成以下步骤:</p><ol class=""><li id="bfee" class="jq jr hi is b it iu ix iy jb js jf jt jj ju jn jv jw jx jy bi translated">设置:导入库</li><li id="2463" class="jq jr hi is b it jz ix ka jb kb jf kc jj kd jn jv jw jx jy bi translated">加载数据集和探索性数据分析</li><li id="fb66" class="jq jr hi is b it jz ix ka jb kb jf kc jj kd jn jv jw jx jy bi translated">文本预处理</li><li id="cec5" class="jq jr hi is b it jz ix ka jb kb jf kc jj kd jn jv jw jx jy bi translated">从文本中提取矢量(矢量化)</li><li id="1c02" class="jq jr hi is b it jz ix ka jb kb jf kc jj kd jn jv jw jx jy bi translated">运行ML算法</li><li id="416b" class="jq jr hi is b it jz ix ka jb kb jf kc jj kd jn jv jw jx jy bi translated">结论</li></ol><h1 id="e70e" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">步骤1:导入库</h1><p id="1682" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">第一步是导入以下库列表:</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="37eb" class="lq kf hi lm b fi lr ls l lt lu">import pandas as pd<br/>import numpy as np</span><span id="e223" class="lq kf hi lm b fi lv ls l lt lu"><strong class="lm hj">#for text pre-processing</strong><br/>import re, string<br/>import nltk<br/>from nltk.tokenize import word_tokenize<br/>from nltk.corpus import stopwords<br/>from nltk.tokenize import word_tokenize<br/>from nltk.stem import SnowballStemmer<br/>from nltk.corpus import wordnet<br/>from nltk.stem import WordNetLemmatizer</span><span id="f2cd" class="lq kf hi lm b fi lv ls l lt lu">nltk.download('punkt')<br/>nltk.download('averaged_perceptron_tagger')<br/>nltk.download('wordnet')</span><span id="3541" class="lq kf hi lm b fi lv ls l lt lu"><strong class="lm hj">#for model-building</strong><br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.naive_bayes import MultinomialNB<br/>from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix<br/>from sklearn.metrics import roc_curve, auc, roc_auc_score</span><span id="70ec" class="lq kf hi lm b fi lv ls l lt lu"><strong class="lm hj"># bag of words</strong><br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.feature_extraction.text import CountVectorizer</span><span id="8f6a" class="lq kf hi lm b fi lv ls l lt lu"><strong class="lm hj">#for word embedding</strong><br/>import gensim<br/>from gensim.models import Word2Vec</span></pre><h1 id="5243" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">步骤2:加载数据集和EDA</h1><p id="44f1" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">我们将在本文中使用的数据集是著名的“<a class="ae jp" href="https://www.kaggle.com/c/nlp-getting-started/overview" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> <em class="jo">自然语言处理灾难推文</em> </strong> </a>”数据集，其中我们将预测给定推文是关于真实灾难(目标=1)还是关于真实灾难(目标=0)</p><p id="b14e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">在这场比赛中，你面临的挑战是建立一个机器学习模型，预测哪些推文是关于真正的灾难，哪些不是。你可以访问一个由10，000条推文组成的数据集，这些推文都是经过人工分类的。</em></p><p id="90f7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在Kaggle笔记本中加载数据集:</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="05ef" class="lq kf hi lm b fi lr ls l lt lu"><strong class="lm hj">df_train</strong>= pd.read_csv('../input/nlp-getting-started/train.csv')<br/><strong class="lm hj">df_test</strong>=pd.read_csv('../input/nlp-getting-started/test.csv')</span></pre><p id="cf0f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们在训练(有标签的)数据集中有7613条推文，在测试(无标签的)数据集中有3263条推文。这是我们将用于构建模型的训练/标记数据集的快照</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/a3bef1e334aef2186996e6495cde01ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*uAstTRsjtV9_BcYQEMaidw.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">标记数据集的快照</figcaption></figure><p id="16a4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">探索性数据分析(EDA) </strong></p><ol class=""><li id="b266" class="jq jr hi is b it iu ix iy jb js jf jt jj ju jn jv jw jx jy bi translated"><strong class="is hj"> <em class="jo">类别分布</em> </strong>:类别0(无灾害)的推文比类别1(灾害推文)多。我们可以说，该数据集相对平衡，有4342条非灾难推文(57%)和3271条灾难推文(43%)。因为数据是平衡的，所以在构建模型时我们不会应用SMOTE这样的数据平衡技术</li></ol><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="423a" class="lq kf hi lm b fi lr ls l lt lu">x=df_train['target'].value_counts()<br/>print(x)<br/>sns.barplot(x.index,x)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/1e4dd9fe392a5f8e2c1911c3b993abeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*VwjpWSYPZkGghSuTE8hufA.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">阶级分布</figcaption></figure><p id="bdce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.<strong class="is hj"> <em class="jo">缺失值</em> </strong>:我们在位置字段中有大约2.5k个缺失值，在关键字列中有61个缺失值</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="be37" class="lq kf hi lm b fi lr ls l lt lu">df_train.isna().sum()</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/19dfba46fb74b2fd67696e41389d6f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*mAaLurGBYU4_hgj1LhMS8A.png"/></div></figure><p id="241b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.<strong class="is hj"> <em class="jo">一条微博的字数</em> </strong>:灾难微博比非灾难微博更啰嗦</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="ab2a" class="lq kf hi lm b fi lr ls l lt lu"><strong class="lm hj"># WORD-COUNT</strong><br/>df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))<br/>print(df_train[df_train['target']==1]['word_count'].mean()) #Disaster tweets<br/>print(df_train[df_train['target']==0]['word_count'].mean()) #Non-Disaster tweets</span></pre><p id="2981" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一条灾难微博的平均字数是15.17个字，而一条非灾难微博的平均字数是14.7个字</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="0b79" class="lq kf hi lm b fi lr ls l lt lu"><strong class="lm hj"># PLOTTING WORD-COUNT</strong><br/>fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))<br/>train_words=df_train[df_train['target']==1]['word_count']<br/>ax1.hist(train_words,color='red')<br/>ax1.set_title('Disaster tweets')<br/>train_words=df_train[df_train['target']==0]['word_count']<br/>ax2.hist(train_words,color='green')<br/>ax2.set_title('Non-disaster tweets')<br/>fig.suptitle('Words per tweet')<br/>plt.show()</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/c3e2915c3e17c5d9c65bf659c4a9441d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N_WC8zzdN8t3c2uRJ8g61g.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">每条推文的字数图(灾难与非灾难推文)</figcaption></figure><p id="337d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.<strong class="is hj"> <em class="jo">一条微博的字数</em> </strong>:灾难微博比非灾难微博长</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="fc7b" class="lq kf hi lm b fi lr ls l lt lu"><strong class="lm hj"># CHARACTER-COUNT</strong><br/>df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))<br/>print(df_train[df_train['target']==1]['char_count'].mean()) #Disaster tweets<br/>print(df_train[df_train['target']==0]['char_count'].mean()) #Non-Disaster tweets</span></pre><p id="5fdc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一条灾难微博的平均字数是108.1个，相比之下，一条非灾难微博的平均字数是95.7个</p><h1 id="0463" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">步骤3:文本预处理</h1><p id="07f8" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">在我们开始建模之前，我们需要通过删除标点符号&amp;特殊字符、清理文本、删除停用词和应用词汇化来预处理我们的数据集</p><p id="43a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">简单的文本清理流程</em> </strong>:一些常见的文本清理流程包括:</p><ul class=""><li id="0285" class="jq jr hi is b it iu ix iy jb js jf jt jj ju jn me jw jx jy bi translated">删除标点符号、特殊字符、网址和标签</li><li id="fa0a" class="jq jr hi is b it jz ix ka jb kb jf kc jj kd jn me jw jx jy bi translated">删除前导空格、尾随空格和额外空格/制表符</li><li id="6e65" class="jq jr hi is b it jz ix ka jb kb jf kc jj kd jn me jw jx jy bi translated">错别字、俚语被纠正，缩写以其长形式书写</li></ul><p id="93da" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">停用词移除</em> </strong>:我们可以使用<em class="jo"> nltk从英语词汇中移除一系列通用停用词。</em>这样的词有“我”、“你”、“一个”、“这个”、“他”、“哪个”等。</p><p id="55fe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"/></p><p id="b3cf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"><em class="jo"/></strong>词汇化:是将单词还原为其基本形式的过程</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/c41968d5aa9eaddf100f488fda13da0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*fgnyGjSb0s__kNAdGhxZpw.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">词干化与词汇化</figcaption></figure><p id="e4d5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面是文本预处理的代码:</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="9f31" class="lq kf hi lm b fi lr ls l lt lu">#<strong class="lm hj">convert to lowercase, strip and remove punctuations</strong><br/>def preprocess(text):<br/>    text = text.lower() <br/>    text=text.strip()  <br/>    text=re.compile('&lt;.*?&gt;').sub('', text) <br/>    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  <br/>    text = re.sub('\s+', ' ', text)  <br/>    text = re.sub(r'\[[0-9]*\]',' ',text) <br/>    text=re.sub(r'[^\w\s]', '', str(text).lower().strip())<br/>    text = re.sub(r'\d',' ',text) <br/>    text = re.sub(r'\s+',' ',text) <br/>    return text</span><span id="56a9" class="lq kf hi lm b fi lv ls l lt lu"><br/> <br/># <strong class="lm hj">STOPWORD REMOVAL</strong><br/>def stopword(string):<br/>    a= [i for i in string.split() if i not in stopwords.words('english')]<br/>    return ' '.join(a)</span><span id="9fe4" class="lq kf hi lm b fi lv ls l lt lu"><strong class="lm hj">#LEMMATIZATION</strong><br/># Initialize the lemmatizer<br/>wl = WordNetLemmatizer()<br/> <br/># This is a helper function to map NTLK position tags<br/>def get_wordnet_pos(tag):<br/>    if tag.startswith('J'):<br/>        return wordnet.ADJ<br/>    elif tag.startswith('V'):<br/>        return wordnet.VERB<br/>    elif tag.startswith('N'):<br/>        return wordnet.NOUN<br/>    elif tag.startswith('R'):<br/>        return wordnet.ADV<br/>    else:<br/>        return wordnet.NOUN</span><span id="9287" class="lq kf hi lm b fi lv ls l lt lu"># Tokenize the sentence<br/>def lemmatizer(string):<br/>    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags<br/>    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token<br/>    return " ".join(a)</span></pre><p id="5ef7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">最终预处理</strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="fc7c" class="lq kf hi lm b fi lr ls l lt lu">def finalpreprocess(string):<br/>    return lemmatizer(stopword(preprocess(string)))</span><span id="69a7" class="lq kf hi lm b fi lv ls l lt lu">df_train['clean_text'] = df_train['text'].apply(lambda x: finalpreprocess(x))<br/>df_train.head()</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/9eecc803c20217a94c163306ac2a3899.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BJhuy-_7ZMY83wtS9Pxw9w.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">清理的文本快照</figcaption></figure><h1 id="f3af" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">步骤4:从文本中提取矢量(矢量化)</h1><p id="8c07" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">在建立机器学习模型时，很难处理文本数据，因为这些模型需要定义良好的数字数据。将文本数据转换成数字数据/向量的过程称为<strong class="is hj">矢量化</strong>或者在NLP世界中称为单词嵌入<strong class="is hj">。<em class="jo">单词包</em> </strong> <em class="jo"> (BoW) </em>和<strong class="is hj"> <em class="jo">单词嵌入</em></strong><em class="jo">(</em>with Word 2 vec)是将文本数据转换为数值数据的两种众所周知的方法。</p><p id="a50d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">包单词</strong>有几个版本，对应不同的单词评分方法。我们使用Sklearn库通过以下方法计算BoW数值:</p><ol class=""><li id="03cc" class="jq jr hi is b it iu ix iy jb js jf jt jj ju jn jv jw jx jy bi translated"><strong class="is hj"> Count vectors </strong>:它从文档语料库中构建一个词汇表，并计算单词在每个文档中出现的次数</li></ol><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/fb4cde29c5dc0dc42f4d1a8ddf682bce.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*jkAHb_uUsZGleEAbIW8bpg.jpeg"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">计数向量</figcaption></figure><p id="6935" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.<strong class="is hj">术语频率-逆文档频率(tf-Idf) </strong>:计数向量可能不是将文本数据转换为数字数据的最佳表示。因此，除了简单的计数，我们还可以使用单词包的高级变体，它使用<strong class="is hj">术语频率-逆文档频率</strong>(或Tf-Idf) <strong class="is hj">。</strong>基本上，一个单词的值随着在文档中的计数成比例地增加，但是它与该单词在语料库中的频率成反比</p><p id="a234" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo"> Word2Vec </em> </strong>:使用<strong class="is hj">词袋</strong>技术的一个主要缺点是，它不能从向量中捕获词的含义或关系。Word2Vec是使用浅层神经网络学习单词嵌入的最流行的技术之一，浅层神经网络能够捕捉文档中单词的上下文、语义和句法相似性、与其他单词的关系等。</p><p id="d76d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以使用这些方法中的任何一种将我们的文本数据转换成数字形式，这将用于构建分类模型。考虑到这一点，我将首先使用下面提到的代码将数据集划分为训练集(80%)和测试集(20%)</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="a1d4" class="lq kf hi lm b fi lr ls l lt lu"><strong class="lm hj">#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST</strong><br/>X_train, X_test, y_train, y_test = train_test_split(df_train["clean_text"],df_train["target"],test_size=0.2,shuffle=True)</span><span id="4186" class="lq kf hi lm b fi lv ls l lt lu"><strong class="lm hj">#Word2Vec<br/></strong># Word2Vec runs on tokenized sentences<br/>X_train_tok= [nltk.word_tokenize(i) for i in X_train]  <br/>X_test_tok= [nltk.word_tokenize(i) for i in X_test]</span></pre><p id="dc46" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面是使用<strong class="is hj"> <em class="jo">单词包</em> </strong> <em class="jo"> </em>(使用Tf-Idf)和<strong class="is hj"> Word2Vec </strong>进行矢量化的代码</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="f7aa" class="lq kf hi lm b fi lr ls l lt lu"><strong class="lm hj">#Tf-Idf</strong><br/>tfidf_vectorizer = TfidfVectorizer(use_idf=True)<br/>X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) <br/>X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)</span><span id="cdeb" class="lq kf hi lm b fi lv ls l lt lu"><strong class="lm hj">#building Word2Vec model</strong><br/>class MeanEmbeddingVectorizer(object):<br/>    def __init__(self, word2vec):<br/>        self.word2vec = word2vec<br/>        # if a text is empty we should return a vector of zeros<br/>        # with the same dimensionality as all the other vectors<br/>        self.dim = len(next(iter(word2vec.values())))</span><span id="3f39" class="lq kf hi lm b fi lv ls l lt lu">def fit(self, X, y):<br/>        return self</span><span id="5ca1" class="lq kf hi lm b fi lv ls l lt lu">def transform(self, X):<br/>        return np.array([<br/>            np.mean([self.word2vec[w] for w in words if w in self.word2vec]<br/>                    or [np.zeros(self.dim)], axis=0)<br/>            for words in X<br/>        ])</span><span id="dee0" class="lq kf hi lm b fi lv ls l lt lu">w2v = dict(zip(model.wv.index2word, model.wv.syn0)) df['clean_text_tok']=[nltk.word_tokenize(i) for i in df['clean_text']]<br/>model = Word2Vec(df['clean_text_tok'],min_count=1)     <br/>modelw = MeanEmbeddingVectorizer(w2v)</span><span id="20d4" class="lq kf hi lm b fi lv ls l lt lu"><strong class="lm hj"># converting text to numerical data using Word2Vec</strong><br/>X_train_vectors_w2v = modelw.transform(X_train_tok)<br/>X_val_vectors_w2v = modelw.transform(X_test_tok)</span></pre><h1 id="fd74" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">第五步。运行ML算法</h1><p id="c848" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">是时候在矢量化数据集上训练一个<strong class="is hj">机器学习模型</strong>并进行测试了。现在我们已经将文本数据转换为数值数据，我们可以在<strong class="is hj"><em class="jo">X _ train _ vector _ tfi df</em></strong>&amp;<strong class="is hj"><em class="jo">y _ train</em></strong><em class="jo">上运行ML模型。</em>我们将在<strong class="is hj"><em class="jo">X _ test _ vectors _ tfi df</em></strong>上测试该模型，得到<strong class="is hj"> <em class="jo"> y_predict </em> </strong>并进一步评估该模型的性能</p><ol class=""><li id="e201" class="jq jr hi is b it iu ix iy jb js jf jt jj ju jn jv jw jx jy bi translated"><strong class="is hj"> Logistic回归:</strong>我们先从最简单的一个Logistic回归开始。您可以使用下面几行代码在scikit中轻松构建一个LogisticRegression</li></ol><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="68e8" class="lq kf hi lm b fi lr ls l lt lu"><strong class="lm hj">#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)</strong></span><span id="5047" class="lq kf hi lm b fi lv ls l lt lu">lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')<br/>lr_tfidf.fit(X_train_vectors_tfidf, y_train)  </span><span id="adfd" class="lq kf hi lm b fi lv ls l lt lu"><strong class="lm hj">#Predict y value for test dataset</strong><br/>y_predict = lr_tfidf.predict(X_test_vectors_tfidf)<br/>y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]</span><span id="fa3a" class="lq kf hi lm b fi lv ls l lt lu">print(classification_report(y_test,y_predict))<br/>print('Confusion Matrix:',confusion_matrix(y_test, y_predict))<br/> <br/>fpr, tpr, thresholds = roc_curve(y_test, y_prob)<br/>roc_auc = auc(fpr, tpr)<br/>print('AUC:', roc_auc)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/92278f36aa7a5e3d4db104e219b62e84.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*-L3XINyp8lzxNlGAZKeqKQ.png"/></div></figure><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="b508" class="lq kf hi lm b fi lr ls l lt lu"><strong class="lm hj">#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)</strong><br/>lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')<br/>lr_w2v.fit(X_train_vectors_w2v, y_train)  #model</span><span id="0746" class="lq kf hi lm b fi lv ls l lt lu"><strong class="lm hj">#Predict y value for test dataset</strong><br/>y_predict = lr_w2v.predict(X_test_vectors_w2v)<br/>y_prob = lr_w2v.predict_proba(X_test_vectors_w2v)[:,1]</span><span id="08a1" class="lq kf hi lm b fi lv ls l lt lu">print(classification_report(y_test,y_predict))<br/>print('Confusion Matrix:',confusion_matrix(y_test, y_predict))<br/> <br/>fpr, tpr, thresholds = roc_curve(y_test, y_prob)<br/>roc_auc = auc(fpr, tpr)<br/>print('AUC:', roc_auc)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/f6d09eff1710d80a4b7a335672aaa147.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*UkwjmKPaLBYEx-2L8oRKbw.png"/></div></figure><p id="be66" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.<strong class="is hj">朴素贝叶斯:</strong>这是一种概率分类器，它利用了<a class="ae jp" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯定理</a>，这是一种基于可能相关的条件的先验知识使用概率进行预测的规则</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="94c4" class="lq kf hi lm b fi lr ls l lt lu"><strong class="lm hj">#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)</strong></span><span id="ba56" class="lq kf hi lm b fi lv ls l lt lu">nb_tfidf = MultinomialNB()<br/>nb_tfidf.fit(X_train_vectors_tfidf, y_train)  </span><span id="fa9c" class="lq kf hi lm b fi lv ls l lt lu"><strong class="lm hj">#Predict y value for test dataset</strong><br/>y_predict = nb_tfidf.predict(X_test_vectors_tfidf)<br/>y_prob = nb_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]</span><span id="1f8b" class="lq kf hi lm b fi lv ls l lt lu">print(classification_report(y_test,y_predict))<br/>print('Confusion Matrix:',confusion_matrix(y_test, y_predict))<br/> <br/>fpr, tpr, thresholds = roc_curve(y_test, y_prob)<br/>roc_auc = auc(fpr, tpr)<br/>print('AUC:', roc_auc)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/8ebb3713e02cd301e15b75f30ce19ac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*97CFF2qPirVK8vIR4Q0-NA.png"/></div></figure><p id="25cf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，您可以选择最佳模型(<em class="jo"> lr_tfidf </em>)来估计未标记数据集的“目标”值(<em class="jo"> df_test </em>)。这是代码</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="bcfe" class="lq kf hi lm b fi lr ls l lt lu"><strong class="lm hj">#Pre-processing the new dataset</strong><br/>df_test['clean_text'] = df_test['text'].apply(lambda x: finalpreprocess(x)) #preprocess the data<br/>X_test=df_test['clean_text'] </span><span id="f10e" class="lq kf hi lm b fi lv ls l lt lu"><strong class="lm hj">#converting words to numerical data using tf-idf</strong><br/>X_vector=tfidf_vectorizer.transform(X_test)</span><span id="b8a5" class="lq kf hi lm b fi lv ls l lt lu"><strong class="lm hj">#use the best model to predict 'target' value for the new dataset </strong><br/>y_predict = lr_tfidf.predict(X_vector)      <br/>y_prob = lr_tfidf.predict_proba(X_vector)[:,1]<br/>df_test['predict_prob']= y_prob<br/>df_test['target']= y_predict<br/>final=df_test[['clean_text','target']].reset_index(drop=True)<br/>print(final.head())</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/da54b44e00bcfab13759dcf07ddcb1a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*cpN9tiUzuRL-kESncrbWAA.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">未标记数据集的预测目标值</figcaption></figure><h1 id="68cc" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated"><strong class="ak">结论</strong></h1><p id="5956" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">在本文中，我演示了构建一个文本分类模型的基础知识，该模型比较了<strong class="is hj">单词包</strong>(使用Tf-Idf)和<strong class="is hj">单词嵌入</strong>与Word2Vec。通过以下方式，您可以使用此代码进一步增强模型的性能</p><ul class=""><li id="5e8c" class="jq jr hi is b it iu ix iy jb js jf jt jj ju jn me jw jx jy bi translated">使用其他分类算法，如<strong class="is hj">支持向量机(SVM)、XgBoost、集成模型、神经网络等。</strong></li><li id="8e6a" class="jq jr hi is b it jz ix ka jb kb jf kc jj kd jn me jw jx jy bi translated">使用Gridsearch调整模型的超参数</li><li id="f0ad" class="jq jr hi is b it jz ix ka jb kb jf kc jj kd jn me jw jx jy bi translated">使用先进的单词嵌入方法，如<strong class="is hj"> GloVe </strong>和<strong class="is hj"> BERT </strong></li></ul><p id="2305" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你喜欢这篇文章，请点赞、评论并分享。欢迎您的反馈！</p><div class="mm mn ez fb mo mp"><a href="https://github.com/vijayaiitk/NLP-text-classification-model" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">vijayaitk/NLP-文本-分类-模型</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">在GitHub上创建一个帐户，为vijayaitk/NLP-text-classification-model开发做贡献。</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">github.com</p></div></div><div class="my l"><div class="mz l na nb nc my nd io mp"/></div></div></a></div><p id="17b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jp" href="https://www.linkedin.com/in/vijayarani/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/vijayarani/</a></p></div></div>    
</body>
</html>