<html>
<head>
<title>Image Super-Resolution Using RCAN: Residual Channel Attention Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用RCAN的图像超分辨率:剩余通道注意网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/image-super-resolution-using-rcan-residual-channel-attention-networks-82a12d3005a7?source=collection_archive---------23-----------------------#2021-04-03">https://medium.com/analytics-vidhya/image-super-resolution-using-rcan-residual-channel-attention-networks-82a12d3005a7?source=collection_archive---------23-----------------------#2021-04-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="61f7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">摘要</h1><p id="dbad" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">该文提出了一种新的模型结构，该结构由残差中残差(RIR)块组成，每个块具有信道关注度。总管道是400层深度卷积网络。更具体地说，该文件建议</p><ul class=""><li id="b56d" class="ka kb hh je b jf kc jj kd jn ke jr kf jv kg jz kh ki kj kk bi translated">剩余块中的剩余块，形成RIR结构。</li><li id="3e9c" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">一个长的和短的跳跃连接，其目的是将低频特征与计算的高频特征一起传送。</li><li id="9197" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">提出了针对每个残差块的信道注意机制，通过自注意对每个信道信息进行不同的加权。</li><li id="20d9" class="ka kb hh je b jf kl jj km jn kn jr ko jv kp jz kh ki kj kk bi translated">证明了具有高表示能力的非常非常深的网络可以显著提高感知和定量SR性能。该模型仅使用MSE损失，尽管如下图所示实现了很好的感知性能。</li></ul><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kq"><img src="../Images/5576e26b0e479bef0af94bd41eaf6068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DxrkiCarboA0Ivmlp3_mBw.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">RCAN的例子</figcaption></figure><h1 id="ab14" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">模型架构</h1><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lg"><img src="../Images/02a0765210bc1d191e24ace43e7b0b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7_8kbOsUrjw-r5S3Tpuj-g.png"/></div></div></figure><p id="43d8" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">最重要的是，本文提出了一个RIR深度特征提取网络。RIR由G剩余群(RG)和短跳连接组成。然后，每个RG由B个剩余信道注意块(RCAB)组成。作者认为，这种RIR架构允许训练非常深的网络。</p><p id="492a" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">根据ResNet论文，具有跳跃连接的剩余块被堆叠，以实现具有优越性能的可训练的1000+层网络。然而，在SR中，直到这篇文章才出现这种情况，因为它很难因为深入而获得性能增益。多尺度跳跃连接的RIR结构能够训练更深层次的网络。</p><h1 id="7903" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">渠道关注</h1><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es lk"><img src="../Images/bea1b40295dd63afb068474cc9cee1a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*oXVOB1pwv48auR8o6SWqYw.png"/></div></figure><p id="0d32" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">通道自我注意是一种基于图像关注特定通道信息的自我注意机制。本文应用全局平均池和线性层，并在展平的向量上进行最终的sigmoid激活，以生成频道式注意力地图。注意力图通过逐元素乘法来应用。在许多问题中，我们没有使用通道式注意力。但是在SR中，该模型应该与输入大小成比例，这限制了普通自我注意层对引导注意的使用。</p><h1 id="3618" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">剩余信道关注块(RCAB)块</h1><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es ll"><img src="../Images/a3691b7ba8fea97700da6fcc8b4359af.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*v64jOvDVlUzjlOCNEPWKSg.png"/></div></figure><p id="ae7c" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">RCAB模块是模型架构最基本的构建模块。每个RCAB块具有两个由通道注意力引导卷积层。它在RCAB块级别也有跳过连接。因此，在建议的管道中有三个级别的跳过连接:RCAB跳过连接、长跳过连接和短跳过连接。</p><h1 id="635f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lm"><img src="../Images/6067f606746f6c453e3915bec7ecd265.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uIUQ5iSAL-7WM20sXRmubw.png"/></div></div></figure><p id="5918" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">本文比较了长跳连接(LSC)、短跳连接(SSC)和通道注意(CA)的效果。如上表所示，多重跳过连接似乎对模型性能非常有益。频道关注也稍微提高了性能。</p><p id="1ecf" class="pw-post-body-paragraph jc jd hh je b jf kc jh ji jj kd jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">本文最重要的贡献在于，它通过在多个尺度上利用跳跃连接来训练非常深的CNN。</p></div></div>    
</body>
</html>