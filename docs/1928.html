<html>
<head>
<title>Interpretable Machine Learning — A Short Survey</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释的机器学习——一个简短的调查</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/interpretable-machine-learning-a-short-survey-157ba2a56bb8?source=collection_archive---------13-----------------------#2021-03-26">https://medium.com/analytics-vidhya/interpretable-machine-learning-a-short-survey-157ba2a56bb8?source=collection_archive---------13-----------------------#2021-03-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="fb72" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">大家好，今天我要讲的是一篇新的研究论文“<a class="ae jj" href="https://arxiv.org/abs/2010.09337" rel="noopener ugc nofollow" target="_blank"><strong class="in hi"><em class="jk">——可解释机器学习——简史、现状与挑战</em> </strong> </a>”。最近的会议被IML的论文所主导，但是这个领域已经有200年的历史了。最大的挑战是缺少被社区接受的可解释性的严格定义。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es jl"><img src="../Images/602cc868350e262426500aa30817d140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*CxVy3XidGS43KuaJFc2j0w.png"/></div></figure></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><h1 id="092b" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">介绍</h1><p id="122e" class="pw-post-body-paragraph il im hh in b io kr iq ir is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji ha bi translated">在部署ML模型时，行业中最常见的一个问题是该模型是否可信。在我短暂的行业生涯中，我有很多关于模型如何做出决策/预测的问题。一些常见的问题是某个特定特征有多重要，或者为什么模型会做出这种特定的预测。当在安全、客户服务等领域部署特定的ML模型时，可解释性通常是一个重要因素。此外，IML可用于调试或证明模型及其预测，并进一步改进模型。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><h1 id="c3a2" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">历史</h1><p id="f746" class="pw-post-body-paragraph il im hh in b io kr iq ir is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji ha bi translated">随着深度学习的进步，IML的研究达到了顶峰。但是，IML的根非常古老。早在19世纪初，Gauss、Legendre和Quetelet就使用了线性回归模型，并从那时起发展成为大量的回归分析工具，如广义加法模型。这些模型预先做出某些分布假设或限制模型的复杂性，因此强加了模型的内在可解释性。</p><p id="5cac" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">与此相反，ML算法遵循非线性、非参数方法，其中模型复杂性由超参数或通过交叉验证来控制。因此，最大似然模型具有非常好的预测性能，但可解释性差。</p><p id="4cab" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">尽管在ML中的可解释性还没有得到充分的探索，但是在IML已经做了一些杰出的工作。随机森林的内置“特征重要性”度量是IML的重要里程碑之一。最近，许多模型不可知的解释方法被引入，它们适用于不同类型的ML模型。而且还开发了特定于模型的解释方法，例如解释深度神经网络或树集成。回归分析和基于规则的ML至今仍是重要和活跃的研究领域，并正在融合在一起，例如基于模型的树、规则拟合等回归模型和基于规则的ML都可以作为独立的ML算法，也可以作为IML方法的构建模块。</p></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><h1 id="4487" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">IML模型</h1><p id="5d21" class="pw-post-body-paragraph il im hh in b io kr iq ir is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji ha bi translated">IML模型的区别在于它们是否分析模型组件、模型敏感度或代理模型。一些IML方法通过给单个模型组件赋予意义来工作(左)，一些方法通过分析模型预测的数据扰动来工作(右)。替代方法是其他两种方法的混合，使用(扰动的)数据近似ML模型，然后分析可解释的替代模型的组成部分。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es kw"><img src="../Images/7ecf1d5eeb029f8ba23914c6e03a3836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ave20yspgZzMYr9w4iib5Q.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx translated">莫尔纳尔等人《<a class="ae jj" href="https://arxiv.org/abs/2010.09337" rel="noopener ugc nofollow" target="_blank"> <strong class="bd jv"> <em class="lf">可解释机器学习——简史、现状与挑战</em> </strong> </a>》</figcaption></figure><h2 id="e8ae" class="lg ju hh bd jv lh li lj jz lk ll lm kd iw ln lo kh ja lp lq kl je lr ls kp lt bi translated">(1)分析可解释模型的组成部分:</h2><p id="c68b" class="pw-post-body-paragraph il im hh in b io kr iq ir is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji ha bi translated">在这些方法中，我们关注模型的单个组件，而不是整个模型。因此，不一定需要理解整个模型，但是为了分析模型的特定组件，需要将它分解成我们可以单独解释的部分。可解释模型是具有学习到的参数和学习到的结构的模型，它们可以被赋予某种解释，就像线性回归、决策树等。例如，线性回归模型的权重可以解释为单个特征对模型预测的影响。决策树有一个学习结构，其中每个节点都有一个基于特定特征的划分。这有助于我们通过跟踪节点来追踪决策树做出的预测。但是，这仅在高维情况下的某一点上有效。具有大量特征的线性回归模型不再具有可解释性，因此一些方法试图减少要解释的特征，例如LASSO。</p><div class="lu lv ez fb lw lx"><a href="http://statweb.stanford.edu/~tibs/lasso.html" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">套索页面</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">套索页面L1约束拟合的统计和数据挖掘套索是一个收缩和选择的方法…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">statweb.stanford.edu</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml jr lx"/></div></div></a></div><h2 id="b742" class="lg ju hh bd jv lh li lj jz lk ll lm kd iw ln lo kh ja lp lq kl je lr ls kp lt bi translated">(2)分析更复杂模型的组件:</h2><p id="3da6" class="pw-post-body-paragraph il im hh in b io kr iq ir is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji ha bi translated">我们还可以分析复杂模型的组件。例如，我们可以将CNN层学习到的特征可视化。</p><div class="lu lv ez fb lw lx"><a href="https://distill.pub/2017/feature-visualization/" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">特征可视化</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">越来越多的人意识到神经网络需要能够被人类理解。神经网络领域…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">蒸馏. pub</p></div></div><div class="mg l"><div class="mm l mi mj mk mg ml jr lx"/></div></div></a></div><p id="db38" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">一些方法试图通过引入一些单调性约束或修改的损失函数来解开CNN学到的概念，从而使组件更容易解释。</p><h2 id="5b8a" class="lg ju hh bd jv lh li lj jz lk ll lm kd iw ln lo kh ja lp lq kl je lr ls kp lt bi translated">(3)解释个人预测:</h2><p id="2623" class="pw-post-body-paragraph il im hh in b io kr iq ir is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji ha bi translated">大多数用于研究最大似然模型敏感性的方法都是模型不可知的。他们通过分析输入数据中微小扰动的模型预测来工作。我们区分本地和全球的解释。局部方法侧重于个体模型预测。一种流行的本地IML方法是Shapley值。</p><div class="lu lv ez fb lw lx"><a href="https://christophm.github.io/interpretable-ml-book/shapley.html" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">5.9 Shapley值|可解释的机器学习</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">预测可以通过假设实例的每个特征值是游戏中的“玩家”来解释，其中…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">christophm.github.io</p></div></div><div class="mg l"><div class="mn l mi mj mk mg ml jr lx"/></div></div></a></div><p id="4493" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">一些IML方法依赖于特定于模型的知识来分析输入特征的变化如何影响输出。例如，显著图用于CNN。显著性图产生显示改变像素如何改变预测的热图。</p><div class="lu lv ez fb lw lx"><a rel="noopener follow" target="_blank" href="/@ODSC/visualizing-your-convolutional-neural-network-predictions-with-saliency-maps-9604eb03d766"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">使用显著图可视化您的卷积神经网络预测</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">在许多情况下，理解为什么模型预测一个给定的结果是一个关键的细节，模型用户和必要的…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">medium.com</p></div></div><div class="mg l"><div class="mo l mi mj mk mg ml jr lx"/></div></div></a></div><h2 id="4e79" class="lg ju hh bd jv lh li lj jz lk ll lm kd iw ln lo kh ja lp lq kl je lr ls kp lt bi translated">(4)解释全局模型行为:</h2><p id="9fb2" class="pw-post-body-paragraph il im hh in b io kr iq ir is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji ha bi translated">全局模型解释方法用于解释模型对于给定的特定数据集的平均表现。全局解释的一个有用的区别是“特征重要性”和“特征效应”。</p><p id="80ed" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">要素重要性根据要素对预测的重要性/相关性对其进行排序，例如随机森林的置换要素重要性。</p><div class="lu lv ez fb lw lx"><a href="https://christophm.github.io/interpretable-ml-book/feature-importance.html" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">5.5排列特征重要性|可解释的机器学习</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">置换特征重要性度量了置换后模型预测误差的增加</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">christophm.github.io</p></div></div><div class="mg l"><div class="mp l mi mj mk mg ml jr lx"/></div></div></a></div><p id="04b3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">“特征效应”显示了输入特征的变化如何改变预测的结果。流行的特征效应图是部分相关图、个体条件期望曲线、累积局部效应图和函数方差分析。</p><div class="lu lv ez fb lw lx"><a href="https://christophm.github.io/interpretable-ml-book/pdp.html" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">5.1部分相关图(PDP) |可解释的机器学习</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">部分相关性图(短PDP或PD图)显示了一个或两个特征对预测结果的边际影响</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">christophm.github.io</p></div></div><div class="mg l"><div class="mq l mi mj mk mg ml jr lx"/></div></div></a></div><div class="lu lv ez fb lw lx"><a href="https://christophm.github.io/interpretable-ml-book/ale.html" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">5.3累积局部效应(ALE)图|可解释的机器学习</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">累积的局部效应描述了特征如何平均影响机器学习模型的预测。麦芽酒…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">christophm.github.io</p></div></div><div class="mg l"><div class="mr l mi mj mk mg ml jr lx"/></div></div></a></div><h2 id="83a0" class="lg ju hh bd jv lh li lj jz lk ll lm kd iw ln lo kh ja lp lq kl je lr ls kp lt bi translated">(5)替代模型:</h2><p id="39fb" class="pw-post-body-paragraph il im hh in b io kr iq ir is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji ha bi translated">代理模型是试图模仿相关ML模型行为的可解释模型。代理方法仅需要ML模型的输入和输出数据来训练代理ML模型。他们把ML模型当作一个黑箱。LIME是局部替代方法的一个示例，它通过学习与待解释数据点接近的数据的可解释模型来解释单个预测。</p><div class="lu lv ez fb lw lx"><a href="https://christophm.github.io/interpretable-ml-book/lime.html" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">5.7本地代理(LIME) |可解释的机器学习</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">局部代理模型是可解释的模型，用于解释黑箱机器的个别预测…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">christophm.github.io</p></div></div><div class="mg l"><div class="ms l mi mj mk mg ml jr lx"/></div></div></a></div></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><h1 id="d4d7" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">挑战</h1><p id="12b6" class="pw-post-body-paragraph il im hh in b io kr iq ir is ks iu iv iw kt iy iz ja ku jc jd je kv jg jh ji ha bi translated">IML领域的一些挑战是:</p><p id="e13d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">(1)缺乏对可解释性的适当定义。</p><p id="167f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">(2)特征依赖引入了重要性归属和外推的问题。</p><p id="3c60" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">(3)许多IML方法提供了解释，但没有量化解释的不确定性。模型本身及其解释都是根据数据计算出来的，因此具有不确定性。</p><p id="5b8d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">(4)如论文中所述，“理想情况下，模型应反映其潜在现象的真实因果结构，以便能够进行因果解释。可以说，如果在科学中使用ML，因果解释通常是建模的目标。但是大多数统计学习过程仅仅反映特征之间的相关结构，并且分析数据生成过程的表面，而不是其真实的内在结构。需要进一步的研究来了解我们何时被允许对一个ML模型进行因果解释。”</p></div></div>    
</body>
</html>