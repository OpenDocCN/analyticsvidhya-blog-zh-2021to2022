# æ–‡æœ¬åˆ†ç±»â€”ä»è¯è¢‹åˆ° BERT â€”ç¬¬ 3 éƒ¨åˆ†(å¿«é€Ÿæ–‡æœ¬)

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-3-fasttext-8313e7a14fce?source=collection_archive---------3----------------------->

![](img/78e35b6cfda2995bfc40717357daf631.png)

æ ¼ä¼¦Â·æƒ å‹’åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) æ‹æ‘„çš„ç…§ç‰‡

è¿™ä¸ªæ•…äº‹æ˜¯ä¸€ç³»åˆ—æ–‡æœ¬åˆ†ç±»çš„ä¸€éƒ¨åˆ†â€”â€”ä»è¯è¢‹åˆ° BERT åœ¨ Kaggle ç«èµ›ä¸Šå®æ–½å¤šç§æ–¹æ³•ï¼Œç”± Jigsaw(Alphabet çš„å­å…¬å¸) ***å‘½åä¸ºâ€œ [*ã€æœ‰æ¯’è¯„è®ºåˆ†ç±»æŒ‘æˆ˜ã€‘*](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) â€ã€‚*** åœ¨è¿™åœºæ¯”èµ›ä¸­ï¼Œæˆ‘ä»¬é¢ä¸´çš„æŒ‘æˆ˜æ˜¯å»ºç«‹ä¸€ä¸ªå¤šå¤´æ¨¡å‹ï¼Œèƒ½å¤Ÿæ£€æµ‹ä¸åŒç±»å‹çš„æ¯’æ€§ï¼Œå¦‚*å¨èƒã€æ·«ç§½ã€ä¾®è¾±å’ŒåŸºäºèº«ä»½çš„ä»‡æ¨ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰æŸ¥çœ‹ä¹‹å‰çš„æ•…äº‹ï¼Œä¸€å®šè¦æŸ¥çœ‹ä¸€ä¸‹ï¼Œå› ä¸ºè¿™å°†æœ‰åŠ©äºç†è§£æœªæ¥çš„äº‹æƒ…ã€‚*

[ç¬¬ä¸€éƒ¨åˆ†(BagOfWords)](https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9)

[ç¬¬äºŒéƒ¨åˆ†(Word2Vec)](https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-2-word2vec-35c8c3b34ee3)

åœ¨æ—©æœŸçš„æ•…äº‹ä¸­([ç¬¬ 2 éƒ¨åˆ†(Word2Vec)](https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-part-2-word2vec-35c8c3b34ee3) )ï¼Œæˆ‘ä»¬ä½¿ç”¨ Gensim ä¸ºå¥å­ä¸­ä½¿ç”¨çš„å•è¯è·å¾—é¢„è®­ç»ƒçš„ Word2Vec æ¨¡å‹/åµŒå…¥å‘é‡ï¼Œå°†å®ƒä»¬æ˜ å°„åˆ°è¾“å‡ºå˜é‡ toxicã€severe_toxicã€æ·«ç§½ã€å¨èƒã€ä¾®è¾±ã€identity_hateï¼Œå¹¶ä½¿ç”¨ sklearn çš„å¤šè¾“å‡ºé€»è¾‘å›å½’åˆ†ç±»å™¨åŒ…è£…å™¨ä¸ºæ‰€æœ‰ 6 ä¸ªè¾“å‡ºå˜é‡åˆ›å»ºé€»è¾‘å›å½’æ¨¡å‹ã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ fastText åº“æ¥ç”Ÿæˆå¥å­çš„åµŒå…¥å’Œæ–‡æœ¬åˆ†ç±»ã€‚äº‹å®ä¸Šï¼Œè¿™ç»™äº†æˆ‘ä»¬ä¸€ä¸ªä¸€æ°”å‘µæˆçš„é€‰æ‹©ã€‚ç›´è§‰éƒ¨åˆ†å°†æ¯”å…¶ä»–æ–¹æ³•ç®€çŸ­ï¼Œå› ä¸ºå¿«é€Ÿæ–‡æœ¬çš„æ–‡æ¡£/é˜…è¯»ææ–™æœ‰é™ã€‚

***ä»€ä¹ˆæ˜¯ fastTextï¼Ÿ***

![](img/d599cfe55c78919d02e940a4886ae32e.png)

2016 å¹´ï¼Œè„¸ä¹¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢(FAIR)å¼€æºäº† fastTextï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¸®åŠ©æ„å»ºæ–‡æœ¬è¡¨ç¤ºå’Œåˆ†ç±»çš„å¯æ‰©å±•è§£å†³æ–¹æ¡ˆçš„åº“ã€‚fastText å°† Word2Vec ä¸­å•è¯åµŒå…¥çš„æ€æƒ³å‘å‰æ¨è¿›äº†ä¸€æ­¥ï¼Œå­¦ä¹ äº†å­—ç¬¦ n å…ƒè¯­æ³•çš„è¡¨ç¤ºï¼Œå¹¶å°†å•è¯è¡¨ç¤ºä¸º n å…ƒè¯­æ³•å‘é‡çš„æ€»å’Œã€‚ä»¥å•è¯â€œwhereâ€å’Œ n = 3 ä¸ºä¾‹ï¼Œç”¨å­—ç¬¦ n-grams è¡¨ç¤º:<wh whe="" her="" ere="" re="">ã€‚é™¤äº†å­è¯åµŒå…¥å½¢å¼çš„æ–‡æœ¬è¡¨ç¤ºä¹‹å¤–ï¼Œå®ƒè¿˜æä¾›äº†ç°æˆçš„åˆ†ç±»æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è¢«ä¼˜åŒ–ä»¥ä¸è¿™äº›åµŒå…¥ä¸€èµ·å·¥ä½œå¹¶ç»™å‡ºå¿«é€Ÿç»“æœã€‚</wh>

***ä¸ºä»€ä¹ˆéœ€è¦ fastTextï¼Ÿ***

ä¸å¸¸è§„çš„ word2vec åµŒå…¥ç›¸æ¯”ï¼ŒfastText æœ‰ä¸¤ä¸ªä¸»è¦ä¼˜ç‚¹:

1.  ***Word2Vec é¢ä¸´è¯æ±‡é‡ä¸è¶³çš„é—®é¢˜(OOV)*** å‡è®¾æˆ‘ä»¬ä»é›¶å¼€å§‹è®­ç»ƒä¸€ä¸ª Word2Vec æ¨¡å‹ï¼Œæˆ‘ä»¬å»ºç«‹ä¸€ä¸ªåŒ…å«è®­ç»ƒæ•°æ®ä¸­æ‰€æœ‰å•è¯çš„è¯æ±‡è¡¨ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬åœ¨æµ‹è¯•æ•°æ®ä¸­æœ‰ä¸€ä¸ªå¯èƒ½éœ€è¦åµŒå…¥çš„æ–°å•è¯ï¼Œæ–°çš„ç¼ºå¤±å•è¯å°†æ˜¯ OOVã€‚åœ¨ word2vec ä¸­æˆ‘ä»¬å®Œå…¨å¿½ç•¥äº†è¿™æ ·çš„è¯ã€‚é€šè¿‡åœ¨ fastText ä¸­ä½¿ç”¨å­å•è¯åµŒå…¥ï¼Œæˆ‘ä»¬è¯•å›¾ä¸ºä¸€ä¸ªä¹Ÿæ˜¯ OOV çš„å•è¯è·å¾—åµŒå…¥
2.  ***é€šè¿‡å¯¹æ¯ä¸ªå•è¯ä½¿ç”¨ä¸åŒçš„å‘é‡è¡¨ç¤ºï¼ŒWord2Vec æ¨¡å‹å¿½ç•¥äº†å•è¯çš„å†…éƒ¨ç»“æ„:*** åœ¨ word2vec ä¸­ï¼Œæ¯ä¸ªå•è¯éƒ½æ˜¯åŸºäºå®ƒå‡ºç°çš„ä¸Šä¸‹æ–‡å”¯ä¸€å­¦ä¹ çš„ã€‚ä¾‹å¦‚ï¼Œboxer å’Œ boxing åœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨ï¼Œæˆ‘ä»¬æ— æ³•æ•æ‰æ½œåœ¨çš„ç›¸ä¼¼æ€§ã€‚å°†å…¶åˆ†è§£ä¸ºå­—ç¬¦ n-gram ä¼šæœ‰æ‰€å¸®åŠ©

***fast text æœ‰ä»€ä¹ˆé¢å¤–çš„å¥½å¤„ï¼Ÿ***

è™½ç„¶ fastText è¶…è¶Šäº†å•è¯çº§åˆ«ï¼Œè¾¾åˆ°äº†å­—ç¬¦-n-gram çº§åˆ«ï¼Œä½†æ˜¯å®ƒéå¸¸å¿«(å› æ­¤å¾—å)ã€‚å®éªŒè¡¨æ˜ï¼ŒfastText åœ¨å‡†ç¡®æ€§æ–¹é¢é€šå¸¸ä¸æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨ä¸ç›¸ä¸Šä¸‹ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒå’Œè¯„ä¼°æ–¹é¢å¿«äº†è®¸å¤šæ•°é‡çº§ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ ‡å‡†çš„å¤šæ ¸ CPU åœ¨ä¸åˆ°ååˆ†é’Ÿçš„æ—¶é—´å†…å¯¹è¶…è¿‡ 10 äº¿ä¸ªå•è¯è¿›è¡Œ fastText è®­ç»ƒï¼Œå¹¶åœ¨ä¸åˆ°ä¸€åˆ†é’Ÿçš„æ—¶é—´å†…å¯¹ 312K ä¸ªç±»åˆ«ä¸­çš„ 50 ä¸‡ä¸ªå¥å­è¿›è¡Œåˆ†ç±»ã€‚

æ­¤å¤–ï¼ŒfastText æä¾›äº†åœ¨ç»´åŸºç™¾ç§‘å’Œ Crawl ä¸Šè®­ç»ƒçš„ 157 ç§è¯­è¨€çš„è¯å‘é‡(è¿™å¤ªä»¤äººæƒŠè®¶äº†)ã€‚

***fast text æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ***

1.  ***åˆ›å»ºå•è¯åµŒå…¥:*** å­å•è¯æ¨¡å‹åŸºäºæ¥è‡ª Word2Vec çš„ skip-gram æ¨¡å‹ï¼Œå¹¶ä¸”ä¸ä½¿ç”¨å•è¯çš„çŸ¢é‡è¡¨ç¤ºï¼Œè€Œæ˜¯ä½¿ç”¨å­—ç¬¦ n å…ƒè¯­æ³•çš„çŸ¢é‡è¡¨ç¤ºçš„å¹³å‡å€¼ã€‚å…¶ä»–ä¸€åˆ‡éƒ½ä¸è·³æ ¼æ¨¡å‹éå¸¸ç›¸ä¼¼ã€‚
2.  ***æ–‡æœ¬åˆ†ç±»:*** ä¸‹å›¾æ‘˜è‡ªå®é™…è®ºæ–‡ã€Šé«˜æ•ˆæ–‡æœ¬åˆ†ç±»é”¦å›Šã€‹ï¼Œå…¶ä¸­ä»‹ç»äº†ç”¨äºåˆ†ç±»çš„ fastTextã€‚fastText ä½¿ç”¨ç±»ä¼¼äº Word2Vec ç½‘ç»œçš„æµ…å±‚ç¥ç»ç½‘ç»œã€‚

![](img/1971e55e25940c171ebd0cfccfc79ce5.png)

æˆ‘ä»¬ä½¿ç”¨ softmax å‡½æ•° f æ¥è®¡ç®—é¢„å®šä¹‰ç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒã€‚äº‹å®ä¸Šï¼Œå®ƒä½¿ç”¨äº†ä¸€ç§åŸºäºéœå¤«æ›¼ç¼–ç æ ‘çš„ç§°ä¸ºåˆ†å±‚ softmax çš„ä¸œè¥¿(ç®€è€Œè¨€ä¹‹ï¼Œæœ€å¸¸è§çš„å•è¯/å­—æ¯è¢«èµ‹äºˆæœ€å°çš„ä»£ç )ã€‚å› æ­¤ï¼Œåœ¨å±‚æ¬¡ softmax ä¸­ï¼ŒèŠ‚ç‚¹çš„æ¦‚ç‡æ€»æ˜¯ä½äºå…¶çˆ¶èŠ‚ç‚¹çš„æ¦‚ç‡ã€‚å½“æˆ‘ä»¬æœ‰å¤§é‡çš„ç±»æ—¶ï¼Œä»¥åŠåœ¨æˆ‘ä»¬æœç´¢æœ€å¯èƒ½çš„ç±»æ—¶ï¼Œè¿™éƒ½æ˜¯æœ‰å¸®åŠ©çš„ã€‚

![](img/9d6a135fbc9673bdaacd82c41274fd48.png)

ä¾‹å¦‚ï¼Œå¦‚æœæ—…è¡Œã€é£Ÿç‰©å’Œå°åº¦ç¾é£Ÿæ˜¯ 3 ä¸ªç±»åˆ«ï¼Œå¦‚æœæ—…è¡Œçš„æ¦‚ç‡æ›´é«˜ï¼Œæˆ‘ä»¬ç”šè‡³ä¸éœ€è¦è®¡ç®—é£Ÿç‰©å’Œå°åº¦ç¾é£Ÿçš„æ¦‚ç‡ã€‚è¿™é™ä½äº†å¤æ‚æ€§ã€‚

æ­¤å¤–ï¼Œå®ƒè¿˜ä½¿ç”¨å•è¯ n å…ƒè¯­æ³•ä½œä¸ºé™¤åµŒå…¥ä¹‹å¤–çš„é™„åŠ ç‰¹å¾æ¥æ•è·å…³äºå±€éƒ¨è¯åºçš„ä¸€äº›éƒ¨åˆ†ä¿¡æ¯ï¼Œå¦åˆ™åœ¨æ­£å¸¸çš„ BagOfWords ä¸­æ•è·è¿™äº›ä¿¡æ¯åœ¨è®¡ç®—ä¸Šæ˜¯éå¸¸æ˜‚è´µçš„ã€‚

å¯¹äºå¯¹å®Œæ•´ä»£ç æ„Ÿå…´è¶£çš„äººï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°å®ƒ[ã€‚é‚£ä¹ˆè®©æˆ‘ä»¬æ·±å…¥ç ”ç©¶ä»£ç ğŸ‘¨â€ğŸ’»](https://www.kaggle.com/anirbansen3027/jtcc-fasttext-supervised)

> **å®ç°**

1.  ***è¯»å–æ•°æ®é›†***

![](img/0543de08e345f6b77adbe68a2a0922ba.png)

æé†’ä¸€ä¸‹ï¼Œè¿™æ˜¯è®­ç»ƒæ•°æ®çš„æ ·å­

***4ã€‚åŸºæœ¬é¢„å¤„ç†***

å°±é¢„å¤„ç†è€Œè¨€ï¼Œæˆ‘ä»¬æ­£åœ¨æ‰§è¡Œä»¥ä¸‹æ­¥éª¤:

1.  æˆ‘ä»¬ä¸ºä¸€äº›æ ‡ç‚¹ç¬¦å·å¼•å…¥äº†ç©ºæ ¼ï¼Œæ¯”å¦‚ï¼Ÿ, ., ), (, !å¹¶åˆ é™¤äº†ä¸€äº›åªæ˜¯ä¸ºäº†ä½¿æ–‡å­—æ›´æ¸…æ™°
2.  æˆ‘ä»¬å·²ç»åˆ é™¤äº†â€œ\n â€,å› ä¸ºæˆ‘ä»¬å·²ç»å¯ä»¥åœ¨æ–‡æœ¬ä¸­çœ‹åˆ°å¾ˆå¤š
3.  æˆ‘ä»¬åšäº†ä¸€äº› Unicode æ ‡å‡†åŒ–æ¥å¤„ç†ä¸€äº›å¯èƒ½å‡ºç°çš„ Unicode é—®é¢˜
4.  è¿™ä¸ªæ˜¯æœ€é‡è¦çš„ï¼Œå°†æ‰€æœ‰è¾“å‡ºå˜é‡çš„äºŒè¿›åˆ¶æ ‡ç­¾ä» 0 å’Œ 1 è½¬æ¢ä¸º _ _ class _ _ 0 å’Œ __class__1ï¼Œå› ä¸º fastText åˆ†ç±»å™¨éœ€è¦è¿™æ ·åšã€‚æˆ‘ä»¬åªéœ€è¦å¯¹ training_data è¿™æ ·åšï¼Œå› ä¸ºåˆ†ç±»å™¨ä¸ä¼šæŸ¥çœ‹éªŒè¯æ•°æ®/æµ‹è¯•æ•°æ®çš„çœŸå®æ ‡ç­¾
5.  æ‰“ä¹±æ•°æ®é›†ä»¥å¼•å…¥ä¸€äº›éšæœºæ€§å¹¶æ¶ˆé™¤æœ‰åºæ€§(å¦‚æœå­˜åœ¨çš„è¯)

```
# Lets do some cleaning of this text
def clean_it(text,normalize=True):
    # Replacing possible issues with data. We can add or reduce the replacemtent in this chain
    s = str(text).replace(',',' ').replace('"','').replace('\'',' \' ').replace('.',' . ').replace('(',' ( ').\
            replace(')',' ) ').replace('!',' ! ').replace('?',' ? ').replace(':',' ').replace(';',' ').lower()
    s = s.replace("\n"," ")

    # normalizing / encoding the text
    if normalize:
        s = s.normalize('NFKD').str.encode('ascii','ignore').str.decode('utf-8')
    return s# Now lets define a small function where we can use above cleaning on datasets
def clean_df(data, cleanit= False, shuffleit=False, encodeit=False, label_prefix='__class__'):
    # Defining the new data
    df = data[['comment_text']].copy(deep=True)
    for col in y_cols:
        df[col] = label_prefix + data[col].astype(str) + ' '
    # cleaning it
    if cleanit:
        df['comment_text'] = df['comment_text'].apply(lambda x: clean_it(x,encodeit))
    # shuffling it
    if shuffleit:
        df.sample(frac=1).reset_index(drop=True)
    return df# Transform the datasets using the above clean functions
df_train_cleaned = clean_df(train, True, True)
df_val_cleaned = clean_df(val, True, True, label_prefix='')
```

![](img/103f1a7393c85b6ef649845feae66628.png)

å› æ­¤ï¼Œå¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å·²ç»æ›´æ”¹äº†è®­ç»ƒæ•°æ®é›†çš„æ ‡ç­¾ç±»ï¼Œä½†æ²¡æœ‰æ›´æ”¹éªŒè¯é›†çš„æ ‡ç­¾ç±»ï¼Œå› ä¸º fastText åˆ†ç±»å™¨ä¸ä¼šæŸ¥çœ‹å®ƒä»¬

***5ã€‚è®­ç»ƒå’ŒéªŒè¯å¿«é€Ÿæ–‡æœ¬åˆ†ç±»å™¨***

*   ç”±äº fastText åˆ†ç±»å™¨è¾“å…¥ä¸€ä¸ªå¸¦æœ‰æ–‡æœ¬æ•°æ®å’Œç±»æ ‡ç­¾çš„ CSV æ–‡ä»¶ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨åœ¨æ—©æœŸç¬”è®°æœ¬ä¸­ä½¿ç”¨çš„å¤šè¾“å‡ºåˆ†ç±»å™¨åŒ…è£…å™¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»è¿è¡Œ for å¾ªç¯æ¥ä¸ºæ¯ä¸ªè¾“å‡ºå˜é‡è®­ç»ƒå•ç‹¬çš„æ¨¡å‹ï¼Œå¹¶å°†æ¯ä¸ªè¾“å‡ºå˜é‡çš„é¢„æµ‹å­˜å‚¨åœ¨éªŒè¯é›†ä¸­ã€‚
*   train_supervised æ˜¯ç”¨äºå¿«é€Ÿæ–‡æœ¬åˆ†ç±»çš„å‡½æ•°ã€‚æˆ‘ä»¬å¯ä»¥è°ƒæ•´å­¦ä¹ å‚æ•°æ¥æ”¹è¿›æ¨¡å‹ã€‚
*   åˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¿˜æ²¡æœ‰ API å¯ä»¥æ¥å—ä¸€ä¸ªéªŒè¯é›†ï¼Œå¹¶ç»™å‡ºè‚¯å®šæƒ…å†µçš„æ¦‚ç‡ã€‚æˆ‘ä»¬ä¸€æ¬¡åªèƒ½å¾—åˆ°ä¸€ä¸ªå¥å­çš„æ¦‚ç‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªéªŒè¯è¯­å¥è¿è¡Œä¸€ä¸ª for å¾ªç¯ï¼Œå¹¶å°†æ¦‚ç‡å­˜å‚¨åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­ã€‚æˆ‘ä»¬éœ€è¦æ¦‚ç‡ï¼Œå› ä¸ºæ€§èƒ½æŒ‡æ ‡æ˜¯ ROC-AUC

```
#Will contain all the predictions for validation set for all the output variables
all_preds = []
#Iterating over all output variables to create separate models
for col in tqdm(y_cols):
    #Path for saving the training dataset
    train_file = '/kaggle/working/final_train.csv'
    #Saving the Output Variable and the text data to a csv
    df_train_cleaned[[col, "comment_text"]].to_csv(train_file, header=None, index=False, columns=[col, "comment_text"]) 
    #Training the model
    model = train_supervised(input=train_file, label="__class__", lr=1.0, epoch=2, loss='ova', wordNgrams=2, dim=200, thread=2, verbose=100)
    #Predictions for validation sets for that ouput variable
    col_preds = []
    #Iterating over each sentence in the validation set
    for text in df_val_cleaned["comment_text"].values:
        #Get the prediction for class 1
        pred = model.predict(text, k = 2)[1][1]
        #Append the prediction to the list of predictions for that output variable
        col_preds.append(pred)
    #Append the list of predictions for a output variable to the overall set of predictions for all columns
    all_preds.append(col_preds)
```

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»ä»éªŒè¯é›†çš„æ¨¡å‹ä¸­è·å¾—äº†é¢„æµ‹ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ç»“æœ/æ€§èƒ½ã€‚ç”±äºç«äº‰ä½¿ç”¨å¹³å‡ ROC-AUC ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œæˆ‘ä»¬å°†åœ¨ç¬”è®°æœ¬ä¸­ä½¿ç”¨ç›¸åŒçš„æŒ‡æ ‡ã€‚

```
#Function for calculating roc auc with given actual binary values across target variables and the probability score made by the model
def accuracy(y_test, y_pred):
    aucs = []
    #Calculate the ROC-AUC for each of the target column
    for col in range(y_test.shape[1]):
        aucs.append(roc_auc_score(y_test[:,col],y_pred[:,col]))
    return aucs#Actual Labels
y_val_actuals = df_val_cleaned[y_cols].astype("int").to_numpy()
#Prediction probability - minor ordering
all_preds_array = np.transpose(np.array(all_preds))
#Calculate the mean of the ROC-AUC for each of the ouput variable
mean_auc = mean(accuracy(y_val_actuals,all_preds_array))
```

éªŒè¯é›†ä¸Šçš„ ROC-AUC è¶‹å‘äº 0.77 å·¦å³ï¼Œè¿™æ¯” Word2Vec æ¨¡å‹å¥½å¾—å¤šã€‚æ­¤å¤–ï¼Œè¿™äº›åªæ˜¯ 2 ä¸ªæ—¶æœŸå’Œæœªè°ƒæ•´å‚æ•°çš„æ—©æœŸç»“æœã€‚æˆ‘è®¤ä¸ºé€šè¿‡æ›´å¥½çš„è°ƒæ•´ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚å­¦ä¹ å’Œé¢„æµ‹ä¹Ÿéå¸¸å¿«ã€‚

7 .**ã€‚æ”¹è¿›çš„ç»“æœå’ŒèŒƒå›´**

![](img/69bce93a8b13f92bee1d104bbfa5fe10.png)

Kaggle æ’è¡Œæ¦œåˆ†æ•°

*æ‰˜å¤šæ–¯:*

1.  å¯ä»¥åšæ›´å¥½çš„æ–‡æœ¬é¢„å¤„ç†ã€æ‰“å­—é”™è¯¯çº æ­£ç­‰æ¥è¿›ä¸€æ­¥æ”¹è¿›æ¨¡å‹
2.  å°è¯•è°ƒæ•´è¶…å‚æ•°ä»¥è·å¾—æ›´å¥½çš„ç»“æœ

è¿™æ˜¯å…³äºå¿«é€Ÿæ–‡æœ¬çš„ã€‚ä¸‹ä¸€ä¸ªå°†æ˜¯å…³äºå·ç§¯ç¥ç»ç½‘ç»œ(CNN)ã€‚ç»†èƒç¥ç»ç½‘ç»œé€šå¸¸ç”¨äºè®¡ç®—æœºè§†è§‰ï¼Œç„¶è€Œï¼Œå®ƒä»¬å·²ç»åº”ç”¨äºå„ç§ NLP ä»»åŠ¡æœ‰ä¸€æ®µæ—¶é—´äº†ï¼Œç»“æœæ˜¯æœ‰å¸Œæœ›çš„ã€‚åœ¨é‚£ä¹‹å‰ä¿æŒå®‰å…¨ã€‚åŒæ ·ï¼Œæ•´ä¸ªä»£ç å‘ˆç°åœ¨[(è¿™é‡Œ)](https://www.kaggle.com/anirbansen3027/jtcc-fasttext-supervised)ã€‚è¯·ä»¥å›ç­”å’Œé¼“æŒçš„å½¢å¼æä¾›æ‚¨çš„åé¦ˆ:)