<html>
<head>
<title>Training high-quality models without DensePose markup</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在没有密集标记的情况下训练高质量模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/training-high-quality-models-without-densepose-markup-90476757a3da?source=collection_archive---------14-----------------------#2021-08-01">https://medium.com/analytics-vidhya/training-high-quality-models-without-densepose-markup-90476757a3da?source=collection_archive---------14-----------------------#2021-08-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="d1b3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有没有可能在没有相应的DensePose标记的情况下，训练一个高质量的模型，从一张照片预测动物体表的3D坐标？答案是肯定的！</p><p id="26a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">来自脸书AI的研究人员在CVPR 2020大会上介绍了这个问题。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/90040be2add72b6d818a33f67f975bb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HK0THvsjTLgVfwLV"/></div></div></figure><h1 id="9923" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">关于密集任务</h1><p id="ec7b" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">早些时候，这些研究人员向科学界展示了一个新的DensePose-COCO数据集和一个处理这些数据的神经网络架构(<a class="ae kr" href="https://arxiv.org/abs/1802.00434" rel="noopener ugc nofollow" target="_blank">文章</a>)。该数据集由来自COCO 2014的图像中的人的特别组装的标记组成。</p><h1 id="9077" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">关于数据集的更多信息</h1><p id="5150" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">数据包括:</p><ul class=""><li id="d82f" class="ks kt hh ig b ih ii il im ip ku it kv ix kw jb kx ky kz la bi translated">照片中人物的边框，</li><li id="f034" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">像素完美的前景-背景遮罩，</li><li id="8146" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">32x身体部位的分割，在上面的面具里面，</li><li id="ae69" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">以及每张照片的一大组三元组<strong class="ig hi"> <em class="lg"> (c，u，v) </em> </strong>，其中<strong class="ig hi"> <em class="lg"> c </em> </strong>是身体部分的索引，<strong class="ig hi"> <em class="lg"> u，v </em> </strong>是身体部分内的测地线坐标。</li></ul><p id="09ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它们组装如下:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lh"><img src="../Images/81144f671ed90c7531d5dd6c2feb0e81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/0*o-UAlSD9UlxfihfC"/></div></figure><p id="6093" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">任务是确定人类图像上随机投掷的点(在分割掩模内)和3D人类模型的六个预渲染图像上的点之间的对应关系，即SMPL模型在不同角度的六个2D投影上的点。得到这个标记后，研究人员恢复了人体表面这些点的<strong class="ig hi"> <em class="lg"> (c，u，v) </em> </strong>坐标。</p><p id="7bb5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据这个原理，COCO 2014数据集内的<strong class="ig hi"> 5万张</strong>人物图片，收集了<strong class="ig hi">500万个</strong>点。</p><p id="8471" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种数据允许我们建立一个类似Mask-RCNN的模型，用于从图像中预测具有3D坐标的掩模。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es li"><img src="../Images/cfbb1a8accc8fa0c437911a1677ccd71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/0*i5W10-jGUJU6VdAe"/></div></figure><p id="cff9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个决定值得注意的是后续的着装。由于神经网络允许你得到一个人的模型，你可以在这个模型上放一个衣服的模型。下面的例子:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lj"><img src="../Images/cab0325c1c1274d533e2890223aaf191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EZlJWG1RexmF0RH0"/></div></div></figure><p id="84d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">视频格式的可视化可以在<a class="ae kr" href="https://www.youtube.com/watch?v=Dhkd_bAwwMc" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="2413" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">现在说说猴子</h1><p id="075c" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">在这篇新论文中，研究人员建议使用DensePose-COCO和COCO数据集的标记来解决预测动物表面三维坐标的类似问题。然而，建议<em class="lg">不要根据上述算法标记动物的图像</em>。有人认为，你不仅可以将现有的标记用于黑猩猩，还可以用于任何其他在解剖学上与人类不太相似的动物。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lk"><img src="../Images/a8e0d318ae748c44a8fc0ad2dda8da53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*URH5VbDyJhHKoCNL"/></div></div></figure><p id="9411" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过这种方式，研究人员成功实现了公制密度平均精度等于34.9的质量。在标记了一类人的数据后，第一篇文章中的模型显示结果= 46.8，考虑了度量从0到100变化的事实。知识转移的好结果？</p><p id="50e7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了衡量新模型的质量，有必要标记一定数量的黑猩猩照片(就像对人类做的一样)。为此，提出了一种方法来恢复人类模型的SMPL点和非常详细的艺术3D黑猩猩模型之间的对应关系。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ll"><img src="../Images/76c84dc8872ce5811f1b72c508bfabc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Xnn0Z9ggXFm1y5Vn"/></div></div></figure><p id="9d67" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这不仅使得收集必要的评估标记成为可能，而且也不会显著地改变度量模型质量的代码。</p><h1 id="570b" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">给我看看代码！</h1><p id="26c4" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">像通常情况下由著名的科学团体如公平发表的文章一样，它们伴随着一个代码。对于这两篇文章，都可以在<a class="ae kr" href="https://github.com/facebookresearch/detectron2/tree/master/projects/DensePose" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上的官方detectron2库中找到。</p><p id="6d95" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">来自第一篇文章的早期代码，使用基于Caffe2的detectron的第一个版本编写，也可以在GitHub上找到。</p></div></div>    
</body>
</html>