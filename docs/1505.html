<html>
<head>
<title>Introduction to XGBoost Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost算法简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-xgboost-algorithm-d2e7fad76b04?source=collection_archive---------0-----------------------#2021-03-05">https://medium.com/analytics-vidhya/introduction-to-xgboost-algorithm-d2e7fad76b04?source=collection_archive---------0-----------------------#2021-03-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f5293be9f0de6a0a79153f7ff3369488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yAGsAkwd4xnkHYErCyQl4Q.jpeg"/></div></div></figure><h1 id="2c47" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">介绍</h1><blockquote class="jo jp jq"><p id="60d4" class="jr js jt ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">XGBoost代表“极限梯度提升”。XGBoost是一个优化的分布式梯度增强库，设计为高效、灵活和可移植。它在梯度推进框架下实现机器学习算法。它提供了一种并行树提升，以快速准确的方式解决许多数据科学问题。</p></blockquote><p id="a52c" class="pw-post-body-paragraph jr js hi ju b jv jw jx jy jz ka kb kc kq ke kf kg kr ki kj kk ks km kn ko kp hb bi translated">XGBoost是一个软件库，您可以下载并安装到您的机器上，然后从各种界面访问它。具体来说，XGBoost支持以下主要接口:</p><ul class=""><li id="39a4" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp ky kz la lb bi translated">命令行界面(CLI)。</li><li id="6dfa" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated">C++(编写库的语言)。</li><li id="a30f" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated">Python接口以及scikit-learn中的一个模型。</li><li id="ad16" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated">r接口以及caret包中的一个模型。</li><li id="9e09" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated">朱莉娅。</li><li id="f914" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated">像Scala这样的Java和JVM语言以及Hadoop这样的平台。</li></ul><h1 id="0c8c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">XGBoost特性</h1><p id="c35d" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">该库专注于计算速度和模型性能，因此几乎没有多余的东西。然而，它确实提供了许多高级功能。</p><h1 id="77d9" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">模型特征</h1><p id="ac05" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">该模型的实现支持scikit-learn和R实现的特性，并增加了正则化等新特性。支持三种主要形式的梯度增强:</p><ul class=""><li id="8d58" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp ky kz la lb bi translated"><strong class="ju hj">梯度推进</strong>算法也叫梯度推进机包括学习率。</li><li id="a6ff" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">随机梯度推进</strong>，在每个分割级别的行、列和列进行子采样。</li><li id="07ab" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">L1和L2正则化的正则化梯度增强</strong>。</li></ul><h1 id="7544" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">系统功能</h1><p id="c707" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">该库提供了一个用于各种计算环境的系统，尤其是:</p><ul class=""><li id="048b" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp ky kz la lb bi translated"><strong class="ju hj">树构造的并行化</strong>在训练期间使用你所有的CPU核心。</li><li id="13ef" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">分布式计算</strong>用于使用机器集群训练非常大的模型。</li><li id="efd6" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">核外计算</strong>适用于不适合内存的超大型数据集。</li><li id="43d4" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">数据结构和算法的缓存优化</strong>充分利用硬件。</li></ul><h1 id="6bd4" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">算法特征</h1><p id="9077" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">该算法的实现是为计算时间和存储器资源的效率而设计的。设计目标是充分利用可用资源来训练模型。一些关键的算法实现特性包括:</p><ul class=""><li id="716b" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp ky kz la lb bi translated"><strong class="ju hj">稀疏感知</strong>实现自动处理缺失数据值。</li><li id="d028" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">块结构</strong>支持树构造的并行化。</li><li id="a329" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">继续训练</strong>这样你就可以根据新数据进一步提升已经拟合好的模型。</li></ul><p id="4ceb" class="pw-post-body-paragraph jr js hi ju b jv jw jx jy jz ka kb kc kq ke kf kg kr ki kj kk ks km kn ko kp hb bi translated">XGBoost是免费的开源软件，可以在许可的Apache-2许可下使用。</p><h1 id="8b94" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">为什么要用XGBoost？</h1><p id="2838" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">使用XGBoost的两个原因也是该项目的两个目标:</p><ol class=""><li id="1a7e" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp lm kz la lb bi translated">执行速度。</li><li id="d0c5" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp lm kz la lb bi translated">模特表演。</li></ol><figure class="lo lp lq lr fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/f96496111251bc6551236265a56874bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*ozf-ftCx-jy2jII4cEv9YA.png"/></div></figure><h1 id="fed8" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是助推…？</h1><blockquote class="jo jp jq"><p id="d0d8" class="jr js jt ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">Boosting是一种集成学习技术，用于从几个串联的弱分类器中构建一个强分类器。Boosting算法在处理偏差-方差权衡时起着至关重要的作用。与仅控制模型中的高方差的bagging算法不同，boosting控制两个方面(偏差和方差),并且被认为更有效。</p></blockquote><p id="1f1f" class="pw-post-body-paragraph jr js hi ju b jv jw jx jy jz ka kb kc kq ke kf kg kr ki kj kk ks km kn ko kp hb bi translated">以下是几种升压算法:</p><ol class=""><li id="6d96" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp lm kz la lb bi translated">自适应增强</li><li id="b76e" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp lm kz la lb bi translated">梯度推进</li><li id="204d" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp lm kz la lb bi translated">XGBoost</li><li id="4490" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp lm kz la lb bi translated">CATBoost</li><li id="98a0" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp lm kz la lb bi translated">轻型GBM</li></ol><h1 id="08e8" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">adaboost算法</h1><p id="a558" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">AdaBoost是Adaptive Boosting的缩写。AdaBoost是为二进制分类开发的第一个成功的boosting算法。此外，这也是理解boosting算法的最佳起点。从某种意义上说，它是自适应的，后续建立的分类器被调整以支持那些被先前分类器错误分类的实例。它对噪音数据和异常值很敏感。</p><p id="c34c" class="pw-post-body-paragraph jr js hi ju b jv jw jx jy jz ka kb kc kq ke kf kg kr ki kj kk ks km kn ko kp hb bi translated">AdaBoost使用多次迭代来生成单个复合强学习器。它通过迭代添加弱学习者来创建强学习者。在训练的每个阶段，一个新的弱学习者被添加到集成中，并且一个加权向量被调整以聚焦于在前几轮中被错误分类的例子。结果是分类器比弱学习器分类器具有更高的准确度。</p><figure class="lo lp lq lr fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/9f60b28cf2d14bca57a1abbb38e10eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*FhRJ8_8J0fWo3-JjzCLcPg.png"/></div></figure><h1 id="fd26" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">梯度推进</h1><p id="fc3a" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">梯度推进是建立预测模型最强大的技术之一，它被称为AdaBoost 的<strong class="ju hj">推广。梯度提升的主要目的是通过使用梯度下降优化算法添加弱学习器来最小化损失函数。一般化允许使用任意可微分损失函数，将该技术扩展到二元分类问题之外，以支持回归、多类分类等。</strong></p><blockquote class="jo jp jq"><p id="e79c" class="jr js jt ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated"><strong class="ju hj">梯度推进有三个主要组成部分</strong>。</p></blockquote><ul class=""><li id="9d5f" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp ky kz la lb bi translated"><strong class="ju hj">损失函数:</strong>损失函数的作用是估计模型在用给定数据进行预测时有多好。这可能因问题的类型而异。</li><li id="affa" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">弱学习者:</strong>弱学习者是指与随机猜测相比，对数据分类很差的人。弱学习器主要是决策树，但是GBM中也可以使用其他模型。</li><li id="a3b0" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">加法模型:</strong>这是一个迭代和连续的过程，一次一步地添加决策树。每次迭代应该减少损失函数的值。添加固定数量的树，或者一旦损失达到可接受的水平或者在外部验证数据集上不再改善，训练就停止。</li></ul><h1 id="7504" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">对梯度增强的改进</h1><p id="4e2e" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">梯度提升是一种贪婪算法，可以快速地使训练数据集过拟合。因此，正则化方法被用来通过减少过拟合来提高算法的性能。</p><ul class=""><li id="408c" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp ky kz la lb bi translated"><strong class="ju hj">子采样:</strong>这是为GBM引入的最简单的正则化方法。这提高了模型的泛化性能并减少了计算量。子采样将随机性引入拟合过程。在每次学习迭代中，只有随机部分的训练数据被用于拟合连续的基础学习者。训练数据被采样而没有替换。</li><li id="d25b" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">收缩:</strong>收缩常用于岭回归，它将回归系数收缩为零，从而减少潜在的不稳定回归系数的影响。在GBM中，收缩用于减少每个额外安装的基础学习者的影响。它减少了增量步骤的大小，从而降低了每次连续迭代的重要性。这种技术背后的直觉是，通过采取许多小步骤比采取较少的大步骤来改进模型更好。如果其中一个提升迭代被证明是错误的，其负面影响可以在后续步骤中轻松纠正。</li><li id="837c" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">提前停止:</strong>可以从决策树中得出的一个重要的实际考虑是提前停止或树修剪。这意味着，如果集合被修剪了树的数量，对应于误差曲线上的验证集最小值，则过拟合将以最小的精度代价被规避。另一个观察结果是，考虑提前停止的最佳推进次数随着收缩参数λ而变化。因此，应考虑提升次数和λ之间的权衡。</li></ul><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/417b070859b16c86f4fb3eda27a46c6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9BowVV2E_LctnBrU6ZJw4A.jpeg"/></div></div></figure><h1 id="2354" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">XGBoost</h1><p id="bcb0" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">XGBoost代表极端梯度增强。它在最近几天变得流行起来，并由于其可伸缩性而在结构化数据的应用机器学习和Kaggle竞争中占据主导地位。</p><p id="0dc5" class="pw-post-body-paragraph jr js hi ju b jv jw jx jy jz ka kb kc kq ke kf kg kr ki kj kk ks km kn ko kp hb bi translated">XGBoost是梯度增强决策树(GBM)的扩展，专门用于提高速度和性能。</p><h1 id="61cf" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">XGBoost特性</h1><ul class=""><li id="5dad" class="kt ku hi ju b jv lh jz li kq lu kr lv ks lw kp ky kz la lb bi translated"><strong class="ju hj">正则化学习:</strong>正则化项有助于平滑最终学习到的权重，以避免过拟合。规则化的目标将倾向于选择使用简单和预测函数的模型。</li><li id="6f62" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">梯度树提升:</strong>在欧几里德空间中，不能使用传统的优化方法来优化树集成模型。相反，该模型是以相加的方式训练的。</li><li id="679b" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">收缩和列子采样:</strong>除了正则化目标，还使用了两种附加技术来进一步防止过拟合。第一种技术是弗里德曼提出的收缩。在树提升的每一步之后，收缩通过因子η缩放新增加的权重。类似于随机优化中的学习率，收缩减少了每棵树的影响，并为未来的树留下空间来改进模型。</li></ul><p id="f430" class="pw-post-body-paragraph jr js hi ju b jv jw jx jy jz ka kb kc kq ke kf kg kr ki kj kk ks km kn ko kp hb bi translated">第二种技术是列(特征)子采样。这种技术用于随机森林。列子采样比传统的行子采样更能防止过拟合。列子样本的使用也加速了并行算法的计算。</p><h1 id="6e69" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">分裂算法</h1><ul class=""><li id="5b0b" class="kt ku hi ju b jv lh jz li kq lu kr lv ks lw kp ky kz la lb bi translated"><strong class="ju hj">精确贪婪算法:</strong>树学习中的主要问题是寻找最佳分裂。该算法列举了所有特征上所有可能的分裂。枚举连续特征的所有可能的分裂在计算上要求很高。</li><li id="09e3" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">近似算法:</strong>精确贪婪算法非常强大，因为它贪婪地列举了所有可能的分裂点。然而，当数据不完全适合内存时，就不可能有效地做到这一点。近似算法根据特征分布的百分位数提出候选分割点。然后，该算法将连续特征映射到由这些候选点分割的桶中，聚集统计数据，并基于聚集的统计数据在提议中找到最佳解决方案。</li><li id="a655" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">加权分位数草图:</strong>近似算法的一个重要步骤是提出候选分裂点。XGBoost有一个分布式加权分位数草图算法，可以有效地处理加权数据。</li><li id="f514" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">稀疏感知的分裂发现:</strong>在许多现实世界的问题中，输入x稀疏是很常见的。稀疏有多种可能的原因:</li></ul><ol class=""><li id="5a5a" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp lm kz la lb bi translated">数据中存在缺失值</li><li id="a4e0" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp lm kz la lb bi translated">统计数据中经常出现零条目</li><li id="c682" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp lm kz la lb bi translated">特征工程的产物，例如一键编码</li></ol><p id="4687" class="pw-post-body-paragraph jr js hi ju b jv jw jx jy jz ka kb kc kq ke kf kg kr ki kj kk ks km kn ko kp hb bi translated">让算法知道数据中的稀疏模式是很重要的。XGBoost以统一的方式处理所有稀疏模式。</p><h1 id="07c5" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">XGBoost的系统特性</h1><ul class=""><li id="405e" class="kt ku hi ju b jv lh jz li kq lu kr lv ks lw kp ky kz la lb bi translated">在训练过程中使用所有CPU核心并行构建树。收集每一列的统计数据可以并行化，为我们提供了一个并行的查找拆分的算法。</li><li id="2093" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated">缓存感知访问:XGBoost旨在优化硬件的使用。这是通过在每个线程中分配内部缓冲区来实现的，其中可以存储梯度统计数据。</li><li id="ae8e" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated">内存中容纳不下的超大型数据集的核外计算块。</li><li id="1aaf" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated">使用机器集群训练超大型模型的分布式计算。</li><li id="84bc" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated">用于并行学习的列块。树学习最耗时的部分是将数据排序。为了降低排序的成本，数据以压缩格式按排序顺序存储在列块中。</li></ul><h1 id="d89c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">XGBoost的目标</h1><ul class=""><li id="1991" class="kt ku hi ju b jv lh jz li kq lu kr lv ks lw kp ky kz la lb bi translated">执行速度:XGBoost几乎总是比来自R、Python Spark和H2O的其他基准实现更快，而且与其他算法相比，它确实更快。</li><li id="7100" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated">模型性能:在分类和回归预测建模问题上，XGBoost在结构化或表格化数据集上占优势。</li></ul><h1 id="60cf" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">XGBoost算法—参数</h1><h1 id="05df" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">a.一般参数</h1><p id="152b" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">以下是Xgboost算法中使用的一般参数:</p><ul class=""><li id="285d" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp ky kz la lb bi translated"><strong class="ju hj">静音</strong>:默认值为0。您需要为打印运行消息指定0，为静默模式指定1。</li><li id="99e7" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj"> booster </strong>:默认值为GBtree。您需要指定要使用的助推器:GBtree(基于树)或GBlinear(线性函数)。</li><li id="0ec3" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj"> num_pbuffer </strong>:由XGBoost算法自动设置，无需用户设置。更多细节请阅读XGBoost的文档。</li><li id="c616" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj"> num_feature </strong>:由XGBoost算法自动设置，无需用户设置。</li></ul><h1 id="923c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">b.助推器参数</h1><p id="5fc6" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">下面我们讨论了Xgboost算法中特定于树的参数:</p><ul class=""><li id="d85e" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp ky kz la lb bi translated"><strong class="ju hj"> eta </strong>:默认值设置为0.3。您需要指定更新中使用的步长收缩，以防止过度拟合。在每一步提升之后，我们可以直接得到新特征的权重。eta实际上缩小了特征权重，以使提升过程更加保守。范围是0到1。低eta值意味着模型对过度拟合更稳健。</li><li id="3b1c" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">伽玛</strong>:默认值设置为0。您需要指定在树的叶节点上进行进一步分区所需的最小损失减少量。越大，算法就越保守。范围是0到∞。伽玛越大，算法越保守。</li><li id="3e41" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">最大深度</strong>:默认值设置为6。您需要指定树的最大深度。范围是1到∞。</li><li id="4748" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj"> min_child_weight </strong>:默认值设置为1。您需要指定子对象所需的最小实例重量总和(hessian)。如果树划分步骤产生叶节点。然后实例权重之和小于最小子权重。那么构建过程将放弃进一步的划分。在线性回归模式中，对应于每个节点中所需的最小实例数。越大，算法就越保守。范围是0到∞。</li><li id="eab7" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj"> max_delta_step </strong>:默认值设置为0。我们允许每棵树的重量估计值为。如果该值设置为0，则表示没有约束。如果将其设置为正值，则有助于使更新步骤更加保守。通常，这个参数是不需要的，但它可能有助于逻辑回归。尤其是一个班级极度不平衡的时候。将其设置为1–10的值可能有助于控制更新。范围是0到∞。</li><li id="ad66" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">子采样</strong>:默认值设置为1。您需要指定训练实例的子样本比率。将其设置为0.5意味着XGBoost随机收集了一半的数据实例。需要种植树木，这将防止过度适应。范围是0到1。</li><li id="90e6" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj"> colsample_bytree </strong>:默认值设置为1。构建每个树时，需要指定列的子抽样比率。范围是0到1。</li></ul><h1 id="1dcb" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">c.线性助推器特定参数</h1><p id="e719" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">这些是XGBoost算法中的线性升压器特定参数。</p><ul class=""><li id="56b8" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp ky kz la lb bi translated"><strong class="ju hj">λ和α</strong>:这是关于权重的正则化项。Lambda默认值假定为1，alpha为0。</li><li id="494f" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj"> lambda_bias </strong> : L2正则化项on bias，默认值为0。</li></ul><h1 id="9c2f" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">d.学习任务参数</h1><p id="3005" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">以下是XGBoost算法中的学习任务参数</p><ul class=""><li id="443b" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp ky kz la lb bi translated"><strong class="ju hj"> base_score </strong>:默认值设置为0.5。您需要指定所有实例的初始预测得分，全局偏差。</li><li id="4472" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj">目标</strong>:默认值设置为reg: linear。您需要指定您想要的学习者类型。这包括线性回归、泊松回归等。</li><li id="245e" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj"> eval_metric </strong>:您需要指定验证数据的评估指标。并且将根据目标分配默认度量。</li><li id="85e5" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><strong class="ju hj"> seed </strong>:和往常一样，您在这里指定种子来复制同一组输出。</li></ul><h1 id="4162" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">摘要</h1><p id="e6da" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">在这篇文章中，您发现了应用机器学习的XGBoost算法。</p><p id="008c" class="pw-post-body-paragraph jr js hi ju b jv jw jx jy jz ka kb kc kq ke kf kg kr ki kj kk ks km kn ko kp hb bi translated">你学到了:</p><ul class=""><li id="1da8" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp ky kz la lb bi translated">XGBoost是一个用于开发快速和高性能梯度增强树模型的库。</li><li id="a09e" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated">XGBoost在一系列困难的机器学习任务上取得了最佳性能。</li></ul><h1 id="e615" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结论</h1><p id="3768" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">与其他算法相比，XGBoost是一种更快的算法，因为它是并行和分布式计算。XGBoost是在系统优化和机器学习原则方面的深入考虑下开发的。这个库的目标是推动机器的计算极限，提供一个可伸缩的、可移植的、精确的库。</p><h1 id="31c7" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">官方XGBoost资源</h1><p id="5e9c" class="pw-post-body-paragraph jr js hi ju b jv lh jx jy jz li kb kc kq lj kf kg kr lk kj kk ks ll kn ko kp hb bi translated">XGBoost上最好的信息来源是项目的官方GitHub资源库。</p><p id="4289" class="pw-post-body-paragraph jr js hi ju b jv jw jx jy jz ka kb kc kq ke kf kg kr ki kj kk ks km kn ko kp hb bi translated">从那里，您可以访问<a class="ae lx" href="https://github.com/dmlc/xgboost/issues" rel="noopener ugc nofollow" target="_blank">问题跟踪器</a>和<a class="ae lx" href="https://groups.google.com/forum/#!forum/xgboost-user/" rel="noopener ugc nofollow" target="_blank">用户组</a>，它们可用于提问和报告bug。</p><p id="bfbc" class="pw-post-body-paragraph jr js hi ju b jv jw jx jy jz ka kb kc kq ke kf kg kr ki kj kk ks km kn ko kp hb bi translated">包含示例代码和帮助的链接的一个很好的来源是<a class="ae lx" href="https://github.com/dmlc/xgboost/tree/master/demo" rel="noopener ugc nofollow" target="_blank">棒极了的XGBoost页面</a>。</p><p id="1fb0" class="pw-post-body-paragraph jr js hi ju b jv jw jx jy jz ka kb kc kq ke kf kg kr ki kj kk ks km kn ko kp hb bi translated">还有一个<a class="ae lx" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">官方文档页面</a>，其中包括一系列不同语言的入门指南、教程、操作指南等等。</p><p id="5d5c" class="pw-post-body-paragraph jr js hi ju b jv jw jx jy jz ka kb kc kq ke kf kg kr ki kj kk ks km kn ko kp hb bi translated">XGBoost上有一些更正式的论文，值得一读，以了解更多关于该库的背景知识:</p><ul class=""><li id="b98c" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp ky kz la lb bi translated">2014年，希格斯玻色子的发现与增强树。</li><li id="fcf4" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp ky kz la lb bi translated"><a class="ae lx" href="https://arxiv.org/abs/1603.02754" rel="noopener ugc nofollow" target="_blank"> XGBoost:一个可扩展的树提升系统</a>，2016。</li></ul><p id="d56e" class="pw-post-body-paragraph jr js hi ju b jv jw jx jy jz ka kb kc kq ke kf kg kr ki kj kk ks km kn ko kp hb bi translated"><strong class="ju hj">参考文献</strong>:</p><ol class=""><li id="2f00" class="kt ku hi ju b jv jw jz ka kq kv kr kw ks kx kp lm kz la lb bi translated"><a class="ae lx" href="https://arxiv.org/pdf/1603.02754.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1603.02754.pdf</a>(XGBoost:一个可扩展的树提升系统)</li><li id="c6dc" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp lm kz la lb bi translated"><a class="ae lx" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank">https://xgboost . readthedocs . io/en/latest/tutorials/model . html</a></li><li id="08af" class="kt ku hi ju b jv lc jz ld kq le kr lf ks lg kp lm kz la lb bi translated"><a class="ae lx" href="https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/gentle-introduction-xgboost-applied-machine-learning/</a></li></ol></div></div>    
</body>
</html>