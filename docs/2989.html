<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/many-to-many-multilingual-translation-model-m2m-100-27ddc324f78e?source=collection_archive---------14-----------------------#2021-05-28">https://medium.com/analytics-vidhya/many-to-many-multilingual-translation-model-m2m-100-27ddc324f78e?source=collection_archive---------14-----------------------#2021-05-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/5393cd5d3438d3f865757620368bc778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GL5S6Wptgo5GJ44Z0YTTtw.jpeg"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">照片由<a class="ae hu" href="https://unsplash.com/s/photos/language?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae hu" href="https://unsplash.com/@sonereker?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Soner Eker </a>拍摄</figcaption></figure><p id="d77d" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">这篇文章旨在总结题为“超越以英语为中心的多语言机器翻译<a class="ae hu" href="https://arxiv.org/abs/2010.11125" rel="noopener ugc nofollow" target="_blank">的论文中的发现。这篇文章的标题进步是M2M_100预训练多语言翻译模型。除了标题成就之外，通过论文中介绍的研究还取得了两项额外的进展——多语言数据挖掘方法最终扩展了两个大型多语言翻译语料库，以及实施了用于改进超大型深度学习模型到翻译模型上下文的训练的方法。</a></p><p id="96fe" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">下面的文章将从大纲开始，呈现论文的整体概要。然后，描述为训练模型而产生的数据集，训练如此大的模型所采用的一些策略，最后是M2M 100模型设计的分解。</p><h1 id="66bb" class="iu iv hx bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">摘要</h1><p id="9d73" class="pw-post-body-paragraph hv hw hx hy b hz js ib ic id jt if ig ih ju ij ik il jv in io ip jw ir is it ha bi translated">本文介绍了多语言翻译模型M2M_100，它实现了语言之间的直接翻译。能够直接在100种语言之间进行翻译。消除了在以英语为中心的翻译模式中使用英语作为中间语言的需要。M2M_100优于以英语为中心的多语言模型、已发布的双语模型和其他已发布的直接翻译多语言模型。此外，在一项盲测中，人工翻译对M2M的评分高于以英语为中心的模型。</p><p id="d392" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">由于多语言翻译数据挖掘的改进，这种模型的训练成为可能。作者利用并扩展了多语言翻译库CCMatrix [ <a class="ae hu" href="https://arxiv.org/abs/1911.04944" rel="noopener ugc nofollow" target="_blank"> 1 </a>和CCAligned [ <a class="ae hu" href="https://www.aclweb.org/anthology/2020.emnlp-main.480/" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]用于训练和测试数据。他们定义了一个基于语系分组的挖掘策略，并选择跨越称为<em class="jx">桥语言、</em>的分组的语言，并演示了如何使用该策略来改进语言对矩阵的稀疏挖掘，而不是随机采样。最后，他们展示了如何利用回译的合成双文本来提高低资源语言的翻译。</p><p id="ad4e" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">为了训练这样一个大型模型，作者实施了最近的密集扩展改进，如优化器状态分片和梯度检查点，以减少处理状态所需的内存，以及模型并行性，以跨多个设备分割训练。</p><h1 id="3c5c" class="iu iv hx bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">数据挖掘</h1><p id="8c4d" class="pw-post-body-paragraph hv hw hx hy b hz js ib ic id jt if ig ih ju ij ik il jv in io ip jw ir is it ha bi translated">选择的语言覆盖范围包括来自不同地理语系和多种文字的广泛使用的语言，目标是高覆盖全球语言。此外，语言仅限于那些存在公共评估数据和单语数据的语言。</p><p id="076c" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated"><strong class="hy jy"> <em class="jx">定义:</em> </strong>互为翻译的句子对称为<em class="jx">双文本</em>数据。这种类型的数据构成了基于转换器的翻译模型的训练集和测试集。</p><h2 id="39e4" class="jz iv hx bd iw ka kb kc ja kd ke kf je ih kg kh ji il ki kj jm ip kk kl jq km bi translated">扩展Corpi</h2><p id="09e5" class="pw-post-body-paragraph hv hw hx hy b hz js ib ic id jt if ig ih ju ij ik il jv in io ip jw ir is it ha bi translated">实验者利用并扩展了两个多语言双文本语料库，CCMatrix和CCAligned。CCMatrix在挖掘双文本时使用全局方法，它将一种语言中的每个独特句子与另一种语言中的所有独特句子进行比较，以找到双文本对。CCAligned首先预先选择可能包含相互翻译的文档，然后在配对的文档中挖掘双文本。</p><p id="8952" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">这两种方法都使用LASER [ <a class="ae hu" href="https://arxiv.org/abs/1812.10464" rel="noopener ugc nofollow" target="_blank"> 3 </a>编码器生成的语言不可知语义嵌入来执行句子比较。</p><h2 id="2e5f" class="jz iv hx bd iw ka kb kc ja kd ke kf je ih kg kh ji il ki kj jm ip kk kl jq km bi translated">采矿战略</h2><p id="bc4e" class="pw-post-body-paragraph hv hw hx hy b hz js ib ic id jt if ig ih ju ij ik il jv in io ip jw ir is it ha bi translated">为了解决为每一对语言挖掘数据的禁止性，作者通过定义语系分组和桥接语言展示了稀疏挖掘的方法。语系将相似的语言分组，并且针对该组中的所有其他语言来挖掘该组中的所有语言。桥接语言是跨组挖掘数据的语言。这些语言通常是每个组中拥有最多资源的语言。</p><p id="0492" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">这是一种有效的稀疏挖掘策略，优于其他稀疏挖掘策略，同时使模型能够学习如何跨组翻译语言，而不管特定语言对是否是桥接语言对。</p><h2 id="67c3" class="jz iv hx bd iw ka kb kc ja kd ke kf je ih kg kh ji il ki kj jm ip kk kl jq km bi translated">回译</h2><p id="793c" class="pw-post-body-paragraph hv hw hx hy b hz js ib ic id jt if ig ih ju ij ik il jv in io ip jw ir is it ha bi translated">回译是通过将一种语言中的句子翻译成另一种语言来创建合成双文本的过程。作者展示了这种技术在应用于低资源语言以增加这些语言的训练数据从而提高翻译性能时的价值。</p><p id="a4d2" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">作者首先训练了一个1.2B参数的M2M模型，并用它来评估哪些语言对表现最差。然后，使用相同的模型来生成添加到训练集中的合成翻译。这些双文本对用特殊的编码器端标记来标记，以向模型指示该对是合成的。单语句子来自清理后的CommonCrawl [ <a class="ae hu" href="https://commoncrawl.org/" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]语料库。从中挖掘CCAligned和CCMatrix的相同语料库。</p><h2 id="4ed0" class="jz iv hx bd iw ka kb kc ja kd ke kf je ih kg kh ji il ki kj jm ip kk kl jq km bi translated">平衡语言</h2><p id="9683" class="pw-post-body-paragraph hv hw hx hy b hz js ib ic id jt if ig ih ju ij ik il jv in io ip jw ir is it ha bi translated">作者描述了<em class="jx"> Sinkhorn温度采样</em>来将温度采样策略扩展到多对多的设置，以便在代表性不足和过度的语言之间平衡采样。</p><h1 id="28df" class="iu iv hx bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">计算改进技术</h1><p id="5902" class="pw-post-body-paragraph hv hw hx hy b hz js ib ic id jt if ig ih ju ij ik il jv in io ip jw ir is it ha bi translated">作者主要使用两种技术来提高训练性能:(1)通过优化器状态分片[ <a class="ae hu" href="https://arxiv.org/abs/1910.02054" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]和梯度检查点[ <a class="ae hu" href="https://arxiv.org/abs/1604.06174" rel="noopener ugc nofollow" target="_blank"> 6 </a> ]来减少GPU上的内存消耗。(2)张量并行的模型分割[ <a class="ae hu" href="https://arxiv.org/abs/1811.06965" rel="noopener ugc nofollow" target="_blank"> 7 </a>、<a class="ae hu" href="https://arxiv.org/abs/2004.09910" rel="noopener ugc nofollow" target="_blank"> 8 </a>、<a class="ae hu" href="https://arxiv.org/abs/1909.08053" rel="noopener ugc nofollow" target="_blank"> 9 </a>、<a class="ae hu" href="https://arxiv.org/abs/1811.02084" rel="noopener ugc nofollow" target="_blank"> 10 </a> ]。</p><p id="512c" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">进行了实验分析，以确定较宽的模型比较深的模型具有更好的伸缩性和性能。</p><h1 id="84ed" class="iu iv hx bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">模型设计</h1><h2 id="1d49" class="jz iv hx bd iw ka kb kc ja kd ke kf je ih kg kh ji il ki kj jm ip kk kl jq km bi translated">标记化</h2><p id="d60c" class="pw-post-body-paragraph hv hw hx hy b hz js ib ic id jt if ig ih ju ij ik il jv in io ip jw ir is it ha bi translated">几乎所有的自然语言处理都在某种形式的记号上工作。作者使用SentencePiece [ <a class="ae hu" href="https://arxiv.org/abs/1808.06226" rel="noopener ugc nofollow" target="_blank"> 11 </a> ]产生从训练数据集中学习的子词单元来完成这一点。SentencePiece是合适的，因为它被设计成与没有分段的语言兼容。</p><h2 id="731a" class="jz iv hx bd iw ka kb kc ja kd ke kf je ih kg kh ji il ki kj jm ip kk kl jq km bi translated">模型结构</h2><figure class="ko kp kq kr fd hj er es paragraph-image"><div class="er es kn"><img src="../Images/ec94c2fee8f87c5f33c0aa03b231c9bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*qEp0h9qLjKc_1l2xGF7wGQ.png"/></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">图片直接来自《超越英语……》一文。</figcaption></figure><p id="0c6a" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">M2M_100是基于编码器-解码器转换器架构的序列到序列模型的扩展。用非书呆子的话解释这个:</p><ul class=""><li id="724a" class="ks kt hx hy b hz ia id ie ih ku il kv ip kw it kx ky kz la bi translated">序列到序列主要是指为了优化通过将输入序列转换为给定目标输出而生成的输出序列的质量而设计的模型。例如，提高从法语句子翻译生成的英语句子的质量。</li><li id="4834" class="ks kt hx hy b hz lb id lc ih ld il le ip lf it kx ky kz la bi translated">编码器-解码器转换器架构是具有“编码器”转换器和“解码器”转换器的神经网络模型的结构。编码器获取一个令牌序列，并生成一个标准长度嵌入—该序列的数学表示。嵌入内容被输入到解码器中。该解码器的任务是将嵌入转换成标记序列，因为它们应该在目标空间中。例如，首先将法语句子转换为表示该句子含义的嵌入。然后，将这种嵌入转换成表达相同意思的英语句子。</li></ul><p id="3e0a" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">因此，其核心M2M_100是一个相当标准的序列到序列模型，建立在相当标准的变压器架构上，建模规模的挑战集中在如何有效地在挖掘的bitext数据山上进行训练。从这里，建模者能够变得有创造性，并且利用语言家族的自然分组来扩展体系结构，使得上面讨论的神经网络性能优化技术可应用。</p><p id="86b1" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">这是通过用一组并行子层替换一些Transformer子层来实现的，每个并行子层对应一组预定义的语言。每个子层可以部署在不同的GPU上，以提高训练和推理速度，以及提高内存使用效率，因为并非所有语言都对所有参数感兴趣。</p><p id="8247" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">这些语言特定的子层可以应用于编码器或解码器端。然而，在实践中仅适用于模型的解码器端。</p><p id="c072" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">这种架构的一个潜在缺点是，模型为语言组学习的信息变成了该组的孤岛。为了解决这个问题，作者实现了随机重路由，并进行实验来优化数量。随机重路由是将句子馈送到随机选择的语言组子层而不是确定选择的语言组子层的过程。</p><p id="1229" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">利用语言组子层的另一个优点是，可以将新的语言子层添加到已经预先训练好的转换器中，以学习特定于语言的组件。从而容易地将翻译对覆盖范围扩展到新的语言。</p><h1 id="ee56" class="iu iv hx bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">总体结果</h1><p id="7823" class="pw-post-body-paragraph hv hw hx hy b hz js ib ic id jt if ig ih ju ij ik il jv in io ip jw ir is it ha bi translated">作者使用BLEU [ <a class="ae hu" href="https://www.aclweb.org/anthology/P02-1040/" rel="noopener ugc nofollow" target="_blank"> 12 </a> ]衡量所有结果研究的翻译质量。他们将M2M 100的输出与针对WMT基准数据集提交的模型以及人工翻译进行了比较。</p><p id="3522" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">总体而言，与其他模型相比，M2M 100模型在英语方向的翻译方面有适度的改进，在低资源语言和非英语方向的翻译方面有显著的改进。当从M2M_100翻译的例子相对于由人工翻译评估的以英语为中心的模型翻译被仔细检查时，这是成立的。</p><h1 id="411a" class="iu iv hx bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">参考</h1><p id="6e43" class="pw-post-body-paragraph hv hw hx hy b hz js ib ic id jt if ig ih ju ij ik il jv in io ip jw ir is it ha bi translated">[1] Holger Schwenk、Guillaume Wenzek、Sergey Edunov、Edouard Grave、Armand Joulin和Angela Fan。CCMatrix:挖掘网络上数十亿高质量的平行句子。arXiv预印本arXiv:1911.04944，2019。</p><p id="0104" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">[2]艾哈迈德·基什基、维什拉夫·乔德里、弗朗西斯科·古兹曼和菲利普·科恩。CCAligned:跨语言网络文档对的大规模集合。进行中。2020年EMNLP的。</p><p id="9e39" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">[3]米克尔·阿特克斯和霍尔格·施文克。大规模多语言句子嵌入，实现零枪击跨语言迁移及其他。在<a class="ae hu" href="https://arxiv.org/abs/1812.10464," rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1812.10464,</a>2018年。</p><p id="4c52" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">[4] Guillaume Wenzek、Marie-Anne Lachaux、Alexis Conneau、Vishrav Chaudhary、Francisco Guzmán、Armand Joulin和Edouard Grave。Ccnet:从网络抓取数据中提取高质量单语数据集。arXiv预印本arXiv:1911.00359，2019。</p><p id="e578" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">[5] Samyam Rajbhandari、Jeff Rasley、Olatunji Ruwase和何玉雄。零:面向训练万亿参数模型的内存优化。ArXiv，2019。</p><p id="171c" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">[6]陈天琦、、、张和卡洛斯·盖斯特林。用次线性记忆代价训练深度网络。arXiv，abs/1604.06174，2016。</p><p id="d2ea" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">[7]，郑有龙，安库尔·巴普那，奥尔罕·菲拉特，陈德浩，米娅·陈，李孝忠，吉泉·恩吉亚姆，郭维乐，吴永辉，等. Gpipe:利用流水线并行性有效训练巨型神经网络.《神经信息处理系统进展》，第103–112页，2019年。</p><p id="63ff" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">[8] Chiheon Kim、Heungsub Lee、Myungryong Jeong、Woonhyuk Baek、Boogeon Yoon、Ildoo Kim、Sungbin Lim和Sungwoong Kim。torchgpipe:用于训练巨型模型的实时流水线并行性。arXiv预印本arXiv:2004.09910，2020。</p><p id="5615" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">[9] Mohammad Shoeybi、Mostofa Patwary、Raul Puri、Patrick LeGresley、Jared Casper和Bryan Catanzaro。威震天-lm:利用gpu模型并行性训练数十亿参数语言模型。arXiv预印本arXiv:1909.08053，2019。</p><p id="0f5c" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">[10] Noam Shazeer，Youlong Cheng，Niki Parmar，Dustin Tran，Ashish Vaswani，Penporn Koanantakool，Peter Hawkins，HyoukJoong Lee，Hong，Cliff Young，等. Mesh-tensorflow:超级计算机的深度学习.《神经信息处理系统进展》，第10414–10423页，2018年。</p><p id="c621" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">[11]工藤多久和约翰·理查森。SentencePiece:一个简单且独立于语言的子词分词器和去分词器，用于神经文本处理。arXiv预印本arXiv:1808.06226，2018。</p><p id="9e32" class="pw-post-body-paragraph hv hw hx hy b hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it ha bi translated">[12]基肖尔·帕皮尼尼、萨利姆·鲁科斯、托德·沃德和魏·朱婧。Bleu:一种自动评估机器翻译的方法。《计算语言学协会第40届年会论文集》，311-318页，2002年。</p></div></div>    
</body>
</html>