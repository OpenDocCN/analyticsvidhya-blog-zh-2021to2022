<html>
<head>
<title>Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/word-embeddings-aaef8c6bd04a?source=collection_archive---------15-----------------------#2021-05-28">https://medium.com/analytics-vidhya/word-embeddings-aaef8c6bd04a?source=collection_archive---------15-----------------------#2021-05-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="971d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文旨在使用该领域一些最著名的论文中的信息，为单词嵌入提供直觉。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/16c1cfa009b3feee9ed55a03f93b19a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8Idm5uM-ru6voa1v"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">马库斯·斯皮斯克在<a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="a7d3" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">我们为什么需要单词向量？</strong></h1><p id="18d3" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">另一种选择是将文档中的每个单词表示为一个独热编码向量。我们试着用这个来思考一些问题。</p><ol class=""><li id="5c8d" class="kw kx hh ig b ih ii il im ip ky it kz ix la jb lb lc ld le bi translated">词汇量:每个单词都可以用一个和你的词汇量一样长的形状来表示。这可能是数百万的数量级。</li></ol><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lf lg l"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lh"><img src="../Images/5da1c1f20d5fd28f36d40f21c796a598.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*LnpoIVO9i1Z9zYqe7eE71g.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">one_hot_encode的输出示例(“该城镇靠近城市”)</figcaption></figure><p id="31a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.<strong class="ig hi">语义</strong>:现在，除了规模问题，我们失去了“应该靠近的单词之间的相似性”。换句话说，用这种表示法，所有的单词都是相互正交的。在高维空间中，它们都指向不同的方向。</p><p id="a6f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">理想情况下，在上面给出的例子中,“城镇”和“城市”应该有相似的向量表示。但在这种情况下，它们的余弦相似度为0(它们彼此成90度角)。</p><p id="447f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，理想情况下，我们想要的是用一个比词汇表的大小小得多的向量来表示每个单词。此外，我们还希望这些向量本身能为单词提供一些“意义”。有趣的是，在实践中，增加或减少两个或更多这样的单词会产生有趣的结果。例如，vector(德国)+ vector(首都)可能导致vector(柏林)。</p><h1 id="ffd9" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">我们如何构建这些词向量？</strong></h1><p id="765b" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">虽然有几种方法，但其中一个需要理解的关键概念是<strong class="ig hi"> <em class="li">上下文</em> </strong> &amp; <strong class="ig hi"> <em class="li">中心词</em> </strong>。</p><p id="bfc4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">直觉(被称为<strong class="ig hi">分布语义</strong>)是文档中接近特定单词的单词传达了该单词的意思。例如，如果我们有一个像<strong class="ig hi">‘老师经常骂学生’</strong>这样的句子。对于一个<strong class="ig hi">中心单词student </strong>和一个大小为5的窗口(取单词的两边)，我们将在它的<strong class="ig hi">上下文中有单词<strong class="ig hi">‘老师’，‘责骂’，‘相当地’，‘经常’</strong>。现在，如果我们有一个足够大的语料库，我们可以通过在每个单词的上下文中查看所有单词来找到这个单词的表示。</strong></p><p id="da7d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在有两种方法可以做到这一点，我们可以只是<strong class="ig hi">统计一个单词在上下文</strong>中出现的频率，并为每个单词找出一个表示，或者我们可以训练一个神经网络，通过<strong class="ig hi">预测给定我们的中心单词</strong>的上下文向量来找出哪些向量最适合。在本文中，我将讨论后者。</p><h1 id="c418" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak"> Word2vec: </strong></h1><p id="3e16" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">最初，这个想法是由Tomas Mikolov于2013年在这篇名为“<a class="ae js" href="https://medium.com/r?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1301.3781.pdf" rel="noopener">向量空间中单词表示的有效估计</a>的<a class="ae js" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中提出的，他在“<a class="ae js" href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">单词和短语的分布式表示及其组合性</a>中对其进行了进一步改进。第一篇论文介绍了两种方法。</p><ol class=""><li id="af87" class="kw kx hh ig b ih ii il im ip ky it kz ix la jb lb lc ld le bi translated">连续单词包:根据上下文单词预测中心单词。</li><li id="db05" class="kw kx hh ig b ih lj il lk ip ll it lm ix ln jb lb lc ld le bi translated">跳跃式语法:给定一个中心词，预测上下文词。</li></ol><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lo"><img src="../Images/3d9261edd9fe8d002c0aed8d400fce10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*9WQ7elCLY4D9dnf_njTw8g.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">论文中描述的模型。</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lp"><img src="../Images/b43511ef420a7388461a434bf9bdeaa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*33qMCZ68OcQK0pPcxpAEyA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd jv">跳过图</strong>插图</figcaption></figure><p id="df37" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如图所示，使用跳格模型，我们需要找到P ( context | center)。但是我们最初的目标是找出单词向量，对吗？下面是我们对目标函数的构建。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/50f42413c74883171d5fe6fa37d49681.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*V3zEw-VcUaAKXP7h34y-uw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">似然函数</figcaption></figure><p id="6175" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种情况下，参数theta指的是我们的单词向量。我们必须<strong class="ig hi">在给定中心词的情况下，最大化我们的模型预测上下文词的可能性。</strong>内积是针对特定中心词的，并且似然函数是作为中心词的全部词(T个总词)。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/24ec88e90342e5a43a0566599ac17f96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*zCycvQR-IqRqbu1RyRCKRw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">’</figcaption></figure><p id="046e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于skip-gram和CBOW模型，我们需要两个向量来表示每个单词。一个用于当单词在上下文中时，另一个用于当单词在中心时。我们称这些向量为<strong class="ig hi"> U &amp; V </strong>。为了将它表示为一个分布，我们采用<strong class="ig hi">这两个向量</strong>的点积，使用指数使点积为正，并用词汇表中的所有向量对其进行归一化。<strong class="ig hi">我们的目标应该是找到向量U &amp; V，它最大化正确单词出现在给定中心单词的上下文中的概率。</strong>定义概率，<strong class="ig hi"> Vcenter </strong>通过将我们的<strong class="ig hi"> V矩阵(N x V) </strong>乘以一位热码编码输入向量来给出，其中N是嵌入维数，V是vocab的大小。其本身乘以大小为(V×N)的<strong class="ig hi"> U矩阵中的所有上下文向量。最终，我们得到词汇表中每个单词的“分数”，这些分数又被传递给softmax函数。在某种程度上，这就像多类分类，在给定中心词的情况下，我们预测哪个“词”在上下文中。</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ls"><img src="../Images/63bee67e707d6b72db6f734088c2407a.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*V-bTcOxScCRQkECyEQ1fTA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">最终目标</figcaption></figure><p id="0675" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了让我们更容易求导，我们在概率前面贴一个对数。此外，我们添加了一个负号，这样我们就必须“最小化”函数，而不是取最大值。我们还添加了(1/T ),使事情的规模不变，不依赖于语料库的大小。</p><p id="7827" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了训练这个模型，我们需要使用梯度下降来更新向量。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lt"><img src="../Images/b4e948c11426373134813fa9687dc193.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*Xxf7qhZddjG9NfGkYkrCZw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">使用梯度下降更新权重</figcaption></figure><p id="cfed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们举一个具体的例子，来看一下我们必须采取的每一个步骤，以通过模型的正向传递。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lf lg l"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">softmax的定义函数</figcaption></figure><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lf lg l"/></div></figure><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="90a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最初，文本被热编码&amp;其中一个单词被传递到网络。在这种情况下，<strong class="ig hi">‘Ease’</strong>作为中心词传递(按词汇排序，第二个词)，从句子中我们可以看到，Ease(带有一个<strong class="ig hi">大小为2 </strong>的窗口)的上下文词是<strong class="ig hi">‘with’&amp;‘town’</strong>。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lu"><img src="../Images/8f8f88539ad6453744ff4095f18f523c.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*0o2-XbZCZmdd2_zy_5oCDw.png"/></div></figure><p id="7c48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们随机初始化上下文和中心词矩阵V &amp; U。</p><p id="6fd4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">同样，<strong class="ig hi">V</strong>的每一列包含每个单词在<strong class="ig hi">中心</strong>时的嵌入，<strong class="ig hi">每行</strong>包含每个单词在<strong class="ig hi">上下文</strong>时的嵌入。例如，这是矩阵u的初始化。</p><pre class="jd je jf jg fd lv lw lx ly aw lz bi"><span id="3fa2" class="ma ju hh lw b fi mb mc l md me">#### U matrix as a dict with vocab as the key</span><span id="f115" class="ma ju hh lw b fi mf mc l md me">{'dangerous': array([ 1.2398147 , -0.87909626,  0.38028566]),  'ease':       array([0.08540021, 0.18544306, 0.07096524]), <br/>'man':        array([ 0.14981179, -1.40251029,  1.02452393]),  <br/>'passed':     array([0.10055589, 1.09979612, 1.63213385]),  <br/>'the':        array([-0.22917658,  1.32505777, -0.74246023]), <br/>'through':    array([0.21855831, 1.05448857, 0.71948246]),  <br/>'town':       array([-0.17041045, -1.60960955, -0.56273196]),  <br/>'with':       array([-1.52561797,  0.00701094, -0.46619808])}</span></pre><p id="c2ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们向前传递一个单词到网络之后，我们接收预测每个上下文单词的softmax概率。</p><pre class="jd je jf jg fd lv lw lx ly aw lz bi"><span id="9c3f" class="ma ju hh lw b fi mb mc l md me">##Predicted probability distribution</span><span id="82fe" class="ma ju hh lw b fi mf mc l md me">{'dangerous': 0.0006688799877958782,  <br/>'ease':       0.024971817920074583,  <br/>'man':        0.004299169023935694,  <br/>'passed':     <strong class="lw hi">0.9438920599330834</strong>, <br/> 'the':       0.004081941089464888,  <br/>'through':    0.0027767995527184113,  <br/>'town':       0.009752776322407003,  <br/>'with':       0.009556556170520179}</span></pre><p id="414b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过我们的胡言乱语正常初始化，网络以0.94的概率预测<strong class="ig hi">通过</strong>为上下文中概率最高的单词，中心为“ease”。但是真正的分布，对于中间的单词ease，对于with和town，概率为1，而对于其他任何地方，概率为0。</p><pre class="jd je jf jg fd lv lw lx ly aw lz bi"><span id="c10d" class="ma ju hh lw b fi mb mc l md me">###True Distribution</span><span id="36f0" class="ma ju hh lw b fi mf mc l md me">{'dangerous': 0,<br/> 'ease': 0,<br/> 'man': 0,<br/> 'passed': 0,<br/> 'the': 0,<br/> 'through': 0,<br/> <strong class="lw hi">'town': 1,<br/> 'with': 1</strong>}</span></pre><p id="32f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以我们比较并训练我们的单词向量U &amp; V，直到我们的输出最终看起来像真实的分布。我将忽略向后传递的细节，因为我相信在我们目前拥有的框架中自动微分是可用的，它真的没有那么重要。但是如果你感兴趣，你可以在这里查看它是如何工作的<a class="ae js" href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="4254" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于word2vec，这种方法还有另一个问题，每次我们都必须遍历整个vocab，正如前面讨论的那样，这在计算上不是很大。如果你考虑复杂性，它的数量级是O( V + V ),其中V是你词汇量的大小。</p><p id="23d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们用所谓的<em class="li"> </em> <strong class="ig hi"> <em class="li">负采样</em> </strong>来解决这个问题，这在上面提到的 <strong class="ig hi"> <em class="li">的第二篇<a class="ae js" href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">论文中有介绍。</a></em> </strong>我们不使用softmax函数，而是使用sigmoid函数，将其转化为二项式分类问题(2类，正&amp;负)。</p><p id="2cd5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们从不在上下文中的单词中取样，并把它们作为否定类别。在我们运行的中心词“ease”的例子中，我们采样了其他一些词，如“the ”,并将其用作否定类。我们从这些类中抽取K个样本。因此，我们得到的复杂度不是更新整个词汇表，而是~ O (K)。</p><h1 id="d51d" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">结果</strong></h1><p id="d8f2" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">如果你想在自己的语料库上训练它们，在深度学习框架上相当简单。最初的word2vec论文的模型是根据谷歌的新闻数据训练的。后来，多年来，人们在维基百科数据集上训练他们的模型，并显示出更好的结果。这可能是因为维基百科中的单个文档有更多与某个主题相关的单词，而在新闻中，我们可能有更多不同的单词。尽管如此，我将在本节中使用原始的单词向量。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lf lg l"/></div></figure><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="89db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们查看训练过的单词向量，例如国王、王后、猫、狗和动物。然后我们用奇异值分解对向量进行二阶近似。(您可以从我以前的文章<a class="ae js" rel="noopener" href="/analytics-vidhya/why-you-should-consider-studying-linear-algebra-as-a-data-scientist-part-1-svd-9d0ba3189a3e">这里</a>了解更多关于SVD的信息)</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mg"><img src="../Images/acead62c0e48b96359fb7da0dcd36f2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*FrRH9Tg0GaXH_wFU9Q-CtA.png"/></div></figure><p id="40e8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以看到，即使是第二等级的近似词，如动物、狗和猫聚集在一起，而国王和王后更接近。尽管向量(最初是300维)到2维的投影使我们失去了很多信息，但我们可以多少想象出一些向量是如何相似的。</p><p id="9c28" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以使用完整的向量来找出一些单词的类比和相似之处。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="400c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">得出的一些相似之处如下:</p><pre class="jd je jf jg fd lv lw lx ly aw lz bi"><span id="6bd0" class="ma ju hh lw b fi mb mc l md me">#Analogy<br/>similarity(w['king']-w['man']+ w['woman'],w['queen']) ---&gt; 0.73 </span><span id="1ad4" class="ma ju hh lw b fi mf mc l md me">#Dissimilar words<br/>similarity(w['king'], w['car'])  ---&gt; 0.03</span><span id="d4fc" class="ma ju hh lw b fi mf mc l md me">#Similar words<br/>similarity(w['man'], w['woman']) ---&gt; 0.76</span></pre></div></div>    
</body>
</html>