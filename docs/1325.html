<html>
<head>
<title>Beginner-friendly scraping with BeautifulSoup</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">初学者友好的刮削与美丽的Soup</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/beginner-friendly-scraping-with-beautifulsoup-8e0642cbf3ec?source=collection_archive---------28-----------------------#2021-02-23">https://medium.com/analytics-vidhya/beginner-friendly-scraping-with-beautifulsoup-8e0642cbf3ec?source=collection_archive---------28-----------------------#2021-02-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="ba91" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">新冠肺炎数据的列表抓取—使用worldometer作为源</h2></div><p id="d783" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Web抓取是数据科学最有趣的方面之一。能够对任何网站的数据进行自己的分析和预测使得这个领域更加令人兴奋！通过这个被称为网络搜集的过程从网站收集数据的范围是无限的。然而，这并不像在word文档中复制粘贴那么简单。但是多亏了python社区中的天才和大量的库，如Scrapy、Selenium和BeautifulSoup，这个过程并不太乏味。在本文中，我们将探讨如何使用BeautifulSoup从worldometer中抓取新冠肺炎数据。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es js"><img src="../Images/bd71e31392f881aa36654162bb587b2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*REy3O6AK5vVfjZYj5FBb3w.jpeg"/></div></div><figcaption class="ke kf et er es kg kh bd b be z dx translated">来源:<a class="ae ki" href="https://pixabay.com/users/foodiefactor-5992183/" rel="noopener ugc nofollow" target="_blank"> FoodieFactor </a>，via <a class="ae ki" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank"> pixabay </a> (Pixabay许可)</figcaption></figure><p id="2c0c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在过去的一年里，新冠肺炎吸引了全世界的注意力。关于这种给世界带来巨大灾难的病毒，还有许多问题没有得到解答。因此，实际上任何一种分析都可以为我们自己和潜在的社会带来更好的理解和有用的见解。因此，我选择worldometer作为本指南的参考。</p><h1 id="f1e1" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">所以让我们开始吧！</h1><p id="58d6" class="pw-post-body-paragraph iw ix hh iy b iz lb ii jb jc lc il je jf ld jh ji jj le jl jm jn lf jp jq jr ha bi translated">让我们从导入必要的库开始。</p><pre class="jt ju jv jw fd lg lh li lj aw lk bi"><span id="c242" class="ll kk hh lh b fi lm ln l lo lp">import pandas as pd<br/>import numpy as np<br/>from bs4 import BeautifulSoup<br/>import requests<br/>import html</span></pre><p id="38d1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">取表有两种方法，其中一种简单得多，并且只对少数人有效，但对我无效。<strong class="iy hi"> <em class="lq"> read_html() </em> </strong>方法用于直接从网站获取表格，不需要太多额外的工作。这对于维基百科的表格来说很容易。如果你的目标是转向分析，而不是把精力花在学习如何刮擦上，这是值得一试的。如果您想跳过前面的步骤并快速获取数据，请向下滚动到备选项2。请注意，本文的目标是用BeautifulSoup探索传统的刮擦方式，我将它称为“替代方案1”。</p><p id="859f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们首先将我们的网站保存到一个变量中，比如url。</p><pre class="jt ju jv jw fd lg lh li lj aw lk bi"><span id="22f9" class="ll kk hh lh b fi lm ln l lo lp">url = '<a class="ae ki" href="http://www.worldometers.info/coronavirus/'" rel="noopener ugc nofollow" target="_blank">http://www.worldometers.info/coronavirus/'</a></span></pre><p id="0fa9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将通过使用请求礼貌地从worldometer获取Html内容。<strong class="iy hi"> <em class="lq">请求</em> </strong>用于从网站获取原始数据。</p><pre class="jt ju jv jw fd lg lh li lj aw lk bi"><span id="1da4" class="ll kk hh lh b fi lm ln l lo lp">r = requests.get(url)</span></pre><h2 id="a81e" class="ll kk hh bd kl lr ls lt kp lu lv lw kt jf lx ly kv jj lz ma kx jn mb mc kz md bi translated">备选方案1</h2><p id="a836" class="pw-post-body-paragraph iw ix hh iy b iz lb ii jb jc lc il je jf ld jh ji jj le jl jm jn lf jp jq jr ha bi translated">汤来了！使用下面的代码，我们将从保存在变量“r”中的响应中获取内容。<strong class="iy hi"><em class="lq">beautiful soup</em></strong><em class="lq"/>简而言之，帮助分离有用的信息，如链接、文本、标题等。从Html标签，然后可以用于进一步的分析。<strong class="iy hi"> <em class="lq"> lxml </em> </strong>特性是用来解释Html代码的解析器。</p><pre class="jt ju jv jw fd lg lh li lj aw lk bi"><span id="3140" class="ll kk hh lh b fi lm ln l lo lp">soup = BeautifulSoup(r.text,'lxml')</span></pre><p id="23d9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你试着调用变量soup，你会看到大量的信息。我们的目标是通过执行下面的代码，只从中获取相关的表格信息。</p><pre class="jt ju jv jw fd lg lh li lj aw lk bi"><span id="d8a2" class="ll kk hh lh b fi lm ln l lo lp">lst_crucial = [str(i) for i in list(soup.find_all('table')[0].find_all('td'))]</span></pre><p id="1374" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，我们这里有什么？上面的代码通过使用一个稍微复杂的列表理解，将我们的汤缩小到只包含数据帧所需的信息。可以尝试执行每个<strong class="iy hi"> soup.find_all('table') </strong>然后<strong class="iy hi">soup . find _ all(' table ')【0】</strong>然后<strong class="iy hi">(soup . find _ all(' table ')【0】。分别查找_all('td') </strong>以准确了解我们在这里做了什么。</p><p id="e5af" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> soup.find_all('table'): </strong>给出了网页上的所有表格，我们只关心第一个表格，这将我们带到</p><p id="8161" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">soup . find _ all(' table ')[0]:</strong>现在我们已经获得了所需的与表相关的所有信息，我们可以进一步将范围缩小到数据。我们不需要关于表格的样式等信息。所以我们将做如下</p><p id="3de0" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> (soup.find_all('table')[0]。find _ all(' TD '):</strong>“TD”是表数据，这是我们从整个soup中需要的唯一信息。注意，使用这段代码消除了“tr”和“th”标签？</p><p id="bfe7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">剩下的代码只是将相关数据的每个元素保存到一个我称之为“lst _ crucial”的列表中(因为这个列表构成了我们其余代码的基础)——以字符串的形式。当然，这也可以在for循环中完成，但是我发现列表理解更加有效和优雅。</p><p id="ea60" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">好吧！我们已经得到了我们需要的，让我们把列表转换成表格吧！为此，让我们从导入<strong class="iy hi"> re </strong>开始，然后创建一个空数据帧。<strong class="iy hi"> <em class="lq"> Re </em> </strong>是一个库，它使我们能够使用regex从字符串中找到必要的元素。</p><pre class="jt ju jv jw fd lg lh li lj aw lk bi"><span id="6ee2" class="ll kk hh lh b fi lm ln l lo lp">import re<br/>df = pd.DataFrame()</span></pre><p id="fe2d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在让我们开始创建列，并将列表中的相关数据添加到每个列中。</p><pre class="jt ju jv jw fd lg lh li lj aw lk bi"><span id="da15" class="ll kk hh lh b fi lm ln l lo lp">df['Countries'] = [''.join([re.findall('&gt;(.*?)&lt;', lst_crucial[i-13:i+1][0])[1]]) for i,v in enumerate(lst_crucial) if 'world-population' in v]</span></pre><p id="084d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">事实上，另一个列表理解，我们将在每一栏中增加12个，但都与这个非常相似。上面的代码是做什么的？如果你观察我们的lst _ crucial，你会注意到数据是一行一行写上去的，每一行都以国家的“总人口”结尾，每个国家都有一个以“世界人口”开头的href引用。我们的目标是通过提供此范围来导出列表中在此项目之前的13个项目，并包含此项目—<strong class="iy hi">“I-13:I+1”</strong>(“I”是第13个项目的索引，包含“世界人口”)。我们不希望我们的国家列列表的所有元素，而只是国家。于是有了<strong class="iy hi">re . find all(&gt;(。*?)&lt;'，lst _ critical[I-13:I+1][0])[1]])</strong>，我们实际上是在提取&gt; &lt;标签之间的所有元素，因为这是国家所在的位置，我们只从每行13个项目中的第一个项目进行提取，因此是<strong class="iy hi"> [0] </strong>，从&gt; &lt;之间的三组regex元素中提取，我们想要第二组，即国家，因此是<strong class="iy hi"> [1] 【T25</strong></p><p id="3b0d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于所有其他列，代码遵循类似的结构。下面的代码是为下一个专栏编写的。</p><pre class="jt ju jv jw fd lg lh li lj aw lk bi"><span id="137b" class="ll kk hh lh b fi lm ln l lo lp">df['Total_cases'] = [''.join([k for s in lst_crucial[i-13:i+1][1] for k in s if k.isdigit()]) for i,v in enumerate(lst_crucial) if 'world-population' in v]</span></pre><p id="53fa" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这段代码中，我们只是从组成一行的13个项目中的第二个项目中取出数字元素，因此是<strong class="iy hi"> [1] </strong> —回想一下，我们对countries列做了同样的事情，但是使用了[0]作为第一个元素。<strong class="iy hi"> <em class="lq">。join </em> </strong>“本质上是将每个数字连接成一个字符串，形成所需的数字。我们将继续对所有其他列进行同样的操作，因为它们都是数字。下面是最终的代码:</p><figure class="jt ju jv jw fd jx"><div class="bz dy l di"><div class="me mf l"/></div><figcaption class="ke kf et er es kg kh bd b be z dx translated">按作者嵌入</figcaption></figure><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es mg"><img src="../Images/ca1bd5150529838a3548c0912e95178f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/1*n3SICBCBoKO6Xx61K6nbhw.gif"/></div><figcaption class="ke kf et er es kg kh bd b be z dx translated">作者GIF</figcaption></figure><p id="3137" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">瞧啊！我们从worldometer获得了今天的完整数据框架，准备进行进一步的分析。</p><h2 id="9524" class="ll kk hh bd kl lr ls lt kp lu lv lw kt jf lx ly kv jj lz ma kx jn mb mc kz md bi translated">备选方案2</h2><p id="6442" class="pw-post-body-paragraph iw ix hh iy b iz lb ii jb jc lc il je jf ld jh ji jj le jl jm jn lf jp jq jr ha bi translated">除了上面的选择，我们可以尝试这个捷径，也许可行，也许不可行。我们将对之前保存在“<strong class="iy hi"> r </strong>”中的url内容调用pandas <strong class="iy hi"> read_html() </strong>方法。让我们将它保存为“dfs ”,因为内容中有多个数据帧。最后，调用<strong class="iy hi"> dfs[0] </strong>给我们第一个表，这个表对我们来说是相关的。</p><pre class="jt ju jv jw fd lg lh li lj aw lk bi"><span id="13c8" class="ll kk hh lh b fi lm ln l lo lp">r = requests.get(url)<br/>dfs = pd.read_html(r.text, attrs = {'id':'main_table_countries_today'})<br/>dfs[0]</span></pre><p id="700f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">以下是输出的一个片段。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es mh"><img src="../Images/1d2950a75459a5706f789f69371e443b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hwiQjEMcT95q7lhdTulcPg.png"/></div></div><figcaption class="ke kf et er es kg kh bd b be z dx translated">作者照片</figcaption></figure><p id="c6b4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这就是目前的全部内容，请继续关注关于covid数据分析的另一篇文章</p></div></div>    
</body>
</html>