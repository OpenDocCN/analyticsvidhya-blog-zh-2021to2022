# 构建 TFIDF 模型的步骤。

> 原文：<https://medium.com/analytics-vidhya/natural-language-processing-the-steps-to-building-a-tfidf-model-11b480e037bb?source=collection_archive---------8----------------------->

自然语言处理是机器学习的广泛应用之一。这是机器学习的子域，旨在过滤和分析大量文本“文档”(这是该领域的普遍叫法)，并使用分类/回归算法进行预测。

在这里，我们将探索机器学习领域采用的经典模型之一:TFIDF(术语频率-逆文档频率模型)。但是这些技术术语是什么呢？为什么它们与分类相关，例如对电子邮件是否是垃圾邮件进行分类？让我们看一看。

**NLP 算法的基础**

NLP 算法的主要焦点是仅在提供给它的文本语料库上进行预测。例如，如果我们有一个由许多列组成的数据框和一个文本列，NLP 算法将只对包含文本数据的列执行操作。这包括删除不必要的标点符号，并将俚语词(如果正在分析短信)转换为算法可以解码并进一步使用的词。

这与可能占用数据帧的一些相关特征(列)来进行预测的其他算法形成对比。例如，一个线性回归模型在产生输出之前，可以将许多数字列作为输入。

然而，自然语言处理也可以与诸如随机森林分类器之类的其他算法结合使用，只要数据已经被转换成与那些算法兼容的格式。

现在让我们了解一种简单的电子邮件分类方法，通过这种方法，我们可以开始初步了解 NLP 算法是如何工作的。

**将伪造文本转换成机器可以理解的格式的主要步骤:**

电子邮件可以是各种类型的。它们可以是官方的电子邮件、公告、给爱人的信，或者只是让我们彼此聊天的电子邮件。随之而来的是各种各样的文本格式。非官方的电子邮件涉及许多速记单词，如“u”、“lyk”、“2day”。ML 算法用来过滤出最常见的单词如“this”、“that”、“me”、“you”的字典不包含这些单词的简写。(我们将探究为什么我们需要过滤掉像这样的单词来做出最准确的预测)。因此，第一步可能包括，**清理文本，删除速记单词和不必要的标点符号**。

在继续之前，有必要了解 TFIDF 模型背后的数学原理以及它的实际使用方式。

**词频—逆文档频率**

让我们来理解为什么 TFIDF 模型是被广泛接受的邮件分类模型之一。

TFIDF 模型的目标是使用文本中每个单词的频率，并预测单词组合的频率是否更有可能将一段文本标记为垃圾邮件或真实文本。这可以借助以下数据来理解:

![](img/c389f9c7c5d4ab2b883d95ee90068bd2.png)

普通电子邮件中最常见单词的频率直方图。信用:mygreatlearning.com

上面的直方图描述了在被视为正常(非垃圾)电子邮件的电子邮件中最常出现的单词的频率。可以观察到‘亲爱的’这个词出现的次数最多，而‘钱’出现的次数最少。

![](img/3f0ab98446e86c92a60218ef6821a6ea.png)

垃圾邮件中最常见单词的频率直方图。信用:mygreatlearning.com

上面的直方图清楚地描述了最有可能被归类为垃圾邮件的电子邮件将包含单词“Money ”,而“Lunch”出现了 0 次。

这是使用 TFIDF 模型的基础。它主要根据给定文本中单词的频率来预测输出。

注意:这里我们将使用单词“document”来指代一组文本。一个特定的文档可以代表一封电子邮件，甚至是一条文本消息。

**词频**:文档中给定单词的计数。它使用一个非常简单和直观的公式来计算计数。文档中任何单词的词频是该文档中单词的总数除以该文档中单词的总数。

它基本上告诉我们，一个单词与给定的文档有多相关。

**逆文档频率**:逆文档频率基本上可以看做是一种工具，用来过滤掉那些最常用作连词，在分类时不提供任何有价值信息的词。例如，单词‘and’，‘or’，‘this’，‘you’是一些由于使用频率高而获得较大 TF 分数的单词。然而，我们看到它们实际上并没有提供很多与分类相关的信息。

因此，这些词可能会在 TF 中获得高分，但当它乘以 IDF 值时，它会降低不提供相关信息的词的因子。

IDF 通常通过计算特定单词在一组文档中的频率，然后取其倒数的对数来计算。

在这种情况下，我们使用一组文档而不是单个文档的原因是因为在词汇使用中常见的单词(如上所述)可能会在许多文档中多次出现。这将提供这样的信息，那个单词可能只是一个连词，它的高频率可能只是它在语法中频繁使用的结果。

下图是一个简单但直观的示例，用于根据真实数据理解 TFIDF。

![](img/cb9f056833c569f3e1eae6d202ae0bce.png)

图片来源:[https://www.kirenz.com/post/2019-09-16-r-text-mining/](https://www.kirenz.com/post/2019-09-16-r-text-mining/)

假设我们从尼古拉·特斯拉或阿尔伯特·爱因斯坦的书中得到一页文字，我们要建立一个模型来预测作者的名字。这将是一个分类任务，因为我们必须将其分为两类(两个作者)。上图清楚地显示，与尼古拉·特斯拉或阿尔伯特·爱因斯坦的兴趣领域紧密相关的单词获得最高的 TFIDF 分数。例如，爱因斯坦研究引力的本质和相对论，而特斯拉更倾向于电磁学。因此，“参考”、“相对论”等词在爱因斯坦的著作中更常见，而“灯泡”、“线圈”等词在特斯拉的著作中更常见。

另一方面，像“一个”、“基于”、“两个”、“许多”这样的词并不具体指出与作者本人相关的任何事情，因此在 TF-IDF 等级中排名相对较低。

现在我们对 TFIDF 有了一个基本的概念，让我们看看如何创建一个单词包(BoW)来实现 TFIDF。

**词汇袋模型(文本预处理)**

单词袋(BoW)模型是一种常用的模型，它基本上从提供给它的一组词汇中学习，并从中创建一个*单词袋*，该单词袋可进一步用于数学运算，如创建向量。

单词袋模型然后包含在*整个文档集*中的一组唯一单词以及每个单词的出现频率。

创建这个模型后，可以用它对任何消息进行矢量化，以数字格式表示它。

例如，考虑两条消息:

留言 1:今天阳光明媚，我们去喝杯咖啡吧！

信息 2:我更喜欢在星期二喝一杯美式咖啡，但是一杯浓缩咖啡也可以！

过滤后，我们得到一个最相关的单词列表，如下所示:

相当

快活的

去

杯子

咖啡

更喜欢

美式咖啡

每星期二

浓咖啡

作品】。

因此，第一条消息可以编码为:[1，1，1，1，0，0，0，0]

第二个是:[0，0，0，2，0，1，1，1，1]

这就创建了我们的单词包模型，TfidfTransformer 可以使用它了。

这里还需要注意的是，由于数据集中可能有一个巨大的单词列表，并且所有的消息将**而不是**包含所有的单词。因此，向量中的大部分位置都是零。因此，这些信息存储在单词袋模型中的方式是使用一个*稀疏矩阵。*

![](img/372558d80fdcd6e70f2a4672b639f24a.png)

一个单词和信息如何存储在稀疏矩阵中的简单表示。

**在单词包上实现 tfi df**

我们在单词包模型上创建的向量现在可以被 TFIDF 转换器使用了。基本上，它帮助我们得到每个向量的 TFIDF 值。

其实现方式是，接收*每个单独的向量*，然后计算该向量中每个单词的 TFIDF 分数。

注意，每个向量代表一个文档(一段单独的文本，如消息)。

向量包含特定单词在向量中出现频率的信息，我们还有稀疏矩阵，它告诉我们该单词在文档集中出现的频率。

然后利用该信息来计算每个单词的 TFIDF 得分。这就产生了一个新的矢量。(在 Python 中实现时，它返回一个序列，其中包含单词的位置及其 TFIDF 分数)。它看起来会像这样:

![](img/4e6dde53f314269a949ea2f87a348be7.png)

TFIDF 为一袋单词打分。第一列代表每个单词在矩阵中的位置，右边的一列代表它的 TFIDF 得分。

我们同样可以找出文档集中所有单词的 TFIDF 得分，这将是训练我们模型的下一步。

# **使用朴素贝叶斯算法进行分类**

一旦我们为整个模型创建了 TFIDF 分数，我们就可以最终使用它来训练模型并进行预测。

具体来说，该模型将做的是，它将获取任何单词的 TFIDF 分数，并将其预测为 ham(“正常邮件”)或垃圾邮件。

实现这一点的方法是使用朴素贝叶斯分类器，该分类器使用概率中的贝叶斯定理。

让我们简要地看一下朴素贝叶斯分类器所使用的贝叶斯算法。

贝叶斯算法的公式如下:

![](img/8063493a7d5c71e4f9c6591ccde689a6.png)

一个很好的例子展示了概率中的贝叶斯定理。演职员表:[https://blogs . Oracle . com/ai-and-data science/post/贝叶斯推理导论](https://blogs.oracle.com/ai-and-datascience/post/introduction-to-bayesian-inference)

贝叶斯定理作用于*条件概率的原理。*具体来说，它的工作是在另一个事件 B 已经发生的情况下，预测一个事件 A 发生的概率。它使用上面的公式进行计算。

公式中使用的符号如下:

P(A|B):给定 B 已经发生，A 发生的概率。

P(B|A):给定 A 已经发生，B 发生的概率。

P(A):事件 A 发生的概率。

P(B):事件 B 发生的概率。

现在，这种用于将数据点(我们的文本/电子邮件)分类为一个类别(垃圾邮件)的方法是，它获取每个文本的特征，并计算它属于每个类别的概率*。*

具有较高概率的类将作为数据点的选定类。

还有 V *oila！*我们现在可以成功地将每封邮件分类为垃圾邮件。

如今，谷歌最受欢迎的消息工具 Gmail 也内置了谷歌魔法(他们喜欢称之为 T1)，可以判断一封电子邮件对用户来说是否重要(T2)。例如，假设您填写了一份申请表，并希望通过电子邮件收到回复。收到回复后，Gmail 会用“重要”标签对其进行标记。如果你将鼠标悬停在标签上，它会告诉你谷歌认为这对你很重要。

这就是自然语言处理的实际应用，它可以帮助你从每天收到的成千上万封邮件中过滤出最重要的邮件，并将不相关的邮件发送到垃圾邮件文件夹。

这是给初学者的 NLP 简介。如果有帮助，一定要让我知道！