# 解决机器学习问题:特征选择(第四部分)

> 原文：<https://medium.com/analytics-vidhya/solve-machine-learning-problem-feature-selection-part-4-cadd00e7da56?source=collection_archive---------16----------------------->

![](img/b25c9bbf8fa7c619711f6c81f79d6aa2.png)

# 介绍

在建立机器学习模型时，数据集中的所有变量对建立模型都有用的情况几乎是罕见的。特征选择是从所有特征中选择相关特征子集的过程，用于建模。

如果我们有几个特征，那么它很容易解释模型，不太可能过度拟合，但它会给出较低的预测精度。
如果我们有更多的特征那么就很难解释模型，更有可能过拟合，它会给出很高的预测精度。
它有助于我们在不损失太多信息的情况下降低特征的维数。

# **过滤方法**

过滤方法通过单变量统计而不是交叉验证性能来提取特征的内在属性。这些方法比包装方法速度更快，计算成本更低。当处理高维数据时，使用过滤方法在计算上更便宜。

## 方差阈值

方差阈值是特征选择的简单基线方法。它移除方差不满足某个阈值的所有特征。默认情况下，它会移除所有零方差要素，即在所有样本中具有相同值的要素。我们假设具有较高方差的特征可能包含更多有用的信息，但是请注意，我们没有考虑特征变量或特征和目标变量之间的关系。

![](img/bf7f50627f8c969b0cd592bc48788c55.png)

我们可以看到应用方差阈值方法前后特征数量的变化。

## 单变量选择方法

单变量特征选择方法通过基于单变量统计测试(如 ANOVA)选择最佳特征来工作。它可以被看作是估计器的预处理步骤

1.  **SelectKBest** 该方法根据 k 个最高分选择特征。例如，我们可以对样本执行 mutual_info_regression 测试，以便只从数据集中检索最佳特征。

输入返回单变量分数和 p 值(或仅返回 SelectKBest 和 SelectPercentile 的分数)的评分函数
**chi2** 当它仅包含与类相关的非负特征(如布尔或频率)时使用。
**f_regression** 用于具有连续变量的回归当变量是分类变量时，我们使用**f _ class if
mutual _ info _ class if**它适用于分类模型的连续和离散变量
**mutual _ info _ regression**它适用于回归模型的连续和离散自变量

![](img/e6382aad8dfc685cdf0a4091346c609a.png)

我们使用 mutual_info_regression 作为评分函数，并提取前 4 个特征。

**2。SelectPercentile** 根据最高分数的百分位数选择特性。使用评分函数返回单变量分数和 p 值。

![](img/d284a5743bee0abfc352767ec036cd83.png)

我们使用 mutual_info_regression 作为评分函数，并提取前 3 个百分位数的特征。

# 带热图的关联矩阵

相关性是对两个或更多变量的线性关系的度量。通过相关性，我们可以从一个变量预测另一个变量。使用相关性进行特征选择背后的逻辑是好的变量与目标高度相关。此外，变量应该与目标相关，但它们之间应该不相关。

![](img/7a2da3263d6fcf1b5bc2450c613907a0.png)

如果两个变量相关，我们可以从另一个预测一个。因此，如果两个特征相关，模型实际上只需要其中一个，因为第二个特征不会添加额外的信息。

![](img/a5ca6c9eb92b934636867279690535f0.png)

# **嵌入方法**

## 套索正则化

正则化包括向机器学习模型的不同参数添加惩罚，以降低模型的自由度，即避免过度拟合。在线性模型正则化中，惩罚应用于乘以每个预测值的系数。从不同类型的正则化，拉索或 L1 的属性，能够缩小一些系数为零。因此，可以从模型中删除该特征。

![](img/0a8537f2a5b00dc0a2c06bda13b79f0e.png)![](img/6093d93b7e14d106dbce46a5d8a67dbe.png)

我们可以看到特征系数减少到零。套索后最初特征数 284 特征数 255。

如果惩罚太高，重要的特征被删除，我们会注意到算法的性能下降，然后意识到我们需要减少正则化。

# 随机森林重要性

随机森林是一种 Bagging 算法，它聚集了指定数量的决策树。随机森林使用的基于树的策略自然地根据它们提高节点纯度的程度来排序，或者换句话说，根据所有树的杂质(基尼杂质)的减少程度来排序。杂质减少最多的节点出现在树的开始，而杂质减少最少的节点出现在树的结尾。通过修剪特定节点下的树，我们可以创建最重要特征的子集。

当训练一个树时，可以计算每个特征减少杂质的程度。一个特征减少杂质越多，该特征就越重要。在随机森林中，每个特征的杂质减少可以跨树进行平均，以确定变量的最终重要性。

![](img/d2334c79206d8d8e66dda34ef13c52c9.png)

默认情况下，SelectFromModel 将选择重要性大于所有特性的平均重要性的那些特性，但是我们可以根据需要更改该阈值。

![](img/23d40b8655a56b3971f678561d12c502.png)

在上面的图片中，我们看到了执行随机森林前后的特征数量。

# 结论

我们已经讨论了一些特征选择的技术。特征选择是一个广泛而复杂的领域，已经进行了大量的研究来找出最佳的方法。这取决于机器学习工程师来组合和创新方法，测试它们，然后看看什么对给定的问题最有效。