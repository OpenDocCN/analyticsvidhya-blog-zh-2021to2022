<html>
<head>
<title>Get to know your k-Nearest Neighbor</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解你的k近邻</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/get-to-know-your-k-nearest-neighbor-821029bf19a6?source=collection_archive---------21-----------------------#2021-01-07">https://medium.com/analytics-vidhya/get-to-know-your-k-nearest-neighbor-821029bf19a6?source=collection_archive---------21-----------------------#2021-01-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="71b4" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">简单有效</h1><p id="7df0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">k-最近邻算法(k-NN)是一种受监督的、基于实例的非参数算法，它利用特征空间中的k个最近的实例。受监督意味着它需要输入标签(目标值)和训练数据，以便学习一个函数，该函数将基于过去的配对从输入充分生成未来的输出。基于实例转化为使用特定训练集中实例之间的比较，而不是用于未来数据的泛化。最后，非参数表示算法的自力性，因为其结构是基于现有数据及其内部关系推断的，而不需要大量使用预先确定的模型参数。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es ka"><img src="../Images/fabc618b2013cd5ef02e8ebcab9f8c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/0*-NxoLmtIg3Ar76_9"/></div><figcaption class="ki kj et er es kk kl bd b be z dx translated">我们的穷圈属于哪里？</figcaption></figure></div><div class="ab cl km kn go ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ha hb hc hd he"><h1 id="c225" class="ie if hh bd ig ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb bi translated">何时使用它</h1><p id="b708" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">它既可以用于回归，也可以用于分类。</p><ul class=""><li id="0d51" class="ky kz hh je b jf la jj lb jn lc jr ld jv le jz lf lg lh li bi translated">分类:每个对象被分配到其k个最近邻中最常见的类别。</li><li id="7445" class="ky kz hh je b jf lj jj lk jn ll jr lm jv ln jz lf lg lh li bi translated">回归:代替对象本身，输出由一个值组成，该值是其k个最近邻居的所有值的平均值。</li></ul></div><div class="ab cl km kn go ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ha hb hc hd he"><h1 id="94c5" class="ie if hh bd ig ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb bi translated">为什么使用它</h1><p id="96d0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">由于它倾向于对拓扑上接近的对象进行分离和重新分组，因此对于多个类非常有用。因此，它可以通过生成数据集的近似子组来整理数据集，这反过来使分析师能够更好地了解数据类别，同时在可视化聚类后提供关于聚类的大量密度信息。</p></div><div class="ab cl km kn go ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ha hb hc hd he"><h1 id="80d1" class="ie if hh bd ig ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb bi translated">什么是k？</h1><p id="ed79" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">如上所述，k代表了一个预先确定的度量，即在决定将我们的对象分类到哪里时，我们将考虑多少个相近的例子。之后，下一个大问题是‘多少‘k’是好的？坦率地说，试错法(几乎)总是一个强有力的盟友。记住直觉，即必须消除方差，以便离群值不会影响结果(小“k”)，同时考虑平衡，不让一组特定的对象支配该领域(大“k”)，这有助于确定最佳解决方案的模式。</p></div><div class="ab cl km kn go ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ha hb hc hd he"><h1 id="91cf" class="ie if hh bd ig ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb bi translated">距离</h1><p id="4f0e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">存在几种数学方法来执行该算法，并且它们的区别基于距离度量。下面是三种最常用的:</p><ul class=""><li id="785c" class="ky kz hh je b jf la jj lb jn lc jr ld jv le jz lf lg lh li bi translated"><strong class="je hi">Manhattan</strong>:<strong class="je hi">| x1 x2 |+| y1 y2 |</strong>，本质上是为点a1的坐标为(x1，y1)和点a2的坐标为(x2，y2)的平面定义的简单形式，后者代表前者的最近邻。</li><li id="2b40" class="ky kz hh je b jf lj jj lk jn ll jr lm jv ln jz lf lg lh li bi translated"><strong class="je hi">闵可夫斯基</strong>:它的基本原理是可以用长度表示为向量的距离。以地图为例，我们可以看到，如果我们测量城市之间的距离，就不能有任何负值——方向只有正值，因此向量是范数——长度是我们测量的基础，多个城市可以连接在一起形成复杂的路线。本质上，这个宏观的例子可以缩小为数据点。</li></ul><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lo"><img src="../Images/83cd361e78e1ff970b1547a1fa311b75.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/0*8bwrzXe3_D6h0hUL"/></div></figure><ul class=""><li id="7346" class="ky kz hh je b jf la jj lb jn lc jr ld jv le jz lf lg lh li bi translated"><strong class="je hi">欧几里德</strong>:经典表述方式:两点坐标的绝对数值差(这里分别为‘p’和‘q ’)等于它们的距离。</li></ul><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lp"><img src="../Images/350e1b43c61e5990cf826a9392bc14dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/0*hbzh12l2m-DFWyK5"/></div></figure><blockquote class="lq lr ls"><p id="800a" class="jc jd lt je b jf la jh ji jj lb jl jm lu lv jp jq lw lx jt ju ly lz jx jy jz ha bi translated"><strong class="je hi">更高维度的欧几里德变异</strong>。</p></blockquote><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es ma"><img src="../Images/39a0b80fcf41fabcdadd52d0d908c6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/0*5d_flbdzppPc0FNN"/></div></figure></div><div class="ab cl km kn go ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ha hb hc hd he"><h1 id="346f" class="ie if hh bd ig ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb bi translated">结论</h1><p id="6e0f" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这种有效的算法是我们大学数据科学课上最先教授的算法之一，主要是因为它的重要性，其次是因为它易于理解。总之，它是每个数据科学家的工具箱中的一个有用的补充，并且仍然在各种项目中扮演着重要的角色，因此了解它是有好处的。</p></div></div>    
</body>
</html>