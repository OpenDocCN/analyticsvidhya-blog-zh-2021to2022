<html>
<head>
<title>Introduction to Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-apache-spark-b2d4ff8aacec?source=collection_archive---------14-----------------------#2021-06-20">https://medium.com/analytics-vidhya/introduction-to-apache-spark-b2d4ff8aacec?source=collection_archive---------14-----------------------#2021-06-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/ccf717b10902fe6215ecd093ca4701f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HZ1Z0YzCq-ZFqNlb"/></div></div></figure><div class=""/><p id="db81" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当今世界运行于数据之上，每天都会产生数万亿字节的数据。随着大数据系统成为必需品，传统系统一夜之间变得过时。虽然我们可以说大多数组织不处理如此大规模的数据，但对更好的数据管理的需求却在不断增长，以便组织能够快速扩展全球标准。</p><p id="22c9" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">SQL表正在被NoSQL或图形数据库所取代。熊猫和Numpy数据框架现在正在成为大数据框架的一部分。Hadoop、Spark和Hive正在成为行业的必需品。Apache Spark是最常用、也是最受推荐的大数据框架之一。在我们理解Apache Spark之前，了解一些基本系统是很重要的。</p><h1 id="2d3f" class="jn jo hs bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">什么是MapReduce？</h1><p id="d6c5" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">MapReduce是一种编程模型，用于处理支持并行处理的大型数据集。需要注意的是，在一些用例中，并行处理不起作用，在这种情况下，Map Reduce和Apache Spark都没有用。</p><p id="7bc8" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">MapReduce是Hadoop生态系统和Spark中的主要组件之一。它旨在通过将工作拆分为处理较小数据集的独立任务来支持大量数据的并行处理。MapReduce从用户那里获取整个数据集，将其分割成更小的任务(MAP)，并将它们分配给worker节点。一旦所有工作节点成功地完成它们的每个独立任务，来自独立活动的结果被聚集(减少)并作为对应于整个数据集的单个结果返回。通常，Map和Reduce函数是用户定义的函数，用于解决代码试图解决的业务用例。</p><figure class="kr ks kt ku fd hj er es paragraph-image"><div class="er es kq"><img src="../Images/9eadf16135d4dea7bd68486748018333.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*HKrnfV6hJeFa2kxRgzA1LQ.jpeg"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">MapReduce正在工作。(图片来自tutorialspoint.com)</figcaption></figure><h1 id="05cc" class="jn jo hs bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">什么是火花？</h1><p id="3683" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">Apache Spark是一个通用的集群计算系统。像MapReduce一样，它与一组计算机(节点)一起工作，并行处理工作并改善响应时间。然而，与MapReduce不同，Spark集群具有内存中的性质。内存中特性是一个允许Spark集群在节点上缓存数据的特性，而不是每次都从磁盘中获取数据。由于数据量巨大，通常需要很长时间的读写操作现在对于每个节点都是一次性操作，从而节省了时间并提高了处理速度。</p><p id="dd59" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">数据的内存处理是使用<strong class="ir ht">弹性分布式数据集</strong> (RDD)完成的。用户必须指定操作，rdd跨所有节点在内存中分发数据和执行操作。</p><p id="52e6" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Spark生态系统设计为两层——Spark核心，第二层是一个库和API包。</p><figure class="kr ks kt ku fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es kz"><img src="../Images/aa3f09fcff025e2b0f8825299bce43ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KIbmwoHnhJgekSqCTSuKnA.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">databricks.com解释的星火生态系统。</figcaption></figure><p id="29d8" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Apache spark不提供集群管理或存储管理工具。通常，人们使用YARN进行集群管理，并将分布式数据存储在Hadoop文件系统(HDFS)或AWS的S3系统上。Spark还有一个名为SPARK ENGINE的计算引擎，它负责将任务分成更小的任务，调度任务进行并行处理，向集群提供数据，并报告故障。它也是中间管理层，与集群管理器和数据管理器交互。</p><p id="a9b2" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Apache Spark核心API——在R、SQL、Python、Scala和Java中可用——最初用于编写数据处理逻辑。这些API基于rdd，缺少一些性能优化器。然而，由于没有额外的开销，它们也为用户提供了最大程度的定制和灵活性，以根据组织的需求进行编码。</p><p id="6ab6" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了克服核心API的缺点并提供更有针对性的支持，引入了基于核心API的第二层Apache Spark。第二层通常分为4组逻辑APIs库:</p><ol class=""><li id="da40" class="la lb hs ir b is it iw ix ja lc je ld ji le jm lf lg lh li bi translated">SparkSQL和Dataframes。<br/>这允许用户将SQL命令应用于Spark数据帧。它们主要用于结构化和半结构化数据。</li><li id="ccd2" class="la lb hs ir b is lj iw lk ja ll je lm ji ln jm lf lg lh li bi translated">流式传输。<br/>这些API用于处理连续传入的无界数据流。</li><li id="c1e5" class="la lb hs ir b is lj iw lk ja ll je lm ji ln jm lf lg lh li bi translated">Mllib。<br/>这个库用于支持所有可以部署在Spark框架上的机器学习活动。</li><li id="72ff" class="la lb hs ir b is lj iw lk ja ll je lm ji ln jm lf lg lh li bi translated">GraphX。<br/>这是一个允许将图形处理算法应用于可用数据集的库。</li></ol><h1 id="6359" class="jn jo hs bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">Spark vs Hadoop</h1><p id="e9e7" class="pw-post-body-paragraph ip iq hs ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">一个常见的问题是——当MapReduce已经是Hadoop的一部分时，为什么还要使用Spark？或者——当Spark构建在Hadoop生态系统之上时，它的优势是什么？Spark和Hadoop之间的一些关键区别是:</p><ul class=""><li id="6c57" class="la lb hs ir b is it iw ix ja lc je ld ji le jm lo lg lh li bi translated"><strong class="ir ht">性能</strong> —与Spark相比，Hadoop相对<em class="lp">慢</em>，因为磁盘操作。Spark的速度<em class="lp">更快</em>，由于其内存特性，更适合实时分析。Spark在内存中快100倍，在磁盘上快10倍。但是，当其他需要资源的服务正在运行时，执行时间可能会减少。</li><li id="9ff6" class="la lb hs ir b is lj iw lk ja ll je lm ji ln jm lo lg lh li bi translated"><strong class="ir ht">数据处理</strong> — Hadoop只批量处理数据<em class="lp">即。</em> <em class="lp">顺序</em>分步加工。Spark以<em class="lp">批处理、实时和图表形式处理数据。</em></li><li id="0c2a" class="la lb hs ir b is lj iw lk ja ll je lm ji ln jm lo lg lh li bi translated"><strong class="ir ht">机器学习</strong> — Hadoop使用Mahout进行代数运算，并有<em class="lp">稀缺的ML库</em>支持。Spark有一个<em class="lp">大型ML库</em>用来构建管道和执行超参数调优。</li><li id="b62b" class="la lb hs ir b is lj iw lk ja ll je lm ji ln jm lo lg lh li bi translated"><strong class="ir ht">易用性</strong>—Hadoop MapReduce<em class="lp">没有交互模式，需要大量编码。</em>另一方面，Spark有很多高级API，具有<em class="lp">更少的编码和交互模式。</em></li><li id="2b95" class="la lb hs ir b is lj iw lk ja ll je lm ji ln jm lo lg lh li bi translated"><strong class="ir ht">成本</strong> — Hadoop比<em class="lp">便宜</em>，因为它需要更多的磁盘内存。Spark需要更大的内存来运行，这使得它更加昂贵。</li><li id="17db" class="la lb hs ir b is lj iw lk ja ll je lm ji ln jm lo lg lh li bi translated"><strong class="ir ht">容错</strong> — Hadoop是容错的<em class="lp">，因为它跨多个节点复制数据</em>。Spark RDDs是容错的<em class="lp">，因为它们在内存中。</em></li></ul><p id="01cb" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">总的来说，当系统需要更便宜、与时间无关和更容错时，就选择Hadoop。当算法迭代且需要交互式数据处理或机器学习时，会选择Spark。当预期结果是实时的时，Spark也有优势。</p><p id="fd00" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">既然对Spark生态系统及其明显优势的简要介绍已经完成，在以后的文章中，我将描述SparkSQL和PySpark等概念。</p></div><div class="ab cl lq lr go ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ha hb hc hd he"><p id="c7a6" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢您阅读本博客。如果你喜欢这篇文章，请留下掌声。如果对如何提高我的工作质量有什么建议，我想听听。:)<br/>欢迎对下一篇博客的主题提出建议。:D </p></div></div>    
</body>
</html>