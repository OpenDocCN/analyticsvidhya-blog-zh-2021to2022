<html>
<head>
<title>Apache Beam — From Zero to Hero Pt. 2: Streaming Pipelines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">阿帕奇光束-从零到英雄Pt。2:流式管道</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/apache-beam-from-zero-to-hero-pt-2-streaming-pipelines-2aa9b53ab387?source=collection_archive---------9-----------------------#2021-02-23">https://medium.com/analytics-vidhya/apache-beam-from-zero-to-hero-pt-2-streaming-pipelines-2aa9b53ab387?source=collection_archive---------9-----------------------#2021-02-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/a6afff52488fe81d8ec72995440fb335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bBif8ONi1NQfQLEq"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">马修·施瓦茨在<a class="ae hu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/><p id="40ed" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">系列上一篇:<br/> <a class="ae hu" rel="noopener" href="/analytics-vidhya/apache-beam-from-zero-to-hero-pt-1-batch-pipelines-ee117c37abc8">阿帕奇光束——从零到英雄Pt。1:批量流水线</a></p><p id="67d7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这篇文章中，我们将实现一个流管道，同时涵盖Apache Beam的其余基本概念。让我们首先解释什么是流式管道，以及它与批处理管道的区别。</p><h1 id="250e" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">介绍</h1><p id="91f1" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">批处理管道和流式管道之间的基本区别在于，批处理管道会一直运行，直到完成输入数据(数据量是有限的)，而流式管道会一直运行(当然，直到手动停止)，因为输入数据是无限的。</p><p id="e1d7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以用不同的方式来说:批处理管道从有界的PCollections获得数据，而流式管道从无界的PCollections获得数据——让我们更详细地讨论这些术语。</p><h1 id="bae8" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">有界与无界p集合</h1><p id="c7f2" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">PCollection是有界还是无界由生成它的数据源决定。如果我们从数据库或文件等数据源读取数据(就像上一篇文章中那样)，我们会得到一个有界的PCollection，如果我们从Pub/Sub(或基本上任何其他排队数据源)读取数据，我们会得到一个无界的PCollection。</p><p id="ba11" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这两种类型的p集合在管道中的处理方式不同。虽然可以读取和处理有界的PCollection，而不需要为数据指定任何批处理方法，但是对于无界的p collection，我们必须定义一种方法来将连续的数据流分成批处理，以便对其应用一些转换——在Apache Beam中，这被称为<strong class="iw hy"> Windowing </strong>，我将在本文中进一步讨论这一点。</p><p id="7d59" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们从实现一个流管道开始，并回顾它的不同组件，就像我们在上一篇文章中所做的那样。</p><h1 id="ee76" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">管道要求</h1><p id="7d63" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">我们将通过以下步骤实现管道:</p><p id="d129" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="kv"> 1。</em><strong class="iw hy"><em class="kv">read-from-pubsub</em></strong><em class="kv">:从Google Cloud Pub/Sub中读取源源不断的JSON元素*这里我们将使用</em> <strong class="iw hy"> <em class="kv"> Pub/Sub连接器。</em> </strong> <em class="kv"> <br/> 2。</em> <strong class="iw hy"> <em class="kv">滤镜</em> </strong> <em class="kv">:只通过那些“持续时间”字段不超过100的元素——我们将学习</em> <strong class="iw hy"> <em class="kv">滤镜变换。</em> </strong> <em class="kv"> <br/> 3。</em> <strong class="iw hy"> <em class="kv">窗口</em> </strong> <em class="kv">:每分钟批量处理元素——我们将学习如何使用</em> <strong class="iw hy"> <em class="kv">窗口。</em> </strong> <em class="kv"> <br/> 4。</em> <strong class="iw hy"> <em class="kv">计算-度量</em> </strong> <em class="kv">:计算每批的这两个度量:<br/> (1)元素数量<br/> (2)“持续时间”字段的平均值<br/> 5。</em><strong class="iw hy"><em class="kv">Write-to-bq</em></strong><em class="kv">:将结果写入big query——这里我们将再次重温</em> <strong class="iw hy"> <em class="kv"> BigQuery连接器</em> </strong> <em class="kv">，它在流管道中的工作方式与批处理管道略有不同..</em></p><p id="dc34" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="kv">每个元素都是JSON格式，结构如下:<br/> { "id": 1，" type": "chair "，" duration": 20 } </em></p><p id="1422" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">首先，我们需要建立管道的基本结构:</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="d03c" class="lf jt hx lb b fi lg lh l li lj">import argparse<br/>import apache_beam as beam<br/>from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions</span><span id="3554" class="lf jt hx lb b fi lk lh l li lj">def run(argv=None, save_main_session=True):<br/>  parser = argparse.ArgumentParser()<br/>  parser.add_argument(<br/>  ...</span><span id="b8a3" class="lf jt hx lb b fi lk lh l li lj">  known_args, pipeline_args = parser.parse_known_args(argv)<br/>  pipeline_options = PipelineOptions(pipeline_args)<strong class="lb hy"><br/>  pipeline_options.view_as(StandardOptions).streaming = True</strong></span><span id="b326" class="lf jt hx lb b fi lk lh l li lj">  with beam.Pipeline(options=pipeline_options) as p:<br/>    (p<br/>      ....)</span><span id="d22d" class="lf jt hx lb b fi lk lh l li lj">if __name__ == '__main__':<br/>  run()</span></pre><p id="0d93" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们通过在选项中指定来将管道定义为流式管道。现在有了基本的结构，让我们实现第一步——使用<em class="kv">发布/订阅连接器</em>从发布/订阅订阅中读取数据。</p><h1 id="b7fa" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">发布/订阅连接器</h1><p id="cf07" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">Pub/Sub是Google针对消息传递和事件驱动系统的托管解决方案，我们将使用它作为管道的数据源。我们使用函数<em class="kv"> ReadFromPubSub </em>返回一个无界的PCollection:</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="20e6" class="lf jt hx lb b fi lg lh l li lj">with beam.Pipeline(options=pipeline_options) as p:<br/>  (p<br/><strong class="lb hy">    | 'Read from pubsub' &gt;&gt; beam.io.ReadFromPubSub(subscription=known_args.subscription))</strong></span></pre><p id="9b80" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里我们可以向函数传递其他几个参数，比如“with_attributes”和“timestamp_attribute ”,你可以在<a class="ae hu" href="https://beam.apache.org/releases/pydoc/2.27.0/apache_beam.io.gcp.pubsub.html" rel="noopener ugc nofollow" target="_blank">这里</a>中读到更多关于它们的内容，但是在我们的例子中，缺省值就足够了。</p><p id="8e56" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">接下来的步骤需要将元素访问为dict，为此，我们需要首先将传入消息的有效负载从base64转换为UTF-8，然后将其转换为字典:</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="4e72" class="lf jt hx lb b fi lg lh l li lj">(p<br/>  | 'Read from pubsub' &gt;&gt;  beam.io.ReadFromPubSub(subscription=known_args.subscription)<br/><strong class="lb hy">  | 'To Json' &gt;&gt; beam.Map(lambda e: json.loads(e.decode('utf-8'))))</strong></span></pre><p id="c34b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下一步是过滤掉“持续时间”大于100的元素。我们通过<em class="kv">滤波器</em>变换来实现。</p><h1 id="9a88" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">过滤数据</h1><p id="dad4" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">Apache Beam中有一个特殊的过滤器转换，它获得一个布尔函数——只有当函数返回true时，元素才会被向前传递:</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="c9a3" class="lf jt hx lb b fi lg lh l li lj"><strong class="lb hy">def within_limit(x, limit):<br/>  return x &lt;= limit</strong></span><span id="5e7b" class="lf jt hx lb b fi lk lh l li lj">with beam.Pipeline(options=pipeline_options) as p:<br/>  (p<br/>    | 'Read from pubsub' &gt;&gt; beam.io.ReadFromPubSub(subscription=known_args.subscription)<br/>    | 'To Json' &gt;&gt; beam.Map(lambda e: json.loads(e.decode('utf-8')))<br/><strong class="lb hy">    | 'Filter' &gt;&gt; beam.Filter(within_limit, 100))</strong></span></pre><p id="e8aa" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下一步是每分钟批量处理我们的数据。为此，我们需要将<em class="kv">窗口</em>引入到我们的管道中。</p><h1 id="c8c2" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">开窗术</h1><p id="3de4" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">由于流式管道处理无限的数据流，因此某些操作需要批处理策略才能工作(执行聚合。例如)。</p><p id="cb2c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Apache Beam提供了几个现成的窗口函数，我们也可以自己编写一个。在我们的例子中，我们可以使用已经提供的固定时间窗口*** -只需将数据分成1分钟一批:</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="1336" class="lf jt hx lb b fi lg lh l li lj">with beam.Pipeline(options=pipeline_options) as p:<br/>  (p<br/>    | 'Read from pubsub' &gt;&gt;   beam.io.ReadFromPubSub(subscription=known_args.subscription)<br/>    | 'To Json' &gt;&gt; beam.Map(lambda e: json.loads(e.decode('utf-8')))<br/>    | 'Filter' &gt;&gt; beam.Filter(within_limit, 100)<strong class="lb hy"><br/>    | 'Window' &gt;&gt; beam.WindowInto(window.FixedWindows(6)))</strong></span></pre><p id="db48" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">* * *只要PCollection元素具有timestamp属性，就可以使用<em class="kv"> fixedWindows </em>函数。在我们的例子中，<em class="kv"> ReadFromPubSub </em>返回一个元素的PCollection，将消息的发布时间戳作为时间戳属性，因此我们不需要添加任何东西。你可以在<a class="ae hu" href="https://beam.apache.org/documentation/programming-guide/#windowing" rel="noopener ugc nofollow" target="_blank">这里</a>阅读更多关于不同类型的窗口功能。</p><p id="c55c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下一步，我们需要为每批计算2个指标。在前一篇文章中，我们可以使用像<em class="kv"> sum </em>这样的现成集合，而这一次我们需要编写自己的集合——我们需要引入一个<em class="kv">自定义组合器</em>。</p><h1 id="0010" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><em class="ll">自定义合并器</em></h1><p id="cd71" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">为了计算每批的平均值和计数，我们需要实现CombineFN类(这是几种方法之一):</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="5973" class="lf jt hx lb b fi lg lh l li lj">class CountAndMeanFn(beam.CombineFn):<br/>  def create_accumulator(self):<br/>    return 0.0, 0</span><span id="8005" class="lf jt hx lb b fi lk lh l li lj">  def add_input(self, sum_count, input):<br/>    (sum, count) = sum_count<br/>    return sum + input[‘duration’], count + 1<br/>  <br/>  def merge_accumulators(self, accumulators):<br/>    sums, counts = zip(*accumulators)<br/>    return sum(sums), sum(counts)<br/>  <br/>  def extract_output(self, sum_count):<br/>    (sum, count) = sum_count<br/>    return {<br/>      'processing_time': datetime.utcnow()<br/>        .strftime('%Y-%m-%dT%H:%M:%S.%fZ'),<br/>      'count': count,<br/>      'mean': sum / count if count else float('NaN')<br/>    }</span></pre><p id="8cea" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们来分解一下:</p><ol class=""><li id="292f" class="lm ln hx iw b ix iy jb jc jf lo jj lp jn lq jr lr ls lt lu bi translated">在<em class="kv"> create_accumulator </em>方法中，我们定义了累加器的初始状态。</li><li id="9ea2" class="lm ln hx iw b ix lv jb lw jf lx jj ly jn lz jr lr ls lt lu bi translated">在<em class="kv"> add_input </em>方法中，我们实现了向当前累加器添加元素的行为。我们希望对元素计数+对持续时间字段求和(以便能够在以后计算平均值)。</li><li id="9cd2" class="lm ln hx iw b ix lv jb lw jf lx jj ly jn lz jr lr ls lt lu bi translated">在<em class="kv"> merge_accumulators </em>方法中，我们实现了将多个累加器合并为一个累加器的行为(如果在这个步骤中有多个工人，这是相关的)。</li><li id="37b1" class="lm ln hx iw b ix lv jb lw jf lx jj ly jn lz jr lr ls lt lu bi translated">在<em class="kv"> extract_output </em>中，我们定义了累加器的最终输出——在我们的例子中，用我们刚刚计算的度量向前传递一个JSON元素；</li></ol><p id="3166" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们将它添加到管道中——我们不想按任何特定的键分组，而是要总结整个批处理，所以我们将使用<em class="kv"> CombineGlobally </em>:</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="28f6" class="lf jt hx lb b fi lg lh l li lj">(p <br/>  | 'Read from pubsub' &gt;&gt; beam.io.ReadFromPubSub(subscription=known_args.subscription)<br/>  | 'To Json' &gt;&gt; beam.Map(lambda e: json.loads(e.decode('utf-8')))<br/>  | 'Filter' &gt;&gt; beam.Filter(within_limit, 100)<br/>  | 'Window' &gt;&gt; beam.WindowInto(window.FixedWindows(300))<br/><strong class="lb hy">  | 'Calculate Metrics' &gt;&gt; beam<br/>                            .CombineGlobally(CountAndMeanFn())<br/>                            .without_defaults())</strong></span></pre><p id="5a94" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们现在要做的就是将数据加载到BigQuery中。</p><h1 id="f797" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">再次写入BigQuery</h1><p id="b062" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated"><em class="kv"> *(为了简洁起见，从现在开始我将使用BQ而不是big query)</em></p><p id="04eb" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在批处理和流管道中，写入BQ的工作方式略有不同。阿帕奇梁对BQ有2种可能的加载方式:<br/> 1。FILE_LOADS方法——这是我们在上一篇文章中已经讨论过的方法(它是批处理管道的默认方法)。<br/> 2。STREAMING_INSERTS方法——这是流式管道的默认方法。</p><p id="0a42" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">两者之间的区别非常明显——FILE _ LOADS方法将要写入BQ的行聚集到文件中，然后加载这些文件，而在STREAMING_INSERTS方法中，行是“一次一行”插入的(BQ针对流功能优化了该过程，如果您自己尝试一次插入一行，实际上会导致非常差的性能)。</p><p id="ed05" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">可以为流式管道定义FILE_LOADS加载方法，尽管默认设置为***:</p><ol class=""><li id="d670" class="lm ln hx iw b ix iy jb jc jf lo jj lp jn lq jr lr ls lt lu bi translated">便宜多了。BigQuery流并不便宜，而将文件加载到BigQuery是免费的- <a class="ae hu" rel="noopener" href="/google-developer-experts/trimming-down-over-95-of-your-bigquery-costs-using-file-loads-d08dd3d8b2fd">这篇</a>文章详细描述了如何通过使用FILE_LOADS来大幅削减BQ成本。</li><li id="b18f" class="lm ln hx iw b ix lv jb lw jf lx jj ly jn lz jr lr ls lt lu bi translated">有一个流大小限制——最大行大小是5MB，而在BigQuery中通常是100MB。虽然超过5MB的行并不常见，但随着时间的推移，这可能会成为一个真正的问题。</li></ol><p id="32e4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">其他一些已知的限制在<a class="ae hu" href="https://cloud.google.com/bigquery/quotas#streaming_inserts" rel="noopener ugc nofollow" target="_blank">这里</a>描述。</p><p id="b065" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">***如果您真的需要流插入提供的相对较低的延迟，那么它是值得考虑的，但也值得考虑您是否使用了正确的工具来完成这项工作——也许BigQuery在这里不一定是最合适的选择。</p><p id="442f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由于我使用Google Cloud Platform的免费层来写这篇文章，所以我将使用FILE_LOADS方法——我需要手动指定FILE_LOADS作为加载方法，并添加一个触发频率(在加载到BQ之前，一批中应该有多少条记录):</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="f0c3" class="lf jt hx lb b fi lg lh l li lj">(p <br/>  | 'Read from pubsub' &gt;&gt; beam.io.ReadFromPubSub(subscription=known_args.subscription)<br/>  | 'To Json' &gt;&gt; beam.Map(lambda e: json.loads(e.decode('utf-8')))<br/>  | 'Filter' &gt;&gt; beam.Filter(within_limit, 100)<br/>  | 'Window' &gt;&gt; beam.WindowInto(window.FixedWindows(300))<br/>  | 'Calculate Metrics' &gt;&gt; beam<br/>                            .CombineGlobally(CountAndMeanFn())<br/>                            .without_defaults()<br/><strong class="lb hy">  | 'Write to BigQuery' &gt;&gt; beam.io.WriteToBigQuery(<br/>                             known_args.table_spec,<br/>                             schema=table_schema,<br/>                             method=                                                                                <br/>                     beam.io.WriteToBigQuery.Method.FILE_LOADS,</strong><br/>                             <strong class="lb hy">triggering_frequency=1, </strong><br/>                             <strong class="lb hy">write_disposition=<br/>                     beam.io.BigQueryDisposition.WRITE_APPEND,<br/>                             create_disposition=<br/>                     beam.io.BigQueryDisposition.CREATE_IF_NEEDED))</strong></span></pre><p id="0c3e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了使用STREAMING_INSERTS方法，您可以删除触发频率和方法规范。</p><p id="787a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">就这样，我们的管道完工了。让我们运行它(我生成了一个writer进程，它在后台向Pub/Sub写入消息，它在repo中可用):</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="ae07" class="lf jt hx lb b fi lg lh l li lj">$ python -m main --temp_location gs://tests_tmp_1/tmp</span></pre><p id="21f1" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们每分钟都在汇总数据，所以在运行管道10分钟后，我停止了它，这是BQ中的结果:</p><figure class="kw kx ky kz fd hj er es paragraph-image"><div class="er es ma"><img src="../Images/045384a1181ddc6640275a27819ec87f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*7FGvHxrrrlgl7OeD63Usbg.png"/></div></figure><h1 id="2ba2" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">结论</h1><p id="d376" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">帖子里的完整代码可以在<a class="ae hu" href="https://github.com/SockworkOrange/blog-posts/tree/main/apache-beam-streaming-pipelines" rel="noopener ugc nofollow" target="_blank">这里</a>找到。希望你现在已经有了一个基本的(也许更多)流式管道，它们如何与批处理管道区分开来，当然，还有如何用Python自己实现一个。</p></div></div>    
</body>
</html>