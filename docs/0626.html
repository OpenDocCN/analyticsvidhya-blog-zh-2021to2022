<html>
<head>
<title>Sentiment Analysis of Movie Reviews pt.2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">电影评论的情感分析第二部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sentiment-analysis-of-movie-reviews-pt-2-45045225a263?source=collection_archive---------22-----------------------#2021-01-24">https://medium.com/analytics-vidhya/sentiment-analysis-of-movie-reviews-pt-2-45045225a263?source=collection_archive---------22-----------------------#2021-01-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="5d5e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">第二部分— LSA</h2></div><p id="9855" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">更多代码链接到我的Github:<a class="ae jt" href="https://github.com/charliezcr/Sentiment-Analysis-of-Movie-Reviews/blob/main/sa_p2.ipynb" rel="noopener ugc nofollow" target="_blank">https://Github . com/charliezcr/情操分析-电影评论/blob/main/sa_p2.ipynb </a></p><h2 id="b048" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">概述</h2><p id="6606" class="pw-post-body-paragraph ix iy hi iz b ja kp ij jc jd kq im jf jg kr ji jj jk ks jm jn jo kt jq jr js hb bi translated">还记得在《T2》第一部中，我从imdb上的评论数据集中对电影评论进行了情感分析。在最后一部分中，我首先将所有评论的文本预处理成小写词干，去掉数字和标点符号。然后，使用TF-IDF作为单词嵌入，将所有单词向量化成稀疏矩阵。之后，我运行了几个选定的机器学习模型来对特征、稀疏矩阵进行分类，并评估它们的性能。</p><p id="8f47" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，这只是情感分析的基础。因为我们有一个相对较小的数据集(1000个条目)，所以我们不需要考虑特征的维度。上次的特征维度是2317。</p><p id="94e0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，如果我们有一个大的数据集，比如一百万个条目呢？1，000，000 x 2，317的大小对于稀疏矩阵来说是超大的。存储和运行它需要强大的计算能力。这也非常耗时。因此，我们需要降低特征的维度，并在不损失准确性的情况下加快我们的机器学习过程。在这一部分，我将对LSA进行情感分析。</p><h2 id="6f14" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">LSA和奇异值分解</h2><p id="bf02" class="pw-post-body-paragraph ix iy hi iz b ja kp ij jc jd kq im jf jg kr ji jj jk ks jm jn jo kt jq jr js hb bi translated">潜在语义分析(LSA)是主题建模中常用的单词嵌入方法。另一方面，它对文本分类也很有用。简而言之，LSA就是对TD-IDF矢量化后的矩阵进行奇异值分解(SVD)。如果您不熟悉TF-IDF，请参考本研究的Pt.1。奇异值分解是一种用于自然语言处理的强大的矩阵分解方法。在自然语言处理中，我们总是会遇到大量的特征。我们可以使用SVD来选择这些维度中的一小部分，以获得一个截断矩阵来处理机器学习。(这里是SVD 的<a class="ae jt" href="https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm" rel="noopener ugc nofollow" target="_blank">教程)</a></p><p id="8fc2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了简单解释SVD，我们可以将矩阵A分解为</p><p id="8618" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ku"> A=USV </em> </strong></p><p id="8809" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">原始矩阵A是恩智浦矩阵。u是一个nxn矩阵，包含n个左奇异向量。v是一个pxp矩阵，包含p个右奇异向量。s是恩智浦对角线矩阵，对角线上包含奇异值。对角线以外的其余元素为0。奇异值σσ按其值排列:σ1 &gt; σ2 &gt; … &gt; σk</p><p id="2ede" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在所有这些奇异值中，我们可以选择少量我们想要的奇异值。例如，在这种情况下，我将在所有2317个奇异值中选择前100个，以便显著降低维数。我们将保留Uk中的前100个左奇异向量，奇异矩阵Sk中的前100个奇异值，Vk中的前100个右奇异向量，将它们相乘得到截断矩阵Ak，如下:</p><p id="2a39" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"><em class="ku">UkSkVk = Ak</em>T3】</strong></p><p id="3b4e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在Python中，我们可以使用scikit-learn来获得截断的SVD，并将其归一化以转换特征矩阵。代码如下:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="4b5f" class="ju jv hi la b fi le lf l lg lh"><strong class="la hj">from</strong> sklearn.decomposition <strong class="la hj">import</strong> TruncatedSVD<br/><strong class="la hj">from</strong> sklearn.pipeline <strong class="la hj">import</strong> make_pipeline<br/><strong class="la hj">from</strong> sklearn.preprocessing <strong class="la hj">import</strong> MinMaxScaler</span><span id="8640" class="ju jv hi la b fi li lf l lg lh"><em class="ku"># performance SVD</em><br/>svd <strong class="la hj">=</strong> TruncatedSVD(100)<br/><em class="ku"># performance LSA and normalization</em><br/>lsa <strong class="la hj">=</strong> make_pipeline(svd, MinMaxScaler())<br/>X_lsa <strong class="la hj">=</strong> lsa<strong class="la hj">.</strong>fit_transform(X)</span></pre><p id="975f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以将这段代码集成到<a class="ae jt" href="https://crzheng97.medium.com/sentiment-analysis-of-movie-reviews-pt-1-1a52daa90cdc" rel="noopener">第1部分</a>中使用的预处理方法中，并转换数据集。下面是预处理函数的2.0版本:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="f4d7" class="ju jv hi la b fi le lf l lg lh"><strong class="la hj">from</strong> nltk.stem <strong class="la hj">import</strong> PorterStemmer    <em class="ku"># stem the words</em><br/><strong class="la hj">from</strong> nltk.tokenize <strong class="la hj">import</strong> word_tokenize <em class="ku"># tokenize the sentences into tokens</em><br/><strong class="la hj">from</strong> string <strong class="la hj">import</strong> punctuation<br/><strong class="la hj">from</strong> sklearn.feature_extraction.text <strong class="la hj">import</strong> TfidfVectorizer <em class="ku"># vectorize the texts</em><br/><strong class="la hj">from</strong> sklearn.model_selection <strong class="la hj">import</strong> train_test_split <em class="ku"># split the testing and training sets</em><br/><strong class="la hj">from</strong> sklearn.decomposition <strong class="la hj">import</strong> TruncatedSVD <em class="ku"># SVD</em><br/><strong class="la hj">from</strong> sklearn.pipeline <strong class="la hj">import</strong> make_pipeline<br/><strong class="la hj">from</strong> sklearn.preprocessing <strong class="la hj">import</strong> MinMaxScaler</span><span id="0829" class="ju jv hi la b fi li lf l lg lh"><strong class="la hj">def</strong> preprocess(path):<br/>    '''generate cleaned dataset<br/>    <br/>    Args:<br/>        path(string): the path of the file of testing data</span><span id="3ed6" class="ju jv hi la b fi li lf l lg lh">    Returns:<br/>        X_train (list): the list of features of training data<br/>        X_test (list): the list of features of test data<br/>        y_train (list): the list of targets of training data ('1' or '0')<br/>        y_test (list): the list of targets of training data ('1' or '0')<br/>    '''<br/>    <br/>    <em class="ku"># text preprocessing: iterate through the original file and </em><br/>    <strong class="la hj">with</strong> open(path, encoding<strong class="la hj">=</strong>'utf-8') <strong class="la hj">as</strong> file:<br/>        <em class="ku"># record all words and its label</em><br/>        labels <strong class="la hj">=</strong> []<br/>        preprocessed <strong class="la hj">=</strong> []<br/>        <strong class="la hj">for</strong> line <strong class="la hj">in</strong> file:<br/>            <em class="ku"># get sentence and label</em><br/>            sentence, label <strong class="la hj">=</strong> line<strong class="la hj">.</strong>strip('\n')<strong class="la hj">.</strong>split('\t')<br/>            labels<strong class="la hj">.</strong>append(int(label))<br/>            <br/>            <em class="ku"># remove punctuation and numbers</em><br/>            <strong class="la hj">for</strong> ch <strong class="la hj">in</strong> punctuation<strong class="la hj">+</strong>'0123456789':<br/>                sentence <strong class="la hj">=</strong> sentence<strong class="la hj">.</strong>replace(ch,' ')<br/>            <em class="ku"># tokenize the words and stem them</em><br/>            words <strong class="la hj">=</strong> []<br/>            <strong class="la hj">for</strong> w <strong class="la hj">in</strong> word_tokenize(sentence):<br/>                words<strong class="la hj">.</strong>append(PorterStemmer()<strong class="la hj">.</strong>stem(w))<br/>            preprocessed<strong class="la hj">.</strong>append(' '<strong class="la hj">.</strong>join(words))<br/>    <br/>    <em class="ku"># vectorize the texts by tfidf</em><br/>    vectorizer <strong class="la hj">=</strong> TfidfVectorizer(stop_words<strong class="la hj">=</strong>'english', sublinear_tf<strong class="la hj">=True</strong>)<br/>    X_tfidf <strong class="la hj">=</strong> vectorizer<strong class="la hj">.</strong>fit_transform(preprocessed)<br/>    svd <strong class="la hj">=</strong> TruncatedSVD(100)<br/>    <em class="ku"># perform lsa</em><br/>    lsa <strong class="la hj">=</strong> make_pipeline(svd, MinMaxScaler())<br/>    X_lsa <strong class="la hj">=</strong> lsa<strong class="la hj">.</strong>fit_transform(X_tfidf)<br/>    <em class="ku"># split the testing and training sets</em><br/>    X_train, X_test, y_train, y_test <strong class="la hj">=</strong> train_test_split(X_lsa, labels, test_size<strong class="la hj">=</strong>0.2)<br/>    <strong class="la hj">return</strong> X_train, X_test, y_train, y_test</span><span id="3ddb" class="ju jv hi la b fi li lf l lg lh">X_train, X_test, y_train, y_test <strong class="la hj">=</strong> preprocess('imdb_labelled.txt')</span></pre><h2 id="5d9c" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">表演</h2><p id="8e34" class="pw-post-body-paragraph ix iy hi iz b ja kp ij jc jd kq im jf jg kr ji jj jk ks jm jn jo kt jq jr js hb bi translated">我们可以将LSA转换的数据应用于机器学习模型，并监控其性能的变化。请记住，在第1部分中，线性判别分析的结果是:</p><blockquote class="lj lk ll"><p id="4e12" class="ix iy ku iz b ja jb ij jc jd je im jf lm jh ji jj ln jl jm jn lo jp jq jr js hb bi translated"><em class="hi">线性判别分析的时间成本():0.79秒<br/>线性判别分析的准确度():0.71 </em></p></blockquote><p id="3961" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们的数据维度大大降低了。我们应该期待这次的时间成本会大幅改善。此外，这次我们甚至不需要增加数据密度！</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="ada9" class="ju jv hi la b fi le lf l lg lh">Time cost of LinearDiscriminantAnalysis(): 0.05s<br/>The accuracy of LinearDiscriminantAnalysis(): 0.73</span></pre><p id="9f27" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们刚刚将时间成本从0.79秒降低到0.05秒。就速度而言，这是一个巨大的飞跃！</p><p id="6b6c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">记得上次，我们还尝试了逻辑回归，多项式，SVC，SGD和MLP分类器。表现为:</p><blockquote class="lj lk ll"><p id="1eea" class="ix iy ku iz b ja jb ij jc jd je im jf lm jh ji jj ln jl jm jn lo jp jq jr js hb bi translated"><em class="hi">logistic regression()的时间成本:0.03s<br/>logistic regression()的准确度:0.825 </em></p><p id="353a" class="ix iy ku iz b ja jb ij jc jd je im jf lm jh ji jj ln jl jm jn lo jp jq jr js hb bi translated"><em class="hi">多项式的时间成本():0.0s <br/>多项式的准确度():0.825 </em></p><p id="1d98" class="ix iy ku iz b ja jb ij jc jd je im jf lm jh ji jj ln jl jm jn lo jp jq jr js hb bi translated"><em class="hi">SVC()的时间成本:0.09秒<br/>SVC()的精度:0.835 </em></p><p id="03bc" class="ix iy ku iz b ja jb ij jc jd je im jf lm jh ji jj ln jl jm jn lo jp jq jr js hb bi translated"><em class="hi">SGD classifier()的时间成本:0.0s<br/>SGD classifier()的精度:0.82 </em></p><p id="ee3b" class="ix iy ku iz b ja jb ij jc jd je im jf lm jh ji jj ln jl jm jn lo jp jq jr js hb bi translated"><em class="hi">MLP分类器的时间成本():3.47秒<br/>MLP分类器的准确度():0.81 </em></p></blockquote><p id="66c7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以尝试所有这些模型，观察它们的性能:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="e214" class="ju jv hi la b fi le lf l lg lh">Time cost of LogisticRegression(): 0.08s<br/>The accuracy of LogisticRegression(): 0.76</span><span id="cf71" class="ju jv hi la b fi li lf l lg lh">Time cost of MultinomialNB(): 0.0s<br/>The accuracy of MultinomialNB(): 0.775</span><span id="0c99" class="ju jv hi la b fi li lf l lg lh">Time cost of SVC(): 0.08s<br/>The accuracy of SVC(): 0.745</span><span id="f1a3" class="ju jv hi la b fi li lf l lg lh">Time cost of SGDClassifier(): 0.01s<br/>The accuracy of SGDClassifier(): 0.75</span><span id="6876" class="ju jv hi la b fi li lf l lg lh">Time cost of MLPClassifier(): 0.68s<br/>The accuracy of MLPClassifier(): 0.77</span></pre><p id="b174" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以看到，尽管所有这些模型的精度都有所下降，但速度却加快了。对于复杂的模型MLP分类器，时间成本显著降低。因此，MLP分类器和线性判别分类器可以包含在集成分类器中。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="88c9" class="ju jv hi la b fi le lf l lg lh">ensemble([LinearDiscriminantAnalysis(),LogisticRegression(),MultinomialNB(),SVC(),SGDClassifier(),MLPClassifier()])</span><span id="b3f6" class="ju jv hi la b fi li lf l lg lh">Time cost: 1.03s<br/>Accuracy: 0.78</span></pre><figure class="kv kw kx ky fd lq er es paragraph-image"><div class="er es lp"><img src="../Images/7690459152ae4ec44a22b67a45c7da6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*2vgbZRBWRk4gyGybf6GuzQ.png"/></div></figure></div></div>    
</body>
</html>