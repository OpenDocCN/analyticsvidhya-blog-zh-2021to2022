<html>
<head>
<title>Lesser Known Facts/Short cuts in Spark(PART1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">鲜为人知的事实/火花的捷径(第一部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/lesser-known-facts-short-cuts-in-spark-part1-77596e367676?source=collection_archive---------0-----------------------#2021-06-12">https://medium.com/analytics-vidhya/lesser-known-facts-short-cuts-in-spark-part1-77596e367676?source=collection_archive---------0-----------------------#2021-06-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/e25f73d32b4ce87257089e9d35e7b598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oAvOxnXRMCVQwQgV"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">马克·弗莱彻·布朗在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="63ad" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">作为一名数据工程师，我们每天都面临挑战，需要解决一些不常见的案例。</p><p id="8328" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们不能把同样的思维过程应用到所有的地方。我们需要考虑需要付出的努力以及如何减少时间。</p><p id="9210" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">但与此同时，我们应该在下结论之前了解内部情况。</p><p id="9b13" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这篇博客中，我将讨论Spark中的一些技巧和窍门以及一些鲜为人知的事实，这些对我们大多数数据工程师来说都很有用。</p><h1 id="090c" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">1.count()总是触发对每一行的求值？</h1><p id="f97a" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">我们知道。缓存或持久化是一种转换。除非我们对缓存的数据帧调用任何操作，否则它不会被具体化。</p><p id="432e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我跑了又怎么样。指望数据帧？它应该缓存/物化数据，对吗？</p><p id="085e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">不尽然，与rdd相比，数据帧和数据集上的计数是不同的。</p><p id="9caf" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在RDD，我们没有rdd上的catalyst optimiser，因此它将始终评估一切。</p><p id="e0a2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">但是另一方面，通过使用catalyst，DataFrame/Dataset变得更加智能，它变成了SQL中的“select count(*) from …”的等价物，这可以在不扫描某些数据格式的数据的情况下完成(例如，Parquet)。</p><p id="6d33" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">但是请注意，另一方面，缓存数据帧/数据集确实需要缓存所有内容。</p><p id="1ed4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">需要了解更多？</p><p id="6bec" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae it" href="https://databricks.com/blog/2017/02/16/processing-trillion-rows-per-second-single-machine-can-nested-loop-joins-fast.html" rel="noopener ugc nofollow" target="_blank">https://databricks . com/blog/2017/02/16/processing-万亿行每秒-单机-can-nested-loop-joins-fast . html</a></p><h1 id="e22b" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">2.在数据框中查找空列的快捷方式？</h1><p id="7138" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">在包含100列的数据帧的情况下，我们需要获得每一列中的空记录的计数，我们可以应用。对每一列使用空函数，或者只使用一个简单的命令，如:</p><p id="9c09" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">df . select(df . columns . map(c =&gt;sum(col(c). is null . cast(" int "))。别名(c)): _*)。show() </strong></p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kv"><img src="../Images/dc15b41cfa5e05b3277aa310b70f571c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nzoaHxcTrtXuRUmXHEmSAQ.png"/></div></div></figure><h1 id="7f15" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">3.什么是metastore超时？我为什么要在乎？</h1><p id="fbcb" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated"><strong class="iw hi"> <em class="la">无法获取到jdbc的连接:MySQL://&lt;&gt;/central _ metastore？use SSL = true&amp;requires sl = false。休眠7000毫秒。剩余尝试次数:2次</em> </strong></p><p id="bb41" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">举一个例子，我们使用spark将数据输入ADLSGen2/S3，使用API或HTTPS调用，例如使用sprimgml库调用SFDC。</p><p id="2045" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae it" href="https://github.com/springml/spark-salesforce" rel="noopener ugc nofollow" target="_blank">https://github.com/springml/spark-salesforce</a></p><p id="cb07" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Springml spark-salesforce库将帮助您连接到SFDC并获取数据。</p><p id="fa2d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这使得HTTPS民调不同于其他JDBC民调。</p><p id="78a9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">吞吐量根据我们在每个https调用中尝试访问的列数而变化。</p><p id="7d38" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当我们试图访问一个拥有数百万条记录的巨大对象时，Spark将花费大量时间查询SFDC。</p><p id="bae3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当spark完成GET请求时，hive metastores超时。</p><p id="c99b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">无法获取到jdbc的连接:mysql:// &lt;&gt; /central_metastore？useSSL=true&amp;requireSSL=false。休眠7000毫秒。剩余尝试次数:2</p><p id="a3b1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这种情况下，我们需要调整以下参数。</p><pre class="kw kx ky kz fd lb lc ld le aw lf bi"><span id="855b" class="lg jt hh lc b fi lh li l lj lk"><strong class="lc hi">spark.hadoop.hive.server2.session.check.interval(default 60000ms)</strong></span><span id="0fdb" class="lg jt hh lc b fi ll li l lj lk"><strong class="lc hi">spark.hadoop.hive.server2.idle.operation.timeout(default 7200000ms)</strong></span><span id="24da" class="lg jt hh lc b fi ll li l lj lk"><strong class="lc hi">spark.hadoop.hive.server2.idle.session.timeout(default 900000ms)</strong></span><span id="635f" class="lg jt hh lc b fi ll li l lj lk"><strong class="lc hi"><em class="la">spark.hadoop.hive.server2.idle.operation.interval</em></strong></span></pre><p id="e49c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">会话/操作超时的检查间隔，以毫秒为单位，可通过设置为零或负值来禁用。<br/>例如，值“60000”表示1分钟，表示每1分钟检查一次会话。</p><p id="5a5f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">T4 b . spark . Hadoop . hive . server 2 . idle . operation . time outT6】</strong></p><p id="6974" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这段时间内未被访问时，操作将被关闭，单位为毫秒。通过设置为零来禁用。</p><p id="5277" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于正值，仅检查终端状态下的操作(已完成、已取消、已关闭、出错)。对于负值，检查所有操作，而不管状态如何。<br/>例如，值“7200000ms”表示如果查询/操作仍在运行，将在2小时后超时。</p><p id="b6cf" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"><em class="la">c . spark . Hadoop . hive . server 2 . idle . session . time out</em></strong></p><p id="8380" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果会话在给定的持续时间(毫秒)内未被访问，会话将被关闭。</p><p id="aae7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以禁用设置为零或负值。<br/>值“900000”表示会话将在15分钟不活动后超时。</p><h1 id="b743" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak"> 4。我该用什么，Savemode。覆盖还是模式(“覆盖”)？？</strong></h1><p id="5a15" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">当我们在SparkSQL上工作时，一些开发人员使用“overwrite”模式，一些使用“Savemode.overwrite”模式。</p><p id="3b02" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">两者有区别吗？</p><p id="39f9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">不，“覆盖”是Savemode.overwrite的简写命令</p><p id="c924" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">看看下面的源代码。</p><pre class="kw kx ky kz fd lb lc ld le aw lf bi"><span id="ea68" class="lg jt hh lc b fi lh li l lj lk"><strong class="lc hi">/**<br/> * Specifies the behavior when data or table already exists. Options include:<br/> * — `overwrite`: overwrite the existing data.<br/> * — `append`: append the data.<br/> * — `ignore`: ignore the operation (i.e. no-op).<br/> * — `error` or `errorifexists`: default option, throw an exception at runtime.<br/> *<br/> * @since 1.4.0<br/> */</strong></span><span id="38ae" class="lg jt hh lc b fi ll li l lj lk"><strong class="lc hi">def mode(saveMode: String): DataFrameWriter[T] = {<br/> this.<em class="la">mode </em>= saveMode.toLowerCase(Locale.<em class="la">ROOT</em>) match {<br/> case “overwrite” =&gt; SaveMode.<em class="la">Overwrite<br/> </em>case “append” =&gt; SaveMode.<em class="la">Append<br/> </em>case “ignore” =&gt; SaveMode.<em class="la">Ignore<br/> </em>case “error” | “errorifexists” | “default” =&gt; SaveMode.<em class="la">ErrorIfExists<br/> </em>case _ =&gt; throw new IllegalArgumentException(s”Unknown save mode: $saveMode. ” +<br/> “Accepted save modes are ‘overwrite’, ‘append’, ‘ignore’, ‘error’, ‘errorifexists’.”)<br/> }<br/> this<br/>}</strong></span></pre><h1 id="225b" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">5.火花不同VS丢弃重复</h1><p id="63ac" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">distinct和DropDuplicates之间的主要区别仅在于列的子集。</p><p id="4882" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在Distinct函数中，我们需要在调用函数之前传递select子句中的列。</p><p id="e3d3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">其中，<code class="du lm ln lo lc b">dropDuplicates(colNames)</code>将在删除给定列的重复项后返回数据帧中的所有列。</p><p id="deba" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">从<a class="ae it" href="https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/sql/DataFrame.html#dropDuplicates()" rel="noopener ugc nofollow" target="_blank"> javadoc </a>来看，distinc()和dropDuplicates()没有区别。</p><pre class="kw kx ky kz fd lb lc ld le aw lf bi"><span id="3717" class="lg jt hh lc b fi lh li l lj lk"><strong class="lc hi">dropDuplicates</strong></span><span id="c525" class="lg jt hh lc b fi ll li l lj lk"><strong class="lc hi">public DataFrame dropDuplicates()</strong></span><span id="e7ca" class="lg jt hh lc b fi ll li l lj lk"><strong class="lc hi">Returns a new DataFrame that contains only the unique rows from this DataFrame. This is an alias for distinct.</strong></span></pre><p id="46af" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">dropDuplicates()是在1.4中作为distinct()的替代而引入的，因为您可以使用它的重载方法来获得基于列子集的唯一行。</p><p id="adb7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">未完待续..</p><p id="5fa1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">第2部分已经在这里发表了。</p><div class="lp lq ez fb lr ls"><a href="https://link.medium.com/lExxlOuM9hb" rel="noopener follow" target="_blank"><div class="lt ab dw"><div class="lu ab lv cl cj lw"><h2 class="bd hi fi z dy lx ea eb ly ed ef hg bi translated">鲜为人知的事实/火花的捷径(第二部分)</h2><div class="lz l"><h3 class="bd b fi z dy lx ea eb ly ed ef dx translated">这是鲜为人知的事实/火花捷径的第二部分，我将在这里谈论一些未知的和…</h3></div><div class="ma l"><p class="bd b fp z dy lx ea eb ly ed ef dx translated">link.medium.com</p></div></div><div class="mb l"><div class="mc l md me mf mb mg in ls"/></div></div></a></div><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mh"><img src="../Images/0564afa916f47d189c7aef68aa837e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qiGLGnfnMfJCvTyhLgt5jg.png"/></div></div></figure><p id="97b6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你可以在评论中发表你的反馈。</p><p id="0591" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">阿吉特·库玛尔·谢蒂</p><p id="f228" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">大数据工程师—热爱大数据、分析、云和基础设施。</p><p id="43ed" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae it" href="https://ajithshetty28.medium.com/subscribe" rel="noopener">订阅</a> ✉️ || <a class="ae it" href="https://ajithshetty28.medium.com/" rel="noopener">更多博客</a>📝|| <a class="ae it" href="https://www.linkedin.com/in/ajshetty28" rel="noopener ugc nofollow" target="_blank">链接于</a>📊|| <a class="ae it" href="https://ajithshetty.github.io/" rel="noopener ugc nofollow" target="_blank">个人资料页面</a>📚|| <a class="ae it" href="https://github.com/ajithshetty/" rel="noopener ugc nofollow" target="_blank"> Git回购</a>👓</p><p id="ec72" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">订阅我的:</strong> <a class="ae it" href="https://justenoughdata.substack.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hi">每周简讯刚好够数据</strong> </a></p></div></div>    
</body>
</html>