<html>
<head>
<title>Activation Functions in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的激活函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-in-neural-networks-d0d0ab8eaa4b?source=collection_archive---------16-----------------------#2021-05-27">https://medium.com/analytics-vidhya/activation-functions-in-neural-networks-d0d0ab8eaa4b?source=collection_archive---------16-----------------------#2021-05-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/98e835adac456ab74c518b877dbb07b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YECeOxlko9KoOJNw8RNm3A.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">艾莉娜·格鲁布尼亚克在<a class="ae it" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="ead2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在开始激活函数之前，最好先介绍一下神经网络。</p><p id="744b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">神经网络技术非常类似于我们的大脑如何理解。大脑将刺激作为输入，处理它们，并相应地输出。神经网络与我们大脑中的许多神经元相连。它接受输入，做一些数学运算来处理输入，然后输出结果。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es js"><img src="../Images/2116522527e05c36c4656833363530f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*0cXUVvcAlSTWWbf6L3GOOw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">简单双输入神经元</figcaption></figure><p id="695d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由于上述图像神经元有2个输入(x1和x2)，因此每个输入都要乘以权重。这些加权输入与偏差相加，如下所示。</p><p id="7687" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="jx">(x1∫权重)+(x2∫权重)+偏差</em></p><p id="4a55" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">此后，我们的主题激活功能开始与这些神经元一起工作。进行上述计算后，这个和通过一个激活函数产生如下答案。</p><p id="2a58" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="jx">y = f((x1∫权重)+(x2∫权重)+偏差)</em></p><p id="ff1a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">那么“这些激活功能背后的目的是什么”这个问题就产生了。激活函数的主要目的是对输入执行非线性转换，使其能够解决复杂的问题。如果没有激活函数，权重和偏差将只有线性结果。</p><p id="c327" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">除此之外，激活功能决定去激活或激活神经元以获得期望的输出。此外，它有助于使输出正常化。尽管这些神经网络由于激活函数而非常复杂，但它只需要有限的计算时间。</p><p id="c97d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们看看不同的激活功能，了解它们是如何工作的:</p><p id="724c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 1。二元步进激活功能</strong></p><p id="1d5a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">该功能是基于阈值的激活功能。如果Y值高于阈值，则激活神经元。如果小于阈值，就不是。这可以通过一个简单的if-else代码库来实现。</p><p id="1e5e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="jx"> f(z) = 1如果z &gt; 0否则0如果z &lt; 0 </em></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es jy"><img src="../Images/65b22f035a8e551eb48e88af6ecf7741.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*uTWtAtkCsBu1Gw7pkwvOjw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">阶跃函数的图形表示</figcaption></figure><p id="b0cb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">简单地创建一个二元分类器。应该说“是”或“不是”的东西。这由一个简单的1和0的函数来表示。</p><p id="15d3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">假设要连接多个神经元，以便为相关任务引入更多的类。和一个以上的输出1。现在很复杂，因为我们不能决定应该是哪个类。你会希望网络只激活1个神经元，其他的应该是0。如果激活不是二进制的，而是说“70%激活”或“30%激活”等等，那就更好了。然后你就可以找出激活率最高的作为选择。这可以通过线性函数来实现。</p><p id="02bf" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 2。线性激活功能</strong></p><p id="5ff2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">它接受输入，即来自神经元的加权和，并产生与输入成比例的输出信号。该线性函数优于阶跃函数，因为如上所述，它允许多次激活，而不仅仅是1和0。</p><p id="2d85" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="jx"> f(z)=a*z </em></p><p id="a2ea" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这种情况下,“a”可以是任何常数值。这意味着梯度与z无关。下降应该是一个恒定的梯度。因此，由反向传播引起的误差的任何预测误差变化都是恒定的。这导致了糟糕的预测。</p><p id="e5d3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">也认为神经网络有两层，但在线性激活中，最后一层不过是第一层输入的线性函数。无论我们如何组合，整个网络仍然相当于线性激活的单层。所以这两层都可以用一个单层代替。</p><p id="ff87" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 3。乙状结肠功能</strong></p><p id="c2e1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这主要在我们进行决策或预测时使用，因为这输出在0到1之间的最小范围内，因此预测会更准确。这种非线性激活函数工作效率很高，主要用于前馈神经网络。sigmoid函数表示为:</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es jz"><img src="../Images/284b00ca5b1999f4eb8ba9d7ba8f9077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*HJi3_LjrxDT5-Nn15b40Ig.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">sigmoid函数的图形表示</figcaption></figure><p id="ab46" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当考虑sigmoid函数的图形时，在X值-2到2之间，Y值急剧下降。这意味着X区域的微小变化将显著影响Y区域。它倾向于激活曲线的两侧(x=-2到x=2)。通过这种方式，它可以在预测中创建清晰的变化，以获得更好的结果。</p><p id="d162" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">作为sigmoid函数的一个缺点，考虑到函数两端的图形，Y值变化不大。这意味着两个区域的梯度都很小。这就产生了“渐变消失”的问题。通过这种方式，网络学习速度变得更慢或者非常慢。但是有一些方法可以解决这个问题，对于分类问题，sigmoid仍然是一个非常好的函数。</p><p id="0843" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 4。双曲正切激活函数(Tanh) </strong></p><p id="cc9b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">双曲正切函数类似于sigmoid函数。该函数表示为:</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es ka"><img src="../Images/b5d5e08228f573d4f1c4d2207ffecfd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*CLh0BhthknXHPQoonu_O8Q.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">双曲正切函数的图形表示</figcaption></figure><p id="8fd5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如上所述，这个函数的所有特征类似于sigmoid函数，但是双曲正切函数的梯度比sigmoid函数更强。因此，对于该决定，sigmoid或tanh将取决于与您的要求相关的梯度强度。</p><p id="36ff" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 5。Softmax激活功能</strong></p><p id="f541" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">此函数生成范围在0和1之间的输出，概率之和等于1。此外，函数的指数充当非线性函数。这是SoftMax激活函数的方程式。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kb"><img src="../Images/65434322b37f52217c48218d3bfdee67.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*ZyI_zkFTKpV3fWmUXitWvw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Softmax激活方程式</figcaption></figure><p id="1e93" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里LHS意味着神经元输出层的值。然后这些指数除以指数之和进行归一化，然后转换成概率。</p><p id="4f80" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 6。ReLU(整流线性单元)激活功能</strong></p><p id="5ebb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是目前最流行的函数，范围从0到无穷大，所有的负值都转换为零。这以良好的计算速度工作，并且ReLU本质上是非线性的。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kc"><img src="../Images/dd01418469c941cb5b862dc77357e923.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*lUIB-dhQzzE22nsTozrF8A.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">ReLU函数的图形表示</figcaption></figure><p id="e32d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">作为具有随机初始化的权重的网络，由于ReLU的性质，几乎50%的网络给出0个激活。这意味着更少的神经元被激活，网络变得更轻。</p><p id="519a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于在上述激活函数(sigmoid和tanh)中观察到的消失梯度问题，该函数是一个很好的解决方案。</p><p id="7027" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">结论</strong></p><p id="5986" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">像ReLU、sigmoid和tanh这样的激活函数得到了最大的关注。但是存在某些不同的特征，并且在使用相关任务之前必须研究每个特征。此外，您也可以尝试自定义功能。</p><p id="967d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">但是ReLU大部分时间作为通用近似器工作，因为它有助于消除消失梯度问题，该问题导致训练过程训练中的主要问题并降低神经网络模型的性能和精度。</p><p id="d9a0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过这篇文章，我试图对激活函数，它们在神经网络中是如何工作的，它们之间的主要区别等等给出一个概述性的解释。我已经讨论了6种常用的激活函数及其局限性。希望你对激活函数有了全面的了解。</p><p id="2f70" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">参考文献</strong></p><ul class=""><li id="6d81" class="kd ke hh iw b ix iy jb jc jf kf jj kg jn kh jr ki kj kk kl bi translated"><a class="ae it" href="https://www.analyticssteps.com/blogs/7-types-activation-functions-neural-network" rel="noopener ugc nofollow" target="_blank">https://www . analyticssteps . com/blogs/7-types-activation-functions-neural-network</a></li><li id="ffad" class="kd ke hh iw b ix km jb kn jf ko jj kp jn kq jr ki kj kk kl bi translated"><a class="ae it" rel="noopener" href="/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0">https://medium . com/the-theory-of-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884 E0</a></li></ul></div></div>    
</body>
</html>