<html>
<head>
<title>End to End Transformer Architecture — Encoder Part</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">端到端变压器架构—编码器部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/end-to-end-transformer-architecture-encoder-part-657df6975e33?source=collection_archive---------4-----------------------#2021-04-13">https://medium.com/analytics-vidhya/end-to-end-transformer-architecture-encoder-part-657df6975e33?source=collection_archive---------4-----------------------#2021-04-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="43d3" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">你只需要一点点注意力就能理解‘你需要的注意力’这篇论文</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/57009da8551fbd67f43bd1ef10b38c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9UbY9WOJFeofF8dQ"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">阿瑟尼·托古列夫在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="7fee" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">各位学者好，</p><p id="99c4" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">在几乎所有最新的NLP模型中，如Bert、GPT、T5以及许多变体中，都使用了变压器。有时我们只使用变压器的编码器(Bert)或解码器(GPT)。</p><p id="f013" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">为了首先理解这些模型，我们应该深入了解Transformer的工作原理及其元素的作用。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ks"><img src="../Images/eddf51cb676a215508ded289bc42dd69.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*oMooKywkOSHMOcFblhNnvw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图1—【https://arxiv.org/pdf/1706.03762.pdf T4】</figcaption></figure><p id="f4c9" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">RNN和LSTM网络用于机器翻译、下一个单词预测、文本生成等顺序任务。但是这些模型的问题是它们不能捕捉到长期的依赖性。</p><p id="a87b" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">《变形金刚》通过一种叫做“自我关注”的特殊类型的关注克服了LSTM和RNN的这种局限。</p><p id="60e0" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">假设，我们想将英语句子(源句子)转换成印地语句子(目标)。我们将源句子作为输入输入到编码器。编码器学习英语句子的表达，并把它交给解码器。解码器获取该表示并生成一个印地语句子。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kt"><img src="../Images/57e13d4c12751b3030d1e58505c6132b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*StFLE-IHlPIrjEyNT2tdlA.png"/></div></figure><p id="3721" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这里的问题是编码器和解码器如何将句子从一种语言转换成另一种语言。要理解这一点，我们首先要详细了解编码器和解码器。</p><h1 id="89f6" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">编码器</h1><p id="2692" class="pw-post-body-paragraph ju jv hi jw b jx ku jz ka kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr hb bi translated">在这里，我们将只了解编码器。</p><p id="5d87" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">转换器包含N个编码器的堆栈，这些编码器相互堆叠。每一个都将输出发送到上面，最终的编码器生成单词的表示。</p><p id="f965" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">在论文中，作者使用N=6，但在“Bert base”中我们使用12个编码器，在“Bert large”中使用24个，而“DistilBERT”使用6个编码器。即我们可以尝试不同数量的编码器值。</p><blockquote class="kz la lb"><p id="4204" class="ju jv lc jw b jx jy jz ka kb kc kd ke ld kg kh ki le kk kl km lf ko kp kq kr hb bi translated">编码器的主要优势之一是它们的语义表示，即使是对于相同的拼写单词。</p></blockquote><p id="8f98" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">在句子<em class="lc">‘学蟒蛇容易’</em>和<em class="lc">‘蟒蛇咬了有毒’</em>。像谷歌的“Word2Vec”和facebook的“FastText”这样的算法将在两个句子中创建相同的“python”嵌入(表示)。但是基于transformer模型理解它与单词的关系，并在两个句子中创建不同的嵌入。也就是说，单词“python”在句子1和2中的表示是不同的。</p><p id="2a7d" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">好吧，但是变压器是怎么做到的呢？</p><p id="4a8e" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这就是注意力层的魔力。现在让我们详细了解一下‘自我关注’。考虑下面一篇研究论文中的句子。</p><h2 id="4514" class="lg ig hi bd ih lh li lj il lk ll lm ip kf ln lo it kj lp lq ix kn lr ls jb lt bi translated"><em class="lu">‘法律永远不会完美，但它的应用应该是’</em></h2><p id="e898" class="pw-post-body-paragraph ju jv hi jw b jx ku jz ka kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr hb bi translated">在上面的句子中，发音“its”指的是<em class="lc">“法律”</em>，我们可以通过阅读句子清楚地理解这一点，但是模型将如何理解这一点呢？答案是自我关注。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/783eddcd335843170ce5c4ce0b537cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*AGDTML0PSMLf0Ikybh-m6w.png"/></div></figure><p id="f4ff" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这里，我们的模型首先计算单词<em class="lc">‘The’</em>的表示，然后计算单词<em class="lc">‘Law’</em>的表示，以此类推所有其他单词。然后，它将每个单词的表示与所有其他单词联系起来。所以经过计算表示，单词<em class="lc">‘其’</em>与单词<em class="lc">‘法’</em>更有关联。这是它理解关系的方式。</p><blockquote class="lw"><p id="c929" class="lx ly hi bd lz ma mb mc md me mf kr dx translated">好吧…我明白这一点，但你能解释一下这个模型是如何从数学上做到这一点的吗？</p></blockquote><p id="8f8c" class="pw-post-body-paragraph ju jv hi jw b jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr hb bi translated">首先，我们将所有单词转换成嵌入。x1将是单词<em class="lc">‘The’</em>的嵌入，x2是单词<em class="lc">‘Law’</em>的嵌入，以此类推。我们称之为输入矩阵，输入矩阵的大小为(字数X 512)，因此在这种情况下，大小为11 X 512。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/ad579659bda887107ea7b9958ac2c589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*sQb-vsffJwvQm5r4AnwgtQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">11 X 512输入嵌入矩阵</figcaption></figure><p id="7eae" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">请注意，这是我们可以随机生成的，因为我们的模型将在训练时学习和更新。</p><p id="9436" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">现在，根据这个输入矩阵，我们必须创建Q、K和V矩阵。为了创建这些，我们需要Wq矩阵、Wk矩阵和Wv矩阵。Wq、Wk和Wv的大小为512 X 64。</p><p id="8ab7" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">现在，我们将输入矩阵乘以Wq、Wk和Wv，分别生成Q、K和V。</p><p id="e9f5" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">Q == &gt;输入嵌入X Wq = = &gt;(11×512)。dot(512 X 64) ==&gt; (11 X 64)</p><p id="85a5" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">K == &gt;输入嵌入X Wk ==&gt;(11 X 512)。dot(512 X 64) ==&gt; (11 X 64)</p><p id="e05b" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">V== &gt;输入嵌入X Wv ==&gt;(11 X 512)。dot(512 X 64) ==&gt; (11 X 64)</p><p id="26e0" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">所以Q，K和V矩阵的形状是11 X 64。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mm"><img src="../Images/de7502e574341c30d75acd1268a91491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pO8sD--5Pvf7cXJsNm-Bpg.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">Q、K和V的(11 X 64)矩阵应该是这样的</figcaption></figure><p id="374b" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">为了理解每个单词与句子中其他单词的关系，我们使用Q、K和V矩阵来研究自我注意。</p><blockquote class="kz la lb"><p id="f463" class="ju jv lc jw b jx jy jz ka kb kc kd ke ld kg kh ki le kk kl km lf ko kp kq kr hb bi translated">S <strong class="jw hj"> tep1: </strong>计算Q.dot(K^T)就是矩阵q和矩阵k的转置的点积</p></blockquote><p id="6b46" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">我们知道例子中每个矩阵的大小是(11 X 64)，所以我们取点积= = &gt; q.dot(k^t = = &gt;(11 x64)。dot((11X64)^T)</p><p id="0b74" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">(11 X 64)。dot(64 X 11) ==&gt; (11 X 11)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mn"><img src="../Images/17f296705f4e90dfda5590c02c17281e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*hvrDCnw6Cz7CbW2D6Olpfg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">(11 X 11) Q.K^T矩阵</figcaption></figure><p id="1c1d" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这里第一行表示单词<em class="lc">‘The’</em>与句子中所有其他单词的关系，倒数第二行表示单词<em class="lc">‘it’</em>与句子中所有其他单词的关系，依此类推。</p><p id="c973" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这里我们可以看到，单词<em class="lc">、</em>与自身的关系比其他单词高，同样，单词<em class="lc">、</em>和<em class="lc">、</em>与其他单词的关系也比其他单词高。</p><blockquote class="kz la lb"><p id="34f1" class="ju jv lc jw b jx jy jz ka kb kc kd ke ld kg kh ki le kk kl km lf ko kp kq kr hb bi translated"><strong class="jw hj">第二步:</strong>将Q.dot(K^T除以q(列数)的维数的平方根。</p><p id="3f4d" class="ju jv lc jw b jx jy jz ka kb kc kd ke ld kg kh ki le kk kl km lf ko kp kq kr hb bi translated">(q.dot(k^t)/sqrt(q的维数) )</p></blockquote><p id="deb3" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这里我们将刚刚计算的(11×11)矩阵除以8。64是Q、K和V矩阵的维数，64的平方根是8，因此我们除以8得到稳定的梯度。</p><blockquote class="kz la lb"><p id="ecc8" class="ju jv lc jw b jx jy jz ka kb kc kd ke ld kg kh ki le kk kl km lf ko kp kq kr hb bi translated"><strong class="jw hj">第三步</strong>:为了归一化第二步的结果，我们取softmax。</p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mm"><img src="../Images/618743b7da1bf081418a810295419aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*77s-aHasQwgFaHnxhd92oQ.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">(11 x 11)sqrt((q.dot(k^t)/sqrt(q的维数) ))</figcaption></figure><p id="0995" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">我们知道softmax值的列相加应该等于1。由此我们可以说，单词<em class="lc">‘it’s与单词<em class="lc">‘Law’</em>有4 </em> 0%相似。</p><blockquote class="kz la lb"><p id="f115" class="ju jv lc jw b jx jy jz ka kb kc kd ke ld kg kh ki le kk kl km lf ko kp kq kr hb bi translated"><strong class="jw hj">第四步:</strong>计算最终的注意力矩阵。</p></blockquote><p id="1bed" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">注意力矩阵“Z”是通过由分数加权的值向量(V)的总和来计算的(步骤3的结果)。步骤4的输出是(句子长度X 64) ie 11 X 64</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mo"><img src="../Images/a35bd1c18ff39464f0163202f2e906c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cx1czlsO558PeuXD3epEXA.png"/></div></div></figure><p id="ea64" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">第一排将是</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mm"><img src="../Images/78c9b7ad46721e12956e89dbbfa05f4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*su3BNp3POQPjFcC8TmNHMA.jpeg"/></div></div></figure><p id="b5d1" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">Z1是单词“the”的表示。其尺寸为(1×64 ),包含来自“the”向量的值的90 %。</p><p id="2f1c" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">类似地，我们将计算所有单词的表示。即直到Z11。例如，Z2(它是单词“Law”的表示)包含来自向量“it”的值的40 %。</p><p id="e659" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这就是我们的模型如何理解单词“法律”和“它”是相似的。</p><blockquote class="kz la lb"><p id="a00d" class="ju jv lc jw b jx jy jz ka kb kc kd ke ld kg kh ki le kk kl km lf ko kp kq kr hb bi translated">到目前为止，我们已经看到了自我关注，现在让我们看看多头关注。</p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mp"><img src="../Images/fc3953b29847d886387daab47ad230be.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*O9VJzj2QqsVadsvu6SwoFg.png"/></div></figure><p id="0a57" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">无论我们在自我关注层做了什么，我们只是做了N次，并连接N的结果，乘以权重向量W0。其中N是注意头的数量。</p><p id="aeb6" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这在一个单词可能有多种含义的情况下很有用。在句子<em class="lc">中，“法律永远不会完美，但它的应用应该是”</em></p><p id="040d" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">单词<em class="lc">‘it’s’</em>的关注值，就是单词<em class="lc">‘Law’</em>的值向量。在这里，实际单词'<em class="lc"> it's </em>的注意力值是由单词'<em class="lc"> Law </em>支配的。但这在这里没问题，因为单词“it's”的意思是模糊的，因为它可能指的是“<em class="lc"> Law </em>”。</p><p id="3fbb" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">因此，如果其他单词的值向量在前面的示例中所示的情况下支配实际单词，其中实际单词是模糊的，那么这种支配是有用的，否则，它将导致理解单词的正确含义的问题。因此，为了确保我们的结果是准确的，我们将计算多个注意力矩阵，然后连接它们的结果，而不是计算单个注意力矩阵。</p><p id="b694" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">使用多头注意力背后的想法是，如果我们使用多个注意力头，而不是使用单个注意力头，那么我们的注意力矩阵将更加准确</p></div><div class="ab cl mq mr gp ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="hb hc hd he hf"><p id="39ed" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">您一定想知道，我们看到了一种叫做位置编码的东西，它增加了输入嵌入，但是我们还没有介绍它。现在你已经理解了编码器的大部分，所以是时候介绍它了。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mx"><img src="../Images/0d4eea563ee56119a0cf0e461c86f804.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*hEQEn9U8LnKgSFXk4nSTwQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">别担心，在这里</figcaption></figure><blockquote class="kz la lb"><p id="49a5" class="ju jv lc jw b jx jy jz ka kb kc kd ke ld kg kh ki le kk kl km lf ko kp kq kr hb bi translated"><strong class="jw hj">位置编码:</strong></p></blockquote><p id="6bb6" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">在RNN和LSTM，我们一个字一个字地传递我们的输入，也就是说，在我们的例子中，我们必须传递一个字<em class="lc">、</em>，然后传递一个字<em class="lc">、【法律】、</em>，等等。这就叫递归。但是在transformer pass中，所有的单词都是并行的，即不遵循递归。</p><p id="db9a" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">但这里的一个问题是，如果我们平行地输入一个句子，那么它如何理解语序，因为要理解句子的意思，知道语序是非常重要的。因此，我们不是直接输入嵌入，而是添加一些指示单词位置的信息。</p><p id="d747" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">所以现在输入矩阵不仅仅是嵌入，它还嵌入了位置信息。</p><p id="8fda" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated"><strong class="jw hj">由于我们将位置嵌入矩阵添加到输入嵌入中，它必须与输入嵌入的大小相同(11×512)。</strong></p><p id="7c88" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">如何计算位置嵌入矩阵？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es my"><img src="../Images/24edcb9c01ed352c4a8a8ce5c09e991f.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*HyKZhEvr1HKvrcbvgpcbow.png"/></div></figure><p id="469a" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">“pos”表示单词在句子中的位置，“I”表示嵌入的位置。“三维模型”是模型的维度(512)</p><p id="7ef9" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">“位置”的范围从0到10，“I”的范围从0到255。</p><p id="3acf" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">位置嵌入矩阵的维数与输入嵌入矩阵的维数相同。如果我们设pos=0，i=0，那么对于P(pos，2i) =&gt; P(0，0) = 0，对于P(pos，2i+1) =&gt; P(0，1) = cos(0) = 1</p><p id="a110" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">类似地，如果我们把所有的‘位置’和‘I’值放在一起，那么我们得到(11×512)矩阵。这就是我们如何计算整个11×512矩阵。</p><blockquote class="kz la lb"><p id="9686" class="ju jv lc jw b jx jy jz ka kb kc kd ke ld kg kh ki le kk kl km lf ko kp kq kr hb bi translated"><strong class="jw hj">前馈网络:</strong></p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mz"><img src="../Images/28b7d3c9bc7909f4d0b76a2cc18e7c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*07XD-ljo_5Rvfhx_JOU3jg.png"/></div></figure><p id="5dc4" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">前馈网络由两个具有ReLU激活的密集层组成。前馈网络的参数在句子的不同位置上是相同的，而在编码器块上是不同的。</p><blockquote class="kz la lb"><p id="5599" class="ju jv lc jw b jx jy jz ka kb kc kd ke ld kg kh ki le kk kl km lf ko kp kq kr hb bi translated"><strong class="jw hj">添加和定额组件:</strong></p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es na"><img src="../Images/12bd0bf7786d333cf217cc3a9e8d758f.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*WmO2FerhZvOCHvr6dlzoGQ.png"/></div></figure><ol class=""><li id="4891" class="nb nc hi jw b jx jy kb kc kf nd kj ne kn nf kr ng nh ni nj bi translated">将多头注意力子层的输入连接到其输出</li><li id="3861" class="nb nc hi jw b jx nk kb nl kf nm kj nn kn no kr ng nh ni nj bi translated">将前馈子层的输入连接到其输出。</li></ol><p id="1981" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">层规范化通过防止每个层中的值变化太大来促进更快的训练。</p><p id="0dfc" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">现在，我们已经了解了所有组件及其用途，现在让我们看看如何将所有组件放在一起。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es np"><img src="../Images/b65087b6c665a9265929141f3ae63261.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*efiGQpPMXzMoYf0wtIwd-A.png"/></div></figure><p id="763f" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">大家总结一下。</p><ol class=""><li id="6fa7" class="nb nc hi jw b jx jy kb kc kf nd kj ne kn nf kr ng nh ni nj bi translated">将一个句子的所有单词转换成输入嵌入，并在其中加入位置嵌入。</li><li id="57f7" class="nb nc hi jw b jx nk kb nl kf nm kj nn kn no kr ng nh ni nj bi translated">通过输入嵌入与Wq、Wk和Wv的点积计算Q、K和V矩阵。<strong class="jw hj"><em class="lc">【W】</em></strong></li><li id="4e54" class="nb nc hi jw b jx nk kb nl kf nm kj nn kn no kr ng nh ni nj bi translated">我们计算查询矩阵和关键矩阵<strong class="jw hj">(q.dot(k^t)</strong>之间的点积，并获得相似性分数。</li><li id="60e2" class="nb nc hi jw b jx nk kb nl kf nm kj nn kn no kr ng nh ni nj bi translated">接下来，我们将(Q.dot(K^T))除以关键向量的维数的平方根==&gt; <strong class="jw hj"> (Q*K^T)/sqrt(dk) </strong></li><li id="b6ba" class="nb nc hi jw b jx nk kb nl kf nm kj nn kn no kr ng nh ni nj bi translated">然后，我们应用softmax函数来归一化分数并获得分数矩阵。<strong class="jw hj">softmax(q*k^t)/sqrt(dk)</strong></li><li id="bfa1" class="nb nc hi jw b jx nk kb nl kf nm kj nn kn no kr ng nh ni nj bi translated">我们通过将得分矩阵乘以值矩阵v来计算关注矩阵z。<strong class="jw hj">v * softmax(q*k^t)/sqrt(dk)</strong></li><li id="9d34" class="nb nc hi jw b jx nk kb nl kf nm kj nn kn no kr ng nh ni nj bi translated">无论我们在自我关注层做了什么，我们只是做了N次，并连接N的结果，乘以权重向量W0。</li><li id="eed3" class="nb nc hi jw b jx nk kb nl kf nm kj nn kn no kr ng nh ni nj bi translated">我们将注意力矩阵作为输入馈送到下一个子层，即前馈网络。前馈网络将注意力矩阵作为输入，并将编码器表示作为输出返回。</li><li id="8e4c" class="nb nc hi jw b jx nk kb nl kf nm kj nn kn no kr ng nh ni nj bi translated">接下来，我们将从编码器1获得的输出作为输入馈送给它上面的编码器(编码器2)。</li><li id="366e" class="nb nc hi jw b jx nk kb nl kf nm kj nn kn no kr ng nh ni nj bi translated">编码器2将其传送到下一个编码器，最后，最后一个编码器(第6个)返回给定输入句子的编码表示作为输出。</li></ol><p id="6f32" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">参考:</p><ol class=""><li id="d968" class="nb nc hi jw b jx jy kb kc kf nd kj ne kn nf kr ng nh ni nj bi translated"><a class="ae jt" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1706.03762.pdf</a>(论文)</li><li id="03db" class="nb nc hi jw b jx nk kb nl kf nm kj nn kn no kr ng nh ni nj bi translated">我从这本伯特的书(【https://amzn.to/3e2ALt1】)中学到了很多</li></ol></div></div>    
</body>
</html>