<html>
<head>
<title>Why Data Scientists Should Learn Scrapy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么数据科学家应该学习Scrapy</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/why-data-scientists-should-learn-scrapy-9187fce6e7e1?source=collection_archive---------29-----------------------#2021-04-03">https://medium.com/analytics-vidhya/why-data-scientists-should-learn-scrapy-9187fce6e7e1?source=collection_archive---------29-----------------------#2021-04-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="5d3e" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">高级刮擦解决方案</h2></div><p id="c64e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">没有什么比刮网更让我热血沸腾了。想想有多少数据在外面到处流动——没有相关的API，没有官方的下载链接。难道你不想自己得到那些数据吗，就像你的狩猎采集祖先自己得到野兽一样？</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es js"><img src="../Images/773763d715cf3729114c72fc3ec45a68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*a6tI2-YhFTZnjjrAi_ycNA.jpeg"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">你的祖先在吃某种犰狳。</figcaption></figure><p id="1fc4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你是一个对网络抓取感兴趣的充满活力的数据科学家，你应该学习<a class="ae ke" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank"> Scrapy </a>。Scrapy是Python唯一最强大的抓取工具——它对于请求库就像熊猫对于Python字典一样。它快速、灵活、功能丰富，并且有据可查。毫无疑问，Scrapy是Python的首选抓取包。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kf"><img src="../Images/76126e76916e0aceda4d3f8a5a06fe83.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*3-JIyAGxuaBCkYhN6VUIvA.png"/></div></figure><h1 id="30c3" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">什么是Scrapy？</h1><p id="a690" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">Scrapy是一个用于开发蜘蛛的<strong class="iy hi">高级</strong>应用框架。蜘蛛是一种程序，它战略性地导航一个网站(或多个网站)并提取结构化数据。想象一下，如果你可以跳过开发网络抓取应用程序的底层管道，只关注你的抓取任务的细节——这就是Scrapy提供的。当Scrapy做得更好时，为什么要开发自己的蜘蛛框架、请求调度器、复制过滤器、下载器和导出机制？</p><h2 id="e77c" class="ld kh hh bd ki le lf lg km lh li lj kq jf lk ll ks jj lm ln ku jn lo lp kw lq bi translated">有什么好的？</h2><p id="441a" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">Scrapy构建在<a class="ae ke" href="https://twistedmatrix.com/trac/" rel="noopener ugc nofollow" target="_blank"> Twisted </a>异步网络框架之上，是Python可用的最快的抓取工具。它使用<a class="ae ke" href="https://en.wikipedia.org/wiki/Concurrency_(computer_science)" rel="noopener ugc nofollow" target="_blank">并发</a>向服务器发送大量请求，并以极快的速度下载网页。听起来很刺耳？我建议你像个男人一样。我没有时间去尊重服务器，你也一样。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es lr"><img src="../Images/1a598cb5bf8070105a60a589ca930ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NqoRf5hOb1RXeB_W7LqKDw.jpeg"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">愤怒的服务器。</figcaption></figure><p id="76cd" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">开玩笑的。你应该总是小心翼翼，这通常意味着配置下载延迟和并发设置。您可以将以下常量添加到<code class="du lw lx ly lz b">settings.py</code>:</p><pre class="jt ju jv jw fd ma lz mb mc aw md bi"><span id="2c34" class="ld kh hh lz b fi me mf l mg mh"># seconds of delay<br/>DOWNLOAD_DELAY = 0.25</span><span id="66a3" class="ld kh hh lz b fi mi mf l mg mh"># max simultaneous requests per IP<br/>CONCURRENT_REQUESTS_PER_IP = 8</span></pre><p id="4329" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">或者，您可以启用<a class="ae ke" href="https://docs.scrapy.org/en/latest/topics/autothrottle.html" rel="noopener ugc nofollow" target="_blank"> AutoThrottle </a>扩展，它可以根据服务器延迟和您的目标并发设置动态调整下载延迟。配置节流将防止您被禁止，也防止您惹恼运行您的目标网站的人。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mj"><img src="../Images/800e7afa5f008de696b4409ce0843420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BuQ4aoxuE8VJajzMpQtpHQ.jpeg"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">谢谢你的服务。</figcaption></figure><p id="49b3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">正如我已经提到的，Scrapy非常灵活，并且装载了许多有用的特性。它附带了各种用于不同常见用例的spider类，以及用于下载图像和其他文件的基础设施。你可以使用你最喜欢的HTML数据提取工具，比如<a class="ae ke" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a>和<a class="ae ke" href="https://lxml.de/" rel="noopener ugc nofollow" target="_blank"> lxml </a>。您还可以创建自己的数据处理管道来帮助清理数据。您甚至可以将蜘蛛部署到云中，并设置它们定期运行。想象一下，你可以利用云端蜘蛛多年的数据做些什么。</p><p id="7808" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我想强调的最后一点是，Scrapy是有据可查的。你将能够学习如何做任何你需要做的事情。文档不仅仅是一个API参考，还包括了Scrapy设计中每个概念的详细指南。我建议从<a class="ae ke" href="https://docs.scrapy.org/en/latest/intro/tutorial.html" rel="noopener ugc nofollow" target="_blank">教程</a>开始，然后边学边做。</p><h2 id="cb64" class="ld kh hh bd ki le lf lg km lh li lj kq jf lk ll ks jj lm ln ku jn lo lp kw lq bi translated">你什么时候不会用Scrapy？</h2><p id="1c63" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">Scrapy最适合需要在多个页面上爬行的重型抓取任务，可能是定期的。如果你想从一个表，一个页面，一次检索数据，你不需要Scrapy。可以用<a class="ae ke" href="https://docs.python-requests.org/en/master/" rel="noopener ugc nofollow" target="_blank">请求</a>，甚至熊猫的惊人强大<a class="ae ke" href="https://pandas.pydata.org/docs/reference/api/pandas.read_html.html" rel="noopener ugc nofollow" target="_blank"> pd.read_html </a>。</p><p id="a323" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">或者，如果您需要抓取大量使用动态加载内容的网站，而这些内容无法从其HTML源访问，您可能希望使用<a class="ae ke" href="https://selenium-python.readthedocs.io/index.html" rel="noopener ugc nofollow" target="_blank"> Selenium </a>。Selenium是为测试web应用程序而设计的跨平台浏览器自动化工具。然而，这是一台笨重的机器，(也就是说，速度慢，有很大的依赖性)，除非万不得已，否则我不推荐使用它。Selenium是一个底层的<strong class="iy hi"> </strong>刮擦解决方案，它没有太多刮擦基础设施。另外，事实证明，没有Selenium也可以访问动态加载的内容。参见<a class="ae ke" href="https://docs.scrapy.org/en/latest/topics/dynamic-content.html" rel="noopener ugc nofollow" target="_blank">本页</a>关于处理动态加载内容的零碎文档。</p><h1 id="6572" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">推荐功能</h1><p id="d327" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">虽然<a class="ae ke" href="https://docs.scrapy.org/en/latest/intro/tutorial.html" rel="noopener ugc nofollow" target="_blank">教程</a>超出了本文的范围，但我还是想强调几个我最喜欢的spider初学者特性。</p><h2 id="43f1" class="ld kh hh bd ki le lf lg km lh li lj kq jf lk ll ks jj lm ln ku jn lo lp kw lq bi translated">(被低估的)站点地图蜘蛛</h2><p id="51a9" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">当开发一个蜘蛛时，你要么继承基本的<code class="du lw lx ly lz b">Spider </code>类，要么继承Scrapy的一个专门的子类。Scrapy提供了在不同情况下有用的各种<code class="du lw lx ly lz b">Spider<strong class="iy hi"> </strong></code>子类。我最喜欢的子类是<code class="du lw lx ly lz b">SitemapSpider</code>，尽管开发人员忽略了在Scrapy CLI中包含它的模板。</p><p id="7555" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然而，当我想抓取一个网站时，我做的第一件事就是检查它是否有一个<a class="ae ke" href="https://www.sitemaps.org/index.html" rel="noopener ugc nofollow" target="_blank">站点地图</a>。网站地图本质上只是一个包含链接和元数据的XML文件，旨在帮助蜘蛛导航网站。如果我想从一个有成千上万个产品页面的电子商务商店中抓取产品，从网站地图中跟踪链接几乎总是比从页面中提取链接更可取。你从所有你需要的链接开始，这个列表是相对干净的。另外，网站地图很常见。为什么不利用它们，在轻松模式下爬行呢？</p><p id="f41e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了了解在Scrapy中创建一个蜘蛛有多简单，请看下面来自<a class="ae ke" href="https://docs.scrapy.org/en/latest/topics/spiders.html" rel="noopener ugc nofollow" target="_blank">文档</a>的例子。</p><pre class="jt ju jv jw fd ma lz mb mc aw md bi"><span id="216e" class="ld kh hh lz b fi me mf l mg mh"><strong class="lz hi">from</strong> <strong class="lz hi">scrapy.spiders</strong> <strong class="lz hi">import</strong> SitemapSpider<br/><br/><strong class="lz hi">class</strong> <strong class="lz hi">MySpider</strong>(SitemapSpider):<br/>    sitemap_urls = ['http://www.example.com/sitemap.xml']<br/>    sitemap_rules = [<br/>        ('/product/', 'parse_product'),<br/>        ('/category/', 'parse_category'),<br/>    ]<br/><br/>    <strong class="lz hi">def</strong> parse_product(self, response):<br/>        <strong class="lz hi">pass</strong> <em class="mk"># your HTML parsing code here</em><br/><br/>    <strong class="lz hi">def</strong> parse_category(self, response):<br/>        <strong class="lz hi">pass</strong> <em class="mk"># your HTML parsing code here</em></span></pre><p id="97e6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在<code class="du lw lx ly lz b">MySpider </code>体中定义的类变量有特殊的意义。蜘蛛将从从<code class="du lw lx ly lz b">sitemap_urls </code>获取网站地图并解析它们以提取网站页面的URL开始。它发送来自匹配<code class="du lw lx ly lz b">'/product/'</code>到<code class="du lw lx ly lz b">parse_product</code> <em class="mk">、</em>的URL的响应以及来自匹配<code class="du lw lx ly lz b">'/category/'</code>到<code class="du lw lx ly lz b">parse_category</code>的URL的响应。您只需为每种页面类型编写HTML数据提取代码。这几乎和使用BeautifulSoup请求一样简单。</p><h2 id="40f9" class="ld kh hh bd ki le lf lg km lh li lj kq jf lk ll ks jj lm ln ku jn lo lp kw lq bi translated">HTTP缓存</h2><p id="cfee" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">当开发一个新的蜘蛛时，您会想要反复测试它，以确保它行为正确并提取所需的数据。这对于目标网站来说可能非常烦人。将以下常量添加到您的<code class="du lw lx ly lz b">settings.py</code>将启用无过期缓存，因此在测试您的蜘蛛时，您将永远不必发送一个特定的请求超过一次。</p><pre class="jt ju jv jw fd ma lz mb mc aw md bi"><span id="cfc4" class="ld kh hh lz b fi me mf l mg mh">HTTPCACHE_ENABLED = True<br/>HTTPCACHE_EXPIRATION_SECS = 0<br/>HTTPCACHE_DIR = 'httpcache'</span></pre><p id="98b9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这种方法的一个警告是，你可能会在硬盘上留下一个非常大的文件夹。非常大的文件夹需要很长时间才能删除，这很烦人。为了缓解这个问题，我启用了gzip压缩。</p><pre class="jt ju jv jw fd ma lz mb mc aw md bi"><span id="a3ce" class="ld kh hh lz b fi me mf l mg mh">HTTPCACHE_GZIP = True</span></pre><h2 id="584d" class="ld kh hh bd ki le lf lg km lh li lj kq jf lk ll ks jj lm ln ku jn lo lp kw lq bi translated">图像管道</h2><p id="a91f" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">如果你想抓取图像，你需要使用内置的<a class="ae ke" href="https://docs.scrapy.org/en/latest/topics/media-pipeline.html" rel="noopener ugc nofollow" target="_blank">图像管道</a>。您可以通过向<code class="du lw lx ly lz b">settings.py</code>添加以下内容来启用它:</p><pre class="jt ju jv jw fd ma lz mb mc aw md bi"><span id="8bce" class="ld kh hh lz b fi me mf l mg mh">ITEM_PIPELINES = {'scrapy.pipelines.images.ImagesPipeline': 1}</span></pre><p id="ec67" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由于您刚刚开始，您会希望将图像存储在您的硬盘上(而不是远程服务器上)。为此，添加以下内容:</p><pre class="jt ju jv jw fd ma lz mb mc aw md bi"><span id="9d41" class="ld kh hh lz b fi me mf l mg mh"><strong class="lz hi">import </strong>os<br/>IMAGES_STORE = os.path.join('valid', 'directory')</span></pre><p id="efe5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在当你从一个网站上抓取物品时，你可以在<code class="du lw lx ly lz b">image_urls </code>字段中放一列图片网址，Scrapy会自动下载并存档。</p><h1 id="ecdc" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">结论</h1><p id="b2e2" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">没有任何其他Python包可以与Scrapy竞争作为一个重型刮擦解决方案。所谓的Scrapy替代品，Selenium和Requests，其实并不在一个档次上。Requests是一个用于发出HTTP请求的库，Selenium是一个用于浏览器自动化的工具。两者都是底层的抓取工具，需要你自己搭建抓取基础设施。这就像在开始一个分析项目之前试图改造熊猫一样——没有一个头脑正常的人会去尝试。</p><p id="4361" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你对web抓取感兴趣，你应该学习Python最强大的抓取包。你会对它的效率印象深刻。</p><h1 id="26a6" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">加入</h1><p id="7347" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">Scrapy是免费的，<a class="ae ke" href="https://github.com/scrapy/scrapy" rel="noopener ugc nofollow" target="_blank">开源</a>，由<a class="ae ke" href="https://www.zyte.com/" rel="noopener ugc nofollow" target="_blank"> Zyte </a>维护。我与Zyte或任何其他赞助公司都没有关系。我写这篇文章是发自内心的，基于我的电子商务网络抓取经验。我花了一段时间才自己发现Scrapy，我浪费了无数的时间在低级别的替代品上。</p></div></div>    
</body>
</html>