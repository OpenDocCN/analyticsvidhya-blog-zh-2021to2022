<html>
<head>
<title>Ensemble Learning Methods for everyone</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向所有人的集成学习方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ensemble-learning-methods-for-everyone-4db66cc4f8f6?source=collection_archive---------29-----------------------#2021-01-10">https://medium.com/analytics-vidhya/ensemble-learning-methods-for-everyone-4db66cc4f8f6?source=collection_archive---------29-----------------------#2021-01-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/4f9567130d3c16c2a82e3cce11d07736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7UWenucrymQFyfPVo3BCWw.jpeg"/></div></div></figure><p id="a119" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">大家好🙌。</p><p id="cd7f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">今天我们将讨论一种非常强大的机器学习算法，这种算法一直主导着Kaggle竞赛，即集成学习。</p><p id="3900" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">集成学习有多种方法，因此今天我们将在一个高层次上检查每一种方法，以便非技术人员也能理解它。</p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="cbc2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">那么，什么是集成学习呢？</p><blockquote class="ju jv jw"><p id="bee5" class="ip iq jx ir b is it iu iv iw ix iy iz jy jb jc jd jz jf jg jh ka jj jk jl jm ha bi translated">合奏——作为一个整体行动或集合在一起的一群事物或人，尤其是经常在一起演奏的一群音乐家。</p></blockquote><p id="98e6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">以上定义直接取自剑桥词典。</p><p id="51f2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">只是用机器学习模型代替音乐家，这就是整体学习。因此，这是一组机器学习模型(通常称为基础模型)一起工作以获得最终答案。这符合C.S.Lewis的名言(系综可以有两个以上的模型):</p><blockquote class="ju jv jw"><p id="ffc4" class="ip iq jx ir b is it iu iv iw ix iy iz jy jb jc jd jz jf jg jh ka jj jk jl jm ha bi translated">"三个臭皮匠胜过一个诸葛亮，不是因为两个人都不会犯错，而是因为他们不太可能在同一个方向上出错。"</p></blockquote><p id="7e2a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">注意，拥有相似的模型与拥有一个模型是一样的，因为它会有相似的观点，集成学习的首要目标是尽可能地使用彼此独立的模型。</p><p id="6e36" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">独立性↑方差↓ </strong></p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="52f7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">以下是实现第一个目标的不同方法:</p><ol class=""><li id="6f39" class="kc kd hh ir b is it iw ix ja ke je kf ji kg jm kh ki kj kk bi translated">使用不同的模型(例如:SVM，决策树，逻辑回归。)</li><li id="ef9b" class="kc kd hh ir b is kl iw km ja kn je ko ji kp jm kh ki kj kk bi translated">使用不同的数据子集</li></ol><p id="c1c9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">集成学习有两种主要类型，</p><ol class=""><li id="a225" class="kc kd hh ir b is it iw ix ja ke je kf ji kg jm kh ki kj kk bi translated">集合多个模型做最终决定<strong class="ir hi">(装袋/粘贴)</strong></li><li id="3f13" class="kc kd hh ir b is kl iw km ja kn je ko ji kp jm kh ki kj kk bi translated">按顺序一次使用一个型号。在每一步，下一个模型都试图最小化前一个模型所犯错误。<strong class="ir hi">(增压)</strong></li></ol></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><h1 id="122b" class="kq kr hh bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">装袋/粘贴</h1><ul class=""><li id="53b9" class="kc kd hh ir b is lo iw lp ja lq je lr ji ls jm lt ki kj kk bi translated">将<strong class="ir hi">子集</strong>的数据用于每个基本型号。</li><li id="63ee" class="kc kd hh ir b is kl iw km ja kn je ko ji kp jm lt ki kj kk bi translated">装袋→数据子集通过替换进行采样</li><li id="3ffa" class="kc kd hh ir b is kl iw km ja kn je ko ji kp jm lt ki kj kk bi translated">粘贴→对数据子集进行采样而不替换</li></ul><p id="e000" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">子集的数量取决于要组合的基础模型的数量。假设您想要使用M个模型，那么您需要从训练数据集中采样M个子集。每个子集，通常称为包，必须小于训练集的大小，经验法则是每个包使用大约60%的训练数据集。</p><p id="7708" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下面是装袋的示意图。请注意，在D1子集中存在相同的数据A，因为我们选择一个数据，替换它，然后选择另一个，这个过程重复，直到子集中的所有数据都被采样，然后对所有子集重复这个过程。</p><figure class="lv lw lx ly fd ii er es paragraph-image"><div class="er es lu"><img src="../Images/7ab39bff89442317c6f0addfee46c33c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*ypxRU_tWq9Pqtc9_YJzOUw.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">装袋示例</figcaption></figure><p id="0ab7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于粘贴，除了取样部分，每个过程都是相同的。因为它是在没有替换的情况下完成的，所以在一个子集中不能有相同的数据，在任何其他子集中也不能有相同的数据，一旦你选择了一个，你就不能再选择相同的一个。</p><p id="b72a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在你创建了M个子集之后，你在每个子集上训练一个模型，模型可以是相同的也可以是不同的。用决策树训练所有子集被称为“随机森林”,随机森林的一个特殊之处是，它不仅接受行的子集，而且每棵树只使用特征(列)的子集。</p><p id="9ee2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一旦每个模型在数据子集上被训练，每个模型输出单独的答案，在那里它们被聚集以做出最终答案。</p><p id="0450" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在分类中，个人答案有三种汇总方式:</p><ol class=""><li id="f51e" class="kc kd hh ir b is it iw ix ja ke je kf ji kg jm kh ki kj kk bi translated">硬投票——选择投票最多的类别。例如，如果您有3个模型，其中一个将数据分类为A类，另外两个分类为B类，那么最终答案将是B类，因为它得到了2/3的投票。</li><li id="42a5" class="kc kd hh ir b is kl iw km ja kn je ko ji kp jm kh ki kj kk bi translated">软投票——选择平均概率最高的类别。假设您有3个模型，模型1: [40%，60%]，模型2:[45%，55%]，模型3:[95%，5%]，其中列表中的项目表示类别的概率→<code class="du md me mf mg b">[% of data belonging to class A, % of data belonging to class B]</code>。现在，您计算平均类别概率[(40+45+95)/3，(60+55+5)/3] = [60%，40%]，平均概率较大的类别是最终答案，因此在软投票中，我们将数据识别为属于类别a。</li></ol><p id="4306" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于回归问题:</p><ol class=""><li id="58d1" class="kc kd hh ir b is it iw ix ja ke je kf ji kg jm kh ki kj kk bi translated">取所有输出的<strong class="ir hi">平均值</strong>。因为单个模型的所有答案都是连续值。</li><li id="54be" class="kc kd hh ir b is kl iw km ja kn je ko ji kp jm kh ki kj kk bi translated">取所有输出的<strong class="ir hi">中值</strong>。</li></ol><p id="0f2c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后，从多基础模型中汇总答案的第三种方法称为:</p><p id="735b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">3.<strong class="ir hi">堆叠</strong> —每个模型的输出被传递到另一个模型。下图是堆叠的非常简单的版本，其中所有基本模型输出回答一个元模型，但是堆叠可以有多个层和多个元模型。</p><figure class="lv lw lx ly fd ii er es paragraph-image"><div class="er es mh"><img src="../Images/c70c9b77d7ec0b6c73954c592f8832ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*hxjpoujZMBiqaGg7RrwgyQ.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">图片来自<a class="ae kb" href="http://supunsetunga.blogspot.com/" rel="noopener ugc nofollow" target="_blank">http://supunsetunga.blogspot.com/</a></figcaption></figure></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><h1 id="35c1" class="kq kr hh bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">助推</h1><p id="dbcc" class="pw-post-body-paragraph ip iq hh ir b is lo iu iv iw lp iy iz ja mi jc jd je mj jg jh ji mk jk jl jm ha bi translated">顺序学习法，在每个阶段，模型从以前的错误中学习(以前的模型所犯的错误)。</p><ul class=""><li id="e8a4" class="kc kd hh ir b is it iw ix ja ke je kf ji kg jm lt ki kj kk bi translated">减少<strong class="ir hi">偏差</strong>的好方法。</li></ul><p id="7067" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请注意，第一个模型的输出被标记为绿色和红色(错误)。现在红色的<strong class="ir hi">比绿色的</strong>更重，它被传递给下一个模型，因此它更关注之前出错的部分，并重复这个过程。</p><figure class="lv lw lx ly fd ii er es paragraph-image"><div class="er es ml"><img src="../Images/075c6627359c223edf56808b7649a10e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*4uHUcBqlEKxIikAw2X8zsg.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">升压示例</figcaption></figure><p id="2c22" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有多种类型的增强方法，如AdaBoost、Gradient Boost、XGBoost等。它们之间的差异取决于它们如何使用以前的输出来更好地预测下一个模型。</p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="cb66" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">今天到此为止！希望它对非技术人员和技术人员有帮助，这个博客旨在帮助任何人，即使没有数学背景。对于那些想知道每种方法实际上是如何在幕后工作的人，我将为每种深入内部工作的方法创建一个博客。</p><p id="8477" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢您的阅读！反馈是非常受欢迎的，请评论，如果有任何错误信息，我会尽快纠正他们！</p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="3feb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">参考资料:</p><ul class=""><li id="7f0d" class="kc kd hh ir b is it iw ix ja ke je kf ji kg jm lt ki kj kk bi translated"><a class="ae kb" href="https://www.packtpub.com/product/python-machine-learning-third-edition/9781789955750" rel="noopener ugc nofollow" target="_blank"> Python机器学习第3版第7章</a></li><li id="d87a" class="kc kd hh ir b is kl iw km ja kn je ko ji kp jm lt ki kj kk bi translated"><a class="ae kb" rel="noopener" href="/@yuezhuge/newly-published-book-the-quest-for-machine-learning-4c4ebd1020d3">探索机器学习第13章</a> →不确定是否有英文版。</li><li id="a414" class="kc kd hh ir b is kl iw km ja kn je ko ji kp jm lt ki kj kk bi translated"><a class="ae kb" href="https://www.youtube.com/watch?v=wr9gUr-eWdA&amp;ab_channel=stanfordonline" rel="noopener ugc nofollow" target="_blank">斯坦福CS229第10讲——决策树和集成方法</a></li><li id="3451" class="kc kd hh ir b is kl iw km ja kn je ko ji kp jm lt ki kj kk bi translated"><a class="ae kb" href="https://www.youtube.com/watch?v=KIOeZ5cFZ50&amp;ab_channel=KrishNaik" rel="noopener ugc nofollow" target="_blank">合奏技法视频</a></li><li id="0eaf" class="kc kd hh ir b is kl iw km ja kn je ko ji kp jm lt ki kj kk bi translated"><a class="ae kb" href="https://www.youtube.com/watch?v=2Mg8QD0F1dQ&amp;ab_channel=Udacity" rel="noopener ugc nofollow" target="_blank">自举聚合视频</a></li></ul></div></div>    
</body>
</html>