<html>
<head>
<title>Exploring different types of LSTMs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索不同类型的LSTMs</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/exploring-different-types-of-lstms-6109bcb037c4?source=collection_archive---------6-----------------------#2021-02-21">https://medium.com/analytics-vidhya/exploring-different-types-of-lstms-6109bcb037c4?source=collection_archive---------6-----------------------#2021-02-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="d3c6" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="ae30" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">最近，我们做了一个有趣的项目，在这个项目中，我们使用不同类型的LSTMs对来自Kaggle的电影评论数据集进行了情感分析，并分别获得了良好的准确率。让我们了解项目中使用的LSTM概念如下:</p><h2 id="eee3" class="ka if hh bd ig kb kc kd ik ke kf kg io jn kh ki is jr kj kk iw jv kl km ja kn bi translated">LSTM</h2><p id="5bf7" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">长短期记忆网络是一种特殊的RNNs，能够学习长期依赖性。</p><p id="f8ab" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">rnn在单Tanh层的重复模块中具有简单的结构，如下所示:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/fb483b4fcd2ff45863386c05d60933ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vODZnovJafo8L_qZ-Tk1xQ.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">标准RNN</figcaption></figure><p id="1989" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">与rnn不同，LSTMs有两个输入门、遗忘门和输出门，它们以特殊方式相互作用，如下图所示:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lj"><img src="../Images/cace770db820b756504598221c2c97d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5W8FrASMi93Z81NlAui4w.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">LSTM</figcaption></figure><h2 id="ff22" class="ka if hh bd ig kb kc kd ik ke kf kg io jn kh ki is jr kj kk iw jv kl km ja kn bi translated">项目详情:</h2><p id="1146" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们已经对电影评论数据集进行了情感分析。让我们看看数据集在下图中的样子:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lk"><img src="../Images/cf537872a8a5e1fd031ae9742016ba89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AbB15iqtUjnptm-NmwENnQ.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">电影评论数据集</figcaption></figure><p id="3366" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">我们可以看到，数据有四列，其中短语列包含用户提供的评论，相应的情感列分为五个类别，即从“0”(非常差)到“4”(非常好)，如下所示:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es ll"><img src="../Images/60b4cb5e860aee922ffb5af9f5d913ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*wyfKWNTnh30EQ1DvP-KC5A.png"/></div></figure><p id="d1f0" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">现在，我们已经对数据集有了一个概念，我们可以对数据集进行预处理。</p><h1 id="0c59" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">预处理</h1><p id="2a40" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">现在，通过考虑短语列，我们对数据集应用NLP文本处理步骤如下:</p><blockquote class="lm ln lo"><p id="a8b4" class="jc jd lp je b jf ko jh ji jj kp jl jm lq kq jp jq lr kr jt ju ls ks jx jy jz ha bi translated">将文本改为小写</p></blockquote><pre class="ku kv kw kx fd lt lu lv lw aw lx bi"><span id="d034" class="ka if hh lu b fi ly lz l ma mb">train['Phrase']=train.Phrase.apply(<strong class="lu hi">lambda</strong> x: x.lower())</span></pre><blockquote class="lm ln lo"><p id="e153" class="jc jd lp je b jf ko jh ji jj kp jl jm lq kq jp jq lr kr jt ju ls ks jx jy jz ha bi translated">识别和删除标点符号</p></blockquote><pre class="ku kv kw kx fd lt lu lv lw aw lx bi"><span id="a683" class="ka if hh lu b fi ly lz l ma mb"><strong class="lu hi">def</strong> remove_punc(text):<br/>    <strong class="lu hi">for</strong> i <strong class="lu hi">in</strong> string.punctuation:<br/>        text=text.replace(i,' ')    <br/>    <strong class="lu hi">return</strong> text<br/>train['Phrase']=train.Phrase.apply(remove_punc)</span></pre><blockquote class="lm ln lo"><p id="537d" class="jc jd lp je b jf ko jh ji jj kp jl jm lq kq jp jq lr kr jt ju ls ks jx jy jz ha bi translated">删除停用词</p></blockquote><pre class="ku kv kw kx fd lt lu lv lw aw lx bi"><span id="b2b2" class="ka if hh lu b fi ly lz l ma mb">stopword_list=stopwords.words('english')<br/>stopword_list.remove('no')<br/>stopword_list.remove('not')<br/>train['Phrase'] = train.Phrase.apply(<strong class="lu hi">lambda</strong> x : " ".join(x <strong class="lu hi">for</strong> x <strong class="lu hi">in</strong> x.split() <strong class="lu hi">if</strong> x <strong class="lu hi">not</strong> <strong class="lu hi">in</strong> stopword_list))</span></pre><blockquote class="lm ln lo"><p id="5767" class="jc jd lp je b jf ko jh ji jj kp jl jm lq kq jp jq lr kr jt ju ls ks jx jy jz ha bi translated">单词标记化</p></blockquote><pre class="ku kv kw kx fd lt lu lv lw aw lx bi"><span id="321c" class="ka if hh lu b fi ly lz l ma mb">train['Phrase']=train.Phrase.apply(word_tokenize)</span></pre><blockquote class="lm ln lo"><p id="fb38" class="jc jd lp je b jf ko jh ji jj kp jl jm lq kq jp jq lr kr jt ju ls ks jx jy jz ha bi translated">移除数字</p></blockquote><pre class="ku kv kw kx fd lt lu lv lw aw lx bi"><span id="c7e2" class="ka if hh lu b fi ly lz l ma mb"><strong class="lu hi">def</strong> remove_numbers(words):    <br/>    new_words = []<br/>    <strong class="lu hi">for</strong> word <strong class="lu hi">in</strong> words:<br/>        new_word = re.sub("\d+", "", word)<br/>        <strong class="lu hi">if</strong> new_word != '':<br/>            new_words.append(new_word)<br/>    <strong class="lu hi">return</strong> new_words<br/>train['Phrase']=train.Phrase.apply(remove_numbers)</span></pre><blockquote class="lm ln lo"><p id="6a10" class="jc jd lp je b jf ko jh ji jj kp jl jm lq kq jp jq lr kr jt ju ls ks jx jy jz ha bi translated">词汇化动词</p></blockquote><pre class="ku kv kw kx fd lt lu lv lw aw lx bi"><span id="3bcc" class="ka if hh lu b fi ly lz l ma mb"><strong class="lu hi">def</strong> lemmatize_verbs(words):   <br/>    lemmatizer = WordNetLemmatizer()<br/>    lemmas = []<br/>    <strong class="lu hi">for</strong> word <strong class="lu hi">in</strong> words:<br/>        lemma = lemmatizer.lemmatize(word, pos='v')<br/>        lemmas.append(lemma)<br/>    <strong class="lu hi">return</strong> lemmas<br/>train['Phrase']=train.Phrase.apply(lemmatize_verbs)</span></pre><blockquote class="lm ln lo"><p id="5619" class="jc jd lp je b jf ko jh ji jj kp jl jm lq kq jp jq lr kr jt ju ls ks jx jy jz ha bi translated">为数据创建了Wordcloud</p></blockquote><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mc"><img src="../Images/7b4db6ae8a9d9f7b3c58485786871823.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*nSbnueuFShteo2DNVpOLHw.png"/></div></figure><p id="7ebf" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">预处理完成后，我们将数据分成训练和测试数据集。现在，我们尝试应用不同的LSTM模型，并检查准确性，如下所示:</p><h1 id="4a33" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">不同的LSTM模式</h1><h2 id="5a77" class="ka if hh bd ig kb kc kd ik ke kf kg io jn kh ki is jr kj kk iw jv kl km ja kn bi translated">经典LSTM</h2><p id="9fd3" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这种结构由4个门控层组成，单元状态通过这些门控层工作，即2输入门、遗忘门和输出门。输入门协同工作，选择要添加到单元状态的输入。遗忘门基于当前单元状态来决定遗忘哪个旧单元状态。输出门决定通过它们发送什么输出。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es md"><img src="../Images/372566723a7d9dafc7c814ee6632512f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*L69bb4OirTPvwRvkDLVFng.png"/></div></figure><p id="a68d" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">我们将经典的LSTM(长短期记忆)应用于建模的训练数据并拟合模型。</p><pre class="ku kv kw kx fd lt lu lv lw aw lx bi"><span id="7c1d" class="ka if hh lu b fi ly lz l ma mb">EMBEDDING_DIM = 128<br/>lstm_out = 196<br/>a = len(tokenize.word_index)+1<br/>model = Sequential()<br/>model.add(Embedding(a, EMBEDDING_DIM, input_length=max_len))<br/>model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2 ))<br/>model.add(Dense(5, activation='softmax'))<br/>model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>print(model.summary())<br/>model.fit(X_train, y_train, batch_size=128, epochs=24, verbose=1)</span></pre><p id="6d2c" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">在这里，我们为模型使用了一个LSTM层，优化器是Adam，在大约24个时期后达到了80%的准确率，这很好。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es me"><img src="../Images/1edb8717f7230c3082d0ebafe6c389fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FjmJRUvcmnNvfx4SeMmMHQ.png"/></div></div></figure><p id="45ae" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">值得注意的是，通过应用不同类型的LSTMs(基本上是rnn ),研究了相同的数据集。</p><h2 id="733e" class="ka if hh bd ig kb kc kd ik ke kf kg io jn kh ki is jr kj kk iw jv kl km ja kn bi translated">堆叠LSTM</h2><p id="7121" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">堆叠的LSTM只不过是具有多个LSTM层的LSTM模型。LSTM层向下一个LSTM层提供顺序输出。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mf"><img src="../Images/5741f084b6e5d241322090f1679a27cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*wJhPNlp86LkWFv4Vd4kkhA.png"/></div></figure><p id="13cc" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">我们已经应用了堆叠LSTM，它只不过是增加了多个lstm并适合模型。</p><pre class="ku kv kw kx fd lt lu lv lw aw lx bi"><span id="f856" class="ka if hh lu b fi ly lz l ma mb">EMBEDDING_DIM = 128<br/>lstm_out = 196<br/>a = len(tokenize.word_index)+1<br/>model2 = Sequential()<br/>model2.add(Embedding(a, EMBEDDING_DIM, input_length=max_len))<br/>model2.add(LSTM(lstm_out, return_sequences=<strong class="lu hi">True</strong>, dropout=0.2, recurrent_dropout=0.2))<br/>model2.add(LSTM(lstm_out))<br/>model2.add(Dense(5, activation='softmax'))<br/>model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>print(model.summary())<br/>model2.fit(X_train, y_train, batch_size=128, epochs=24, verbose=1)</span></pre><p id="eb2f" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">使用两个LSTM层的模型和优化是亚当，实现了80%的准确率。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mg"><img src="../Images/3959eeb9677c25a01c87d7fecfd7bf28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UuxsZODfkbclez1m_Y1eCw.png"/></div></div></figure><h2 id="3027" class="ka if hh bd ig kb kc kd ik ke kf kg io jn kh ki is jr kj kk iw jv kl km ja kn bi translated">双向LSTM</h2><p id="d0e8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">双向LSTM在输入序列上训练两个而不是一个，这意味着第一个输入序列和第二个输入序列是它的反向拷贝。更快地改进模型的学习。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mh"><img src="../Images/50fb6533cdc0c9e85a9fa3321f0510d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*N78Lm_Fn5LkF-G9Rb3PzMA.jpeg"/></div></figure><p id="5cb1" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">我们应用了双向LSTM并拟合了模型。</p><pre class="ku kv kw kx fd lt lu lv lw aw lx bi"><span id="6d80" class="ka if hh lu b fi ly lz l ma mb"><strong class="lu hi">from</strong> <strong class="lu hi">keras.layers</strong> <strong class="lu hi">import</strong> Bidirectional<br/>EMBEDDING_DIM = 128<br/>a = len(tokenize.word_index)+1<br/>model3 = Sequential()<br/>model3.add(Embedding(a, EMBEDDING_DIM, input_length=max_len))<br/>model3.add(Bidirectional(LSTM(64, return_sequences=<strong class="lu hi">True</strong>)))<br/>model3.add(Bidirectional(LSTM(64)))<br/>model3.add(Dense(5, activation='softmax'))<br/>model3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>print(model.summary())<br/>model3.fit(X_train, y_train, batch_size=128, epochs=24, verbose=1)</span></pre><p id="b9bd" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">在这里，我们看到，我们不仅对模型使用了双向LSTM，而且还使用了多个层，它也是堆叠的，优化器是Adam，在这里，我们在24个时期后达到了81%的准确性，但我们可以更进一步，训练模型以获得更好的准确性。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es me"><img src="../Images/c1ff5e8ee923f710e15d807029127155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wGvuGdrtUgkEfX_fO9EZjQ.png"/></div></div></figure><h2 id="b33e" class="ka if hh bd ig kb kc kd ik ke kf kg io jn kh ki is jr kj kk iw jv kl km ja kn bi translated"><strong class="ak"> GRU(门控循环单元)</strong></h2><p id="c9ac" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">门控递归单元神经网络基本上由两个门组成，即复位门和更新门。重置门有助于捕获序列中的短期相关性，更新门有助于捕获序列中的长期相关性。这两个门控制着每个隐藏单元在处理序列时必须记住或忘记多少。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mi"><img src="../Images/1914b47cce0770ac3bc7059f6a843bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KtzCrRt8I3IxbmR1KLcMzg.png"/></div></div></figure><p id="9dd8" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">我们将GRU应用于该模型，并实现了81%的准确率。我们可以在下面找到代码:</p><pre class="ku kv kw kx fd lt lu lv lw aw lx bi"><span id="8028" class="ka if hh lu b fi ly lz l ma mb">model4 = Sequential()<br/>model4.add(Embedding(a, EMBEDDING_DIM, input_length=max_len))<br/>model4.add(GRU(64))<br/>model4.add(Dense(5, activation='softmax'))<br/>model4.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>print(model.summary())<br/>model4.fit(X_train, y_train, batch_size=128, epochs=24, verbose=1)</span></pre><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mj"><img src="../Images/aa4fd106269dcc4b02d1ffc1c23d9965.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V_UJ9DDaLZaHKe2xXYZBUA.png"/></div></div></figure><h2 id="f9ff" class="ka if hh bd ig kb kc kd ik ke kf kg io jn kh ki is jr kj kk iw jv kl km ja kn bi translated"><strong class="ak">双向GRU</strong></h2><p id="2f28" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">作为双向LSTM，双向GRU也是双向RNN，这意味着，BGRU只不过是双向的GRU。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mk"><img src="../Images/5ca59f5f62ad0edb58cb0533fe818452.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*fd56MWFESU1wGhg0830dbw.png"/></div></figure><p id="5c17" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">我们已经为模型应用了BGRU，优化器是Adam，实现了79%的准确率，如果模型被训练更多的时期，可以实现更多。</p><pre class="ku kv kw kx fd lt lu lv lw aw lx bi"><span id="a929" class="ka if hh lu b fi ly lz l ma mb">model5 = Sequential()<br/>model5.add(Embedding(a, EMBEDDING_DIM, input_length=max_len))<br/>model5.add(SpatialDropout1D(0.2))<br/>model5.add(Bidirectional(GRU(64)))<br/>model5.add(Dropout(0.2))<br/>model5.add(Dense(5, activation='softmax'))<br/>model5.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>print(model.summary())<br/>model5.fit(X_train, y_train, batch_size=128, epochs=24, verbose=1)</span></pre><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ml"><img src="../Images/b41a11b4decfed1370a4475135241b5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C0SDRcFXgns2KRCp1M674g.png"/></div></div></figure><h1 id="c4e1" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><p id="0b86" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">总之，我们已经知道所有的LSTMs都是RNNs的亚型。由于消失梯度问题，训练RNN解决某些问题变得很困难，为了克服这一点，我们使用LSTM，它使用一个特殊的单位和标准单位，这些单位可以控制记忆何时忘记以及何时获得输出。GRU是具有简化结构的LSTM，并且不使用单独的存储单元，而是使用较少的门来控制信息流。</p><p id="82a3" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">在这个项目中，我们利用经典的LSTM完成了一个完整的自然语言处理项目，取得了80%的准确率。我们更进一步，了解了不同类型的LSTMs及其使用相同数据集的应用。对于双向LSTM和GRU，我们分别达到了大约81%的准确度，然而，我们可以为更少的时期训练模型，并且可以达到更好的准确度。</p><p id="353c" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><em class="lp">因此，总的来说，根据我们的要求，这个项目的主要收获包括不同类型的LSTMs及其数据集实现的基础知识。</em></p></div></div>    
</body>
</html>