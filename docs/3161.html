<html>
<head>
<title>Logistic Regression with Gradient Descent Explained | Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降逻辑回归解释|机器学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-with-gradient-descent-explained-machine-learning-a9a12b38d710?source=collection_archive---------0-----------------------#2021-06-14">https://medium.com/analytics-vidhya/logistic-regression-with-gradient-descent-explained-machine-learning-a9a12b38d710?source=collection_archive---------0-----------------------#2021-06-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1a74" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">什么是逻辑回归？为什么用于分类？</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/e09e2746bc6f06f7a1afa4e43fdef740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E6cgn7-hsUYeGZm2sAsWew.jpeg"/></div></div></figure><h1 id="722b" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">什么是分类问题？</h1><p id="475d" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">一般来说，监督学习包括两种类型的问题设置。</p><ul class=""><li id="b15e" class="kx ky hi kd b ke kz kh la kk lb ko lc ks ld kw le lf lg lh bi translated"><strong class="kd hj">回归</strong>:这是一类问题，数据科学家使用合适的模型对自变量和<strong class="kd hj">连续因变量</strong>之间的关系进行建模，并使用该模型对未来输入数据进行准确预测。<br/>例如，<br/>在给定温度的情况下，预测某一天的冰淇淋销量。<br/>在这里，冰淇淋的销售额是一个连续变量，这意味着它可以取任何值。<br/> <em class="li">例如:500，100，10000，52，123，931…等等。</em></li><li id="e709" class="kx ky hi kd b ke lj kh lk kk ll ko lm ks ln kw le lf lg lh bi translated"><strong class="kd hj">分类:</strong>这是一类问题，数据科学家必须找到自变量和<strong class="kd hj">离散</strong>因变量之间的关系，并使用它来给出准确的未来预测。<br/>例如，<br/>根据温度和湿度读数预测是否会下雨。如果下雨，输出将是1。如果没有，它将是0。<br/>这里，要预测的值只能取某些值。在上述情况下，下雨或不下雨(0或1)。</li></ul><p id="e13f" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">在继续下一步之前，参考<a class="ae lr" rel="noopener" href="/analytics-vidhya/linear-regression-with-gradient-descent-derivation-c10685ddf0f4">带梯度下降的线性回归</a>了解线性回归如何工作，以及一种称为梯度下降的算法如何成为线性回归工作的关键。同样的梯度下降算法是我们将在逻辑回归中使用的算法，很多东西将与上面提到的帖子类似。所以，先读一下这个会让你对逻辑回归有更好的理解。</p><h1 id="eaf6" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">线性回归方法也适用于分类吗？</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ls"><img src="../Images/ecd0f5a1184fbd32f1704d8fc1d77290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*-npXwoam2BfN_zQysGnzbA.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图1.1</figcaption></figure><p id="366e" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">让我们考虑一个问题陈述。鉴于肿瘤的大小，我们需要预测它是恶性的还是良性的。<em class="li">(良性y=0，恶性y=1)。<br/> </em>将肿瘤的大小与肿瘤的类型相对照，给了我们左图。</p><p id="e919" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">在回归设置中，我们可以拟合最佳拟合线，并预测线上高于特定阈值的点是恶性的，低于该点的点是良性的。</p><p id="25cf" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">这种方法存在一些问题:</p><ul class=""><li id="ee11" class="kx ky hi kd b ke kz kh la kk lb ko lc ks ld kw le lf lg lh bi translated">Y的值可以超过1并低于0。但是，我们需要一个输出，作为预期输出发生的概率。有了这个概率，我们就应该能够对输出进行分类了。</li><li id="e920" class="kx ky hi kd b ke lj kh lk kk ll ko lm ks ln kw le lf lg lh bi translated">杠杆点可能会极大地影响这条线。</li></ul><h1 id="5afe" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">Sigmoid函数和预测</h1><p id="737f" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">sigmoid函数是一个神奇的函数，它能让我们得到期望的概率作为输出。<br/>它通过压缩0到1之间的任何值来实现这一点。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/698cc061611efa530ea7fd9a56858818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RqXFpiNGwdiKBWyLJc_E7g.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图2.1</figcaption></figure><p id="75b0" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">图2.1表示s形函数。它的数学公式是sigmoid(x) = 1/(1+e^(-x)).</p><p id="a8ff" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">类似于线性回归，我们也有权重和偏差。我们首先将输入乘以这些权重，然后加上偏差。最终结果将进入sigmoid函数，给出0到1之间的概率。</p><ul class=""><li id="96bc" class="kx ky hi kd b ke kz kh la kk lb ko lc ks ld kw le lf lg lh bi translated">z = x*w + b <br/>其中w是权重，b是偏差。</li><li id="2a80" class="kx ky hi kd b ke lj kh lk kk ll ko lm ks ln kw le lf lg lh bi translated">h_theta = sigmoid(z)</li></ul><p id="f25d" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">这些权重和偏差最初是随机选择的。但是，梯度下降算法将确保这些参数被更新，以很好地完成分类任务。</p><p id="847f" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">在这个sigmoid函数的帮助下，我们可以根据概率成功地预测输出</p><h1 id="3dd3" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">逻辑回归成本函数背后的直觉</h1><p id="ce6a" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">由于梯度下降是正在使用的算法，第一步是定义一个<strong class="kd hj">成本函数</strong>或<strong class="kd hj">损失函数。<br/> </strong>这个函数应该以这样一种方式来定义，它应该能够告诉我们我们的模型的预测与最初的结果偏离了多少。</p><p id="1410" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">那么，我们如何定义这样一个函数呢？</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ly"><img src="../Images/3fc72113e9a745e65885579aea1c848b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXRcbUTGYujX_TZBWT2IMw.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图3.1(吴宇森的讲座)</figcaption></figure><p id="9be7" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">在图3.1中，Cost(h_theta(x)，y)是我们一直在寻找的函数。但是，这个函数是如何工作的呢？为了理解这一点，我们需要将函数分成两部分。一部分解释实际输出为1时的功能，另一部分解释实际输出为0时的功能。<br/>简而言之，当实际已知目标为1时，我们希望模型的预测尽可能接近1，否则接近0。</p><p id="e1ea" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">在J(theta)的等式中，<em class="li"> Y </em>代表实际目标值，<em class="li"> h_theta </em>是我们模型的输出。<em class="li">h _θ</em>将在下面解释。但是，让我们假设我们的模型已经有一种方法来进行预测，并且我们有一个定义好的h_theta。这些预测将介于0和1之间。所以，我们会得到一个概率作为输出。</p><h2 id="0061" class="lz jk hi bd jl ma mb mc jp md me mf jt kk mg mh jv ko mi mj jx ks mk ml jz mm bi translated">第1部分:当Y = 1时</h2><p id="ad40" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">当实际目标是1时，我们希望我们的模型的预测尽可能接近1。因此，我们的成本函数应该随着我们的模型的预测远离1而趋向0而增加惩罚。我们的模型的惩罚应该减少，因为它的预测越来越接近1。因此，我们现在的目标是为此定义一个函数<br/>，这个函数就是:- log(x)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mn"><img src="../Images/1289537171e33c9ce1b24a68129c35e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*-lkg-7GB5ZB5P-kdymJYEA.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图3.2: -对数x图</figcaption></figure><p id="814d" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">在图3.2中，认为y轴是成本，x轴是模型的预测。<em class="li">注意:我们模型的预测不会超过1，也不会低于0。所以，这部分不在我们的担心范围之内。</em></p><p id="225c" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">当模型的预测更接近1时，惩罚更接近0。随着它从1进一步向0移动，罚值增加。Sol，实际目标为1时可以使用此功能。</p><h2 id="b084" class="lz jk hi bd jl ma mb mc jp md me mf jt kk mg mh jv ko mi mj jx ks mk ml jz mm bi translated">第2部分:当Y = 0时</h2><p id="557e" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">同样，当Y等于0时，我们不希望模型的预测尽可能接近0。这意味着更接近0的值的惩罚更低，而远离0且接近1的值的惩罚更高。因此，合适的函数是-log(1-h_theta(x))</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mo"><img src="../Images/ed60723f40a492d699e9b1ab096723ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*6qVastjnAmXy-n2l4AhX3w.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图3.3(成本函数的第二部分)</figcaption></figure><p id="4daf" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">图3.3表示成本函数的第二部分。即-log(1-h_theta(x))。</p><p id="f01c" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">假设X轴是我们的模型预测的值，Y轴是模型得到的惩罚，假设原始目标是0。</p><p id="d911" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">准备成本函数的两个部分。为了确保当y=1时第一部分激活，并且第二部分不干扰，并且当y=0并且第一部分不干扰时第二部分激活，我们将<em class="li"> y </em>和<em class="li"> (1-y) </em>项添加到成本函数中。<br/>最后，我们得到了图2.1中用蓝色突出显示的成本函数。</p><h1 id="2fa7" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">梯度下降和成本函数导数</h1><p id="97ac" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">现在，我们已经定义了一个成本函数，目标是找到最优的<em class="li"> w </em>和<em class="li"> b </em>，使得我们的数据集的成本函数最小化。这就是梯度下降的用武之地。通过这样做，模型学习参数，以减少它的惩罚，从而做出更准确的预测。</p><p id="9eb7" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated"><em class="li">梯度下降的算法就不再解释了。这在上面提到的链接</em>中有解释</p><p id="1747" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">我们希望找到成本相对于w和b是如何变化的，以便慢慢地改变原始的w和b来获得最优参数。<br/>逻辑回归成本函数梯度的推导如下图所示</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mp"><img src="../Images/85e9afede044513eaea8bb230b3d622a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*dlzaDViN3Zh7J2qOVI91HQ.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图4.1</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mq"><img src="../Images/e4a2a1c5c66c1eefb5facdbe8bc987ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*C9dVNor0MLJwBBROEWjCqQ.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图4.2</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mr"><img src="../Images/96181e01bcd4879a83fc27e27b98e89b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*I6AXoRONCe_Sn2WrXqjJZg.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图4.3</figcaption></figure><p id="65d1" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">找到梯度后，我们需要用原始的w和b减去梯度。我们进行减法，以便将梯度的值向斜率的相反方向移动，从而确保成本下降。</p><p id="7da1" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">成本函数是一个函数，它告诉我们我们的模型与我们能够创建的最理想的模型偏离了多少。因此，确保参数以降低该成本函数的方式被优化，将确保我们得到一个好的分类器，假设点是线性可分的，并且一些其他次要因素。</p><h1 id="2d71" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">结论</h1><p id="5bc4" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">与线性回归相似，我们定义了一个成本函数，用于估计模型预测与原始目标之间的偏差，并通过更新原始w和b，使用梯度下降法将其最小化。<br/>这确保了我们可以使用这些w和b来使用模型进行未来分类。使用sigmoid函数将连续输出转换为概率输出。</p><p id="483b" class="pw-post-body-paragraph kb kc hi kd b ke kz ij kg kh la im kj kk lo km kn ko lp kq kr ks lq ku kv kw hb bi translated">因此，逻辑回归和线性回归之间的唯一2个区别是逻辑回归中的成本函数和sigmoid函数，这使其适合于分类问题设置。</p></div></div>    
</body>
</html>