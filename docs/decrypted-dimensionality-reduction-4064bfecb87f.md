# 解密降维

> 原文：<https://medium.com/analytics-vidhya/decrypted-dimensionality-reduction-4064bfecb87f?source=collection_archive---------11----------------------->

今天，我们将看到降维。我们将介绍几种重要的特征提取和特征选择技术。让我们开始旅程吧…

随着特征数量的增加，模型变得更加复杂。特征的数量越多，过度拟合的可能性就越大。根据大量特征训练的机器学习模型越来越依赖于它被训练的数据，进而过度拟合，导致在真实数据上的表现不佳，达不到目的。

假设你有一条 100 码长的直线，你在上面的某个地方丢了一便士。不会太难找到的。你沿着这条线走，需要两分钟。现在让我们假设你有一个边长为 100 码的正方形，你在它的某个地方丢了一便士。这相当困难，就像在两个粘在一起的足球场上寻找一样。可能需要几天。现在是一个 100 码宽的立方体。这就像搜索一个足球场大小的 30 层建筑。呃。当你有更多的维度时，在空间中搜索会变得更加困难。

避免过度拟合是执行降维的主要动机。我们的训练数据的特征越少，我们的模型做出的假设就越少，就越简单。但这还不是全部，降维还有更多好处，比如-
1 *。更少的误导性数据意味着模型准确性的提高。
2。更少的维度意味着更少的计算。数据越少意味着算法训练越快。
3。更少的数据意味着需要更少的存储空间。
4。较少的维度允许使用不适合大量维度的算法
5。移除多余的特征和噪声。
6。将数据的维度减少到 2D 或 3D 可以让我们精确地绘制和可视化它。*

降维可以通过特征选择方法和特征工程方法来完成。特征选择是为您的样品识别和选择相关特征的过程。特征工程是通过对现有特征应用某种变换或执行某种操作，从现有特征手动生成新特征。

我们可以删除低方差的特征，因为它们对区分数据点和其他数据点帮助很小或没有帮助。因为方差取决于比例，所以您应该始终首先归一化您的要素。
我们还可以删除许多缺失值的特征。

# 特征提取技术-

## **问:为什么我们不能使用简单的投影将数据投影到更低的维度？**

**Ans。**因为它没有保存我们数据的全局或局部信息。虽然像 PCA 和 t-SNE 这样的技术保留了全局信息，但是 LDA 也保留了局部信息。

## PCA-

主成分分析(PCA)是一种无监督算法，它创建原始特征的线性组合。PCA 能够通过查看成对距离最大化方差和最小化重建误差来做到这一点。新要素是正交的，这意味着它们是不相关的。此外，它们按照“解释方差”的顺序排列第一个主成分(PC1)解释了数据集中最大的方差，PC2 解释了第二个最大的方差，这是第一个主成分无法解释的，依此类推。在执行 PCA 之前，应始终归一化数据集，因为变换取决于比例。如果你不这样做，规模最大的特征将主导你的新的主要组成部分。

从几何学角度来说，主成分代表解释最大方差的数据方向，也就是说，捕捉数据的大部分信息的线。这里，方差和信息之间的关系是，一条线携带的方差越大，数据点沿该线的离差越大，沿该线的离差越大，该线具有的信息就越多。

步骤-**1。标准化-** 这一步的目的是将连续初始变量的范围标准化，这样每一个变量对分析的贡献都是相等的，因为我们知道 PCA 对方差起作用。

**2。协方差矩阵计算-** 这一步的目的是了解输入数据集的变量是如何相对于平均值变化的，或者换句话说，看看它们之间是否有任何关系。

协方差矩阵是一个 p × p 对称矩阵(其中 p 是维数),其条目是与所有可能的初始变量对相关联的协方差。

**3:计算协方差矩阵的特征向量和特征值，识别主成分——**你首先需要知道的是，它们总是成对出现的，所以每个特征向量都有一个特征值。它们的数量等于数据的维数。协方差矩阵的特征向量实际上是方差最大(信息最多)的轴的方向，我们称之为主分量。特征值就是附加在特征向量上的系数，它给出了每个主分量的方差。通过按特征值的顺序排列特征向量，从高到低，你可以得到重要性排序的主成分。有了主成分后，为了计算每个成分占方差(信息)的百分比，我们用每个成分的特征值除以特征值之和。

**4。特征向量-** 特征向量是一个简单的矩阵，它以我们决定保留的组件的特征向量为列。这使它成为降维的第一步，因为如果我们选择只保留 n 个特征向量中的 p 个特征向量(分量)，最终的数据集将只有 p 维。

**5。沿着主成分轴重铸数据-** 在这一步中，也就是最后一步，目的是使用由协方差矩阵的特征向量形成的特征向量，将数据从原始轴重新定向到由主成分表示的轴(因此命名为主成分分析)。这可以通过将原始数据集的转置乘以特征向量的转置来实现。
最终数据集=特征向量。T * OriginalDataset。t(原始数据集应标准化)

这里要意识到的一件重要的事情是，主成分不太容易解释，没有任何实际意义，因为它们是由初始变量的线性组合构成的。

PCA 是一种实用的通用技术。实现起来又快又简单。 **PCA** 应**使用**主要用于强相关的变量。如果变量之间的关系很弱， **PCA** 不能**很好地**减少数据。参考相关矩阵来确定。一般来说，如果大部分相关系数小于 0.3， **PCA** 会**不会**有帮助。

PCA 是数据从一个坐标系到另一个坐标系的旋转。新数据科学家犯的一个常见错误是将 PCA 应用于非连续变量。虽然在技术上可以对离散变量或作为热编码变量之一的分类变量使用 PCA，但您不应该这样做。分类变量根本不是数字，因此没有方差结构。

在相对较小的数据集上，前几个组件可以解释数据集中几乎所有的差异。我见过其他数据科学家错误地认为，这意味着最后几个组件可以忽略不计，前几个组件才是最重要的特性。PCA 是特征选择的有效方法的唯一途径是，如果最重要的变量恰好是那些变化最大的变量。然而，这通常不是真的。

我们可以通过从 sklearn.decomposition 导入 PCA 来使用它，然后将 n 个分量设置为输出数据集中的特征数。

虽然 PCA 降低了维数，但是当处理多类数据时，有必要以类间分离的方式降低维数。LDA 是用于相同目的的算法。

此外，主成分分析受异常值的影响很大。

## 线性判别分析

LDA 就像 PCA 的意思是降维技术，但它的重点是最大化已知类之间的可分性。它被用作分类、降维和数据可视化的工具。这是监督学习中最常用的降维技术。

> LDA 使用两个标准来创建新轴:
> 1。最大化两个类的平均值之间的距离。
> 2。最小化每个类中的变化。

LDA 方法与主成分分析非常相似，两者都是线性变换降维技术，但也追求一些差异；

*   LDA 和 PCA 最早的区别是 PCA 可以做更多的特征分类，而 LDA 可以做数据分类。
*   当在 PCA 下变换到另一个空间时，真实数据集的形状和位置改变，而在 LDA 中变换到不同空间时，形状和位置没有改变。LDA 只提供了更多的类可分性。
*   PCA 可以表示为无监督算法，因为它避免了类标签，并且专注于寻找方向(主成分)以最大化数据集中的方差，与此相反，LDA 被定义为监督算法，并且计算方向以呈现轴并最大化多个类之间的间隔。

如果您愿意将维数减少到 1，您可以将所有内容都投影到 x 轴，但 LDA 使用来自两个特征的信息来创建一个新轴，从而最小化方差并最大化两个变量的类距离。

您可以通过三个步骤实现这一点:

*   首先，你需要计算类之间的可分性，即不同类的平均值之间的距离。这被称为*类间方差*。

![](img/af88d8bff7418ddbf95e418b61db959a.png)

*   其次，计算每一类的均值与样本的距离。它也被称为类内方差。

![](img/5972d3998b4ff733c9861720d5553db9.png)

*   最后，构造最大化类间方差和最小化类内方差的低维空间。p 被认为是低维空间投影，也叫费雪准则。

![](img/7fe165311ed9251a7ec4d146c4fb292f.png)

然后我们对我们的特征矩阵 X 和 P 进行点积，以获得新的特征矩阵。

LDA 模型对您的数据做出的假设:

*   数据中的每个变量在绘制时都是钟形曲线，即高斯曲线。
*   每个变量的值围绕平均值变化相同的量，即每个属性具有相同的方差。

但是当分布的平均值被共享时，线性判别分析失败，因为 LDA 不可能找到使两个类线性可分的新轴。在这种情况下，我们使用非线性判别分析。

该方法有许多扩展和变化。一些流行的扩展包括:

*   **二次判别分析(QDA)** :每一类都使用自己的方差(或有多个输入变量时的协方差)估计。
*   **灵活判别分析(FDA)** :使用非线性输入组合，如样条函数。
*   **正则化判别分析(RDA)** :将正则化引入方差(实际上是协方差)的估计，调节不同变量对 LDA 的影响。

> 记住，PCA 和 LDA 都是线性技术。只有当我们的数据是线性可分时，它们才是有效的。

PCA 在每类样本数较少的情况下表现更好。而 LDA 对于具有多个类的大型数据集工作得更好

我们以后会更多地见到艾达。

## 问:何时使用 LDA 而非 PCA，反之亦然？

1.  当数据集很小时，使用主成分分析
2.  如果每个类别的样本不相同，则使用 PCA
3.  对于多类使用 LDA
4.  如果你的数据集具有正态分布，LDA 是更好的选择。

## 奇异值分解(SVD)

[](https://towardsdatascience.com/understanding-singular-value-decomposition-and-its-application-in-data-science-388a54be95d) [## 理解奇异值分解及其在数据科学中的应用

### 在线性代数中，矩阵的奇异值分解(SVD)是将矩阵分解成三个部分的分解

towardsdatascience.com](https://towardsdatascience.com/understanding-singular-value-decomposition-and-its-application-in-data-science-388a54be95d) 

总之，两者使用几乎相同的方法。PCA 使用协方差矩阵的特征分解。对称矩阵通过沿向量的特征向量拉伸或收缩来变换向量。但这不是非对称矩阵的情况，因此我们不能对非对称矩阵使用特征分解，这就是 SVD 发挥作用的地方。我们也可以通过奇异值分解来执行 PCA。事实上，sklearn 的 PCA 使用 SVD 而不是特征值分解，因为这有许多好处，并且这也使 PCA 能够处理稀疏矩阵。所以对于 ***降维来说，使用 SVD 而不是 PCA 通常没有好处。*** 除了降维，奇异值分解还有很多应用。

## t-SNE(t-分布式随机邻域嵌入)

t-SNE 是一种无监督的非线性技术，主要用于数据探索和可视化高维数据。t-SNE 算法计算高维空间和低维空间中的实例对之间的相似性度量。然后，它尝试使用成本函数来优化这两个相似性度量。

然而，t-SNE 不是一种聚类方法，因为它不像 PCA 那样保留输入，并且值可能经常在运行之间改变，所以它纯粹是为了探索。当然，t-SNE 并不是唯一使用局部结构的方法。有很多其他的方法，但是 t-SNE 在实践中效果很好。它还尽可能地解决了拥挤问题(*高维空间中有些相似的点在低维空间中相互重叠*)，因为它无法完全解决。t-SNE 试图在变换过程中最好地保持同一邻域中的数据点之间的距离，因此比其他技术更好地解决了拥挤问题。此外，它使用局部和全局结构(但很少)，这使它比其他人更好。但是不能保证全局结构的保持。

拥挤问题请参考-【https://www.youtube.com/watch?v=hMUrZ708PFk】T4&list = PLupD _ xfct 8 mhqckuaxmexhe 0 ajndu 0 MHz&index = 4

因为分布是基于距离的，所以所有的数据都必须是数字。您应该通过二进制编码或类似的方法将分类变量转换为数字变量。对数据进行标准化通常也很有用，这样每个变量都处于相同的范围内。这避免了具有较大数值范围的变量主导分析。

步骤-

**第一步:创建概率分布**

1.  假设你选择一个点

![](img/d36bcc55257b96bfe4028e4de992fadd.png)

在数据集中

2.然后，将在数据集中选取另一个点 x_j 作为邻居的概率定义为

![](img/98aec5c9a176d6fe1be50814281146e2.png)

这个概率与以 x_i 为中心的一个**高斯**的概率密度成正比，对于较远的点，被选为邻居的概率迅速恶化，但永远不会达到 0。高斯分布是一种常用的分布，是点之间相似性的概率度量的自然选择。

在 SNE 霸王龙中，我们希望所有点的邻居数量大致相同，以防止任何一个点产生不相称的影响。这意味着我们想要

![](img/2130db6170773dd47a4e70e68e9e420f.png)

对于人口密集区域的点较小，对于稀疏区域的点较大。因此，我们不能对所有的点都保持相同的标准差。我们对此进行量化的方法是指定一个名为**困惑**的超参数。困惑度基本上是任意点的有效邻居数，t-SNE 算法对 5 到 50 之间的任意值都相对有效。较大的困惑会考虑更多的全局结构，而较小的困惑会使嵌入更集中于局部。基本上，困惑度越高，方差的值就越高。

**步骤 2:重新创建概率分布**

让我们来表达低维映射

![](img/f8a6785696f2ebd30d9f33795461a021.png)

如同

![](img/bd80514911976a57472a63025c383a53.png)

我们的基本直觉是，我们希望让低维映射表达类似的概率分布。我们也可以通过高斯分布来做到这一点，但它有一个“短尾巴”，这意味着附近的点可能会挤在一起，从而导致拥挤问题。

为了分散这些观点，我们想要一个有较长尾部的概率分布。这就是为什么 t-SNE 使用单自由度的学生 t 分布(也称为柯西分布)。具体来说，分布是:

![](img/2cbcc1583ab9f0cb77ba1bd91778439a.png)

通过对分布 p 和 q 之间的 KL 散度进行梯度下降来优化该分布。KL 散度是一个概率分布如何偏离第二个预期概率分布的度量。

梯度可以表示为

![](img/177792a304ecf699f9f1cf5f13c7fa54.png)

直观上，梯度代表两点之间吸引/排斥的强度和方向。正梯度表示吸引，而负梯度表示两点之间的排斥。这种“推拉”最终使点在低维空间沉淀下来。

它也使用早期的夸张手法。它控制原始空间中的自然簇在嵌入空间中的紧密程度以及它们之间的空间大小。对于较大的值，自然簇之间的空间在嵌入空间中会较大。同样，该参数的选择并不十分关键。

当允许嵌入点自由移动时，有可能过早地形成不想要的簇，导致参数陷入局部最小值。为了防止这一点，SNE 霸王龙使用了“**提前压缩**的招数。这种技巧包括在优化的早期阶段简单地向成本函数添加 L2 惩罚，以使嵌入点之间的距离保持较小。这种优化的优势是一个超参数，但无论如何设置，t-SNE 都表现得相当稳健。

> 然而，在这个过程之后，输入特征不再是可识别的，并且您不能仅基于 t-SNE 的输出做出任何推断。t-SNE 也没有学习任何从高维到低维的映射函数。因此，我们不能在测试时将一个新的数据点转换到更低的维度来完成预测任务。因此，t-SNE 不能用于降维，只能用于可视化和数据探索。另一方面，我不会将 t-SNE 的输出作为分类器的输入。主要是因为 t-SNE 是高度非线性的，有点随机，你可以得到非常不同的输出，取决于不同的运行和不同的困惑值。

# SNE 霸王龙的局限性

与 PCA 等方法不同，t-SNE 是非凸的，这意味着它有多个局部最小值，因此更难以优化。这意味着，每次运行它都会给出不同的结果。

一个假设是流形的局部结构是线性的。这个假设很重要的原因是相邻点之间的距离是以欧几里德距离来度量的，欧几里德距离假设是线性的

不能保证全局结构的保持。它仅在大的困惑值下被保留。但是我们不使用大的值，因为那样我们的可视化就没有用了。因此，我们不能很好地保护全球结构。

tSNE 的计算消耗了太多的内存，这在使用**大困惑**时变得尤为明显

如果特征数量非常多，强烈建议使用另一种降维方法(例如，针对密集数据的 PCA 或针对稀疏数据的 TruncatedSVD)将维度数量减少到合理的数量(例如，50)，然后执行 t-SNE 进行可视化。TruncatedSVD 给出与 SVD 相同的结果，但是比 SVD 快得多。

与 PCA 相比，t-SNE 在相同样本量的数据上花费的时间要长得多。在百万样本数据集上可能需要几个小时，而 PCA 将在几秒或几分钟内完成。它在数据点数量上具有二次时空复杂度。这使得它在应用于包含超过 10，000 个观察值的数据集时特别慢并且消耗资源。

PCA 它是一种数学技术，但是 t-SNE 是一种概率技术。

t-SNE 方法不知道类别标签；完全无人监管。尽管如此，它仍然可以找到数据的二维表示，根据原始空间中各点的接近程度来清楚地划分类别。

我们可以用它-

从 sklearn.manifold 进口 TSNE

![](img/2e0d11f4951d7663b29c1ee9be95f97b.png)

因此，t-SNE 在大多数情况下比其他数据可视化技术更好，但在降维方面却不行。它非常适合于高维数据集的可视化。它在可视化方面也有好处，因为它是一种非线性技术。

## 我们不能用 SNE 霸王龙来解释集群大小、集群间距离和集群密度。

## 要记住的几点-

1.  对相同的参数运行 t-SNE 多次。由于每次都会给出不同的结果，因此建议多次运行 t-SNE 以便更好地理解。
2.  运行 t-SNE 迭代，直到形状稳定。
3.  尝试不同的困惑值，因为每个值给出不同的结果，然后通过考虑所有这些值来解释。

所以，在现实生活中，我们运行 t-SNE 很多次，选择最好的结果。

流形学习不是 PCA 的另一种变体，而是一种推广。在主成分分析中表现良好的东西几乎保证在 t-SNE 或另一种流形学习技术中表现良好，因为它们是一般化的。 很像一个物体是苹果也是水果(一个概括)，如果某个东西没有产生与它的概括相似的结果，通常是有问题的。另一方面，如果两种方法都失败了，数据可能天生就很难建模。

## UMAP( **一致流形逼近和投影**)—

UMAP 是麦金尼斯等人的一项新技术，与 t-SNE 相比，它有很多优点，最明显的是速度更快，更好地保存了数据的全局结构。UMAP 可以在不到 3 分钟的时间内投影 784 维、70，000 点的 [MNIST](http://yann.lecun.com/exdb/mnist/) 数据集，相比之下，scikit-learn 的 t-SNE 实现需要 45 分钟。

与 t-SNE 相比，UMAP 的输出之间的最大差异是本地和全球结构之间的平衡——UMAP 通常更擅长在最终投影和速度中保留全球结构。这意味着集群间的关系可能比 t-SNE 更有意义。然而，值得注意的是，因为 UMAP 和 t-SNE 在投影到较低维度时必然会扭曲数据的高维形状，所以较低维度中任何给定的轴或距离仍然不能直接用 PCA 等技术来解释。

请注意，使用 t-SNE，需要极高的困惑度(~1000)才能开始看到全局结构的出现，并且在如此大的困惑度值下，计算时间会显著延长。同样值得注意的是，t-SNE 投影在不同的运行中变化很大，不同的高维数据被投影到不同的位置。虽然 UMAP 也是一种随机算法，但令人惊讶的是，每次运行产生的投影是如此相似，而且参数不同。这也是因为与 SNE 相比，UMAP 更加重视全球结构。UMAP 是一种通用的降维技术，可用作机器学习的预处理，而 t-SNE 则不是这种情况

在这里，聚类大小和聚类之间的距离也无关紧要，绘制一个以上的图总是明智的。

像 t-SNE、UMAP 等算法也被称为基于邻居的算法

**它到底是如何工作的-**

它的工作方式和 SNE 霸王龙很相似，但是有一些不同

*   UMAP 在高维度中使用指数概率分布，但是 T2 不需要像 tSNE 那样的欧几里德距离，而是任何距离都可以被插入。另外，概率是**未归一化:**

![](img/0d3b1767ac9c4bb74fd3112054ab7705.png)

这里 *ρ* 是一个重要的参数，表示每个第 I 个数据点到其第一个最近邻的距离。这确保了管汇的**局部连接性**。换句话说，这为每个数据点提供了一个局部自适应指数内核，因此**距离度量从点到点**变化。

*   **UMAP 没有将标准化**应用于高维或低维概率，这与 tSNE 非常不同，感觉很奇怪。然而，仅从高维或低维概率的函数形式中，人们可以看到它们已经针对段[0，1]进行了缩放，并且结果是，归一化的**缺失，像分母**一样，显著减少了计算高维图**的时间** **。**
*   UMAP 使用最近邻居的数量而不是困惑度。

![](img/c526e2e5c41372eb39867d7fdecdab63.png)

*   UMAP 使用曲线族 1/(1+*a* y*^(2*b*)对**低维中的距离概率建模，不完全是学生 t 分布，但非常非常相似**，请再次注意**没有应用归一化**:

![](img/c8558fd9468706756979251361bf0601.png)

其中 *a* ≈1.93， *b* ≈0.79 为默认 UMAP 超参数(实际上为 min_dist = 0.001)。在实践中，UMAP 用 **min_dist** 超参数从非线性最小二乘拟合到分段函数中找到 *a* 和 *b* :

![](img/78a5746f6c440a5121e894731b1a7e13.png)

*   UMAP 使用**二元交叉熵(CE)** 作为代价函数，而不是像 tSNE 一样使用 KL-divergence。

![](img/0ef330a001536f2223a9d99b50f9f297.png)

CE 成本函数中的这一额外(第二)项使 UMAP 能够捕获**全局数据结构**，这与 tSNE 相反，tSNE 只能以中等的困惑值对局部结构建模。

*   最后，UMAP 使用**随机梯度下降(SGD)** 而不是像 tSNE / FItSNE 一样的常规**梯度下降(GD)** ，这既加快了计算速度，又消耗了更少的内存。

详情请参考[https://umap-learn . readthe docs . io/en/latest/how _ umap _ works . html](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html)。

# 为什么 UMAP 能保留全球结构

与 tSNE 相反，UMAP 使用**交叉熵(CE)** 作为成本函数，而不是 KL 散度:

![](img/1f026bc48fedc7107e6b42e6a4f88819.png)

这导致局部-全球结构保存平衡的巨大变化。在 **X** 的小值处，我们得到与 tSNE 相同的限制，因为第二项由于预因子和对数函数比多项式函数慢的事实而消失:

![](img/3d5a71cce00020f7d2ec76a87204cb87.png)

因此 **Y** 坐标被迫很小，即 *Y* → 0，以使惩罚最小化。这与 tSNE 的行为完全一样。然而，在大的 **X** 的相反极限，即 *X* →∞时，第一项消失，第二项的预因子变为 1，我们得到:

![](img/7e253fe49db9c0bb17aeb2f675e41a8b.png)

这里，如果 **Y** 很小，我们会得到很高的惩罚，因为 **Y** 在对数的分母中，因此 **Y 被鼓励变大，使得对数下的比率变成 1，我们得到零惩罚**。因此我们得到了 *Y* → ∞ at *X* → ∞因此当从高维空间移动到低维空间时**全局距离被保留**，这正是我们想要的。

# 为什么 UMAP 比多伦多快

*   首先，我们**在最近邻数量的定义中去掉了对数部分**，即不像 tSNE 那样使用全熵:

![](img/2dbb067f968acac352983265a04e54dc.png)

由于在算法上对数函数是通过泰勒级数展开来计算的，并且由于对数函数比线性函数慢，实际上在线性项前面放一个对数前因子并不会增加太多，因此最好完全跳过这一步。

*   第二个原因是我们**省略了高维和低维概率的归一化**
*   **应用随机梯度下降(SGD)代替常规梯度下降(GD)**
*   增加原始数据集中的维数，我们在数据上引入稀疏性，即我们得到越来越碎片化的流形，即有时有**密集区域**，有时有**孤立点** **(局部破碎流形)**。UMAP 通过引入**局部连通性 *ρ* 参数**解决了这个问题，该参数通过引入考虑了局部数据连通性的自适应指数核将稀疏区域(在某种程度上)粘合在一起。这正是为什么 **UMAP 可以(理论上)处理任意数量的维度，并且在将其插入主降维过程之前不需要预降维步骤(自动编码器，PCA)** 的原因。

> 只要 tSNE 使用 KL-divergence 作为代价函数，它就不能在全局距离保持上与 UMAP 竞争

目前还不知道 umap 的任何缺点。

要使用，你必须安装软件包-

conda 安装-c conda-forge umap-learn

[https://umap-learn.readthedocs.io/en/latest/api.html](https://umap-learn.readthedocs.io/en/latest/api.html)为它的方法。

使用请参考 https://github.com/lmcinnes/umap 的

【https://umap-learn.readthedocs.io/en/latest/parameters.html】为所有参数使用

参考[https://umap-learn . readthedocs . io/en/latest/embedding _ space . html](https://umap-learn.readthedocs.io/en/latest/embedding_space.html)了解何时使用哪种嵌入。

*正如我们所知，UMAP 也是一种随机算法，因此最好绘制多个图表，以便更好地了解数据。*

我们还可以使用多线程来获得极快的性能。详情请参考[https://umap-learn . readthe docs . io/en/latest/reproducibility . html](https://umap-learn.readthedocs.io/en/latest/reproducibility.html)。

与非监督降维相比，更大的 n-邻居用于监督降维。此外，提供类标签使分类更加清晰。如果您有已知类的数据，并且希望在对单个点进行有意义的嵌入的同时对它们进行分离，那么监督 UMAP 正好可以满足您的需求。t-SNE 不能用于监督降维。

UMAP 比 SNE 霸王龙快得多，但还是慢得多。如果你的数据有很多维度(300 以上)，那么首先尝试通过主成分分析将特征减少到 40-50 个，然后通过 UMAP 进一步减少。因为对大数据应用 UMAP 可能会导致非常多的时间和内存。

> 对于较小的数据集，时间和复杂性并不重要，因为它们大致相同。这些对于更大的数据集很重要。数据越大，需要的时间和内存就越多。但是，与其他算法相比，一些算法对于较小的数据集花费更多的时间，但是对于较大的数据集花费较少的时间，反之亦然。

![](img/43dd03997e72048f791398d38d4467fc.png)

根据线的斜率，对于更大的数据集，UMAP 和 SNE 霸王龙之间的差异只会越来越大。

## 简单总结一下我们在 UMAP 做的事情

假设我们有一个 K 维的数据集。如果我们在更高的维度上捕捉点的拓扑表示，那么我们可以通过保持相同的拓扑在更低的维度上变换它们。因此，为了捕捉拓扑表示，我们必须为它生成一个开覆盖。所以我们给我们的数据点生成像圆圈一样的覆盖。

![](img/40ba427ca0f2eb812299179a0883579e.png)

然后，我们将同一封面下的数据点与和该封面接触的所有其他数据点连接起来，结果是这样的-

![](img/5ca0a41899ed463031a008bad1ecbea1.png)

但是这导致了几个不相连的组件，所以这不能表示我们的拓扑表示。

因此，我们将其设置为-关于一个点的覆盖延伸到该点的第 *k* 个最近邻，其中 *k* 是我们用来近似本地距离感的样本大小。这意味着，每个点都有自己独特的距离函数，我们可以简单地选择半径为 1 的球，相对于这个局部距离函数！

![](img/c4ae357318b7d6f93596f6811798c630.png)

选择小的 *k* 意味着我们想要一个非常局部的解释，这将更准确地捕捉精细的细节结构和变化，而较大的 k 将携带更多的全局信息。

我们可以进一步使用模糊拓扑——在这里，在一个开集里不再是二元的是或否，而是一个介于零和一之间的模糊值。显然，当我们远离球的中心时，点在给定半径的球中的确定性将会衰减。

![](img/f203436d3bc594dd1f878b5a968627b6.png)

请记住，作为一种拓扑表示，整个图形应该是连通的，这意味着每个点都应该至少与一个其他点相连，也就是说，我们应该完全相信模糊圆会延伸到每个点的最近邻居。

然后，我们将同一封面下的数据点与和该封面接触的所有其他数据点连接起来。

![](img/50fe2ce75bc83371c224e86de27fa16c.png)

正如我们看到的，现在它是一个连通图，所以我们可以捕捉它的拓扑表示。

但是，每个点都有其自己的相关联的局部度量，并且从点 *a* 的角度来看，从点 *a* 到点 *b* 的距离可能是 1.5，但是从点 *b* 的角度来看，从点 *b* 到点 *a* 的距离可能仅仅是 0.6。哪一点是对的？我们如何决定？回到我们基于图形的直觉，我们可以认为这是具有不同权重的有向边。在任意两点之间，我们可能有两条边，并且这些边上的权重彼此不一致。然后，为了将其转换为无向图，我们在整个图中应用边权重组合公式。

然后我们将其转换到低维，然后使用交叉熵来保持拓扑表示不变。在低维中，我们用欧氏矩阵来度量点与点之间的距离。我们在 t-SNE 中也做了同样的事情(在更高的维度中捕获距离，然后将其投影到更低的维度，然后优化数据点以具有相同的距离)，但是通过使用不同的方法。但是 t-SNE 并不是如此的合适和有原则。

***所以，UMAP 也同样喜欢 t-SNE，但用的只是更恰当和准确的方法。***

## Isomap-

再次是非线性无监督维数减少流形技术，其使用等距映射和邻居数量的概念。线性方法基于**欧几里德距离**来降低维度，而 ISOMAP(等距映射)在多元数据点中使用**测地线距离**方法。

Isomap 按照以下步骤工作:

1.  它根据流形距离确定相邻点，并连接固定半径内的点。
2.  它计算在上述步骤中确定的点之间的测地线距离。
3.  然后通过对测地距离度量进行特征值分解，找到数据的低嵌入。

在非线性流形中，当且仅当邻域结构可以近似为线性时，欧几里德距离度量才成立。如果邻域包含空洞，那么欧几里德距离可能会产生很大的误导。因此，我们在这里使用测地线距离。

![](img/0d681124b7de5a2365fea9bb27d9f312.png)

现在，如果我们看看基于欧几里德度量的 1-D 映射，我们看到对于相距很远的点(a & b)映射得很差。只有可以近似位于线性流形(c & d)上的点才能给出满意的结果。另一方面，看看测地线距离的映射，它很好地将近点近似为邻居，将远点近似为远点。图像中两点之间的测地线距离由两点之间的图形距离来近似。

你可以通过 sklearn 实现为-

> *从 sklearn.manifold 导入 Isomap*

但是现在这种技术用得不多了，因为我们有更好的技术，比如 UMAP 的 tSNE，所以我们就不再赘述了。

![](img/cea60bdbdc18e1922915a6aa1f73f5d5.png)

## 局部线性嵌入(LLE)

LLE 算法是一种无监督的非线性降维方法。LLE 首先找到这些点的 k 个最近邻。然后，它将每个数据向量近似为其 k 个最近邻的加权线性组合。最后，它计算从其相邻向量中最佳重构向量的权重，然后产生由这些权重最佳重构的低维向量。它通过使用均方误差来优化权重。

*它可以被认为是一系列局部主成分分析，它们被全局比较以找到最佳非线性嵌入。*

LLE 算法的一个优点是只有一个参数需要调整，即 K 值，或者被认为是聚类的一部分的最近邻居的数量。

它有时可能比 PCA 表现得更好，有时则更差。

但是这个现在也不用了，所以我们也不讨论这个了。

## 独立成分分析-

ICA 是一种线性降维方法，它将数据集转换成独立分量的列。盲源分离和“鸡尾酒会问题”是它的其他名称。它假设每个数据样本都是独立成分的混合物，并且它的目标是找到这些独立成分。独立组件意味着两个组件之间没有线性或非线性依赖关系。

PCA 和 ICA 的主要区别在于 PCA 寻找不相关的因素，而 ICA 寻找独立的因素。

因此，由于 PCA 生成主成分，ICA 生成独立成分。但是它不能将特征排序为 PCA。因此，PCA 给出了比 ICA 更好的降维和降维结果。但是当主成分分析的假设不成立时，它可以代替主成分分析。否则很少用于降维。但它用于不同的任务。例如，ICA 可用于分离两个信号，如同一音频中两个人的声音等。

## 因素分析-

[](https://www.datacamp.com/community/tutorials/introduction-factor-analysis) [## Python 中的因子分析简介

### 因子分析是一种探索性的数据分析方法，用于寻找潜在的影响因素

www.datacamp.com](https://www.datacamp.com/community/tutorials/introduction-factor-analysis) 

因子分析用几个潜在的但不可观察的随机量(称为因子)来描述许多变量之间的协方差关系。因子分析认为变量可以根据它们的相关性进行分组。可以假设特定组中的变量之间高度相关，但是它们与不同组中的变量的相关性相对较小。那么可以说，每组变量代表一个单一的潜在结构(或因素)，它负责观察到的相关性。因子分析可以看作是对协方差矩阵σ的一种近似尝试。

1.  如果您假设或希望测试导致观察变量的潜在因素的理论模型，请运行因子分析。
2.  如果您想简单地将相关的观察变量减少到一个更小的重要独立复合变量集，请运行主成分分析。

*   PCA 有助于减少变量的数量，同时保留数据中的大部分信息，而 EFA 有助于测量未观察到的(潜在的)、无误差的变量。
*   当变量没有任何共同点时，如上面的例子，EFA 不会找到一个明确定义的潜在因素，但 PCA 会找到一个明确定义的主成分来解释数据中的最大方差。

你可以用它-

> 来自 sklearn.decomposition 导入因子分析

## 多维标度-

**多维标度**是一种距离保持的流形学习方法。所有的流形学习算法都假设数据集位于低维的平滑、非线性流形上，并且可以通过保留更高维空间的一个或多个属性来找到映射***f:RD->RD***(D>>D)。

MDS 取一个相异矩阵 *D(* )这个**相异矩阵**是一个**矩阵**，它表示两个集合之间的相似性配对。 *)* 其中 Dij 表示点 *i* 和 *j* 之间的距离(通常为欧几里德距离)，并在较低的维度上产生映射，尽可能地保持差异。记住，相异度/距离矩阵是对角线等于零的对称矩阵。

因此，它保持距离，意味着如果两个点在特征空间中是接近的，那么它在潜在因子空间中应该是接近的。

![](img/a7bc34187a1f326225840f9c5e8c2b83.png)

它有两种类型-

**公制 MDS-**

我们计算数据的相异矩阵。然后通过优化一个代价函数，在低维中找到一个最优配置。

![](img/8af62b85728cedd65fc090180253364c.png)

这个损失函数也被称为应力。因此，给定一个距离或相异矩阵 D(X)，MDS 试图找到上面称为 f (dij)的 D(Y)和 n 个数据点 y1，y2，…。yn，通过优化上述损失函数。

度量 MDS 有一个称为经典 MDS 的子类型，它不是优化成本函数，而是使用特征值分解。它的应用更广泛，所以我们将只讨论它。它也被称为主坐标分析(PCoA)。

它不同于主成分分析，因为主成分分析基于样本之间的相关性创建图，而 MDS 基于样本之间的距离创建图。样本彼此越接近，它们就越接近。

![](img/51c9434959626b411cb26d3a1c17ef68.png)

它通过计算每个要素样本之间的成对距离来计算距离。我们可以用任何方法计算距离，比如欧几里德距离。但是在使用欧几里德距离时，它给出了与 PCA 相同的结果，因为基于最小化线性距离的聚类与最大化线性相关性是相同的。因此，我们通常使用其他措施。MDS 的解释与 PCA 相同，即具有较大特征值的特征向量解释了大多数数据。

> 公制 MDS 适用于定量数据(不是序数)。对于有序数据，我们使用非公制 MDS。

**非公制 MDS-**

它也被称为**序数 MDS** 。在这里，重要或有意义的不是距离值的度量，而是它相对于其他对象对之间的距离的值。它作用于等级顺序而不是距离。

度量多维缩放创建点的配置，其点间距离接近给定的相异度。这有时是一个过于严格的要求，而非公制的缩放被设计来稍微放松它。非度量缩放不是试图逼近相异点本身，而是逼近它们的非线性但单调的变换。由于单调性，输出图上更大或更小的距离将分别对应更大或更小的相异度。

NMDS 是一个**[](https://sites.google.com/site/mb3gustame/reference/ranked-data)**。这意味着原始距离数据被替换为等级。因此，不是物体 A 距离物体 2.1 个单位，距离物体 4.4 个单位，而是物体 C 是距离物体 A“第一”最远的，而物体 C 是“第二”最远的。虽然关于距离大小的信息丢失了，但是基于等级的方法通常对于不具有可识别分布的数据更稳健。因此，它也可以容忍缺失的成对距离。****

****非公制 MDS 算法的基本步骤是:****

1.  ****找出点的随机配置，例如从正态分布中取样。****
2.  ****计算两点之间的距离 d。****
3.  ****找到近似的最佳单调变换，以获得最佳缩放的数据 f(x)[ **单调变换**是一种将一组数**变换为另一组保持原始组顺序的方法，它是一个将实数映射为实数的函数，它满足以下性质:如果 x > y，则 f(x) > f(y)，简单地说，它是一个严格递增函数。]******
4.  ****通过寻找新的点配置，最小化最佳缩放数据和距离之间的压力。****
5.  ****将压力与某个标准进行比较。如果应力足够小，则退出算法，否则返回 2。****

****设 x 表示随机配置的向量，f(x)表示单调变换，d 表示点距离，则****

****![](img/58566cc746f74314ff1106f9256e0e58.png)****

****MDS 已经有了距离形式的输入矩阵(即城市之间的实际距离)，因此距离在输入矩阵中有意义，并根据这些距离创建实际物理位置的地图，而在非公制的 MDS 中，距离只是等级的表示(即高到 7 或低到 1)，它们本身没有任何意义，但它们需要使用欧几里德几何创建地图，然后地图仅显示由地图上坐标之间的距离表示的等级相似性。****

****虽然 PCA *为您保留了* **m** 的重要维度，但非公制的 MDS *将*的配置与 **m** 的维度相匹配(您预先定义了 **m** )，并且它比 PCA 通常能更直接、更准确地在地图上再现差异****

> ****通常我们更关心相对定位而不是绝对差异，在这种情况下，非公制比公制 MDS 更受欢迎。****

****我们可以通过-****

> ****从 sklearn.manifold 进口 MDS****

******PCA vs MDS-******

****MDS 可用于我们不知道坐标、只知道相对距离或主成分分析假设不匹配的数据。否则五氯苯甲醚就足够了。****

******t-SNE vs MDS-******

****SNE 霸王龙不是为了保持距离而设计的，而 MDS 是为了保持距离。t-SNE 只是聚集相同的数据，但它可以放在任何地方。例如，如果一些数据点位于北侧，那么 SNE 霸王龙可以将它们聚集在一起，并将其放置在东侧，从而扭曲方向。t-SNE 显示出比仅在局部数据点对上工作的经典多维尺度更好地保持全局结构。****

****但是 MDS 只适用于小数据集，对于大数据集，它比其他技术花费更多的时间。对于小数据集，它比 t-SNE 快，但比其他常见的降维技术慢，但对于大数据，它甚至比 t-SNE 需要更多的时间。****

****![](img/1996b982a0e1d41f12271721b00d3cd0.png)****

****我们有时也使用 MDS 的 sammon 映射，而不是常规的 MDS，因为它可以更好地保留局部精细结构。****

****Isomap 也有点像 MDS，但它使用测地线距离。即使是 SNE 霸王龙也可以被解释为有特殊功能的 MDS。****

# ****特征选择技术-****

## ****过滤方法-****

****过滤方法通过单变量统计而不是交叉验证性能来提取特征的内在属性。这些方法比包装方法速度更快，计算成本更低。当处理高维数据时，使用过滤方法在计算上更便宜。****

******信息增益-******

****信息增益或 IG 衡量一个特征给出了关于该类的多少信息。因此，我们可以确定在给定的一组训练特征中，哪个属性对于区分要学习的类别最有意义。****

****因此，我们计算目标和其他特征之间的信息增益，并选择得分最高的特征。当它用于特征选择时，它被称为互信息。它提供了通过包含给定特征向模型传递多少有用信息的度量。因此，包含具有高互信息的要素有助于提高模型精度。****

****![](img/3aca4abca5bbd00ad8582a5a5daaf6b0.png)****

****它可以用作-****

> ****从 sklearn.feature_selection 导入 mutual_info_classif 进行分类，从 sklearn.feature_selection 导入 mutual_info_regression 进行回归****

****真正的互信息不可能是负数。如果它的估计结果是负的，它就被零代替。****

****它还保留了特征和目标之间的非线性关系。****

******统计检验-******

****我们使用 t 检验、卡方检验、方差分析等统计检验来计算 p 值，然后使用它来进行特征选择。*有关统计测试的更多详情，请参考第 45 页注释 1。*****

****因此，我们计算每个特征在改进模型 w.r.t 目标中的重要性。****

****![](img/64973ffe02c56302292d2cd1f2744029.png)****

****这些测试只能说明哪些特征与目标变量的关系最密切。****

****在特征选择的情况下，我们希望测试的假设是:真或假:这个特征与响应变量无关。我们想对每个特征测试这个假设，并决定这些特征在预测反应中是否有意义。在某种程度上，这就是我们处理关联逻辑的方式****

****我们通常将这些统计测试或互信息或其他测试的结果交给 sklearn 的`[**SelectKBest**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) **or** [**SelectPercentile**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile)`，这些函数作为测试进行上述测试，以提供具有所需数量特征的输出。****

****需要记住的一点是，筛选方法不会移除多重共线性。因此，在为数据训练模型之前，还必须处理要素的多重共线性。****

****这些是过滤方法。请记住，与包装方法不同，它们不依赖于模型，而只依赖于数据。它们非常适合于消除不相关的、多余的、不变的、重复的和相关的特征。****

******相关系数-******

****我们一般去除与目标变量相关性低的特征。****

****对于线性关系，我们使用皮尔逊相关系数。如果两个变量是非线性但单调的，那么我们可以使用 Spearman 的秩相关系数。这很像皮尔逊相关性，但是皮尔逊相关性评估线性关系，而斯皮尔曼相关性评估单调关系(无论线性与否)。****

****斯皮尔曼系数对连续和离散序数变量都适用。如果数据是离散的，但不是有序的，那么我们使用肯德尔的等级相关系数。****

****我们还消除了多重共线性特征。****

> ****过滤方法着眼于单个特征，以确定其相对重要性。一个特性本身可能没什么用，但是当它与其他特性结合起来时，可能是一个重要的影响因素。过滤方法可能会遗漏这些特征。但是它们被使用是因为它们很快并且不依赖于我们使用的模型。****

## ****包装方法-****

****包装器需要某种方法来搜索所有可能的特征子集的空间，通过学习和评估具有该特征子集的分类器来评估它们的质量。特征选择过程是基于一个特定的机器学习算法，我们正试图适应一个给定的数据集。它遵循贪婪搜索方法，根据评估标准评估所有可能的特征组合。包装方法通常比过滤方法具有更好的预测准确性。它们为期望的机器学习算法找到最优特征子集，因此，它们非常昂贵。****

****一般来说，包装方法以下列方式工作:****

*   ******搜索特征子集:**使用搜索方法(如下所述)，我们从可用的特征中选择一个特征子集。****
*   ******建立机器学习模型:**在这一步中，在先前选择的特征子集上训练选择的 ML 算法。****
*   ******评估模型性能:**最后，我们用选择的度量评估新训练的 ML 模型。****
*   ******重复:**整个过程从新的特征子集开始，训练新的 ML 模型，等等。****

****我们停下来，直到满足期望的条件，然后在评估阶段选择具有最佳结果的最佳子集。****

****我们安装库 mlxtend 来使用包装方法。****

******正向特征选择-******

****也称为向前步进功能选择(或顺序向前功能选择— SFS)，这是一种迭代方法，我们首先分别评估所有功能，然后选择性能最佳的功能。在第二步中，它测试所选特征与剩余特征的所有可能组合，并保留产生最佳算法性能的组合。并且通过在每次迭代中一次添加一个特征来继续循环，直到达到预设标准。****

****我们用它来-****

> ****从 mlxtend.feature_selection 导入 SequentialFeatureSelector****

****并且我们为正向特征选择设置 forward=True。****

******向后特征选择-******

****与正向特征选择相同，只是以相反的方式进行。****

****我们从数据集中的所有特征开始，然后评估算法的性能。此后，在每次迭代中，反向特征消除一次移除一个特征，这产生使用评估度量的最佳执行算法。该特性也可以被描述为剩余可用特性中最不重要的特性。并且继续移除一个又一个特征，直到满足某个标准。****

****我们设置 forward = False 来使用 SequentialFeatureSelector 作为向后特征选择器。****

****除了成本之外，以上两个还有一个缺点，即，由于我们知道正向特征选择器在每次迭代中添加特征，当我们添加一个在开始时有用的特征，但是在添加更多特征之后，现在无用时，会出现问题。在这一点上，没有办法删除这种功能。与反向选择器相同，但方向相反。****

******顺序浮动-******

****它只是解决上述问题的一个扩展。****

****这种方法的工作方式很容易理解。让我们在这两种方法的背景下探讨一下:****

*   ******步浮动前进选择:**每前进一步，**只要目标函数增加就后退一步。******
*   ********步进浮动后退选择:**每后退一步， **SFBS** 就前进一步，只要目标函数增加。******

****要使用它，我们必须将 SequentialFeatureSelector 的 floating 参数设置为 true。****

******递归特征消除-******

****这只是一个简单方法的别出心裁的名字，其工作原理如下:****

1.  ****根据所有数据特征训练模型。该模型可以是基于树的模型、lasso、逻辑回归或其他能够提供 ***特征重要性*** 的模型。根据您选择的合适指标评估其性能。****
2.  ****导出特征重要性以相应地排列特征。****
3.  ****删除最不重要的特征，并根据剩余的特征重新训练模型。****
4.  ****使用之前的评估指标来计算结果模型的性能。****
5.  ****现在测试评估指标是否减少了任意的阈值(您也应该定义这个阈值)。如果是的话，这意味着这个特性很重要。否则，您可以删除它。****
6.  ****重复步骤 3-5，直到移除所有特征(即评估)。****

****你可能会想说这就像我们在关于包装器方法的文章中所做的后退特性选择一样，**但它不是**。不同的是，SBS 首先消除所有的特征，以便确定哪一个是最不重要的。但在这里，我们从机器学习模型的派生重要性中获得这些信息，因此它只删除一次特征，而不是在每一步删除所有特征。****

****这就是为什么这种方法比纯包装方法更快，比纯嵌入方法更好。但作为一个缺点，主要问题是我们必须使用一个任意的阈值来决定是否保留一个特征。****

****因此，该阈值越小，子集中包含的要素就越多，反之亦然。****

****它被认为是嵌入方法和包装方法的混合方法。****

****我们可以用它来-****

> ****从 sklearn.feature_importance 导入 RFE****

******详尽的特征选择-******

****最后，该方法搜索所有可能的特征组合。它的目标是找到性能最好的特征子集——我们可以说这是对特征子集的强力评估。它会创建从 **1** 到 **N** 的所有特征子集，其中 **N** 为特征总数，对于每个子集，它会构建一个机器学习算法，并选择性能最佳的子集。****

****我们用它来-****

> ****从 mlxtend.feature_selection 导入 ExhaustiveFeatureSelector****

****因为包装方法非常昂贵，所以我们首先删除像多重共线或一些其他标准的特性，然后使用包装方法来节省时间。****

****还要注意，与使用来自过滤器方法的特征子集相比，使用来自包装器方法的特征子集使模型更容易过度拟合。****

## ****嵌入方法-****

****嵌入式方法在机器学习算法本身的构造中完成特征选择过程。换句话说，它们在模型训练期间执行特征选择，这就是为什么我们称它们为嵌入式方法。学习算法利用其自身的变量选择过程，同时执行特征选择和分类/回归。****

****嵌入式方法结合了过滤器和包装器方法的优点，解决了我们遇到的这两个问题。方法如下:****

*   ****它们像包装方法一样考虑了特性的交互。****
*   ****它们像过滤方法一样更快。****
*   ****它们比过滤方法更准确。****
*   ****他们为被训练的算法找到特征子集。****
*   ****他们更不容易过度拟合。****

******拉索回归-******

****从不同类型的正则化，拉索或 L1 的属性，能够缩小一些系数为零。因此，可以从模型中删除该特征。****

******随机森林重要性-******

****随机森林是一种 Bagging 算法，它聚集了指定数量的决策树。随机森林使用的基于树的策略自然会根据它们提高节点纯度的程度进行排序，或者换句话说，根据所有树的杂质( **Gini 杂质**)的减少程度进行排序。杂质减少最多的节点出现在树的开始，而杂质减少最少的节点出现在树的结尾。因此，通过修剪特定节点下的树，我们可以创建最重要特征的子集。****

****但是嵌入方法也有缺点，因为它们只能被某些算法使用。****

## ****用于特征选择的其他方法-****

******排列重要性-******

****它用随机噪声替换一个特征值，然后训练模型。如果打乱某个特征的值会增加模型误差，则该特征是“重要的”。在这种情况下，模型依赖于用于预测的特征。****

****我们可以用它-****

> ******from****Eli 5 . sk learn****导入**置换重要性****

******特征重要性-******

****通过使用模型的要素重要性属性，可以获得数据集每个要素的要素重要性。要素重要性为数据的每个要素提供一个分数，分数越高，要素对输出变量越重要或越相关。这些只能用于具有 attribute _feature_importance 的模型****

******自动编码器-******

****我们将在我的下一篇博客中看到这一点。****