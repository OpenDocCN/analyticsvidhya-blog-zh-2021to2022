<html>
<head>
<title>How to Setup PySpark and MongoDB for Building Data Pipeline</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何设置PySpark和MongoDB来构建数据管道</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-setup-pyspark-and-mongodb-for-building-data-pipeline-b528f56b30aa?source=collection_archive---------1-----------------------#2021-09-10">https://medium.com/analytics-vidhya/how-to-setup-pyspark-and-mongodb-for-building-data-pipeline-b528f56b30aa?source=collection_archive---------1-----------------------#2021-09-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="3d7b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">概述</strong></p><p id="81e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你以前听说过数据管道吗？如果没有，让我给你解释一下数据管道的简单定义。数据管道也可以称为ETL管道，它是一组将原始数据转换为业务问题的可操作答案的过程。数据科学管道使数据验证过程自动化；提取、转换、加载(<a class="ae jc" href="https://www.snowflake.com/data-warehousing-glossary/etl/" rel="noopener ugc nofollow" target="_blank">ETL</a>)；机器学习和建模；改版；并输出到数据仓库或可视化平台(Snowflake，n.d .)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/5a976275e58c918447cc2eab253a387d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Km_1ZcQmlsdYEJv010sGA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图0 Spark + MongoDB(来源:【https://www.mongodb.com/integrations/mongodb-spark】T4)</figcaption></figure><p id="9187" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好了，现在我将向您展示如何使用PySpark和MongoDB建立一个数据管道架构。</p><p id="c6be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">设置和安装</strong></p><p id="0b71" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1 .首先我们需要安装FindSpark、PySpark、PyMongo等第三方库。您可以使用命令pip install library-name或conda install library-name(如果您使用的是Anaconda提示符)来安装库。</p><p id="7c3e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.其次，我们需要安装Apache Spark，你可以从<a class="ae jc" href="https://dlcdn.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">这里</a>安装。之后，在C://目录下创建一个名为“Spark”的新文件夹，并将该文件夹复制粘贴到您下载到Spark文件夹的zip文件中。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jt"><img src="../Images/b84321e70420c9a09f34fad2b9497c5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k4aLSNgf4umHbRocgTCKgQ.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图Spark文件夹内</figcaption></figure><p id="2b07" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.在我们安装了Apache Spark之后，现在我们需要配置Spark home环境，以便在windows中运行Spark。首先进入这台PC-&gt;属性-&gt;高级系统设置-&gt;环境变量，然后点击新建，新建一个变量名为SPARK_HOME，变量值为SPARK文件的目录，例如:“C:\ SPARK \ SPARK-3 . 0 . 3-bin-Hadoop 2.7”。之后，单击变量“Path”，创建一个新变量%SPARK_HOME%\bin，然后单击确定。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ju"><img src="../Images/183b5d3540c91f82c58918a2946908e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*PmdKAdL8zHHNlu3nycDZpQ.jpeg"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图2设置环境</figcaption></figure><p id="b0fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.在我们配置了Spark home之后，现在我们需要配置我们的Hadoop home。我们需要做的第一件事是从这里的<a class="ae jc" href="https://github.com/steveloughran/winutils/raw/master/hadoop-2.7.1/bin/winutils.exe" rel="noopener ugc nofollow" target="_blank">下载文件“winutils . exe”</a>在windows中运行Hadoop需要这个文件。下载文件后，将其复制并粘贴到Spark文件夹内的“bin”文件夹中。现在回到这台PC- &gt;属性- &gt;高级系统设置- &gt;环境变量点击新建，创建一个新的变量名为HADOOP_HOME变量值与SPARK_HOME相同。之后，单击变量“Path”，创建一个新变量%HADOOP_HOME%\bin，然后单击确定。</p><p id="626b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">5.一旦我们安装了Spark和Hadoop，我们现在可以做的下一件事就是从<a class="ae jc" href="https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html#license-lightbox" rel="noopener ugc nofollow" target="_blank">这里</a>安装Java 8 JDK。</p><p id="629f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">6.在我们安装了Spark和Java之后，现在是时候检查我们的Spark是否安装和配置正确了。以管理员身份打开命令提示符，然后键入“spark-shell”。如果我们看到一个欢迎信息，祝贺Spark成功安装在我们的机器上。如果没有，请在C://中创建一个名为“tmp”的新文件夹，然后在其中创建一个名为“hive”的新文件夹，如果您使用的是旧版本的Spark，这是可选的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jv"><img src="../Images/32796bffee4a0319064ab2706fe489b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5x05r5etU3wGsd-S6-_D-g.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图3火花外壳</figcaption></figure><p id="570b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">7.在Spark成功运行后，我们需要做的下一件事是下载<a class="ae jc" href="https://www.mongodb.com/try/download/community" rel="noopener ugc nofollow" target="_blank"> MongoDB </a>，并选择一个社区服务器。在这个项目中，我使用的是MongoDB 5.0.2 for Windows。当你安装MongoDB时，你会遇到一个服务器定制设置，供个人使用，只需将其设为默认设置，不要做任何更改。安装MongoDB后，打开安装目录，然后打开Mongo.exe，然后键入“DB”。如果输出是“test ”,这意味着MongoDB安装成功。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jw"><img src="../Images/2e99ca27d770eeb5fd3b466cad99de75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjWgGxvkLv7ncE7md_Dpmw.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图4测试MongoDB</figcaption></figure><p id="ea83" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">8.只要我们知道如何与MongoDB接口，我们所能做的就是使用命令行，这不是真正的用户友好，因此我们需要一个工具来处理这种情况。我们将使用的软件是<a class="ae jc" href="https://s3.mongobooster.com/download/releasesv6/nosqlbooster4mongo-6.2.17.exe" rel="noopener ugc nofollow" target="_blank"> NoSQLBooster </a>，但是如果你喜欢使用命令行也没问题。一旦我们已经安装了NoSQLBooster，接下来我们需要做的就是建立连接，打开NoSQLBooster- &gt;文件- &gt;连接- &gt;创建- &gt;默认设置- &gt;测试连接。连接成功运行后，我们需要在localhost下创建一个数据库。您必须右击本地主机，然后创建一个数据库。您可以随意命名您的数据库，在本例中，我为下一个项目将我的数据库命名为“Quake”。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jx"><img src="../Images/b6841c27083e5cdb6ce51b02d7cd53b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qVdPs8RVylYPLmKK6p7zQ.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图5 NoSQLBooster连接设置</figcaption></figure><p id="190e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">MongoDB安装完成后，我们接下来可以用Jupyter笔记本测试PySpark。您可以简单地这样做:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jy"><img src="../Images/a191f05a10bb09c328c718ccd75e1fd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*n4ZCSpZXLXxo0aMAPWQWVQ.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图6用Jupyter笔记本测试PySpark</figcaption></figure><p id="e152" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果您看到“hello spark”消息，您可以验证PySpark与Jupyter笔记本成功配合工作。</p><p id="a77a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好了，一旦我们的数据管道架构安装完成，PySpark与Jupyter笔记本集成，现在我们可以做我们的数据科学项目了。</p><p id="b92d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我希望我的文章能帮助你完成成为一名专业数据科学家的旅程。</p><p id="61e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">谢谢大家！</p><p id="b374" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">编制人:Vandany Lubis</p><p id="6191" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">非常感谢<a class="ae jc" href="https://github.com/EBISYS" rel="noopener ugc nofollow" target="_blank"> EBISYS </a>教我如何做到这一点</p></div></div>    
</body>
</html>