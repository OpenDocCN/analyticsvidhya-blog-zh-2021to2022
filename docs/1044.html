<html>
<head>
<title>Beating Pong using Reinforcement Learning — Part 1 DDDQN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用强化学习击败Pong第1部分DDDQN</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/beating-pong-using-reinforcement-learning-part-1-dddqn-f7fbf5ad7768?source=collection_archive---------15-----------------------#2021-02-11">https://medium.com/analytics-vidhya/beating-pong-using-reinforcement-learning-part-1-dddqn-f7fbf5ad7768?source=collection_archive---------15-----------------------#2021-02-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="8433" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">我强化学习之旅的开始</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/36605537df29ea452b0b2c5b63299fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tawUDYGdDUW7omuogvGM2g.jpeg"/></div></div></figure><p id="e352" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">作者安东尼奥·李斯</em></p><h1 id="77e9" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">介绍</h1><p id="b87b" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">大家好，这是我在这个博客上的第一篇技术文章。我希望你会喜欢它。</p><p id="33dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">今天我们讨论强化学习。在过去的几个月里，由于封锁等原因，我多年来第一次有了一些空闲时间，所以我决定学习强化学习(RL)。博弈论是我在大学最喜欢的科目之一，但同时，我一直认为它不能像现在这样应用到现实生活中。你不能计算所有可能行为的期望值，除非你有像囚徒困境这样的简单场景。</p><p id="2292" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">RL从同样的概念出发，但是方法却大不相同。你定义并训练一个神经网络来近似每个动作/状态的值(这就是所谓的基于值的方法)。</p><p id="b3d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">理论的细节我就不赘述了，但是我推荐<a class="ae ks" href="https://www.perlego.com/book/771717/deep-reinforcement-learning-handson-apply-modern-rl-methods-with-deep-qnetworks-value-iteration-policy-gradients-trpo-alphago-zero-and-more-pdf?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=1-EU%20%7C%20en%20%7C%20DSA%20Test%2018.12.20&amp;utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=11871053709&amp;utm_term=&amp;utm_content=487151523099&amp;gclid=Cj0KCQiA6Or_BRC_ARIsAPzuer9g8uK3V8w7olmvOtR4S61TShe5U8MeV-S3-ZeyaNZvNbXHt3JjiLAaAiuJEALw_wcB" rel="noopener ugc nofollow" target="_blank">这本书</a>。因为加入了<a class="ae ks" href="https://events.linuxfoundation.org/ray-summit/" rel="noopener ugc nofollow" target="_blank"> Ray summit conference </a>，所以收到了免费的副本(这是一个非常有趣的构建分布式应用的库，你可以在这里<a class="ae ks" href="https://github.com/ray-project/ray" rel="noopener ugc nofollow" target="_blank">查看</a>)。</p><p id="5749" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你喜欢学术论文，我最近读了一篇论文<a class="ae ks" href="https://www.jair.org/index.php/jair/article/view/12412/26638" rel="noopener ugc nofollow" target="_blank">深度强化学习:最先进的演练</a>。这是一个很好的阅读，在高层次上学习在这个领域已经开发的算法。</p><p id="28c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面使用的大部分代码都是从这本<a class="ae ks" href="https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>中获得灵感的，所以非常感谢法比奥·m·格雷茨。</p><p id="a4b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以事不宜迟，我将与你分享我第一次尝试解决pong的实验。</p><p id="e752" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">P.s .最初的计划是写两种不同的方法，DDDQN和A2C，但这篇文章太长了，所以我将在这里讨论DDDQN，在下一篇文章中讨论A2C。</p><h1 id="62a0" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">五金器具</h1><p id="2ed9" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">在深入代码之前，我想先说一下使用的硬件。为了训练第一个算法(DDDQN)，我在我的mac上开发了它，然后在谷歌云虚拟机上用特斯拉K80启动了训练。结果是这样的(我有将近200美元的信用额度，所以我没有支付):</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kt"><img src="../Images/669edbb5c28458d08e3d5d762458cb18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2yJ_y-vC5PkhTFV7"/></div></div></figure><p id="7f8f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以我需要一个替代品，幸运的是，在我家里，我有一台GTX 1050 TI的旧电脑，我用它来进行图像识别，最近还用来玩轻度游戏。所以我决定格式化它，安装ubuntu 20.04，安装Nvidia驱动，CUDA等等，我就可以启动PyTorch和Jax了。但问题是，我想用Tensorflow，但我的CPU对TF2来说太老了；它需要从源头开始构建。这是一个非常令人沮丧的过程的开始。我开始设置docker(我需要一个隔离的环境来避免灾难)，下载Tensorflow的最后一个docker映像，安装Bazel，安装NVIDIA Container Toolkit，并按照糟糕的文档从源代码构建Tensorflow(该死的google！).经过20个小时的编译和多次祈祷，我能够导入TensorFlow并看到GPU，这是2020年最令人满意的时刻之一。</p><h1 id="2d6e" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">开放的健身房环境</h1><p id="dedd" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">第一步是选择要解决的环境，你可以在这里找到<a class="ae ks" href="https://github.com/openai/gym/wiki/Table-of-environments" rel="noopener ugc nofollow" target="_blank"/>open ai提供的所有环境的列表。我选择解决pong，是因为它相对来说比较容易解决，也比较快。通过这种方式，我可以专注于算法的实现，并很快看到它们是否收敛，但同时，它并不是那么简单(例如CartPole ),你不能欣赏所用算法的变化。</p><h1 id="5a3d" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">效用函数和环境</h1><p id="9403" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">我们开始查看OpenAI给出的环境，为了查看我们将在jupyter笔记本中生成的mp4，我们将使用以下函数(您还需要使用命令xvfb-run -s "-screen 0 1400x900x24运行jupyter笔记本/lab):</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="814b" class="kz jq hh kv b fi la lb l lc ld">import base64<br/>import IPython</span><span id="1274" class="kz jq hh kv b fi le lb l lc ld"><em class="jc">def</em> embed_mp4(filename):<br/>  """Embeds an mp4 file in the notebook."""<br/>  video = <em class="jc">open</em>(filename,'rb').read()<br/>  b64 = base64.b64encode(video)<br/>  tag = '''<br/>  &lt;video width="640" height="480" controls&gt;<br/>    &lt;source src="data:video/mp4;base64,{0}" type="video/mp4"&gt;<br/>  Your browser does not support the video tag.<br/>  &lt;/video&gt;'''.<em class="jc">format</em>(b64.decode())</span><span id="7863" class="kz jq hh kv b fi le lb l lc ld">  return IPython.display.HTML(tag)</span></pre><p id="a9d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了使jupyter笔记本的开发更容易，我还使用了下面的特别插件，它可以创建文本文件并向其中添加代码:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="3495" class="kz jq hh kv b fi la lb l lc ld">from IPython.core.magic import register_cell_magic</span><span id="a5fd" class="kz jq hh kv b fi le lb l lc ld">@register_cell_magic<br/><em class="jc">def</em> write_and_run(line, cell):<br/>    argz = line.split()<br/>    <em class="jc">file</em> = argz[-1]<br/>    mode = 'w'<br/>    if <em class="jc">len</em>(argz) == 2 and argz[0] == '-a':<br/>        mode = 'a'<br/>    with <em class="jc">open</em>(<em class="jc">file</em>, mode) as f:<br/>        f.write(cell)<br/>    get_ipython().run_cell(cell)</span></pre><p id="60cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们可以开始与环境互动了:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="e3e6" class="kz jq hh kv b fi la lb l lc ld">import gym<br/>import os<br/>import matplotlib.pyplot as plt<br/>from PIL import Image<br/>import imageio<br/>import random<br/>import numpy as np<br/>import cv2<br/>import base64<br/>import IPython<br/>from config import ENV_NAME<br/>from utilities import embed_mp4</span><span id="ea85" class="kz jq hh kv b fi le lb l lc ld"># Create environment<br/>env = gym.make(ENV_NAME)<br/>print("The environment has the following {} actions: {}".\<br/>      <em class="jc">format</em>(env.action_space.n, env.unwrapped.get_action_meanings()))</span><span id="0657" class="kz jq hh kv b fi le lb l lc ld">with imageio.get_writer("../video/random_agent.mp4", fps=60) as video:<br/>    terminal = True<br/>    for frame in <em class="jc">range</em>(10000):<br/>        if terminal:<br/>            env.reset()<br/>            terminal = False</span><span id="6898" class="kz jq hh kv b fi le lb l lc ld">        # Breakout require a "fire" action (action #1) to start the<br/>        # game each time a life is lost.<br/>        # Otherwise, the agent would sit around doing nothing.<br/>        action = random.choice(<em class="jc">range</em>(env.action_space.n))</span><span id="fe4c" class="kz jq hh kv b fi le lb l lc ld">        # Step action<br/>        _, reward, terminal, info = env.step(action)</span><span id="b1a5" class="kz jq hh kv b fi le lb l lc ld">        video.append_data(env.render(mode='rgb_array'))</span></pre><p id="546c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如你所见，我们的随机代理，右边的那个，表现不太好:</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lf lg l"/></div></figure><h1 id="5bf2" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">过程图象</h1><p id="9268" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">环境的输入是具有形状(210，160，3)的图像，我们不能直接处理这些图像，原因有两个:</p><ul class=""><li id="da03" class="lh li hh ig b ih ii il im ip lj it lk ix ll jb lm ln lo lp bi translated">这在计算上太昂贵了</li><li id="4f30" class="lh li hh ig b ih lq il lr ip ls it lt ix lu jb lm ln lo lp bi translated">Conv2D的GPU Tensorflow 2实现需要方形输入</li></ul><p id="919a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，我们使用以下函数来消除颜色，裁剪和调整图像大小:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="3626" class="kz jq hh kv b fi la lb l lc ld"># This function can resize to any shape but was built to resize to 84x84<br/><em class="jc">def</em> process_image(image, shape=(84, 84)):<br/>    """Preprocesses a 210x160x3 frame to 84x84x1 grayscale</span><span id="e0dc" class="kz jq hh kv b fi le lb l lc ld">    Arguments:<br/>        frame: The frame to process.  Must have values ranging from 0-255</span><span id="b9dd" class="kz jq hh kv b fi le lb l lc ld">    Returns:<br/>        The processed frame<br/>    """<br/>    image = image.astype(np.uint8)  # cv2 requires np.uint8</span><span id="9a08" class="kz jq hh kv b fi le lb l lc ld">    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)<br/>    image = image[34:34+160, :160]  # crop image<br/>    image = cv2.resize(image, shape, interpolation=cv2.INTER_NEAREST)<br/>    image = image.reshape((*shape, 1))</span><span id="8c10" class="kz jq hh kv b fi le lb l lc ld">    return image</span></pre><p id="c7b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用OpenCV删除表示颜色的最后一个维度(函数COLOR_RGB2GRAY)，我们裁剪图像(我们不需要记分牌)，然后因为我们需要一个正方形图像作为GPU的输入，我们将图像大小调整为84x84x1(这是Atari游戏的<a class="ae ks" href="https://arxiv.org/pdf/1312.5602.pdf" rel="noopener ugc nofollow" target="_blank">最佳实践</a>):</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lf lg l"/></div></figure><h1 id="ae78" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">游戏包装</h1><p id="7055" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">我们定义了OpenAi健身房的包装器，最大的改进是:</p><ul class=""><li id="0b1e" class="lh li hh ig b ih ii il im ip lj it lk ix ll jb lm ln lo lp bi translated">我们应用process_image函数来获得84×84×1的图像；</li><li id="7f02" class="lh li hh ig b ih lq il lr ip ls it lt ix lu jb lm ln lo lp bi translated">我们将输入定义为四个图像的连接，这给了代理基本上看到球的动态的能力；</li><li id="3d07" class="lh li hh ig b ih lq il lr ip ls it lt ix lu jb lm ln lo lp bi translated">当评估代理时，它将在每集开始时采取动作1(“开火”)，在范围[1，no_op_steps]内随机次数。这将确保每次都有不同的初始随机条件(<a class="ae ks" href="https://www.datascienceassn.org/sites/default/files/Human-level%20Control%20Through%20Deep%20Reinforcement%20Learning.pdf" rel="noopener ugc nofollow" target="_blank"> Mnih et al. 2015 </a>)。</li></ul><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="0337" class="kz jq hh kv b fi la lb l lc ld">import random<br/>import gym<br/>import numpy as np<br/>from process_image import process_image</span><span id="829a" class="kz jq hh kv b fi le lb l lc ld"><em class="jc">class</em> PongWrapper(<em class="jc">object</em>):<br/>    """<br/>    Wrapper for the environment provided by Openai Gym<br/>    """</span><span id="6c1b" class="kz jq hh kv b fi le lb l lc ld">    <em class="jc">def</em> __init__(self, env_name: <em class="jc">str</em>, no_op_steps: <em class="jc">int</em> = 10, history_length: <em class="jc">int</em> = 4):<br/>        self.env = gym.make(env_name)<br/>        self.no_op_steps = no_op_steps<br/>        self.history_length = 4 # number of frames to put together (we need dynamic to see where the ball is going)</span><span id="0b47" class="kz jq hh kv b fi le lb l lc ld">        self.state = None<br/>        <br/>    <em class="jc">def</em> reset(self, evaluation: <em class="jc">bool</em> = False):<br/>        """Resets the environment<br/>        Arguments:<br/>            evaluation: Set to True when we are in evaluation mode, in this case the agent takes a random number of no-op steps if True.<br/>        """</span><span id="8a86" class="kz jq hh kv b fi le lb l lc ld">        self.frame = self.env.reset()<br/>        <br/>        # If in evaluation model, take a random number of no-op steps<br/>        if evaluation:<br/>            for _ in <em class="jc">range</em>(random.randint(0, self.no_op_steps)):<br/>                self.env.step(1)</span><span id="11fd" class="kz jq hh kv b fi le lb l lc ld">        # For the initial state, we stack the first frame four times<br/>        self.state = np.repeat(process_image(self.frame), self.history_length, axis=2)</span><span id="8ce0" class="kz jq hh kv b fi le lb l lc ld">    <em class="jc">def</em> step(self, action: <em class="jc">int</em>, render_mode=None):<br/>        """<br/>        Arguments:<br/>            action: An integer describe action to take<br/>            render_mode: None doesn't render anything, 'human' renders the screen in a new window, 'rgb_array' returns also an np.array with rgb values<br/>        Returns:<br/>            processed_image: The processed new frame as a result of that action<br/>            reward: The reward for taking that action<br/>            terminal: Whether the game has ended<br/>        """<br/>        new_frame, reward, terminal, info = self.env.step(action)</span><span id="10c8" class="kz jq hh kv b fi le lb l lc ld">        processed_image = process_image(new_frame)</span><span id="aa5d" class="kz jq hh kv b fi le lb l lc ld">        self.state = np.append(self.state[:, :, 1:], processed_image, axis=2) # replace the first observation of the previous state with the last one</span><span id="def6" class="kz jq hh kv b fi le lb l lc ld">        if render_mode == 'rgb_array':<br/>            return processed_image, reward, terminal, self.env.render(render_mode)<br/>        elif render_mode == 'human':<br/>            self.env.render(render_mode)</span><span id="e16d" class="kz jq hh kv b fi le lb l lc ld">        return processed_image, reward, terminal</span></pre><h1 id="673c" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">重放缓冲器</h1><p id="d597" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">我们想要使用的算法(DDDQN)是一种脱离策略的方法，因为更新的策略不同于行为策略。当我们建立网络时，这一点会变得更清楚，现在，这基本上意味着我们的网络可以从过去的事件中学习以前的政策。</p><p id="fe3c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将用于训练网络的SGD优化的一个基本要求是，训练数据是独立且同分布的(i.i.d .)。但是，如果代理与环境相互作用，输入序列可能是高度相关的。这就是为什么我们需要一个重放缓冲区来记录代理收集的经验，然后我们从这个集合中获取一批训练数据来更新权重。最简单的实现是一个固定大小的缓冲区，将新数据添加到缓冲区的末尾，以便将最老的体验推出。</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="4931" class="kz jq hh kv b fi la lb l lc ld">import os<br/>import random<br/>import numpy as np</span><span id="2bba" class="kz jq hh kv b fi le lb l lc ld"><em class="jc">class</em> ReplayBuffer(<em class="jc">object</em>):<br/>    """<br/>    Replay Memory that stores the last size transitions<br/>    """<br/>    <em class="jc">def</em> __init__(self, size: <em class="jc">int</em>=1000000, input_shape: <em class="jc">tuple</em>=(84, 84), history_length: <em class="jc">int</em>=4, reward_type: <em class="jc">str</em> = "integer"):<br/>        """<br/>        Arguments:<br/>            size: Number of stored transitions<br/>            input_shape: Shape of the preprocessed frame<br/>            history_length: Number of frames stacked together that the agent can see<br/>        """<br/>        self.size = size<br/>        self.input_shape = input_shape<br/>        self.history_length = history_length<br/>        self.count = 0  # total index of memory written to, always less than self.size<br/>        self.current = 0  # index to write to<br/>        self.reward_type = reward_type</span><span id="8c09" class="kz jq hh kv b fi le lb l lc ld">        # Pre-allocate memory<br/>        self.actions = np.empty(self.size, dtype=np.int32)<br/>        self.rewards = np.empty(self.size, dtype=np.float32)<br/>        self.frames = np.empty((self.size, self.input_shape[0], self.input_shape[1]), dtype=np.uint8)<br/>        self.terminal_flags = np.empty(self.size, dtype=np.<em class="jc">bool</em>)<br/>        self.priorities = np.zeros(self.size, dtype=np.float32)</span><span id="e373" class="kz jq hh kv b fi le lb l lc ld">    <em class="jc">def</em> add_experience(self, action, frame, reward, terminal, clip_reward=True, reward_type="integer"):<br/>        """Saves a transition to the replay buffer<br/>        Arguments:<br/>            action: An integer between 0 and env.action_space.n - 1 <br/>                determining the action the agent perfomed<br/>            frame: A (84, 84, 1) frame of the game in grayscale<br/>            reward: A float determining the reward the agend received for performing an action<br/>            terminal: A bool stating whether the episode terminated<br/>        """<br/>        if frame.shape != self.input_shape:<br/>            raise ValueError('Dimension of frame is wrong!')</span><span id="7c0f" class="kz jq hh kv b fi le lb l lc ld">        if clip_reward:<br/>            if reward_type == "integer":<br/>                reward = np.sign(reward)<br/>            else:<br/>                reward = np.clip(reward, -1.0, 1.0)<br/>        # Write memory<br/>        self.actions[self.current] = action<br/>        self.frames[self.current, ...] = frame<br/>        self.rewards[self.current] = reward<br/>        self.terminal_flags[self.current] = terminal<br/>        self.priorities[self.current] = <em class="jc">max</em>(self.priorities.<em class="jc">max</em>(), 1)  # make the most recent experience important<br/>        self.count = <em class="jc">max</em>(self.count, self.current+1)<br/>        self.current = (self.current + 1) % self.size # when a &lt; b then a % b = a</span><span id="0825" class="kz jq hh kv b fi le lb l lc ld">    <em class="jc">def</em> get_minibatch(self, batch_size: <em class="jc">int</em> = 32):<br/>        """<br/>        Returns a minibatch of size batch_size<br/>        Arguments:<br/>            batch_size: How many samples to return<br/>        Returns:<br/>            A tuple of states, actions, rewards, new_states, and terminals<br/>        """</span><span id="a5d4" class="kz jq hh kv b fi le lb l lc ld">        if self.count &lt; self.history_length:<br/>            raise ValueError('Not enough memories to get a minibatch')</span><span id="80a2" class="kz jq hh kv b fi le lb l lc ld">        indices = []<br/>        for i in <em class="jc">range</em>(batch_size):<br/>            while True:<br/>                # Get a random number from history_length to maximum frame<br/>                index = random.randint(self.history_length, self.count - 1)</span><span id="ed39" class="kz jq hh kv b fi le lb l lc ld">                # We check that all frames are from same episode with the two following if statements.<br/>                if index &gt;= self.current and index - self.history_length &lt;= self.current:<br/>                    continue<br/>                if self.terminal_flags[index - self.history_length:index].<em class="jc">any</em>():<br/>                    continue<br/>                break<br/>            indices.append(index)</span><span id="ac9d" class="kz jq hh kv b fi le lb l lc ld">        # Retrieve states from memory<br/>        states = []<br/>        new_states = []<br/>        for idx in indices:<br/>            states.append(self.frames[idx-self.history_length:idx, ...])<br/>            new_states.append(self.frames[idx-self.history_length+1:idx+1, ...])</span><span id="e0b5" class="kz jq hh kv b fi le lb l lc ld">        states = np.transpose(np.asarray(states), axes=(0, 2, 3, 1))<br/>        new_states = np.transpose(np.asarray(new_states), axes=(0, 2, 3, 1))</span><span id="fe58" class="kz jq hh kv b fi le lb l lc ld">        return states, self.actions[indices], self.rewards[indices], new_states, self.terminal_flags[indices]</span><span id="686c" class="kz jq hh kv b fi le lb l lc ld">    <em class="jc">def</em> save(self, folder_name):<br/>        """<br/>        Save the replay buffer<br/>        """</span><span id="5e1a" class="kz jq hh kv b fi le lb l lc ld">        if not os.path.isdir(folder_name):<br/>            os.mkdir(folder_name)</span><span id="3e3d" class="kz jq hh kv b fi le lb l lc ld">        np.save(folder_name + '/actions.npy', self.actions)<br/>        np.save(folder_name + '/frames.npy', self.frames)<br/>        np.save(folder_name + '/rewards.npy', self.rewards)<br/>        np.save(folder_name + '/terminal_flags.npy', self.terminal_flags)</span><span id="4c78" class="kz jq hh kv b fi le lb l lc ld">    <em class="jc">def</em> load(self, folder_name):<br/>        """<br/>        Load the replay buffer<br/>        """<br/>        self.actions = np.load(folder_name + '/actions.npy')<br/>        self.frames = np.load(folder_name + '/frames.npy')<br/>        self.rewards = np.load(folder_name + '/rewards.npy')<br/>        self.terminal_flags = np.load(folder_name + '/terminal_flags.npy')</span></pre><p id="b982" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">重放缓冲器存储来自代理与环境的动作、奖励、输入(帧)、终端标志(如果情节结束)、交互的全局计数和当前索引的交互。请注意，引用论文<a class="ae ks" href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank">通过深度强化学习进行人类级控制</a>“由于不同游戏之间的分数差异很大，我们将所有积极奖励削减为1，将所有消极奖励削减为-1，保持0奖励不变。以这种方式削减奖励限制了误差导数的规模，并且使得在多个游戏中使用相同的学习率变得更容易”</p><p id="7b19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">get_minibatch函数遍历重放缓冲区，并从相同事件的输入和输出状态、动作、奖励和终端标志中返回数组。然后我们有保存和加载函数，这些函数将重放缓冲区的NumPy数组保存到磁盘上或从磁盘上加载。</p><h1 id="319b" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">网络</h1><p id="2a52" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated"><a class="ae ks" href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank"> DQN(深度Q-网络)算法</a>是DeepMind在2015年通过用深度神经网络增强一种名为Q-Learning的经典RL算法开发的。</p><p id="6477" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Q-Learning算法基于Q(a，s)函数的概念，该函数返回在该状态下该行为的预期未来回报。传统的DQN只是试图用神经网络来逼近Q函数。DQN是基于价值方法的一个例子，因为它试图逼近Q函数。另一种方法是政策梯度，我将在下一篇文章中介绍。</p><p id="3938" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将使用DQN的一个变体，称为DDDQN，代表决斗双深度Q网络。双部分来自名为<a class="ae ks" href="https://arxiv.org/pdf/1509.06461.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="jc">用双Q学习</em> </a>进行深度强化学习的论文。作者证明了基本DQN倾向于高估<em class="jc"> Q </em>的值，因此他们建议引入两个神经网络，目标网络和训练网络。经训练的网络用于为下一状态选择动作，目标网络用于估计<em class="jc"> Q </em>的值，以使用贝尔曼方程更新网络的权重。目标网络将使用来自已训练网络的权重每n步更新一次。</p><p id="bc5e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">决斗网络体系结构在<a class="ae ks" href="https://arxiv.org/pdf/1511.06581.pdf" rel="noopener ugc nofollow" target="_blank">深度强化学习决斗网络体系结构</a>中介绍，作者<br/>证明了网络试图逼近的Q值可以划分为多个量:</p><ul class=""><li id="afde" class="lh li hh ig b ih ii il im ip lj it lk ix ll jb lm ln lo lp bi translated">状态的值，V(s)</li><li id="5cdd" class="lh li hh ig b ih lq il lr ip ls it lt ix lu jb lm ln lo lp bi translated">在这种状态下行动的优势，A(s，A)。</li></ul><p id="5562" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将估计Q值的任务分成两个量产生了更好的训练稳定性、更快的收敛和更好的结果。</p><p id="3e59" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好了，现在我们可以看到实现了，没什么特别的，正如你在下面看到的，它使用了一组2d卷积，然后将所有东西展平，并将神经网络分成两部分，以估计状态值和动作的优点:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="efe2" class="kz jq hh kv b fi la lb l lc ld">import tensorflow as tf<br/>from tensorflow.keras.initializers import VarianceScaling<br/>from tensorflow.keras.layers import (Add, Conv2D, Dense, Flatten, Input,<br/>                                     Lambda, Subtract)<br/>from tensorflow.keras.models import Model<br/>from tensorflow.keras.optimizers import Adam, RMSprop # we'll use Adam instead of RMSprop</span><span id="a610" class="kz jq hh kv b fi le lb l lc ld"><em class="jc">def</em> build_q_network(n_actions, learning_rate=0.00001, input_shape=(84, 84), history_length=4, hidden=1024):<br/>    """<br/>    Builds a dueling DQN as a Keras model<br/>    Arguments:<br/>        n_actions: Number of possible actions<br/>        learning_rate: Learning rate<br/>        input_shape: Shape of the preprocessed image<br/>        history_length: Number of historical frames to stack togheter<br/>        hidden: Integer, Number of filters in the final convolutional layer. <br/>    Returns:<br/>        A compiled Keras model<br/>    """<br/>    model_input = Input(shape=(input_shape[0], input_shape[1], history_length))<br/>    x = Lambda(lambda layer: layer / 255)(model_input)  # normalize by 255</span><span id="74f3" class="kz jq hh kv b fi le lb l lc ld">    x = Conv2D(32, (8, 8), strides=4, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)<br/>    x = Conv2D(64, (4, 4), strides=2, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)<br/>    x = Conv2D(64, (3, 3), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)<br/>    x = Conv2D(hidden, (7, 7), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)</span><span id="d85a" class="kz jq hh kv b fi le lb l lc ld">    # Split into value and advantage streams<br/>    val_stream, adv_stream = Lambda(lambda w: tf.split(w, 2, 3))(x)  # custom splitting layer</span><span id="dae0" class="kz jq hh kv b fi le lb l lc ld">    val_stream = Flatten()(val_stream)<br/>    val = Dense(1, kernel_initializer=VarianceScaling(scale=2.))(val_stream)</span><span id="9125" class="kz jq hh kv b fi le lb l lc ld">    adv_stream = Flatten()(adv_stream)<br/>    adv = Dense(n_actions, kernel_initializer=VarianceScaling(scale=2.))(adv_stream)</span><span id="42d1" class="kz jq hh kv b fi le lb l lc ld">    # Combine streams into Q-Values<br/>    reduce_mean = Lambda(lambda w: tf.reduce_mean(w, axis=1, keepdims=True))  # custom layer to reduce mean</span><span id="cd45" class="kz jq hh kv b fi le lb l lc ld">    q_vals = Add()([val, Subtract()([adv, reduce_mean(adv)])])</span><span id="49b5" class="kz jq hh kv b fi le lb l lc ld">    # Build model<br/>    model = Model(model_input, q_vals)<br/>    model.<em class="jc">compile</em>(Adam(learning_rate), loss=tf.keras.losses.Huber())</span><span id="e36c" class="kz jq hh kv b fi le lb l lc ld">    return model</span></pre><h1 id="14db" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">代理人</h1><p id="be2b" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">最后，我们实现了代理的逻辑。</p><p id="c2bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了克服开发与探索的权衡，我们使用ε贪婪策略，因此对于概率ε，我们的代理将采取随机行动(ε在训练开始时会很高，后来会降低)。</p><p id="8921" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们也有两个神经网络，训练和目标网络，如前所述。</p><p id="d78a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在学习功能中，我们可以看到用于计算梯度和更新训练网络权重的贝尔曼方程，然后我们有保存和加载功能，只保存两个网络和重放缓冲区，这样您就可以从离开的地方重新开始训练会话。</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="1bc0" class="kz jq hh kv b fi la lb l lc ld">import json<br/>import os<br/>import numpy as np<br/>import tensorflow as tf</span><span id="cf8b" class="kz jq hh kv b fi le lb l lc ld"><em class="jc">class</em> DDDQN_AGENT(<em class="jc">object</em>):<br/>    <em class="jc">def</em> __init__(self,<br/>                 dqn,<br/>                 target_dqn,<br/>                 replay_buffer,<br/>                 n_actions,<br/>                 input_shape=(84, 84),<br/>                 batch_size=32,<br/>                 history_length=4,<br/>                 eps_initial=1,<br/>                 eps_final=0.1,<br/>                 eps_final_frame=0.01,<br/>                 eps_evaluation=0.0,<br/>                 eps_annealing_frames=1000000,<br/>                 replay_buffer_start_size=50000,<br/>                 max_frames=25000000):</span><span id="0f1b" class="kz jq hh kv b fi le lb l lc ld">        self.n_actions = n_actions<br/>        self.input_shape = input_shape<br/>        self.history_length = history_length</span><span id="8c96" class="kz jq hh kv b fi le lb l lc ld">        # Memory information<br/>        self.replay_buffer_start_size = replay_buffer_start_size<br/>        self.max_frames = max_frames<br/>        self.batch_size = batch_size</span><span id="f370" class="kz jq hh kv b fi le lb l lc ld">        self.replay_buffer = replay_buffer</span><span id="d025" class="kz jq hh kv b fi le lb l lc ld">        # Epsilon information<br/>        self.eps_initial = eps_initial<br/>        self.eps_final = eps_final<br/>        self.eps_final_frame = eps_final_frame<br/>        self.eps_evaluation = eps_evaluation<br/>        self.eps_annealing_frames = eps_annealing_frames</span><span id="e548" class="kz jq hh kv b fi le lb l lc ld">        # Slopes and intercepts for exploration decrease<br/>        # Calculating epsilon based on frame number<br/>        self.slope = -(self.eps_initial - self.eps_final) / self.eps_annealing_frames<br/>        self.intercept = self.eps_initial - self.slope*self.replay_buffer_start_size<br/>        self.slope_2 = -(self.eps_final - self.eps_final_frame) / (self.max_frames - self.eps_annealing_frames - self.replay_buffer_start_size)<br/>        self.intercept_2 = self.eps_final_frame - self.slope_2*self.max_frames</span><span id="a2ca" class="kz jq hh kv b fi le lb l lc ld">        # DQN<br/>        self.DQN = dqn<br/>        self.target_dqn = target_dqn</span><span id="9ded" class="kz jq hh kv b fi le lb l lc ld">    <em class="jc">def</em> get_action(self, frame_number, state, evaluation=False):<br/>        """<br/>        Get the appropriate epsilon value from a given frame number and return the action to take (ExplorationExploitationScheduler)<br/>        """<br/>        # Calculate epsilon based on the frame number<br/>        <br/>        if evaluation:<br/>            eps = self.eps_evaluation<br/>        elif frame_number &lt; self.replay_buffer_start_size:<br/>            eps = self.eps_initial<br/>        elif frame_number &gt;= self.replay_buffer_start_size and frame_number &lt; self.replay_buffer_start_size + self.eps_annealing_frames:<br/>            eps = self.slope*frame_number + self.intercept<br/>        elif frame_number &gt;= self.replay_buffer_start_size + self.eps_annealing_frames:<br/>            eps = self.slope_2*frame_number + self.intercept_2</span><span id="6c51" class="kz jq hh kv b fi le lb l lc ld">        # With chance epsilon, take a random action<br/>        if np.random.rand(1) &lt; eps:<br/>            return np.random.randint(0, self.n_actions)</span><span id="8105" class="kz jq hh kv b fi le lb l lc ld">        # Otherwise, query the DQN for an action<br/>        q_vals = self.DQN.predict(state.reshape((-1, self.input_shape[0], self.input_shape[1], self.history_length)))[0]<br/>        return q_vals.argmax()</span><span id="50a9" class="kz jq hh kv b fi le lb l lc ld">    <em class="jc">def</em> update_target_network(self):<br/>        """<br/>        Update the target Q network<br/>        """<br/>        self.target_dqn.set_weights(self.DQN.get_weights())</span><span id="3131" class="kz jq hh kv b fi le lb l lc ld">    <em class="jc">def</em> add_experience(self, action, frame, reward, terminal, clip_reward=True):<br/>        """<br/>        Just to simplify the add experience recall function<br/>        """<br/>        self.replay_buffer.add_experience(action, frame, reward, terminal, clip_reward)</span><span id="6356" class="kz jq hh kv b fi le lb l lc ld">    <em class="jc">def</em> learn(self, batch_size, gamma):<br/>        """<br/>        Sample a batch and use it to improve the DQN<br/>        """</span><span id="8531" class="kz jq hh kv b fi le lb l lc ld">        states, actions, rewards, new_states, terminal_flags = self.replay_buffer.get_minibatch(batch_size=self.batch_size)</span><span id="64b3" class="kz jq hh kv b fi le lb l lc ld">        # Main DQN estimates best action in new states<br/>        arg_q_max = self.DQN.predict(new_states).argmax(axis=1)</span><span id="72d5" class="kz jq hh kv b fi le lb l lc ld">        # Target DQN estimates q-vals for new states<br/>        future_q_vals = self.target_dqn.predict(new_states)<br/>        double_q = future_q_vals[<em class="jc">range</em>(batch_size), arg_q_max]</span><span id="1bc8" class="kz jq hh kv b fi le lb l lc ld">        # Calculate targets (bellman equation)<br/>        target_q = rewards + (gamma*double_q * (1-terminal_flags))</span><span id="8a05" class="kz jq hh kv b fi le lb l lc ld">        # Use targets to calculate loss (and use loss to calculate gradients)<br/>        with tf.GradientTape() as tape:<br/>            q_values = self.DQN(states)</span><span id="5a1f" class="kz jq hh kv b fi le lb l lc ld">            one_hot_actions = tf.keras.utils.to_categorical(actions, self.n_actions, dtype=np.float32)<br/>            Q = tf.reduce_sum(tf.multiply(q_values, one_hot_actions), axis=1)</span><span id="85a4" class="kz jq hh kv b fi le lb l lc ld">            error = Q - target_q<br/>            loss = tf.keras.losses.Huber()(target_q, Q)</span><span id="f0c9" class="kz jq hh kv b fi le lb l lc ld">        model_gradients = tape.gradient(loss, self.DQN.trainable_variables)<br/>        self.DQN.optimizer.apply_gradients(<em class="jc">zip</em>(model_gradients, self.DQN.trainable_variables))</span><span id="8d25" class="kz jq hh kv b fi le lb l lc ld">        return <em class="jc">float</em>(loss.numpy()), error</span><span id="0bd3" class="kz jq hh kv b fi le lb l lc ld">    <em class="jc">def</em> save(self, folder_name, **kwargs):<br/>        """<br/>        """</span><span id="03a4" class="kz jq hh kv b fi le lb l lc ld">        # Create the folder for saving the agent<br/>        if not os.path.isdir(folder_name):<br/>            os.makedirs(folder_name)</span><span id="76c9" class="kz jq hh kv b fi le lb l lc ld">        # Save DQN and target DQN<br/>        self.DQN.save(folder_name + '/dqn.h5')<br/>        self.target_dqn.save(folder_name + '/target_dqn.h5')</span><span id="f057" class="kz jq hh kv b fi le lb l lc ld">        # Save replay buffer<br/>        self.replay_buffer.save(folder_name + '/replay-buffer')</span><span id="8ce6" class="kz jq hh kv b fi le lb l lc ld">        # Save info<br/>        with <em class="jc">open</em>(folder_name + '/info.json', 'w+') as f:<br/>            f.write(json.dumps({**{'buff_count': self.replay_buffer.count, 'buff_curr': self.replay_buffer.current}, **kwargs}))  # save replay_buffer information and any other information</span><span id="b17c" class="kz jq hh kv b fi le lb l lc ld">    <em class="jc">def</em> load(self, folder_name, load_replay_buffer=True):<br/>        """<br/>        """</span><span id="0ca6" class="kz jq hh kv b fi le lb l lc ld">        if not os.path.isdir(folder_name):<br/>            raise ValueError(f'{folder_name} is not a valid directory')</span><span id="b9dc" class="kz jq hh kv b fi le lb l lc ld">        # Load DQNs<br/>        self.DQN = tf.keras.models.load_model(folder_name + '/dqn.h5')<br/>        self.target_dqn = tf.keras.models.load_model(folder_name + '/target_dqn.h5')<br/>        self.optimizer = self.DQN.optimizer</span><span id="8fc4" class="kz jq hh kv b fi le lb l lc ld">        # Load replay buffer<br/>        if load_replay_buffer:<br/>            self.replay_buffer.load(folder_name + '/replay-buffer')</span><span id="9000" class="kz jq hh kv b fi le lb l lc ld">        # Load info<br/>        with <em class="jc">open</em>(folder_name + '/info.json', 'r') as f:<br/>            info = json.load(f)</span><span id="96fd" class="kz jq hh kv b fi le lb l lc ld">        if load_replay_buffer:<br/>            self.replay_buffer.count = info['buff_count']<br/>            self.replay_buffer.current = info['buff_curr']</span><span id="69dd" class="kz jq hh kv b fi le lb l lc ld">        del info['buff_count'], info['buff_curr']  # just needed for the replay_buffer<br/>        return info</span></pre><h1 id="7513" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">培训和结果</h1><p id="26ec" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">在进入训练细节之前，我们需要使用一些东西来跟踪我们的进展，许多人使用tensorboard，我更喜欢<a class="ae ks" href="https://wandb.ai/site" rel="noopener ugc nofollow" target="_blank"> wandb </a>。它非常容易使用，而且运行在云中，所以你可以从任何一台电脑上查看进度。所以我们开始配置wandb:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="3bce" class="kz jq hh kv b fi la lb l lc ld">config = <em class="jc">dict</em> (<br/>  learning_rate = 0.00025,<br/>  batch_size = 32,<br/>  architecture = "DDDQN",<br/>  infra = "Ubuntu"<br/>)</span><span id="1af7" class="kz jq hh kv b fi le lb l lc ld">wandb.init(<br/>  project="tensorflow2_pong",<br/>  tags=["DDDQN", "CNN", "RL"],<br/>  config=config,<br/>)</span></pre><p id="0aea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们定义环境，初始化目标和训练网络、重放缓冲区和代理:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="2494" class="kz jq hh kv b fi la lb l lc ld">pong_wrapper = PongWrapper(ENV_NAME, NO_OP_STEPS)<br/>print("The environment has the following {} actions: {}".<em class="jc">format</em>(pong_wrapper.env.action_space.n, pong_wrapper.env.unwrapped.get_action_meanings()))</span><span id="a540" class="kz jq hh kv b fi le lb l lc ld">MAIN_DQN = build_q_network(pong_wrapper.env.action_space.n, LEARNING_RATE, input_shape=INPUT_SHAPE)<br/>TARGET_DQN = build_q_network(pong_wrapper.env.action_space.n, input_shape=INPUT_SHAPE)</span><span id="2ea1" class="kz jq hh kv b fi le lb l lc ld">replay_buffer = ReplayBuffer(size=MEMORY_SIZE, input_shape=INPUT_SHAPE)<br/>dddqn_agent = DDDQN_AGENT(MAIN_DQN, TARGET_DQN, replay_buffer, pong_wrapper.env.action_space.n, <br/>                    input_shape=INPUT_SHAPE, batch_size=BATCH_SIZE, <br/>                   replay_buffer_start_size=REPLAY_MEMORY_START_SIZE,<br/>                   max_frames=MAX_FRAMES)</span></pre><p id="058a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后我们有main()函数，它将在包含训练循环的脚本中被调用，算法的评估与wandb:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="beee" class="kz jq hh kv b fi la lb l lc ld"><em class="jc">def</em> main():<br/>    global frame_number, rewards, loss_list<br/>    while frame_number &lt; MAX_FRAMES:<br/>        epoch_frame = 0<br/>        while epoch_frame &lt; EVAL_FREQUENCY:<br/>            start_time = time.time()<br/>            pong_wrapper.reset()<br/>            episode_reward_sum = 0<br/>            for _ in <em class="jc">range</em>(MAX_EPISODE_LENGTH):<br/>                action = dddqn_agent.get_action(frame_number, pong_wrapper.state)<br/>                processed_frame, reward, terminal = pong_wrapper.step(action)<br/>                frame_number += 1<br/>                epoch_frame += 1<br/>                episode_reward_sum += reward</span><span id="4c60" class="kz jq hh kv b fi le lb l lc ld">                # Add experience to replay memory<br/>                dddqn_agent.add_experience(action=action,<br/>                                     frame=processed_frame[:, :, 0], # shape 84x84, remove last dimension<br/>                                     reward=reward, clip_reward=CLIP_REWARD,<br/>                                     terminal=terminal)</span><span id="af67" class="kz jq hh kv b fi le lb l lc ld">                # Update agent<br/>                if frame_number % UPDATE_FREQ == 0 and dddqn_agent.replay_buffer.count &gt; REPLAY_MEMORY_START_SIZE:<br/>                    loss, _ = dddqn_agent.learn(BATCH_SIZE, gamma=DISCOUNT_FACTOR)<br/>                    loss_list.append(loss)</span><span id="ca4f" class="kz jq hh kv b fi le lb l lc ld">                # Update target network<br/>                if frame_number % NETW_UPDATE_FREQ == 0 and frame_number &gt; REPLAY_MEMORY_START_SIZE:<br/>                    dddqn_agent.update_target_network()</span><span id="b645" class="kz jq hh kv b fi le lb l lc ld">                # Break the loop when the game is over<br/>                if terminal:<br/>                    terminal = False<br/>                    break</span><span id="19bc" class="kz jq hh kv b fi le lb l lc ld">            rewards.append(episode_reward_sum)</span><span id="c040" class="kz jq hh kv b fi le lb l lc ld">            wandb.log({'Game number': <em class="jc">len</em>(rewards), '# Frame': frame_number, '% Frame': <em class="jc">round</em>(frame_number / MAX_FRAMES, 2), "Average reward": <em class="jc">round</em>(np.mean(rewards[-10:]), 2), \<br/>                      "Time taken": <em class="jc">round</em>(time.time() - start_time, 2)})<br/>        # Evaluation<br/>        terminal = True<br/>        eval_rewards = []<br/>        evaluate_frame_number = 0</span><span id="de90" class="kz jq hh kv b fi le lb l lc ld">        for _ in <em class="jc">range</em>(EVAL_STEPS):<br/>            if terminal:<br/>                pong_wrapper.reset(evaluation=True)<br/>                life_lost = True<br/>                episode_reward_sum = 0<br/>                terminal = False</span><span id="2b3c" class="kz jq hh kv b fi le lb l lc ld">            action = dddqn_agent.get_action(frame_number, pong_wrapper.state, evaluation=True)</span><span id="bce5" class="kz jq hh kv b fi le lb l lc ld">            # Step action<br/>            _, reward, terminal = pong_wrapper.step(action)<br/>            evaluate_frame_number += 1<br/>            episode_reward_sum += reward</span><span id="9405" class="kz jq hh kv b fi le lb l lc ld">            # On game-over<br/>            if terminal:<br/>                eval_rewards.append(episode_reward_sum)</span><span id="5245" class="kz jq hh kv b fi le lb l lc ld">        if <em class="jc">len</em>(eval_rewards) &gt; 0:<br/>            final_score = np.mean(eval_rewards)<br/>        else:<br/>            # In case the first game is longer than EVAL_STEPS<br/>            final_score = episode_reward_sum<br/>        # Log evaluation score<br/>        wandb.log({'# Frame': frame_number, '% Frame': <em class="jc">round</em>(frame_number / MAX_FRAMES, 2), 'Evaluation score': final_score})</span><span id="1853" class="kz jq hh kv b fi le lb l lc ld">        # Save the networks, frame number, rewards and losses. <br/>        if <em class="jc">len</em>(rewards) &gt; 500 and PATH_SAVE_MODEL is not None:<br/>            dddqn_agent.save(f'{PATH_SAVE_MODEL}/save_agent_{time.strftime("%Y%m%d%H%M") + "_" + <em class="jc">str</em>(frame_number).zfill(8)}', \<br/>                             frame_number=frame_number, rewards=rewards, loss_list=loss_list)</span></pre><p id="51d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，我们总结了调用主函数的脚本:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="a917" class="kz jq hh kv b fi la lb l lc ld">if __name__ == "__main__":<br/>    try:<br/>        main()<br/>    except KeyboardInterrupt:<br/>        # Save the model, I need this to save the networks, frame number, rewards and losses. <br/>        # if I want to stop the script and restart without training from the beginning<br/>        if PATH_SAVE_MODEL is None:<br/>            print("Setting path to ../model")<br/>            PATH_SAVE_MODEL = "../model"<br/>        print('Saving the model...')<br/>        dddqn_agent.save(f'{PATH_SAVE_MODEL}/save_agent_{time.strftime("%Y%m%d%H%M") + "_" + <em class="jc">str</em>(frame_number).zfill(8)}', \<br/>                             frame_number=frame_number, rewards=rewards, loss_list=loss_list)<br/>        print('Saved.')</span></pre><p id="f339" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如你所看到的，如果我们因为某种原因需要停止运行，脚本将建模和重放缓冲区，这样我们可以在多个会话中训练，这是非常重要的，因为要获得好的结果，它需要运行几天。</p><p id="6efe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面您可以看到培训奖励和评估分数，因为您可以看到代理在大约100万帧后变得非常优秀:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lv"><img src="../Images/973d65d6d74e83b37257419483079054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w3UhR96E0BqtlSkr"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lv"><img src="../Images/2cba38e29c5df74387b05aff62a28e47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ePQ1YtlWLvoqJBao"/></div></div></figure><p id="1cef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们可以看到代理是如何玩这个游戏的:</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="2fd6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以在这里找到<a class="ae ks" href="https://antonai.blog/?p=137" rel="noopener ugc nofollow" target="_blank">的原文</a>以及我的<a class="ae ks" href="https://github.com/antonai91/reinforcement_learning/tree/master/dddqn" rel="noopener ugc nofollow" target="_blank"> Github </a>上的所有代码。有任何问题，可以通过<a class="ae ks" href="https://www.linkedin.com/in/lisiantonio/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我。</p><p id="061f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你喜欢这篇文章，分享给你的朋友和同事吧！我将在下一篇文章中看到你，同时保重，保持安全，并记住<em class="jc">不要成为另一块墙上的砖</em>。</p><p id="f32e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Anton.ai</p></div></div>    
</body>
</html>