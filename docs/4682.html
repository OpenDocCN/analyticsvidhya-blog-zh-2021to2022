<html>
<head>
<title>PySpark CheatSheet and More</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark CheatSheet及更多</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pyspark-cheatsheet-and-more-a41b338c9d3e?source=collection_archive---------0-----------------------#2021-12-30">https://medium.com/analytics-vidhya/pyspark-cheatsheet-and-more-a41b338c9d3e?source=collection_archive---------0-----------------------#2021-12-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/4d9a4f66bf0a7383c1da6c10f2e54877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X0NdQ2IWPcvVzgIW"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx translated">安娜·克鲁兹在<a class="ae hu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/><h1 id="525e" class="iu iv hx bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">介绍</h1><p id="6605" class="pw-post-body-paragraph js jt hx ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">Apache Spark和Apache Hadoop都是大数据处理的开源框架。对于大规模数据处理，Spark可以比Hadoop快100倍，但是，Hadoop有分布式文件系统(HDFS)，而Spark是建立在弹性分布式数据集(<strong class="ju hy"> RDDs </strong>)上的。通常，我们在Hadoop之上使用Spark进行数据计算。</p><p id="632a" class="pw-post-body-paragraph js jt hx ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">在本文中，我们将重点关注<strong class="ju hy"> RDD </strong>的数据操作。</p><p id="49c3" class="pw-post-body-paragraph js jt hx ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">所有代码都可以在<a class="ae hu" href="https://github.com/chiang9/Medium_blog/blob/main/pyspark/spark_cheatsheet.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a>上找到。</p><blockquote class="kv kw kx"><p id="86a4" class="js jt ky ju b jv kq jx jy jz kr kb kc kz ks kf kg la kt kj kk lb ku kn ko kp ha bi translated">数据集:</p><p id="5a16" class="js jt ky ju b jv kq jx jy jz kr kb kc kz ks kf kg la kt kj kk lb ku kn ko kp ha bi translated">我们将使用<a class="ae hu" href="https://grouplens.org/datasets/movielens/" rel="noopener ugc nofollow" target="_blank"> movielens </a>数据集(ml-100k)进行演示。</p><p id="7171" class="js jt ky ju b jv kq jx jy jz kr kb kc kz ks kf kg la kt kj kk lb ku kn ko kp ha bi translated">Datacamp还为pyspark提供了一个清晰的备忘单。</p><p id="7fdc" class="js jt ky ju b jv kq jx jy jz kr kb kc kz ks kf kg la kt kj kk lb ku kn ko kp ha bi translated">更多信息:<a class="ae hu" href="https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python" rel="noopener ugc nofollow" target="_blank">https://www . data camp . com/community/blog/py spark-cheat-sheet-python</a></p></blockquote></div><div class="ab cl lc ld go le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ha hb hc hd he"><h1 id="ffe5" class="iu iv hx bd iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr bi translated">1.初始化火花环境</h1><p id="3b56" class="pw-post-body-paragraph js jt hx ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">每当我们想要使用Spark引擎时，我们需要首先初始化Spark环境。</p><h2 id="5555" class="lo iv hx bd iw lp lq lr ja ls lt lu je kd lv lw ji kh lx ly jm kl lz ma jq mb bi translated">火花背景</h2><p id="bc22" class="pw-post-body-paragraph js jt hx ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">驱动程序将使用SparkContext与集群进行连接和通信，并与资源管理系统协调作业。</p><h2 id="8ef5" class="lo iv hx bd iw lp lq lr ja ls lt lu je kd lv lw ji kh lx ly jm kl lz ma jq mb bi translated">火花会议</h2><p id="7b15" class="pw-post-body-paragraph js jt hx ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">从Spark 2.0开始，SparkSession构建了不同数据源的网关，比如SQL或Hive。在Spark 1.x中，我们需要定义SQLContext或HiveContext来与相应的源进行通信。</p><p id="ecfe" class="pw-post-body-paragraph js jt hx ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><em class="ky">简而言之，如果我们想要使用Spark数据帧或数据集，我们需要定义SparkSession，否则SparkContext可以执行RDDs </em></p><figure class="mc md me mf fd hj"><div class="bz dy l di"><div class="mg mh l"/></div></figure></div><div class="ab cl lc ld go le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ha hb hc hd he"><h1 id="85d6" class="iu iv hx bd iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr bi translated">2.加载数据</h1><p id="8b83" class="pw-post-body-paragraph js jt hx ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">我们能够用可迭代数据或外部数据源创建<em class="ky"> rdd </em>。我们将首先以文本字符串的形式加载movielen数据文件，稍后再进行操作。</p><p id="5210" class="pw-post-body-paragraph js jt hx ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><em class="ky">注意:在访问</em><strong class="ju hy"><em class="ky">rdd . todf()</em></strong><em class="ky">函数之前需要定义SparkSession，否则会抛出异常。</em></p><figure class="mc md me mf fd hj"><div class="bz dy l di"><div class="mg mh l"/></div></figure><pre class="mc md me mf fd mi mj mk ml aw mm bi"><span id="7036" class="lo iv hx mj b fi mn mo l mp mq">+------+---+<br/>|    _1| _2|<br/>+------+---+<br/>| apple| 10|<br/>|orange| 20|<br/>| peach|100|<br/>+------+---+</span></pre></div><div class="ab cl lc ld go le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ha hb hc hd he"><h1 id="8c99" class="iu iv hx bd iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr bi translated">3.资料检索</h1><p id="e0e7" class="pw-post-body-paragraph js jt hx ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">通过使用以下命令，我们能够检索rdd中的数据信息。</p><ul class=""><li id="bd06" class="mr ms hx ju b jv kq jz kr kd mt kh mu kl mv kp mw mx my mz bi translated">getNumPartitions()</li><li id="4fd8" class="mr ms hx ju b jv na jz nb kd nc kh nd kl ne kp mw mx my mz bi translated">计数()，计数键()，计数值()</li><li id="9594" class="mr ms hx ju b jv na jz nb kd nc kh nd kl ne kp mw mx my mz bi translated">collect()、collectAsMap()、take( <em class="ky"> num </em>)、top( <em class="ky"> num </em>)</li><li id="c9b2" class="mr ms hx ju b jv na jz nb kd nc kh nd kl ne kp mw mx my mz bi translated">max()、min()、mean()、stdev()、variance()、histogram( <em class="ky">箱号</em>)、stats()</li></ul><pre class="mc md me mf fd mi mj mk ml aw mm bi"><span id="cc98" class="lo iv hx mj b fi mn mo l mp mq">rdd_movie.take(3)</span><span id="f18a" class="lo iv hx mj b fi nf mo l mp mq">&gt;&gt; ['196\t242\t3\t881250949', '186\t302\t3\t891717742', '22\t377\t1\t878887116']</span></pre></div><div class="ab cl lc ld go le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ha hb hc hd he"><h1 id="9438" class="iu iv hx bd iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr bi translated">4.数据处理</h1><p id="0129" class="pw-post-body-paragraph js jt hx ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">在PySpark中处理数据可能会让你想起<strong class="ju hy">熊猫</strong>数据帧。与Pandas类似，PySpark也提供了对<em class="ky">分组、聚集、排序和归约</em>的功能。然而，有一些功能是相似的，但执行方式不同。</p><h2 id="c78b" class="lo iv hx bd iw lp lq lr ja ls lt lu je kd lv lw ji kh lx ly jm kl lz ma jq mb bi translated">地图与平面地图</h2><p id="4fbc" class="pw-post-body-paragraph js jt hx ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">地图和平面地图看起来相似，但它们是不同的，值得特别注意。</p><p id="6031" class="pw-post-body-paragraph js jt hx ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">map函数能够维护原始数据形状的列表结构，而flatMap会对列表结构进行解包，形成一个大的列表数据结构。</p><p id="38f2" class="pw-post-body-paragraph js jt hx ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">在下面的单元格中，我们将通过用适当的分隔符分隔rdd_movie rdd来直观显示这种差异。</p><figure class="mc md me mf fd hj"><div class="bz dy l di"><div class="mg mh l"/></div></figure><h2 id="e8b8" class="lo iv hx bd iw lp lq lr ja ls lt lu je kd lv lw ji kh lx ly jm kl lz ma jq mb bi translated">groupBy vs groupByKey</h2><p id="e8eb" class="pw-post-body-paragraph js jt hx ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">groupBy函数将根据输入中函数的结果对数据进行分组，groupByKey将根据原始rdd的键对数据进行分组。</p><figure class="mc md me mf fd hj"><div class="bz dy l di"><div class="mg mh l"/></div></figure><pre class="mc md me mf fd mi mj mk ml aw mm bi"><span id="9e6b" class="lo iv hx mj b fi mn mo l mp mq">groupby result:</span><span id="bf9e" class="lo iv hx mj b fi nf mo l mp mq">userid last digit = 6<br/>['196', '242', '3', '881250949']<br/>['186', '302', '3', '891717742']<br/>['166', '346', '1', '886397596']<br/>['6', '86', '3', '883603013']</span><span id="3a2c" class="lo iv hx mj b fi nf mo l mp mq"><br/>groupByKey result:</span><span id="be23" class="lo iv hx mj b fi nf mo l mp mq">rating = 1<br/>['22', '377', '1', '878887116']<br/>['166', '346', '1', '886397596']<br/>['181', '1081', '1', '878962623']<br/>['276', '796', '1', '874791932']</span></pre><h2 id="5a90" class="lo iv hx bd iw lp lq lr ja ls lt lu je kd lv lw ji kh lx ly jm kl lz ma jq mb bi translated">reduce vs reduceByKey</h2><p id="e852" class="pw-post-body-paragraph js jt hx ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">在深入研究代码之前，我们需要认识到<em class="ky"> reduce </em>是一个动作，而<em class="ky"> reduceByKey </em>是转换。</p><p id="2dfc" class="pw-post-body-paragraph js jt hx ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">原始文档上的定义:<a class="ae hu" href="https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html" rel="noopener ugc nofollow" target="_blank">https://spark . Apache . org/docs/2 . 2 . 0/rdd-programming-guide . html</a></p><p id="759b" class="pw-post-body-paragraph js jt hx ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><em class="ky"> reduce:使用func函数(接受两个参数并返回一个)聚合数据集的元素。该函数应该是可交换的和可结合的，这样它就可以被正确地并行计算。</em></p><p id="d22a" class="pw-post-body-paragraph js jt hx ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><em class="ky"> reduceByKey:当在(K，V)对的数据集上调用时，返回(K，V)对的数据集，其中每个键的值使用给定的reduce函数func进行聚合，该函数的类型必须是(V，V) = &gt; V。</em></p><p id="c238" class="pw-post-body-paragraph js jt hx ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">在本节中，我们将演示一些函数示例。</p><figure class="mc md me mf fd hj"><div class="bz dy l di"><div class="mg mh l"/></div></figure></div><div class="ab cl lc ld go le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ha hb hc hd he"><h1 id="a056" class="iu iv hx bd iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn ln jp jq jr bi translated">5.MovieLen数据集上的示例用法</h1><p id="cdba" class="pw-post-body-paragraph js jt hx ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">在这一节中，我们将使用PySpark RDD来处理电影的数据，并检索相应的流派。</p><figure class="mc md me mf fd hj"><div class="bz dy l di"><div class="mg mh l"/></div></figure><pre class="mc md me mf fd mi mj mk ml aw mm bi"><span id="94e2" class="lo iv hx mj b fi mn mo l mp mq">[['1', 'Toy Story (1995)', ['Animation', "Children's", 'Comedy']],<br/> ['2', 'GoldenEye (1995)', ['Action', 'Adventure', 'Thriller']]]</span></pre><p id="a98f" class="pw-post-body-paragraph js jt hx ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">感谢您的阅读，新年快乐。</p></div></div>    
</body>
</html>