<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/paper-review-26863d87d9e?source=collection_archive---------7-----------------------#2021-02-05">https://medium.com/analytics-vidhya/paper-review-26863d87d9e?source=collection_archive---------7-----------------------#2021-02-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><p id="5ad4" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">论文评论</p><h1 id="97a1" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">使用卷积自动编码器的深度聚类</h1><p id="ec61" class="pw-post-body-paragraph hf hg hh hi b hj jc hl hm hn jd hp hq hr je ht hu hv jf hx hy hz jg ib ic id ha bi translated">作者:郭喜峰，，恩珠，，国防科技大学计算机学院，2017。<a class="ae jh" href="https://xifengguo.github.io/papers/ICONIP17-DCEC.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="b59c" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated"><strong class="hi ji">结构</strong>:由于这篇文章是<a class="ae jh" href="https://arxiv.org/pdf/1511.06335.pdf" rel="noopener ugc nofollow" target="_blank">深度嵌入聚类(DEC) </a>的即兴之作，我将概述DEC，然后是利用卷积自动编码器(DCEC)进行聚类的深度聚类。</p><blockquote class="jj jk jl"><p id="470f" class="hf hg jm hi b hj hk hl hm hn ho hp hq jn hs ht hu jo hw hx hy jp ia ib ic id ha bi translated"><strong class="hi ji">注</strong>:我还增加了背景部分，这将为详细理解本文提供坚实的基础。</p></blockquote><p id="bd22" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">我的视频解说:<a class="ae jh" href="https://youtu.be/fxUaJSfZMxk" rel="noopener ugc nofollow" target="_blank">链接</a></p><h2 id="4006" class="jq if hh bd ig jr js jt ik ju jv jw io hr jx jy is hv jz ka iw hz kb kc ja kd bi translated">用于聚类分析的无监督深度嵌入</h2><p id="4967" class="pw-post-body-paragraph hf hg hh hi b hj jc hl hm hn jd hp hq hr je ht hu hv jf hx hy hz jg ib ic id ha bi translated">图1代表了DEC的直升机视图。该模型是2016年最先进的模型。</p><ul class=""><li id="f2c0" class="ke kf hh hi b hj hk hn ho hr kg hv kh hz ki id kj kk kl km bi translated">预训练<strong class="hi ji">堆叠自动编码器</strong> (SAE)</li><li id="6499" class="ke kf hh hi b hj kn hn ko hr kp hv kq hz kr id kj kk kl km bi translated">扔掉解码器</li><li id="7a6e" class="ke kf hh hi b hj kn hn ko hr kp hv kq hz kr id kj kk kl km bi translated">用k-means初始化聚类中心</li><li id="5387" class="ke kf hh hi b hj kn hn ko hr kp hv kq hz kr id kj kk kl km bi translated">用<strong class="hi ji">聚类损失</strong>(学生的t分布)微调模型</li></ul><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ks"><img src="../Images/cada93e0c612515ced98b79e0d3c2fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yRQVm-k4XCyFbJW6KB_53g.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">图1:用于聚类分析的无监督深度嵌入(直升机视图)</figcaption></figure><h2 id="737a" class="jq if hh bd ig jr js jt ik ju jv jw io hr jx jy is hv jz ka iw hz kb kc ja kd bi translated">使用卷积自动编码器的深度聚类</h2><p id="f6f1" class="pw-post-body-paragraph hf hg hh hi b hj jc hl hm hn jd hp hq hr je ht hu hv jf hx hy hz jg ib ic id ha bi translated">在本文中，作者指出了以前方法中的一些缺陷:</p><ul class=""><li id="3c97" class="ke kf hh hi b hj hk hn ho hr kg hv kh hz ki id kj kk kl km bi translated">DEC不考虑重建损失来微调模型，这可能会扭曲自动编码器的嵌入层并提供较差的结果。因此，在DCEC，作者考虑了重建损失和聚类损失来微调模型。</li><li id="e5da" class="ke kf hh hi b hj kn hn ko hr kp hv kq hz kr id kj kk kl km bi translated">堆叠式自动编码器由于其独特的训练方式，需要很长时间来预训练模型。因此，在DCEC卷积自动编码器的使用，因为它需要相对较少的时间来训练，并提供更好的结果。</li></ul><p id="9d2e" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">算法结构:</p><ul class=""><li id="4445" class="ke kf hh hi b hj hk hn ho hr kg hv kh hz ki id kj kk kl km bi translated">预训练<strong class="hi ji">卷积自动编码器</strong></li><li id="5b34" class="ke kf hh hi b hj kn hn ko hr kp hv kq hz kr id kj kk kl km bi translated">用k-means初始化聚类中心</li><li id="7a24" class="ke kf hh hi b hj kn hn ko hr kp hv kq hz kr id kj kk kl km bi translated">用<strong class="hi ji">聚类损失</strong>(学生的t分布)和重构损失微调模型</li></ul><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es li"><img src="../Images/0d24418bf897979e9f6031c250713743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v0IMmrYzd1giCv02r40XHg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">图2:卷积自动编码器的深度聚类(直升机视图)</figcaption></figure><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es lj"><img src="../Images/4b87ba8718aba6004186ecb042c89ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_-O_QhVM2GOrsxtFuQob6g.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">表1:无监督深度嵌入聚类(DEC)和使用卷积自动编码器的深度聚类(DCEC)之间的比较</figcaption></figure></div><div class="ab cl lk ll go lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="ha hb hc hd he"><h1 id="19ac" class="ie if hh bd ig ih lr ij ik il ls in io ip lt ir is it lu iv iw ix lv iz ja jb bi translated">1.背景</h1><p id="d3ab" class="pw-post-body-paragraph hf hg hh hi b hj jc hl hm hn jd hp hq hr je ht hu hv jf hx hy hz jg ib ic id ha bi translated">本节将为理解提议方法的次要细节提供坚实的基础。我不打算解释它背后的数学原理，但这一节将帮助你直观地理解这篇论文。本文有两个主要部分:</p><ol class=""><li id="672e" class="ke kf hh hi b hj hk hn ho hr kg hv kh hz ki id lw kk kl km bi translated">维数减少/特征提取，</li><li id="612c" class="ke kf hh hi b hj kn hn ko hr kp hv kq hz kr id lw kk kl km bi translated">聚类算法。</li></ol><p id="716a" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">我们的目标是对大量未标记的数据进行聚类，然后人们会天真地问，为什么需要降维。让我们首先通过MNIST数据集的例子来理解这一点。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es lx"><img src="../Images/30b9e928e5211d02020bfc6f83b497ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o1Q2GwKMfMMmMOIo-P709A.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">图1(a):28x 28图像的TSNE·2D(未降维)(b):降维至10维的TSNE·2D</figcaption></figure><p id="a2d0" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">图1 (a)和(b)说明了降维的重要性。图1 (a)表示没有降低到更低维度的聚类。在这里，我们可以观察到数字是不可区分的。换句话说，数字没有很好地聚集在一起。图1 (b)通过减少到10个维度来表示TSNE 2D。不是使用28x28图像(784个参数)，而是将维度减少到10个参数，并将这些参数用于TSNE图。在该图中，我们可以观察到在训练autoencoder仅20个时期后相对较好地聚集的手写数字。Autoencoder减少了线性和非线性流形的维数。我们将在本章中详细理解这一点。</p><p id="f7b9" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">生成上述示例的源代码被上传到Jovian平台。<a class="ae jh" href="https://jovian.ai/1potdish/mnist-tsne" rel="noopener ugc nofollow" target="_blank">图1(a) </a>，<a class="ae jh" href="https://jovian.ai/1potdish/autoencoder-tensorflow-pytorch" rel="noopener ugc nofollow" target="_blank">图1 (b) </a></p><p id="ad2a" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">上面的例子证明了降低维数对于更好的聚类的重要性。但是，如果太低，就没什么帮助了。</p><h2 id="f334" class="jq if hh bd ig jr js jt ik ju jv jw io hr jx jy is hv jz ka iw hz kb kc ja kd bi translated"><strong class="ak"> 1.1降维/特征提取</strong></h2><p id="fbe9" class="pw-post-body-paragraph hf hg hh hi b hj jc hl hm hn jd hp hq hr je ht hu hv jf hx hy hz jg ib ic id ha bi translated">在这一小节中，解释了一些重要的降维方法，以及其中哪种方法适合于所提出的方法。</p><h2 id="0a17" class="jq if hh bd ig jr js jt ik ju jv jw io hr jx jy is hv jz ka iw hz kb kc ja kd bi translated">1.1.1 <strong class="ak">主成分分析</strong></h2><p id="1483" class="pw-post-body-paragraph hf hg hh hi b hj jc hl hm hn jd hp hq hr je ht hu hv jf hx hy hz jg ib ic id ha bi translated">这是线性流形中最基本的降维方法。<strong class="hi ji">最小化均方差</strong>和<strong class="hi ji">最大化方差</strong>都导致相同的PCA算法。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ly"><img src="../Images/2f3289cf09f49ecfd267b2945ce5651f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TH3FQbC0vKjAXJv-_0WLjA.png"/></div></div></figure><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es lz"><img src="../Images/a54423fb162448504b300eccae68a11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*4Dm5ebVNlfaDlHFNASwPAg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">图2 PCA</figcaption></figure><p id="73ed" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">图2主成分分析:在这里我们可以看到，我们如何用最小化均方误差和最大化方差这两种不同的方法得到相同的算法结果。<a class="ae jh" href="https://builtin.com/data-science/step-step-explanation-principal-component-analysis" rel="noopener ugc nofollow" target="_blank">参考</a></p><p id="cff2" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">限制:仅适用于线性流形。如图2所示，权重向量W1和W2将彼此正交。</p><h2 id="e9c5" class="jq if hh bd ig jr js jt ik ju jv jw io hr jx jy is hv jz ka iw hz kb kc ja kd bi translated">自动编码器</h2><p id="b147" class="pw-post-body-paragraph hf hg hh hi b hj jc hl hm hn jd hp hq hr je ht hu hv jf hx hy hz jg ib ic id ha bi translated">Autoencoder可以降低线性和非线性流形的维数。因此，PCA的局限性可以通过实现自动编码器来克服。还有其他方法，如核PCA、局部线性嵌入(LLE)、ISOMAP(等距映射)等。可用于非线性流形。然而，这里我们将集中讨论自动编码器。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ma"><img src="../Images/1b2cba11192c7ebee743f4f9a8ff1c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RYtkg0S9y3QnbwU0rJAsVw.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">图3自动编码器</figcaption></figure><p id="1c05" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">图3显示了自动编码器的简单结构。𝑥代表输入向量，我们降低维度的前三个向量(蓝色)称为编码器，最低维度的层(黑色)称为瓶颈，而增加维度(橙色)的向量称为解码器。编码器降维，解码器与编码器正好相反，可以理解为向上采样数据的转置层。</p><p id="d95a" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">损失函数将试图通过减小均方误差来减小输入向量和输出向量之间的差异。Autoencoder应该对输入数据足够敏感，以便重建数据(𝐸(𝑥、𝑥')，对输入数据不敏感，以免过度拟合(𝑟𝑒𝑔𝑢𝑙𝑎𝑟𝑖𝑧𝑎𝑡𝑖𝑜𝑛)</p><p id="d58a" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">总误差=重建损失+正则化</p><h2 id="1eaf" class="jq if hh bd ig jr js jt ik ju jv jw io hr jx jy is hv jz ka iw hz kb kc ja kd bi translated">1.1.3堆叠自动编码器</h2><p id="2e2f" class="pw-post-body-paragraph hf hg hh hi b hj jc hl hm hn jd hp hq hr je ht hu hv jf hx hy hz jg ib ic id ha bi translated">栈式自动编码器(SAE)以其独特的训练方式在特征提取方面非常流行。这可以从图4中理解。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mb"><img src="../Images/3a0a7b664d0dff165c3ca7ea6735567e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*giVwvi9VVCWwuG5iOgW8sw.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">图3堆叠式自动编码器</figcaption></figure><p id="cc4d" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">堆叠式autoencoder单独训练每一层，一旦所有层都被训练，它就将它们彼此堆叠起来。正因为如此，它需要更多的参数来训练，因此需要更多的时间。在更深编码器层的情况下，更优选的是寻找不同的方法。(在本文中，作者提出了不同的方法，证明比SAE更好)</p><h2 id="54aa" class="jq if hh bd ig jr js jt ik ju jv jw io hr jx jy is hv jz ka iw hz kb kc ja kd bi translated">1.1.4可变自动编码器</h2><p id="63f7" class="pw-post-body-paragraph hf hg hh hi b hj jc hl hm hn jd hp hq hr je ht hu hv jf hx hy hz jg ib ic id ha bi translated">众所周知，可变自动编码器用于生成图像。简单自动编码器和变化自动编码器之间的关键区别是编码器组件和损失函数。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mc"><img src="../Images/b18a9fd938c8279e5d354493311abc08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EjRxxiTVILhSt-1jfeDrwQ.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">图4变型自动编码器(<a class="ae jh" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" rel="noopener" target="_blank">参考</a>)</figcaption></figure><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es md"><img src="../Images/92a47caf520f7c5906a63fbf79c2c8d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eXQAQQqC_sn1Oa2TKbif9A.png"/></div></div></figure><p id="5884" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">KL散度不利于均值和对数方差向量与标准正态分布参数显著不同的观测值(均值向量= 0，对数方差= 0)。示例代码可在<a class="ae jh" href="https://keras.io/examples/generative/vae/" rel="noopener ugc nofollow" target="_blank"> Keras网页</a>上获得。</p><h2 id="90c9" class="jq if hh bd ig jr js jt ik ju jv jw io hr jx jy is hv jz ka iw hz kb kc ja kd bi translated">1.1.5 <strong class="ak">卷积自动编码器</strong></h2><p id="f73b" class="pw-post-body-paragraph hf hg hh hi b hj jc hl hm hn jd hp hq hr je ht hu hv jf hx hy hz jg ib ic id ha bi translated">在提出的方法中，作者使用卷积自动编码器来提取低维特征。它非常类似于简单的自动编码器。变化在于输入矩阵的形状和参数的形状。不是对输入的原始图像进行矢量化。例如，在MNIST的情况下，数据输入形状将是(28 × 28 × 1)。这种结构从图5中很容易理解。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es me"><img src="../Images/03fa3921c260c1abcb986bbff4935e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LGWbmLGFl3wcDBP60I1lTg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">图5卷积自动编码器</figcaption></figure><p id="b390" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">与堆叠式自动编码器不同，它需要的训练参数相对较少，性能优于SAE，这意味着训练时间更少。此外，提供更好结果。总损耗将与等式2相同。实现了与图5相同的结构，notebook可以在jovian平台上使用。</p><h2 id="fda3" class="jq if hh bd ig jr js jt ik ju jv jw io hr jx jy is hv jz ka iw hz kb kc ja kd bi translated"><strong class="ak"> 1.2聚类方法</strong></h2><p id="08f9" class="pw-post-body-paragraph hf hg hh hi b hj jc hl hm hn jd hp hq hr je ht hu hv jf hx hy hz jg ib ic id ha bi translated">在这一小节中，解释了不同的聚类方法及其局限性和优点。对于本文来说，理解两种主要方法很重要:1)硬分配(k-mean方法)，2)软分配。</p><h2 id="9e6d" class="jq if hh bd ig jr js jt ik ju jv jw io hr jx jy is hv jz ka iw hz kb kc ja kd bi translated">k均值算法</h2><p id="1d77" class="pw-post-body-paragraph hf hg hh hi b hj jc hl hm hn jd hp hq hr je ht hu hv jf hx hy hz jg ib ic id ha bi translated">该方法采用k-means方法进行初始聚类。在接下来的会议中，我们将看到作者在何处以及如何实现，但现在让我们了解什么是k-means集群以及它是如何工作的。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mf"><img src="../Images/7e00a14ad7474556e613711961b87582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1g7NlUc4791rr-vc9A9T6w.png"/></div></div></figure><p id="96b5" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">由于k-means算法为数据点分配1或0，因此也称为硬分配。这种方法对初始猜测非常敏感。它可能收敛到局部最优，并可能提供不好结果。这种行为可以从图5中观察到。图5(a)收敛到全局最优，以及(b)收敛到局部最优。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mg"><img src="../Images/9f5767cde4ad13b07145b4db246512d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n5Zjwy_jREGLPLKs506HoA.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">图5 (a)(带蓝色边框)图5(b)(带红色边框)</figcaption></figure><p id="050d" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">为了更好的理解，k-means方法从零开始编码(用numpy和scipy)笔记本<a class="ae jh" href="https://jovian.ai/1potdish/k-means-method" rel="noopener ugc nofollow" target="_blank">上传</a>到jovian平台。</p><h2 id="266f" class="jq if hh bd ig jr js jt ik ju jv jw io hr jx jy is hv jz ka iw hz kb kc ja kd bi translated">1.2.2 <strong class="ak">期望最大化算法(EM算法)</strong></h2><p id="1bc1" class="pw-post-body-paragraph hf hg hh hi b hj jc hl hm hn jd hp hq hr je ht hu hv jf hx hy hz jg ib ic id ha bi translated">EM算法包括如下两个主要步骤:</p><p id="8185" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">步骤e:基于参数的当前估计，为对数似然的期望创建一个函数。</p><p id="b5f8" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">m步骤:计算参数，最大化在E步骤中找到的期望对数似然。</p><p id="160f" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">然后，这些参数估计用于确定下一个E步骤中潜在变量的分布。</p><p id="9201" class="pw-post-body-paragraph hf hg hh hi b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ha bi translated">K-means方法是EM算法的一个版本。对于聚类损失，作者使用了学生的t分布，这是一种不同于k均值(硬分配)的软标记方法。软标记意味着代替将数据点分配给一个固定聚类；它定义了每个聚类的概率。例如，90%分配给分类1，5%分配给分类2，5%分配给分类3。随着我们看到更多的数据，置信水平将增加，最终将收敛到高斯分布。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mh"><img src="../Images/d66aac27440399e1893ffb4b467f91ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PSkMFHYJrutBaeBKUMgJ1g.png"/></div></div></figure></div></div>    
</body>
</html>