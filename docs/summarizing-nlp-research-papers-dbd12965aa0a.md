# NLP 研究论文综述

> 原文：<https://medium.com/analytics-vidhya/summarizing-nlp-research-papers-dbd12965aa0a?source=collection_archive---------9----------------------->

## NLP 研究论文摘要博客和视频的阅读列表

![](img/04b6af94c4c743bb768156587616c402.png)

图片来自[来源](https://unsplash.com/photos/f2Bi-VBs71M)

T 他的博客是一个前哨，目的是组织我到目前为止在媒体上为**解释 NLP 研究论文**写的所有博客。同样为了便于搜索，我将所有的博客归类到一个共同的高层次主题下。

在你开始批评我没有在这篇文章中注明原创作者之前😠— *与作者、组织和论文相关的所有信息都出现在各自的博客帖子中*。😊*所以* *快乐阅读……*

**P.S.** 我会继续更新这个博客，当我添加更多的研究论文摘要时。 ***(最近更新时间:2021 年 6 月 12 日)—日/月/年***

# 到目前为止涵盖的主题…

1.  设备上 NLP *(1 张纸)*
2.  文本相似度 *(2 篇论文)*
3.  文本摘要 *(5 篇论文)*
4.  关键词提取 *(11 篇论文)*
5.  查询扩展 *(2 篇论文)*
6.  聊天机器人和对话系统 *(1 篇论文)*
7.  NLP 中的数据扩充 *(3 篇论文)*
8.  问题解答 *(1 篇论文)*
9.  其他 *(6 篇论文)*

**论文总数= 32 ……还有很多要走……**

![](img/d71f06711dad928a4a767fb60d77856f.png)

**设备上的自然语言处理**指的是经过优化的小型精确自然语言处理模型，可以直接部署在移动设备上。*下面是我迄今为止总结的一些博客*

1.  *高效系统用于* ***移动设备上的******([博客](/mlearning-ai/efficient-system-for-grammar-error-correction-on-mobile-devices-3a207105b7a3) / [视频](https://www.youtube.com/watch?time_continue=343&v=3rVn14m8zaM))***

***![](img/f81b964b27d102620833b68b93b575d9.png)***

*****文本相似度**的任务是测量任意两个文本片段之间的**接近度** **值**。它们可以在句法层次***(面向句法)***以及语义层次** *(面向意义)进行计算。以下是我迄今为止总结的一些博客—******

1.  ******一种基于* ***图的文本相似度*** *方法与自然语言处理* ( [博客](/mlearning-ai/a-graph-based-text-similarity-method-with-named-entity-information-in-nlp-abc7f1201d96))中的命名实体信息*****
2.  ********基于方面的文档相似度*** *使用变形金刚* ( [博客](https://link.medium.com/Vom69zlZMgb)，[视频](https://www.youtube.com/watch?v=ZO6QWG7-Ye0))*****

*****![](img/fc1247b3b1d468363d18d65008748dd8.png)*****

*****文本摘要的任务是缩短一组文档？，创建一个代表原始文档中最重要或最相关信息的子集？。下面是我迄今为止总结的一些博客*****

1.  ********飞马*** *:利用提取的间隙句进行抽象概括的预训练* ( [博客](/analytics-vidhya/pegasus-pre-training-with-extracted-gap-sentences-for-abstractive-summarization-acb238aa1096) / [视频](https://www.youtube.com/watch?v=QY8oZxS0txs))*****
2.  ******借力* ***伯特*** *对讲座* ( [博客](/analytics-vidhya/leveraging-bert-for-extractive-text-summarization-on-lectures-294feb643486) / [视频](https://www.youtube.com/watch?v=JU6eSLsp6vI))*****
3.  ******了解* ***T5 型号*** *:文字转文字转换变压器型号* ( [博客](https://towardsdatascience.com/understanding-t5-model-text-to-text-transfer-transformer-model-69ce4c165023) / [视频](https://www.youtube.com/watch?v=91iLu6OOrwk))*****
4.  ******实体级* ***事实一致性*** *抽象文本摘要* ( [博客](https://prakhar-mishra.medium.com/entity-level-factual-consistency-in-abstractive-text-summarization-cb19e8a48397) / [视频](https://www.youtube.com/watch?v=P9wr8IBfDQs))*****
5.  ********多句压缩*** *:在 Word 图中寻找最短路径* ( [博客](https://prakhar-mishra.medium.com/multi-sentence-compression-finding-shortest-paths-in-word-graphs-be02c9065bdc))*****

*****![](img/f9b566b8fe162fc6e1e0e14aee597b8f.png)*****

*******关键词提取**是识别最能描述内容的术语/短语的任务。它们可以是提取性的，也可以是抽象的。*以下是我迄今为止总结的一些博客—******

1.  ********自然语言处理中十大热门*** *关键词提取算法* ( [博客](/mlearning-ai/10-popular-keyword-extraction-algorithms-in-natural-language-processing-8975ada5750c))*****
2.  ********embebed****:使用句子嵌入的简单无监督关键短语提取* ( [博客](https://towardsdatascience.com/embedviz-simple-unsupervised-keyphrase-extraction-using-sentence-embeddings-97ed5e16ad00) / [视频](https://www.youtube.com/watch?v=ykClwtoLER8))*****

*****![](img/139e2d4bb9a8bfe5a32889dca63f0706.png)*****

*******查询扩展**是在信息检索系统中重构给定查询以提高检索性能的过程。下面是我迄今为止总结的一些博客*****

1.  ********伯特-QE:*** *用于文档重排序的情境化查询扩展* ( [博客](/nerd-for-tech/bert-qe-contextualized-query-expansion-for-document-re-ranking-4f0f421840b9) / [视频](https://www.youtube.com/watch?v=WAv6LsIJZbs))*****
2.  ******神经查询扩展为* ***代码搜索*** ( [博客](https://towardsdatascience.com/neural-query-expansion-for-code-search-3d60ebe8b751) / [视频](https://www.youtube.com/watch?v=QpTZ_-6uio8))*****

*****![](img/e05ee7e08df197c8abf0cf51831f6f7c.png)*****

*******会话智能体**是一个用自然语言理解并与人类交流对话的对话系统。下面是我迄今为止总结的一些博客*****

1.  ********DialoGPT:*** *会话回应生成的大规模生成性预训练* ( [博客](https://towardsdatascience.com/dialogpt-large-scale-generative-pre-training-for-conversational-response-generation-5ceb783428dc) / [视频](https://www.youtube.com/watch?v=Zo679MYoJns))*****

*****![](img/b5d5edc83125a7aae2e2cb4123c7a0de.png)*****

*******数据扩充**是一种广泛使用的技术，通过添加稍微修改的副本来增加现有数据的数据量。*以下是我迄今为止总结的一些博客—******

1.  ********EDA:简单的数据扩充*** *提高文本分类任务性能的技术* ( [博客](https://towardsdatascience.com/eda-easy-data-augmentation-techniques-for-boosting-performance-on-text-classification-tasks-3e61a56d1332) / [视频](https://www.youtube.com/watch?v=-1unNLkwImw))*****
2.  ******使用预先训练好的变压器模型进行数据扩充(* ***BERT、GPT2 等*** *)* ( [视频](https://www.youtube.com/watch?v=9O9scQb4sNo&list=PLsAqq9lZFOtUg63g_95OuV-R2GhV1UiIZ))*****
3.  ******通过* ***利用 NLP 云 API，如 spaCy、SyntaxNet、WordNet、NMT*** ( [视频](https://www.youtube.com/watch?v=6-fPy5j-Uzg&list=PLsAqq9lZFOtUg63g_95OuV-R2GhV1UiIZ))简化文本数据增强*****

*****![](img/3ccaccf1b4bec2671d9fc9b5c73cd971.png)*****

*****问题回答(QA) 是关于建立自动回答人类用自然语言提出的问题的系统。下面是我迄今为止总结的一些博客*****

1.  ******训练* ***问答*** *模型来自合成数据* ( [博客](https://prakhar-mishra.medium.com/training-question-answering-models-from-synthetic-data-research-paper-summary-2220186703f) / [视频](https://youtu.be/_fNYVuFrgP8))*****

*****![](img/2383ce4ba2d9b0ab906a307f793a2e25.png)*****

1.  ********ByT5*** *:用预先训练好的字节到字节模型走向无令牌的未来* ( [博客](https://towardsdatascience.com/byt5-towards-a-token-free-future-with-pre-trained-byte-to-byte-models-3638791a44b2))*****
2.  ******深度自然语言处理为* ***LinkedIn 搜索系统*** ( [博客](https://prakhar-mishra.medium.com/deep-natural-language-processing-for-linkedin-search-systems-6d136978bcfe) / [视频](https://www.youtube.com/watch?v=l3O7bCn1JI0))*****
3.  ********预训练、提示、预测*** *:自然语言处理中提示方法的系统综述(* [*博客*](/mlearning-ai/the-idea-of-prompt-based-learning-in-natural-language-processing-4055c77002fa) */* [*视频*](https://www.youtube.com/watch?v=K3MasIU25Zw) *)******
4.  ******基于情感词典的特征为* ***情感分析*** *短文本(* [*博客*](/mlearning-ai/features-for-short-text-sentiment-classification-432dc2cb556c) *)******
5.  ********无监督的话题分割*** *与伯特的会面* [*视频*](https://www.youtube.com/watch?v=uIdqcGNoI_o) */* [*博客*](https://prakhar-mishra.medium.com/unsupervised-topic-segmentation-of-meetings-with-bert-embeddings-summary-46e1b7369755) *)******
6.  ********BERT score****:用 BERT (* [*视频*](https://www.youtube.com/watch?v=Nq4VKXhumSY) */* [*博客*](https://prakhar-mishra.medium.com/bertscore-evaluating-text-generation-with-bert-beb7b3431300) *)******

> *****我希望你喜欢读这篇文章。如果你愿意支持我成为一名作家，可以考虑注册[成为一名媒体会员](https://prakhar-mishra.medium.com/membership)。每月只需 5 美元，你就可以无限制地使用 Medium*****

*****非常感谢您的宝贵时间。我希望你觉得这个前哨站有用。请把它分享给任何你认为可能从这个 🥰中受益的人*****